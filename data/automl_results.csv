brand,text,sentiment
Intel,Why does this need an article? It's a tweet by an official account praising their own product.,Negative
Intel,"The B580 has 200W TDP, in a perfect world and TDP scales linearly, the B770 would be 50% faster, that would put it around the 5060Ti/9060XT.  If the price also scales linearly, that would be around 375€, seeing that the 9060XT is going for 350€ now, it's gonna be tough competition.",Neutral
Intel,Im really looking forward to panther lake X. 4-4-4 core configuration and Xe3 iGPU with sr-iov is perfect for running a Linux-Windows mixed vm environment without having to get a gaming laptop with a dedicated GPU for virtualisation.,Positive
Intel,I hope the Linux driver support and performance is good in these,Positive
Intel,"Intel ARC needs to maintain their momentum. They have an excellent pricing strategy and genuinely compelling features, it's time they released a card that competes in the midrange. And no, I don't count the A770. As a B580 owner, increased ARC adoption rates will be sure to benefit all cards in the range, so I really hope that intel is committed for the long-haul here. They are not in the position to be burning consumers anymore",Positive
Intel,"Releasing a GPU more than 1 year after the B580 came out seems weird to me. Unless this is a new architecture, or is using Intel's own process, and fabs.",Neutral
Intel,"4070 performance for $350-400, I'm calling it now.",Neutral
Intel,Hopefully they've seen Nvidia and AMD fuck things up by having two VRAM configurations and know not to do that.,Negative
Intel,"300W? Sounds like they're chasing the big boys. Hope the performance justifies the power draw, leaks can be misleading.",Neutral
Intel,"Hello Revolutionary_Pain56! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,They wouldn’t need a B770 or mystery GPU if they actually released more than just a B50 to the masses.,Neutral
Intel,"I don't know what the driver situation is like a year later, but B580 was anywhere between a 4060ti and a 3060 (or less if the driver really choked), so comparing B770 to a single Nvidia point of reference probably isn't the whole story.  Intel has been selling a big chip with a lot of hardware relative to what they charge, so when the drivers work Battlemage can punch way above its price class. I expect the same this time.",Neutral
Intel,"It's been deleted, so it might even be inaccurate.",Negative
Intel,Ad revenue.,Neutral
Intel,Trying to apply logic or rules to the internet is a waste of time.,Negative
Intel,"Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds. That however will mean a bigger die and viability might be questionable (considering they're already massive for the performance).",Neutral
Intel,"Price doesn't scale linearly because die sizes make defects scale quadratically. so pricing is the same, 2 50mm\^2 dies are cheaper than 1 100mm\^2 die     However in GPUs there is a fixed cost for every GPU so there is a sweet spot",Neutral
Intel,"As always, TDP is a semi-arbitrary figure and has little to do with what the GPU requires.  Most GPU's of today have heavily inflated TDP's simply to try and juice benchmarks on review day as much as possible.",Neutral
Intel,"The BMG-G31 is supposed to have 32 Xe cores in 8 render slices on a 256-bit memory bus, compared to the 20 Xe cores and 5 render slices on a 192-bit memory bus for the BMG-G21. Unless Battlemage is seriously memory bandwidth-limited, it should be almost 50% more performant.  The only question is die size. If it's 50% larger than the 270 mm^2 BMG-G21, that would exceed 400 mm^2. The GB203 in the RTX 5080 is 378 mm^2 for context.",Neutral
Intel,"That ""perfect world"" of yours seems to violate basic physics though...",Neutral
Intel,With tdp of 300 w it better be RTx 5070 or 9070 territory for much low price,Neutral
Intel,Intel never confirmed SR-IOV on Panther Lake - did they?,Neutral
Intel,"You can choose between high performance and crashes (xe) or low performance and stable (i915), and with Intel firing linux devs left and right I wouldn't expect much improvement any time soon.",Negative
Intel,That would be an amazing value proposition.,Positive
Intel,Rtx 5070 16gb for 380$,Neutral
Intel,I’d be happy if they didn’t gate the Arc Pro B60 behind bad distributors.,Negative
Intel,"So banking on the hope, that *everyone* ***else*** *somehow falls behind by accident*, only for Intel to succeed?  If that's their business-plan (looking at their foundry-woes, it seems it is), that's an awfully idiotic business-model.  ---- Last thing I heard, was redditors moaning about en masse that monopolies are bad. *Which one is it?!*",Negative
Intel,"The B50 is not a gaming GPU and actually underperforms in gaming tasks compared the the B580. They need to have an actual range of cards, not just a budget option, and even more budget option, and a server/workstation GPU. The B770 is essential to compete in the midrange",Negative
Intel,"A year later the drivers are fantastic, seriously not even a single hiccup. Been playing Hogwarts legacy at 4k 60fps with Xess Quality upscaling, and no frame gen.",Negative
Intel,"> Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds.   The problem with this idea, is that this would cost them far more money, as you need more die space, which they already use relatively inefficiently compared to nVidia.  They can't really afford not to use every bit of die space they have for all that its worth.",Negative
Intel,"Battlemage doesn't have the ability to add more Xe cores per render slice, this is something Intel has changed for Xe3. The BMG-G31 will have 128 ROPs, the same as an RX 9070 XT, or more than an RTX 5080.",Neutral
Intel,"FWIW that is not how pricing structures work. It's a bit more complex than that.   E.g. defect rates scale with die size, that is true. But larger dies also have more budget for DFM structures, that can lead to fewer overall functional faults than smaller dies and more variability and binning opportunities. So although smaller dies tend to be on average cheaper, it is not necessarily that 2 dies half the size will be cheaper than the twice as large single die.",Neutral
Intel,"afaik it works on every iGPU since skylake, but the driver is not in the mainline kernel",Neutral
Intel,I'm using an Arc A770 right now in Linux.  With i915 performance was unusably (for me) low.  With xe it's been fine.,Neutral
Intel,"the driver is already open source right? i think it will get better over time on virtue of being open source, but relying on intel to fix it now probably isnt gonna pan out.",Neutral
Intel,"Well it kinda has to be, the 4070 came out nearly three years ago.",Neutral
Intel,5060 performance for twice the price isn't a good deal.,Neutral
Intel,"I could see that. Nvidia really bailed out Intel by making the 5070 not much faster than the 4070 without using MFG to cheat lol   Edit: for all the Nvidiots downvoting, [the truth hurts](https://www.techspot.com/review/2960-nvidia-geforce-rtx-5070/#RT-1440p-png)",Neutral
Intel,This seems an absurd overreaction. All I'm saying is they don't do a 5060ti or 9060xt situation where there's a 8 gig model and a 16 gig model.,Negative
Intel,"It’s not but the B50 is the only Arc Pro that isn’t gated behind a bad vendor like Hydratech.    If they can’t properly launch the B60, why should I trust Intel or it’s partners with the B770 or some mystery GPU?",Negative
Intel,5060 is not nearly as performant as the 4070,Neutral
Intel,"Interesting results. If this is representative for consumer laptops, Panther Lake is a much bigger upgrade than most here, including me, expected. But it almost seems too good to be true somehow.",Positive
Intel,is Geekbench a CPU or a GPU benchmark?,Neutral
Intel,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,How does this compare to the Snapdragon X2 Elite?,Neutral
Intel,4 pcores  8ecores 4 lpcores..,Neutral
Intel,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",Negative
Intel,"Is Intel just ""squeezing the toothpaste"" again ? Even a low-frequency single-core 288V gets 2,700+ on Geekbench, while the 285H gets 2,600+ in single-core and 14,785 on multi-core. Therefore, TL;DR: I don't see Panther Lake being a huge improvement over the current Alder/Arrow Lake pairing. We will have to wait and see the power consumption, though.",Neutral
Intel,"I’m sorry, but that’s awful? Only 9% better single core when it has a better node and a newer architecture? Compared to what Apple and Qualcomm achieve every year, that’s pathetic",Negative
Intel,"Probably because GeekBench 6 only scales to a certain point, where more cores won’t help with improving performance compared to improving core IPC",Negative
Intel,"Fr, I really need to get a new light laptop (bc my old one's hinge is broken), but starting to feel  like I'd be better off waiting for Panther Lake than compromising with a bulky gaming laptop....",Negative
Intel,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",Negative
Intel,cpu,Neutral
Intel,"Probably one of the worst benchmarks out there for multicore tbh, I’d be more curious about the cb r24 scores",Neutral
Intel,"Panther Lake doesn't bring any major changes to the cores. It's mainly about bringing node shrink, redesigned SoC, and new iGPU.",Neutral
Intel,"It’s obvious that this is the viewpoint of an outsider. Professionals would never look at it this way. Professionals first evaluate a processor based on its specifications, features, and process technology. Lunar Lake and Arrow Lake are *not* using some outdated process — they use TSMC’s then-most-advanced N3B node, Intel’s first time adopting it. Meanwhile, Panther Lake uses Intel’s own **18A** process.  Based on the current benchmark results, Intel’s 18A appears to outperform TSMC’s N3B by at least the same margin that **Intel 4** trailed behind N3B — which is an astonishing result.  Every day you hear people saying how much TSMC has advanced, how far ahead its processes are, how “outdated” Intel’s nodes are, how AMD’s processors using TSMC have excellent efficiency. These kinds of statements have been repeated endlessly over the past decade.  Yet today, Intel is using its newest process node to **clearly surpass** TSMC’s top process from just one year ago.",Neutral
Intel,"the real test will be how many watts the X9 388H needs to achieve its scores, because the 285HX needed like 90 watts to achieve its scores  so if the X9 could hit its scores while on its base TDP (65 watts) then thats a \~40% increase in efficiency, not bad",Neutral
Intel,"But it does that while clocked almost 6% lower, so the IPC gain is actually decent. Especially considering most people expected Panther Lake to be a side grade because of the small architectural changes on the cores.",Positive
Intel,"For the use cases of PTL, Geekbench (which is mostly consumer focused) is a good indicator.  It doesn't assume that its workloads are perfectly parallel, it assumes some threads are used more heavily than others, so its value in nT is influenced by its 1T.    If someone is using this for rendering or other highly parallelizable workloads they might want to look into a subtest or into an alternative benchmark, but for typical consumers it seems like Geekbench is a good approximation of their experience.",Positive
Intel,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",Negative
Intel,"Is macOS not an option? Because IMO, MacBook Air is *the* thin and light laptop to get, hands down.",Neutral
Intel,"Couple of subtests leverage some new arm vector instructions and get huge scores, but those have limited influence to the overall score. Apple is better across the board, though the difference isn’t as big as the overall score suggests.   One difference is that since geekbench is distributed as binary it’s compiled more directly for apple architectures specifically while others use more generic targets. But that has very limited effect.",Neutral
Intel,But they have a gpu compute test too,Neutral
Intel,"Geekbench claims it's much more realistic than those multicore tests that scale nearly perfectly with tons of cores, and I think that's a fair take. It's not as if they didn't know how to create a benchmark that scales like other nT tests do, geekbench 5 nT does that.   I wouldn't call it worse, just different.",Negative
Intel,Geekbench runs common workloads as they are commonly implemented. It gives you a score on how well a multi core implementation of that workload would actually run in that CPU.   I think that is far more useful than some perfectly parallel workload measuring max power and core count.,Positive
Intel,"I have carefully compared the various models across Geekbench, PassMark, and the differences between Meteor Lake, Arrow Lake, and Lunar Lake. If my judgment is correct, the theoretical peak performance of the 484 in Cinebench R23 should reach around **24,500**; the 285H scores **22,500**. Compared with the 285H, it should be easier for the 484 to achieve high scores because its power requirements are significantly lower than the previous generation built on TSMC N3B.  Its peak performance will not be extremely strong because the frequency is not high. IPC is likely improved by around **10–11%**, but clock speeds drop by about **6%**. Overall, that means single-core performance should only rise by **4–5%**.  The improvement will be most noticeable in Geekbench. Since PassMark single-core also shows gains, the IPC uplift and resulting single-core increase should be quite certain. If Geekbench were the only source, it would still be questionable, but PassMark is more solid and has higher reference value.  Overall, in terms of peak performance, the uplift is average—around **10%**, close to that figure.  However, the real key is the **efficiency gains**. I believe they will be excellent. Compared with the 285H, which requires **65 W** to reach **20,000** points in Cinebench R23, I estimate that the **388H** may only need **40–45 W**.  I also estimate that the Cinebench R24 score should fall around **1300–1400**. Compared with Qualcomm’s X Elite 2 at **1950**, there is still a significant gap—but the two products differ drastically in scale.  Overall, Panther Lake’s greatest achievements lie in several aspects:  1. **Energy efficiency** — likely the best among all x86 products. 2. **Performance per mm²** — excellent. For example, the 484: if you look at its die shot, the total area of the CPU (including the CPU tile’s 4P and 4 LPE cores and all caches) is essentially equal to the die area of a traditional monolithic 8-core design. That means the 484 uses the same silicon resources as past 8-core chips, yet **no AMD mobile 8-core processor surpasses it**, either in raw performance or efficiency. 3. It also offers better performance-per-area than Qualcomm’s processors. The X Elite 2 has **18 cores**, including **12 “very large” cores**—similar in size to Intel P-cores—and **6 large cores**, each larger than Intel’s E-cores. The die area of this chip is **2.5× larger** than Panther Lake 484’s.",Neutral
Intel,"I mean, it does bring SOME changes to the cores, both are a next generation, it just isn't a more radical change like will be happening with NVL.  A mid single digit improvement is still pretty decent.",Positive
Intel,">The benchmark is very friendly to ARM and least favorable to AMD.   How so?   >The only valid reference is **same-generation, same-architecture comparisons**,  Geekbench is nice because it explicitly allows cross ISA comparisons. You don't have to take my word on it either, Intel and AMD themselves have used geekbench before to compare themselves to the ARM competition.   Same thing applies to spec and cinebench 2024.",Positive
Intel,My old 14900hx gets 35k multi core in cinebench r23,Neutral
Intel,"What's with the Cinebench fascination? At any rate. Geekbench 6 runs a raytracing test, and the 388H leak shows it at 29700 points compared to a 285H scoring 25300 points. That would place the Cinebench R23 scores at about 20% higher for the 388H  https://browser.geekbench.com/v6/cpu/15500755  https://browser.geekbench.com/v6/cpu/15474224  At any rate, the reason Geekbench doesn't scale perfectly with more threads is because a lot of workloads hit scaling limits due to Amdahl's Law, or memory bandwidth limitations. This applies to SPECint and SPECfp results for multiple threads as well.",Neutral
Intel,"lemme know once UTAU and Fighter Maker 2002 works on Arm macOS    (my point is that I work with a lot of old abandonware apps that barely even run on x86, so there's no chance in hell they gonna work on macOS)",Negative
Intel,"Thus useless to compare high CPU core counts.  If you actually need more than 8 cores you also have workloads that scale much better than Geekbench 6. It's especially dumb to claim this CPU is close to a 16 core, 32 thread zen 5 cpu based on Geekbench...",Neutral
Intel,I have workloads that scale fine with 16 threads and would scale fine with 32. People who actually buy high end multicore CPUs have a use for them.,Positive
Intel,It runs for far too short a time to reflect accurate multi-core performance.  People don't get a multitude of cores to run a task for a few seconds.  They do it for tasks that take minutes or hours to complete.  I'd argue it spends too little time on single-core tests as well.  I don't trust it to provide any useful information about anything other than transient performance.,Negative
Intel,I agree but the problem is it's being mindlessly used to compare MT scores as in this article.,Negative
Intel,Bro there's no need to spam this same comment like 4x in the same post's comment section T-T,Negative
Intel,"Why's the scaling ""problematic""? Its nT scaling is by design because GB6 is trying to replicate common consumer workloads which are rarely embarrassingly parallel. If you wanna see how well nT scaling for rendering is, there's cinebench for that.",Neutral
Intel,"was your 35,000 score achieved with power consumption above 100W? Can you try it now at 80W and see how many points are left? Also, limit it to 40W and check if it can reach 20,000 points. Because I estimate that the 388H has a chance to hit 20,000 points at 40W.",Neutral
Intel,"Ok, a simple no would have been fine.   Seems like those extremely old apps would run on any old POS x86 machine, if anything harder to run on modern hardware but hey what do I know. Best of luck.",Negative
Intel,"I mean, shortness is more of a problem if a device can cool itself properly or not rather than a problem of the CPU itself, unless said CPU in question is impossible to cool in that form factor",Neutral
Intel,"It provides useful information about the chip itself to real computer architecture enjoyers. Idk if gb6 changed it but geekbench has historically correlated with spec scores. Longer running programs like cinebench test the whole system including the thermal solution but geekbench gives a much better view into the pure performance of the cpu itself (and the associated memory system :/). Besides, you can always slap on a bigger cooler if thermals are that limiting.",Positive
Intel,Geekbench correlates with SPEC really well while taking a fraction of the time to run. Making it run for more minutes changes nothing,Negative
Intel,"It's being used for comparison because that's what we have. AFAIK, this is the *only* 388H benchmark we have",Negative
Intel,That looks like an AI post to me,Negative
Intel,It has been going hayway since SME just like GB5 had issues with AES Skewing results,Negative
Intel,Fair enough but I'd rather they kept something similar to GB5 multicore test in addition to their new 'more realistic' one.,Neutral
Intel,Incredible hardware news. Thanks for the share.,Positive
Intel,"Frankly, I wouldn't buy one for gaming, though I must admit Battlemage is pretty sweet for video editors thanks to 10-bit AV1 and 4:4:4 chroma on the HEVC side + you also get two codec engines (at least on the B580 with the same G21 core).  For perspective, you'll have to move up to Nvidia GB203 (RTX5070Ti), or better, to get your hands on two or more NVENC engines for the same 10-bit AV1 + 4:4:4 H.265.  If I was a serious video editor, this is *the* graphics card I would get.",Positive
Intel,The main bit that intrigues me about these ARC GPUs is their Linux gaming performance & how they compare to their windows performance.,Positive
Intel,Not in the same system the 1080 ti was in.,Neutral
Intel,"One interesting data point is he's testing with 7500f. We have no comparison with contemporaries or higher end CPU to examine CPU bottleneck, but it's a realistic scenario and system for the card.  Interesting how that 1gig made all the difference in TLOU2",Positive
Intel,"idk why, intel gpu is so expensive in my country like bruh that gpu perform worse than cheaper nvidia/amd. those sucker trying to scam buyer just cause ""intel"" name in it.",Negative
Intel,Yeah intel's quick sync is very good at video editing and streaming as well. Even preferred over nvenc in streaming (no idea about video editing),Positive
Intel,"I would and I did (Intel B50 gpu).  So far, zero regrets and zero issues on linux.  Edit: Fedora for those that are curious.",Positive
Intel,Rumour has it Linus torvalds uses an Intel card because he wanted something on a budget that could drive dual 6k screens.,Neutral
Intel,"For desktop use Intel on Linux is great, but gaming performance and compatibility is horrifically bad.",Negative
Intel,"it's the retailers, they don't sell well and need higher margins",Neutral
Intel,Its not a rumor lol he did a video with Linus tech tips and specifically requested they put a b580 in the PC they built him  [link to video](https://youtu.be/mfv0V1SxbNA?si=jT_3dFy1H40vrVjk),Neutral
Intel,"Performance is a little worse than on windows, but compatibility is not horrifically bad. It's pretty much the same as on windows.",Negative
Intel,"I believe Linus Torvalds wanted an ARC Pro B50, but settled for the B580 because that's what LMG could get their hands on",Neutral
Intel,can you imagine a bunch of nerds whispering about which graphics card an old man uses?,Negative
Intel,"I'm curious as to how much worse. I've considered an upgrade to a B570 due to them being seen for £150 new, putting it into used RX 6600/6600 XT territory, but if the Linux performance of say a B580 on Linux falls closer to either or, then it's probably not a worthwhile choice over the used AMD options for me.",Negative
Intel,"TLDW:    GPU Models Tested: MSI Shadow 2X RTX 5050, Intel Arc B580 FE      16 games average:    1080P, High-Ultra Settings:     Native TAA: Arc B580 is 14% faster, 23% faster at 1% lows due to higher VRAM        DLSS 4 Quality vs XeSS Ultra Quality: Arc B580 is ~11% faster     DLSS 4 Quality XeSS Quality: Arc B580 is ~20% faster     DLSS 4 Balanced XeSS Balanced: Arc B580 is ~15% faster     DLSS 4 Performance vs XeSS Performance: Arc B580 is ~14% faster",Neutral
Intel,"""There was a time, about a decade ago when the $250 price tag offered solid products, but the world has changed""  Yep, inflation. $250 in 2015 money is $342 in todays money. And you can get a very solid product at that price tier, the RX 9060 XT is $369 on Newegg.  GPU prices haven't gone up, you money is just worth way less.",Neutral
Intel,"5050 really has no right to exist at the price it does. B580 is obviously being sold at near cost or even a loss however, it's not exactly a fair comparison but that doesn't matter to consumers.  If you just want to game then I can't see any reason to consider anything else at this price point.",Negative
Intel,I'd still probably go nvidia here as I don't trust intel's compatibility with older titles and the like.   Still it would probably be better to spend $20 more on a 9060 xt 8 gb or $50 more on a 5060 than either of these.,Neutral
Intel,"If UE5 games generally run this poor on Intel GPUs, there might be trouble ahead as there are lots of those games in the pipeline.  You still couldn't get ~~more~~ me to buy an Intel GPU, even if I was desperate for a cheap GPU right now. I'd just adjust my settings.",Negative
Intel,"The B580 is decent enough, but it might be better to just save a bit more and get a 16GB 9060 XT for $350 or something. That card is likely to last 10 years flat at this point, and it will definitely last at least 5.  And yes the 5050 is not good. Getting something with a half-decent iGPU would be a better use of your money at that point.",Negative
Intel,The biggest issue is that he did not test PCIE 3.0 vs 4.0 vs 5.0. Those GPUs are very likely to go into budget builds or as upgrades to older motherboards like the B450.,Negative
Intel,Imagine spending $250 on a GPU when you could literally just save $100 more for like a 100% percent more performance.,Neutral
Intel,Wait isn't xess a lower resolution per quality setting?,Neutral
Intel,"yeah there is a reason why there are 5050s for 210  the thing is a sub 200 dollar GPU, which matches it capability and vram well, its more or less the I want to step up from igpu deal",Neutral
Intel,Maybe for the low end but high end I can’t even buy a card at msrp outside of America.,Negative
Intel,"*ignores that this is a 50 tier product and should be compared with the 950 and 1050*  This kinda of ""but but inflation"" virtue signalling I'd very unhelpful to these kinds of discussions. It's as if you're saying people should stop complaining gpus are several times more expensive than they used to be with the actual low end market completely destroyed.",Negative
Intel,Tech is supposed to beat inflation. Look at monitors or TVs or SSDs (before now) or CPUs or ....,Neutral
Intel,All the tech tubers are just turning into old men shouting at clouds. They will probably all be replaced by younger people living in the now soon enough.,Negative
Intel,"First of all 250 euros bought way more gpu in 2015 than 360 does today. And the lower end and midrange gpus were much less cut down vs the high end chipa today.  A 5050 sits where the 750ti did when the 980ti was out. Now you get entry level performance for mid end prices  Have wages actually increased that much? Because that is the only useful measure of ""inflation"". Everything else is just corporate profits   If prices for everything go up but wages don't then that leaves less money for frivolous shit like ram and storage and laptops and consoles, not more.  Even in my country where our wages are automatically indexed to match inflation, our purchasing power has dropped because the actual cost of living isn't properly represented in whichever calculation is used for the inflation number.  Houses have gone up by 100+ percent since 2015, rents have gone up by over 60 percent, grocery prices have more than doubled, utility prices have risen sharply, public transport has more than tripled in cost.  Minor expenses like clothing or a tv you buy every ten years have stayed flat, but that isnt what people are spending 80 percent of their income on.",Neutral
Intel,"Shh, everyone knows that prices only go up on luxury goods due to evil corporations, after all how will people live without their computer not being 800% faster than last year?",Negative
Intel,"> Yep, inflation. $250 in 2015 money is $342 in todays money.   People really need to stop using CPI. I can bet you that GPUs don't make it to the market basket. Yes, your money's value has fallen but not by that much.",Negative
Intel,You're getting downvoted for speaking the truth.  The RTX 5050 should be a $150-180 GPU for the price and value it offers but unfortunately people are gonna defend the price tag that the card was set for by Nvidia,Negative
Intel,"For a while you could get them for $229, which would be more acceptable vs a 5060 for $299, making it the same FPS/$. But the 5060 is actually the one on sale right now for only $30-$35 more. 30% faster for like 12% more money.",Positive
Intel,"I'd say that the cheapest new GPU that I'd blanket recommend with no ifs, buts and caveats is the 9060XT 16GB, everything below that either struggles with outright performance, VRAM or software issues like Arc.",Neutral
Intel,"Intel checks all the right boxes on paper (generous VRAM, decent pricing compared to competitors, an alternative to the duopoly) but the recent CPU overhead stuff coupled with the crapshoot that is trying to play older games and it just isn't worth it",Negative
Intel,"Yeah they cover this at the start of the video https://youtu.be/lLe5AP6igjw?t=229   XeSS 1.3 shifts everything down a tier, so their quality scaling ratio is everyone elses balanced ratio.  Older versions of XeSS match DLSS/FSR scaling ratios.",Neutral
Intel,"I know quality is, not 100% sure about others. dlss quality preset uses higher resolution than XeSS and FSR quality presets",Neutral
Intel,Its fine for people who need a dGPU but not a beast for work. think stuff like CAD or Photoshop. It will also be fine for people who only play competitive multiplayer games.,Neutral
Intel,Nvidia cards are bellow MSRP here in eastern europe. AMD cards slightly above MSRP.,Neutral
Intel,"They are all selling below MSRP in the UK. £979 is MSRP for a 5080 and I can buy 3 in stock models for less than that price without much searching, at scan.co.uk.  If you are in South America its probably your countries insane import taxes, protecting their home grown GPU market lol.  29 upvotes from children who have not bothered to check or do any kind of reasoning.",Neutral
Intel,"> Tech is supposed to beat inflation.  And it does, wtf are you trying to claim?  $100 CPUs these days run circles around 6700K which was the flagship in 2015. A B580 is faster than a GTX 980 Ti, which was the flagship card of 2015.",Neutral
Intel,"It does. For the price of a 1993 CRT TV, you can get a flat-screen LED thrice the size and with 10 times the resolution.  SSDs? A 2 TB nvme is a fraction today than a 128 GB Sata one was a little over a decade ago.   What actually changed is inflation, and that the buying power of today's middle class person decreased significantly relative even to the 2000s.",Neutral
Intel,Gpus are way more expensive to produce,Negative
Intel,"> Tech is supposed to beat inflation.   It does, despite wafer prices increasing the last 10 years.",Neutral
Intel,Wrote a fucking who? The fact that TVs are cheaper in nominal terms than they were 15 years ago does not mean it has to be the same thing with every other tech product. TVs are not products manufactured necessarily on cutting-edge expensive nodes.,Negative
Intel,"It does. For the same amount of money, you get way better GPU(unless your braindead thinks the gtx970 has same performance of 9060xt)",Positive
Intel,"It does, but also, inflation has been extremely bad for 5 years.",Negative
Intel,"You are just making shit up at this point. NVIDIA GeForce GTX 760 (2013) release price: $250.   That was a shit card, arguably a worse product than the 9060 XT is today, when you compare it to contemporary rivals. How do i know it was shit? I had it.",Negative
Intel,TSMC inflation is FAR higher than CPI. You are half right,Neutral
Intel,"according to US bureau of labour staticstics that measures the CPI it includes  all personal computers (desktops, laptops, tablets) and related equipment (printers, monitors, smartwatches, smartphones). It does not look at GPUs specifically, but the effect of that will be visible.",Neutral
Intel,I doubt ppl are gonna defend the 5050 considering a 5060 or an 8GB 9060XT is not much more and a fair bit faster.,Negative
Intel,"i mean, the 5050 off of amazon rn is 210, so it is getting there as a sub 200 dollar GPU for improving over iGPU right",Neutral
Intel,I pretty much but there's nothing else people *trust* in the category because apparently Arc cards are for professional nerds or something whereas I haven't had a real bad driver issue in over 2 years with my A770  People are also conditioned to fear older gen GPUs so 6xxx and 7xxx parts are sitting on shelves waiting for blowout discounts. People would still rather spend more on a basic nvidia from the 5000 series.,Negative
Intel,B580 user here. I coupled it with a R5 5500 and as of the recent updates the card just seemed to run much better vs when I got it last July.  There was a video before which also revealed that the CPU overhead is now being addressed in subsequent updates.  https://youtu.be/gfqGqj2bFj8?si=PyAfB2NhqZKWWVXY  I’d say it’s getting better and that I’d recommend it over a 5050 since the overhead is now fixed/negligible.,Positive
Intel,Intel is in their second GPU generation. Its going to take a lot longer to catch up with the institutional knowledge and practical application in videogames that the others were developing for over 20 years. The CPU overheard was not an issue in Intel iGPUs and Alchemist because GPUs never got fast enough to matter. It is only now that they noticed that issue since the GPU is far enough to create it.,Neutral
Intel,And that only in terms of native resolution and does not mean equal final image quality.,Neutral
Intel,\>dlss quality preset uses higher resolution than XeSS and FSR quality presets  Not exactly. FSR and DLSS are evenly matched in internals at all quality presets.,Neutral
Intel,I’m in Australia lol. The msrp for the 5090 is 1999 USD which translates to 3011 AUD The cheapest 5090 is 4800 AUD. That’s not even close at all to the msrp…,Neutral
Intel,"It sure is funny every time all those 6700k/8700k era CPU's pop up on used parts sites or FB Marketplace and still expecting close to initial prices.  Who even buys them anymore? At least a Q6600 has retro value ,but those are just obsolete.",Negative
Intel,"Not if you consider how the workloads being run on them have also changed. A GTX 970 ($450 inflation adjusted) would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  In other words, demand for performance has outstripped performance improvements, and those improvements are not felt as much.",Neutral
Intel,"A 100$ CPU in 2015 would easily run 2015 made software. A 100$ CPU in 2025 would barely run the electron JS slop. This includes Windows.  In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Yes yes yes it is very good on benchmarks but I don't stare at benchmarks all day. I use my computer for things you do at computer. Don't force my CPU to crunch how much digits of Pi it can compute.  A 980 Ti can easily run top 2015 games. Now? My laptop barely runs modern AAA games without looking a blurry mess. I simply can't fucking understand how you people look at the glorified motion blur and call ""yup it is the pinnacle of computer graphics"". How the fuck majority of modern AAA games look any better than RDR2 can anyone fucking tell me?",Neutral
Intel,"Why aren't you comparing relative buying power of 2014/2015 vs now then?  $650 got you what in 2014, a GTX 980TI?  $330 got you what in 2014? How close to the top end are both these things?    That's $900/$455 today, thereabouts.  What does $900 get you today?  Does that buy you anywhere near the top end?  And how does that product compare in relation to others above and below it?  Because the $330 product in question ($455 today) got you about ~75% to top end performance for ~half~ MSRP of the 980TI.  How does a $455 product of today square up relative to the top end?   Why don't we throw in a GTX 980TI vs a GTX 280 comparison while we're at it.  Make things really interesting.  I'll let you fill in those blanks (along with the $330 card in question) hoping you actually learn something in the process here.  The bar is very low, try not to trip.    The underlying point that user was making was pretty obvious if you read the comment they responded to.",Neutral
Intel,"cmon man, give him some slack, he just made shit up cause it's convenience for his argument.",Negative
Intel,How is it comparable? The 9060 XT is a very good card for 250. Ideally it'd be around 200 or below but for 250 you get a card that's a bit overkill for even 1080 P gaming.,Positive
Intel,In quite a few countries the 9060 XT is at or below the RTX 5050s MSRP.,Neutral
Intel,Then why did the comment above mine get multiple downvotes? It's Reddit and that's how it goes unfortunately,Negative
Intel,6x and 7x are priced far too high for old stock and are poor value compared to nvidias 50 series. They really haven't had a good price/performance low-mid end card since the 6700XT which are extinct at retail.,Negative
Intel,"I think they must have to go into every game, and adjust that t fix it, because it's not a universal fix it seems. Maybe per-game optimizations .",Negative
Intel,yep. XeSS 1.3 is closer to DLSS 3 rather than DLSS 4 in terms of image quality. Its good enough to game on in my opinion.,Positive
Intel,"> Who even buys them anymore?  The best SKUs on sockets have always demanded a premium in the used market. Since that's where people upgrading old machines will go.  And many machines from OEMs are not readily upgradable with just new boards/CPUs combos. Since they use custom form factors etc. So it's either a in socket upgrade or replace the whole machine. The socket 6700k is on is also especially affected by the ""premium"" factor. Since there's no lower end SKU with 4C/8T. You either get the 6700/7700 variants or are stuck with lower thread count.",Negative
Intel,"> would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  I think your memory is impacted by the expectations at the time. And the problem of reviews often using older titles inflating numbers, ffs some are still benching with GTA V to this day.   The [970](https://tpucdn.com/review/nvidia-geforce-gtx-1060/images/witcher3_1920_1080.png) couldn't even get 60 fps in witcher 3. Which was released in 2015.  And the performance it got in Witcher 3. Was not much better than what the 5060 Ti got in [Black Myth Wukon](https://tpucdn.com/review/msi-geforce-rtx-5060-ti-gaming-16-gb/images/black-myth-wukong-1920-1080.png)  Which even including 2025 titles. Is one of the hardest/heaviest titles with the worst performance. You can expect much better performance in almost every title. Just like the 970 was doing better than it did in Witchers 3.   But to argue that we got a lot better performance back then in the games releasing at the time, that is just false.",Negative
Intel,Yes 3.5GB of memory in 2015 was soooo much better than 12GB today /s,Positive
Intel,"All the GPU makers are betting on you using DLSS/FSR/XeSS as part of your usage to play games. Maybe even frame generation along with Relex, and all the other tech they ship GPUs with. They used to only rely on you using regular AA techniques.   If you ignore all those options you have today, and pay like it's 2015, it might be worse a lot of the time. If you use those options, you're generally way ahead of where a GTX 970 would fall. So it depends if you're willing to adopt new rendering tech, or rejecting it.",Neutral
Intel,"No, not really, in 2015, you happily accepted 45 FPS on not the highest settings at 1080p",Neutral
Intel,"This don't sounds like CPU problems at all, more like either Win11 is a vibe coded pile of bugs, or ACPI problems.",Negative
Intel,"> Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Something is wrong. I keep reading people's experiences of stuff like this and I haven't experienced it, I'm not doubting it but I'm so curious as to what is wrong.  In particular I read a lot of people saying Windows Explorer takes forever to open etc",Negative
Intel,"> In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  That's Windows for you.",Negative
Intel,it does not matter how close to the top GPU is. its a completely useless comparison.,Negative
Intel,I'm not sure what you're saying. The 9060xt is $250 only in 2013 money. They are arguing it's better value than a GTX 760.,Neutral
Intel,"Ppl up/downvote kinda randomly, doesn't really mean much post can go from +/-20 to the opposite real quick sometimes.  Anyways It's at +8 currently was at +something(2 maybe?) when i commented so who cares.",Neutral
Intel,"Nvidia and AMD do a lot of per-game optimization in the driver as well. In some cases very brutally, for example Nvidia is known for grabbing all games DX12 drawcalls and rearranging them in driver because the way game handles it is inefficient.",Negative
Intel,I remember upgrading to a 970 in 2016 and still being unable to max Witcher 3 at 1080p60 but got close enough,Neutral
Intel,"funnily enough, GTA 5 Enhanced Edition can be quite a benchmark for ray tracing nowadays. But it took to this year for it to be released. I think we can consider it a testbed for whats going to be implemented in GTA 6.",Neutral
Intel,"Bringing up 1080p no RT Wukong benchmarks sort of makes the point for me: the only way these cards look comparable is if we pretend features and standards are the exact same they were a decade ago.  High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen. RT was not a thing in 2015, now it is and Nvidia marketing really wants you to use it. It's like you're comparing Witcher 3 on Ultra settings to Wukong on Medium or High settings, and acting like it's apples to apples.  The moment you take modern displays and features (including DLSS to be fair) into account, it paints a picture where technology has moved on, developers and players would love to move on, and GPUs are struggling to make that jump.",Neutral
Intel,"if you run out of memory today the game swaps textures and continues running, it just looks uglier.   If you run out of memory in 2015 it starts using the superslow 0.5 GB and everything breaks.",Neutral
Intel,Nvidia is certainly expecting DLSS+FG to be the typical use case. The vast majority of their benchmark and marketing material is with those two.,Neutral
Intel,"I've got nothing against DLSS, I use it whenever I can, but sometimes it's just not enough to bridge that gap.  Another user brought up Witcher 3 and Wukong as an example of a graphically advanced 2015 game vs a graphically advanced 2025 game. The 970 would get 50+ fps on Ultra settings Witcher 3. Max out Wukong on a 5060 Ti and no amount of DLSS will make that card stop crying and screaming.",Neutral
Intel,"Like, Debian has no issues running on a n150 with multiple docker containers without instantly spiking the cpu to 100%.",Neutral
Intel,"I found a way to sort of kinda make file explorer slow. But its really a perfect storm thing. Have multiple screens, one of which is running in HDR and another in SDR. Have the file explorer tree open. Have a HDD, slower the better.  When you browse folders it refreshes the tree. When it refreshes the tree it asks connected devices if they are online, including the HDDs. Now move the window back and forth between your screens. When the explorer moves into HDR screen, it gets redrawn. Same when it moves to SDR screen. I suspect but cannot confirm there is a bug where the old instance is not cleaned correctly. So now when you browse it asks all devices if they are online 10 times. 100 times. At some point youll start noticing actual delays in opening folders.  Works even better if you havent restarted for a month.",Neutral
Intel,"the opening notepad thing, if you use taskbar it has a bad habit of not actually opening notepad until it finishes the online search for apps called notepad or whatever you typed. Disabling online search in start makes it fly really fast.",Negative
Intel,The 9060 XT 8GB is currently retailing for 250$ in many areas. I'm saying that the 760 isn't an ARGUABLY worse product. It is a worse product for it's time straight up.,Negative
Intel,"Fair point, but isn't lower and competitive prices good for us?",Neutral
Intel,">  and acting like it's apples to apples.  Apples to apples would be comparing W3 performance for both cards.  Wukong even without RT is a CONSIDERABLY more advanced game graphically than original W3.   >High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen.   And? Better monitors showing up doesn't change the laws of physics and basic economics. It doesn't make scaling with die shrinks suddenly increase. With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.   And before you start harping on about die sizes. The die in the 5060 Ti is actually more expensive than the die used on the 970. Wafer price increases more than compensates for the size difference.",Neutral
Intel,"Looked it up. Wither 3 got 52 FPS at Ultra settings, no Nvidia Hairworks turned on, for a GTX 970. Wukong gets 42 FPS at the cinematic preset native resolution, which is actually intended for cinematics, but developers allow people to enable anyways. As Digital Foundry has said, they maybe shouldn't.  Gets over 70 FPS if you turn the preset down 1 notch to high. No upscaling, or frame generation, or hardware RT, which is like what Nvidia Hairworks was for Witcher 3. It's really not hard to get Wukong to run at 90 FPS on a 5060ti with some minor tweaks.",Neutral
Intel,According to TPU review maxed out Wukong with DLSS got 42.3 fps. Not exactly the 50 fps you remmeberr for witcher but close. Heres a link to the review: https://www.techpowerup.com/review/black-myth-wukong-fps-performance-benchmark/5.html,Neutral
Intel,"That's very interesting thank you, I can definitely see why I haven't experienced it.  You genuinely wonder how Microsoft are testing these days.",Positive
Intel,"> With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.  Except in practice from 2005 to 2015 you got considerably more advanced graphics *and* higher resolutions *and* generally higher framerates too. Now either you pay up or you gotta pick one.  As for the rest of your post, it's more of a digression. All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.",Negative
Intel,"> As Digital Foundry has said, they maybe shouldn't.  hard disagree. As someone who does not have a lot of time for videogames and often end up playing older games with newer cards, those beyond high settings are great as it allows me to make use of my newer card and make the old game look better.",Neutral
Intel,"I disagree with comparing RT to Hairworks, when the visual impact as well as the emphasis put on it by Nvidia is so much bigger. I also disagree with using 1080p as a reference for Wukong, when high res and high refresh rate monitors are as cheap and plentiful as 1080p was back then.  Imagine you went back to 2015 and told the GTX 970 guy he's supposed to play his games at 2005 resolution and turn off antialiasing, how do you think he'd react?",Negative
Intel,">All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.  Why complain about something that there are valid reasons for lol",Negative
Intel,"It just makes such a small difference in UE5, it's really not worth losing 30% performance over for this engine. They would agree with you for a lot of other games, and Avatar Pandora kind of has a hidden setting, they'll maybe make available in menu at some point. Right now you need to modify a config file to enable it. Maybe they just need to wait until the final patch of a game to show those settings, years after launch, or just name them ""next gen"" or ""experimental"" with the setting below called ""ultra"".",Negative
Intel,"You can use DLSS and frame generation to play at higher resolutions. That's their intent. Especially UE5 games, because TSR was developed by Epic for a reason. The games on UE5 are really never intended to be run at a native resolution. I don't tell people to run UE5 at a 2013 resolution, but I also don't tell them to have the 2013 mindset that everything has to be run at native resolution, and that's the only way to play it. 1440p Balanced DLSS should give you around 50-60 fps without frame generation.",Neutral
Intel,"The performance loss does not mater for future (re)plays.  The config settings are usually hidden because during testing there were instabilities found that they didnt think was worth fixing. There were some games that had settings beyond ultra with names like ""Extreme,"" ""Nightmare,"" or ""Insane"".",Negative
Intel,"3050 performance at half the power, can't argue with that.  I do wonder how long it will be before the low-end dGPU disappears entirely from laptops, suspect these new chips probably aint it due to price but it's presumably coming.",Neutral
Intel,"While Geekbench's OpenCL isnt super popular for GPU testing;  I still cant believe a mainstream integrated GPU is actually competing with laptop 3050Ti, and these results still aren't with final optimizations!  by the time Panther Lake comes out, Intel claiming that its as fast as laptop RTX 4050 might actually be true lol",Negative
Intel,Are these better than Ryzen igpu? How do they compare to RTX 4050 AND 5050 in non demanding game?,Neutral
Intel,"I'm excited sure but if the claims are true how can anyone here expected these to be reasonably priced and sell well? If it competes with a laptop dedicated GPU it will be priced 2-3 times that laptop. The 285H isn't even that popular and its expensive, hope the 385H is affordable AND the claims are true.",Neutral
Intel,"The score is slightly above a GTX 1650 Super which in turn is slightly above a GTX 1060 which is barely matched by current most performing 128 bits iGPU (Lunar Lake / Strix Point)  Even if we expect 50% improvement compared to 890M, that's still half the performance level of a full power mobile RTX 3060. (roughly half a PS5 too)",Neutral
Intel,This would still no where be close to M4/M5 in single core and GPU,Neutral
Intel,"Hello 6950! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"The problem with buying a device relying on Intel graphics drivers, is that you're getting a device relying on Intel graphics drivers.  I'd rather not.",Negative
Intel,"iGPUs have already killed off the MX series, suppose it’s a real possibility other low end dGPUs also get killed off",Neutral
Intel,"More likely what is considered a low end dGPU will shift, just as a current low end dGPU would have been an high end model a decade ago.",Neutral
Intel,"iGPUs is why low end desktop parts dont exist anymore, its going to do the same for mobile parts.",Neutral
Intel,"It'll happen eventually but they have a few issues.  First is memory bandwidth, they'd need to put a wider bus on it to address which is costly.  Second is the memory itself, they need something with higher bandwidth (but that might be addressed with LPDDR6).  To address the bandwidth issue they often need to put on extra cache, the 12Xe3 core is basically maxing out the bandwidth of the 'standard' chip, and if they went to say 20Xe3 cores (which would be 4060+ level) then they would need a bigger memory bus and more cache (Strix Halo put 20mb of MALL cache to try and address, and a 256 bit bus) and that is more design work.  They'd also have an issue with the socket - the APU would be too big to fit into a standard socket and would need a custom one which means a custom motherboard which increases costs.  Another is heat - while more efficient overall, all the heat is in one spot so the computer needs to be thoughtful about moving that out.  A think that in 2028 big APU solutions can compete well against XX50 series and most XX60 cards.  It isn't happening before then however.",Negative
Intel,Apple's M5 already beats the 3050Ti though.,Neutral
Intel,I mean the early Iris pro onboard GPU’s traded blows with gtx 650m at a lower power draw. It’s been done before. Still nice to see but nothing ground breaking,Neutral
Intel,We will have to wait for the actual laptops to release & see how they do in real games; performance in synthetic benchmarks does not always line up in real tests.,Neutral
Intel,PTL should have comparable or possibly even lower production costs than ARL. There's some room for optimism.,Neutral
Intel,this GPU is also coming into the 355H btw (more specifically there's a Ultra X7 version of the 355H with the full 12-core GPU)     and iirc last time the 255H was pretty popular,Neutral
Intel,"Way we should think about it in my mind is that it's got 12Xe3 cores, and 1 Xe2 core = 2 RDNA 3.5 cores.  Strix Halo has 40 RDNA 3.5 cores (though with a bigger bus and enhanced cache) which is kind of like 20 Xe2 cores (maybe a bit more), and Strix Halo basically ties with a 4060.  If you assume a 5-10% improvement from the architecture going Xe2 to Xe3 to offset the bus and cache (which isn't *as* limiting with only 60% of the hardware), then you're probably at about 60% of a 4060 power, which is more or less where this looks to fall.  4060 mobile passmark = \~17,400  4050 mobile passmark = \~14,300 (81% 4060)  60% 4060 mobile passmark = \~10,500  3050 mobile passmark = \~10,100  That puts it pretty much at a 3050.  With a little sprinkle of cutting edge upscaling and a bit of frame gen it'll be just fine for light titles, but won't be a dedicated monster.  It's just a good solid chip for light-weight long-battery general purpose use that can play some titles on the side (or play older titles with confidence).  I suspect it'll review pretty well, since the reviewers tend to be excessively games-focused and this will appeal, but it's not a games machine - it's a surprisingly zippy long-life chip that's a good fit for all-day laptops.  EDIT: 8060S (Strix Halo) is \~18,000 for reference.  EDIT 2: 890M (strix point's top end) about 8,100, so the PTL 12Xe chip will smoke that.  I'm excited about it, it looks like a great chip!",Neutral
Intel,"It is not 2023 anymore. Intel Arc drivers work quite well, and Lunar Lake graphics are already pretty solid in the low power category. No need to keep repeating this nonsense when you haven't even used the devices.",Neutral
Intel,"Only sticking point seems to be cost I suppose, implementing a truly competitive iGPU looks like it will be expensive if Strix Halo is anything to go by.  I think the Intel/Nvidia partnership might at least come close to making it a reality to have a 50/60 tier integrated GPU in a laptop that isn't much more expensive than one with a dGPU currently... in theory it could/should be cheaper.",Neutral
Intel,Not really? Even optimistically speaking this won't be close to a 5050 laptop when accounting for bandwidth limitations in real world tasks that use the cpu and gpu at the same time.,Negative
Intel,"It's a good point. There really isn't any truly low end dGPU options out there now as far as I know, Nvidia abandoned the GT line and someone up there mentioned MX is dead etc.",Negative
Intel,I’m assuming the caveat to posts like this is “running a version of windows/linux”,Neutral
Intel,"and the 780M when fed enough power also comes within range, and the 890M also beats it.",Neutral
Intel,"It could beat a 5090, it would still be useless until the bootloader is open.",Negative
Intel,Wasn't the previous Intel igpu really good for games and efficient?,Positive
Intel,"Even if what you said was true, I know enough about Intel to know that even if it worked great, they'd find a way to fuck it up eventually. That's what they do.",Negative
Intel,">if Strix Halo is anything to go by.  I think Strix Halo skews how expensive a competitive iGPU could be. Strix Halo was very low volume and was also, imo, overbuilt. I think it's definitely possible to have a more cost competitive big-iGPU SoC",Negative
Intel,Prices for Strix Halo are rumored to fall in the new year; I'm curious to see if it ever reaches 1000-1500 dollar laptops like I've seen suggested it could. Seems like a ridiculous idea right now.,Negative
Intel,And in gaming.,Neutral
Intel,I mean...technically Asahi Linux exists for Macs. Though not the M5,Neutral
Intel,Then there's the 8050S & 8060S,Neutral
Intel,"How so? It is not even beating, at best barely even with 3050 35W (non-Ti).",Negative
Intel,I'm sure thats the first thing that comes to mind when people purchase a thin and light.,Neutral
Intel,"The Arc 140V like in Lunar Lake 268V? it's about 3050 Mobile (35W) sort of performance in real tests. There's a 140T in some chips too, I think it's less powerful than the V.",Neutral
Intel,Strix point is the better comparison,Neutral
Intel,"I hope so. I was a little surprised by the price but as you say, they clearly have a market for that thing and it isn't particularly price sensitive.",Neutral
Intel,they are not really comparable to the traditional APUs,Neutral
Intel,These are 256bit bus devices and have even fatter GPUs .....,Neutral
Intel,"Wait, is the 780/880m close to the 3050? I had heard that it was close to the 1050 at some point... As an 780m owner, it sounds like I can suddenly play way more games than I imagined?",Neutral
Intel,yes that one,Neutral
Intel,"Right, but Strix Point suffers from the same problems that PTL-X will: 128b bus chocking the memory bandwidth.  If iGPUs want to truly replace entry level dGPU, there needs to be wider bus variants",Neutral
Intel,"Might seem so, but mobile 3050 is way slower than let's say 1660 laptop.  Still, you can play many games, just at lower res and settings. You can look up benchmarks/tests for handhelds with this gpu (or ryzen z1/z1 extreme)",Neutral
Intel,"No doubt , I meant in terms of availability",Neutral
Intel,Why can't they have a low core count CPU but still keep the full iGPU?,Neutral
Intel,"While this has pretty much been known for a while, you really do get the feeling when looking at these frequency and power stats that Intel hoped for more from 18A.  It's a good node and a good chipset, I think it's targeted and designed well and will do well in the market, but it doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.",Positive
Intel,"6 different top models with just 100mhz between them is so stupid, that should be 1 or MAYBE 2 models for that core configuration  But just hope the 356H is cheap then",Negative
Intel,Is this desktop or laptop?,Neutral
Intel,">16C (4P + 8E + 4LP)  I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  Two clusters make sense. What am I supposed to do with three?!  Unless I'm mistaken, AMD seems to be sticking with full fat Zen 5 cores in both Strix Halo and ""Fire Range,"" and frankly, it sounds like Intel is just trying to keep up in the core count race with its plethora of 'baby' cores.  Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.   I use my laptop for convenience and my desktop for compute heavy work.",Neutral
Intel,"16 cores, but only 4 of them will be fast. The rest will be slow cores.",Neutral
Intel,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"Nice. My a770 still slaying 4k ultra gaming in 2025. No stutters no freezing just responsive beautiful gaming. Ofcourse it’s paired with 14900k so my igpu technically has identical specs but I keep temps down this way. Also have an RTX 50 series. I love arc graphics. Bang for buck AT HIGHER RESOLUTIONS — that’s where it’s at. And I’ve had 4090 (sold), amd 9070(sold). Never would I pay what they are asking for a 5090. Just because I wasn’t 100 percent thrilled with price per FPS with the 4090. Once you have had all of them you will see Intel is the way to go with 4k gaming(unless you like paying thousands of dollars). Arc doesn’t have shadows and artifacts . RTX does. Bad. Blurs my picture even on best settings possible.",Positive
Intel,Upselling,Neutral
Intel,IMO it's probably not economical for them to do it.   Gaming laptops/handhelds are already niche enough as it is -- doesn't make sense for them to package both and for OEMs to carry both for just for a ~$1-200 delta between the units. Those who want the full GPU will just pay for it.,Negative
Intel,"You gotta remember these CPU’s don’t have multi-threading. AMD gets 16 threads out of their 8 core z1 and z2 extreme but with these core ultra’s you get one core and that’s it. A full on physical core is better than a virtual core, but the same amount of cores with multi-threading is much better than just a same core-count CPU without it. Intel are offering up to 16 cores which is 16 cores/16 threads, but generally should offer better multi-core performance vs an 8-core 16-thread. If anything it’s nice Intel have the ability to improve core count offerings without significantly raising the power requirements.   I for one veered away from the MSI Claw 8 AI+ as a standalone PC solution because it lacked the multi-core performance I needed with it only having 8 physical cores and no multi-threading. A 16-core offering is exactly what I need.",Neutral
Intel,Because nobody would buy it.,Negative
Intel,"yeah, sucks cause they said handhelds was a priority for them",Negative
Intel,"Not great, not terrible. In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.",Negative
Intel,According to jaykihn0 it's a heat issue: BSPD and transistor density is flattening the VF curve on the high-end. The ST power curve is substantially improved at lower frequencies over Lunar Lake and Arrow Lake.,Neutral
Intel,"If anything, seems strictly worse than N3, even compared to older iterations. That's not great for a node that was supposed to go toe to toe with N2. Especially if the production cost is more in line with the latter.",Negative
Intel,"18a is initially launching with its lower power variant, which is why the only nova cpus that will use it for the compute tile are the laptop ones. The higher power variant comes later",Neutral
Intel,"Jaykihn, whose tweets these leaks are based on, says that the design characteristics cannot be extrapolated to the node itself.   So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.",Neutral
Intel,"> […] It doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.  Well, we already had rumors of Intel struggling to hit actual intended frequency-goals, no?  Though especially the actual performance-metrics of the node will be quite sobering I think.  The power-draw of these SKUs is supposed to be a TDP of *officially* 25W – In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4) and 65/80W (4+4+4 and 4+8+4) respectively.  So it must be seen, if these parts are any more efficient in daily usage, or just a side-grade to Lunar Lake.",Negative
Intel,"Panther Lake is pretty much a pipe cleaner, so it should still improve a bit.  Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD. If that is the case, it will only really be allowed to shine in high powered desktop and server chips, while power limited chips won't be able to reap as many of the benefits of 18A, since laptops tend to target the efficiency sweet spot, rather than clocks.  My understanding is that 18A will scale much better with extra power than N3 and N2. Once you pass the sweet spot, 10% extra performance on N2 might take 30% extra power, while 18A should continue to scale much more linearly for a good while longer.  So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones, especially in use cases where SRAM density is less of a concern. AFAIK Intel CPUs aren't as cache-dependent as AMD, so SRAM density might not be the highest priority, whereas slugging it out with AMD and trying to beat them on frequency might?  From what I've heard, Intel is still just about matching AMD in terms of IPC, while Arrow lake is very efficient, just not great for gaming. If Intel wants the CPU crown back, simply pushing more power could go a long way in clawing back the deficit, assuming 18A does indeed scale well with more power. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.",Neutral
Intel,> 6 different top models with just 100mhz between them is so stupid  They've always done that because of the OEMs who like a million segments.,Negative
Intel,Laptop.,Neutral
Intel,Since when is any user determining what cores they want to use and when?,Neutral
Intel,It makes sense to have three as the low-power cores prioritize keeping the power consumption low for battery life but to achieve this they aren't attached to the ring bus which hurts performance so 8 regular E cores are also used,Neutral
Intel,">I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  From a user's perspective, CPU's are a blackbox and the inner-working details are irrelevant. What matters is results. They want, typically, a balance of performance, battery life, heat / fan noise, and cost, placing greater emphasis on one of these categories over the others.  The idea is that P cores = performance optizied  E cores = Area optimized  LP-E Cores = power optimized.  The theory being to have different core types focus on different parts of PPA.  E cores and LP-E cores are the same microarchitecture. Just LP-E cores are off-ring so the ring (and rest of the cores) can be powered down in light-load scenarios.  Intel's biggest problem is that looking at each of the cores, the P cores perform their designated role the worst. 200% larger than E cores for \~10% more IPC and \~15% more clockspeed is a pretty rough trade off - and by the time you've loaded all P cores and need to load E cores too, that clockspeed difference diminishes hard.  Intel's better off (and all rumors point this way) to growing the E cores slightly and making that architecture the basis of the new P and E cores (like Zen vs Zen-C)",Neutral
Intel,"> Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.  You are the niche use case. The average/median user is using H-class laptops.   These are meant to beat Strix Point and its refresh, which it will do and at better economics for Intel than Arrow Lake H.",Neutral
Intel,"And with all the issues Windows already has with the scheduler, I fully expect it to never be able to properly utilize the different cores. I expect a similar situation as with previous versions, where it's sometimes necessary to disable the slow cores to eliminate stuttering in games, because they sometimes offload tasks to them which slows the whole game down to a crawl.",Negative
Intel,"> I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  > Two clusters make sense. What am I supposed to do with three?!  Ain't these Low-Power Efficiency-Cores aren't even usable by the user anyway in the first place (and only through and for Windows' scheduler to maintain)? As I understand it, LPE-cores are essentially placebo-cores for marketing.",Negative
Intel,Darkmont should really not be that far off Cougar Cove. Certainly not *slow*.,Neutral
Intel,The E-Cores are plenty fast for most use-cases by now and there are 8 of them in there. Aren‘t they even ahead of the skylake IPC by now? 8 will be doing a great deal with a good clock frequency.  Only the 4 LP-Cores are kind of weak. But they aren‘t attached to the ring-bus and really only used for offloading low req Backgroundtasks and an idling machine. So that you can turn off the faster cores when they aren‘t necessary.  But for performant gaming and productivity it is basically a 12 core machine.,Positive
Intel,I hope there's a 4+0+4+12 for handhelds but they might not consider adding a whole new tile configuration packaging line worth it over just giving the OEM a discount.,Negative
Intel,I find that weird. Who buys a 20 core CPU for GPU performance equal to like an RX 580?,Negative
Intel,"Yeah as of now only MSI looks committed to using Intel for their handheld, and not only are they not that successful but they had also branched out to AMD for another line of their gaming handheld.",Neutral
Intel,Gaming handhelds are niche but not gaming laptops (relative to gaming desktops). Steam hardware survey reveals a large number of mobile gpus very high up in the charts.,Neutral
Intel,Both Apple's M series and LNL use decent iGPUs for their 4+4 parts.,Positive
Intel,"This is a minority use case, but is a very valid one. I run a home server (/r/homelab) and could use some extra GPU power for video encoding and some basic AI stuff. Though this is just a home server so I don’t want to put in a dedicated GPU due to the desire to not spend all of my discretionary income on my power bill. Right now I have a 9th gen i7 that is good enough for the basics of what I am doing, but I’ve been keeping my eye out for a replacement. I’m not alone in that need, but I do get this is a minority group. I’m sure there are other use cases though. Saying no one would buy it is wrong.",Neutral
Intel,"IIRC Arrow Lake's compute tile is fabbed on N3B, which is pretty much TSMCs earliest N3 node for mass fabrication.  Its other tiles are on a mix of mature 5nm- and 7nm-class nodes.",Neutral
Intel,"> In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.  Arrow Lake was the pipe cleaner for 20A which in turn was deemed unnecessary to move into production because everything was going so well that they could use that groundwork to get a head start and move straight to 18A. That's right from Intel's mouth. Now the goalposts shift once again.",Neutral
Intel,"> Not great, not terrible.  For what it's worth, I'm glad and somewhat relieved, that the never-ending charade of 18A (and ultimate sh!t-show it eventually again amounted to, after their blatant Vapor-ware 20A-stunt) finally is about to come to some lousy end, after years of constant shady re-schedules, even more bi-weekly Intel-lies and basically +2 Years of delays again.  So it will be a bit of … 'consolation' I guess, not having to constantly read that 18A-sh!t in every darn Intel-news.",Negative
Intel,"I'm curious to see the core's power curve itself. Or the core+ring, I forget what exactly Intel measures here for their software power counters.   Intel claims PTL has outright lower SoC power than LNL and ARL, so if the power savings are coming from better uncore design and not the node gains themselves....",Neutral
Intel,"> BSPD and transistor density is flattening the VF curve on the high-end   He doesn't make this claim. BSPD in particular was supposed to help the most at high voltage, and the density does not appear to meaningfully differ from N3, which it still regresses relative to.    Best case scenario, the node, for other reasons, can't hit as high voltages as N3, and the top of the curve is just capped. But you wouldn't expect thermals to be such an issue then.    Doesn't seem to be any evidence of the core-level VF curve benefiting from the node either.",Neutral
Intel,"Jaykihn attributes only density to the supposed heat issues, not BSPD. Which makes sense because the process guy and Stephen Robinson both talked about having to space out the signal and power wires when working with BSPD.  If they prioritized density, then it is possible that frequency could not reach the theoretical maximum due to crosstalk or heat issues.",Neutral
Intel,"18A has always been low density, but expected to compensate for it with very efficient frequency scaling past peak perf/W. It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Wait for the desktop/server chips before you call it. With fewer thermal constraints and much higher power budgets, they should be able to push well past the perf/W peak, where it should continue to scale well and for a long time before it starts hitting severely diminishing returns.  I'm not talking 300W monster CPUs, but the scaling between a 65W and 105W 18A CPU should be more significant than on current nodes, where you might gain like 10-15% additional performance from that 50% power bump. I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.",Neutral
Intel,> So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.  Why would they suddenly decide to underclock it to such a degree?,Neutral
Intel,"Tbh, LNL battery life (note: not the same thing as loaded efficiency) with -U/-P/-H perf levels and market reach would still be a very good thing. It's just a shame to see Intel finally straighten out their SoC architecture only to be hamstrung by a subpar node.",Neutral
Intel,">In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4)      Intel's existing 2+8 parts have a PL2 of 57W, so seems to me OEMs are permitted to target the same PL2 they've been targeting on U series.  >and 65/80W (4+4+4 and 4+8+4) respectively.  Which is a fairly large drop compared to existing H series. ARL-H PL2 is up to 115W, PTL-H PL2 will be up to 65/80W",Neutral
Intel,">So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones,  NVL-S being external adds serious question marks to this.   I wonder if post NVL-S they go back to using further iterations of 18A though for even desktop, if they feel confident enough that they can hit high enough Fmax (even throwing away power and density) on those compute tiles.   Maybe for RZL or Titan Lake in 28' (going off memory for codenames lol).   >especially in use cases where SRAM density is less of a concern.  On paper, Intel has caught up to TSMC (even N2) in SRAM density.   >AFAIK Intel CPUs aren't as cache-dependent as AMD,  Generally speaking Intel uses way higher capacity caches than AMD, AMD uses smaller but faster caches.   >. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.  This seems *extreme*. Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?",Neutral
Intel,"I actually suspect otherwise - speculation on my part, but I suspect that they were running into unexpected process issues when they pumped the frequency and/or voltage higher so they had to scale back performance.  It almost feels like a process bug that they discovered when making the actual chips.  They've also talked about kind of weak yields on 18A on their latest earnings call.  That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  If they have a 'double upgrade' opportunity in both fixing the discovered 18A issues and implementing the originally planned 18AP improvements then it could end up being a somewhat bigger jump.  Don't think it'll be an N2 beater though.  18AP *might* beat the best of N3 in some applications, we'll see.  For that you'd want to look ahead to 14A where the CFO mentioned on the earnings call (and he was somewhat realistic in other areas so it's more believable here) that 14A has been a positive surprise and they're actually pretty excited about it.  It's possible that if Intel gets more experience on High-NA versus TSMC that they might start having an edge later on.  Maybe.",Neutral
Intel,"> Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD   And yet we see the exact opposite here. The leaker claims these go to to 65W, even 80W TDP. There's plenty of power to hit any ST boost the silicon can handle, yet it *regresses* vs N3B ARL. And that's with a year to refine the core as well.    > If that is the case, it will only really be allowed to shine in high powered desktop and server chips   Server chips are low voltage. Even lower typically than mobile chips. High-V only matters for client.    > My understanding is that 18A will scale much better with extra power than N3 and N2   There is no reason to believe that at this time. Notice that Intel themselves are using N2 for their next flagship silicon, including desktop.   Edit: Also, for the ""pipe cleaner"" rhetoric, remember how they cancelled 20A claiming 18A was doing so well and ahead of schedule? Even more obviously a lie now.",Neutral
Intel,Having to roll the dice on the scheduler doesn't make things better.,Negative
Intel,"Since release of Windows 7, which allowed users to easily set which cores are used on per application basis in first party software (task manager).",Neutral
Intel,"It's been a thing for a while as a means to circumvent the Windows kernel's shitty scheduling, especially on processors with asymmetric core or chiplet arrangements. Tbf, Windows has slowly begun to catch up and run cores a little more efficiently, but some people still swear by apps like Project Lasso for manually controlling their process affinity.",Neutral
Intel,"Users aren't ""determining"" anything because they don't have to.   On smartphones, they've virtually zero control over their own hardware.   And on the x86 side, I don't think people are buying big.LITTLE hardware in droves, and even if they are, I doubt the multiple core clusters are their deciding factor.   And if I had a big.BABY CPU, I’d definitely be tempted to play around with it, and yes, I don’t see the big idea behind having three clusters, aside from marketing gimmicks and artificially inflated MT benchmark scores.  From what I've heard, Windows Scheduler has trouble dealing with just two clusters as it is. On Linux, the experience is, at best, tolerable.  At the very least, I've certain issues with Intel marketing these CPUs has having ""16 cores."" I know a lot of 'normies,' first hand, who have fallen prey to this deceptive marketing tactic.",Negative
Intel,"> E cores and LP-E cores are the same microarchitecture   In this case, they're even the same physical implementation. 100% identical to the compute cores. Also, for MTL/ARL they were neither power nor area optimized.",Neutral
Intel,The average user is using U series,Neutral
Intel,">, which it will do and at better economics for Intel than Arrow Lake H.  Doesn't seem like this will be the case till the end of 26' though. At least from the earnings call a few weeks ago.",Neutral
Intel,"The average user runs apps like Teams, entirely off of the LPE core of Lunarlake and Pantherlake",Neutral
Intel,"LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  4 Darkmont LPE, in therory, should be a significant improvement.",Negative
Intel,"They are definitely more than usable if it’s anything like LNL, which basically defaults to them.  If it’s more like ARL or MTL, it’s placebo except for S0 sleep.",Positive
Intel,"The LP cores in Meteor Lake/Arrow Lake were too weak to be usable as multithreaded boosters, this isn't the case with Lunar Lake and Panther Lake will be no different",Neutral
Intel,"Windows are supposed to be using these LPE cores for lighter task such as web browsing or Word documentation or idling so that the they won't have to activate the rest of the cluster -> Reduced power draw and therefore good battery life. Meteor Lake originated with these but they were so slow they were pretty useless outside of idling. Lunar Lake also had 4 LPE cores functionally and we see how good those cores are, and these Panther Lake are supposed to be built on that but with additional 8 E cores for people that really value multi-core performance and to also solve the one major weakness of Lunar Lake, if it is even that major in the first place.",Neutral
Intel,>Aren‘t they even ahead of the skylake IPC by now  They're definitely ahead of Skylake now. Cougar Cove IPC vs Darkmont is gonna be like 10% better,Positive
Intel,> Only the 4 LP-Cores are kind of weak   Not for LNL or PTL. They're not crippled like the MTL/ARL cores. Should be full speed Darkmont. That's like 12th or even 13th gen big core.,Neutral
Intel,"The LPE cores being weak is probably the point anyway, or at least partially. It doesn't need to usurp an insane amount of wattage to get to full speed because it doesn't scale well, but it stays fast enough at lower wattage to default to that power profile -> Improved battery life.",Neutral
Intel,They are ahead of Raptor lake P cores and Zen 4 in IPC under 40W,Neutral
Intel,Honestly I wonder if the 4 e-core cluster on the compute tile is outright more power efficient than the 4 p-cores.    I wish some reviewers would do power efficiency testing with different core count configurations enabled on LNL and ARL. Just for curiosities sake.,Neutral
Intel,"At this point, I just hope they have a handheld chip with just 8 e cores (ensuring the CPU tile is as efficient as it gets) and a decent GPU. Maybe in time for an Intel + Nvidia chip, which could be a dream handheld chip.",Positive
Intel,Gaming laptop with iGPU should be niche. Most people just get Nvidia in their gaming laptops.,Neutral
Intel,Apple gets their margins in upselling memory and storage they don't need to worry about the added complexity of more SKUs for the sake of price laddering,Neutral
Intel,"Perhaps fully mature was an overstatement, but it wasn’t the first product on N3B, the node was already in HVM for over a year at that point IIRC.",Neutral
Intel,"We all know that's a lie and 20A was a disaster. Why repeat it now? 18A may not be perfect, but it exists and these chips are comming soon.",Negative
Intel,What delays? 2025 node in 2025?,Neutral
Intel,">BSPD in particular was supposed to help the most at high voltage  Says who?  BSPD improves signal integrity, lowers IR Drop, and helps improve density, at the expense of heat dissipation.   Idk how a technology that is known to increase heat density was supposed to *improve* fMax",Neutral
Intel,"> but expected to compensate for it with very efficient frequency scaling past peak perf/W   Quite frankly, the only people ""expecting"" that seem to be people on the internet unwilling to admit Intel underdelivered with 18A. There's been no objective reason to believe that was even a target for the node. If anything, the exact opposite. This was supposed to be the node where Intel focused more on low voltage for data center and AI.    > where you rarely push past the best perf/W due to thermal and battery constraints   That is not the case. ST turbo in -H series laptops has always been high, nearly on par with the desktop chips. At the power budgets being discussed, there's plenty available to hit whatever the silicon is capable of, and Intel's never been shy about pushing the limits of thermals.    And again, when you adjust for the power/thermal envelope, you *still* see a clock speed regression vs even ARL-H. The best possible outcome for Intel is actually that they can't hit the same peak voltage but look ok at low to mid.    > Wait for the desktop/server chips before you call it   Server is typically low voltage, or mid voltage at most. It's lower down than even laptop chips. And what desktop? Intel's using N2 for their flagship chips for NVL, which should really have been an obvious indicator for how 18A fairs.    > I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.   Intel had numbers in their white paper. BSPD, all else equal, was maybe a couple percent at high V. It's not going to produce dramatically different scaling, and again, we have direct evidence to the contrary here.    In general, Intel's had a lot of process features over the years that look a lot better on slides than they end up doing in silicon.",Neutral
Intel,"> 18A has always been *Xyz* …  There's always some excuse for Intel's stuff the last years, to accidentally NOT perform as expected, no?  > It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Hear, hear. Isn't that all to convenient …  > Wait for the desktop/server chips before you call it.  … and when is that supposed to happen? Nova Lake will be TSMC's N2.  What *Desktop* CPU-line will be on Intel 18A anyway then?",Negative
Intel,Give reasons why you claim this is an 'underclock' that was done 'suddenly'.,Negative
Intel,"Of course, Lunar was quite competitive, yet it was mainly so on performance due to Intel basically sugar-coating the living benchmark-bar out of it via the on-package LPDDR5X-8533 RAM. That OpM for sure *majorly* polished its efficiency-metrics by a mile …  Though looking back the recent years, that's how Intel always masked rather underwhelming progress on their process-technology – Hiding the actual architectural inefficiencies and shortcomings behind a invisible *wall of obfuscation* by only ever deliberately bundling it with other stuff like newer PCi-Express versions of 4.0/5.0 or newer, faster, crazy high-clocking RAM-generations.  So Lunar Lake while very strong, was mainly so due to being propped by OpM, and TSMC's processes of course.",Neutral
Intel,"The article claims it's ""TDP"", which would typically refer to PL1. Might be leakers getting their terminology mixed up, but I wouldn't necessarily assume a large drop in power limits.",Neutral
Intel,AFAIK Intel was at 165W in mobile back then …,Neutral
Intel,"> Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?  [6%](https://web.archive.org/web/20240901235614/https://www.anandtech.com/show/18894/intel-details-powervia-tech-backside-power-on-schedule-for-2024/2)",Neutral
Intel,"I wonder if the issues Intel's facing with 18A partially explain TSMC's very conservative approach to adopting ~~it~~ BSPD.  I image in reality, 18A (and AP's) fMax limits are almost entirely the reason for N2 in NVL-S",Neutral
Intel,"> That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  There's no doubt that given enough resources they can fix the node to the point where it's good enough for HVM in high performance chips. It's what they did with 10nm after all but it took them several years to go from poorly yielding Cannon Lake/Ice Lake to okay Tiger Lake and good Alder Lake.  If we take the talk about 18A ""margins"" from the recent analysts call to be a euphemism for something like node yields and a stand in for when it will be ""fixed"" then we could be talking 2027 before 18A gets to that point. A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.",Positive
Intel,"Yeah I know they talked about yield issues, but everything indicated that it was not a case of manufacturing defect density, but rather something else.   That would imply issues with hitting target frequencies, which as you say, they should be able to iron out in due time. That is as long as it isn't a fundamental issues, which I have a hard time believing, given how unconcerned they seemed about it.   I'm fairly confident it will be ironed out before Clearwater Forest hits mass production.",Neutral
Intel,"There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery. It offers better voltage control and less current leakage, leading to higher efficiency at any given voltage, but especially past the point where current leakage starts becoming more of an issue, i.e. at very high frequency.  A pipe cleaner running at low frequencies is to be expected. Wait for the desktop chips and you will see P cores hitting well over 6+ GHz advertised all core boost, possibly 6.5 GHz. I wouldn't be surprised to see some users hitting 7 GHz stable.",Positive
Intel,The modern CPU cores schedule all cores equally with E core priority because nowadays the difference between E core and P core is less than between AMD X3D CCD and higher clocking CCD,Neutral
Intel,"Windows Scheduler has come a long way but I mean come on, this is pretty pedantic. The commenter above is clearly not using a tool to assign cores to specific tasks they are just being annoying.",Negative
Intel,4 skymont already seem pretty good in LNL,Positive
Intel,> LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  Makes you think why Intel even went all the way to integrate those and waste precious die-space doing so …,Negative
Intel,They could be useful if they could check your email and stuff while the laptop is effectively sleeping.,Neutral
Intel,PLT's LP-E cores take after LNL. ARL/MTL's LP-E cores design is dead,Neutral
Intel,I hope those are powerful enough to eventually act as the low-power booster these were once supposed to.,Neutral
Intel,> Meteor Lake originated with these but they were so slow they were pretty useless outside of idling  The last info I had on the back of my mind was that these couldn't even be associated by the user and were basically reserved for Windows itself. Is that still the case now?,Negative
Intel,"Yeah after being reminded that the LPE cores are also Darkmont like the E-Cores I looked up the layout in detail and the differences is simply the P and E cores are on a large Cluster with L3 Cache and a fast ring-bus, but the LPE cores are seperated from that Ring-bus and don't have any L3 Cache (just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster).     They are for sure also much more limited in power-budget and frequency, but they can be very fast in general. The main usage is being able to disable the large cluster in Idle or low cpu usage to heavily reduce power draw.     The communication in between the clusters is much slower. So anything that relies on synchronisation and communication between cores is only really fast (latency-wise) if it is kept within the same cluster.      The LPE cores therefore can be very fast actually, but only if the task stays within the cluster. Modern demanding games will therefore 99% soley run on the large cluster, because they would run slower if they spread across all of them. The small cluster could be used to offload background tasks to themselves (if the powerlimits aren't hit), which could at least improve 1% lows and fps stability.",Neutral
Intel,"It's not just more power efficient, but also much faster at low power which you kinda want since you'd be mostly GPU bottlenecked. The good thing is that they've made that part of their thread director tech.",Positive
Intel,Yes it is. Memory uses power and IPC of LPE should be past Zen 3,Neutral
Intel,Intel makes e-core only CPUs (N100 etc) but at this point they're based on several gen-old e-core design. They're good enough for what they are so they might not update them for a while still though.,Neutral
Intel,"even 4 e-cores is enough, dont forget we use to pair up i7-7700K with GTX1080/1080Ti.  4 e cores + even stronger iGPU probably a better combo for handheld.",Positive
Intel,Apple absolutely does price ladder their SoCs.,Neutral
Intel,"True. But the node itself may be kinda mid. It seemed much, much more complex than N3E, and it's not as if the node was any sort of highlight in the products Apple used it in.   Going back to A17 reviews and such, there were serious questions presented about N3B vs N4P.",Neutral
Intel,I'm fairly sure that's what u/ProfessionalPrincipa was also implying.,Neutral
Intel,> We all know that's a lie and 20A was a disaster. Why repeat it now?   Probably in response to all the people assuming there must be some kind of upside.,Negative
Intel,"It was supposed to be a 2024 node. And given the perf downgrade and admitted yield problems, seems more like they're only delivering the originally promised metrics in '26 or even '27.",Negative
Intel,"> What delays? 2025 node in 2025?  What del— *Seriously now!?* Where you living under a rock the last years by any chance? o.O  18A is neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: *'cause Yields again!*) will be 1H26 and most likely even end of first half of 2026.  So it's a 2H24-node, which Intel is only able to offer actual volume basically +1–2 years later. That's called *»delay«*.  **Edit:** And not to mention the actual massive performance- and metric-regression u/Exist50 pointed out already. 18A is basically 20A in disguise. Since 20A wasn't actually knifed, but just relabeled as ""18A"" instead.",Negative
Intel,> Says who?   Intel. It was part of their own published results about PowerVia on the Intel 4 test chip. The gains range from ~negligible at low-V to around 6% at high-V.    Also explains why TSMC is not adopting it in the same way.,Neutral
Intel,Nova Lake is full on N2?,Neutral
Intel,"It's literally a regression from the prior gen, for a node that was supposed to be ""leadership"", mind you.    I don't know why it's hard to acknowledge that 18A is simply underperforming.",Negative
Intel,"Nah, credit where credit is due. LNL made a ton of fundamental design changes that PTL should also benefit from. Yes, they also benefitted a lot from both having an actually decent node and on-package memory, and no, they're not on par with the likes of Apple or Qualcomm, but merely making ARL monolithic on N3 would not have delivered these gains.",Positive
Intel,They definitely have their terminology mixed up. There's no chance PTL-H has an 65W/80W PL1,Negative
Intel,Which was never the PL1 but rather the PL2.,Neutral
Intel,"Didn't TSMC delay their BSPD node too? Ik they changed it from appearing in N2P vs A16, but unsure if there was a timeline shift in that as well.   Honestly, what's going on with N2P/N2/A16 seems to be kinda weird, timeline wise.",Negative
Intel,2027 just seems like hitting baseline parametric yields. Being able to get to an actual improvement in terms of performance is a whole other thing. Not an easy road.,Negative
Intel,">A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.  How it can be 24 product in 27 when it was 25 node in 25 with products on shelfs in january 26, rofl?",Negative
Intel,Intel themselves claim they won't be hitting industry acceptable yield till 2027 for 18A.,Neutral
Intel,"> There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery.    No, it's not. The main long term advantage of BSPD is to reduce the pressure on the metal layers as they do not scale like logic does. Better PnP is another advantage, but secondary. No one cares much about high-V these days.    > A pipe cleaner running at low frequencies is to be expected   As a reminder, ARL-20A was supposed to be the pipe cleaner, and they claimed they cancelled it because 18A was doing so well! PTL was supposed to be the volume product, a year after 18A was nominally supposed to be ready.    > Wait for the desktop chips   What desktop chips? PTL-S was cancelled long ago, and for NVL, they're moving the good compute dies (including desktop) back to TSMC on N2. A decision which should demonstrate the node gap quite clearly...   Also, ST boost for -H chips is in the same ballpark as desktop ones. They're already at the high end of the curve.",Neutral
Intel,"They aren't the same though, hence the roll of the dice to see if you actually get the performance you paid for.",Neutral
Intel,Because they still improve battery life under very light loads.,Positive
Intel,Yeah I think you had to utilize tool such as Project Lasso. I tried to assess the LPE cores to HWINFO64 and it was rather painful to use so I understand why these cores are never used otherwise lol.,Negative
Intel,"Yeah. The efficiency and overall battery life will probably comes down to how Windows manages threads, which seems to be pretty decent now with Lunar Lake seeing as they got pretty good battery life with the same design philosophy.",Positive
Intel,"> just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster  It's also accessible by the large cluster, fyi.",Neutral
Intel,"For those games back in the day, yeah. But modern games can and do use more than 4 cores.   Depends how low end/power you want to go, of course. 4 cores + eGPU cuold be enough too depending on your games: [https://youtu.be/XCUKJ-AgGmY?t=278](https://youtu.be/XCUKJ-AgGmY?t=278)",Neutral
Intel,"The e-cores are unlikely to be hyperthteaded though. 8 e-cores gives you 8 threads just like a 7700K did, though none of them share cores. It feels like a sweet spot for a handheld in 2025 imho.   This is while AMD competitors use 8c/16t of full Zen. An 8 e-core solution would remain weaker but more than sufficient on the CPU side, while allowing enough power for what truly matters (a capable GPU). If paired with an Nvidia iGPU, that could be the perfect handheld chip, if their partnership produces one like that.",Positive
Intel,"Those ""people"" are either bots or trolls. No sane person thinks Intel is crushing it with 18A.",Negative
Intel,"Whats the name of the close the gap initiative?  5 nodes in 4 years... announced in 2021  2021+4=2025  You can even read 3rd party articles from 2021 talking about 18A in 2025.  https://www.tomshardware.com/news/intel-process-packaging-roadmap-2025   >Intel didn't include it in the roadmap, but it already has its next-gen angstrom-class process in development. 'Intel 18A' is already planned for ""early 2025"" with enhancements to the RibbonFET transistors.",Neutral
Intel,">neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: 'cause Yields again!) will be 1H26 and most likely even end of first half of 2026.  How it aint 25 node when there will be products on shelves in january 26?",Neutral
Intel,FMax at same power consumption is higher true. But the temperature is just too high. Both temperature and power consumption are important considerations,Neutral
Intel,"High end NVL is N2, low end is 18A. At least for compute dies.",Neutral
Intel,"That's what we know so far, yes. At least the performance-parts. The lower end is supposed to be 18A, I guess?",Neutral
Intel,"> Nah, credit where credit is due.  I said verbatim, that Lunar Lake was quite competitive, and I meant it, unironically. It took long enough.  > LNL made a ton of fundamental design changes that PTL should also benefit from.  I haven't disputed that —  The modular concept was surely ground-breaking for Intel, and urgently needed!  Though, it's kind of ironic, how Intel all by itself proved themselves liars, when it took them basically +6 years since 2017, for eventually coming up with only a mere chiplet-copycat and their first true disintegrated chiplet-esque design …  For a design, which *in their world-view*, Intel was working on their tiles-approach already since a decade, which is at least what they basically claimed when revealed by 2018 – I called that bullsh!t the moment I first heard it. Surprise, surprise, they again straight-up *lied* about it …  They most definitely did NOT have had worked on anything chiplets/tiles before, if it took them *this* long.  Just goes to show how arrogant Intel was back then, letting AMD cooking their Zen in complete silence since 2012/2013, for their later Ryzen. *Still boggles my mind, how Intel could let that happen* …  > Yes, they also benefitted a lot from both having an actually decent node and on-package memory […]  In any case, we can't really deny the fact, that Intel basically cheated on Lunar Lake using OpM, eventually creating a halo-product, which ironically was quite sought after, but yet expensive asf to manufacture.  If AMD would've been to cheat like that (using OpM) in a mobile SKU, while dealing some marked cards in such a underhanded manner, it would've been ROFL-stomped Lunar Lake …",Neutral
Intel,"Willing to believe that, though then I have to wonder why it would be so hard to cool at such a substantially reduced PL2. Also if/how they could pitch this as an HX replacement.",Neutral
Intel,"Yes, I think it was 45- or 65W-parts PL1-wise. The spread was still obscene and outright insane.",Negative
Intel,I'm excited for 18A-P. Intel subnode improvements have always seemed to bring decent uplifts (Intel 10SF being a notable example).,Neutral
Intel,"The difference is no longer stark. As I said, its now smaller than between 2 CCDs in the most popular CPU line ever made in actual raw performance. ie it's not something entirely unique at the scale the differences are",Neutral
Intel,"How even, if these weren't even used with MTL!?",Neutral
Intel,"Yup, pretty much paper-cores for marketing-reasons alone basically.",Neutral
Intel,We already have games tested on Skymont E cores. They are very fast,Positive
Intel,"Yes we know modern games can use more cores, but having smaller amount of cores means saving power budget. Those power consumption saved are better off use to juice iGPU.   Lets not forget the E-Cores here are skymont, not skylake. Skymont is definitely more powerful cores. The exact same reason why i5-7600K is more power than i7-2600K.  given how iGPU for handheld are no way near GTX1080 yet. I think it is better to pump more juice for better iGPU.",Neutral
Intel,Go to the intel stock subreddit lol,Neutral
Intel,"You'd be surprised how many people are willing to take Intel at their word on it.  Just look at previous threads here. Every time Intel posts nonsense slides, lot of people come crawling out of the woodwork.",Negative
Intel,"They are doing about as much as the ""sane"" people expected from 18A.",Negative
Intel,"> 5 nodes in 4 years... announced in 2021  It was supposed to start with Intel 7 in '21 and end with 18A readiness in H2'24.   https://img.trendforce.com/blog/wp-content/uploads/2023/10/17144859/intel-4-years-5-nodes.jpg  And now in the way end of '25, we're getting something more like what they originally promised for 20A, *if that*, and now they're saying yields won't be ""industry standard"" until as late as 2027. It's a disaster by any objective standards. Their failure with 18A literally got the CEO fired...",Neutral
Intel,"> How it aint 25 node when there will be products on shelves in january 26?  It seems, you don't actually grasp the concept of the term »difference«. *2025 and 2026 is actually NOT the same!*",Negative
Intel,so the igpu is propably tsmc too?  do we know if thats because of poor performence or poor manufacturing capazity?,Negative
Intel,I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   I think they'll probably have ARL-R for HX,Neutral
Intel,"Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage. Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.",Neutral
Intel,"The LP-E cores were a failure in MTL/ARL's design.  They're very useful in LNL and that trend follows with PTL/NVL.  They're definitely not just ""placebo cores"" - LNL uses them more often then the P cores and they can be activated and used in all core workloads as well.",Neutral
Intel,"They were, just not as often as Intel would have liked.",Neutral
Intel,"For MTL and ARL, yes. For LNL and PTL though, I haven't test them out personally but seeing the battery results of LNL chips made me think that the LPE cores of those things seem to be legit.",Neutral
Intel,"Oh year, right, I think I missed (or forgot) that. Darkmont should be even better but I'm guessing not by that much.",Positive
Intel,Kinda making my point.,Neutral
Intel,"Bro xd  Node readiness and product readiness and product on shelves are 3 different date  Node must be ready before product, product must be ready before being on shelves.  Officially 18A was ready around Q2 25",Neutral
Intel,"> so the igpu is propably tsmc too?  18AP, iirc. Good *enough* (or at least, assumed to be when originally planned) not to be worth the extra cost of TSMC.  > do we know if thats because of poor performence or poor manufacturing capazity?  Poor performance. Intel can't afford to be constantly a node behind. It's just too big of a gap.",Negative
Intel,"> I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   But that's exactly it. They said 65W vs 80W was in relation to cooling challenges, but either is more than sufficient for max ST boost. Idk, maybe reading too much into too little information.    > I think they'll probably have ARL-R for HX   At one point it sounded like they wanted to pitch PTL as a partial replacement. Guess it's not good enough though.",Neutral
Intel,"> Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage.  Of course, I already knew that. Binned Desktop.  > Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.  Well, up until 2018/2019/2020 Intel actually didn't really uncapped any of them and those parts were hard-limited to only pull their max 45–65W TDP.  Only later on when they were trying to hold pace with AMD, Intel insanely increased the TDP to pull to insane numbers (90W, 135W, 165W) … Other than that and prior, you had to resort to modded BIOS.  So until even Apple burned their thick skin on some i9 (**Cough* i9-9980HK in the 16"" MacBook Pro 2019), you didn't had such insane TDPs to begin with anyway (except for BIOS-mods).",Neutral
Intel,"As said, I owned a MTL-machine back then, and back then you couldn't even associated these cores after all, since you couldn't pin anything on them – So yes, *back then*, these were placebo-cores and they were in fact pretty much 'on paper-cores for marketing-reason' and basically useless.  Seems Intel managed to improve their performance a lot and make them actually useful, which is good!",Negative
Intel,"Yes, MTL's LPE-cores were basically a dud, LNL was fairly workable and PTL will be hopefully potent.",Neutral
Intel,haha,Neutral
Intel,> Officially 18A was ready around Q2 25  Which is still at least 4 quarters *past* its due-date actually …,Neutral
Intel,"That's not strictly true, the (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  https://www.ultrabookreview.com/27215-asus-rog-g703gx-review/#a5  >That comes with a few drawbacks, though, and one of the most important is the noise development. Asus provides three different power modes for this laptop, which you can use to juggle between performance, thermals, and noise:  Silent – CPU limited at 45 W and 4.2x multiplier, GPU limited at 140 W, fans only ramp up to about 40-43 dB in games; Balanced – CPU limited at 90 W and 4.2x multiplier, GPU limited at 140 W; Turbo – CPU limited at 200 W and 4.7x multiplier, GPU limited at 200 W, fans ramp up to 55-56 dB in games.  It was always up to the laptop OEM to configure the power limit and there was no hard cap enforced by Intel.",Neutral
Intel,"The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, negating the entire point of them.  In addition, MTL/ARL had the LP-E cores on the SoC tile which made them functionally useless for full nT load.  LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks *can* stay entirely within the LP island, *and* they're on the same compute tile, so they're also used in full nT workloads.",Neutral
Intel,"> That's not strictly true …  Yes it actually is.   > The (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  Dude, you're basically confirming exactly what I wrote, by literally picking a **i9 9980HK**-equipped notebook of 2019, which is even exactly the very infamous SKU I was talking about …  I was talking exactly on (among else) that very CPU, even Apple couldn't tame and had to undervolt (still without being able to prevent the later sh!t-storm) until eventually abandoning Intel altogether.  ---- All I'm saying is, all the years prior with only quad-core, Intel \*never\* (nor any OEMs for that matter) went above and beyond the official TDP – The only lone exception from this, were those super-bulky Schenker Desktop-replacements.  That only suddenly changed by 2017–2019, when Intel suddenly increased core-count in mobile swiftly from Quad- to Hexa- and ultimately Octa-cores, while pushing the TDP in quick succession  into insane territory.  *That was the time, Asus took about a year to manage applying liquid-metal en masse, for Intel-notebooks!*  You are right with your sample here, yet you only confirm what I said before. Only past quad-cores it was.",Neutral
Intel,"> The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  Do you happen to know, if that (ring-bus related) was the same principle on MTL?  > The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, **negating the entire point of them**.  Yeah, talking about throwing some completely untested sh!t onto the market, irregardless of the fact, if the product makes sense or not — A true classic, I'd say. Just Intel being Intel.  Since dropping those LPE-cores on MTL, would've actually made the mask and thus needed die-space *smaller*, which in turn would've actually *increased* their yields \*and\* in return profit-margins already …  But muh, benchmark bars and ""AMD has moar corez!!"", I guess.  > LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks can stay entirely within the LP island, and they're on the same compute tile, so they're also used in full nT workloads.  Is there any greater penalty (latency or cache-flush-wise) for moving threads off the LP-island to P-cores?  *Thanks for the insides so far though!* I can't really see through anymore to all these constant arch-changes.",Neutral
Intel,"I recently picked up an MSI Claw 7 AI+, and have been pleasantly surprised with the current state of their drivers. I would absolutely be interested in a Celestial GPU if they have anything targeting 9070~ or higher performance level. Here's hoping!",Positive
Intel,I'm glad they are doing these followup videos. Gotta give Intel credit where due - their cards are getting so much better over time.,Positive
Intel,The Lunar Lake handhelds would be really popular if they were just a little cheaper.,Neutral
Intel,They said their high idle power is an architecture issue so they can't fix that,Negative
Intel,God forbid Intel supports Day 1 GPU drivers longer than 5 years,Neutral
Intel,"true but thats not really that big of a deal, and can be fixed with some tweaks [https://www.reddit.com/r/IntelArc/comments/161w1z0/managed\_to\_bring\_idle\_power\_draw\_down\_to\_1w\_on/](https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/)",Neutral
Intel,So just like AMD then.,Neutral
Intel,"They usually do tho? We dont know about their dGPUs yet because it hasn't been 5 years.  Their Xe Laptop gpus are 5years+ and are supported as of this month.  No idea why you're hating on intel GPUs mate, i'm used to everyone cheering the underdog not the otherway around.",Neutral
Intel,"I think that guy's a misinformed user that is saying that AMD 5000/6000 series won't get driver updates anymore lol, when AMD just said that those gens aren't slated for game optimizations that use their ""latest technologies"" (FSR4 at the time of writing) anymore. Those gens still will get driver updates but AMD gave up on trying to implement FSR4 on them, like the leaked scripts earlier this year had performance reduction from 20% to up to 30% FPS to use FSR4 for 6000 series cards since they don't have the hardware and just using brute force software for FSR4.   I'd agree that my 6700 XT never getting FSR4 sucks, but the misinfo circling around is that we'd never get anymore driver updates lol.",Negative
Intel,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? … but Xe3 plus will be enough to call it C series.",Neutral
Intel,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Positive
Intel,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,Neutral
Intel,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,Neutral
Intel,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",Negative
Intel,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",Negative
Intel,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",Neutral
Intel,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",Positive
Intel,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes Xe³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   Xe³ is a Xe³ based architecture like Xe³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. Xe³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",Neutral
Intel,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,Neutral
Intel,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",Neutral
Intel,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",Positive
Intel,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,Negative
Intel,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,Neutral
Intel,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,Neutral
Intel,That's not what Peterson was talking about context wise when he addressed this in the video.,Negative
Intel,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entry‑level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",Neutral
Intel,[AMD RDNA 3.5’s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),Neutral
Intel,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",Neutral
Intel,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",Neutral
Intel,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",Neutral
Intel,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",Neutral
Intel,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,Neutral
Intel,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",Neutral
Intel,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,Neutral
Intel,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",Neutral
Intel,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,Neutral
Intel,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",Neutral
Intel,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",Positive
Intel,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",Positive
Intel,MLID must have an aneurysm seeing the guy still employed at Intel,Neutral
Intel,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",Positive
Intel,Igpus not discrete gpus,Neutral
Intel,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",Negative
Intel,Hahaha I can hear Tom saying “fucking Tom Peterson has been *lying* to you” as you say that,Negative
Intel,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,Negative
Intel,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,Neutral
Intel,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,Neutral
Intel,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",Negative
Intel,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,Negative
Intel,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,Negative
Intel,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,Negative
Intel,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",Neutral
Intel,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",Neutral
Intel,You stole what I was going to say... take my upvote.,Neutral
Intel,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",Negative
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  One thing seems clear, this solution will be cheaper than standard AI data center products.  Hopefully, Intel can get and maintain footholds in medium-sized AI research market",Neutral
Intel,No way that power usage is at idle. Even if there was only one performance state like Tesla GPUs they wouldn't consume that much power.,Negative
Intel,that's insane vram density,Neutral
Intel,"This thing only has a 1Gb Ethernet port? I don't know much about the use case for this thing, but that seems surprisingly low. Simply from the use case of uploading training data to this I feel like something faster is necessary.",Negative
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Is there a fork of chrome that runs on gpus,Neutral
Intel,"Now do it with pro dual version of b770s, 64gb each. Could be a far more economical inference solution than what AMD is providing",Positive
Intel,but is that faster than a single 5090?,Positive
Intel,Is this enough VRAM for modern gaming?,Neutral
Intel,Nvidia: ill commit s------e,Neutral
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  If they rely on PCI-E for interconnect, bandwidth will be anemic between cards for training purposes. Might be ok for inferencing, but for training, you need the bandwidth.  That's not unclear at all.  This thing is probably not useful for anything, but serving many users with small models for inferencing.",Neutral
Intel,"I doubt we'll see any UALink this year, but perhaps  on the 160GB  Crescent Island card next year.  Intel hasn't announced anything, but it seems obvious.",Negative
Intel,"You think Intel is going to maintain these footholds any longer than they would need to?  This push of their dedicated GPUs into the professional space, is just a mere attempt to get rid of the unsold inventory of ARC-GPUs at the highest possible price-tag right from the beginning – Dumping those into the enterprise- and datacenter-space, in noble hope that someone clueless might bite upon the Intel-brand, and then dip into it at high costs *at non-existing support software-support* Intel pretends to deliver some time in the future.  Since as soon as the bulk of it was sold, Intel will cut the whole division and call it a day, leave anyone hanging dry.  That's what's planned with the whole professional-line of them – Buying this, is a lost cause as a business.",Negative
Intel,"I think they've just taken the 400W per card TDP and multiplied it by 16 to get the 6400W figure, so I think they're trying to say minimum power draw at full load, not at idle. ""Minimum power draw"" is definitely really odd phrasing for that number. Surely you'd think they'd say ""Total GPU TDP"" or something less confusing",Neutral
Intel,I don't think servers are supposed to stay idle for long.,Negative
Intel,"> the 768 GB model uses five 2,700 W units (totaling 10,800 W), while the 384 GB version includes four 2,400 W units (7,200 W)    And an insane power consumption.",Neutral
Intel,You basically always put in dedicated NICs in those kind of servers. The onboard ethernet is rarely used.,Neutral
Intel,Could that make it very cost effective for any particular use cases?,Neutral
Intel,"Even the fancy NVIDIA interconnects are a bottleneck for training performance, as are the memory bandwidth and even L2 bandwidth. One architects the algorithm around that. A tighter bottleneck is obviously not ideal, but it might still make sense depending on the price of the hardware.  As long as a copy of the full model and optimization state fits on the 24GB bank the PCI-E interconnect isn't really too limiting for training. Small models, like a 1B parameters voice model, will naturally fit. The larger the model you want to train, the more complex the architecture might be, but there are ways to train even models over 100B parameters on such hardware, like routing distributed fine grained experts. Not that I think it's a good idea at this point.",Neutral
Intel,It's VideoCardZ so they probably got information from a tweet or Reddit post and copy/pasted without doing any thinking whatsoever.,Neutral
Intel,"I assumed that would be the case, I just didn't see any room for expansion. It's pretty crowded with 16 full sized cards.",Neutral
Intel,"I don't really see any, unless you can find a workload that will fit the card and multiply that workload for 32 users, but as each chip is performance wise less than a 3090, it has to be a fairly light workload.",Neutral
Intel,"1B might work for a voice model, but useful LLM models ARE 100GB+. Smaller models are fun little toys.",Neutral
Intel,At least its a human hallucination and not AI hallucination.,Neutral
Intel,Can also be bad translation.,Negative
Intel,"according to the datasheet theres two free pcie 4.0 x16 slots on one version and two pcie 5.0 x8 slots on the other one. Both say ""support high speed NIC"" so I assume there is at least enough room for that",Neutral
Intel,"I wonder if these might make good controllers for ai powered robots. Something highly specialised, like a fruit sorter or something. I don’t know. I’m just hoping Intel can find a buyer because it’s good for the market to have more competition",Neutral
Intel,"It's a thing where the next generation of this card would be a viable competitor for the generative AI crowd, but not this one, where it can't compete with a 5 year old 3090.  For robotics, you just want inference. There are much better options focused on low power, small form factor NPUs custom made exactly for that, and they can be paired with small SBCs like a Raspberry Pi.",Neutral
Intel,No word on OMM so I guess that's reserved for Xe3-P. Wonder what other changes that architecture XE3-P will have.  The new Asynchronous RT Unit functionality in Xe3 is very cool and now Intel also has Dynamic VGPR. Neat.,Positive
Intel,"It's interesting they lump it in with B-series, given the architecture is pretty distinctly different from Xe2 used in Battlemage. Note that BMG/Celestial names have been explicitly used for dGPUs, however. They're not synonymous with Xe2/Xe3.",Neutral
Intel,"It would be nice if Intel released a small, budget friendly, passive cooled, PCIe powered dGPU with multiple QuickSync units, made specifically for AV1 encode and H.265 4:2:2 chroma with 10-bit depth.   It's unfortunate that one has to cough up $2,000+ (at least around here) for an RTX5090 to have three NVENC units at their disposal, and the 5090 still can't compete with a passive-cooled Apple Macbook that draws a couple of watts at peak.   You know things are crazy when Apple is the only company that caters to your specific needs!",Neutral
Intel,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Very bleak chance that Intel will continue with discrete gpu line up. Pretty shit decision tho,Negative
Intel,"So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In&nbsp;card under this precisely defined term.  Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – *In favour of Nvidia-GPUs?*",Neutral
Intel,Xe3P needs that feature parity with RDNA 5 and RTX 60 overall since its a 2027 product.,Neutral
Intel,"hopefully that means NVL-AX is not getting axed. at least, the 24 core die. heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch. super weird, not believing too much into it. curious about Druid, people seem optimistic, idk why lol.",Neutral
Intel,yeah super weird -- but that does bode well for the next dgpu being based on Xe3,Neutral
Intel,"Arc Pro B50 was pretty interesting to me, it was a modern 75W card that isn’t gimped.",Positive
Intel,With a board with lots of PCIe x16 slots you can stack A310's or A380s and get along fine. I do 3 transcodes of av1 great on a single one right now.,Positive
Intel,The reason they'll never make this is the same reason they don't put SR-IOV on consumer cards.,Negative
Intel,Can you do this work on a $600 Mac mini? Or do you need something more expensive.,Neutral
Intel,"Yeah prob, but by how much. Looking forward to the post CES reviews and Xe3 better be a major leap over Xe2.",Positive
Intel,They will if they have xe cores that gen. They'll only fully kill it if they abandon that core development entirely to exclusively use Nvidia graphics tiles,Neutral
Intel,">Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  What in the world are you talking about. This roadmap says nothing of the sort.",Negative
Intel,"No, they are saying the GPU in Panther Lake is Xe3, but instead of classifying it as a new-gen architecture, they are saying it's just an extension of Xe2, so they've put it under the B series product family.",Neutral
Intel,100% but I fear this is another botched gen so it will at best be on par with current gen offerings.,Negative
Intel,"Intel buying chips from Nvidia won't be as cost efficient as them developing their own chips. Intel can't abandon GPU development in the short term or it'll be left behind in the long term (Nvidia can easily sell their Intel shares and pivot away from Intel leaving Intel with nothing).    If Intel will stick to making dGPUs and expand to competing in gaming with said GPUs they benefit greatly from the experience and talent that dGPU development yields, this makes dGPU development a no brainer. If AMD can afford to make dGPUs Intel definitely can as well.",Neutral
Intel,"> heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch  Would be very interesting if so, though I'd be surprised if projects get resurrected in this spending climate. Competitiveness might also be tight in '27.",Neutral
Intel,Celestial was based on Xe3p.,Neutral
Intel,"Then that can't just be a minor tweak, but perhaps it's just market segmentation.",Neutral
Intel,I don't think Mac minis support hardware AV1 _encoding_. Apple only recently supported hardware AV1 _decoding_ so their devices aren't the best suited to AV1 media,Negative
Intel,> They will if they have xe cores that gen  Why do you believe so? They made iGPUs without dGPUs before.,Neutral
Intel,"They won't Nvidia tiles are for specific market segments, they aren't a one size fits all solution.",Neutral
Intel,That's not what my colleague's say,Neutral
Intel,It wouldn't be a Helpdesk_Guy comment if it didn't include insane made up bullshit to disparage Intel.,Negative
Intel,"Exactly. *Good Lord are y'all shortsighted!* Are you falling for their shenanigans and typical tricks again?  Crucial is, what Intel does NOT mention – That is anything future dedicated graphics, as in *Add-In&nbsp;cards*.  They're purposefully declare PTL as de-facto dedicated GPU, yet any thing of future **d**GPUs is ABSENT.",Negative
Intel,"Congrats, you looked at the pictures. We all did. Now look at it *again* …  Now explain to me, why ARC alchemist is separated (since it's a line of dedicated graphics-cards, duh?!), while Battlemage gets PTL's X^e Graphics iGPU-tiles put alongside, despite such tiles classically does NOT count as a graphics-card but rather a iGPU (no matter how large).  The actual answer is, that this on the slide was surely NOT done slide by accident, but it figuratively puts PTL's X^e Graphics tiles of PTL's iGPU *on par* with their dedicated Battlemage-cards, which (at least in Intel's eyes) shall from now on signal a ""continuation"" of  Battlemage itself. Thus, there won't be any further dedicated graphics-cards like Celestial as classical Add-In cards, which still is way overdue anyway …  Since if not so, then WHY are PTL's iGPU-tiles labeled as ""Intel ARC B-Series""?! *You see the virtue signalling?*    You're also aware, that Intel recently dropped the Intel Iris&nbsp;Graphics theme and now calls all iGPUs ""ARC""?",Positive
Intel,">Intel can't abandon GPU development in the short term  Well Intel now uses the same architecture across both their iGPU and dGPU, so even if their dGPU products get killed, iGPU demand(indirectly, through Core Ultra processor demand) can fund continued GPU architecture dev. Intel still has the majority market share in the iGPU market, so they have the demand to justify continued R&D into GPU dev for iGPU alone.",Neutral
Intel,Xe3p is a significant architectural advancement says Tom Petersen.,Neutral
Intel,"Care to elaborate where I was blatantly wrong and it didn't actually turned out mostly or even exactly as I predicted prior when popping the prospect of Xyz, often months to years in advance?  A single example would be sufficient here, mind you.",Negative
Intel,"They did not declare PTL as a de-facto dedicated GPU. By definition it's *not* a dedicated GPU and they never said it is.  They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs. You're trying to fill in missing info with a nonsense interpretation.   What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".",Negative
Intel,"So much buzzwords, yet it sounds like a stroke.  You need help.",Negative
Intel,"Alchemist had both desktop discrete and mobile discrete. Intel's roadmap isn't a 'graphics card only' chart, it's an IP roadmap. That's why Panther Lake's Xe3 iGPU shows up under Arc B-series, they are saying Xe3 is an extension of Xe2, where as Xe3P (Celestial) is the next Arc family.",Neutral
Intel,"> A single example would be sufficient here, mind you.  Sure, here you go:  > So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In card under this precisely defined term.  >Their non-show is basically code for: “We're canceling everything dedicated, and our eGPUs we consider now dGPUs.”  >So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – In favour of Nvidia-GPUs?",Neutral
Intel,"> They did not declare PTL as a de-facto dedicated GPU.  Except that's exactly what they did – look at the damn pictures my friend, Intel purposefully group it into the same category as Battlemage's dedicated GPUs and Add-In graphics-cards and thus, iGPU-tiles as on par as dGPU cards.  > By definition [e.g. Panther Lake's iGPU-tile] it's not a dedicated GPU and they never said it is.  \*Sigh I *know* this, well all do! Yet Intel wants to pretend, that PTL's on-die eGPU X^e Graphics tile is now basically declared, what *Intel* now considers a **d**GPU. The implication is strikingly obvious here.  > They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs.  Ex-act-ly! You really don't get it, don't you? They deliberately leave everything what *classically* would count as a dGPU open and without ANY path forward, meanwhile their X^e 3-tiles are now considered dGPUs by Intel.  > You're trying to fill in missing info with a nonsense interpretation.  No, I don't. I'm just one of those being able to decypher Intel's typically backhanded tricks and try to explain that they're virtue signaling the worst: ARC as a line of dedicated graphics-cards is dead and won't get a follow-up, instead, Intel now considers it replaced by iGPU X^e Graphics-tiles (like the one in Panther Lake).  > What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".  Their trickery here, that they basically announce the discontinuation of ARC graphics-cards through the back-door.",Negative
Intel,I wonder why they're using such naming. If Xe3 is just Xe2 enhanced then why not call it Xe2.5 or Xe2+. And if Xe3P is a new architecture why not call it the real Xe3?,Neutral
Intel,"Just wait and see. I sadly have a horrific on point streak to remain in the right with anything Intel, even if I call things months to years in advance.",Negative
Intel,"The implication here is incredibly obvious:  ""Intel Arc B-Series consistes of Battlemage discrete graphics *and* PTL iGPUs"".  That's the implication here. It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL",Neutral
Intel,Because it's most likely still the same foundation. Xe3 just have additional optimizations and HW blocks along the lines of OMM (confirmed) and whatever new tech Intel decides is worth pushing for higher end parts.    But perhaps it's more of a RDNA 3.5 situation. Some early Druid tech backported to XE3P.    Yeah I have zero clue just guessing xD,Neutral
Intel,"> It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL  Of course not, since it would instantly tank their reputation and will kill all further ARC-sales?!  Do you actually think, that Intel is that stupid to plain announce the death of ARC-series graphics-cards? Do you think they'd even sell a hundreds cards after such a announcement? Who would by a dead-end line of graphics-cards, which won't even get any further driver-support and still have disastrous drivers like Intel's ARC? NO-ONE!  Of course they virtue signal it through the backdoor here, to preserve the sell-off the of the rest of pile of shame of their B-series graphics-cards as long as possible (leaving users with none support afterwards), and *only then* pull the plug afterwards when they went through their inventory …  Since when are you following Intel? Intel has always done so, and only announced a sudden discontinuation once the inventory was sold (leaving customers in the open with no further support overnight), despite virtue signalling a continuation all the time (to preserve a proper sell-off).  ---- Wasn't this exactly the same story with **Optane** back then? Each and every question upon a future support was answered in the affirmative, only to pull the plug overnight again as usual …  I'm not another piracy of cons here, it's Intel pulling the same Optane-stunt with ARC – Happily selling off their cards, knowing darn well, that they will pull the plug, yet refuse to actually play with open cards again.",Negative
Intel,"> > It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL >  > Of course not  Great, we're all agreed that you're making shit up when you said  > Panther Lake now Intel classifies as a dGPU",Negative
Intel,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,Neutral
Intel,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",Positive
Intel,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,Neutral
Intel,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",Neutral
Intel,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",Positive
Intel,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,Neutral
Intel,we are witnessing the downfall of pc gaming in real time,Negative
Intel,Why should they? They are going to buy NVidia GPUs for everything now.,Neutral
Intel,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,This could be awesome more completion The better,Positive
Intel,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,Negative
Intel,"Multi-Post News Generation, with three articles interpolated per source.",Neutral
Intel,I'm excited for Intels new GPU,Neutral
Intel,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",Negative
Intel,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",Neutral
Intel,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,Neutral
Intel,Competition *,Neutral
Intel,Obligatory article quoting reddit post quoting another article quoting original reddit post.,Neutral
Intel,"Fake frames, fake articles! /s",Neutral
Intel,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,Neutral
Intel,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",Neutral
Intel,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",Neutral
Intel,"patents expire after 15 years, they will have to share it then.",Neutral
Intel,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,Neutral
Intel,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",Neutral
Intel,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,Negative
Intel,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,Negative
Intel,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",Neutral
Intel,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",Neutral
Intel,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,Neutral
Intel,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,Negative
Intel,"Okay then, what's the Xe GPU roadmap looking like then?",Neutral
Intel,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",Neutral
Intel,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,Negative
Intel,They barely lived on before the deal.,Neutral
Intel,"It'll live on our hearts, yes.",Neutral
Intel,Lol if you believe that I have a bridge to sell you in Brooklyn,Neutral
Intel,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",Positive
Intel,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",Neutral
Intel,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,Neutral
Intel,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,Negative
Intel,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",Neutral
Intel,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",Neutral
Intel,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",Neutral
Intel,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,Neutral
Intel,"Always selling out actually, they just can't produce that much",Neutral
Intel,And I have another if you think nVidia is capable of keeping this deal running for that long...,Neutral
Intel,Intel will hopefully split their fab business from the rest of the company either way.,Neutral
Intel,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",Neutral
Intel,> they just can't produce that much  because they are losing money on them,Negative
Intel,Trade bridges.,Neutral
Intel,Intel's arc is dead with or without nvidia deal.,Neutral
Intel,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",Negative
Intel,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,Negative
Intel,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",Negative
Intel,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",Negative
Intel,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",Neutral
Intel,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",Neutral
Intel,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",Negative
Intel,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,Neutral
Intel,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",Negative
Intel,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",Neutral
Intel,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",Neutral
Intel,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",Neutral
Intel,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",Negative
Intel,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,Neutral
Intel,Far more than I expected them to come out at. Damn.,Negative
Intel,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",Negative
Intel,I’m getting one when it releases in Australia,Neutral
Intel,Thats great and all but when will there be stock? (Canada),Positive
Intel,Pytorch libs are getting better for intel and amd support but there are still a lot of 3090 cards on the used market and those don’t need hoops to jump through to get compute working.  If I was handed a budget this low by a boss I’d just ask them to buy a lot of 3090 cards.,Neutral
Intel,I'm disappointed.  My order was canceled,Neutral
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",Negative
Intel,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,Neutral
Intel,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",Positive
Intel,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",Negative
Intel,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",Neutral
Intel,The 3090 does not have ECC on it's VRAM nor certified drivers,Neutral
Intel,Totally not worth it.,Negative
Intel,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,Positive
Intel,"I'm more looking forward to the B50, but obviously local pricing is everything.",Positive
Intel,"Yeah, the website now says: ""This GPU is only available as part of a whole system. Contact us for a system quote.""",Neutral
Intel,"It has SR-IOV, certified drivers and other professional features...",Neutral
Intel,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",Neutral
Intel,">The gaming cards require significantly less robust driver and application support  Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  It was like that since A770 release. When some games straight up don't work or have unplayable bugs, you are literally paying for own suffering.  That said, B50/B60 are good, though i wish B50 worked with ""Project Battlematrix"" that is Intels NVLink. B60 was also expected to be for 500$, not 600$, but it is what it is.",Negative
Intel,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 1 year!?,Neutral
Intel,"...Do integrated graphics support SR-IOV? Do any of Intel's iGPUs support the ""pro"" version of their drivers,   Like, obviously iGPUs have less compute power and less bandwidth than anything dedicated, but. Part of me feels like iGPUs should actually have the features of the ""pro"" cards, given they aren't replaceable, with the actual pro cards providing performance.",Neutral
Intel,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",Neutral
Intel,Do not underestimate the lack of CUDA.,Negative
Intel,"Bought my 3090 for MSRP 4.5 years ago and honestly would never have imagined that it would hold 50% of its value.  I want to get another one when they drop to $200 or so, so that I can finally be disappointed by SLI and make 14 year old me happy by finally clicking the bridge onto two GPUs and firing up a benchmark, but who knows if/when that day will come.",Neutral
Intel,A used 3090 is only $100 more.,Neutral
Intel,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",Negative
Intel,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,Neutral
Intel,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,Positive
Intel,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",Neutral
Intel,Atlas 300i duo,Neutral
Intel,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",Neutral
Intel,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",Neutral
Intel,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,Neutral
Intel,Yeah it was very scummy imo,Positive
Intel,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",Neutral
Intel,It was meant to be a joke. Not so funny I guess.,Neutral
Intel,"B580 needed to get out very early to beat RTX 5060 and RX 9060 to the market. Nvidia's and AMD's cards outperform it in gaming raster performance thanks to having been in the market for much longer. They have the more efficient architecture and software for gaming. It wouldn't be great for marketing if Arc came out at the same time with similar pricing, yet got beaten in the FPS benchmarks.  But productivity cards could wait. Arc's always been very competitive in productivity tasks. Maybe because Intel has more experience in this department. So reliable driver is the focus here.",Positive
Intel,">Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  Good driver and application support is not possible to deliver without a significant install base of your product in the wild. PC configurations vary wildly and usually driver issues are about some bad luck combination of hardware, driver versions, and application versions.  The incumbency advantage in this market is immense. Devs will validate that their stuff works on all manner of configurations including an Nvidia GPU, so they get the perception of having bulletproof drivers.",Negative
Intel,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,Neutral
Intel,Why does this matter?,Neutral
Intel,"Some iGPU do support SR-IOV, I think 12th generation and newer. Motherboard, driver, and OS dependent.",Neutral
Intel,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,Negative
Intel,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",Negative
Intel,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",Neutral
Intel,Used market is freaking insane. It is better to grab new or open box.,Negative
Intel,Over here used 3090s are sold for 500-600€.,Neutral
Intel,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",Neutral
Intel,Dude that feeling is so worth it though. I did that same with 2 Titan Xps a couple months ago and the build just looks awesome. Great performance on pre 2018 / SLI supported games as well.,Positive
Intel,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,Negative
Intel,the super gpus are not expected to release soon?,Neutral
Intel,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",Negative
Intel,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",Negative
Intel,Used 4070 Supers dont sell for nearly $700+ on the low dude.,Neutral
Intel,Yes running business of used cards is how its done.....,Neutral
Intel,no fee SR-IOV makes this a substantially different offering then AMD or Nvidia.,Neutral
Intel,Typically run at 250W though to be fair.,Neutral
Intel,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",Negative
Intel,not on the Vram like professional cards,Neutral
Intel,Q4.,Neutral
Intel,I thought it was funny ¯\_(ツ)_/¯,Positive
Intel,The b580 is still good at $250 and it’s only recently in the past month the 5060 and 9060xt 8gb dropped near or at $250 in some instances.  It’s a good card at 1440p aswell and it has 12gb of the other 2 8gb cards. Only problem I have it is the windows 10 drivers being abysmal dog shit if you’d want to play any current game with stuttering and crashes.,Positive
Intel,"Yeah, apparently a year’s worth of it",Neutral
Intel,Because they're super late to the party.,Neutral
Intel,"Wow, 800-900USD after 5 years is more than I would have expected.",Neutral
Intel,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",Negative
Intel,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,Negative
Intel,For what exactly do they need vram without cuda ?,Neutral
Intel,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",Negative
Intel,Gamers are not niche. They are 20 billion a year business.,Neutral
Intel,"False, they’re releasing in december or jan. So about 3 months from now",Neutral
Intel,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",Neutral
Intel,"The 3090 Ti did, but the standard 3090 did not.",Neutral
Intel,"It's a really good general purpose card. Perhaps not the best card if you want to squeeze out the most fps out of games, but it performs really well in a wide range of tasks including gaming. And yes 12GB VRAM really comes in handy.  It can easily be found for under $250. B580 is still cheaper than 5060 and 9060 XT and even RTX 5050 and RX 7600, especially in the UK where you can find a B570 for only 20 pounds more than RTX 3050. That's way cheaper than RX 6600, it's a no brainer. But the prices could only go down thanks to economy of scale mostly. It's been produced for a while, that's why.  Also one of the big reasons it's still selling now is thanks to good publicity. Imagine if it came out as the same time as RX 9060 XT. HWU would tear a new hole regardless of how much of an improvement it is over the Alchemist cards and the impressive productivity performance. Those guys literally don't care about anything other than pushing a dozen more frames in a dozen personally chosen games. Gamer Nexus and Igor too to some extent. It would be bad publicity.",Positive
Intel,"If intel felt they could have released this safely earlier, they would have",Neutral
Intel,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,Neutral
Intel,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",Neutral
Intel,3090s will be much less attractive when the 5080 super and 5070ti super launches.  If the 5070ti super is 750- 800 with 24gb for a new faster GPU with a warranty it hel used market will have to shift.  The 3090s have nvlink but I'm unsure how much that matters for most people.,Neutral
Intel,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,Neutral
Intel,"rendering, working on big BIM / CAD models, medical imaging,...",Neutral
Intel,CUDA isnt the only backend used by AI frameworks,Neutral
Intel,"I use opencl for doing gpgpu simulations, this card would be great for it",Positive
Intel,You mean your 0 profile history because you have it private?,Neutral
Intel,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",Neutral
Intel,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,Negative
Intel,Spoken like a true gamer.,Neutral
Intel,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,Negative
Intel,You still get about 85% performance compared to stock settings.,Neutral
Intel,"Of course, but that’s the point. It took them an excessive amount of time to “feel” like they could have released this",Negative
Intel,What are they doing that's making them money? Or are theu selling the compute somehow?,Neutral
Intel,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",Negative
Intel,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",Neutral
Intel,">rendering  Rendering what? Because if you're talking from a 3D art perspective, all our software is leaps and bounds better with CUDA or outright requires it. You can't even use Intel GPUs with Arnold or Redshift, so good luck selling your cards to a medium scale VFX house that heavily use those render engines",Neutral
Intel,>CUDA isnt the only backend used by AI frameworks    It is not the only backend for all AI frameworks; it is the only backend for the majority of AI frameworks.,Neutral
Intel,It is the only functional one.,Neutral
Intel,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",Neutral
Intel,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",Positive
Intel,0 people are using autodesk with an intel arc gpu lmao,Negative
Intel,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",Negative
Intel,"Oh no, im part of 60% of world populatioin. how terrible.",Negative
Intel,Which is 36% higher performance per watt.       ... to be fair.,Neutral
Intel,The reason they didn't is because it would have been worse to release it earlier,Negative
Intel,Software Development,Neutral
Intel,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,Negative
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"Depends on what software you use, but your right that NVIDIA with Cuda is safe bet when is comes to rendering. I use Enscape and that works great in my AMD gpu",Positive
Intel,Which ones?,Neutral
Intel,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",Positive
Intel,Private profile = Complete troll.  You're not an exception to this rule.,Negative
Intel,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,Negative
Intel,"Right man, my point is that it shouldn’t have been",Negative
Intel,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,Neutral
Intel,"Hell, one of them was just the cooler without the actual GPU.",Negative
Intel,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",Neutral
Intel,for example every local image and video generation system I've seen.,Neutral
Intel,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",Negative
Intel,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",Neutral
Intel,According to whom? With what evidence?   Intel spends more on r&d per year than nvidia and amd combined   Do you think they are just being lazy?,Neutral
Intel,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",Neutral
Intel,Getting good LLMs crammed onto GPUs for coding is a popular topic on /r/locallama.,Positive
Intel,And the one directly under that was the actual PCB...,Neutral
Intel,Im running Comfy UI + Flux on my Strix Halo right now without issue 🤷‍♀️,Neutral
Intel,Propaganda is 90% of chinas economic output though.,Neutral
Intel,"Having same issues, having 40-60% decrease in fps across games. Along with stutters, and fps drops. Hard to run even easy to run games like roblox and LoL. Insane that a windows update caused this somehow.",Negative
Intel,"I think I have an restoration point, but damn it sucks",Negative
Intel,It's working perfectly fine after I rolled back the latest security update.,Positive
Intel,"Make sure you have resizable bar turned on inside your bios and hopefully you got a pretty solid CPU. Other than that, this is one of the better gpus for 1080p gaming.",Positive
Intel,"My current cpu is 5 5600g but I'll upgrade it next year probably, for now it's not that bad ofa bottleneck I hope. I will turn resizeable bar as soon as my new mobo comes, I got the Asus Rog Strix B550-A gaming. Also my monitor is 1440p but from what I've seen, this gpu is perfect for that resolution.",Positive
Intel,no need to be very solid. an entry level cpu suffices  i'm using 225f and most of the time during gaming the gpu is 100% and the cpu is 33%,Neutral
Intel,"It depends on what you're playing and what's currently bottlenecking. When in game make sure there's no FPS cap and either use the MSI Afterburner or Steam FPS counter or even ctrl + alt + del to to bring up task manager and figure out what's being used at 100%. If your GPU is being used at 100% then upgrade that. If it's not then something else is bottlenecking. Do note that CPU bottlenecks often don't appear as typical ""running at 100%"" bottlenecks because of how they work. If the game only uses 1 core and your 1 core is running at 100% but every other core is running at 0% then it'll appear as 25% CPU usage even though it is the bottleneck. You can switch over to logical processors on task manager's performance tab. If I were you I'd personally check to see if I have a CPU bottleneck and if so move to a LGA1700 13400f or a 12600kf that supports DDR4. The used market is always cheaper than buying new. The RTX 2080 is pretty strong and should be able to play most titles at 1080p. Your SSD storage is pretty small, it might be worth to buy an M.2 before SSD prices rise even further.",Neutral
Intel,How much is your budget?  And do you care more about graphics quality or higher FPS? So we can give you a build that fits what you want.,Neutral
Intel,"Ive updated the post a little bit, since i realized it was insanely low effort.  I feel like when i have taskmanager open, its mostly CPU, then followed by GPU.   Tho its not like my RAM is having an easy life either",Negative
Intel,"Ive updated the post a little bit, since i realized it was insanely low effort lol  Uhm, if i had to chose, prolly care more about FPS, and the graphics is a nice bonus when available.",Positive
Intel,All the ram that will be produced in 2026 has been sold so ram prices are going through the roof. It’s impossible to upgrade ram at this point. SSDs are also following suit so if I were in your position even though the CPU is the obvious bottleneck I’d look to upgrade to a 2tb NVME first due to increasing prices. When you upgrade your cpu make sure it’s on a mobo that supports DDR4 and reuse your ram.,Negative
Intel,"Make sure you get a good CPU. A 4060 (or even a 5060) will be more than enough. 16GB of RAM is fine, and a Intel Core i5-14600 would be a great pick ideally with around 10 cores.",Positive
Intel,RTX 2080 has the same power as a 4060 he’s fine there. The problem is his 4 core cpu.,Neutral
Intel,"Oh man, I know how you feel. Sucks to live in a country which gives you the ""3rd world tax"".   In mu country, a 5080 will be ~$2,200. I bought it in the U.S. Amazon store, shipped it into this 3rd party company's warehouse which will ship it to me. Takes 3 weeks in total. But, at least, I paid MSRP for the card + shipping and tarrifs that totaled $1,200.",Negative
Intel,"Late but I chose a B580 and have absolutely 0 regrets. Very stable performance, have been playing some lighter new games at 4k 60fps, and I know I'm going to be futureproofed vs the other cards in its price class due to the 12GB of VRAM. Yes the 9060XT 8gb is technically a stronger performer, I will trade a few frames today for longevity over time, which should also translate into much stronger resale value on the B580 in the future.  But it seems you are finding the 9060Xt for actually significantly cheaper, like $70. That difference might make it worth it to go with an 8gb card. It's not going to be unplayable of course, but I think you may have to upgrade a bit sooner than with the ARC",Positive
Intel,"I feel the B580 to in quite a sweet spot at the moment and I kind of don't see a reason not to go for it. It may not be the best card out there (neither is the 9060 8GB), but in 2025 I would 100% go for the 12GB card over a tad more performance as that will last longer when games start to get more demanding VRAM wise. Sad to see the prices you have to pay for one at the moment, but I also see them rising here and hovering around $325-385 while most of them were under $300 last week (I'm not in the US btw).",Positive
Intel,I would take Rx 9060 Xt even 8 GB over intel arc 580.,Neutral
Intel,"Hey man, fellow Brazilian here. I know things are tough down here. I have not tried it yet, but everywhere I see right now, Intel is the king of cost x benefit. You can also help yourself by selling your card for about 600-700 reais, making your new card ""cost half"" if you go with the arc.  In the u future this will also help when you want to upgrade so your new card ""costs less"". I would go for the 12gb card because with the current landscape, games are demanding more vram and because of AI prices will shoot up.  If you need any help, dm me and I can try to help you out",Neutral
Intel,"Yeah, can't even do that here... It's possible that you would have to pay 92% more when it gets to the country or something...",Negative
Intel,"Yeah, it's possible that I'll stay with the GPU for some time... So, the 12 GB having more longevity, even if the 9060 xt has more raw power... Maybe the safe thing is to get the B580.",Positive
Intel,"Right now sure... But what about longevity? I said that it's possible that I'll have this GPU for years, so... Won't 12 GB age better than raw power with 8 GB? Especially at 1440p.",Neutral
Intel,"Oh, I actually want my card. I have a home server and an NVDIA GPU would be nice for some stuff like transcoding. Also, I may be building a PC for my brother soon, so maybe the 1650 Super will go well in his PC for now. And if I buy another GPU in the future, I can pass the one I'll have for him too.",Positive
Intel,"Maybe I am wrong and the 9060 turns out to be the card that lives the longest, but if I had to make the call in that budget range personally, it would be the B580. Like you I don't really see a reason to go 8GB anymore. If Intel pulls out of the GPU market tomorrow, then I was wrong. If 8GB is not enough anymore by some release in June 2026, I was right. If you somehow could push the budget to the 9060 16GB, then that is a no-brainer as well. Difficult, but I do feel the B580 is very compelling.",Neutral
Intel,"I think raw power will come in more handy right now, and 8 GB is not the end of the life. Or spend a bit more on 16 gb.   You will need to make sacrifices when raw power runs out too.",Neutral
Intel,"Oh... It's not ""a bit more"" it's 1 THOUSAND reais more... At least...",Neutral
Intel,It is hard for people not here to understand that a difference of 180 USD is almost one month minimum wage for us,Negative
Intel,"I would take 8 GB 9060 Xt. Not a great choice for new AAA titles, but will rock everywhere else",Positive
Intel,I have a hard time affording upgrades too. This is why I would get the cheaper GPU which is also faster in most cases.   Is it possible to bring GPU when you visit Brasil?,Negative
Intel,"Makes sense. Thanks for the input, but ""not a great choice for new AAA"" titles makes me scratch my head, because I'm seeing the card being able to run new demading games on Ultra with more than 50 FPS and some games like Doom Dark Ages with more than 100 FPS max settings... You may have higher standarts, but that's absurd triple A performance for me...",Negative
Intel,"I am gaming at 4k if possible on RTx 5070   My last card was Rx 6650 Xt with 8 GB, and Rx 9060 Xt is much better. I think it is a good card, unless you run into rare cases where ram is not enough. So many great games where 8 GB will be plenty",Positive
Intel,"Makes sense, thanks for the input!  I agree, a lot of games that 8 GB is enough, but like, how the market is going, it seems like 8 GB is not being enough even today.  And I'm scared of having the card in 2 years or even more and being extremely limited by 8 GB.",Neutral
Intel,"This would allow you to reuse your current RAM.  [PCPartPicker Part List](https://pcpartpicker.com/list/zCY6t7)  Type|Item|Price :----|:----|:---- **CPU** | [Intel Core i5-14600KF 3.5 GHz 14-Core Processor](https://pcpartpicker.com/product/zyyH99/intel-core-i5-14600kf-35-ghz-14-core-processor-bx8071514600kf) | $209.99 @ B&H  **Motherboard** | [Gigabyte B760M GAMING PLUS WIFI DDR4 Micro ATX LGA1700 Motherboard](https://pcpartpicker.com/product/NMWJ7P/gigabyte-b760m-gaming-plus-wifi-ddr4-micro-atx-lga1700-motherboard-b760m-g-p-wifi-ddr4) | $137.99 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$347.98**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-12-15 14:54 EST-0500 |",Neutral
Intel,"Probably looking at motherboard, ram, and CPU upgrade since they all have to be done at once. I’d go AMD, 7000 series or higher (am5 motherboard platform)  Then, I’d upgrade the power supply to 650 minimum, to prepare for the last thing  Which is the gpu. Yours is still beyond ok, so this is last.",Neutral
Intel,Because of memory prices if you can find a cheap 14th generation CPU and B760 DDR4 motherboard reuse your other parts. If money is no issue probably others have mentioned something AM5 for future upgradability.,Neutral
Intel,"First try updating your drivers, could just be a simple fix.  Could it also be possible that the psu is overloaded? Hopefully someone with a little more knowledge on here can help",Neutral
Intel,I personally don't think it's worth it,Negative
Intel,"your CPU is still fine for those games, at 1440p you will quickly become GPU limited anyway.",Neutral
Intel,I think the better deals are the Microcenter ones with 32GB. Otherwise I wouldn’t do it to downgrade to 16,Positive
Intel,What is your gpu?,Neutral
Intel,"No, not in my opinion.",Neutral
Intel,"It might be worth considering if you see GPU performance loss/low GPU utilization in games when using a performance overlay.   When I had a 5600 I was using a 6800xt (similar performance to 3080/4070 excluding ray tracing) and it was a good pairing at 1440p ultrawide, but I spoke with a few people using a similar setup at regular 1440p who mentioned they do see the CPU limiting GPU performance in some games, so at 1440p I'd anticipate you would see performance loss depending on the game/settings at 1440p with a 5070. At 4k you are a lot less likely to be CPU limited though.   Would that be 2 sticks of 8gb DDR5 or one 16gb stick? If one stick, I'd also check how much performance you might lose also.",Neutral
Intel,"One will let you play every future title that's CPU-bound on a 5600x and increase performance on titles that 5600x's don't do great on (there's not many titles, but some definitely are <60FPS-85FS).  The other will let you play every title that's RAM-bound or RAM-effected with 16GB. There's not many currently out there, but some newer releases like Doom The Dark Ages recommend 32GB and have 16GB as a performance affecting minimum recommendation.  Remember that you can also sell your platform locally. 2x16GB DDR4 RAM alone is ~$100 or more, your CPU is probably $30-60 depending on model, and B550/X570 AM4 motherboard are probably $70-80+. I had luck selling my 5600 AM4 platform/RAM  locally for over $200, but DDR4 & motherboards have probably increased 25% or more since then.   No one has a crystal ball as to whether a 5600x or 16GB of RAM will be more hindering for new releases 2-3 years from now, but I'd rather be sitting on AM5 used parts in 2-3 years than AM4 used parts.   Newegg has a 2x8 B850M-E/2x8GB bundle for $170 currently. Microcenter has a full ATX B650E/7600x/1x16GB bundle for $320. (They also have 7600x for $180 to complete the Newegg bundle).   2x8GB will give you better performance, but that bundle will cost you more. 1x16GB will be more future relevant when paired with an identical stick of RAM, but will have worse performance. Looks like an identical 1x16gbs are Ebaying for about $130 (possibly due to this deal), which is actually cheaper than used 2x8GB and might be the cheapest CL36 6000mhz RAM that I'm currently aware of (if you don't count the 7800X3D Microcenter bundle).",Neutral
Intel,No. I wouldn't do it.,Negative
Intel,"Depends on if you're wanting more frames AND you're CPU-bottlenecked. Easiest way to find out is to enable steam overlay performance monitor (newish feature), launch your game, and see if your GPU is running below high usage (<90%).  Make sure your frames aren't capped and you have the settings you want enabled.",Neutral
Intel,"I suggest if you do upgrade, upgrade to a x3d chip with 32gb ram. I've seen 32gb go for 180 or less on hardwareswap here on reddit.",Neutral
Intel,"Nah. I'd probably try to nab a used 5800x3d for a couple hundred and max out your AM4.  Or, just wait for a better AM5 deal.",Neutral
Intel,"Good point, I think I needed someone to tell me it’s not worth downgrading ram capacity to get on ddr5",Neutral
Intel,"Sorry, I should have included that and edited that in now. I recently got a 5070",Neutral
Intel,I'm starting to gain a better understanding of how to balance CPU and GPU performance. I had been using afterburner but it was not giving me accurate 1% lows. I set up capframex and I'm having better success with that. My plan has been to evaluate my current setup for a while. Just got a bit of FOMO seeing some mobo/ddr5 deals. I'm gonna try to stay the course though  [FWIW it was a 2x8 deal.](https://www.reddit.com/r/buildapcsales/comments/1pt2ycv/bundle_asus_tuf_gaming_b850me_wifi_team_group/) Expired now but I've seen it come back a few times.,Positive
Intel,"If the timeline is its better to be on am5 in 2-3 years, then change in 2-3 years, not in a moment in time where ramprices are going to the moon",Neutral
Intel,"So you're saying if I'm going to go AM5 at some point, might as well save up for an X3D?",Neutral
Intel,"to be honest the 5800x3d I've seen go for close to $400 in some instances, he mind as well just get a 7600x3d or 7800x3d and buy some used ram on facebook marketplace or r/hardwareswap for cheaper than retail rate. literally can be up to 50% off retail on ram.  and all of this will be the same price as a 5800x3d, well not if he went with a 7800x3d in retail that'll be a hundred more or so",Neutral
Intel,Nope just went from 5600x to 7600x3d not a big enough upgrade for the cost. Enjoy the 5070 you made the right decision with it.,Neutral
Intel,Yeah it’s not worth downgrading right now.,Negative
Intel,Definitely not worth it to swap. Stick with your 5600x. Also 16gb DDR5 is probably not great: [https://www.reddit.com/r/pcmasterrace/comments/zvy63q/](https://www.reddit.com/r/pcmasterrace/comments/zvy63q/),Negative
Intel,yeah definitely would waant to get a x3d ship if you're already thinking about going for the 7600x they do have a 7600x3d chip for not too much more. I saw it going for 200 at microcenter. bundle came out to 300 I believe. it's more powerful than the normal 7600x and does better in games.,Positive
Intel,"That's good feedback, thanks. Cheers",Positive
Intel,"Oh interesting, I haven’t seen that before about the 8gb sticks. Man DDR5 has some quirks, can’t use 4 sticks and this. Anyways thanks for helping me feel better about staying with my 5600x",Positive
Intel,"With FG yes you'll most likely be able to get 140+ FPS.   I'd just go with 2x16, but it depends on pricing.",Positive
Intel,I play it with my 4070 It get around 90-150 fps with all cinematic setting with dlss quality and framegen You can go with high or epic setting and use native instead,Neutral
Intel,I have a 7800X3D 9070xt and I believe I get a consistent 140 or so at almost max settings. I haven’t tried the new max settings they released though.,Neutral
Intel,"What about 1440p on low settings? Can you reach 140+ FPS at 1440p on normal or low, with or without DLSS/FG?",Neutral
Intel,yes almost 170+fps all the time,Neutral
Intel,This is what you need https://youtu.be/xlbNsP5ySmA?si=tJ9AJX4DAVqzy-GK,Neutral
Intel,B580 12GB ram,Neutral
Intel,"Since you have an i5-12400F, you must have a dedicated GPU. Resell that GPU to see if it gets you anywhere near the RX 9060 XT 16GB.",Neutral
Intel,I have a 3080 and considered upgrading to a 5080 but I didn’t. What convinced me not to was watching benchmarks and seeing the 5080 isn’t even that much better.,Negative
Intel,"Hmm interesting, thanks for the feedback. I think I am probably more limited by my CPU at this point. The 3080 is a pretty solid card. I just hope it still is in a couple years.",Positive
Intel,If you want something to upgrade you could get a really nice OLED,Positive
Intel,"I honestly don't know much about the benefits of OLED. I have a good IPS gaming monitor, and a color-accurate photography monitor. Would I see any improvement in gaming smoothness with OLED over IPS? Or is it just about the dynamic range?",Negative
Intel,Yes it is,Neutral
Intel,"Motherboard should not have any issues with any graphics care, those are always plug and play and completely interchangeable for modern hardware. Between the cards it really depends on the price, you should always include that along with the primary games you want to play.",Neutral
Intel,"If they are the same price: 5060 Ti 16GB.  If the 5060 Ti 16GB is more than about 10% more expensive: 9060 XT 16GB.  Both are superior to the B580, in performance and drivers.",Neutral
Intel,5060ti 16 Gb if you care about Nvidia features or if it not that much more expensive than 9060XT.  If conditions above are false - 9060XT 16 Gb.,Neutral
Intel,"The 5060Ti 16 GB is the best, the 9060XT is usually the best value (right now it's about 60 Euro cheaper, and therefore the better choice). Only go for the B580 if your budget doesn't get to those other two.  No worries about the motherboard, any PCIe card will work with any motherboard with PCIe slots (meaning all of them in the last 20 years), the only possible issue is if your PSU isn't powerful enough or doesn't have the power connectors for the GPU (yours is well over the requirements and should have no trouble whatsoever).",Positive
Intel,9060XT 16GB,Neutral
Intel,This.,Neutral
Intel,"Hi there! Thanks for the comment.  We ask that posts and comments be in English so they can be understood by as many people as possible. Translations on Reddit are client-side, and not all apps or browsers support auto-translate. Currently many users (and moderators) aren’t able to read your {{kind}.  Could you please submit a new comment in English?  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",Neutral
Intel,"For 200 more to get more than double the performance!? Any time! To your question about the resolution, 1080p to 1440p is about 40% performance hit, so you won't go from 100 fps to 240fps just by lowering the resolution! Some games are CPU or GPS demanding so it really comes down what you play. Competitive games like marvel rivals overwatch and CS2 are CPU and RAM + ssd heavy so your gpu will maybe influence 30% of performance. A 9800x3d will hit 500 fps with 9060xt in CS2 while the same setup with a 5700x will hit maybe 200 fps regardless of the resolution. But in games like God Of War or Spiderman you will notice a big difference with a better GPU!   If you want a middle ground, get a 9070 non XT and you are set for the next 4-6 years in any resolution with great fps/$ value.",Positive
Intel,"The 9070 is great, but it isn’t worth the price jump in my mind. If you upgrade your CPU it would be more justifiable.   I have a 9060 coming and two 1440 monitors. Should do just fine. However, I’m not playing FPS multiplayer.",Neutral
Intel,"I personally 9060xt and it is amazing for what I do. It sounds like you aren’t playing intense games so it definitely works for you. The 9070 is definitely worth the price, but I would get a new cpu to back it up. Basically, you don’t need to upgrade but it’s not a bad idea to do so.",Positive
Intel,For 2k monitors I would definitely recommend the upgrade.,Positive
Intel,"Since you live by microcenter, keep refreshing their open box GPUs and you might see a 9070 XT show up for sub $550.",Neutral
Intel,2k monitor get 9070xt if u can i would just go for 5070 ti but if u really wanna 9070xt will last way long er,Neutral
Intel,In 4k gaming the setup will become obsolete sooner as graphics improve.   And you will get lower fps right from the start.   1440p seems to be the sweet spot.,Neutral
Intel,If you go 1440p you’ll have to update your gpu less and in the age of AI that may be a good thing as prices are absurd for things like memory etc,Neutral
Intel,"If you're not aiming for 150+ FPS, then an X3D is a waste of money and you could look at better GPUs instead.   The 7800X3D/9070XT combo is very well suited to very high FPS 1080P/1440P.",Negative
Intel,4k if you dont play competitive games and 40-60fps is fine  1440p if you do,Neutral
Intel,"4K is generally fine on it, especially with FSR 4, but 1440p is this card's sweet spot",Positive
Intel,"I have a Ryzen 9 5900X CPU & an RX7800XT - I run a triple 1440p Eyefinity setup.  So I often run:   \- 2560 x 1440p (16:9) on a single monitor   \- 7860 x 1440p (48:9) on triple-wide  My setup can handle:    \- 1440p REALLY well on 90%+ of the games I play.  Mostly 90fps+    \- It can run a ton of eyefinity ultrawide with at least 60fps      The Eyefinity setup would be  11,318,400 pixels   A single-monitor 4K would be 8,294,400 pixels  The system you are describing would handle any of the above...but I prefer 1440p monitors because they are cheaper and I really don't personally feel I benefit from the increased resolution on a single monitor.  I have 3 39"" 1440p monitors...got 'em for $299 each.  So much more bang for my buck than getting a single 4K.     Also: if you are interested in getting a triple-monitor setup, ultrawide, or 4K - ""Lossless Widescreen"" is absolute magic for games that are below 60fps.",Neutral
Intel,"i have the same build.  i think you should buy a 1440p monitor.   the 9070xt is an ""entry 4k gpu"", i watched tests and it was about 60fps on average in 4k for games that require more hardware.   i put it together about a week ago. this is my first build (i got help).   you have to change a lot of things in the bios to make it run well + the amd adrenaline.   to give you an example, with 1440p basic settings (in game) average about 280fps in warthunder, i haven't had much chance to play anything else.   and a tip, if you want to install windows 11 on it, the december and november gpu drivers won't work well, but they will on windows 10",Neutral
Intel,Unpopular opinion but I think anything above 120fps doesn’t matter that much. Which this card is capable of achieving max settings with upscaling and frame generation.  Frame generation 4K > 1440p native at same fps.,Neutral
Intel,Ah yes. While typing my post I also tought of that. While the 9070xt now gets decent frames with 4k currently. In a few years it might not.,Neutral
Intel,Yes. As of these comments im most likely to go now with the 1440p monitor. Better furure proofing as well as a smaller cost. 4k sounds so nice but I need to think rationally.  Thank you,Positive
Intel,I see. What kind of gpu then would be good? Doesnt the 9070xt beat the 5070 ti which is more expensive?,Neutral
Intel,Thanks. But im swayed rn to 1440p due to thoughts about future proofing. In a few years those 60-40fps might turn into 50-30. That would not be good.,Negative
Intel,"Yes thats what ive heard too, 4k sounds so tempting tho. But it seems 1440p is the best option. Thank you!",Positive
Intel,"Interesting perspective, thanks! So what you are saying is that I wouldnt have much trouble running in 4k, but 1440p offers much better value currently?   Personally I dont really like the wide or curved monitors and was thinking of a single 27” monitor. I do have a monitor alredy and probably will just allocate it to be my second monitor for discord or whatever on the side.",Positive
Intel,"Hmm, interesting. The [hardwear unboxed video](https://youtu.be/aWfMibZ8t00?si=wK-wVoiKNX7G35ms), which I think we both saw, showed the 9070xt performing at 59 to 74 fps at 4k on avarage. But I think with upscaling it could do 70+ fps in many games.   Could you also elaborate about tinkering with bios thing? Did you have to do it? Why was it neccessary? What was the issue and how much did it effect?  I will run windows 11. Are the drivers just made based on windows 10 and are not yet updated for 11 or what?  Thanks!",Neutral
Intel,"yeah im more than happy with anything over 100 fps. But still if its rn right just edging the 100-60 fps mark on the latest AAA titles, I dont think I’ll be happy much longer with the performance as newer and more demanding games come out",Negative
Intel,"It really depends on the quality and FPS you want to target.   X3D is excellent if you're aiming for 200+ FPS. Here you'd typically use lower settings and resolutions. For example a 1440P 240+Hz monitor.  The 5070 Ti averages slightly better than a 9070XT in raw rasterization power: https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html  But a 5070 Ti is significantly stronger if you're going for high quality with RTX. It's also has an advantage with DLSS for higher FPS with 4K.",Positive
Intel,"Nope, the 5070 Ti is better, but the 9070 XT is a better bang for your buck.",Positive
Intel,yeah  thats perfectly fine,Positive
Intel,"*""So what you are saying is that I wouldnt have much trouble running in 4k, but 1440p offers much better value currently?""*  Yes.      I really wasn't a fan of the curved monitors either...thought they were a gimmick...until I decided to try one.  When you have three of them, you get a good (but still subtle) ""wrap-around"" effect that is really immersive on eyefinity.  I'm with ya on the ultrawide monitors though; simply because 1/2 of what I do is better on single-monitor and usually you'd need some 3rd party software to setup smart frames; and they are way less bang-for-your-buck.  BUT: If you are only planning on getting a single 27 monitor, then you will be good with either 1440p or 4K.",Neutral
Intel,"I can't give a complete answer to the BIOS thing because I'm not an expert. We worked a lot with RAM, we couldn't even enter the BIOS afterwards, so we had to reset the BIOS (but maybe it was just because of my MSI B650 Tomahawk WiFi and Crucial Pro OC Gaming 32GB kit, maybe you won't have this problem), but I just wanted to say something that if you want the best performance. (delay, expo, unlock factory restriction and others)  you can install the driver on Windows 11, but my problem was that when I tried to open something, it wouldn't open anything except folders, which means I couldn't even turn it off, only with the power button on the computer case.  If you don't have a video card, it will be difficult to do anything with it, or you will have to delete the driver somehow. It worked without a driver  Just out of curiosity, how much did you buy RAM for if you already bought it?",Neutral
Intel,"I see, thanks!",Positive
Intel,"Well, then I might consider trying out a curved one. But having three seems quite exessive for me atleast.  Also a bit unrelated but I will then have two monitors, the new and the old one, and im thinking, should I connect my old second monitor to motherboard and use its integrated graphics to minimize the strain on my gpu? Or is it just not that big of a deal?  Thanks!  edit: fixed a typo",Neutral
Intel,"Did this BIOS hassle result from an upgrade to your previous system, or was it a completely new build with all new parts?  For me, I have a very old pc (1060ti and comparable other parts) and Im going for all new parts for this build. I WAS going to buy a bundle deal with ram, cpu and mobo for 749€ but typing this comment out, I went to check the details of it and the deal is gone. Im guessing due to unstable market, very unfortunate and this looks like ill probably have to spend some extra cash.   When I pull the trigger finally on this thing I will probably post about it on some sub.  But for now I dont even know what I will be paying for all of this. It will hover around 2000€ for all parts.",Neutral
Intel,"You'd definitely want to run them both from the GPU. If you are playing on one, and something like watching youTube on the other, then the additional pull on your GPU to watch YouTube is minimal.",Neutral
Intel,"I had to put together a completely new build, I didn't have a computer before, I only had a laptop and a PS4",Neutral
Intel,Alright. Thanks!,Positive
Intel,I see. Thanks!,Positive
Intel,"What do you want to do with your GPU? Are you just overclocking/undervolting for the fun of it or do you pursue a specific performance goal? Without having a goal in mind, it's hard to make a recommendation.",Neutral
Intel,You won't sell a 7900 XTX used for 800 euros,Neutral
Intel,IMO if you want switch to nvidia get 5070 Ti for around 750€ and keep the change or get better cpu/psu/cooling with spare money.,Neutral
Intel,"If u want to switch to nvidia do it, but maybe get a 5070ti then do a safe overclock. Its cheaper.",Neutral
Intel,I think you need buy 5070 TI It will be best solution in your case,Positive
Intel,100%,Neutral
Intel,"I am just looking for good performance in games and efficiency, also I feel like my 7900 XTX is using so much power, i tried lowering power limits or slight UV but still get crashes… My Monitor is LG Ultragear 1440p 240hz, and when I play single player games with gorgeous graphics I don’t want to turn on FSR because it’s horrendous, which means im running native and getting less FPS on High/Ultra settings",Negative
Intel,"Well I feel like it should cost roughly 700e, I bought it about 8 months ago. But cheapest one selling used where I live goes for 800-850e…",Neutral
Intel,"I have ryzen 7 9800x3d, bequiet 1000W psu, deepcool aio, bought everything together. Now I feel like only thing I could change is GPU",Neutral
Intel,"I mean, the 5080 isn’t exactly any better on the power front.  355W nominal for the 7900 XTX versus 360W nominal for the 5080 - they’re both juiced to the gills.    The 5080 is about 15% faster so you would basically be paying about 400 EUR to swap feature sets to NVidia and a reroll on the silicon lottery.  It sounds like your 7900 XTX basically exactly meets AMD spec given the lack of headroom on your sample, which makes sense for the 7900 XTX given how hard it’s being pushed for those last dregs of performance.",Negative
Intel,"Try to filter by ""sold items"" on ebay. That tells you for how much they're actually being sold, compared to the prices sellers wish for.",Neutral
Intel,Where do you live?,Neutral
Intel,"I have XFX Merc 7900 XTX, performance is fine but I think  maybe rtx 5080 would give me a but more performance and looks like RDNA 3 would not be getting FSR 4. My main concern is these driver timeouts that I have been having last 2-3 months. Happening frequently, every days it bound to happen. Sometimes on stock settings with no UV or OC. That whats driving me crazy… I wasn’t planning to switch before driver issues started happening and itself really annoying when you play multiplayer games…",Neutral
Intel,"I live in Serbia, there prices are different. For example here used 5080 doesn’t even exist to be bought yet 😩",Negative
Intel,Serbia,Neutral
Intel,If you’ve tried a bunch of things for troubleshooting the timeouts you could see if you can RMA the card.  Some of them just have issues for whatever reason.,Neutral
Intel,I didn’t think about that really. But will see how it goes. Thanks for the recommendation,Positive
Intel,I don’t know how good Serbian consumer protection law is but it’s definitely worth a try.  Better than trying to sell a lemon.,Positive
Intel,I sold a 3080 for $260. I think you can get a 3070ti for that price.,Neutral
Intel,"Your current GPU is the $100 option.  To double your performance you'll need something on par with RTX 4060 or RX7600 or RTX2080 or RX6650XT - all out of your reach.  RTX2060 Super will give you 50% uplift for $200.  ARC is still hit and miss, but A580 is around the same as 2060 Super. RX6600 is the same category.",Neutral
Intel,"You will be best off navigating the used market. $100 is a pretty tough number to work with that would likely limit you to sidegrades or very small increases - closer to $200 will be able to get you a really nice upgrade, and there are a few decent choices in-between.  I'd be looking at a 3060 Ti personally - they're a really good used option that can be found around the $175-200 range right now. If you can find a good deal, the 6600XT is another solid choice to consider, though it'd be a smaller uplift and they're not much cheaper than the 3060 Ti from what I've been seeing.",Positive
Intel,"If you're rocking 4 sticks of ram, sell 32bg of ram then buy a better GPU. It sounds troll but with current ram prices this might be the best option to increase your budget  32 gb total is more than enough nowadays",Positive
Intel,Id say look for a new b570 or a used 2070 super.,Neutral
Intel,"honestly, why not give the older 1080 Ti a try. it may not have the fancy features but it does go raster for raster with the 4060.  just basing this off of benchmarks from Passmark, the score for the 590 is **9315** and the score for the 1080 Ti is **18607**.   (The 1080 is in the $150 range currently)",Neutral
Intel,"Gaming @ 1080p around $100-200, shoot for an AMD RX 5070xt from ebay. Buddy got one for $69 a few weeks ago. Just depends on how lucky you get.",Neutral
Intel,Someone got a good deal. I'm mainly seeing 3080 10 GB models for around $300 atm - really nice used option right now for sure.,Positive
Intel,He can get a 2070 Super for $200. Has to negotiate. I got a full 3080 PC for $615 with a 10700k.,Neutral
Intel,"That's ingenious, actually.",Neutral
Intel,Going to be a downgrade obviously. Intel has Been getting to the point those cards are actually good but it’s still going from high mid tier to low tier,Negative
Intel,you got a free game from intel,Neutral
Intel,as a low tier card it's quite decent. i use it to play games at 4k,Positive
Intel,Yep went in expecting a downgrade but I don't game too much lately haven't found the latest games all that good and mainly playing older games at the moment so the trade off for now isn't huge for me.,Negative
Intel,Yep Battlefield 6 key.,Neutral
Intel,Very cool Exxazy,Positive
Intel,"![gif](giphy|Idg2rAVGS3xMZtBdhu|downsized)  It’s fine, plastic doesn’t conduct electricity 🫣",Neutral
Intel,![gif](giphy|xT5LMtbEtZnbbCE08g),Neutral
Intel,It lost its sparkle  ![gif](giphy|63VH5uoslatcA),Neutral
Intel,[https://www.enostech.com/sparkle-intel-arc-b-series-graphics-cards/](https://www.enostech.com/sparkle-intel-arc-b-series-graphics-cards/)  Is this your card? So it's like a design / badge of the brand,Neutral
Intel,Remove it before it falls in the fan and fucks it up.,Neutral
Intel,"Each GPU chip is unique. Some are better at overclocking or running cooler, causing stock performance differences.  It's called the Silicon Lottery",Neutral
Intel,What did you need 6 B580s for? AI farm?,Neutral
Intel,"The best card here is about 2.3% better than the worst. I'd chalk that up to run-to-run variance especially with only 3 runs each. I'd be willing to bet some of these individual cards had a similar spread in results.  This is entirely normal. No two cards are entirely identical. One might have a tiny bit better cooler mounting or need a tiny bit less voltage to hit top clocks, and that one will be a tiny bit faster.  If you were to overclock or undervolt and chase a record, that top performer is the first one I would try first, but they are all so close that any one of these could be the technically best one.",Positive
Intel,Any idea on how the temps vary with those scores?,Neutral
Intel,"That doesn't look like too much of a variance to me, I bet nvidia and amd cards would act the same. Not all chips are equal, the silicon lottery determines what you get",Neutral
Intel,Seems within margin of error,Neutral
Intel,pretty close. was thinking of a sparkle b580 as my upgrade as well.,Neutral
Intel,"Neet data. Not many people test a bunch of the same card against itself.  I know with cpus they talk about the ""silicone lottery"" - extreeme overclockers would look for the best CPUs and get a few percent extra performance. I'd guess thats whats happening here.",Neutral
Intel,"Furmark is known for a *lot* of run to run variance. You'd need to do something like ten runs, discard the outliers, and average them.  It's not very useful as a benchmark, doesn't run long enough, which is why you won't see many folk using it as a benchmark: It's a stress test.",Negative
Intel,"So we're talking about about Standard deviation of 78.2 and mean of 9187.6, which means less than 1% deviation from the mean for about 84% percentile of the cards. While sample size isn't to big to draw any conclusions and tests should be more thorough and controlled, this is not high variance.",Neutral
Intel,Average variation +/- 66 with maximum of -113  Max is a 1.22%,Neutral
Intel,thanks. now i can tell if mine is good or bad. wiat wait wait... no ya didn't benchamrk it on a core ultra 9 100series. thats a laptop only chip!,Positive
Intel,"NO ONE NOTICED THE CORE ULTRA 9 ""100"" SERIES?!?! I think he meant 200.",Neutral
Intel,Fucking ai people and their retarded local llm bs,Negative
Intel,"And not just GPUs but CPUs, vehicle engines, electric heaters, gas boilers, light bulbs, batteries etc  Basically anything will have a “tolerance” where the “output” will vary between x and y and is not a constant.",Neutral
Intel,"and also if you get a higher tier gpu then youre guaranteed to get a really good chip out of this lottery, such as gigabyte aorus xtreme cards",Positive
Intel,Yeah I was planning doing some local llm inference,Neutral
Intel,Good question.  I wonder if they are being thermally restrained or power limited.,Neutral
Intel,Very good GPU IMO the sparkle one looks better and clocks higher than stock,Positive
Intel,Hmm ok while I still have the cards not deployed in anything what benchmarks should I do like what exact testing are y'all looking for,Neutral
Intel,Yes I benchmarked it on my mini PC it is a mobile chip,Neutral
Intel,"What I do with my money and my hardware should not upset you. When people say ""AI people,"" we are referring to large corporations and data centers buying up all the GPUs and RAM and using the AI to ruin our lives even more. What **local LLMs** are doing is to bring that power back into our hands, the consumers. The everyday person. Would you like it so that all things to do with AI belong to Google and OpenAI, or would you not want a world where us, the consumers, can fight back with our own models?  if you dont know what to say about a matter its best to enlighten yourself before shitting on something",Negative
Intel,"And also humans, it's called genetic lottery.... Usually pcmr members don't fare too well on that. 😥",Negative
Intel,"Nvidia bins their chips into very tight bins. Board partners aren't even allowed to sell a chip as overclocked when Nvidia deems it to not be, so there's less of a lottery going on than there is in other places.  That being said, there will still be sample variance.",Neutral
Intel,"Why is this guy getting downvoted? Local LLM inference is really cool and you get full control over every part of it. Maybe people are thinking ""AI bad"" but the random hobbyist buying $2k of hardware is not the same as the large corporations buying the entire world's DRAM supply.",Neutral
Intel,Why go for Intel and not AMD or the one who shall not be named? Is Intel best?,Neutral
Intel,what?!? how did you fit the gpu?,Neutral
Intel,"Yep, they have 2 chip models with one permitting factory of and one not. And the ones permitting also tend to get you higher speeds  But for example if you buy an aorus card, chances are you can't go any higher because the chip couldn't make it to be an xtreme model. On the other hand, an xtreme card probably will go up a bit extra",Neutral
Intel,Nuance is too hard for most people.,Negative
Intel,Most pple dont care they are like robots them selves they hear ai and immediately down vote some pple even believe that someone buying a few GPUs directly means they are taking away from gamers people dont care as long they have something to hate on,Negative
Intel,"From what I saw when I was looking, you can get decent amounts of vram for not too bad, I grabbed an intel b580 for my home server cause it was mega cheap and it can rip transcodes pretty ez",Positive
Intel,as others have mentioned currently the best bang for buck is intel for vram and also i got all of these cards for super cheap around 200-240 each also i dont want to support nvidia at all they can go fuck themselves and as for amd there is very little llm support on their cards,Negative
Intel,"that is 6 b580 carts at 12GB each right now they are selling for like 260 USD each at some retailers so that is 1,560 USD all before tax that is.  And you get 72GB vram to use as long as you have a system for using this.  And 3060 12gb are going for about 300-350 and the 5060 ti 16gb is going for about  400-450  then the 9060xt 16gb is going for about 380-450.  so the 3060 12gb is out, it is way to much  Then lets say he got 5 9060 XT 16gb at 380 that's 1900 and that would be 80 GB of vram or 4 cards at 1520 with 64GB of vram.  Then their is the 5060 ti 16gb it would be even more.  With Vulkan API working with different LLM stuff it is easy to use just about all cards now, aside from the compile process if you want to get maximum performance. Other wise LMStudio will get you going with Vulkan fairly decently.  EDIT: My mistake you can get the B580 right now for 239.99 from B&H photo.",Neutral
Intel,Nvidia tends to be the most performative for AI but also most expensive. Intel is the best bang for buck if you're going for high vram counts. It'll be slower but at least the model fits. Amd isn't really a player in the AI space yet,Neutral
Intel,How about an EU chip since you declared people should be independent of american ones hmm,Neutral
Intel,beelink gti 14ultra look it up it has a full pcie slot,Neutral
Intel,"It is hard to predict how chips will overclock. We've had generations where CPUs were apparently binned very conservatively, so even way down the range chips would overclock like crazy (Intel Core 2nd gen) or allow you to unlock whole extra fully functional cores (AMD Phenom triple cores), but we've also seen generations where most headroom was already 'priced in' (Intel 13th/14th gen) or arguably even exceeded. We've seen generations with more and less headroom on the GPU side as well.  That being said, now that consumer GPUs are competing with AI chips, they'll want to squeeze every inch out of each wafer, so I suspect they'll bin things as tightly as possible.",Neutral
Intel,"Yeah thats a fair point, some generations can be liklier to overclock than others",Neutral
Intel,"The B770 feels like it is going to disappoint people. People want a 5070 TI competitor, but it seems more like a 5070 competitor, which could be great for the market.   Their tech suite is starting to feel a bit dated though. Frame gen is fine, but no RT denoiser like AMD and Nvidia is something that could use resolving, but the biggest is XESS XMX slipping into a relatively distant fourth place behind DLSS transformer, FSR ML, and DLSS CNN. A new transformer based architecture could really deliver a boost they need.    As for panther lake it is promising. An efficient yet reasonably powerful architecture with ML hardware for XESS is quite compelling in the gaming space with laptops and handhelds, but also low power laptops. Great encoding and decoding with WiFi 7 is quite nice too. Seemingly it will deliver better performance than stuff like the z2 extreme in the handheld market which is something we desperately need since it was such an incremental jump. Hopefully more big manufacturers consider Intel.   Nova lake is a lot of question marks right now, but it might be reasonably competent which is what we need from Intel. Even if they can’t match the 10800x3d or whatever they might be really viable competition for something like that 10700x or if they managed to do a really big jump the 10950x3d mega chips. I really want to see Intel succeed.  It isn’t good that AMD are starting to creep into the Nvidia style position for high-end CPU. Much like Nvidia in the GPU market has basically sole dominance over the 5080 and upward calibre of product. AMD is kind of dominating when it comes to their x3D processors for high-end gaming performance. That is never a position you want companies to be in.",Negative
Intel,"There is a terrible situation in the CPU's, AMD is dominating, and I will never trust Intel while having the government funding them publicly. Wish Nvidia did something cool in that, but I doubt that, they will probably make some shady movements there too.",Negative
Intel,"You might not trust intel, but I don’t know many people who would care about government subsidies and funding if they offered good value for money CPUs.",Negative
Intel,arrow lake is good  but the ram speed may not be fast enough?,Neutral
Intel,"Yep, its good, whole hate for it on internet is unjust for me, only serious downside is closed upgrade path, RAM? with  current prices you must take what is avalible for reasonable price",Negative
Intel,"If they could hit close to 5070 performance and price it at $399, they might just go from 1% to 2%!",Neutral
Intel,holy shit. these cards are not cancelled?  are they still coming?       theres still hope for Celestial?  https://preview.redd.it/daxvcoxft06g1.jpeg?width=300&format=pjpg&auto=webp&s=8b3dba05fcdcfb82799679baccf7b3c3b05073f6,Negative
Intel,"Over 50% more power, so it should be over 50% more faster.  So slightly faster than the 5060ti and 9060xt.",Positive
Intel,"Spicy.  Nice to see a unit with some power.   Really, their target only needs to be match the 5070/5080/9070XT range.  If they can do that and actually get volume built, they'll sell a ton.   I've other very interesting thing they could do is go heavy on vram, not because it's needed for games but because it's needed for bigger at home AI models.   Drop a 32gb just for fun, or larger and let people run bigger models than can even fit on 4090/5090 cards.  Create a cheap alternative to the pro cards.  Don't even worry about speed.  Just gotta fit model sizes.",Positive
Intel,"Lotta people hoping for 5070 performance who will be pretty disappointed. I'm not saying this will be 5060Ti level necessarily, but probably best we can hope for is something between 5060Ti and 5070 performance but with 16GB of VRAM, in the $350-$399 range.",Negative
Intel,Going to be a very interesting card. In the age of expensive GPUs...,Positive
Intel,"At that TDP it better be very, very close or better than RX 9070 XT... Ok, fine RX 9070, non XT.",Positive
Intel,Hmm for 300watts isn’t that close to 9070xt and 5070TI power usage? Does intel in general use more power? Just from efficiency standpoint and power usage that seems where it may land unless it is power hungry and inefficient compared to competition? Any info is appreciated. Trying to line this up as well 👍.  Either way glad to see it’s finally coming out. I wish they announced it sooner though would have tried to get it over the 9070XT to be honest.,Neutral
Intel,I’ll be watching this with interest. I love my b580 and I’ve had 0 issues with it regardless of whatever crap about them gets spouted online. Never had a crash or driver issue related to the card and the driver update have been coming hard and fast bringing improvements   If these are priced right I’ll be upgrading and replacing my wife’s 3060 with my b580,Positive
Intel,Motherfucker I just bought a B580 because I thought this thing was a myth!,Negative
Intel,I’d sell my 4070s to support them if they can hit the same amount of performance level. I know sounds redundant as hell but need to have more competition in the gpu space,Neutral
Intel,"Why do I suspect that even if this exists, it exists in the area of the curve where power usage explodes and performance stagnates?",Negative
Intel,Gross.  You guys know higher tdp is bad right? It’s literally a measure of power consumption.,Negative
Intel,"The B580 is 4060 performance at almost 200 watts, it's probably gonna be 4070 levels.",Neutral
Intel,"""5070 Performance!""",Neutral
Intel,Just from sales standpoint 1%->2% means 100% more sales. Which is allright ?,Neutral
Intel,They can make it $150 and it still won't increase market share.,Negative
Intel,Well if Nvidia pulls a crucial and stops selling consumer cards then Intel has a shot!,Neutral
Intel,"At 300w TDP it better be, but definitely not at that price.",Neutral
Intel,"Can't wait for first qtr of 2027 to see them!  Honestly quite annoyed with this launch, they are seemingly having issues at this point. Battlemage has been out for a whole year with no news, only leaks of the 700 series cards.",Negative
Intel,"Judging by the late timing maybe they were ""uncancelled"" =p",Neutral
Intel,"That's assuming performance scales linearly with power consumption, I'm not saying it doesn't but that's not really something we can verify without some hard data.",Neutral
Intel,Oof I'm hoping around 5070,Neutral
Intel,I'll take that. The speed bump that comes with fitting everything in vram for AI outclasses raw power by a mile,Neutral
Intel,With the price of memory right now dont count on it.,Negative
Intel,Honestly if it’s 399 and hitting 5070/9070 performance. I’m buying in a heartbeat,Positive
Intel,"Not to ruin your day, but selling your 4070S just to ""support"" them is IMO a stupid decision.  The support you ""provided"" is completely negligible, literally a water droplet in a lake. In exchange, you get a far worse experience in your performance, software, power efficiency and compatibility.  All of that just so that you can feel right in your own PoV lol. Selling it doesnt change the fact that you already gave Nvidia your money.  You will be of bigger help by just keeping your 4070S and instead recommending Intel GPUs to your friends and other people who want to build a new PC (assuming Intel nails the pricing right). Spread the word instead, and dont sabotage yourself.",Negative
Intel,But with TDP performance goes in pair. NVIDIA nailed it and they are low TDP high performance. Thats why they are 90%. But to get some traction you have to compete in performance much more then in TDP. Thats what most games care for - FPS.,Positive
Intel,I doubt you're the only person here who was smart enough to put that together u/flatroundworm.,Negative
Intel,4070 and 5070 are extremely close.,Neutral
Intel,"""4090 Performance!""",Neutral
Intel,"Why not? As others have mentioned, their market share is already successfully increasing. There's no reason to think a new desirable GPU wouldn't continue that trend.",Negative
Intel,"Exactly.  If they were smart, they just build what others don't and build something no one can currently compete with.  Build big and take the sales.   There are three challenges.  One, it needs to be good enough to be on par with the 5070/5080/9070XT.  It needs to be a viable alternative, not 30% worse but on par, however they need to do it.  Two, they need to build a feature no other manufacturer has.  This is the vram part.  Go big.  Make it do something no one can compete with for a period of time   Three is the bigger challenge, but it might or might not matter to certain people.  This is cost and efficiency.  If the cost can be competitive to the 5070 and 9070 XT, they will get considerable sales, not like 1%, but as many sales as they can build for.  The second part efficiency.  If they can be highly efficient, they could market a budget option for AI use, not necessarily fastest but efficient for the work load.  This too could promote a significant sales volume just because it costs less at scale.   But, if very little is as achievable, you can always have a feature no one else has, and you can still sell quite well.",Neutral
Intel,If it's 32 Xe cores (up from 20 on the B580) that'd likely put it somewhere more like a 6800XT or 5060Ti :(,Neutral
Intel,It prevents someone else giving Nvidia their money while propping up intel - if more people did it like this mad lad it would make a difference,Neutral
Intel,"I bought the card used lol , no way I’d pay over msrp . But your statement stands true .",Neutral
Intel,It’s closer to 4070ti. It’s a 23% improvement over 4070. Putting it a little slower than 4070ti but faster than 4070S.,Positive
Intel,"""4100 Performance!""",Neutral
Intel,"It doesnt change the fact that the 4070S was already sold. Nvidia already got their money from that unit. Selling your 4070S DOESNT RETURN what you already gave to nvidia's pockets lol.  You can think of it as **paying for someone** else to get a 4070S.  That is the thing that people like dude above is missing.  In the end, you just sacrificed your own experience for nothing. Its a ***Lose Win*** situation.  I aint gonna be stupid to sabotage myself just for a temporary feeling of being right and morally superior, a feeling that I did the right thing, because in the end, its inconsequential, and I just fucked myself in the process, and what is waiting for me in the end is just regret lol.  You might as well ***keep*** the 4070S, keep the better perf, software, efficiency etc etc and ***just focus on preaching*** to people to buy Intel GPUs (assuming its good value). ***Thats a win win scenario.***  Also, you dont want to mindlessly give Intel your own money. You want them to earn it. Buy Intel GPUs not because you want a 3rd player, but because you know its a good product and helping out a 3rd player in the GPU market is just a nice bonus.  If Intel half assess the B770's pricing and launch, and you still bought it just ""to support the underdog"", you are just incentivizing Intel to continue half assing their approach",Negative
Intel,"Damn I was thinking mobile, those are like 10-15 percent",Negative
Intel,"Private profile, shilling. Jog on buddy, no one knows what the card will be like.",Neutral
Intel,"You keep repeating the idea that “the 4070S was already sold, so nothing changes,” which tells me you’re stuck in the sunk cost fallacy. Nvidia getting money from the original sale is irrelevant to the next decision. What matters in economics is marginal demand. When someone sells their 4070S and chooses Intel for their next GPU, they remove one potential new Nvidia sale (the buyer of the used card may otherwise have purchased new) and simultaneously create one new Intel sale. Those marginal shifts are exactly how incumbents lose revenue and challengers gain market share. Sunk revenue doesn’t grant Nvidia permanent immunity from switching behavior.  A strong second hand market absolutely hurts Nvidia’s future sales. It lowers the effective cost of GPU ownership, pushes price sensitive buyers into used units instead of new ones, and reduces Nvidia’s pricing power. This is standard industrial organization theory used goods cannibalize new goods. Selling the 4070S does not “give Nvidia money,” it gives someone else a path to avoid paying Nvidia at all. Then replacing that card with Intel means Intel gets fresh revenue while Nvidia gets zero. Combine both actions and the net effect is Nvidia loses one new sale, Intel gains one new sale. That is the literal mechanism by which competition grows.  And pretending that “one person doesn’t matter” ignores how collective marginal actions scale. If even a small percentage of consumers switch from buying new Nvidia cards (~€600 on average) to Intel instead, the foregone revenue becomes massive. For example, 100,000 people doing this is €60 million Nvidia doesn’t get and €60 million Intel gains. That’s not “inconsequential”, that’s enough to change pricing strategy, product roadmaps, and R&D priorities. Markets don’t change because one individual defects they change when many individuals do, and that only happens if people stop rationalizing inaction with the sunk cost excuses you’re using.  Your whole argument hinges on a single transaction worldview, as if each GPU purchase is isolated. But the GPU market is dynamic. Nvidia’s future behavior depends on demand elasticity, competitors’ traction, and consumer switching. Selling your Nvidia card and buying Intel is a strategic choice that weakens incumbent dominance, strengthens pricing competition, and signals that alternatives are viable. Yes, you shouldn’t blindly buy Intel if the product is bad but when Intel is competitive, shifting demand is exactly how you incentivize innovation and better pricing across the board. In other words you don’t “sacrifice yourself,” you just refuse to keep subsidizing the monopoly.  In short, your logic collapses because you ignore marginal effects, second hand market cannibalization, collective action, and basic microeconomics. The 4070S sale is sunk the choice of what you buy next is what actually shapes the market.",Negative
Intel,"So what does a private profile got to do with it?  Or did you just ignore all my points.     The fact that said the word ""shilling"" is clear indicator you didnt understand what I said.  Did you want to go full Ad Hominem Fallacy by using my own profile against me while not doing anything with my statements LOL?",Negative
Intel,"Give it time and that guy will switch it up and come back with ""consumers don't matter anymore to Nvidia, only data center"". These corporate bootlickers are very narrow minded.",Negative
Intel,"The fundamental point you have is that a hypothetical card will be worse than the 4070 super, which is silly to say because we just don't know.",Negative
Intel,"Which still doesnt change the fact that itll be less efficienct and have weaker software. The 4070S draws 220W, and has access to a multitude of features at far greater availability and support, that is Nvidia's biggest strength that even AMD stans wont deny.  It still doesnt change the fact that you already gave money to Nvidia.  You could use your own statement against OP and yourself. Why would you sell your GPU when we dont even know how good it is? Lol. You are dumping a good card for an unknown one.  And I already said the following:  >You might as well ***keep*** the 4070S, keep the better perf, software, efficiency etc etc and ***just focus on preaching*** to people to buy Intel GPUs (assuming its good value). ***Thats a win win scenario.***  >You will be of bigger help by just keeping your 4070S and instead recommending Intel GPUs to your friends and other people who want to build a new PC (assuming Intel nails the pricing right). Spread the word instead, and dont sabotage yourself.  Tell me, wheres the shilling here?",Neutral
Intel,"With that price point if you live nearby a microcenter you can swing by there and grab the 7600X3D combo with the MSI B850 motherboard and 1 stick of 16GB DDR5 Gskill Flare Ram, Zotac RTX 5070 Solid, and an air cooler if your choice for about $950 before tax. When Ram prices drop back down in the future you can always go back and grab another 16GB Gskill DDR5 6000 CL36 Gskill Flare X5 single stick to run in dual channel with what you have.",Neutral
Intel,"Micro centers got great bundles on a 7800x3d along with an am5 b650 gen mobo or option to upgrade to x870, along with 32gb of DDR5 for about $600.   This’ll leave you a bit limited on a gpu if that’s your budget though. 5060 may bottleneck, 5070 would be ideal however if you can stretch budget to $1100-1150",Positive
Intel,are you buying in the US?,Neutral
Intel,Nah he said he's in Canada in the post,Neutral
Intel,Nice find. That would be my budget build choice and I'd snap it up at that price!,Positive
Intel,"Saving for a 9060XT is best, it's usually between 250-300 for the 8GB model. It's better than a 5060 at least. And on older platforms AMD will perform a lot better due to their lower CPU overhead, as well as this the B570/B580/5050/5060/5060TI all have PCIE X8 interfaces, which doesn't help especially on PCIE 3.0, especially when running out of Vram, the 9060XT is the only current generation X16 low end card from any of the three.  Go used below that, there's really nothing compelling below and even at that pricepoint you could probably get something better used or continue saving for a 9060XT 16GB since the extra vram will help it last a lot longer although it's a lot more money.",Positive
Intel,"I would personally look for something with a wider bus width than 128-bit.  That being said, maybe a 6700xt or a 3070 TI.  The 10400 is going to bottleneck a ton of cards.",Neutral
Intel,Very informative post. Thank you. Think I will save for the 16gb model.,Positive
Intel,But if I'm playing on high/medium settings no higher than 1440p with a max fps of 60 (my TV) the bottleneck shouldn't be too bad right? I realize my stuff is older but I feel like I can still get a pretty big bump upgrading the gpu.,Neutral
Intel,you should be fine at 1440p.  I would not really worry about getting the latest and greatest card though.,Neutral
Intel,"I know Ram is crazy expensive right now, but you'll seriously lose a lot of performance running only one stick of ram in a Ryzen system. AMD CPUs don't like running single channel. Other then that congrats and welcome back!",Negative
Intel,I lack the funds to buy hardware.,Negative
Intel,That GPU doesn't play well with that CPU my brotha.  Those Intel Battlemage GPUs don't like AM4.  Perhaps try switching to a 13th or 14th gen Core i5 and a cheap DDR4 motherboard.,Negative
Intel,Always something new to learn. Is it gonna bottleneck the system badly or I could buy another module in a couple months?  Thanks mate!,Negative
Intel,It’s tough for most people nowadays. Hope you are able to soon.,Negative
Intel,Why exactly? Rebar issues?,Neutral
Intel,![gif](giphy|eRtdjiQ07r2rm2vik2)  There’s no way I can find an i5 in the price range of the r5. I’ll let you guys know how’s it going in a couple days. Wish me luck and thanks for the heads up.,Negative
Intel,"Not the end of the world, but surely dual channel would be a lot better for performance. I'm not sure if it is a good idea waiting, consindering the actual ram pricing issues, but your on ddr4 and on a budget, so it should be ok. Enjoy your build!",Positive
Intel,"Good to hear.  About the RAM…I know a guy, so I’m probably gonna be fine.",Positive
Intel,"Remember that AM5 is not your only uograde option. Depending on the type and quality of your motherboard, you can upgrade all the way up to a 5700x3d or 5800x3d, both of which are beasts for gaming. Or you could always try to trade your 5600 for a 5600x or 5700x if you don't want to invest too much into it. CPU power matters more in lower resolution and high refresh rate scenarios, but at 1440p your current set up still has plenty of life left.",Neutral
Intel,"5600, 32gb ram, and 5070 will be a good gaming pc. You should be gaming nicely. Much better than A750!",Positive
Intel,"this will work perfectly, you dont need a new cpu to handle a new gpu, the one you already have can handle it perfectly fine.",Positive
Intel,9070 is much better,Positive
Intel,"Lower end AM4 motherboards have only PCIe 3.0 slots, syou might want to check that.",Neutral
Intel,"Thank you guys, I'm a technician, have been for years, did my own research and its all good but then, everywhere you look at the Internet its someone telling you, you need the latest and greatest for something new to work 🙄 Im gonna put it down to mass obsession from those that can afford very expensive rigs 😂",Positive
Intel,"""Gift""",Neutral
Intel,"I have pci4, thanks though 😀",Positive
Intel,"yup. Influence the gift, after all you're partially funding it as you say.",Neutral
Intel,"No, it was bought like I said and after I paid back into a bit  because I thought it was too expensive to spend but its whatever as I said gifted and I'm grateful for the gift.",Positive
Intel,"I used to support Arc GIS where I work. I've since moved to a different area of support, but back then, we used what were classified as engineering laptops. It had 32gb of RAM, 8-core CPU (Intel I7 11850H). Mind you, that was back in 2018/2019. Not sure what they recommend now. But, with that machine, it ran fine. But Pro might require more resources, but my company at the time had just started to use ArcGIS Pro as opposed to ArcMaps, etc. But they were using the engineering laptops at that time with what I stated. So maybe you might be good if that's what you have? I would say these days, that's probably the bare minimum.",Neutral
Intel,I could run gis well on an old ass laptop while only running gis and made due on a lenovo thinkpad before. The later confused the teacher so much but it did work. So ngl your computer is probably more than fine. Save often either way.,Positive
Intel,Refer to ArcGIS's [system requirements](https://pro.arcgis.com/en/pro-app/latest/get-started/arcgis-pro-system-requirements.htm). You probably should've checked *before* buying your computer.,Neutral
Intel,"That’s one thing I learned in class, save as often as you can. We switched from Arcmap to ArcPro, and our desktops that we have struggle with the switch. So, I built I decent gaming computer just for this reason.",Neutral
Intel,"Eh, I’ve built my computer to specifically to be upgraded just in case, if it can run most high end games I’m sure I could be fine. Just wanted to see what people have used ✌️😗",Positive
Intel,Can’t talk for arcmap. We only used pro. But honestly i don’t even know what makes the difference at this point- like in the computer labs some machines were always wonky and for the life of mine it doesn’t make sense that a 10 year old lappy worked better than some of these lab desktops. Pure madness.,Negative
Intel,"You don't mention the PC actually turning off of or crashing. does it crash or not? the only problem you mention having is Arc Raiders logging you out back to the main menu which is not a hardware issue outside of potential Internet problems. A hardware issue would either crash the game, the OS, or shut off the PC, not send you back to the main menu of Arc Raiders.",Negative
Intel,"youtube is not any kind of stress test.   run a ram stress test tool, or pull the ram and test each stick (or half in various configurations)",Neutral
Intel,"Make shure you airflow is not obstructed, if your GPU or CPU temps is high - repaste it.",Neutral
Intel,Yea it was fully turning off but instantly turning back on. I reinstalled drivers and the game seems ok for now but ima clean up the pc a bit and get some new thermal paste going soon,Positive
Intel,Plan on doing that soon!,Neutral
Intel,It may be your GPU or CPU shutting of before reaching a higher temperatures. I had the same. Had to throttle the GPU and undervolt it.,Neutral
Intel,"The Radeon RX 9060 XT offers the highest raw frame rates at 1080p, outperforming the competition by roughly 4-5% on average.  The RTX 5060 provides nearly identical performance but adds the advantage of DLSS 4 for superior upscaling and image quality.  While the Intel Arc B580 is the slowest card, its 12 GB of VRAM allows it to handle Ultra settings that cause the 8 GB cards to stutter.  Ultimately, the video recommends the 16 GB version of the RX 9060 XT as the best long-term choice for modern gaming.",Positive
Intel,Had to sell the 6600 XT and went for the 9060 XT 16GB to play at 1440p. I’m loving it,Neutral
Intel,"Bought a 9060XT 8GB for 247e (renewed on Amazon, Black Friday stuff) and sold the temporary 4060 non-TI 8GB for 220e on marketplace. Good deal...",Positive
Intel,Only compares 8GB cards from teams red and green since it’s only considering <$300.,Neutral
Intel,"I feel like the HUB guys are going too hard with their VRAM crusade. Why recommend a GPU that's slower now just because it might be faster in the future?   A slight downgrade in render resolution or texture quality is hardly even noticeable, and with looming shortages I feel like most studios are going to start optimizing for lower VRAM further reducing the long term disadvantage of 8GB GPUs.",Negative
Intel,"The real answer, buy a used 2080ti. Usable VRAM, DLSS4, it still is 250W so it can run on most PSUs.  It is the most balanced option if you can't afford a 9060XT 16GB.",Positive
Intel,"all of them are power hungry junk, where are good cards?",Negative
Intel,5060 then cuz way better in AI  5% performance cut to gain 2x-3x AI speed,Positive
Intel,What processor are you using with 9060 xt?  Is it the same as you were using with 6600 xt?,Neutral
Intel,"Well yeah, the cheapest RX 9060 XT 16GB is [$380](https://pcpartpicker.com/products/video-card/#c=596&sort=price&page=1&P=11811160064,51539607552) and the cheapest RTX 5060 TI 16GB is [$430](https://pcpartpicker.com/products/video-card/#sort=price&P=11811160064,51539607552&c=593). When you're comparing $300 GPUs, you're not going to bring up a GPU that's nearly another hundred dollar.",Neutral
Intel,"That would be a completely new phenomenon if you look at the past. Sure, some (probably indie) studio will optimize their games, but they would have done so already because they care about their customers.  Nothing will change with the current devs or tech, it's just a temporary issue that memory is that expensive. The prices will be lower in 2027, or we'll get used to it and buy more expensive stuff.",Neutral
Intel,"6 ish year old product that is out of warranty from some rando, is not exactly comparable here and definitely not a ""real answer""",Negative
Intel,"Dunno why you're being downvoted, the 2080 Ti is still very good value for the price and often has good OC headroom. Beats 5060 in most cases and you're right about 11GB being decent",Positive
Intel,The real answer is to stop being cheap and spend money on your hobbies.,Neutral
Intel,you tell us,Neutral
Intel,Why do power requirements matter?   Electricity costs pennies,Neutral
Intel,"Can you elaborate what do you mean by ""AI speed""?",Neutral
Intel,"Yes, same processor, 5600x",Neutral
Intel,"Yeah, I just wanted to point it out because there’s people like me to whom prices in dollars means nothing (or who don’t read the title) and then waste time watching an irrelevant video (though I skipped to the conclusion so not that much time).🙂",Negative
Intel,"Point being? If a cap blows because it's old any repair shop can fix it, If it's a fan dying you can fix that yourself.  On the other side there's not much the warranty can do for running out of VRAM.",Negative
Intel,"In this economy? It doesn't make any sense to not keep perfectly usable hardware that does the job just fine out of a landfill.   A 2080ti or a 3070 or AMD equal is more than enough performance for most people. Easily, and is way better bang for your buck.",Neutral
Intel,"In a time of global economic uncertainty, it's a horrendous time to overspend on hobbies.",Neutral
Intel,"News flash: We're always in ""this economy"". I know someone who works at a fucking McDonalds, has a kid, and spends more money on his hobbies than you do.",Negative
Intel,They sound irresponsible.,Neutral
Intel,"If you spend more on your hobbies than the things you actually need, you might be financially irresponsible",Neutral
Intel,"From r/radeon   * Ray Caching: Only available in Warhammer40K today, more games next year. * Ray Reconstruction: Only available in Black Ops 7 today with more games next year. * AI Frame Gen: Available in Black Ops 7 today with 40 games by end of 2025.",Neutral
Intel,It's almost 2026 and AMD keeps reinstalling the AMD Install Manager that I do not want and have to keep manually uninstalling. Stop this AMD.,Negative
Intel,What is fsr redstone? and which games use it?,Neutral
Intel,"I got a notification for the update in AMD Adrenalin Edition, but it does not appear in the actual install manager lol",Neutral
Intel,I just tested the release on four machines (76X&78XT/78X3D&79XTX/97X&9070XT/75F&76XT). Every system still suffers from crashing drivers when hardware-accelerated apps are used (Chrome/Discord/etc.).  Please fix. :),Negative
Intel,so can I open adrenalin on this one with a rdna 2 igpu and rdna3 gpu or is it still broken like the last version,Neutral
Intel,<--- Int8 rdna2 enjoyer,Neutral
Intel,did they fix enhanced sync and noise suppression yet,Negative
Intel,Did this driver fix purple visual glitches with the RX 7700 XT? It's a known bug that appeared after the driver 25.4.1,Negative
Intel,"The ignorance by amd of Rx 7000 users is astounding tbh, but this is 2025 AMD not prior AMD where they would try to appease a larger user base.  It's going to make me rethink my loyalty for future gfx purchases",Negative
Intel,So we cant test path tracing performance yet on Cyberpunk? Lol,Neutral
Intel,"This is a very underwhelming update for RDNA 4 users I get that this technology needs to mature, but they should already be at a point where the implementation is across more wide array of games. My fallen RDNA 2 and RDNA 3 brothers will be remembered. The only reason AMD gpus are still relevant rn is price, nvidia tax is crazy. GG",Negative
Intel,"Thanks for nothing again, AMD.  Signed, 7900 XTX user.",Negative
Intel,So is there any point to installing this if I'm on RDNA2 and don't have any of the issues that they fixed?,Negative
Intel,This is the worst driver amd made 9060xt for me. 2 games instantly crashes. Indiana jones and silent hill 2. With this driver if you enable ray tracing game hangs and give error.i already report bugs in 25.12.1 and same with 25.11.1 and amd does nothing. every ray tracing titles works ok with 25.10.1 driver and this is bad. amd does not listen users anymore. anyone has any crashes happen like me?thanks...,Negative
Intel,Nothing on Oblivion Remastered crashing? Intermittent application crash or driver timeout on 9000 series when playing Battlefield 6?,Negative
Intel,AMD Software still crashes randomly,Negative
Intel,#AmdNeverAgain Give Fsr4 on rdna3,Neutral
Intel,New update new problems,Positive
Intel,"The adrenalin app just auto updated my 9070xt mid game, now my screen is black with no signal output to my monitor but my music is still playing lol. I waited for 10mins then I had to hard restart my computer for it to say the update failed",Negative
Intel,Pretty dissapointing ngl,Neutral
Intel,Should I get the RTX 5070 ti or 5080 at msrp? I am currently selling my XTX after radio silent news about FSR 4 int 8 on it.,Neutral
Intel,Everything is RDNA 4 exclusive? awesome /s  RIP finewine.,Positive
Intel,Please add the broken noise suppression to “Known Issues”.,Neutral
Intel,"If  this driver update keeps crashing my gpu im not leaving 25.9.2 for a while, im also starting to think about selling my gpu and get nvidea, and really black ops 7 why not a real game like cyberpunk i dont want to waste 70 euro for fifa with guns",Negative
Intel,"Can confirm on my 9060XT that Silent Hill 2 is still crashing and Avatar Frontiers of Pandora currently has a bug when FSR4 is enabled where the entire screen starts flashing like a strobe light, shadowy areas seem to trigger it. This is with both games fully patched & up to date.",Negative
Intel,"Let me see - all the new ""Features"" will be available for Cyberpunk 2077 in at least 1 year time and ONLY with RDNA4 ??",Neutral
Intel,AMD NoiseSuppresion still broken. Since September!,Negative
Intel,"Are pink artifacts fixed on RX 7700 xt, anyone ? It was bugged in 25.11.1 driver last month.",Negative
Intel,Where‘s support for 7000 series? Wtf is this dead meat,Negative
Intel,I’m on a 6000 card is there literally no reason for me to download this,Negative
Intel,"all i want is to be able to capture clips in my games but for whatever reason amd either doesnt understand im in the game, recognizes the game wrong (battlefield 6 shows as elder scrolls online which i dont even have).",Negative
Intel,It's december and still no FSR4 for vulkan.,Negative
Intel,Is this worth updating to from 25.11.1  Is it more stable?,Neutral
Intel,"I had to downgrade to 25.9.1 to have some level of stability, can somebody confirm that the new driver is safe to upgrade to without it messing stuff up?",Neutral
Intel,Still no fsr 4 support for rdna3 🙄,Negative
Intel,"Guys calm down. RDNA3 being moved to maintenance mode is part of their new strategy, no longer ""Fine Wine"", the new approach is Stale Ale. That way their products remain DOA after launch and people won't keep them very long.",Neutral
Intel,idk why I find it so funny that a specific Roblox game got called out in the patch notes,Negative
Intel,Did they fixed the amd noise supression not turning on?,Neutral
Intel,"Anyone know why Cronos: The New Dawn has been showing [""FSR 4""](https://i.ibb.co/nqW2VMng/Cronos-The-New-Dawn-2025-12-04-02-28.png) for me on a 7900 XT for a few weeks? At first it was 3.1.  I know it can be modded in but this is on a new Windows 11 install and I haven't done any modding.",Neutral
Intel,"Looks like new chipset drivers, too.",Neutral
Intel,"I thought the application freeze fix might have stopped monster hunter wilds from crashing on me but nope still does it (DXGI_ERROR_DEVICE_REMOVED,)",Negative
Intel,/u/amd_vik are you aware of assetq corsa evo vr not working on AMD cards since 25.9.1 ? It displays the left and right eyes out of alignment and therefore fails to show a cohesive single image.,Negative
Intel,so no fsr4 support on Vulcan still? this is getting ridiculous,Negative
Intel,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Thank fucking god.,Negative
Intel,Still experiencing 100% gpu usage almost constantly as soon as you boot up BF6 on newer drivers after 25.10.1 and higher temps in general  I'm locking my FPS to 144 but the older drivers is showing better overall temps and less gpu usage for me 🤔  [https://imgur.com/a/ctbMCx7](https://imgur.com/a/ctbMCx7),Negative
Intel,25.11.1 was dog water driver timeout city for me I'm just gonna assume this new one will also be the same.,Neutral
Intel,BF6 crashing after a few minutes in game with that driver on a 6800xt,Negative
Intel,"ever since 25.9.2 still same bug is present even now and now it causes even more problems because ML based FSR and FG fails when it happens: Adrenalin app just shuts down randomly even when idle, no errors, no driver timeout, no dx12 trimeout, just adrenalin itself gets shut down in random times. why wont you guys do something about it finally? Seriously its been so long now... im on 9070XT Steel Legend Dark Edition from ASRock, 80% of your users or more report the same issue FIX IT for the love of GOD. I tried everything hoping its on my side but windows reinstall, DDU and AMD cleanup app and fresh driver install nothing helped its still here",Negative
Intel,"Both 25.12.1 and 25.11.1 drivers have the same bug on RX 9060 XT. When my screen goes blank and later i wake up screen, i have two mouse cursors on the screen, until i launch some app and then will second, fake cursor disappear.",Negative
Intel,I hope this fixes the many crashes I've had since the last update...,Negative
Intel,"Still enjoying the piss out of the 7900XTX on 25.9.2. It chews through everything I throw at it at the settings I choose, don't care about new driver releases unless a new game I want to play doesn't play well on whatever driver I currently have installed.",Negative
Intel,"Even tho I have a 9070xt this is still so underwhelming… We waited 6 months and got basically nothing yet. Sorry for all rdna2, 3 users.  Fun fact: Its been years now that the adrenaline software cant be opened, the only fix ist to press win+p and select only main monitor. Than start it, than swap monitor profile again…   Definetly buying nvidia next time, not supporting this big company anymore, which is behind in every aspect. Image you just want to play alan wake 2 (looks beautiful).",Negative
Intel,"ass. no support for rdna2/3, no new features for rdna2/3, rdna4 have only one game that support all of that, redstone framegen almost identical to 3.1 framegen, frame pacing still there.",Negative
Intel,hardware unboxed tested it and frame facing is broken when amd frame gen is on sadly,Negative
Intel,So the HDMI crashing issues should be fixed in this version yes?,Neutral
Intel,Any news on fixing the gpu vram leak issue on bf6? Sorry I’m lazing not reading the patch notes,Negative
Intel,25.12.1 does not even install on my Minipc (780M) + 6650XT eGPU Setup.   I thought I might fix 25.11.X not opening in an eGPU Setup.   Guess I will be running 25.9.2 for another few Months.  God why something always break? I thought it would be better going all AMD for the eGPU setup.,Negative
Intel,"Yeah I'll still be with 25.9.1 until the texture flickering is fixed in BF6, also instant replay just didn't work in 25.11.1 for me.",Negative
Intel,Will this help Warzone not look so blurry on 7900xt? Game is unplayable,Negative
Intel,So there seem to be two links - going through support>picking GPU(9070XT in this case) downloads the 25.21.1 win 11-b.exe file meanwhile going from this release note article it downloads the win11-c.exe . Any difference?,Neutral
Intel,"im using 6800xt the driver page has the win11-a version and article have win11-c version. which one should i choose i really dont know and this ""different builds"" confusing a lot of people",Neutral
Intel,"Genuine question, why all the hype and rush to release this today when it has just two games to showcase the benefits?",Neutral
Intel,Jesus how long has that Cyberpunk Pathtracing crash been in the known issues. It feels like it's been more then half a year.,Negative
Intel,Installed with no issues,Neutral
Intel,"I can’t play Warzone because I can’t update my bios, there doesn’t seem to be a recent bios update available for my Acer Nitro 5, using Adrenaline. Anyone know if this will help?",Negative
Intel,Doesn't look like they fixed the bug with Enhanced Sync not working properly with Freesync.,Negative
Intel,Any fix planned for 9070 users who cant enable Hardware Lumen on Oblivion Remastered? Game crashes as soon as we turn on the option.,Negative
Intel,Still no fix for Battlefield 6 for those with AMD 6800M GPU. I swear my next setup is going away from AMD if this is not resolved anytime soon.,Negative
Intel,u/AMD_Vik      In 2022 AMD made changes to OpenGL Driver. So since 2022 the extension gl\_ati\_fragment\_shader is missing in the driver. It cause problems in older games like Call of Duty 1 from year 2003. Stutter on some maps and broken water rendering because the games can't use the extension anymore.     Our Community is waiting since 3 years for a fix.,Negative
Intel,in black ops 7 only 25.9.2 driver work better even new 25.12.1 much worse fps drops,Negative
Intel,Very unstable for me (7900XTX). Driver keeps crashing even when I'm just watching videos. Reverting to 25.11.1,Negative
Intel,i just had to roll back to 25.9.2 because 25.12.1 kept crashing my system with poe2   even GGG straight up said don't use 25.10-25.12,Negative
Intel,"After observing you guys for a few days xD, 25.12.1 was installed along with new chipset driver on my system.  To my surprise, unlike previous 25.11.1, Adrenalin interface now runs properly with igpu enabled.  I need to test it out with real games, but for now, I've dodged instant roll back.  FYI, If you're using two or more GPUs, including igpu, on a single system with muti-monitor. Download the C package(1.65GB one including rdna1&2+3&4).",Positive
Intel,"NoUnfortunately, they don't work (( Random crashes remained + In some games, the inability to use frame generation through drivers was added (( Sad ( R5 3600 32gb ram Rx 7700 xt ) Rolled back to 25.9.1 everything works with it",Negative
Intel,"I had a very weird issue:     My PC would just crash when i did an Windows Defender Scan (only Full Scan, it worked fine with QuickScan or other programms like Malewarebytes) like the power was cut. I did a number of things even rollback the chipset driver but that didn't help. Then i rolled back to 25.9.1 + the newest chipset driver and everything worked fine again.   In case somebody had a similiar issue",Negative
Intel,"Anyone having problem with AMD overlay with this update? Somehow not showing at any game even if enabled, if I click to different monitor, it shows up. But when I click back to the game it disappear again.",Negative
Intel,AMD Wattman settings don't apply for the first time they're set. They have to be changed and applied to a different setting and then to the desired one back and forth to get them to work. I use wattman to set my custom fan curve and it's been glitchy since 25.11.1.,Negative
Intel,"Error code 182 for my AMD Radeon™ 780M integrated GPU on my Ryzen 7 8854HS CPU.  All other driver updates before 25.12.1 worked fine on my Lenovo Legion Slim 5 Gen 9, but this one says my GPU is incompatible, even though AMD's driver download page is providing [this download link](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe) to the installer:  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-8000-series/amd-ryzen-7-8845hs.html",Negative
Intel,7000 Series web browser glitch? and sound glitch? huh,Neutral
Intel,"Is it safe to update, 25.9.1 is stable for my 9070XT and causes zero crashes with the timeout bullshit from clock speeds going to 3300+ MHz",Negative
Intel,What does fsr Redstone means ?,Neutral
Intel,"Is this driver more stable than 25.11.1 it was causing driver time outs and i even got a blue screen. I rolled back to 25.9,2 and now im scared to update to this one lol",Negative
Intel,These comments are all over the place is it better than 25.11.1 or not? 😂😂,Negative
Intel,"So in short, still no support for 7000/6000 series, yipee",Negative
Intel,"Idk what happened but after this update my game crashed and then my PC crashed and when I turned it back on AMD Adrenaline disappeared from my PC, completely gone. What did you do lol.",Negative
Intel,For Sale: 7900 XTX - $50 OBO  I know these are no longer desirable due to being left in the dust by AMD after only a few months of real support but hopefully it will be at least a good paper weight for someone.,Neutral
Intel,So now driver frame gen is gone? Unless the game specifically supports it? And the overlay as well? Both are completely gone now after the update...,Negative
Intel,What about the fixes for the 7900xtx crashing all the time?,Neutral
Intel,"«#AmdNeverAgain” Where’s the Christmas gift in the form of FSR 4 for RDNA 3? In the new 2026 year, it might be time to think about switching to Nvidia.",Neutral
Intel,"Toujours pas de FSR4 pour les séries 7000 ? C’est mort. Pour ma part, je n’achèterai plus de cartes AMD. Si Nvidia continue à proposer son DLSS pour les anciennes cartes, alors mon choix est fait.",Neutral
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,"Downloads ""whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe"" for 9070XT, ""whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe"" for 5700 XT  What does it mean?",Neutral
Intel,It took a while for DLSS 4 to get implemented in a good way on 40 series cards too but a version made it there. Give it time. Now if they can just start prodding developers to incorporate that as well it’ll be worth it. Not enough games yet but here’s hoping!,Positive
Intel,Any update the in fact that and adrenaline software is not working when second monitor is connected? Especially using iGPU for second monitor ?,Negative
Intel,Omg I think they fixed the LG oled tv reboot bug,Neutral
Intel,Should i install it directly or should I use AMD cleanup utility first?,Neutral
Intel,some one have problem with instaling?,Neutral
Intel,Does this fix the driver timeouts that were happening with Edge? I had to revert the November update because of that problem,Negative
Intel,Any fix or still need iGPU disabled for 7000 and 9000 cards?,Neutral
Intel,The update is still not showing up in install manager,Negative
Intel,Honestly this software was the bane of my card for the longest time. Not having it anymore stopped so many weird bugs and crashes.,Negative
Intel,Does AMD's Instant Replay record still bug out?,Negative
Intel,Anyone tried the new fsr redstone yet? I am hoping for a big improvement over the old fsr,Positive
Intel,do you guys remove the old drivers before you install new ones? or just install ontop,Neutral
Intel,The path tracing crash STILL on Cyberpunk is absolutely wild to me. Finally AMD has a card capable of playable raytracing but we can't use it on the 'Crysis' of modern times to even test it out.,Negative
Intel,Adrenalin doesn't show this update for me yet lol,Negative
Intel,"> Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Glad for this, it was annoying that we were stuck in 25.9.2",Negative
Intel,Are the issues with SecondLife fixed? Last driver that didn't break textures was 25.9.1,Negative
Intel,9060 non XT 8GB can do the math 7900 XTX Nintendont,Neutral
Intel,"I’m at work, so I cannot check for myself: does this fix the constant crashing in Oblivion when hardware lumen is turned on?",Negative
Intel,Has anyone tested Marvel Rivals on 25.12.1 version of the driver? The only stable driver that works without crashing on that game is the 25.8.1 version.,Neutral
Intel,Adrenalin Panel not showing bug still present… :-((((,Negative
Intel,Am I the only one who doesn't have the new option in the drivers for frame generation with a 9070 XT?  https://i.redd.it/7r86hslx3g6g1.gif,Negative
Intel,"why are there 2 versions, b and c, 1.65Gb and 991Mb, release notes and through the support page, and is it stable or shall i just keep 25.8.1 as any other seems to crash call of duty, regular, other games seem fine,  ryzen 9 7950x3d/rx7900xtx",Negative
Intel,Did it fix the god of war 2018 checkered shadows?,Neutral
Intel,"I spent all this time with 25.9.2 on my 9060xt because the following ones were disgusting to me, I will give this new update a chance and let's hope everything improves a little!!",Negative
Intel,Arc raiders crashes are fixed or not?,Neutral
Intel,"Makes my 9070 XT to constant run on 100% load in bf6 no matter if i play or sit in the menu. Cause device hung, graphic glitches and high temps.   Same with all drivers above 25.9.   25.9.1 works flawless with no errors and the load varies depending on the scenery as it should.",Negative
Intel,Noticing in Hogwarts Legacy with the new FSR and FG enabled over a period of like 30 seconds my 9070XT will go from \~250W used and 200 FPS and then drop down to say 120W used and 90 FPS and then after a short period go back up again. With FG disable it stays consistent 140 FPS-ish,Neutral
Intel,"FYI for ""Driver Only"" guys, 25.12.1 still have an issue to install this option.  l've open ticket to support team for last 2 versions. but I can't follow their request to observe the issue.  Don't know how long to keep using extracted file method. lol  Will see how 25.12.1 ""driver only"" perform.",Negative
Intel,oh nice! they fixed the FSR4 Quality Presets artifact issue,Positive
Intel,"When AMD finished Orange, Yellow Green, PurpleStone, can we unlock FSR Infinity?",Neutral
Intel,"Is it worth updating to this latest driver? I am not planning to use frame gen, is the image quality better or are there any fps improvements in games?",Neutral
Intel,"Updated to 25.12.1 now, before I was on 25.8.1, have a Rx 6800 XT and a Ryzen 7 7700X. Also updated my Chipset-Driver today. Haven't testet much yet, played now for like 1 hour Space Marines 2, watched some Youtube vids since I updated. So far looks ok. Only thing that worried me first was that I found in my Reliability History, 2 critical entries of LiveKernelEvents of code 1a8. But these were written down by Windows on the time, while I was updating my driver. We will see, if anything happens I will keep you updated.",Neutral
Intel,"Despite the device ID-based driver update blocking set in August, it has worked until now. The windows tried to install some driver on the 6700XT just now, and unfortunately, it also replaced the software itself somehow. threw an error message too.  Manual update would not go through unless i removed the driver update block.   What a sad situation.",Negative
Intel,"Anyone else has problems with CS2/Fortnite? Started happening after i updated drivers to 25.11 My whole PC would randomly freeze for like a minute with the ""AMD software detected that a driver timeout has occurred"" error. Once the PC unfreezes i must kill the game from task manager.",Negative
Intel,Does it fix the arc raiders dxgi crash of the previous driver?,Neutral
Intel,"How do I downgrade from this driver?   I’ve tried four different older drivers and all of them give me error 182 – GPU is not supported (RX 9070 XT) during install.   I’ve already used DDU and the AMD Cleanup Utility, but the only driver I can install successfully is 25.12.1.",Negative
Intel,pc started to crash 7900xtx... reverted to 25.11.1,Negative
Intel,Hi me and other people I know. Also forums and Facebook pages . Have had an issue with the frame gen after 25.9.2 . When they released new features we have all had issues where its drops fps and is completely unplayable. Has this been fixed in 25.12.1 I have 7900 xtx 7800x3d. Friend has 9070xt 9800x3d Both have issues. And im on windows 10 he's on windows 11. I used ddu and tried all settings on frame gen and other settings to fix it. Not to mention the drivers where stutters and lower fps without frame gen. Thanks,Negative
Intel,"When I enable V-Sync in the game, I experience lag; it only runs smoothly with V-Sync enabled when I also activate the performance overlay. This problem has existed since driver version 25.11.1.",Negative
Intel,"I have a second card from amd. And both cards have driver problems. Now I have an rx 9070 xt oc. I don't do any undervolting. Everything is at factory settings including the bios. I had 4 driver failures in 7 hours. What good is FSR if the driver doesn't work? It would be good if you finally solved this problem. I can stand it for a while, but if it continues like this, I'm leaving AMD.",Negative
Intel,"Wish they would acknowledge the bug where turning on GPU scaling and integer scaling adds more input delay, so for example the mouse movement will feel sluggish.  Been having this issue for 3 months now since nya bought a a 9070 XT",Negative
Intel,"On the RX 7600S graphics card, Adrenalin does not launch at all, and during installation it removed the driver PCIVEN_1022&DEV_15E2&SUBSYS_15131043&REV_60.",Negative
Intel,"How are those with a Cezanne CPU supposed to install this?  Selecting the 5750G from the drivers download page offers 25.21.1, yet none of the 3 variants of the installer support it.  * whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe (Vega, supposedly?) - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe (combined? ""Systems with RDNA series graphics products"") - nope  Each of them return Error 182.  Even the minimal web installer, amd-software-adrenalin-edition-25.12.1-minimalsetup-251207_web.exe, only offers 25.8.1.  VEN_1002&DEV_1638 is nowhere to be found in the .inf for any of the 25.21.1 variants.",Negative
Intel,I'm still hesistant to upgrade on this driver until they resolve these driver timeouts hell I'm even on 25.9.1 still experiences time to time TDR's.,Negative
Intel,"Is it worth for my 9060XT to go from 25.11 to the latest, Im having some problems where ghost of tsushima crashes.",Negative
Intel,Should I download the new driver version if I have 6800XT? There is nothing in the patch notes about this series... And if yes - why?,Neutral
Intel,Driver is making valorant run like crap for me idk why .,Negative
Intel,im using an rx6600 and up until today i was fine avg 200fps on r6 today the game says its at 22 usaeg when it avgs 1-4 and now it has major fps drops/tears,Neutral
Intel,This driver constantly crashes call of duty for me. Whatever windows update installs(which seems to be 25.10. something) is the most stable there is. 9070XT.,Negative
Intel,"Parabéns, fiz a atualização para 25.12.1 e agora não consigo jogar nada sem travamentos, além do google estar lento",Neutral
Intel,FSR Redstone support? Will my minecraft machine run faster now?,Neutral
Intel,Gonna be able to play modern titles on my HD5750 thanks to redstone !,Positive
Intel,All that build up for dog water. Built my first PC in March and went with a 9070xt full of hope. I'm beginning to understand why AMD is so widely despised.,Negative
Intel,Windows just installed the June   update from AMD. The fck is this,Negative
Intel,I'm not seeing the new update in AMD Install Manager,Neutral
Intel,so in 2028 10 games will have it like FSR4 XD,Neutral
Intel,Anyone knows if it fixed the crashes with Oblivion Remastered and Silent Hill?,Neutral
Intel,"""Intermittent application crash or driver timeout may be observed""  This is not an issue tied to a few games.... is a wide issue for me with a 9060 XT whenever i rise my monitor refresh rate.",Negative
Intel,"Here’s the thing. You either need to address the FSR4/Redstone on RDNA 3 issue with a time frame or I’m just going to refund all my boxed RDNA 4 gpu’s, melt down my XTX’s, and go back to buying Nvidia cards because they’re better in everything but COD (which sucks). People do not support you because your hardware and software are better, they support you because of “AMD Fine Wine”.",Negative
Intel,What a peace of trash. Fsr 4 quality looks dogshit,Neutral
Intel,"Ray Caching in 40K?  Not sure how they got this to work on the tabletop in real life but sounds awesome  In all seriousness there are a large number of games in the Warhammer 40K universe, any chance they are saying which one?  Space Marine 2 Darktide Battlesector   Etc",Positive
Intel,"so pretty much nothing for today, shrug...",Negative
Intel,Is there a partial list of the 40 games with the new frame gen? Is it something different from the fg we already have?,Neutral
Intel,There are well over 100 warhammer 40k games. Did they not specify?,Neutral
Intel,they wont,Neutral
Intel,"It's so annoying.  I would keep it if it didn't constantly pop up trying to get me to install ""AMD Chat"" and ""AMD Privacy View"".  I don't want your shovelware AMD, take a hint.",Negative
Intel,"There should be an option during install to exclude it, it can't be that hard to do. Same as you, u/MihawkBeatsRoger , I also uninstall it afterwards.       Notifying u/AMD_Vik",Neutral
Intel,"This.   Why I took it out are my own reasons and quite frankly, irrelevant. It's my PC and I don't want it. So please AMD, listen to me and keep it off.",Negative
Intel,"Focus on serious matters, this is a joke. If you do not want it feel free to install the driver only version, and be happy u have that choice. If you want the full features of adrenalin, well install manager is one of them.",Negative
Intel,Why not just leave it and don't use it?,Neutral
Intel,It's a rebranding of the entire FSR ecosystem. What's new today is machine learning enhanced frame generation for RDNA4 cards. You can enable it in the driver for any game with FSR 3.1.4 or newer.,Positive
Intel,It adds denoising for Path tracing. In theory it should look way better now,Positive
Intel,All the games that don't use bluestone,Neutral
Intel,"Only one , the new call of duty ATM. So if you enjoy shitty games , have at it",Neutral
Intel,"Same, and I'm still on the October drivers",Neutral
Intel,"Delete the amd install manager in windows. This way you can click ""check for update"" button on adrenaline and get drivers from there no time.",Neutral
Intel,You can download it from the website. The app release notification always lags behind the site. This is nothing new.,Negative
Intel,9070xt i see brave or discord freezing and lagging when watching a YouTube video still. I dont understand how hardware acceleration bug hasn't been fixed yet. Wtf are they doing.,Negative
Intel,Yup same here. Had to roll back to October to fix again,Negative
Intel,25.9.1 works on my 9070 XT. Everything after that is a mess for me,Negative
Intel,Tagging u/AMD_Vik  so they are aware of the issues.       I encountered the same problems on my 6800xt. Figma on chrome is causing random BOSD. The system will just restart without notice. Every single web app seems unstable on my system and memory usage is all over the place. Rolling back to 25.9.1 doesn't fix everything but it eliminates 70% of the issues..,Negative
Intel,Oh well. :/  Funny thing is I rebooted my PC again for a Windows update. The first thing that greeted me after opening a web browser was the driver giving up the ghost.  On 25.11.1.,Neutral
Intel,Ive been wondering what this seemingly random crashing has been. Thanks for this comment!,Negative
Intel,"9800x3d, 6950xt, no issue with either chrome or discord or firefox with hardware accelerated set",Neutral
Intel,"Me too.  Installed 25.12.1, whenever I use YouTube in Full Screen, the whole system freezes, while the sound is still audible, then I have to hard-restart my PC. Happened three times, decided to downgrade to 25.11.1 again.",Negative
Intel,"This should be fixed, I'm not sure why it was omitted from the release notes",Negative
Intel,<--- inte 8 rdna3 enjoyer,Neutral
Intel,"How do I set this up, can't find any info",Neutral
Intel,"I can't speak on enhanced sync, but noise suppression is still busted and not working =/",Negative
Intel,"I'm piggybacking, because I need that info too",Neutral
Intel,Ok I thought I was the only one having the enhanced sync issue because no one replied to any of my posts about it. I use it because then I can lock my fps to 120 (on a 4K OLED TV) and use enhanced sync instead of Vsync because of the screen tearing when locking to 120. Now I have to do the frame lock to 117 which is fine but just annoying me I'm missing out on 3 fps lol it was causing issues in a few games where it would stutter like crazy and it all came down to enhanced sync. I don't use noise suppression so I don't know what's up with that.,Negative
Intel,"Been using it for a few hours with the 7900XTX, so far so good.   Hopefully it's 100% fixed.",Positive
Intel,I hope they fixed it. I will test it now,Positive
Intel,"Did the typical test that I usually do and it didn't show up for me and I'm on the RX 7700XT as well. So hopefully, it's fixed.",Neutral
Intel,"AMD stopped giving a shit about it's fans once the company was saved and they started raking in the money. The change in tone was clear as day. That said, I'll still buy their GPUs because I hate Nvidia far more and I don't see that changing.",Negative
Intel,"yeah my next one will be Nvidia, better features, better performance espacialy with RT/PT   And apperently longer support... and AMD cards in a simmilar performacne bracket don't even cost THAT much less sooo.... jeah I am mad aswell",Negative
Intel,"I agree. AMD has shown poor support for 7000 series owners. If there was a FSR4 int8 leak, AMD should officially release FSR4 for 7000 series owners.  I bought my 7800xt only 2 years ago before RDNA4 cards came out.  Nvidia provides DLSS4 upscaling to their older generations like rtx3000 series",Negative
Intel,looking back rn i think it wasnt worth the 100 dollars gain i gotwhen my 7900xt does consume more than rtx 4070ti and i do have shit features even the antilag+ scam that was one of the main reason i bought the GPU isnt here anymore .,Negative
Intel,And just like that comment and user deleted themselves 😂,Neutral
Intel,"If you're referring to the app crashes with RTPT reflections enabled, we're working with CDPR on a fix",Negative
Intel,Signed /another 7900xtx user,Neutral
Intel,"I came over from NVDA last March, bought a 7900xtx, RMAd it a few weeks ago due to pink/purple pixelation that would randomly happen. Now it's non stop driver timeouts and random performance issues every time I boot my PC or games. I am never buying another AMD card. I'd rather get ripped off by NVDA and not have constant headaches.",Negative
Intel,"Which driver are you currently on? I'm just curious; personally, I'm on 25.9.2, and surprisingly, I have 0 problems, unlike with previous versions. Should I try 25.12.1?",Positive
Intel,"Unterschrieben, Gigabyte 7900 XTX Benutzer.",Neutral
Intel,"Nope. Generally if the driver does not massively increases performance in some game, or you don't have any issues or the issue you have isn't fixed, then it's not worth updating, unless there is some new feature you want.    I reverted back to 25.9.1 (from the top of my head) because with any newer driver BF6 crashes randomly, and neither DICE nor AMD seem to give a damn about it.    And before someone asks, I tried any other fix on the internet for Battlefield and nothing else worked.",Negative
Intel,Same here. Anything above 25.9.2 crashes ray tracing games like Silent Hill  2 and Oblivion Remastered.   Ihr never had a more crash prone GPU than the 9070XT.,Negative
Intel,"Try this, taken from another comment branch https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",Neutral
Intel,Might potentially be fixed by a recent Windows update?  24H2 (and an earlier mini-patch that included this) apparently resolved a lot of crashing for folks.  See [here](https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/),Neutral
Intel,"Yeah I was really hoping they'd have got buy in from a decent number of devs with updates to big RT showcase games like Indiana Jones, Alan Wake 2, Cyberpunk, etc. But Black Ops 7 and Warhammer 40K... and that's it (for the RT features)?",Neutral
Intel,5070ti fs  basically a better 9070xt,Positive
Intel,"I’d wait on 5080ti with more VRAM but these are going to be obscenely expensive knowing nvidia + current RAM prices. Both 5070ti or 5080 are more of a sidegrade than upgrade, not worth the hassle IMO.",Negative
Intel,Get a 5070ti. I never thought I would say that. But this is what is is.. 9 months after release and the drivers are still D.S.,Negative
Intel,What about a secondhand 5070ti?,Neutral
Intel,"I mean, I wouldn't get either. 5070 TI is a sidegrade from the XTX, and 5080 is only slightly better. DLSS and RT would be the only reason.",Neutral
Intel,Sidegrading for an upscaler sounds like a joke.,Neutral
Intel,"I think Linux developers are doing some experiments As of now, FSR 4 (FidelityFX Super Resolution 4) does not officially support RDNA 2 or RDNA 3 GPUs, even on Linux. However, thanks to Develer’s work on VKD3D-Proton 3.0, there is partial and unofficial support for RDNA 3 under specific conditions.  RDNA 3: Partial Support via Develer’s VKD3D-Proton  - Develer’s VKD3D-Proton 3.0 includes support for FP8 (8-bit floating point), which is required for FSR 4. - This means RDNA 3 GPUs (like RX 7600, 7900 XT/XTX) can run FSR 4 in some games via Proton, even though AMD doesn’t officially enable it. - Global override toggles in AMD’s 25.9.1 driver can bypass the FSR 4 whitelist, allowing it to run in FSR 3.1-compatible games.  I hope they succed it will be a slap in the face.",Neutral
Intel,This has been announced for months.,Neutral
Intel,Yeah AMD refusing to port features to any card released before the 9 series makes supporting them really hard.,Negative
Intel,Say thanks they haven't demoted 7000 series to only game drivers,Neutral
Intel,"Your best case is your RX 7900 turning into Balsamico, whatever that means.",Positive
Intel,"Its because RDNA 4 added hardware that 3 and 2 don't have. Now before I get kicked to death by angry people, there is a version of FSR Redstone that uses and INT8 path that is compatible and will work on 2 and 3, however that has not been launched today and AMD have not confirmed it will be.   That isn't to say they won't do it, but right now it's not been announced. Perhaps there will be enough noise to get AMD to change their mind or it might be that they want to get it out on their latest cards first before complicating matters with older RDNA support.  Only time will tell",Neutral
Intel,"Bro the AI accelerators completely got revamped, upscaling technique isn't usually the indicator for 'fine wine', it is when non-upscaling raw performance numbers improve.",Neutral
Intel,Same boat here. Tired of trying.,Neutral
Intel,Thanks for testing. Have you perhaps tested Oblivion Remastered?,Positive
Intel,Finally fixed! It's a christmas miracle!!!,Positive
Intel,I regret getting this 7800xt,Neutral
Intel,Any card released prior to the 9 series.  Amd could give 2 shits as they chase the AI bubble (jokes on them if I was an exec I'd double down on the consumer market to insulate from the impending bubble burst),Negative
Intel,sadly,Neutral
Intel,"Yep, I go back between 23.9.1 and 25.9.2. I couldn't be happier.",Neutral
Intel,"If it's any consolation, I was on an NVidia card for 2+ years where I wasn't getting the DLSS updates. Then they actively removed features when they went to the NVIDIA app.  Looking at AMD's roadmap, RDNA4 looks like a stopgap anyway until RDNA5 (prob will be called UDNA?) comes out. So in another year and a half I'll be in the same situation with my 9060XT.",Negative
Intel,"Use OBS, replay buffer",Neutral
Intel,Was just thinking of giving a shot for Indiana Jones and the Great Circle - I guess not anymore since FSR4 doesn't work with it..,Negative
Intel,"Microsoft had bugs also causing hanging crashes. Everyone loves to blame GPU drivers immediately, but check this out:  https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",Negative
Intel,I also want to know this.,Neutral
Intel,"I'm wondering the same thing, 25.11.1 is still the most stable for me!",Negative
Intel,Wondering too. I bumped back down from 25.11.1 because it was unstable on my machine.,Negative
Intel,Stay on 25.11.1 if you are on RDNA 1 or 2,Neutral
Intel,squash hard-to-find sharp reach memorize fade husky divide subsequent plough   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"~~It messed up my audio, now everything sounds 8-bit. If you're on RDNA4, avoid this update.~~  EDIT: it's not the drivers, after much tinkering I was about to deduce that it was my monitor. So it should be ok to update",Negative
Intel,I can't use anything above 25.9.1 on my 9070 XT,Negative
Intel,forget it they gave u the middle finger move on fuck both amd and nvidia,Negative
Intel,won’t happen,Neutral
Intel,"Fine wine is only a thing for very few and specifics types of wine, typical wine still goes bad over time.",Negative
Intel,What is the source for this or is it trust me bro?,Neutral
Intel,They should just remove this feature. It never worked from day 1..,Negative
Intel,what is the difference,Neutral
Intel,Latest windows update yesterday fixes that.,Neutral
Intel,"Can you tell me if this is also applicable to 25.12.1? There are several (frustratingly unlisted) VR-specific fixes aligned, one of them closely relates to what you've just described",Neutral
Intel,Same here. 25.9.1 makes my problems go away,Negative
Intel,"That was a terrible driver for me also. New one has been night and day improvement, give it a shot.",Negative
Intel,Same for my 9070 XT. Device hung error,Neutral
Intel,"Thanks for reporting, had that once with 25.11.1 + 9070XT (W10) before reverting to 25.9.1 (since then, it never reappeared).",Neutral
Intel,i think your sorry should extend to people with rdna4 cards because this is pretty underwhelming,Negative
Intel,Do you get a firmware update pop up? Is this one?  https://i.redd.it/w46j86mnct6g1.gif,Neutral
Intel,"I'm familiar with this impacting United Offensive, I don't believe we're reintroducing this old vendor specific extension, however. I do have a ticket for the performance issues though; I don't believe this is related to the missing extension.",Neutral
Intel,"Tested for 2 days(1day and 22hrs uptime)  No crash, No BSOD for me so far. Nothing strange.  MS Edge, Google Chrome video playback, youtube...etc all play nice while gaming on main monitor.  Diablo 4, MSFS 2024, Doom dark age, Forever winter(UE5), Witchfire(UE4)...etc All run fine.  Lossless scaling runs fine on spicy vids to all of the above games xD  HWinfo64 and MSI Afterburner, RTSS all run as they should.  (Win11 25H2 uptodate, X670E, igpu(98x3d)+7900xtx+6400 3gpus, 2 monitors, hybrid mode)  Edit) rx 6800 + r7 7700x on win11 25H2, X670E, Single monitor, igpu-disabled -> runs fine.  rx 6700xt + i7 8700k on win11 25H2, Z370, Single monitor, igpu-disabled -> seems good.",Positive
Intel,"25.11.1 had pink artifacts glitch on chromium browsers with 7700 xt but i installed 25.12.1 yesterday and no issue so far, i did not see artifact pink glitches or sound issue so far ?",Neutral
Intel,My 9070 XT hate every driver above 25.9.1,Negative
Intel,I updated to this driver and immediately got a BSOD. Rolled back to October 25.10.2 again,Negative
Intel,offer steep theory scale straight obtainable physical ad hoc selective test   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,thats what I wonder too! Is it more stable??,Neutral
Intel,haha r u fr,Neutral
Intel,la même. C'est scandaleux,Neutral
Intel,"Means that they've created separate driver packages tailored for the specific gens (A rdna1/2, B for RDNA3/4, C - combined fat package that contains both drivers for systems that might have both gens on the same machine (igpu + dgpu) )",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Cleanup utility first, always!",Neutral
Intel,I also wanna know best way updating drivers. DDU kinda annoying but maybe must be done i don't know,Negative
Intel,It's doing it there too.,Neutral
Intel,Never seen any crashes on it with latest driver prior to today 9070xt w11,Negative
Intel,"Just tested it tonight, and for me it's working fine, 9060xt here, windows 11 with the latest update, although i play with the ""medium"" preset which disables ""lumen"", can't say it might work for you but you can try it if it still crashes constantly",Neutral
Intel,Might want to check [https://www.reddit.com/r/radeon/comments/1pjeonb/fyi\_fsr\_ml\_framegen\_requires\_windows\_11/](https://www.reddit.com/r/radeon/comments/1pjeonb/fyi_fsr_ml_framegen_requires_windows_11/) :|,Neutral
Intel,Everything after 25.9.1 is a mess for my 9070 XT,Negative
Intel,nope. I still crash,Negative
Intel,"I fixed my arc raider crashes (mostly in blue gate map load) by running DDU, installing 25.10.1 (down from 25.11.1), and deleting shader caches (dont know if the shader cache delete helped or not). I upgraded to the newest drivers the day after they released and haven't had a single crash since in arc raiders, including w overlay.",Neutral
Intel,"for me, DDU in safe mode, disconnect internet, install 25.9.1 fine for me(9060XT).  I've tried 25.10/ 25.11 and revert back to 25.9.1 with this way. Now observing 25.12",Neutral
Intel,Yes. I downgraded from 25.11.1 because of the crashing. Now been on 25.12.1 all week and havent had any issues come up. You also get proper fsr4 upscaling now.,Neutral
Intel,"I just want to say I think I found the culprit. It also happens on the winupdate one too, because it started crashing all the time.  Core clock boosts itself WAY past what it is declared on the card(I got a Sapphire 9070XT Nitro, supposed to be 3060MHz). Here's the moment before it crashes to a black screen:  [afterburner screenshot](https://i.ibb.co/3YpJFtzM/Screenshot-2025-12-21-030537.png)  The dip in clocks is the moment it crashes. As you can see, it is running well above boost clocks. Hence, freezing in a few minutes, proceeded by a black screen, and a crash. The ups and downs are from me alt tabbing in the graphs, by  the way.   This is with core clock -200mhz applied in Afterburner and no crashes, boosts to just above declared boost clocks. Here the dips in up and down on power are probably me toying around how much exactly -mhz is needed.  [afterburner -200mhz](https://i.ibb.co/YTQfGJtc/11111.png)  All of the crashing behavior so far is replicable in COD, CS2, Cronos New Dawn.  u/AMD_Vik",Negative
Intel,I still have my 5670,Neutral
Intel,"No problems with RX 9070 xt in ARC raiders, i have win10",Positive
Intel,It’s for Darktide apparently,Neutral
Intel,any game with fsr 3.1 fg also has the new fg since drivers override it. it’s also why they stopped versioning fsr. any game with fsr 3.1 should just automatically have any new version of fsr when the drivers update,Neutral
Intel,"It's for Darktide. But it's not even ready for launch there, either.",Negative
Intel,I forgoed any amd software entirely  Use more clock tool  10x better with 0% of the bloat   ^^ helped me get my 4th in world furmark score (7900xtx user),Positive
Intel,If you want to be in control of what’s on your computer then Windows is not the OS for you,Neutral
Intel,"Dumbest take one can have, since installing only the driver won't let you manage the settings at all.  Which has nothing to do with this useless launcher no one wants or needs.",Negative
Intel,Found the install manager dev lol,Neutral
Intel,"It also removes the *Check for Updates* button in Adrenalin, you have to manually remove the Install Manager to get it back.",Neutral
Intel,Thanks.,Positive
Intel,Unfortunately Redstone FG is bugged with poor frame pacing,Negative
Intel,Nice to see the innovation continuing on,Positive
Intel,But only on the 9060 and 9070 right?,Neutral
Intel,Yeah same,Neutral
Intel,"Remember when you could click ""Check for Update"" inside the AMD Software and if there was an update, it would download and install it for you?  Glad they fixed that awful experience, and we have the Installation Manager now.",Neutral
Intel,I remember this mentioned since the  GCN 1.0 days. Lol,Neutral
Intel,"On my end, the driver crashes. Most of the time it manages to recover (sometimes it will crash a few more times before stabilising). Sometimes it doesn't recover (leaving only 1 of my monitors working), so I had to reboot. Then after rebooting, strong chance it'll crash again the moment I open my browser.",Negative
Intel,"\+1 on this. Most games crashed drivers with any newer drivers except 25.9.1, but poe2 i cant play with vulkan or Directx 12 only with Dx11",Negative
Intel,"My experience with switching to amd was so smooth and perfect until 25.9.1. Everything after that just caused stutter issues in games, programs randomly crashing, drivers crashing completely causing my pc to reboot, this is so sad i hope they fix this soon and bring back a stable version asap. Rolling back to 25.9.1 now aswell until that happens.",Negative
Intel,přesně zustávám na 25.9.1 všechno jiné crash,Negative
Intel,Are you able to tell us what the error code is on the BSOD? I don't suppose you have a kernel memory dmp pertaining to one of these failures over at      C:\Windows\MEMORY.DMP,Negative
Intel,Thanks will give it a try after I finish work,Positive
Intel,"Wait, AMD Customer Support told me that 2 monitors connected to iGPU and dGPU has never been officially supported and that this configurations breaks performance… so they told me bullshit?",Negative
Intel,Any update on three Oblivion Remastered and Silent Hill  2 Remake crashes? A lot of us are still with the September drivers because of them.,Negative
Intel,<--- Ditto,Neutral
Intel,Optiscaler lets you inject it. Do not use in multiplayer games though.,Neutral
Intel,it cannot possibly be this difficult to fix when there’s already community workarounds,Negative
Intel,both are still broken somehow,Neutral
Intel,both have been broken since 25.10.1. enhanced sync just makes your display run at an extremely low framerate when freesync is on and then noise suppression just doesn't even turn on. I don't understand how they haven't fixed either of these yet. they haven't even acknowledged it,Negative
Intel,"I have to do the same. My monitor is  240Hz and the TV  120Hz and I have to use Chill, which sometimes will cause stuttering, because enhanced sync always causes stuttering.   Man I'm starting to miss the NVidia setting of just putting vsync on in the driver and everything just working.",Negative
Intel,I did some testing AND as far as I can tell I do think it's actually fixed finally,Positive
Intel,I would continue buying their GPUs if they gave me something to buy.  The XTX has no upgrade path on RDNA4.,Neutral
Intel,"I had Nvidia for years, the main reason I switched was that the drivers went to shit last year. I'm just sick of them in general, too. The 7800 XT I bought has been one of the most trouble free cards I ever had, aside from Adrenalin randomly closing in certain versions.",Negative
Intel,"If I could get my hands on a 5070 Ti I’d happily switch. AMD likes to take advantage of the underdog, for-the-people image whenever it’s convenient but they’ll just as quickly throw us under the bus and fuck us raw once they’ve got the bag.  Is Nvidia a gang of greedy fucks? Sure. But at least the bullshit’s right out front where you can get a good strong whiff of it. You know what you’re in for.",Negative
Intel,"I purchased a 7700 XT and a 7600 8gb I'm March and while I'm satisfied with performance, it would definitely be awesome to have FSR 4 on both cards as FSR 3 and 2.2 (overwatch )leave alot to be desired",Positive
Intel,It's been so long bro :( Hopefully the fix comes with ray regeneration support?,Negative
Intel,"Hey Vik, is there any info for FSR4 Vulkan support?  It's quite sad to see that there still isn't support for it as it has been 9 months by now since the release of the 90 series  Also is there any info about the EAC issue with Star Citizen and the latest drivers?",Negative
Intel,"Amd Noise Supression doesn't work, when I try to turn it on, nothing happens, but in 25.9.1 it works",Negative
Intel,"Hey amd\_vik is amd Aware of the 1 year on going Darktide issues with amd  ( GPU , and specially X3D cpus ? ), and that even the Dev of Darktide ( Fatshark ) seemingly gets ghosted by amd ?  heres some more info specially first links includes a few Dev comments  [https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462](https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462)  [https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f](https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f)  [https://forums.fatsharkgames.com/c/darktide/performance-feedback/97](https://forums.fatsharkgames.com/c/darktide/performance-feedback/97)",Neutral
Intel,"Vik, weren't you on holiday leave? xd",Neutral
Intel,Any fixes for the SecondLife issues we've had the last few months? last driver that didn't break textures was 25.9.1,Negative
Intel,Will this update fix some of the artifacting I’m seeing in cyberpunk with fsr enabled?,Negative
Intel,Also getting driver timeouts in Cyberpunk with RDNA3 with raster or RT. I did not have these problems with my RDNA2 card.,Negative
Intel,"The AMD FSR ML-based Frame Generation option in the Radeon panel disappears in Windows 10.  So I have a question: Is ML-based Frame Generation no longer usable in Windows 10? This option is available in Windows 11, but not in Windows 10.",Neutral
Intel,Can I join if mine's just an XT?,Neutral
Intel,What GPU are you using?,Neutral
Intel,Try reinstalling Windows. That fixed it for me.,Neutral
Intel,"This doesn't work. We are talking about games that crash with or without it, the only difference being the older AMD driver working.",Negative
Intel,I already install the latest update before update drivers its not update related. Vulkan driver is the problem in indina jones and silent hill 2 after windows update 25.11.1 not crashing ray tracing enabled but in 25.12.1 its broken again. So driver is the problem...,Negative
Intel,"You need to compile VKD3D with specific flags to enable this, it's not enabled by default. Based on the changelog, it appears that AMD has blocked support in the standard build.",Negative
Intel,"They said earlier in 2025 they were working on FSR 4 support for RDNA 3, and then it leaked in September with the INT8 version...",Neutral
Intel,"They might as well have lol, they aint getting no new features",Neutral
Intel,They also promised features to the few of us who bought 7900 XTX. Good luck defending them when it's your turn to be disappointed.,Negative
Intel,I expected them not to abandon their king card lmfao. Who does that,Negative
Intel,"Not really, they teased the possibility of including other architectures.",Neutral
Intel,Maybe next time you should read the whole thread before replying.,Negative
Intel,"It's also related to getting new features in generations other than just the latest one, ""bro"".",Neutral
Intel,"I have the 7800 xt hellhound i F love it, tbh i care less about this redstone thing but its frustrating why a 2 year old lineup is abandoned all of a sudden",Negative
Intel,"> I'd double down on the consumer market to insulate from the impending bubble burst  If that bubble bursts nobody is going to have much money to spare for consumer goods. That bubble bursting will tank the entire economy along with it.  *Long* term that might work out better, though.",Negative
Intel,further reminder amd is not your friend sadly,Negative
Intel,Same for me but Doom Eternal. I play at 4k and it needs upscaling at that res.,Negative
Intel,What if I'm on RDNA 4?,Neutral
Intel,Yeah there are no good choices,Positive
Intel,https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html,Neutral
Intel,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html),Neutral
Intel,Adrenalin is for GPUs.   Chipset is for CPU & mobo.,Neutral
Intel,Awesome! I will try that 🤞,Positive
Intel,"No, it's not because it's a driver issue. AMD needs to act up",Negative
Intel,"Yeah im sorry for all of us, already shopping for a 5080 rn…",Negative
Intel,Yes that’s the one. I have no idea where to turn lol,Neutral
Intel,Sad news. Nvidia still supporting old extensions.,Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Negative
Intel,oh wow i haven't had any issues yet but that doesnt mean much. 25.11.1 i didnt have issues for a week or so.,Negative
Intel,"Interesting, I’ll test it today. I was crashing non stop on 25.11.1 so hopefully this update fixes it",Positive
Intel,"cod crashes saying driver doesn't meet requirements, so i upgrade to the ""optimised for BO7"", crashcrash crashcrashcrash, roll back one by one with the same results get to 25.8.1 and stable again, crash once every 2/3/4 weeks before this one it was 24.12.1 crazy, a dozen updates every year and only one or maybe 2 are stable enough to enjoy shooting zombies for any length of time",Negative
Intel,Same,Neutral
Intel,"I have the same model GPU inconsequentially boosting well above the advertised clocks (nearly 3.4GHz) in both windows 10, 11 and fedora 43 with no issues.  This has been discussed several times on this community; whilst the clock behavior may surface other issues or instabilities on the system, it's not in itself the cause of problems.",Positive
Intel,6970 here,Neutral
Intel,Literally the one game I don't play lol,Neutral
Intel,"Yay, I own that one",Positive
Intel,"It's not even out for Darktide yet either. Fatshark clarified that it's experimental and needs more work, so it's not in the live build",Negative
Intel,So it's under the umbrella of the fsr4 override if I understood this correctly. For the fsr2 and 3.0 games I can use optiscaler right? Sorry I just bought a 9070xt coming from nvidia so I need to get used to these things.,Neutral
Intel,I used to do that but a few games can use the FSR4 in driver upgrade.  The enhanced sync was nice too when it worked.,Positive
Intel,"Unfortunately I play games and run software that require Windows so I have it on a separate drive. When I do switch to it (and I update the driver to take advantage of new features), this shit typically happens along with a slew of forced updates.  You are right though, I do primarily run CachyOS.",Negative
Intel,found the linux user,Neutral
Intel,You're talking nonsense.  Engineer managing 2k endpoints and several hundred servers.,Neutral
Intel,That explains why it fucking disappeared,Negative
Intel,Wasn't the dude's claim it has been always bugged with AMD,Neutral
Intel,🌍👨‍🚀🔫👨‍🚀,Neutral
Intel,It's barely an improvement.,Neutral
Intel,It's branding,Neutral
Intel,"Yes, RDNA4 refers to the RX9000 series.",Neutral
Intel,"Uninstalling the install manager brings back the ""check for updates"" functionality until you update again (and have to re-uninstall the install manager)",Neutral
Intel,I have one of these captures if you want it (error code 0x00000119). I've been having a TON of driver timeouts and BSOD for the past couple of driver versions and I've had to roll back to October to resolve them. Seems like any app that has hardware acceleration enabled causes it and exasperated when viewing the system via RDP.,Negative
Intel,Let us know how it goes!,Neutral
Intel,"I don't know how much of an impact this could have on perf since it's not something I've measured. I personally wouldn't do this, though. With a dGPU installed I keep iGFX off.",Negative
Intel,"performance wise it should only be a couple frames of latency, when doing rendering on dgpu and going out through igpu it'll just copy over the frame buffers.   Main impact is on pcie bandwidth as it'll use up quite a lot there, and to a smaller degree RAM load, so you definitely don't want to run some other dynamic load on the igpu when gaming to overwhelm its pcie link. I think on 7000/9000 it's x8 so it may be fine? But I'm really not sure could be x4 too",Neutral
Intel,"We're tracking a failure in silent hill 2 remake, I believe a fix is aligned to a future release. I'll need to check in with oblivion remastered",Neutral
Intel,"Do you have to do that convoluted setup and download the drivers from Limewire, or has Optiscaler wrapped it in to their application?",Neutral
Intel,"So, no official release... ;(",Neutral
Intel,Any tutorial for a noob on RDNA2?,Neutral
Intel,what workaround?,Neutral
Intel,"Same issue with fsr4 on rdna1-3.   It shouldn't be this difficult, it's in a perfectly working state made possible by like one guy's few days worth of work.   And yet AMD just doesn't do it...",Negative
Intel,FUG,Neutral
Intel,Enhanced sync makes games super stuttery even in  25.9.2.,Negative
Intel,So I tried out enhanced sync and the game that I first noticed the issue on is no longer an issue. I've checked like 5 other games and no issues so far. Maybe it's fixed?,Positive
Intel,I have my 5070ti build hooked up to my 240hz Ultrawide Oled because of Multi-Frame Generation. I was hoping that would be part of Redstone but it's not.,Neutral
Intel,"Such a relief, but i am also annoyed because they are ignoring 7000 series... I can literally use FSR 4.0.2 on my 7700XT and it is WAY better than FSR 3.1....",Negative
Intel,I hope it is fixed for me as well 😭🙏. Thanks for the info.,Positive
Intel,yep would have upgraded but with an XTX.... you can cut your vram in 2/3 and have less Raster performance for a good upscaler and better RT performance it's such a stupid fucking problem....,Negative
Intel,"That's not something I'm privy to, but it could be worth reaching out to them to request looking into if they're not already.",Neutral
Intel,"I'm not privy to any of the FSR stuff - that's a different team to mine. I can pass on the feedback.  The Star Citizen EAC issue should be addressed, please let me know how it is.",Neutral
Intel,i still am!   so many fixed issues out of the release notes that I felt the need to stick around and help clear things up in the communities I frequent. I'll go back into hiding again soon,Positive
Intel,"I've seen something like this over at OCUK Forums but weren't given enough data to work with. We've attempted to reproduce a corruption issue but apparently we've not been successful.  Can you give me a step by step breakdown on how to hit this, as well as a clear depiction of the issue?",Negative
Intel,"No, XT peasants needs to form their own group.",Negative
Intel,6800XT.,Neutral
Intel,Some of their marketing said they would like to get it working if possible.,Neutral
Intel,"There are already third party options, but it would be nice to see if Steam Machine drives INT8 FS4 support since it runs on RDNA 3 tech. Let's see what happens in 2026.",Positive
Intel,Yeah there are going to be serious consequences as major retirement funds have invested in all these AI stocks because they have made so much money.,Neutral
Intel,"Give it a try, for my 6800xt it's crashing in almost all games...  ![gif](giphy|QMHoU66sBXqqLqYvGO)",Neutral
Intel,Where does that say rdna 3 is in maintenance mode?,Neutral
Intel,"No, it didn’t 😣",Negative
Intel,"Sorry, out of curiosity, if you close it, it won't let you play? What do you get? Could you send me a photo so I can understand?",Neutral
Intel,"I agree. Please can you raise a ticket requesting support for this over at our GPUOpen and ask other end users and developers to upvote it and leave a comment registering their interest? (please share a link to it here if you do) https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues  As far as I'm aware, the impacted titles are: IL-2 Sturmovik: 1946, Neverwinter Nights Diamond Edition and Call of Duty. If there are any others, I would really appreciate you letting us know.  E: I believe it's posted here: https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues/80",Neutral
Intel,"Just an update - I ended up running DDU and re-installing the latest update and now things are pretty stable, no driver timeouts from hardware accelerated apps either. Could be something to do with the architecture change between driver packages - but doing a complete removal between updates seems required now.",Neutral
Intel,I never seen 1 crash on 25.11.1 although I did use the preview update for windows 11 last week which fixed some amd gpu related crashing and that solved my arc raiders random crashing,Neutral
Intel,"My apologies then - it seems latest driver on Windows seems to be the source of issues then, seems more people have issues posting on /r/AMDHelp , also with 9070XT's. Seems all device hung errors and timeouts recently posted are with 25.12.1. I had no issues on cachyOS (Hyprland) running CS2 too, latest amdgpu.",Negative
Intel,"Also it's considered pretty poor Raytracing. People are usually snobbs who go ""I can't tell the difference"" when it comes to raytracing as a coping mechanism but darktide is the one game I can see why, it's not very noticeable and not worth it.",Negative
Intel,"yes, 3.1 is where AMD adopted the same modular approach as nvidia so any game at fsr 3.1 or above just runs at whatever latest fsr version your driver supports, which is currently 4 although now the versions aren't numbered anymore",Neutral
Intel,Hell yeah 👍🏻   Impressive you can run that on a 5x86,Negative
Intel,"Since you're already an advanced user, perhaps you could block it from installing by selectively blocking AMD in your hosts or pi-hole? It's not a dumb solution, but it's better than having to deal with push-installs.",Neutral
Intel,I might be an ass but I’m not wrong,Negative
Intel,Sorry  If you’re a **consumer** and want to be in control of what’s on your computer then Windows is not the OS for you,Negative
Intel,"Yes, If you mean the bad frame pacing when fps is lower.  I still opt to spent 1-200 hrs of my gaming session with FSR 3 frame gen, 7900xtx.  It's not that bad when the output is close enough to monitor max hz, similar to what hardware unboxed did in thier test.  The generated frame still comes out too early but it has to wait for the monitor's nest refresh which is consistent.",Neutral
Intel,ty,Neutral
Intel,"u/amd_vik it sounds like this person doesnt want the manager to install again, but I am pretty sure you can do custom option to uncheck it. If you do express of course it will put it back sschuler.",Negative
Intel,can you run analyze -v in windbg or fire it over to me via your preferred file sharing method?  I personally like to use https://send.vis.ee,Neutral
Intel,Can confirm this issue is fixed for me on 9800x3d + 9070xt (I had this issue on 25.11.1 and reverted to 25.10.2 until today) 👍,Negative
Intel,"Seems to be working fine, though when I was installing the driver my igpu showed up separately from the dgpu in the installer with a download link. But when re-running it they both show under 25.12.1  Should I be installing some separate older driver for it to keep things like hw accel working or was that just some hiccup?",Neutral
Intel,Oh great will also test after work it’s been headache since last driver update,Positive
Intel,Thank you for taking the time to respond. This has been very frustrating.,Positive
Intel,"I'm sorry to comment directly to you here. Do you have any report about monster hunter wilds performance drops in recent GPU drivers ?    I'm using 9070xt.    I have to use version 25.3.1 to play wilds with no stutters, anything newer gives a lot of stutters in many places.",Negative
Intel,"Yeah you still have to download it on your own, the creator of Optiscaler already said they aren't going to bundle it probably due to the whole legality around it.",Neutral
Intel,"i saw a post that detailed how to essentially replace noise suppresion with the working version in newer drivers, you can probably find it here somewhere",Neutral
Intel,worked fine for me idk,Positive
Intel,still busted for me,Negative
Intel,"Also from what I can tell, enhanced sync is fixed at least on my end.",Neutral
Intel,Thank you for this! been waiting for a fix with Star citizen.,Positive
Intel,Yeah SC seems to be working for now.,Neutral
Intel,"Bonjour, pour le moment sur Star citizen le problème avec EAC fonctionne pour la 7900XT. Merci d avoir réglé le problème. Bonne fêtes de fin d'année.",Neutral
Intel,That's good to hear. What about Noise Suppression not working since 25.9.2?,Negative
Intel,Hmm let me try. So pretty much having installed the latest driver (25.12.1) I just open SecondLife. I look closely at my avatar/character and my skin looks like this  [https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4](https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4) (excuse my outfit but just easier to show)  this is how it's supposed to look and also does on 25.9.1 [https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4](https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4)  I've heard that this doesn't occur on linux but only windows (But I don't have linux so can't say for sure)  I think you need PBR / Materials or some reflection on your skin to see the issue.   If you fly up to around 2000+ meters above ground it becomes easier to see  These are my settings [https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png](https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png)  I have an rx 7900XTX,Neutral
Intel,"Hello! I am actually one of the developers on the client team for Second Life, and I have been trying to figure out how to get in touch - we have found at least one nasty bug on some of the Strix Halo chips with the current drivers.  Can you send me a message here so we can exchange emails?",Negative
Intel,😭😭😭😭,Neutral
Intel,"I had a similar issue with my 6800xt and the other thing that helped was to sit it to fullscreen or borderless and swap back and forth. Now I'm only playing in fullscreen (which is annoying), but it doesnt crash anymore.",Negative
Intel,I have the same card and exactly the same problem. Can't install newer drivers or BF6 just constantly crashes.  I'm on 25.10.2 tho,Negative
Intel,"And it is, and they did, we have the leaked int8 version from September... Just needs official driver implementation now.",Neutral
Intel,Ive had zero crashes for 2 weeks on arc due to the preview update which rolled out to the public this week. You could try underclocking your gpu if it boosts over the limit.,Positive
Intel,"Before the Black Ops 7 (which I don’t own) integration to Warzone, I could click off it & carry on. But since the integration it just closes the game.",Neutral
Intel,"Yes, i have created this github issue.",Neutral
Intel,"If those failures are avoided by clock limiting the board, the problem area could be a different domain entirely (CPU, memory, power, etc.).  The linux remark is interesting, it kind of calls back to similar failures with NV31 in certain apps like Helldivers 2; we had a little internal discussuon about how the amdgpu kernel driver managed to mostly avoid such issues, though I dont recall the outcome.  If you get the opportunity, I'd recommend a suite of system integrity routines as a sanity check; please take a look at [one of my older posts](https://old.reddit.com/r/Amd/comments/1l9ox9r/amd_software_adrenalin_edition_2562_optional/nn3yuay/) for some background.",Neutral
Intel,"Depends on where you look at it. The Morningstar has a bunch of pre baked lighting effects, which make the difference completely unnoticeable unless you study the frames  In mission it makes a pretty significant difference, but wether or not that's something anyone cares about in a fast paced game is another thing entirely.",Neutral
Intel,"They aren't numbered in the sense of like 4.0.2 or like there won't be an ""fsr 5""? Thank you very much btw, very helpful info!",Positive
Intel,Like a charm. :D,Neutral
Intel,"I probably could, but AMD (and any other company, really) should be following the users preference anyways. It is a band aid fix and doesn't solve the problem.  Not a bad idea though.",Negative
Intel,I've been using computers since dos 3.  You're a spanner.  I'm sure MacOS is soooooo much more open.,Neutral
Intel,"Thank you for the idea, I just tried a custom install during an update, was given 2 choices (update/dont update driver and install/dont install privacy view). After installing drivers, step 2/2 was installing the install manager.    After updating through adrenaline using the custom option, I attempted reinstalling again using the auto-detect, custom install. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.   Installing via the WHQL package, custom install follows the same steps as above. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.",Neutral
Intel,Here you go: [https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw](https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw)  I did run in windbg but I have no idea how to save the output unless you just want a copy + paste of it here haha,Neutral
Intel,Appreciate the feedback,Neutral
Intel,Thank you for confirming.  That interesting though. I think the most seamless way to support products from both branches is to use the AMD auto detect tool. Can you tell me how the iGPU is represented in Windows' Device Manager?,Positive
Intel,"[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html)  https://i.redd.it/3vxsa8yave6g1.gif  If you suspect the installation is incorrect, download the package that includes the IGPU driver using the link provided above. The basic version does not include the IGPU driver, but provides a separate download option during installation.  Anyway, it seems like a lot of bugs have been fixed in this version.",Neutral
Intel,"if you can find it, you will be the goat",Positive
Intel,"Yeah everything was fine until I tried to play Resident Evil 2 Remake, then the issue was back but not in the other games. This is a very odd issue.",Negative
Intel,Nvm the issue I was having just happened in Resident Evil 2 Remake.,Neutral
Intel,Thank you for letting us know 👍,Positive
Intel,appreciate the info. I'll ask our technicians to check in with the settings you've provided,Neutral
Intel,I can confirm there is no issue in linux. A windows version running under proton in linux has no issues as well.   In the video there is flickering on head and body. I see only flickering on the head (when running it on the windows pc)  But my body has no layers attached - the body in the video usually comes with layers. But all heads have multiple transparent layers. The problem occurs even when that layers are not in use and are fully transparent.   Probably related.,Neutral
Intel,"Hey there, thank you for reaching out!  I don't suppose it would be possible for one of our devrel folks to contact you via a linden lab email address like business@lindenlab.com?",Neutral
Intel,"But he's not talking about ARC. Oblivion Remastered, Silent Hill 2 Remake and Indiana Jones still crash with the update and the latest driver. No crashes (at least with low rear tracing), under  25.9.2.",Neutral
Intel,"So if you click dismiss, the game closes, did I understand correctly? It doesn't let you enter the COD HQ ? I'm telling you this because I too should update the bios, in fact it happens to me too, but I click dismiss and it lets me play anyway.",Negative
Intel,"for CS2, it was the newest driver that caused crashes exclusively, but on that driver I also got stronger boosts off the bat, hence it crashed faster. Now on 25.10.1(from windows update), COD still crashes with a black screen then tab to desktop with a driver timeout detected. Looking at afterburner(just using it to monitor clocks, no OC/UV applied or anything) the moment the GPU touches 3300+ I get thrown to the desktop. Can't even finish the training course even with ""speedrun strats"" before it crashes. It boosts [momentarily to 3300+](https://i.ibb.co/bgLFC0dp/coreclockcrash.png) and I get a screen freeze, crash, and sent to desktop with a driver timeout.   [These](https://send.vis.ee/download/103635cf66bdb907/#t2lRq409eeNwv6AaafhKJA) are both my crash report submissions. I'd go tomorrow over the stress tests, but I have managed to complete Time Spy/Steel Nomad without issues. And like I said, my system has has 0 issues before on a 2080ti.",Negative
Intel,"FYI, I passed [everything.](https://imgur.com/a/WyB9FeE)  This leaves the driver only. I made sure windows update didn't download its own driver this time, installed 25.12.1, still getting driver timeouts and crashes in games. I don't know what to tell you. Memtest86 also passed without any issues.",Negative
Intel,"I just tried it myself and no I still stand by my statement that there's barely much of a change, I guess it's just subjective on this game. Agree to disagree.     I also still stand by the fact that I don't think enough will use RT to warrant it updating to use caching.",Neutral
Intel,"there won't be an ""fsr 5"" because any game with fsr implemented from here on out should, in theory, be compatible with every future version of fsr made, so numbering them isn't as meaningful. they're probably just going to stick with unofficial codenames like redstone for diffrentiation. Nvidia still uses versioning for DLSS despite it using the same system because it's good for marketing and diffrentiation so I'm not sure that dumping the version numbers is a wise decision but it also makes sense",Neutral
Intel,"I agree with you wholeheartedly, but super users do what they do best - sudo that shit. x)",Positive
Intel,"I've never had AMD Chat or Privacy View force install, I hate they show up in the available software to install when updating, but I just dont click to install them lol, just update the gpu/chipset drivers",Negative
Intel,"…Have you no idea what Microsoft has been doing to Windows in the last decade? I use Linux on all of my Desktops and my home server (total control of what software you install), and Apple for my laptop/phone. Say what you want about Apple but they respect privacy and don’t nag you every two seconds about some AI nonsense or OneDrive.",Neutral
Intel,I guess a snippet of the faulting component from the output would work.  This is a minidump. Do you have a kernel memory dump>?,Neutral
Intel,"sorry i missed this, seems it had expired. maybe someone downloaded it before i did?",Negative
Intel,"Right now in devmgr with re-running the driver installer from the site things look like this https://u.numerlor.me/2faMBA . I also remembered adrenalin has full driver details and everything looks fine there https://u.numerlor.me/w1Snxw https://u.numerlor.me/EOclpA so I think it was just the installer being a bit confused.  Compared to the installer on the first screenshot, when doing the actual update (from inside adrenalin) the Radeon Graphics was a separate item, and had a ""Download driver"" or something along those with the link I mentioned",Neutral
Intel,What about the combined exe? It's still available? That will install both gen but was bugged with control panel disappearing on previous driver.  The combined exe is around 1.6GB.,Neutral
Intel,This might be it? Worth a shot I suppose.  Edit: This worked for me on the latest driver  [https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),Neutral
Intel,geenz@ but yes,Neutral
Intel,any news? SL are not updating their customers with anything constructive and it is affecting most of us.,Negative
Intel,"Hmm, when I can, I’ll have another look! Thanks!",Positive
Intel,"I see. Is this specific to CS2 or does it occur with other apps on your end?  We're presently tracking and working on TDRs in that game specifically, though I'm kind of worried in a way that clock limiting works around this failure.",Negative
Intel,"That's fair. I think the lighting is the only thing that particularly makes a difference. Makes the lighting a lot more realistic, but the reflections are pretty worthless.  But like I said, it's also not necessarily something you'd really notice while playing, since it's not exactly a stop n look around type of game.  >I also still stand by the fact that I don't think enough will use RT to warrant it updating to use caching.  I don't disagree at all. Especially when optimization for AMD hardware in that game has been a substantial criticism since it's release, to the point that ray tracing was compl disabled on AMD cards for a while",Neutral
Intel,"I do not, only the minidump but I've uploaded it again here [https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN\_IxLREw](https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN_IxLREw)",Neutral
Intel,"Yes it should be fixed under that scenario, and the combined package is linked on the release notes:  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html  https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe  Kind of guessing here but I believe the '-c' towards the end of the file name denotes a combined package spanning RDNA support.",Neutral
Intel,This worked for me btw - did it a few days ago before these drivers dropped. When I update I'll be using the same method.,Neutral
Intel,thanks a bunch. I'll pass this on to my ISV contact and see where we get with that.,Positive
Intel,You can find it here [https://github.com/secondlife/viewer/issues/5048](https://github.com/secondlife/viewer/issues/5048),Neutral
Intel,news ?,Neutral
Intel,"COD is the greatest offender - I can't even get through the training course for Zombies without a black screen>driver timeout message, even if I try to speedrun it in a way (because I've attempted it so many times) it is inevitable it's going to crash, that one crashes with this [error](https://i.ibb.co/KjxynXH5/image.png).  Again, NO OC is applied. Other than the ram running at 2666, which as stated with both mem tests successful and went through both by Karhu's test and Memtest, have no issues. Including no issues with my previous GPU,2080ti, again. CS, I can't even start a match with friends because it'll inevitably crash randomly, sometimes it is within 5-10 mins, sometimes it is near instant in a couple of minutes. Tried everything from 25.12.2 to 25.9.1. PSU is a RM1000e, using the 12pin cable natively from the PSU. It is all the way in, this PSU I specifically even got for this GPU as I didn't want to use an adapter to power the card from all the experiences I've read with the 12pin + adapters.  Here is also a [video](https://www.youtube.com/watch?v=cSkaI6WSfJY) of it happening.",Negative
Intel,"huh, that's odd. Do you have any larger files over at       C:\Windows\LiveKernelReports\WATCHDOG\",Neutral
Intel,"Installed the c one. And seems to be working fine. 780M and 6800 here. Still when selecting a specific GPU for a specific app, both energy saver and performance show 6800. This bug has been forever. And it's probably just a registry key when the driver install. Win11.",Neutral
Intel,I do actually have one in there that's 17MB from a BSOD yesterday caused by the AMD driver,Neutral
Intel,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",Negative
Intel,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",Neutral
Intel,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",Negative
Intel,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,Negative
Intel,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,Neutral
Intel,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too 😿.",Negative
Intel,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,Neutral
Intel,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? I’ve spent 1 entire afternoon try every solutions given by Google but today the problem is still there…,Negative
Intel,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",Neutral
Intel,So does this mean Arc Raiders will stop randomly crashing in Windows?,Neutral
Intel,Just installed these zero issues so far!,Positive
Intel,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",Neutral
Intel,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,Neutral
Intel,There was a long delay with the blank screen. Made me a bit nervous,Negative
Intel,At this point i'm sure that cyberpunk will never be fixed.,Negative
Intel,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,Neutral
Intel,No fix for being unable to enable Noise Suppression...,Negative
Intel,When does Linux get this,Neutral
Intel,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",Negative
Intel,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",Negative
Intel,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",Negative
Intel,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,Positive
Intel,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",Neutral
Intel,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",Negative
Intel,Windows update keeps trying to update my driver.,Negative
Intel,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,Negative
Intel,No FSR4 on RDNA3 no care,Neutral
Intel,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",Negative
Intel,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,Positive
Intel,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,Positive
Intel,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,Negative
Intel,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",Neutral
Intel,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",Negative
Intel,This driver was way better than the version before it(for me at least).,Positive
Intel,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",Negative
Intel,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",Neutral
Intel,For me the driver just times out randomly during normal stuff like youtube shorts. Today I opend steam and the driver timed out. That never happend with 25.10.1.,Neutral
Intel,"The Adrenalin Software instantly closes and restarts if I try to click on the ""Record & Stream"" tab (no crash/error report, it simply closes and then restarts in background).       Dunno if it's from 25.11.1 or not, it was the first time I was going to try it. Didn't tried a DDU full reinstall either, just a simple reinstall of the driver but for no use. Guess I will just use other software for recording so whatever but I'm curious if it's really a driver issue since I got no report pop up at all.  Gpu is a 9060 xt 16 gb.",Neutral
Intel,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",Negative
Intel,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",Negative
Intel,"Brooooo, they didn‘t fix the flickering in BF6 when recording…",Negative
Intel,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.  * Fucking LOL.,Negative
Intel,25.10.2 completely broke vsync... not even a mention about this in the notes?,Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,There is new AFMF features too.,Neutral
Intel,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,Neutral
Intel,bf6 fps drop fixed?,Neutral
Intel,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,Neutral
Intel,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",Negative
Intel,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",Neutral
Intel,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",Negative
Intel,How is the driver ? 7700 XT here.,Neutral
Intel,Finally a potential fix for CPU metrics? Look forward to seeing if it’s true!,Positive
Intel,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,Negative
Intel,do yall use ddu for every driver or do yall just update it with the app?,Neutral
Intel,"New AMD update 👏👏👏👏, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",Positive
Intel,I just can’t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video I’m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,Negative
Intel,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",Negative
Intel,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,Neutral
Intel,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,Negative
Intel,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,Neutral
Intel,Think this broke Vulkan in POE2,Neutral
Intel,"Hi [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/), other players and myself are having problems using DXVK with latest drivers. From driver timeouts to black screen. I'm particularly having problems with Fallout New Vegas, but there is reports in other games. How can I help in fixing these issues? Examples: [https://github.com/doitsujin/dxvk/issues/4999](https://github.com/doitsujin/dxvk/issues/4999), [https://github.com/doitsujin/dxvk/issues/5204](https://github.com/doitsujin/dxvk/issues/5204), [https://github.com/doitsujin/dxvk/issues/4851](https://github.com/doitsujin/dxvk/issues/4851)",Neutral
Intel,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",Neutral
Intel,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,Negative
Intel,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,Positive
Intel,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,Negative
Intel,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,Negative
Intel,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",Negative
Intel,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",Negative
Intel,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,Neutral
Intel,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",Negative
Intel,Did AI create these new drivers?,Neutral
Intel,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,Negative
Intel,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",Negative
Intel,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",Positive
Intel,I'm glad the CPU metrics are showing again,Positive
Intel,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",Negative
Intel,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",Neutral
Intel,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),Negative
Intel,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,Neutral
Intel,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",Negative
Intel,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",Negative
Intel,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 😅   What a fucking joke",Negative
Intel,Shits been crashing my system since the update :( sapphire 7900xt,Negative
Intel,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",Negative
Intel,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,Negative
Intel,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",Negative
Intel,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",Negative
Intel,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",Negative
Intel,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",Negative
Intel,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,Negative
Intel,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",Negative
Intel,У меня Мультимедиа контроллер выдает ошибку. Для этого устройства отсутствуют совместимые драйверы. (Код 28),Neutral
Intel,Noise Suppression still broken. 3rd release without that functionality in a row.,Neutral
Intel,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalación del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",Neutral
Intel,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",Negative
Intel,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",Negative
Intel,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,Negative
Intel,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",Negative
Intel,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",Negative
Intel,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,Negative
Intel,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me 🙏",Neutral
Intel,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",Negative
Intel,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",Negative
Intel,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,Neutral
Intel,Still not working AMD NOISE S,Negative
Intel,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,Neutral
Intel,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",Negative
Intel,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,Negative
Intel,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,Negative
Intel,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",Negative
Intel,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",Negative
Intel,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",Neutral
Intel,"Unfortunately, version 25.11.1 does not start with Windows.",Negative
Intel,Is AMD going to come up with another driver soon?,Neutral
Intel,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",Negative
Intel,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,Negative
Intel,"After installation 25.11.1 (from 25.10.2)  black screens entered the chat. After DDU and rollback to 25.10.2 they stayed, and after rollback 25.9.1 the same... RX 5700 XT. Sadly 😞.",Negative
Intel,"is there 25.11.1 for windows 10? the filename that i downloaded from AMD website is ""whql-amd-software-adrenalin-edition-25.11.1-win11-s"" where usually its filename includes windows 10 along the lines",Neutral
Intel,"they need to fix the BF6 texture corruption glitch, it's annoying af. had to roll back to 10.2",Negative
Intel,Any word on fixing the driver timeouts on the 7900xtx its a bloody joke worst gpu i have ever bought,Negative
Intel,Any of you also have issues with afmf2 and the game not opening adrenalin software or showing performance counter after enabling it?,Negative
Intel,"this shit was fucking with my PC, DDU current drivers and reinstalled 25.10 straight from Gigabyte Program and everything works again",Negative
Intel,getting bsod randomly since 25.9.1 sad..,Negative
Intel,"I started having an issue since the 25.11.1 update with unreal editor where all of my tools menus instantly close, nothing else changed except for this driver update and I've heard of Nvidia having similar issues with driver updates in the past so I think it may be the cause, Going to revert to an older driver and see if it works",Negative
Intel,"I've spent the last few days uninstalling, reinstalling, DDUing, doing everything I could think of to get Adrenaline to start/work. It would show the splash screen and then quit. No way of re-starting it. Couldn't open anything that used Vulkan and got errors. Couldn't install the Windows Store version cause ""driver error"". I eventually used DDU one last time and uninstalled everything AMD and was able to just install the driver through MyASUS. Now I'm able to open all the software again that wasn't starting before. I'll be holding off on installing Adrenaline again anytime soon. Sucks cause I want the features, but I couldn't use the programs anyway. I miss having nvidia.",Negative
Intel,"It seems on the latest Radeon driver that freesync is broken within CS2 when running fullscreen windowed. Freesync works initially when the game starts. But as soon as I alt tab, freesync breaks and I get screen tearing. I rolled back to 25.9.1 and I can confirm it works again as expected. So it seems this is a recent regression. Can we get this addressed please? u/AMD_Vik",Negative
Intel,"Been having issues with VLC freezing and stuttering during playback (video only, not audio) since anything after 25.9.1. Guess I'm gonna roll back to that until it gets figured out.... really frustrating.",Negative
Intel,Substance Designer won't start with this one. Access violation with amdvlk64.dll. Adrenaline won't start either,Neutral
Intel,"Sorry but for me the drive give me crash pop up message every time i boot up my pc. Also just right now i got a freeze, black screen to all my monitors.",Negative
Intel,The worst driver this year so far,Negative
Intel,"Still havent fixed the noise cancellation lmao, guess its another month+ of old version :) Thanks amd, truly doing wonders.",Negative
Intel,CS2 crashing with driver timeout after tabbing out or watching streams on 2nd screen 7900xtx,Negative
Intel,"When is 25.12.1 coming out? I have read only bad things about 25.11.1 here, so I wanted to skip this one.",Negative
Intel,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",Neutral
Intel,So no redstone yet,Negative
Intel,FSR AI frame gen??? Didn’t they say that’d it would also have a driver toggle?,Neutral
Intel,Did AMD ever add support for Cronos?,Neutral
Intel,Well Star Citizen will load now!  Now some longer term testing....,Neutral
Intel,Anybody tried this with Anno 117 yet? I’m hoping it helps performance,Positive
Intel,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,Neutral
Intel,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,Neutral
Intel,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,Negative
Intel,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,Negative
Intel,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,Negative
Intel,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,Negative
Intel,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,Negative
Intel,"Here we go again, jetzt stürzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen außer XMP war aktiviert, dann stürzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das übernehmen müsst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team Grün nicht.",Neutral
Intel,Yeah same here LG c5 42inch 😰,Neutral
Intel,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesn’t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",Negative
Intel,"I have this but on display port, HDMI works fine",Neutral
Intel,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",Neutral
Intel,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",Negative
Intel,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,Neutral
Intel,I have the same issue with display port but it’s okay with hdmi :/,Neutral
Intel,"Honestly, I plan to make sure my next display has Display Port in it. Mostly for linux though.",Positive
Intel,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",Neutral
Intel,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,Neutral
Intel,"V25.10.2  here… I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",Neutral
Intel,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",Negative
Intel,combined again it looks like 🤷‍♂️,Neutral
Intel,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,Neutral
Intel,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",Negative
Intel,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",Neutral
Intel,You try install last chipset driver ?,Neutral
Intel,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",Negative
Intel,So it's the driver that's why that happens 😡 and it's not fixed?,Negative
Intel,Thank you for your service,Positive
Intel,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",Negative
Intel,Any update mate?,Neutral
Intel,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",Negative
Intel,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",Negative
Intel,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",Negative
Intel,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",Neutral
Intel,"I'm the opposite, I just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",Neutral
Intel,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDU’d it again to go back to 25.9.2 since games were stuttering.",Negative
Intel,Same.,Neutral
Intel,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",Neutral
Intel,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",Negative
Intel,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,Neutral
Intel,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,Negative
Intel,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",Positive
Intel,If it still crashes set RTX Global Illumination to Static.,Negative
Intel,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",Negative
Intel,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",Negative
Intel,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as they’ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since they’re much deeper in engine code/inputs.",Neutral
Intel,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it 🤓",Neutral
Intel,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",Neutral
Intel,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",Negative
Intel,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,Negative
Intel,Ugh,Neutral
Intel,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",Neutral
Intel,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",Neutral
Intel,"Linux doesn’t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, that’s when you get driver updates, and they’re completely different from windows branch.",Neutral
Intel,just uninstall it I prefer manual check myself.,Neutral
Intel,So AMDs default driver overclocks and doesn’t reflect that in the values?,Neutral
Intel,Same issues here i underclocked it but this new update just made it worse,Negative
Intel,ok it is still crashing ... complete reboot :(,Negative
Intel,"I feel like that crash is more on DICE's side, since Nvidia users get the same exact crash, although less often.  I tried everything I saw on the internet, nothing really works. Sometimes I can play for hours on end, other time game just crashes randomly after 10-15 minutes.  I am going to try to downgrade to 25.9.1 and see how it fares, since I remember that driver being really stable for me (6800XT).  Edit: been playing for 4 hours, no crash yet. Never had such a long session without the game crashing.  Will update in the next few days.  Edit 2: haven't crashed once, been playing at least 2 hours every evening.",Negative
Intel,Okay.,Neutral
Intel,I’m hoping Valve’s new steam machine will push them on that since it’s RDNA3 based.,Neutral
Intel,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,Negative
Intel,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",Negative
Intel,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,Neutral
Intel,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",Negative
Intel,welcome to amd,Neutral
Intel,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,Neutral
Intel,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",Negative
Intel,Same. Never even had Ryzen master installed.,Negative
Intel,"I'm receiving the same error in Event Viewer, but I have installed Ryzen Master. Most likely it's also a component of the Adrenalin drivers for system tuning and monitoring.  Registry search shows two keys for ""AMDRyzenMasterDriverV30"" (in both CurrentControlSet and ControlSet001): Computer\\HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\AMDRyzenMasterDriverV30  The ImagePath points to: C:\\Windows\\System32\\AMDRyzenMasterDriver.sys and the file exists. It's valid.",Neutral
Intel,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,Neutral
Intel,What is redstone?,Neutral
Intel,What's weird is Black Ops 7 has ray regeneration.,Negative
Intel,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",Neutral
Intel,vsync issue fixed with win 11 KB5068861 update.,Neutral
Intel,had no issues with vsync on 25.10.2,Neutral
Intel,works fine for me,Positive
Intel,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,Neutral
Intel,"That it did, lol. My only complaint.",Neutral
Intel,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",Negative
Intel,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,Negative
Intel,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,Negative
Intel,"Fps drop over time? That's a game issue, it's got a memory leak",Negative
Intel,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",Negative
Intel,I’d settle for bf6 going one entire game without drivers crashing the game and freezing pc,Neutral
Intel,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",Negative
Intel,Crashes?,Neutral
Intel,I have this problem in all games.,Negative
Intel,"Hello, I've been having this issue and I have exactly your gpu and cpu, whenever I played valorant and I alt tabed many times the screen goes black and keyboard become unresponsive but I can still hear friends in discord and they can't hear me, after conctacting valorant support and messing with alot of settings I think  what fixed it for me is to add these in windows defender exclusions : C:\\Riot Games\\VALORANT\\live\\VALORANT.exe   C:\\Riot Games\\VALORANT\\live\\ShooterGame\\Binaries\\Win64\\VALORANT-Win64-Shipping   C:\\Program Files\\Riot Vanguard\\vgc.exe   C:\\Program Files\\Riot Vanguard\\vgm.exe   C:\\Riot Games\\Riot Client\\RiotClientServices.exe   I hope this helps",Negative
Intel,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",Neutral
Intel,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",Negative
Intel,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",Neutral
Intel,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,Negative
Intel,Epic version runs just fine.,Positive
Intel,Cyberpunk GOG last version patch runs fine on this driver.,Positive
Intel,"Hey there, can you give an example of how this looks now versus how it's supposed to?",Neutral
Intel,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,Neutral
Intel,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,Negative
Intel,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,Negative
Intel,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,Neutral
Intel,"The game is booting, this message was for the 25.10 they just didn't removed it",Negative
Intel,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,Neutral
Intel,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,Neutral
Intel,"First time yes, i downloaded with -s letter, but the last time i downloaded smth like -combined(1.6 gb). All two's is for WIn 11.",Neutral
Intel,"To be clear, are you able to confirm that VRR is disabled after you alt-tab? Do you have a display-side OSD to verify?",Neutral
Intel,"Good call, it caused nothing but problems for me and pretty severe. Were talking driver timeouts with black screens and even a couple bluescreens.",Negative
Intel,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",Neutral
Intel,My 9070 xt crushes while I try to use fsr 4 on new drivers,Neutral
Intel,Why don't you try it and let us know if you can. Would be helpful for lots of us,Negative
Intel,It's in Redstone. Still not out yet,Neutral
Intel,Didn't work for me...,Negative
Intel,Wait until you see how much your browser's cache is churning...,Neutral
Intel,Why cant you use Adrenalin? I'm using it on 25.9.1,Negative
Intel,I just received a windows extension update for my LG monitor. If you can boot up go check.,Neutral
Intel,The last time I had this problem it was a RAM issue.,Negative
Intel,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,Positive
Intel,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",Neutral
Intel,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,Negative
Intel,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",Negative
Intel,Do u reintall already up to date chipset drivers?,Neutral
Intel,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,Neutral
Intel,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",Negative
Intel,doing so (separation) will create a freak out shitstorm part 2.,Neutral
Intel,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,Neutral
Intel,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),Neutral
Intel,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,Negative
Intel,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",Neutral
Intel,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,Neutral
Intel,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,Negative
Intel,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,Negative
Intel,Thank you for communicating,Positive
Intel,Unfortunately happens to me too. So for me it’s a big issue as I can’t update to this driver until it is fixed 😰,Negative
Intel,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.  Issue goes away using a non 4k 240hz display.     I believe this system crash is deeply related to DSC on Windows.  I only got these two PC bsods when I bought a 4k 240hz display.  Returned a monitor (bad oled) and the issue went away.  Got a new oled a few weeks ago and now I have these bsods again.     Never had a bsod before I got these 4k 240hz displays.  Fresh Windows 11 installs too between both PCs and between my first and second oled.  Systems are both solid and stable.     Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.  Hopefully someone else had experience with them on 4k 240hz.",Negative
Intel,Thank you AMD my bad for getting upset,Positive
Intel,Thank you.,Positive
Intel,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why it’s failing. Would be cool to see the technical details if that’s possible. (I’m actually more interested now on why it’s not working vs just getting it fixed).,Neutral
Intel,Thank you!,Positive
Intel,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,Negative
Intel,Redstone when?,Neutral
Intel,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",Positive
Intel,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,Neutral
Intel,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",Neutral
Intel,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",Neutral
Intel,Non pc monitor tvs are sometimes cheaper especially for larger sizes. I’m on lg c5 oled 42inch and it only has hdmi…,Neutral
Intel,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",Neutral
Intel,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",Negative
Intel,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",Neutral
Intel,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",Neutral
Intel,> Are y'all playing on televisions?  Do you guys not have phones?,Neutral
Intel,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,Neutral
Intel,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,Neutral
Intel,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",Negative
Intel,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",Negative
Intel,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",Neutral
Intel,Why does it seem like driver quality/support has gotten substantially worse this past decade? Are we running out of skilled software engineers or is hardware just getting too out of hand?,Negative
Intel,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,Negative
Intel,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",Negative
Intel,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",Positive
Intel,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",Neutral
Intel,OK thought I was the only one. 25.10 is bad bad,Negative
Intel,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",Negative
Intel,Thanks for testing it,Positive
Intel,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",Positive
Intel,I thought FSR 4 was only on RDNA 4? 🤔,Neutral
Intel,My thoughts exactly. Thanks.,Positive
Intel,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,Neutral
Intel,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,Positive
Intel,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,Neutral
Intel,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",Neutral
Intel,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",Neutral
Intel,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,Neutral
Intel,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,Neutral
Intel,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",Negative
Intel,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",Positive
Intel,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,Negative
Intel,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",Neutral
Intel,I guess you can't drop any hints as to whether this work with CDPR also involves adding Ray Regeneration to the game 👀?,Neutral
Intel,Fun fact - i am dual booting and on Linux this bug is not existent...:)),Negative
Intel,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",Negative
Intel,Hi. Did you ever resolve this? I'm getting the same error. Thanks.,Negative
Intel,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",Negative
Intel,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,Neutral
Intel,It's a thing you can search for on Google,Neutral
Intel,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,Neutral
Intel,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",Neutral
Intel,ahh i'm on Win 10 so probably why I didn't see it.,Neutral
Intel,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,Negative
Intel,"Yes, but was it in the previous WHQL driver ? I'm not sure.",Neutral
Intel,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),Neutral
Intel,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,Neutral
Intel,"I tried everything I saw online: meshes on low, XMP lower/off, chipset drivers reinstall and other stuff. Nothing worked.  I downgraded back to 25.9.1., haven't had a crash in days.  Kinda miss the improvements for AFMF they brought with 25.10 for other games, but eh I'd rather play BF6 without it crashing randomly.",Negative
Intel,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",Negative
Intel,Either launch with curseforge or rollback,Neutral
Intel,"Damn, didn’t work for me last driver either. I can get FSR4 to work in other games just not BF6",Negative
Intel,Sorry for not replying in time with the pictures but I just saw that on Twitter that Beat Saber and AMD are now aware of the issue. The distorted flickering issue on the walls.  https://xcancel.com/BeatSaber/status/1993629046802882685  However there's another issue. I had not actually tried to use an Index at 90Hz until the other day. I discovered that the latency bug is back for 90Hz mode. As in I have to adjust the photon latency to ~5ms in the Steam debug commands to make it usable but not fixed. Just like in the the drivers before 24.12.1.   120Hz mode still works fine.,Neutral
Intel,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",Negative
Intel,You 100 procent sure on this?,Neutral
Intel,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,Positive
Intel,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",Neutral
Intel,Driver with -s letter after black screen and reboot PC tells me that this driver isn't for my graphic card🤡,Negative
Intel,"I had randomly black screens with 24.2.1, this was annoying as hell. Had to DDU the Driver and went back to 23.11.1, after this everything was fine.",Negative
Intel,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,Neutral
Intel,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",Neutral
Intel,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,Neutral
Intel,What do you mean extension update??? Do you mean lg firmware update or something Ina  windows update? Where do I find this?,Neutral
Intel,They do not.,Neutral
Intel,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",Neutral
Intel,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,Negative
Intel,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,Neutral
Intel,"AND is taking away one additional driver feature per day, you say?",Neutral
Intel,"Yes, I’m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select “GPU” you get a file that has a different dimension from the one you download if you choose “CPU”. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with “minimal_install), but Adrenalin App does not open.",Neutral
Intel,Thank you for explaining it before the rage baiters go nuts.,Positive
Intel,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,Negative
Intel,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",Negative
Intel,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",Neutral
Intel,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",Positive
Intel,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",Neutral
Intel,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,Negative
Intel,Was yours the DisplayPort config or HDMI? I may have a fix for this ready if you're available test,Neutral
Intel,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,Neutral
Intel,Already launched in COD 7,Neutral
Intel,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so you’re saying i shoulf switch to hdmi?",Neutral
Intel,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,Neutral
Intel,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,Neutral
Intel,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",Neutral
Intel,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,Neutral
Intel,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",Neutral
Intel,With the compiled leaked DLL you can use it on RDNA3 as well.,Neutral
Intel,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",Neutral
Intel,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,Negative
Intel,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),Negative
Intel,Thank you! Exciting keen to see what it’s like,Positive
Intel,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",Negative
Intel,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,Neutral
Intel,"The issue is if you try to use path tracing. Which to be fair, you probably shouldn't unless the miracle of them getting Virtuous to implement Ray Regeneration in Cyberpunk happens.",Negative
Intel,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,Neutral
Intel,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,Negative
Intel,"Yes. So far, so good. I'm not 100% sure what fixed it.      I uninstalled both Adrenalin and Ryzen Master standalone applications. Deleted the ""amdryzenmasterv"" keys. Rebooted.  Then I installed Adrenalin and used the Ryzen Master installer in Adrenalin (Performance > Metrics > Install Ryzen Master).  I think this problem might have something to do with a handshake breaking between Ryzen Master and Adrenalin, after upgrading just Adrenalin.   From now on, I'll probably do clean installs, removing and reinstalling both Adrenalin and Ryzen Master, through Adrenalin Performance tab.",Positive
Intel,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",Negative
Intel,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",Neutral
Intel,lmao chill out dude go touch some grass,Neutral
Intel,Could be grounds for lawsuit… That’s funny!,Neutral
Intel,Because of MPO.,Neutral
Intel,yeah same with 25.11.1 25.9.2 works for me,Positive
Intel,"25.10.2 was the previous WHQL, so also yes :P",Neutral
Intel,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",Neutral
Intel,Which driver version and does it still crashing?,Neutral
Intel,OK I will install it now and test it and get back to you. Give me 10 mins.,Neutral
Intel,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,Negative
Intel,I'll work with the engineer from that ticket check if that issue has somehow regressed.,Neutral
Intel,We've not been able to reproduce this internally so far. Can you remind me which GPU (was this a 7900XTX?) + connectivity method you're using?,Neutral
Intel,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,Neutral
Intel,"Yup just need to say ""No""",Neutral
Intel,"whew thanks, good think i noticed it first before updating. i have 25.10.2 and 25.9.2 here and they both have windows 10 along their filename so i might as well asked.",Positive
Intel,I don't see how it would work on 23.9.1 lol,Negative
Intel,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",Neutral
Intel,I did it this morning before the new driver and confirm chipset drivers were untouched,Neutral
Intel,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and don’t use the latest drivers. At least AMD owned up to it so I can’t be too upset but hopefully they really do fix this soon as new users may not understand what’s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly it’s stable for them and they don’t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs don’t always have a DP connector at all.,Negative
Intel,"ah, that explains it. Thanks. :)",Positive
Intel,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named “minimal install”). Obviously I’m referring to AMD driver download page.",Neutral
Intel,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",Neutral
Intel,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",Negative
Intel,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",Positive
Intel,What about Noise Suppression not working since 25.9.2?,Neutral
Intel,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,Neutral
Intel,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",Neutral
Intel,Hell yeah 🙂 amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,Positive
Intel,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",Neutral
Intel,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",Neutral
Intel,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,Positive
Intel,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",Neutral
Intel,That was my very first actual driver issue I experienced with AMD.,Negative
Intel,Oh that's nice! I'll look into it when I get the chance.,Positive
Intel,Cool. Thank you,Positive
Intel,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience 😖,Negative
Intel,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,Negative
Intel,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",Negative
Intel,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",Neutral
Intel,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,Positive
Intel,"Fair enough, and yeah sooner the better for all of us",Positive
Intel,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,Neutral
Intel,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",Negative
Intel,Fingers crossed,Neutral
Intel,"Thanks for attempting to retest.  It's a 7900XTX with an Index connected via DisplayPort. I am on the latest 25.11.1 driver.  I run a monitor at 4k 120Hz 10bpc with HDR Off, which uses DSC, as my main and only display. I tried disabling DSC in the monitor settings which runs at 4k 120Hz 8bpc with HDR Off but I don't think I noticed a change in latency. I thought that DSC on and off on two different devices might contribute to the problem but I'm not sure.   I have also tried running the Index under a RX480 on another PC and I fairly certain the latency looks different under 90Hz and looks similar under 120Hz. Can't play much to test though as an RX480 runs the Index at a very blurry setting. Getting around to doing this test is what took me so long to reply.",Neutral
Intel,Were you able to find the issue?,Neutral
Intel,"Allright ty, will Install new, any differences in performance?",Neutral
Intel,"im running 25.11.1 on win10 7900xt. no problems besides afmf2 breaking the performance overlay, which ive had for multiple updates now",Neutral
Intel,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,Neutral
Intel,Thank you for this. This was very helpful. Got adrenaline working fine now.,Positive
Intel,"I wish my LG C4 42"" had a display port. Its my primary monitor.",Neutral
Intel,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man it’s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for me… and have zero time to reinstall Windows.",Negative
Intel,"Don't do that, i'm suffering with both 7900XTX + RVII (and even with RX6400)",Negative
Intel,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",Negative
Intel,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",Neutral
Intel,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,Neutral
Intel,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",Neutral
Intel,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",Positive
Intel,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,Positive
Intel,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,Neutral
Intel,also i have coil whine since this driver 25.11.1. ?!  also in idle sometimes...  very strange driver...,Negative
Intel,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,Neutral
Intel,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",Neutral
Intel,We've still not been able to reproduce this unfortunately. I'll need to check in when I'm back at work next year,Negative
Intel,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,Negative
Intel,"did you download the same filename with the one i mentioned? i tried downloading windows 11 link and it also gave me the same filename, lol",Neutral
Intel,No you can't.,Neutral
Intel,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",Negative
Intel,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",Positive
Intel,"They are TV's, not pc monitors. Buy the right tool for the job",Neutral
Intel,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",Neutral
Intel,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",Negative
Intel,"Yeah, I'm facing the same issue on RX 9060 XT   Is it a GPU driver issue, or a Windows issue that Microsoft needs to fix?",Negative
Intel,since last BF6 Update i had zero crashes also on 25.11.1,Negative
Intel,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",Negative
Intel,What about 25.11.1?,Neutral
Intel,Yeah same for me. Considering how similair win10 and 11 are under the hood i just went with it. Still absolutely no problems sofar.,Positive
Intel,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,Neutral
Intel,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",Negative
Intel,"Look online for fsr 4 on 6000 and 5000 series, you will understand,    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",Neutral
Intel,Did you reboot after setting that key? Is the display with chrome still only partially updating?,Neutral
Intel,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",Negative
Intel,thank you,Positive
Intel,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",Negative
Intel,"Not a typo, I was asking about something else and he missed my point...",Negative
Intel,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",Negative
Intel,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",Negative
Intel,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",Negative
Intel,"What a disgusting build, I love it",Positive
Intel,the content we crave,Neutral
Intel,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",Neutral
Intel,What GPU are you using in your build?  All of them,Neutral
Intel,you're one hell of a doctor. mad setup!,Positive
Intel,The amount of blaspheming on display is worthy of praise.,Neutral
Intel,Brother collecting them like infinity stones lmao,Neutral
Intel,I'm sure those GPUs fight each others at night,Neutral
Intel,Bro unlocked the forbidden RGB gpus combo,Neutral
Intel,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,Neutral
Intel,What the fuck,Negative
Intel,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,Positive
Intel,Yuck,Neutral
Intel,Wait until you discover lossless scaling,Neutral
Intel,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,Neutral
Intel,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",Negative
Intel,Now you just need to buy one of those ARM workstations to get the quad setup,Neutral
Intel,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,Positive
Intel,Love it lol. How do the fucking drivers work? Haha,Positive
Intel,What an amazing build,Positive
Intel,wtf is that build man xdd bro collected all the infinity stones of gpu world.,Negative
Intel,You’re a psychopath. I love it,Positive
Intel,This gpu looks clean asf😭,Neutral
Intel,The only setup where RGB gives more performance. :D,Neutral
Intel,Now you need a dual cpu mobo.,Neutral
Intel,Placona! I've been happy with a 6700xt for years.,Positive
Intel,absolute cinema,Neutral
Intel,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",Neutral
Intel,"Brawndo has electrolytes, that's what plants crave!",Neutral
Intel,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",Neutral
Intel,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",Neutral
Intel,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",Neutral
Intel,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",Neutral
Intel,Team RGB,Neutral
Intel,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",Neutral
Intel,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",Negative
Intel,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",Positive
Intel,"OpenCL works on all of them at once, and is just as fast as CUDA!",Positive
Intel,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",Neutral
Intel,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,Neutral
Intel,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",Positive
Intel,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",Neutral
Intel,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),Negative
Intel,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,Neutral
Intel,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,Neutral
Intel,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",Neutral
Intel,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",Negative
Intel,Thank you so much for the very detailed response!,Positive
Intel,Well worth it!,Positive
Intel,Thank you my man!! Looking forward to run some tests once I get home.,Positive
Intel,That's awesome!,Positive
Intel,"Yes, but SLI is a bad description for it.",Negative
Intel,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",Neutral
Intel,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",Negative
Intel,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",Negative
Intel,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",Neutral
Intel,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",Neutral
Intel,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",Neutral
Intel,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",Neutral
Intel,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",Negative
Intel,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",Positive
Intel,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",Neutral
Intel,Why are you connecting the monitor to the gpu and not the mobo?,Neutral
Intel,"👍   thanks for the info, this'll definitely come in handy eventually.",Positive
Intel,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,Neutral
Intel,No worries mate. Good luck,Positive
Intel,"For some reason I switched up, connecting to the gpu is the way to go. I derped",Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,It's alive. Rejoice.,Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",Neutral
Intel,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",Negative
Intel,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,Neutral
Intel,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,Neutral
Intel,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,Negative
Intel,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,Positive
Intel,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",Neutral
Intel,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",Negative
Intel,I'm fairly sure they use dxvk for d3d9 to 11.,Neutral
Intel,Could just be a cache issue,Neutral
Intel,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,Neutral
Intel,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,Positive
Intel,"According to the graphs, AMD has slightly less overhead than NVIDIA.",Neutral
Intel,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",Negative
Intel,"Lowest with DX11 and older, but not with the newer APIs",Neutral
Intel,And when is the last time HUB did a dedicated video showing the improvement in overhead?,Neutral
Intel,or it's just a cache/memory access issue,Neutral
Intel,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",Neutral
Intel,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",Negative
Intel,"Intel uses software translation for DX11 and lower, so it does matter for them.",Neutral
Intel,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",Neutral
Intel,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",Negative
Intel,That's not true. Intel's issue is being too verbose in commands/calls.,Negative
Intel,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",Negative
Intel,HUB used DX12 games that also showed the issue.  It's something else.,Negative
Intel,"The comment to which I am replying is talking about nVidia, not Intel.",Neutral
Intel,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",Negative
Intel,That's actually... just worse news.,Negative
Intel,I always dreamt of the day APUs become power houses.,Neutral
Intel,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",Neutral
Intel,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",Neutral
Intel,Damn Why is AMD even involved in iGPU,Negative
Intel,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",Positive
Intel,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",Neutral
Intel,almost there,Neutral
Intel,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",Positive
Intel,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",Positive
Intel,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,Negative
Intel,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",Neutral
Intel,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,Negative
Intel,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",Negative
Intel,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",Negative
Intel,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",Neutral
Intel,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",Negative
Intel,yes its so bad. better go buy some steam deck or ally x,Negative
Intel,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,Neutral
Intel,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",Positive
Intel,How are they going to feed all those CUs? Quad-channel LPDDR5X?,Neutral
Intel,That's considerably faster than an XSX.,Positive
Intel,>That's tapping on 4070/7800 levels of performance.  What is?,Neutral
Intel,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",Neutral
Intel,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",Positive
Intel,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,Neutral
Intel,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,Neutral
Intel,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",Neutral
Intel,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",Negative
Intel,It's called satire. You're just salty because you're the butt of the joke.,Neutral
Intel,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,Neutral
Intel,Praying the blade16 gets it.,Neutral
Intel,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",Neutral
Intel,256 bit bus + infinity cache.,Neutral
Intel,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,Neutral
Intel,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",Positive
Intel,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",Neutral
Intel,The rumored 40CU strix halo chip. Not the actual chips released this week.,Neutral
Intel,7500mhz ram and the 780m,Neutral
Intel,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",Neutral
Intel,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",Neutral
Intel,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",Neutral
Intel,Literally where did you see 40-60% uplift at half the power?,Neutral
Intel,> 40-60% performance uplift at half the power  Source?,Neutral
Intel,"i chuckled, then again im not a fanboy of anything",Negative
Intel,Dont expect 40CUs in a handheld anytime soon,Neutral
Intel,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",Negative
Intel,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",Neutral
Intel,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",Neutral
Intel,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,Neutral
Intel,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",Neutral
Intel,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,Neutral
Intel,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,Neutral
Intel,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",Neutral
Intel,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",Negative
Intel,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",Negative
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Neutral
Intel,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,Positive
Intel,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",Negative
Intel,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",Negative
Intel,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,Negative
Intel,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",Neutral
Intel,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",Neutral
Intel,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,Positive
Intel,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,Positive
Intel,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",Positive
Intel,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",Neutral
Intel,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",Negative
Intel,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,Neutral
Intel,"Installs beta software, proceeds to complain about it",Neutral
Intel,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,Negative
Intel,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",Neutral
Intel,What Ghost of Tsushima issue?,Neutral
Intel,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",Neutral
Intel,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",Neutral
Intel,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",Negative
Intel,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,Negative
Intel,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",Neutral
Intel,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",Neutral
Intel,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,Neutral
Intel,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",Negative
Intel,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,Negative
Intel,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,Negative
Intel,The documentation for it would still be in their archives,Neutral
Intel,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",Neutral
Intel,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",Negative
Intel,Wish Arc cards were better. They look so pretty in comparison to their peers,Positive
Intel,Thats actually a pretty solid and accurate breakdown.,Positive
Intel,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,Neutral
Intel,3080 still looking good too,Positive
Intel,What they have peaceful then 4k series?,Neutral
Intel,Just get a 4090. I will never regret getting mine.,Positive
Intel,i miss old good times where radeon HD 7970 as best single core card cost around 400$,Neutral
Intel,"Damn, the A770 is still so uncompetitive...",Negative
Intel,"It's like the free market priced cards according to their relative performance. How weird, right?",Negative
Intel,How is that possibly annoying,Negative
Intel,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,Positive
Intel,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",Positive
Intel,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",Negative
Intel,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,Positive
Intel,8gb perfectly fine today :),Positive
Intel,"Ah yes sure, now where did I leave my 1500 euros?",Neutral
Intel,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",Negative
Intel,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,Positive
Intel,"Yeah, i like the black super series.",Neutral
Intel,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",Neutral
Intel,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",Neutral
Intel,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,Neutral
Intel,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",Negative
Intel,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",Neutral
Intel,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",Negative
Intel,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",Positive
Intel,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",Negative
Intel,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",Negative
Intel,"Yo, I saw the title and thought this gotta be Gnif2.",Neutral
Intel,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",Negative
Intel,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",Negative
Intel,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",Negative
Intel,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",Positive
Intel,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",Neutral
Intel,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",Negative
Intel,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",Negative
Intel,Long but worth it read; Well Done!,Positive
Intel,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",Neutral
Intel,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,Negative
Intel,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",Neutral
Intel,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,Negative
Intel,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,Negative
Intel,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",Negative
Intel,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,Negative
Intel,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",Negative
Intel,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",Negative
Intel,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",Negative
Intel,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,Negative
Intel,100% all of this...  Love looking glass by the by,Positive
Intel,How does say VMware handle this? Does it kind of just restart shit as needed?,Neutral
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",Neutral
Intel,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",Positive
Intel,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",Negative
Intel,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,Negative
Intel,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",Negative
Intel,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",Neutral
Intel,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",Neutral
Intel,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",Negative
Intel,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",Neutral
Intel,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,Neutral
Intel,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",Negative
Intel,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",Negative
Intel,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",Negative
Intel,TL;DR. **PEBKAC**.,Neutral
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",Negative
Intel,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,Positive
Intel,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",Negative
Intel,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,Negative
Intel,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",Negative
Intel,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,Negative
Intel,"Thanks mate I appreciate it, glad to see you here :)",Positive
Intel,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",Positive
Intel,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",Negative
Intel,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",Neutral
Intel,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",Negative
Intel,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,Neutral
Intel,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,Negative
Intel,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",Neutral
Intel,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,Neutral
Intel,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",Positive
Intel,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",Neutral
Intel,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",Negative
Intel,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",Neutral
Intel,"Funny, I saw the title and thought the same too!",Neutral
Intel,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",Negative
Intel,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",Negative
Intel,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,Neutral
Intel,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",Neutral
Intel,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",Negative
Intel,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",Negative
Intel,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",Negative
Intel,ursohot !  back to discord rants...,Neutral
Intel,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,Positive
Intel,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",Negative
Intel,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",Negative
Intel,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,Positive
Intel,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",Negative
Intel,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",Negative
Intel,"It doesn't handle it, it has the same issue.",Negative
Intel,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),Neutral
Intel,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",Negative
Intel,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",Positive
Intel,Me neither. I use a RX580 8GB since launch and not a single problem.,Negative
Intel,Because they're talking absolute rubbish that's why.,Negative
Intel,You are one of the lucky ones!,Neutral
Intel,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",Negative
Intel,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",Negative
Intel,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",Positive
Intel,lol your flair is Please search before asking,Neutral
Intel,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,Negative
Intel,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,Neutral
Intel,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",Negative
Intel,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",Negative
Intel,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,Neutral
Intel,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,Neutral
Intel,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",Negative
Intel,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",Neutral
Intel,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",Positive
Intel,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",Negative
Intel,"""NVIDIA, it just works""",Neutral
Intel,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,Negative
Intel,What is the AMD Vanguard?,Neutral
Intel,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",Negative
Intel,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,Negative
Intel,You misspelled $2.3T market cap....,Neutral
Intel,"Okay yeah fair enough, hadn't considered this. Removed it from my post",Neutral
Intel,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",Neutral
Intel,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",Neutral
Intel,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",Neutral
Intel,This is not a fix. It's a compromise.,Negative
Intel,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",Neutral
Intel,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,Negative
Intel,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",Negative
Intel,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,Neutral
Intel,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,Negative
Intel,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",Negative
Intel,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",Negative
Intel,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",Negative
Intel,The comment I quoted was talking about people playing games having issues.,Neutral
Intel,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,Neutral
Intel,The thing I quoted was talking about people playing games though.,Neutral
Intel,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",Negative
Intel,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",Negative
Intel,"Idk, I don't use Linux",Neutral
Intel,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",Neutral
Intel,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",Neutral
Intel,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),Neutral
Intel,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",Negative
Intel,Because adding a feature for a product literally gives users more control for that product.,Neutral
Intel,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,Neutral
Intel,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",Negative
Intel,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,Neutral
Intel,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",Neutral
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",Negative
Intel,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,Negative
Intel,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",Neutral
Intel,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",Neutral
Intel,*wayland users have joined the chat,Neutral
Intel,You're falling for slogans.,Neutral
Intel,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",Positive
Intel,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,Neutral
Intel,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),Neutral
Intel,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",Neutral
Intel,Honestly after a trillion I kinda stop counting 😂🤣,Negative
Intel,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",Neutral
Intel,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",Negative
Intel,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",Negative
Intel,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",Negative
Intel,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,Negative
Intel,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",Neutral
Intel,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",Neutral
Intel,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",Negative
Intel,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",Negative
Intel,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",Neutral
Intel,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",Negative
Intel,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",Neutral
Intel,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",Negative
Intel,Oh then just ignore my comment 😅,Neutral
Intel,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",Positive
Intel,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,Negative
Intel,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",Positive
Intel,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",Negative
Intel,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",Neutral
Intel,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,Negative
Intel,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",Negative
Intel,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,Negative
Intel,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,Negative
Intel,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",Negative
Intel,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",Neutral
Intel,"Too soon to tell, but hopes are high.",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",Negative
Intel,"Agreed, they cannot rest on their laurels.",Negative
Intel,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",Neutral
Intel,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",Neutral
Intel,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,Neutral
Intel,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,Neutral
Intel,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",Negative
Intel,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",Negative
Intel,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",Negative
Intel,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",Neutral
Intel,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",Neutral
Intel,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,Negative
Intel,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,Negative
Intel,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,Negative
Intel,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",Negative
Intel,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",Negative
Intel,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,Negative
Intel,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",Negative
Intel,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,Neutral
Intel,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",Neutral
Intel,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,Neutral
Intel,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",Neutral
Intel,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",Negative
Intel,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",Negative
Intel,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",Neutral
Intel,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,Neutral
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",Negative
Intel,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",Neutral
Intel,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",Negative
Intel,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",Negative
Intel,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",Negative
Intel,Oh and XE also have bug feature reporting.  Omfg!!!!,Negative
Intel,Nobody is 100% right ;),Neutral
Intel,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),Negative
Intel,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",Negative
Intel,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",Negative
Intel,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",Neutral
Intel,What about using a DP to HDMI 2.1 adapter for that situation?,Neutral
Intel,"2021 my guy, it's right there on the date of the article.",Neutral
Intel,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,Neutral
Intel,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,Neutral
Intel,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",Neutral
Intel,And I guess infallible game developers too then. /s,Neutral
Intel,So you decide what criticism is valid and what not? lol,Neutral
Intel,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,Negative
Intel,"Yup, but do you see them making a big press release about it?",Neutral
Intel,that is not how it works but sure,Neutral
Intel,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,Neutral
Intel,>whine about Redditors.  The irony.,Neutral
Intel,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",Neutral
Intel,learn to comprehend.,Neutral
Intel,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,Neutral
Intel,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",Negative
Intel,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",Negative
Intel,"No, that would be you obviously /s",Neutral
Intel,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,Neutral
Intel,"Yea, given the state of XE drivers every major update has come with significant PR.",Neutral
Intel,Why not ;),Neutral
Intel,Go word salad elsewhere.,Neutral
Intel,"I have replicated the issue reliably yes, and across two different systems.",Neutral
Intel,If discord crashes my drivers.. once every few hours. I have to reboot,Negative
Intel,Discord doesn't crash my drivers  I don't have to reboot.,Negative
Intel,Really love how the 6000 series radeons look.,Positive
Intel,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",Neutral
Intel,That's a good looking line up,Positive
Intel,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",Negative
Intel,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",Positive
Intel,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",Positive
Intel,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,Positive
Intel,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",Negative
Intel,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,Negative
Intel,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",Negative
Intel,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",Negative
Intel,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,Neutral
Intel,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",Positive
Intel,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,Negative
Intel,That 7900xtx sale number is insane,Negative
Intel,That just shows that most people that buy GPU's don't know a thing about them.,Negative
Intel,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",Negative
Intel,best discounts were 6750xt 6800 and 7800xt,Positive
Intel,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,Neutral
Intel,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",Positive
Intel,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",Neutral
Intel,"They're not out of stock there, duh",Neutral
Intel,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,Positive
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",Negative
Intel,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",Neutral
Intel,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",Neutral
Intel,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",Neutral
Intel,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",Neutral
Intel,the card is pretty bad if you missed that somehow,Negative
Intel,AMD probably ships leftover to countries in which they know it will sell,Neutral
Intel,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",Neutral
Intel,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",Negative
Intel,"As you notice the photoshop version differs, so you can't compare them really",Neutral
Intel,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),Neutral
Intel,So basically PC games are never going to tell us what the specs are to run the game native ever again.,Negative
Intel,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",Neutral
Intel,The true crime here is needing FSR to reach these requirements.,Neutral
Intel,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",Neutral
Intel,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),Negative
Intel,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,Neutral
Intel,"well, at least the chart is easy to read, not a complete mess",Neutral
Intel,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,Positive
Intel,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,Negative
Intel,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,Negative
Intel,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",Neutral
Intel,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",Neutral
Intel,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,Positive
Intel,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",Negative
Intel,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",Negative
Intel,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",Negative
Intel,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",Neutral
Intel,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,Positive
Intel,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,Negative
Intel,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",Neutral
Intel,I hope they will bundle this game with CPUs/GPUs,Positive
Intel,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,Negative
Intel,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,Negative
Intel,This game better look like real life with those specs. I does looks beautiful!,Positive
Intel,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",Negative
Intel,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,Negative
Intel,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,Negative
Intel,I love it how absurd these things are these days.,Negative
Intel,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,Negative
Intel,Does anyone know if it supports SLI or crossfire?,Neutral
Intel,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,Neutral
Intel,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",Negative
Intel,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,Negative
Intel,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,Negative
Intel,Why in the f*ck is upscaling included on a specs page?,Negative
Intel,no more software optimization and full upscaling  bleah,Neutral
Intel,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,Negative
Intel,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",Positive
Intel,"I've never even heard of this game, nor care about it, but these system requirements offend me.",Negative
Intel,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,Negative
Intel,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",Positive
Intel,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",Negative
Intel,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,Neutral
Intel,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,Negative
Intel,Nice,Positive
Intel,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",Neutral
Intel,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",Negative
Intel,Native gaming died or what ? Wtf they turning pc gaming into console gaming,Neutral
Intel,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",Negative
Intel,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,Neutral
Intel,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,Negative
Intel,"Omg, it would be a graphic master piece or  bad optimized thing.",Negative
Intel,First time I see matches recommendations for nv and amd GPUs...,Neutral
Intel,Rip laptop rtx 3060 6gb,Neutral
Intel,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",Positive
Intel,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,Neutral
Intel,*NATIVE* resolution gang ftw!,Neutral
Intel,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",Negative
Intel,Looks capped at 60fps?,Neutral
Intel,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",Neutral
Intel,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,Neutral
Intel,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",Negative
Intel,Is it using UE5?,Neutral
Intel,"Ubisoft, rip on launch.",Neutral
Intel,guessing no DLSS3 then?,Neutral
Intel,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,Positive
Intel,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",Neutral
Intel,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",Negative
Intel,Farewell 1660ti… looks like it’s time for an upgrade,Neutral
Intel,well my 3300x is now obsolete for these new AAA games...,Negative
Intel,4k ultra right up my alley 😏,Neutral
Intel,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,Neutral
Intel,7900xtx will do 4k 120fps with FSR 3 then I guess?,Neutral
Intel,What must one do to achieve a higher rank than an enthusiast? A demi-god?,Neutral
Intel,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",Neutral
Intel,Is vrr fixed with frame gen then?,Neutral
Intel,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",Negative
Intel,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",Positive
Intel,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,Neutral
Intel,They recommend upscaling even at 1080p.. disgusting ew,Negative
Intel,pretty much this we all knew they would start using upscaling as a crutch.,Negative
Intel,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,Negative
Intel,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",Negative
Intel,Thanks to all of you that were screaming dlss looks better than native lmao.,Positive
Intel,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",Neutral
Intel,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,Negative
Intel,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",Negative
Intel,Nope we as a community abused a nice thing,Positive
Intel,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",Neutral
Intel,Didn't take them long to make upscaling worthless.,Negative
Intel,i smell a burgeoning cottage industry of game spec reviewers!,Neutral
Intel,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",Negative
Intel,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",Neutral
Intel,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",Neutral
Intel,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,Negative
Intel,yeah this is the new standard,Neutral
Intel,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",Neutral
Intel,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,Neutral
Intel,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",Negative
Intel,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",Positive
Intel,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",Neutral
Intel,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,Negative
Intel,The actual true crime here is even having fsr to begin with. It should just have dlss,Negative
Intel,Seems like they tried to cover every basis with these.,Neutral
Intel,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",Negative
Intel,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",Positive
Intel,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,Neutral
Intel,"GCN support is over, RDNA1 is the lowest currently supported arch.",Neutral
Intel,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",Negative
Intel,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",Negative
Intel,What do you mean forced raytracing?,Neutral
Intel,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,Positive
Intel,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",Neutral
Intel,It's actually 960p :(,Neutral
Intel,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",Neutral
Intel,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,Neutral
Intel,We'll see in a year.,Neutral
Intel,AFAIK  &#x200B;  It is with RT,Neutral
Intel,Seems pretty good to me given it is at 4K with RT,Positive
Intel,"Likely includes the RT features , its also 4k",Neutral
Intel,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",Negative
Intel,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",Negative
Intel,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",Positive
Intel,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,Negative
Intel,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",Neutral
Intel,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",Negative
Intel,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,Neutral
Intel,Exactly! It even got xess so Intel users also can use xess,Negative
Intel,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",Negative
Intel,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",Neutral
Intel,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,Neutral
Intel,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,Negative
Intel,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",Negative
Intel,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",Neutral
Intel,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",Neutral
Intel,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",Neutral
Intel,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",Negative
Intel,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",Negative
Intel,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,Negative
Intel,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",Negative
Intel,"Mirage is PS4 game, Avatar is PS5",Neutral
Intel,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",Negative
Intel,Timed epic exclusivity? Aww man.,Neutral
Intel,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,Negative
Intel,You... are.. joking... right..?,Neutral
Intel,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,Negative
Intel,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,Negative
Intel,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,Negative
Intel,Try ubisoft achievements :),Neutral
Intel,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",Neutral
Intel,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",Negative
Intel,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,Negative
Intel,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),Neutral
Intel,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",Negative
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",Neutral
Intel,yup that is why I will play 1440 UW native with a 7900XTX.,Neutral
Intel,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,Neutral
Intel,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",Neutral
Intel,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",Negative
Intel,Very similar performance I guess,Neutral
Intel,Ye,Neutral
Intel,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,Neutral
Intel,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",Neutral
Intel,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",Negative
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,Negative
Intel,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,Neutral
Intel,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,Neutral
Intel,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",Negative
Intel,"with AFMF it works , didnt test with FSR3 now.",Neutral
Intel,Reading before raging :),Neutral
Intel,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",Negative
Intel,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,Negative
Intel,For 30fps even lol,Neutral
Intel,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",Negative
Intel,Or be happy your shitty video card is still supported.,Negative
Intel,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,Negative
Intel,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,Positive
Intel,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,Negative
Intel,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,Negative
Intel,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,Negative
Intel,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",Negative
Intel,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",Neutral
Intel,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",Negative
Intel,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,Negative
Intel,But it does in many ways.,Neutral
Intel,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",Neutral
Intel,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",Negative
Intel,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,Negative
Intel,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",Neutral
Intel,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",Neutral
Intel,speaking facts my guy,Neutral
Intel,You shouldn't have downvote dood wtf redditors ?,Neutral
Intel,People with AMD cards dislike upscaling more because FSR sucks ass lol.,Negative
Intel,"Uhm, me btw...",Neutral
Intel,"> Who actually plays games at native these days, if it has upscaling?  I do.",Neutral
Intel,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",Negative
Intel,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",Neutral
Intel,onto absolutely nothing.,Negative
Intel,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",Negative
Intel,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,Negative
Intel,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,Neutral
Intel,The game is raytracing only with a ridiculous ammount of foooliage.,Neutral
Intel,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",Negative
Intel,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,Neutral
Intel,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,Positive
Intel,False.  guy blocked me lmao,Negative
Intel,"I thought that was on Linux, though I might be wrong",Neutral
Intel,1070 doesn't have hardware RT though.,Neutral
Intel,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),Neutral
Intel,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,Negative
Intel,Completely agree.,Neutral
Intel,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,Negative
Intel,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),Neutral
Intel,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",Negative
Intel,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",Negative
Intel,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,Negative
Intel,"Derp, derp.",Neutral
Intel,Sounds like John on Direct Foundry Direct every week.,Neutral
Intel,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",Positive
Intel,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",Neutral
Intel,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,Negative
Intel,Avatar is RT only. There is no non RT mode.,Neutral
Intel,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,Negative
Intel,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",Neutral
Intel,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",Neutral
Intel,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,Neutral
Intel,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,Neutral
Intel,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",Negative
Intel,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",Positive
Intel,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,Negative
Intel,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,Negative
Intel,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,Negative
Intel,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",Negative
Intel,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",Neutral
Intel,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",Neutral
Intel,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",Positive
Intel,This is what I am thinking too.,Neutral
Intel,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",Neutral
Intel,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",Neutral
Intel,Let's hope so. 30fps is far from recommended for FSR 3,Positive
Intel,Yeah I heard it works with that hoping it works with fsr3 now,Positive
Intel,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",Negative
Intel,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",Negative
Intel,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",Neutral
Intel,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,Negative
Intel,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,Neutral
Intel,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",Positive
Intel,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",Neutral
Intel,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",Negative
Intel,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",Negative
Intel,Assassins creed origins and odyssey side quests/collectibles oh my god,Positive
Intel,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,Negative
Intel,I appreciate your insights and opinions. Thank you.,Positive
Intel,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",Neutral
Intel,console games started upscaling way before PCs .,Negative
Intel,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,Negative
Intel,you forgot who owns one of the most popular engines out there?,Neutral
Intel,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",Negative
Intel,downvoted by devs lol,Neutral
Intel,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,Positive
Intel,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",Neutral
Intel,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,Negative
Intel,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,Neutral
Intel,Why is there a dialog message about unsupported hardware when you try and run a 390X?,Negative
Intel,It can do software based RT just like every other modern GPU out there.,Neutral
Intel,Well then no wonder rx 5700 can't manage 30 fps lol,Neutral
Intel,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",Positive
Intel,Avatars uses a Lumen like software RT solution.,Neutral
Intel,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",Negative
Intel,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",Negative
Intel,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),Negative
Intel,"Yeah, thatsl happened.",Neutral
Intel,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",Negative
Intel,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",Negative
Intel,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",Neutral
Intel,You didn't understand. It's another Swiss knife engine,Negative
Intel,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",Negative
Intel,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,Positive
Intel,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,Neutral
Intel,The PS5 has a 6700.,Neutral
Intel,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",Neutral
Intel,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",Positive
Intel,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,Negative
Intel,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,Neutral
Intel,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,Neutral
Intel,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,Neutral
Intel,TAA,Neutral
Intel,Temporal anti aliasing.,Neutral
Intel,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",Negative
Intel,"Ahh, welcome to r/FuckTAA",Neutral
Intel,Even 4K looks blurry with some implementations of TAA,Negative
Intel,they're giving you the bare minimum until your upgrade!,Neutral
Intel,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,Neutral
Intel,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",Neutral
Intel,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",Neutral
Intel,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",Neutral
Intel,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",Negative
Intel,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,Neutral
Intel,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",Negative
Intel,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",Neutral
Intel,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,Neutral
Intel,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",Neutral
Intel,No I havent. AMD is bigger than Epic.,Neutral
Intel,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",Negative
Intel,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",Negative
Intel,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",Neutral
Intel,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,Positive
Intel,It's software ray tracing which isn't accelerated by hardware.,Neutral
Intel,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,Neutral
Intel,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,Negative
Intel,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,Negative
Intel,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",Negative
Intel,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,Neutral
Intel,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,Negative
Intel,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",Negative
Intel,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",Neutral
Intel,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,Negative
Intel,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",Positive
Intel,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,Neutral
Intel,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",Neutral
Intel,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",Negative
Intel,Both excellent 👌 Down the Rabbit Hole was another solid one.,Positive
Intel,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",Neutral
Intel,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",Neutral
Intel,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,Negative
Intel,I have to turn it off in borderlands 3. Ugh.,Neutral
Intel,r/FuckTAA,Neutral
Intel,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",Negative
Intel,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",Positive
Intel,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,Negative
Intel,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",Neutral
Intel,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,Negative
Intel,epic is tencent...,Neutral
Intel,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",Negative
Intel,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",Neutral
Intel,You don't need RT hardware to do software RT. That's what I'm saying.,Neutral
Intel,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",Neutral
Intel,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",Neutral
Intel,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),Neutral
Intel,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,Negative
Intel,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",Neutral
Intel,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,Negative
Intel,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,Negative
Intel,Yes hah,Neutral
Intel,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",Negative
Intel,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",Neutral
Intel,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,Positive
Intel,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",Neutral
Intel,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",Neutral
Intel,Oh man the hair... its so crazy how much upscaling kills the hair..,Negative
Intel,i mean they already arent the smartest bulbs considering they went team green.,Neutral
Intel,Reading comrehension dude.,Neutral
Intel,"I do, but thanks for your interest.",Positive
Intel,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,Neutral
Intel,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",Positive
Intel,Nice. It’s on the list. Thanks man,Positive
Intel,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,Neutral
Intel,Probably because TAA is (unfortunately) more prevalent than it ever was.,Negative
Intel,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",Neutral
Intel,Oh my bad if I read that wrong,Negative
Intel,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,Negative
Intel,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,Negative
Intel,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",Positive
Intel,imagine being mad that 4 year old cards arent high end,Neutral
Intel,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,Negative
Intel,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",Neutral
Intel,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",Negative
Intel,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",Neutral
Intel,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",Negative
Intel,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",Neutral
Intel,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",Negative
Intel,"It's for textures, not object edges from FSR use, lol. Two completely different things",Neutral
Intel,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",Negative
Intel,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",Negative
Intel,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",Negative
Intel,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",Negative
Intel,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,Neutral
Intel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",Neutral
Intel,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,Neutral
Intel,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",Negative
Intel,5700 was probably the lowest AMD card they had to test with.,Neutral
Intel,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",Positive
Intel,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,Neutral
Intel,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,Positive
Intel,AMD CAS is aimed to restore detail,Neutral
Intel,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",Neutral
Intel,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,Negative
Intel,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",Neutral
Intel,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",Negative
Intel,Yeah but a 3060ti didn't,Neutral
Intel,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",Neutral
Intel,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",Positive
Intel,"If only Intel had stayed in the memory business!   They'd be enjoying Micron valuations and wild profits and performance from copackaged CPU+GPU+LPDDR of their own design and manufacture...     But no, they'd rather invest billions in buying donuts as a service, or whatever their crazy investements went into.",Negative
Intel,"damn an iGPU using 32GB of vRAM, I wonder if they're testing a Panther Lake laptop with 48GB RAM or even more (since X7 & X9 Panther Lake only accepts soldered memory)",Neutral
Intel,"If Intel is really about to release a B770, honestly the **only thing that could make it competitive is the price**. (FOR ME, competitive in 2026 means <400€) From a performance standpoint, it would need to undercut existing GPUs quite aggressively to make sense, especially given how crowded the mid-range already is.  That said, I’m pretty skeptical about how realistic that is. **With the recent RAM shortages and rising memory costs**, pricing a new card competitively while still keeping margins doesn’t sound easy at all. Memory is a huge part of the BOM, and we’ve already seen how shortages can push prices up across the board.  So unless Intel is willing to take a serious hit on margins (which seems unlikely), I’m not convinced the B770 will land at a price point that truly shakes up the market. Happy to be proven wrong, but for now the pricing question is the big unknown for me.",Neutral
Intel,So there's a 20GB variant. A 28GB variant and a 32 GB variant?,Neutral
Intel,Optane was practically **built** for the type of AI workloads that they're shoveling money at.  If Intel didn't give up literally only a matter of months before GPT released and the bubble began in earnest lol,Neutral
Intel,"If Intel stayed in memory business, it would be long dead in the 80s and killed by Japanese memory companies. CPU remains the top niche area with less competition and deeper moat. See how China has quickly come up with their GPU designs? Well it will take at least another decade for them to make 2nm CPUs",Neutral
Intel,"are people high or something? intel was losing money on optane and their SSD business became irrelevant the minute regular memory manufacturers slammed the market. don't get me wrong, they were some of the most durable on the market, but they were no where near printing money on the memory business.  optane may have survived if their nodes were on schedule, keeping CXL support on schedule, but not because it was profitable.",Negative
Intel,"Their iGPU's since at least Meteor Lake could use 50-75% of system ram, soldered or socketed.   Same issue as with Strix Halo where the iGPU is anemic enough to not be a real option.   Arc GPU's have a similar option that can use up to 50% system ram as expanded vram (with the obvious performance hit)",Neutral
Intel,If the b770 is 5060ti levels even €500 is competitive,Neutral
Intel,"it's not like any of this AI garbage right now is profitable for anyone except nvidia and the hardware companies anyway, it's not stopping everyone from shoveling money into it",Negative
Intel,"> With up to 192GB of VRAM across eight GPUs in a single system, Battlematrix positions itself as a relatively cost-effective alternative to other professional GPU ecosystems for AI inference workloads.",Neutral
Intel,Hope they do some image and video generation benchmarking as well. Nice to see someone testing AI rigs out there.,Positive
Intel,Wish they’d give prompt processing speeds. AI coding generates very few tokens compared to input. Nvidia seem to dominate here.,Neutral
Intel,"How many concurrent users will this serve, 30 devs would be nice",Neutral
Intel,:),Neutral
Intel,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Negative
Intel,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Neutral
Intel,I hope it comes to desktop CPUs,Positive
Intel,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Positive
Intel,Yeah this headline doesn't add up based on my own testing,Negative
Intel,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Negative
Intel,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Positive
Intel,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Positive
Intel,concur  some benchmarks are biased,Neutral
Intel,lateral,Neutral
Intel,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Neutral
Intel,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Neutral
Intel,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Positive
Intel,Answer to strix halo was the partnership with nvidia,Neutral
Intel,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Neutral
Intel,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Neutral
Intel,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Neutral
Intel,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Neutral
Intel,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Neutral
Intel,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Neutral
Intel,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Neutral
Intel,"should have a 3y warranty on it. submit an RMA ticket  regarding the actual query though, the silicon is the same the 14900ks is just a slightly better bin. you wouldn't notice the difference at stock let alone normalized for energy consumption",Neutral
Intel,"I have i9 14900ks, what I did is that I reset bios settings to optimized defaults and then I limit pl1 and pl2 to 150w and enabled XMP, these are the only two settings i changed, the rest is default, and temperatures are in check, i still get the same performance, and it’s very efficient in gaming that way, the extra heat and power consumption of 253 or 320 are not worth it, I recommend just get the ks and make these two changes and forget it.",Neutral
Intel,The performance difference will be tiny and definitely not noticeable with a 3090. Go for the cheaper chip.,Neutral
Intel,"Get the KS for better silicon quality only limit power , set pl1/2 253w and set it to 350 or 325A, definitely want the better 14900ks silicon quality it’s overall better and better IMC as well. It’s a better bin and typically only the best 14900k will run stable 6.2ghz and at lower voltages even if you limit your chip to 6ghz",Positive
Intel,If it doesn’t cost you extra then get the 14900ks and lock all the cores to 5.6 and power limit 256w,Neutral
Intel,"A 14900KS is nothing more then a binned 14900K. Running a 320W/400A extreme setting is not advisable with a AIO. I run my 14900KS on custom loop with 320W/307A performance setting and it does not thermal throttle at all. If you get lucky, you could get a 14900K that can run KS settings. Performance in that case is( should be) identical. Without benchmarks i can't really tell the difference between 125/253/307 and 320/320/400 except the heating of my room.",Negative
Intel,I also PL1/2 at 150. My temps stay under 60c when gaming.,Neutral
Intel,"if the cooling wasn't sufficient disable HT(useless for gaming) and undervolt it this lower CPU temperature by 20c, in games the CPU temperature should be around 65c.",Neutral
Intel,Im using a duel air tower for my 14900k game temps are at 60 to 70,Neutral
Intel,"As an update - I went ahead with the 14900ks and also changed my cooler to a 420mm AIO.   Ensured latest bios update then set Intel presets (performance) but also went ahead and reduced PL to 150w, set temp limits to 70c, system agent voltage to 1.12, 307A, and I was blown away by the temps!! I am getting basically identical performance (+ few fps) to my previous 13900ks, but a whole whopping 30°c cooler in game!!! I would average 80-85, now it’s sitting super chill with same in-game settings on BF6 & ARC at 50-60c.   Thank you everyone for your inputs, I sincerely appreciate it and I’m extremely happy with the outcome!",Positive
Intel,"I have the K version only because of the onboard gpu. In case my GPU gives issues and I'll still be able to boot. But otherwise there is almost no performance gain. I ran my i9-14900k pl on 320 watt and did a cinebench benchmark, temps were ok: average 94c, max 98c with a 360 aio.  That said, go for the cheaper version if you don't need onboard gpu.  Edit: I have my pl on 253w now. No need to go any higher.",Neutral
Intel,5 years warranty on 13 and 14gens now.    I have the 13900ks. Run it at 253w. Clock locked at 5.5ghz. Temp 80c and cinebench 23 39k,Neutral
Intel,"Thank you for the feedback. Forgive me for the dumb question; if I ran either a 14900ks or a 14900k at these settings, would they both have the same temps? Or would the KS still run hotter?",Neutral
Intel,This post is about the K and KS. Both have the same iGPU,Neutral
Intel,even better. i take it they extended tbe warranty period for those products?,Positive
Intel,For 5.5 39k in CB23 is a little low,Neutral
Intel,The ks should run cooler because of the better bin. Less voltage being required to hit certain frequency points.,Neutral
Intel,"Oh shit, I thought only the K had an igpu! Should have gone for the ks version lmao",Negative
Intel,Yes because of the degrading issue.,Neutral
Intel,Lol stock is 5.8ghz lol and most stock after the update get 35k,Neutral
Intel,All core cinebench is not 5.8... I get 39k stock what are you talking about lol,Neutral
Intel,Search on reddit on 13900-14900k.  After the code update stock most 13-14k can barely do 35k. Dont like to your ego brother.   So millions on reddit are getting those score and you are the special bin whose getting a higher score.   Mr 1 post and 7 comment history lollllllllllllllllllll,Negative
Intel,LOLOLOLOLOLOL HAHAHAAHAHAHAH ARE YOU DUMB? This really shows you don't have a 13900k or 14900k,Negative
Intel,Will be a interresting CES,Neutral
Intel,I'm half-expecting this to show up as a server-only AI-focused SKU with video outputs removed.,Neutral
Intel,Merry Christmas everyone,Neutral
Intel,"4070 performance for $400, I'm calling it. Would have been great if this had come out right after the wave of negative press that the 5070 received for only being 10-15% better than the 4070 with a mediocre 12 GB of VRAM, but I feel like Intel missed the boat again if the Steam Hardware survey is anything to go by, the 5070 has really made a comeback with recent sales.",Neutral
Intel,They can't even ship B60's.,Negative
Intel,"What is taking Intel so long?      It's already been almost a year after Battlemage's initial launch. And for what? RTX 5060 performance at the same price with some extra VRAM?  I had really hoped Intel would be able to gain ground on their competitors. At this rate, we'll get the ARC C770 to compete with the RTX 6060 in another 3 years.",Negative
Intel,Aren't they always,Neutral
Intel,give it some time...,Neutral
Intel,Merry Bitmas,Neutral
Intel,"4070 performance for... used 4070 price, now with driver issues and an objectively worse upscaler!  intel greatest hits",Negative
Intel,Sure they can if you search for it   B60  https://www.idealo.at/preisvergleich/OffersOfProduct/207972918_-arc-pro-b60-sparkle.html   Or b50 https://geizhals.at/intel-arc-pro-b50-a3584363.html,Neutral
Intel,"Intel's GPU division has been operating at a loss never mind Intel as a whole and ARC series cards aren't as popular as the enthusiast circles would have you believe. Coupled with how expensive R&D is for things like GPUs, it's hard for them to pump out a competitive product while remaining just profitable enough to undercut AMD and Nvidia.",Negative
Intel,What is taking Nividia so long with the super cards?,Neutral
Intel,"Battlemage gpu chips are made through TSMC and Intel is getting screwed on supply, this is why even if the B770 comes it will only be a small amount. Hopefully Intel can put together enough rare earth to pump out discrete Celestial Gpus but it takes time to ramp everything up. In addition Intel has their chiplet design, EMIB that could take off soon. They may be able to bring Apple back into the fold, but let us hope Discrete Arc lives on.  I have learned to not have expectations for anything that is outside my direct control, I do the best I can to just go with the flow. Whatever will be, will be.",Neutral
Intel,There's definitely been lame ones.,Negative
Intel,"I'm fully aware it's not a great deal, but that's my expectation when it comes to Arc.",Negative
Intel,"These are European links and will be out of stock. I found a mom and pop place back in my old stomping grounds in San Francisco, and they normally only sell B60s in prebuilt systems but a special order is possible.   Intel can't rely on TSMC for Battlemage supply, so let us pray that Discrete Celestial GPUs are made (entirely) at IFS and release in 2026 / 2027.   May your Bits Byte Hard, long live the Arc.",Neutral
Intel,"If CES 2026 comes and goes without any details for Discrete Arc GPUs then it could be awhile. The main thing being promoted is Panther Lake which should be made entirely at IFS, a step in the right direction. The TSMC monopoly is destroying the industry and it hurts companies here in the US.",Negative
Intel,Not sure how the link being European matters. They are European shops and the B60 is in stock.,Neutral
Intel,"We kind of know where it will land. It will be a 3050M level chip, maybe a bit better, but will have improve scaling and frame gen.   8060S is 40 RDNA 3.5 units. One Xe3 unit is about 2.1 RDNA 3.5 units. That put it at about 65% of Strix Halo, though it will have a worse memory bus and no MALL cache. Somewhere in that range.   So that 60% is almost bang on the 3050M. Maybe a bit better. It won’t be like the 4050 but 3050M isn’t bad for an iGPU that fits in a normal socket",Neutral
Intel,"> and no MALL cache  PTL does have 8MB of memory side cache, fwiw.",Neutral
Intel,"I don’t know how this score compares to the 3050M. I only know that this score is about 55% of the B580. And the B580, at 2K and 4K, is about 1.7–2 times the performance of the desktop 3050",Neutral
Intel,and also an iGPU won't be stuck with 6GB of vRAM 😅,Neutral
Intel,Really don't understand why they don't go with a larger cache.  Pretty sure they still have a bunch of cache chunks spread all around the SoC.,Negative
Intel,"in configs without die-to-die memory performance in general should be worse if bandwidth limited. despite not being specifically dedicated to onboard memory like Lunar, people are still planning configs with local LPDDR5x, though peak bandwidth is limited by a 128bit bus.",Negative
Intel,I see it personally as not really being at 20W if you’re asking much from the GPU. It’ll increase the juice dynamically if it gets demanding enough. So it’ll be hard to say unless you force the power limits way down manually.,Neutral
Intel,"I actually like the idea of discrete GPU naming scheme for the new iGPU, 300 series for integrated graphics is really makes sense but they should add 'M' suffix to make it clear.",Positive
Intel,"I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  Also looks like the 3\_8 and 3\_6 versions are differentiating maximum boost clocks, though I wonder if instead those may reflect that configurable upper TDP bound. Might make sense for 65W and 80W to be differentiated if that will coincide with anything about the ""experience-based"" PL1 behavior.",Neutral
Intel,There are 2 dies.  One for professional workload which will be mass produced.  Second is for gaming. Even a 10-12 core xe3 will be barely enough for modern 1080p. Lunarlake can only run alanwake at 1080p at low settings getting only 25 frames so even if this is 50-100% better this is the minimum for a 2026 product.   I see no point of a 8 xe3 core system when all people will do is just complain.,Negative
Intel,"quite confusing, Xe3 should start from C (celestial), if using name like B390  we think this  is a battlemage Card (Xe2)",Neutral
Intel,"I wonder if rumors about Zen 6 clocking way higher than current cpus turn to be true, and the 5.1ghz max on mobile PT mean Amd might have an edge in next generation   Only time will tell",Neutral
Intel,please add M for Mobile or i for iGPU  * Arc B390M Xe3 Graphics * Arc B390i iXe3 Graphics,Neutral
Intel,"A clock speed regression vs the prior gen on N3B, with a remark that it's difficult to cool, really isn't a good look for the process side. 18A branding with more like N4 performance...",Negative
Intel,Will the 10 core Xe be better than radeon 890m or worse?,Negative
Intel,"i mean i get this is a laptop part but man 16 threads is not much to phone home about when it comes to horsepower, isnt next gen desktop aiming for something like 48 threads?",Negative
Intel,Still weaker than x3d,Neutral
Intel,Yes indeed we need that M&M. Mobile platforms are not a priority for me and are dedicated mobile gpus really comparable to Big Boy Discrete GPUs? It is very confusing.  Lunar Lake laptops should fall in price. Has anyone used Lunar Lake and if so which models? Buying latest gen is for guinea pigs and the rich!,Negative
Intel,Doesn't the B already serve that purpose?,Neutral
Intel,">I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  The 10 Xe3 core model is a binned down 12 Xe3 N3 die, and I doubt yields are so bad that they would even be able to find more dies where they have to disable more cores.   The other die is the 4 Xe3 Intel 3 die, so you can't go up from there.",Neutral
Intel,To be fair Alan Wake 2 low settings look great. This was covered by DF awhile back they said in some ways Alan Wake 2s low settings look better than some modern games high.,Positive
Intel,"I mean, but that logic, most of Intel's historical bigger iGPUs don't make sense. There are use cases other than AAA gaming. Media creation is another big one.",Negative
Intel,"Xe3 is not GPU family name but GPU core architecture, it's like Nvidia Ampere, Ada Lovelace. But Alchemist, Battlemage, Celestial is GPU family name.   Panther Lake 12Xe3 being B series GPU makes sense because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture. Intel confirmed Celestial will have Xe3P.",Neutral
Intel,"Think of it like AMD Zen X+ nodes. Ryzen 8000 is more or less a laptop only APU series on Zen 4+.   Xe3 is a half-generation, it doesn't get the letter upgrade to C, but it gets the 3, signifying a new architecture, but not a new generation.  Zen 4+ is a half-generation, it doesn't get the number upgrade to 5, but it gets to be 8000-series, signifying a new architecture, but not a new generation.",Neutral
Intel,"Mobile Zen 6 is likely to come around the same time NVL does. Both should use N2 and will presumably have similar frequencies, well above any 18A parts.",Neutral
Intel,"Considering the timing of 2nm, zen 6 would be around late 2026, and mobile zen 6 late 2027 wide availability. for whatever reason amd takes forever despite high mobile demand, but this quarter it looks like it worked out for them (maybe a big bump up from x3d sales).",Neutral
Intel,What're you doing on a laptop?,Neutral
Intel,"These are for thin and light office notebooks and light gaming. Think Lunar Lake. For CPU power, Nova Lake H will exist.",Neutral
Intel,It's for handhelds and office laptops not hyper enthusiast shit.,Negative
Intel,Nobody buys AMD laptops,Neutral
Intel,">Still weaker than x3d   Source?? Also Panther Lake is H series only, HX will be based on Nova Lake.",Negative
Intel,Yeah so it is weaker for gaming with a dGPU than the 0.2% of laptops currently sold that have either a 7945hx3d or 9955hx3d that makes up for less than 0.1% of all laptop users. What's your point?,Negative
Intel,"While I haven't used it daily or anything, and I've only done initial setup on the Lunar Lake, the feedback we've gotten both on Arrow Lake and for Lunar Lake (e.g. 268V and 265H) Dell models is that it's a big increase in battery life and performance. The integrated graphics (e.g. 140V and 140T) are very capable compared to a **workstation** grade NVIDIA Ada 500 GPU, but they are not even comparable to a gaming GPU like the GeForce 4060 or even a 5050.  The integrated graphics do however get used for 99% of all workloads unless explicitly specified because they are vastly more battery efficient and draw less power compared to a dedicated NVIDIA chip, meaning you can have a much smaller external power supply, and your graphics performance in those basic desktop workloads with one of these chipsets will be **much** better than previous generation Intel chips. Exceptions are obviously something like gaming or AutoCAD that specify to use the high performance dedicated graphics chip.  140V/140T are barely functional for modern AAA gaming, but if you stay 5-10 years back for AAA titles you might be okay. It will smoke most Indie games. Just look at the per title benchmarks for a 140V/140T and you can see if your game benches. You could probably get away with a lot of functional mobile gaming without a dGPU, but I wouldn't expect to be able to play a recent Call of Duty or Black Myth or anything with anything like an acceptable framerate at a decent resolution. This integrated graphics chip compares very favorably to its more common Ryzen 7 equivalent, I believe the 780M, and it's a very good APU for handhelds overall due Lunar Lake's power efficiency compared to other X86 chips.  You have to understand that for these next two generations Intel seems to be making big strides in terms of both power efficiency and integrated graphics for mobile, it's a very attractive option and the first time I've seriously considered a laptop without a dGPU. I think Panther Lake is going to be a very nice kit next year for both laptops and handhelds and give AMD a run for its money.   I suspect AMD genuinely needs a new APU graphics architecture implemented next year to keep up, which I expect them to. Not a bad problem to have.",Positive
Intel,"Yeah for sure. It's a small die and should be yielding pretty high. See also the number of 4+8+4 SKUs. Looks like the larger CPU tile is also yielding decently, so not a ton to cut down.  I'm partly saying that because a larger Intel3 die was certainly possible. Even if it was 6 Xe3 cores and built as half of the larger die (just one of the two render slices)  it would fill the void a bit more.",Positive
Intel,Which is rather silly. They should've named celestial Xe3 and the current Xe3 as Xe2P,Neutral
Intel,"> But Alchemist, Battlemage, Celestial is GPU family name.  Specifically, *discrete* GPU family name.   > because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture  That is simply not true. Xe3 brings much bigger changes over Xe2 than Xe3p does over Xe3. That's why they were named that way.   > Intel confirmed Celestial will have Xe3P.  No, they actually haven't said anything about Celestial (again, as a dGPU) at all. They said that C-series naming (i.e. NVL iGPU) will start with Xe3p.",Neutral
Intel,Highly immersive porn on the go,Neutral
Intel,"Idk, FEM sim of a pressure boiler?",Neutral
Intel,These are H series chips. Even the U series chips don't go down to LNL min power levels.,Neutral
Intel,"NVL-H is 400 series, *replacing* this, next year. Not supplementing this lineup.   Adding more cores won't do anything for gaming.",Negative
Intel,PTL extends up to the -H series too.,Neutral
Intel,"HX this year will still be Arrow Lake.   Nova Lake will be a full line up, with S, U, H, and HX, but end of 2026 / early 2027",Neutral
Intel,">Source??  You can't seriously be asking for a source for whether or not this part will be able to power dGPU gaming laptops better than X3D chips.   >Also Panther Lake is H series only, HX will be based on Nova Lake.  Not till late next year or early 2027. It's all arrow lake till then.",Negative
Intel,> Source?   Common sense suffices. It's a tick core with a clock speed regression at that.,Neutral
Intel,"Surely a cost decision. The 4Xe die, including the choice of Intel 3, is supposed to be the cheapest thing to deliver an acceptable mainstream PC experience. They need PTL to be a proper volume runner and start displacing the RPL that's still a large chunk of sales. WLC should hopefully finish the job.",Neutral
Intel,Just get a Vision Pro?,Neutral
Intel,"Tbh, more cores would just make that go faster, but 16 would already be plenty. Especially for something like that where it's probably going to be a linear analysis and ram constrained if they actually modelled the gas (which would not necessarily be required).",Neutral
Intel,Lmao this is funny we both responded to the same comments with the same things within like 2 minutes of each other.,Negative
Intel,I don't see anything wrong with asking for actual benchmark information especially when there isn't anything official. X3D is nice but it isn't the end all be all. I would be curious to see if Intel can manage to compete.,Neutral
Intel,"Oh I totally agree, but it would've been nice you know? Jumping to 6 Xe3 is a  significant area increase for a tiny tile. I understand exactly why the 4-10 gap exists, but I can't say I don't wish there was something to fill that gap if only because it looks weird.  I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.",Neutral
Intel,"PTL's main changes are fixing MTL/ARL's terrible SoC design, which should net a few % performance. It'll see a mild IPC increase, getting a few more % performance. And it'll lose a bit of clockspeed, erasing most of those gains.  Expect PTL to be very similar performance to ARL, but with lower power consumption, a much better iGPU, and most importantly to Intel: Using their own fabs instead of TSMC.  It absolutely won't be X3D in gaming.  Edit: Actually shocked that people think this would compete with X3D.  9955HX3D is \~16% faster than a 275HX in gaming...and a 275HX itself is easily 10%+ faster than a 285H in gaming.  Not even Intel themselves are claiming this. Their own marketing refers to PTL as ""ARL performance with LNL efficiency"". Nobody realistically expects PTL-H to see a 25%+ gaming improvement over ARL-H. The fact that IPC increase is less than 10% and clockspeed is slightly lower than ARL-H should make this obvious",Neutral
Intel,CGC is a LNC tick. This is well known at this point. And we see it's even a clock speed regression.    Even entertaining the notion it will close the gap to AMD's X3D chips is just delusional.,Neutral
Intel,"Oh, yeah, I get you. Wish they could give more granularity. Just personally think some sacrifices are worthwhile if it can condense Intel's mobile lineup back down to something sane again.   > I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.  Yeah, should be a good fit. Shame they don't have anything with a bit more CPU umph, though. 4+8+4 and only up to 5.1GHz is *fine*, but not great. Especially without an HX replacement.",Neutral
Intel,"Two options seems right, either you care about it or you don't.",Neutral
Intel,"Its not ""delusional"" to want to see actual numbers instead of speculation. I have been in this game long enough to see plenty of speculation even with accurate information not give the actual numbers.",Negative
Intel,"Given how well ARL HX was received in gaming laptops, I think they may wait to have something from NVL take that top spot. 5.1ghz does seem low though. ARL-H will happily do 5.4 and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP.  I suspect these may not be totally final clocks though they do seem reasonable.",Neutral
Intel,"Honestly, surprised ARL-HX is doing as ok as it is. The deficits of the architecture in gaming are well known. If it could hit the same clocks and core counts, PTL should look a lot better still. And all that besides, ARL's cost structure is horrible. For Intel's own sake, the sooner they move on, the better.   > and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP  From the same leaker, these chips are at 65W or even 80W TDPs, so they're not merely power limited. It seems that 18A just significantly underperform some expectations, though in line with some rumors and the gist of the revisions Intel's been making to its projections over the last year or two.  > I suspect these may not be totally final clocks though  If they're defining SKUs and such, these clocks need to be finalized for all practical purposes.",Negative
Intel,"If I'm reading correctly those are max power limits, not the TDP,  though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  As for why ARL HX is doing well in gaming laptops, I think a good bit of that is also part of what made it lackluster on desktop. It doesn't really scale up that well with higher TDPs and power limits, but it does seem to scale down. The 285HX with its 55W TDP and 160W max limit doesn't perform far off the 125W TDP and 250W max of the 285K.  It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system. The 9955HX3D is very impressive, but quite a lot of laptop buyers seem to value the ability to do more laptop-like things with their gaming laptops than the extra frame rate. I'm hoping this gets shaken up as AMD adopts new packaging tech as seen in Strix Halo.",Neutral
Intel,"> If I'm reading correctly those are max power limits, not the TDP, though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  Not guaranteed given it's just a twitter leak, but I'm assuming the leaker is using the term TDP consistently with Intel's historical usage, i.e. PL1. For ARL, 115W is PL2. I would also assume there wouldn't be the disclaimer about it being hard to cool if they cut the PL2 so much, though PowerVia does create some interesting complications there, so maybe not quite apples to apples.  Either way though, don't think it should have much impact on ST boost. You're talking a good 70%-ish of power going to compute, so even at 65W PL2, that's still 40-50W available for one core. Should be *easily* sufficient to hit whatever the silicon is capable of.  > It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system.  Yes, and this is something I've very much looking forward to with NVL-HX. At this point, the biggest demerit of the -HX platform vs -H is the use of standard DDR5 vs LPDDR. That's because it's still based on the desktop silicon with the different SoC/hub tile. But with NVL using a shared SoC die, they should be able to offer an -HX platform with the core counts people expect (though probably limited to single die 8+16), but the power/battery life advantages of -U/-P/-H. In general, should help make the -HX more of a straight-up upgrade than the tradeoffs one faces today.  AMD has this situation even worse today, because there's a much bigger gap between their desktop SoC architecture and the mobile one. Though as you say, they may also bring them closer together in the future.",Neutral
Intel,Try the shunt mod,Neutral
Intel,"Cool, errr...  icy",Neutral
Intel,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Neutral
Intel,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Positive
Intel,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Neutral
Intel,did you use dry ice? how did you hit sub-ambient?,Neutral
Intel,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Neutral
Intel,Are you in the US? If so how were you able to get Maxsun?,Neutral
Intel,Oh... for sure 😁,Positive
Intel,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Positive
Intel,Great work dude! Only 200MHz to go 😉,Positive
Intel,Car coolant in the freezer 😁,Neutral
Intel,That's the way! Let us all know the results.,Positive
Intel,I am in Australia.,Neutral
Intel,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Neutral
Intel,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Neutral
Intel,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Positive
Intel,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Neutral
Intel,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Neutral
Intel,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Negative
Intel,Oh! You put the car coolant to run through a freezer? Wow! Nice,Positive
Intel,But Core Ultra 255Hx is almost $150 more than Zen 4 8945HX on lenovo Legion. That price gap is enough to upgrade 5060m to 5070m.,Neutral
Intel,"Can I redeem the codes to my accounts on a different Intel system? I bought a B860 motherboard and an Ultra 5 245k, but I won't be building that system till Christmas. I'm currently running an 8700K on a Z370.",Neutral
Intel,Idk if I did it wrong but redeemed my cpu but not my arc card on the website. Couldn’t contact support because it kept throwing invalid captcha at me.,Negative
Intel,just purchased a laptop from micro center with a 275hx but don't know how I would redeem this specific offer as I only get the one that lets you pick 1 of 4 games. Does mine not qualify/is micro center not participating?,Negative
Intel,"nope, you need to have installed Ultra processor to get promo game, because Intel used software to check it",Neutral
Intel,I really would like to know how it will compare to AMD Ryzen AI MAX+ 395 (Strix Halo) APU?,Neutral
Intel,That naming scheme really is complete and utter dogshit,Neutral
Intel,Sounds like fooz will be getting a taste soon? Christmas or Q1 2026?,Neutral
Intel,Can we just toss random darts at tech words and numbers to assign a different naming scheme to each and every different Intel Product?,Neutral
Intel,I'm looking forward to check how those series will perform!!,Positive
Intel,"This isn't a ""halo"" product. An upgrade to Arrow Lake/Lunar Lake when it comes to iGPU, but not Strix Halo. It won't cost as much as well.",Neutral
Intel,look forward to new APUs,Positive
Intel,Wait for Nova Lake AX. It will have 48 GPU cores instead of 4/8/12.,Neutral
Intel,Likely will be worse since it has only 1/3 the power usage and significantly smaller chip size.,Negative
Intel,"It's meant to confuse you in purpose, so you ignore it and go by the 3/5/7/9 scheme. Marketing success is dependent on the company leading the customers to the way they want it. So it needs to be complex and confusing.",Negative
Intel,"also why is everything in a lake. like really that's the last place you want your chips to be.  and why is it 255k, 285k and so on instead of just 250k and 280k.  and why is it arc xe3 and not just xe3 ahh  so many weird conventions really. no consistency at all. just pure confusion.  apple and nvidia get it right too, it's really not that hard.",Negative
Intel,"'Strix Halo' is lees about being a halo product, because it’s just an AMD internal naming scheme for the set of specific APU products. APUs that will follow 'Strix Halo' will be called 'Gorgon Point'. Since 'Strix Halo' is already out and the Panther Lake in question is about to come out it might be better to compared it to the next AMD 'Gorgon Point' APU line? But I still think it can be compared to 'Strix Halo'. At least I’m looking forward to see the comparison.  Edit: changed 'Strix Point' into 'Gorgon Point' since that’s what I meant.",Neutral
Intel,You’ll be waiting till 2027 on the amd side.,Neutral
Intel,All halo iGPUs are way too overpriced. Even regular iGPUs are overpriced going into $1K laptops.   And if you want to spend that money you can do it today with Strix Halo.,Negative
Intel,"Lesser power usage doesn’t always equals to performance losses.  See the Apple Axx SOCs and the Qualcomm snapdragon SOCs.  Even with the x86-CPUs over time they had more performance gains while maintaining almost the same power consumption.  Not anymore, but they could catch up with better chip design?",Neutral
Intel,>also why is everything in a lake. like really that's the last place you want your chips to be.  Easier to cool them,Negative
Intel,"Stop trying to find meaning in the naming scheme. Patience grasshopper, all will be revealed in time.",Negative
Intel,Like how is every chip company so bad in naming stuff?  Intel names things from geographical stuff to avoid politics etc. So I get the lakes  The numbers I dont.,Negative
Intel,"> APUs that will follow 'Strix Halo' will be called 'Strix Point'.  What? No, that's not what any of these terms mean. The Strix lineup is already out. ""Strix Halo"" is literally their halo, big compute/iGPU product.  PTL will compete with normal Strix Point and its refresh next year.",Neutral
Intel,Really? This should be good for Intel in 2026.,Positive
Intel,These Halo iGPUs are meant for LLM first and foremost. If you just want to game just get a normal RTX laptop.,Neutral
Intel,They are like 20% more efficient.  You are talking about 200% more efficient if it hits the same performance.    They might as well make it a mobile chip if that is how good it is. Would be great on phones.  Would be better than most pcs right now on 5W envolope.,Positive
Intel,"It's intentional, and non specific. So most go by the 3/5/7/9 naming. In order for them to have max profit, they need to lead you to the chips they want you to buy.",Neutral
Intel,'Strix Halo' is a product line as 'Gorgon Point' is also/another product line. Both have different variants.  In the case of 'Strix Halo'  [Check](https://hothardware.com/news/amd-strix-halo-cpu-rumors)  Find the official lineup here  [AMD](https://www.amd.com/content/dam/amd/en/documents/partner-hub/ryzen/ryzen-consumer-master-quick-reference-competitive.pdf)  Edit: changed 'Strix Point' to 'Gorgon Point' since that what I meant.,Neutral
Intel,👍,Neutral
Intel,"Yes, Strix Point lives *alongside* Strix Halo. Strix Point is already out, and came out *before* Strix Halo at that. It is not the followup to Strix Halo.",Neutral
Intel,You are right. I meant 'Gorgon Point'.,Neutral
Intel,"Gorgon point is just a refresh of strix point. These terms mean less in the next gen anyway as the follow on products have quite different names. Medusa point isn't really the successor to gorgon point, for example.",Neutral
Intel,My brain hurts and I’m still confused,Negative
Intel,That's why you have guys that are completely ChatGPT-levels of confident when they are completely out of whack with the info lol. No worries.,Negative
Intel,"I think him saying ""unreleased products"" could mean it's still coming.",Neutral
Intel,I think B770 is locked in and to release shortly isn't it? sure I saw a leak of packaging details etc.,Neutral
Intel,"We finally got reviews for the B50 and it is a SFF gem! That said… I wish Intel would get better at promoting upcoming products, they seem to be slowed down by the restructuring.  I’m going with the flow, whatever will be will be. I’m waiting for Dividends to kick back in so I can retire with Intel.",Positive
Intel,"Boy, I am glad they picked an easy naming convention.   /s  After Xe, Xe1, Xe2 no doubt next would be Xe3, then Xe3p  After A-series, B-series, no doubt next would be C-series.    Too bad those generations do not line up",Positive
Intel,Intel is not serious with dGPUs,Neutral
Intel,"If it was coming, you expect them to talk about the future even a little bit. Instead, nothing.    They cancelled Celestial over a year ago now. Sounds like things haven't improved since.",Negative
Intel,"I think him saying he doesn't talk about unreleased products is 100% bullshit. He talks about upcoming tech constantly. This is not just unreleased, it's unannounced, unclaimed and nonexistent outside of pure speculation.",Negative
Intel,B50 is interesting once the software is there (planned Q4).,Positive
Intel,"Yes, they are literally just playing. It's all a game lol",Neutral
Intel,"They have released about the same number recently as AMD, I'd say they're pretty serious. The B50 is a pretty compelling product too, big features for the price.",Positive
Intel,They are as serious as AMD.,Neutral
Intel,This interview happened during the quiet period so I don't think he could talk about the future,Negative
Intel,"They cancelled Celestial over a year ago now. Sounds like things haven't improved since.   MLID people showing themselves.  Tom Peterson previously said Celestial discrete hardware was already done and they were working on Druid. So if they cancelled it, it must have really sucked.  I am sure after they release Celestial 70 series beater, MLID will come and say they cancelled the 90 series beater and it's for sure dead from now on!",Negative
Intel,Where did you hear that it was cancelled?,Neutral
Intel,"We are supposed to get B60s also but they will likely be very limited and part of Battlematrix. Intel is moving very slow with Pro and Consumer GPUs and they can’t rely on TSMC for supply and obviously are not ready to manufacture through IFS? We are getting left in the dark, all we can do is wait.",Negative
Intel,There has been no update regarding celestial dGPUs internally.,Negative
Intel,AMD is skipping a generation to focus on the next. Intel has lost its focus on GPUs. These are not the same things.,Negative
Intel,Lol no. ARC was 0 margin product. Now it's fate depends on the whims of VPs not engineers.,Negative
Intel,"You could likewise point not that there was no word about dGPUs in the PTL presentation either. I think people need to accept that it's just not happening, at least for the foreseeable future.",Negative
Intel,"> MLID people showing themselves.  I'm not getting this from MLID.  > Tom Peterson previously said Celestial discrete hardware was already done  No, that's absolutely false. Actually watch the interview instead of reading reddit comments. He said Xe3 (specifically in PTL), not Celestial, was done. And this was after the PTL tapeout was announced, so that didn't even tell us anything new.   And as we now know, they don't consider that even in the same family as what would be Celestial.",Negative
Intel,Ex-Intel coworkers/acquaintances.,Neutral
Intel,>There has been no update regarding celestial dGPUs internally.  Do you have internal information?,Negative
Intel,Why would they? Battlemage is not even finished. Battlemage is not even 1 year old yet. They will still release B7XX gpus and probably B3XX.  I expect them to tallk about celestial by next year.,Negative
Intel,> Intel has lost its focus on GPUs  So despite them repeatedly telling you they have not... they have?,Neutral
Intel,"Battle mage is not a 0 margin product...  I know how much silicon cost etc due to my job. Believe me there is at least %30 gross margin in Battlemage and that is assuming somehow Intel got a worse price compared to my small ass company.     It's not profitable due to amount of R&D it takes to develop it, Intel earns a significant chunk for each Battlemage sold. They are simply not as greedy as Nvidia and AMD to earn market share.",Negative
Intel,Dude XE3 even has some test shipment reports etc. It's too late to cancel.   Sure if it's not good maybe we will only see B580 replacement.    But it's literally impossible and stupid to cancel it right now. Especially given how much gross profit they made from B580,Negative
Intel,"So, no news story has come out stating that?",Neutral
Intel,Xe3P-HPM suggests otherwise,Neutral
Intel,"Yes, through my ex colleagues",Neutral
Intel,"No, I don't listen to them, they have a nasty habit of downplaying bad situations. I'm going by their actions.",Negative
Intel,"> Dude XE3 even has some test shipment reports etc  Celestial wasn't base Xe3, and didn't tape out before cancellation. What test shipments are you referring to? PTL?  Btw, they still aren't saying anything about BMG G31, and that was much further along than Celestial was.   > But it's literally impossible and stupid to cancel it right now.  You can cancel a product at any point before it's released. Anything else would be sunk cost fallacy. Surely you're aware of the massive budget cuts and layoffs they've announced. Not everything can survive.   > Especially given how much gross profit they made from B580  By all reports, BMG still wasn't profitable for them. Hell, even if it *was* profitable, doesn't mean profitable *enough* for Intel to keep funding it in this environment. They're prioritizing spending reduction, not profit maximization.",Negative
Intel,Why would Intel tell you? It would just stall selling all current ARC cards.,Negative
Intel,No. Or at least not from any reliable source. Obviously discounting MLID and his ilk.,Negative
Intel,What about it? That some reference exists in drivers?,Neutral
Intel,"You don't even know that?   Lip Bu has been hiring gpu designers not firing them.   Most of the Cuts are from foundry side and slightly from gaudi side.    Intel if anything is focusing on gpus to create AI inference gpus.    Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.      You should check your sources, Lip Bu would cut 14A before he cuts Inference gpu side.",Neutral
Intel,BMG was 0 margin product,Neutral
Intel,"Of which are recently implemented, as in 'in the past week' which would be exceptionally stupid to do for a cancelled project",Negative
Intel,"> You don't even know that?  What do you claim I do not know?  > Lip Bu has been hiring gpu designers not firing them.  Celestial was cancelled under Gelsinger, as well as several rounds of client GPU layoffs. If Lip Bu is hiring anyone, it's not to build the team back up again.   > Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.   I am specifically talking about client graphics, yes. There's shared work, and arguably should have been more, but they were quite different. Client iGPU, client dGPU, and server dGPU were basically all separate SoC designs.",Negative
Intel,They're also using Xe3p for NVL-P and that Island AI product.,Neutral
Intel,"It wasn't though?  As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  I don't believe there is nothing Intel can do convince you. I heard these news every single time.     Also just look at job listings, there is many for gpu development.",Negative
Intel,"Which means it is being used, produced, and cannot be disqualified yet, nor does anything, not one trustworthy source, show it is cancelled",Negative
Intel,"> As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  Go ahead and point out where *I* said ACM or BMG were cancelled. I have no idea who ""you guys"" are, nor do I care what others are or are not saying.  > I don't believe there is nothing Intel can do convince you.  Well yes, if they cancel a project, and I know they have, I'll say they cancelled it. The way Intel can convince me to say otherwise is by... not cancelling projects. And you *do* realize they haven't talked about client dGPUs past BMG in many, many months, right? It's not like Intel's really denying anything.  I'm not sure what you're looking for here. Should I lie and pretend not to know what I do? To what end? It's not like Intel's harmed by me saying they cancelled such a project. That's not something you can keep a secret indefinitely, and anyone who *really* cares already knows.  > Also just look at job listings, there is many for gpu development.  If you lay off 10 people, 5 more leave on their own, and then you backfill 4, that's still a net loss. They still need some people, but not as many as they had, and not for client dGPU.",Negative
Intel,"Intel have not claimed there to be upcoming Celestial, or more Battlemage or anything like it. How did their silence convince you of anything?",Negative
Intel,Actually after the Nvidia deal there's definite reason for Intel to cancel future ARC development.  They have a partner that makes better GPU than them. Why would they continue? If the Nvidia deal is successful I expect even their iGPU will disappear.  In no sense it makes sense to develop a line that's redundant with what your partner is doing.,Negative
Intel,"> Which means it is being used, produced, and cannot be disqualified yet   Again, Xe3p lives for various things. Celestial, the client dGPU planned to be based on that IP, is dead. I'm not sure what contradiction you think exists here.    > nor does anything, not one trustworthy source, show it is cancelled   Intel's refusal to talk about future gen client dGPUs and the mass layoffs in the hardware team don't tell you anything? Or what about Gelsinger's remarks about less focus on dGPU?    This is how Intel usually handles cancellations btw. They simply pretend it never existed, unless investors really demand to know. Even then they like to delay the acknowledgement.",Negative
Intel,Think pretty easy naming. Any X infront just automatically means better iGPU,Positive
Intel,"Looks pretty snappy at \~75% faster than the top-end LNL chipset and this is with pre-release drivers.  Granted TDP is probably higher.  If they get the drivers cleaned up then it might release a bit higher which makes it a viable (though still materially weaker) thin and light XX50 dGPU model alternative some some of the market.  Should do well addressing the 'I want a thin and light laptop, but I want it to have an ok GPU' crowd.",Negative
Intel,"Panther Lake 12 Xe3 performance looks great to match RTX 3050 laptop performance because the entire chip only draw half power of RTX 3050. Seems like using 18A BPD really paid of to reduce CPU consumption by a lot with the helps of Tsmc N3E for iGPU.    Also it's so weird to see Asus Zephyrus G14 with Intel chip, usually it always has Amd CPU paired with Nvidia GPU. I heard G14 is pretty popular gaming laptop but this laptop during full load can use 120w+ power.    Using Panther Lake 12 Xe3 will makes this laptop looks even more appealing because it reduces power requirements from 120w+ to 45w but still giving about the same GPU performance which is insane. This is a massive game changer for people who use laptop as portable gaming machine and to those who travel a lot. I can totally understand why Asus this time use Panther Lake for G14.",Positive
Intel,"Nice to see the G14 with an Intel CPU. Thought the lineup was AMD only tbh, while the larger G16 laptops get Intel.",Positive
Intel,Naming for these chips are terrible,Negative
Intel,"Can Asus send me this laptop for review? I have 14 followers on snoozetube and 60% are probably bots, but bots are human too?",Neutral
Intel,Can't wait for 14inch laptops with actually good battery life and convenience than the cheap gaming laptops it's going to kill,Positive
Intel,"Nah, should've kept that info at the end like every other Intel and AMD CPU ever made.  But otherwise this branding really feels like AMDs APU line, where they had to emphasize their iGPU was better than average.",Neutral
Intel,"The possibility of being nearly 100% faster than Lunar Lake in some tasks, and minimum possibly 50% faster while being able to fit it into a sub 3lb/1.5kg design with a 80+wH battery is going to really nice. If the 4 LP-e cores scheduling work well and maybe a more efficient OLED panel you could easily get true 24 hrs use on x86",Positive
Intel,"If the game could be 50–60% stronger, that would be That would be a killer",Neutral
Intel,Its GPU part isn’t 18A at all — it’s actually N3E and 4Xe3 integrated graphics use Intel 3.,Neutral
Intel,"That’s been true for the past generations, but it looks like it will change this generation",Neutral
Intel,Still better than Ryzen 365 AI pro MAX+,Positive
Intel,"I disagree, GPU focused = X (like Xe3). Just takes getting used to , but otherwise it follows the same 3 7 9 scheme that probably didn't make much sense at first either :)",Neutral
Intel,https://browser.geekbench.com/v6/compute/compare/5050048?baseline=4771132,Neutral
Intel,The typical consumer doesn't know anything about the last letter. Having it in front will be much more successful to communicate to consumers the difference.,Neutral
Intel,Yea putting ai the model name is disgusting 😂,Neutral
Intel,I can't wait for the Ryzen 688S AI Pro MAX+++,Neutral
Intel,"I agree with this and now snoozetube creators are doing 128gb reviews for the 365 AI Pro Max+ and glossing over the fact that it costs decent money but lacks any kind of power when compared to discrete GPUs.  Amd continues to pump out expensive APUs that are mediocre, while doing everything related to Radeon half heartedly.  Why is that?",Negative
Intel,"Nerds argue over names for tech products but will eventually figure out some kind of logic in why they named it that way. Entire generations need to be released and compared.   As for average users they will always be perpetually clueless and unfortunately will become influenced by an influencer with no integrity and or a store associate who has been trained on scripts that make the most money for the store.  God help us all, I pray for Jesus - just like Pat Gelsinger, who will get no credit for the Intel turn around.",Negative
Intel,Ryzen metaverse Ai max++ 3D Hypercache macroboost,Neutral
Intel,I think it's rather on point. the 395+ is a beast for running large MoE AI models. It's value for money in that respect is almost unbeatable.,Positive
Intel,you forgot the x3dx2    when both cpu tiles are stacked on 3d cache tiles.,Neutral
Intel,Ultra TypeR S-line AMG M Bi-Turbo CCXR LM Harley Davidson Edition,Neutral
Intel,"Very very few know anything more than that, usually completely unaware that there's a whole SKU number after that.  How many times do you hear stories about some user proudly boasting about having an i7, only to find out that it's like a 6th gen, and they don't even realize / believe that something like a i3-12100 is actually a better CPU.     The average user understands the difference between, say, a Core Ultra 5 and 7, because the ideal of 3, 5, 7, 9 being product tiers exist in plenty of industries, like BMW's product line. Bigger number = more performance. How? By how much? No clue to them.  So since the average user is going based off just the name 5, 7, or 9, having that X visible in a location they'll see is certainly very important. They'll notice the X.",Neutral
Intel,"It would be like a Chromebook named ""Chromebook CloudCompute+"" just because that's what those are built for",Neutral
Intel,I personally prefer them Name it Ryzen 3 / 5 / 7 / 9. It’s easy to understand and easy to compare to intels naming but sadly both companies have ruined it now.,Negative
Intel,That's honestly sounds even more cringe. Can you imagine Amd Ryzen 9 395X3DX2 AI Pro Max+? That's ridiculously bad LMAO,Negative
Intel,BMW Individual M760i xDrive Model V12 Excellence THE NEXT 100 YEARS,Neutral
Intel,I actually did my research and found out that core ultra 5 125u is not much different from core ultra 7 155u... Ended up buying ProBook with core ultra 5 125u and saved money for upgrading the ram and SSD,Neutral
Intel,"but i think its gonna happen aye,  i wonder if the RAM bandwidth needs for AI benefit from cache like games do, or are they better slapping more ram channels on it...",Neutral
Intel,"Yeah, all of the U chips within a generation are the same physical chip, just different bins (usually tiny clockspeed differences). I don't think they even have core count differences any more for the most part.",Neutral
Intel,Same core counts too,Neutral
Intel,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
Intel,"To summarize from the article for some folks:  6300 puts this GPU as 33.4% faster than the 140V, 71.3% faster than the 890M, and 54.3% of the 8060S. Just over half of Strix Halo's top config.  Bear in mind though, that this benchmark favors ARC GPUs compared to gaming results. The 140V and 890M are roughly equal and this benchmark puts the 140V as 28.4% faster.",Neutral
Intel,"Ugh, who is naming these products?!?!? Between the internal code names (which are now publicly used) and the actual product names, it's a mess. As bad as monitor naming.",Negative
Intel,"Sick, Panther actually sounds good, and Lunar/Arrow on mobile already sounded good to me. Please keep pumping those iGPU numbers Intel Arc engineer bros.  AutoCAD on integrated Intel graphics WHOOOOOO  AMD step it up next gen please, Intel is no longer a static target in graphics, kthx.",Positive
Intel,This sound more realistic and not as good as the 50% better Intel announced.   For Intels sake I am hoping the 50% is real,Negative
Intel,... with early drivers and probably no decent support yet. Its also probably an engineering sample? 😉,Negative
Intel,Wow pantherlake is looking enticing. Should I wait for novalake? Will novalake have same battery life like lunar lake or pantherlake?,Positive
Intel,"It's going to cost like $10,000, right?",Neutral
Intel,"The top lunar lake has 8Xe2 cores, and this has 12Xe3 cores. 30% faster seems… bad? Like why not 50%+ given 50% more cores and architectural improvements?",Negative
Intel,"We have seen how Lunar Lake on MSI Claw 8 AI+ performs on early firmware and drivers, Intel even managed to improve Lunar Lake performance up to 30% after that. I expect 50% performance improvement on Panther Lake is very possible.",Negative
Intel,seee ceerto seeeh,Neutral
Intel,"It really is a mess, someone should've been fired long ago.",Negative
Intel,"The only thing they want you to care about is 3/5/7/9.  If the naming is too clear, then they can't steer you to the higher margin/profit variants. At some level it needs to be unclear. Not defending them, but how things are.",Negative
Intel,Intel wasn't a static target since the 10th gen. They've been pushing (at least in terms of performance) since then. It's mostly the efficiency which stepped back. And they messed up their current lineup in terms of performance and pricing but they've improved for sure.  Next gen seems good for them though. Hope they price it well. AMD isn't zen 1/2 either. They're bumping core numbers too and introducing some new cores as well.,Neutral
Intel,"I mean, why?  Is AMD going to be coming out with anything more powerful than the HX 370 in that power range?",Neutral
Intel,because 50% more cores need 50% more power for 50% more performance.,Neutral
Intel,"There are so many variables here that we have no way of accurately saying that.  It's possible that Xe3 is simply treated more like RDNA in this benchmark, and that the Xe2 140V is unfairly biased towards in that way. If that is the case, the 890M and 8060S may be better metrics to base off, and we do see over 50% gains for a 50% wider GPU than the 890M.  It's also possible this is a power-limited scenario. Like for like on TDP, this would be a solid improvement given the GPU both has to be moved off-tile compared to Lunar Lake, and has twice as many CPU cores to fight for power.  This could be pre-release drivers not getting the true 100% out of the hardware or silicon with non-final clocks still being tuned. The point is there's no way to know for sure.",Neutral
Intel,I don’t understand why you people continue to use that MSI claw to demonstrate “intel improved lunar lake performance post launch” when there are millions of lunar lake laptops that never had performance issue of that one handheld.,Negative
Intel,"they're keeping their jobs because this is technically better than the past. back in the day you'd have model numbers exclusive to a retail store, much less specific to an OEM, because businesses wanted to feel like they got a bespoke deal.   These days the SKU naming is mostly for accounting purposes, while the ""real"" naming decisions are made by the OEMs. basically most people are buying the Thinkpad/Yoga/ROG ""brand"" rather than the specific processor model, which only a much smaller crowd bothers to comprehend. It's like how people buy the Steamdeck rather than whatever APU is in there, which is fairly old at this point.",Neutral
Intel,"Energy efficiency is huge in mobile but I was explicit in saying they were no longer a static target in graphics, where they were not making significant or impressive gains in iGPUs for many years, it seemed. Now they are making one of the best iGPUs on the market.",Positive
Intel,"Possibly yes.   Current AMD handheld chips are better than Lunar Lake performance. Assuming normal cadence. Next generation would be similar performance to Panther Lake.     I was hoping more of a leap frog, rather than similar performance. %50 would be a very clear edge.",Positive
Intel,Is the TDP fixed to be the same between LL and PTL in this test?  I genuinely don't know.,Neutral
Intel,But LNL's TDP is too low compared to H45 cpu,Negative
Intel,"> because 50% more cores need 50% more power for 50% more performance.  insightful, and easy to forget given feature bragging    :)",Neutral
Intel,FTFY: because 50% more cores need 50% more power for 35-40% more performance.  Cause it don't scale linearly.,Neutral
Intel,"Mostly because people with LNL laptops are less likely to games since gaming are not the point of those laptops -> less testing, whereas the Claw 8 AI+ is a gaming PC handheld, so it is mostly use for gaming purpose and thus have more people testing for its performance.",Neutral
Intel,"As weird as it sounds actually Lunar Lake improvement mainly comes from MSI Claw not laptop, that because majority people who use MSI Claw give feedback the most which is why Intel focusing on the handheld first then laptop.    Intel even use Claw as benchmark for Lunar Lake compared to laptop. You can read from this article :  https://arstechnica.com/gadgets/2025/04/intel-says-its-rolling-out-laptop-gpu-drivers-with-10-to-25-better-performance/   Also Claw got BIOS update way faster than any laptop with the same chip so it helps Intel to mitigate power and boost behavior to maximize the performance. There is so many bug reports on Arc forum, most of them are Claw users, that's why laptop got benefits too.",Positive
Intel,AMD isn’t coming out with a better next gen iGPU for mobile (Gorgon Point) since it’s a simple refresh. Same arch with same CU count based on rumors.  The generation after will be competing with Nova Lake,Negative
Intel,"Amd handheld with Z2E isn't better than Intel Lunar Lake, you can see the comparison on MSI Claw sub or even on youtube. Z2E in most game is 10% slower than Core Ultra 7 258V, it only won in the game where Intel GPU performs bad.    Not to mention at 17w Z2E losing badly to 258V, Intel is on their own league, it's not even competition for Amd.",Negative
Intel,Intel is gonna have better integrated graphics than AMD,Positive
Intel,I have no idea.,Neutral
Intel,"We don't know what TDP was run here, and both MTL amd ARL H have been 28W outside of the Ultra 9 SKUs. It's entirely possible this was run at 28W, which is also in reach of Lunar Lake's boost envelope.",Neutral
Intel,The lunar lake laptops are tested by reviewers all the same as the strix point ones. Regardless it’s misleading because it’s not “lunar lake” but rather the performance profile/boost behaviour of that specific MSI CLAW that was changed and people act like it’s lunar lake’s drivers doing “30% magic”.   It’s not.,Negative
Intel,"And is there documented so-called “large” performance improvements on LNL systems that already performed as expected on day one (it tied 890m on high power and was always better at low power, talking about real games not 3dmark)? Or was it only bringing the Claw back to where LNL should always perform?",Neutral
Intel,"True. I don't see how the next igpu from Amd going to use rdna 4, it won't even support fsr 4. Meanwhile Intel going to push their igpu tech even further with XeSS XMX 3 with Xe3 and Xe3P, they will be way ahead of Amd in igpu market especially when Intel Lunar Lake already beating Amd Strix Point and Z2E.   Intel also dominating mobile market. Honestly it's not looking good for Amd.",Negative
Intel,Yeah people act like strix point is in that segment..... It's not.,Negative
Intel,🫨,Neutral
Intel,Wouldn't be the first time.,Neutral
Intel,"Oh, I guess it be like that.",Neutral
Intel,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Positive
Intel,"""we are tending to prefer e cores now when gaming""   That's very surprising",Neutral
Intel,If only they release a high level C-card. Battlemage kinda never scratched the mark.,Negative
Intel,"The point of Xe3 being actually battlemage instead of Celestial, this is so horribly confusing. I can't understand what's going on, nor why they would do that.",Negative
Intel,I’m guessing Xe3P will be on Intel 3-PT.,Neutral
Intel,"Tom is a funny guy, love it when he gets camera time",Positive
Intel,Cool hope they will manage to release it soon not in 12 months when everyone will be talking about RTX 6000 and RDNA5/UDNA or leaks about them.,Positive
Intel,"When is the panther lake reveal going to be, CES?",Neutral
Intel,"e cores are surprisingly powerful now, and games are getting more multithreaded. It might be better to spread them out over 8 less power hungry cores than 4 P cores and then spill over to the slower cores. Especially in a laptop environment where efficiency matters and the GPU is usually holding back gaming performance so much that CPU performance is really not that important.  In fact, I bet it's mostly down to not spilling over to e cores from P cores. That's what causes slowdowns and stutters usually attributed to e cores. The engine has to wait a little longer for workloads assigned to e cores than to P cores. The if the main thread is assigned to a P core that's all well and good, but if sub tasks are distributed among the P cores and then something important is assigned to a slower e core it holds up the sub tasks and in turn the main thread. But most importantly, it doesn't do so evenly. Maybe you don't need something assigned to e cores, or the e core task is light and doesn't hold up the other threads. Then things will run at the pace of the P cores, but every now and then you'll have a slowdown.",Neutral
Intel,Wonder if there's some energy star requirements.,Neutral
Intel,"Shouldn't be. That's what the math has always said.  As soon as software can scale to ""many"" cores, the tradeoffs that go into single powerful P style cores are a bad deal. Both frequency and sequential optimizations (like multi-level branch prediction) scale poorly.  Gaming benchmarks tend to have a really strong feedback loop that favors last gen hardware design though. So seeing the benefits to E cores for gaming requires 1.) e-cores exist for a while 2.) some games optimize for them.  Long term, an optimal Intel-style processor design will look something like 4 ""legacy"" P cores + dozens of E cores.",Negative
Intel,It's either N3P or 18AP.,Neutral
Intel,That’s why rumors want Intel to throw away P-cores (Israel team) and only keep E-cores (Austin) to make Titan lake a unified core with only E-cores remaining.,Neutral
Intel,>and games are getting more multithreaded.  Can you give me a few AAA examples of modern games which get a noticeable improvement in AVG. FPS or 0.1/1% lows when using more than 8cores/16 thread's ? I'm genuinely curious which games you are talking about.,Neutral
Intel,"Really should focus on the Main/Sub thread part. Most games will usually only load up 1-3 cores, with the rest of the cores only used for incidental workloads with lower priority and sync requirements (Multithreading is hard yo, Multithreading with latency requirements is mindnumbing).  This makes plenty of games very suitable for the P/E architecture, as long as you have enough P cores for the Main threads, the E cores will be perfectly sufficient.",Neutral
Intel,"Energy Star probably won't be around for too much longer, at least in it's current form.  https://www.npr.org/2025/08/13/nx-s1-5432617/energy-star-trump-cost-climate-change",Neutral
Intel,That makes me imagine in a different reality if Xbox had remained with Intel and NVIDIA and their next generation would use an Intel E-core based SoC with NVIDIA chiplet.,Neutral
Intel,It has already happened. Stephen Robinson - heading the Austin team - is now the lead x86core architect.,Neutral
Intel,"Off the top of my head, Starfield, Bannerlord, BeamNG, UE5 games due to how the rendering pipeline works, etc.",Neutral
Intel,"Most games of the past will only load up a few cores, but that's beginning to change. Cyberpunk 2.0 loads up 16 threads/cores easily, and some others like Battlefield 6 also scale pretty well. If you have less than 16 threads on fast cores like Panther Lake and Arrow Lake, then you can run into issues. Or if your 16 threads are on one die and any spillover has to go across the SoC die like with AMD and probably Nova Lake.",Neutral
Intel,"Seeing how Nvidia working together with Intel to make integrated high end GPU i can see the possibility of Xbox using that chip, maybe in the future.",Neutral
Intel,I'm tired of AMD slop consoles and handhelds,Neutral
Intel,">Starfield  [Not really](https://youtu.be/BcYixjMMHFk?t=1015), game doesn't benefit from more cores/threads.  >Bannerlord  Sadly, there's no benchmarks of different CPUs that I could find for this specific game, but I do know that this game heavily relies on a good CPU, but without decent data(review), which shows multiple CPUs tested, it's hard to understand the benefits of more cores/threads,  >UE5 games due to how the rendering pipeline works  It's true that UE5 can utilize 8 cores / 16 threads, but more than that? I'm not sure, if possible, provide a review/video which shows that UE5 scale with more cores/threads, so far, it seems that it is limited at 8c/16t - big channels rarely add UE5 games to their CPU benchmarks, but I found Remnant 2,[ and it doesn't show any benefits](https://youtu.be/3n537Z7pJug?t=1056) of more than 8 cores.  I heard that ""games are getting more multithreaded"" like 4-5 years ago, and in most cases, it wasn't true, with more than 8c/16t almost no games scale on CPU-side, and even when they do, in most cases it's a minor improvement over 8c/16t configuration, like 1-3%.",Negative
Intel,"UE5 by default does not support multithreading well. actually I don't think async shader is even considered a default feature yet despite being added two years ago. Only the editor compilation step uses all threads, but it doesn't need to react to user input so it would be more surprising if it didn't use all threads.  If you're seeing good thread use in a UE5 game is thanks to the developer breaking up work with their own engine changes.",Negative
Intel,"Considering AMD’s lag in gaming performance especially in ray tracing, I would be totally onboard with that.",Positive
Intel,Huh????,Neutral
Intel,"12900K being that high in the Starfield benchmark, like actually within error margins next to 7800X3D despite being an older platform with half the L2 of raptor lake and lower clock speed than even 7700X, shows that the game benefits from more cores and threads very much. Yeah 9950X should outperform it technically I guess but the split L3 between two CCDs probably holds back the advantage of having more cores.  Yeah sadly there's no benchmark for Bannerlord. But supposedly it does disperse its tasks to plenty of threads.   Here's some benchmarks showing core and thread usage in a few games. While the benchmark tool doesn't get into the details of how those software behave obviously, but at least according to the graphs some games like Tarkov really suck at distributing its tasks to several cores but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  [Will This Do? — Intel Core i5-14600KF vs. i5-13600KF vs. R7 7700X vs. i7-14700KF Benchmark - YouTube](https://www.youtube.com/watch?v=HY_weucfLPQ)  [Isn't it a blast now? — Core Ultra 9 285K benchmark. Comparison with R9 9950X, R7 9800X3D, and i9...](https://www.youtube.com/watch?v=q1zAX1VNdf0)  I haven't seen a core thread benchmark for UE5 games yet, but that's just how the engine is supposed to work though.",Neutral
Intel,"To be fair, Sony seems to be fed up with AMD lagging behind and are pushing them to do better. Consoles will likely continue to stick with AMD, at least for the next generation. Intel graphics aren't quite ready yet, and Nvidia is going to be way to expensive unless they take an old SoC and repurpose it like Nintendo.",Neutral
Intel,">12900K being that high in the Starfield benchmark  It's because Creation Engine 2 worked better with Intel hardware until AM5 X3D chips, and 12900K great result in this test just shows that it's still a good CPU with 8 Performance cores, not that it's core count matters in any significant way - for example, 7700X delivers identical performance with lower core count/threads.  >lower clock speed than even 7700X  Clock speeds can't be compared between different architectures, different architecture = different efficiency, what matters is IPC.  >shows that the game benefits from more cores and threads very much  [here's a better example where more intel CPUs are present in Starfield test](https://youtu.be/XXLY8kEdR1c?t=29m5s), as you can see, increased core count/threads past 8 performance cores and 16 thread's is meaningless and won't provide any noticeable performance improvements, 6 faster P-cores on Intel 14600K provided better result than slower 8 P-cores on 12900K.   Going from 7950X to to 7700X results in 5 FPS loss, which is like 2-3% less FPS, and it's mostly because of lower clock speed(5.7Ghz Vs 5.4Ghz boost).   >but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  That's the point, I agree that modern games can and will utilize 8c/16t configuration, but so far I don't see a trend of games becoming more ""multi-threaded"" past that point, as I replied to that person, I'm curious what games now become more multi-threaded than few years ago, I find that observation speculative and without evidence - more cores and threads is a great approach for workloads, but games generally don't care about it past a certain point, in this case it's 8c/16t.  It could change with next-gen consoles, if they will use AMD 12 core CCDs and games will be optimized to utilize more cores&threads.  What's important now is good cores or good cores+X3D cache, not core count - even 7600X3D with 6 cores and 12 threads is better in gaming than most Intel/AMD non-X3D CPUs with way more cores.  Edit: typo",Positive
Intel,"Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X in baldurs gate 3, a highly multithreaded game, it has better 0.1% lows even, though I'm sure that one is a contained incident since BG3 is very dynamic and hence results are not always repeatable. Same with Starfield.   That's because in video games, core count matters less than how fast the CPU can access or manipulate very dynamic types of data in a random memory address.    X3Ds are not just ""whats important now"". They're going to win in old or new games, and they're winning in games because they have a large buffer of low latency data. Not winning in productivity benchmarks because productivity is about processing matters more than accessing data, and its more easily multithreaded too in many cases.    But having more cores is still more advantageous in software and games that can occupy them.",Neutral
Intel,">Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X  I said ""can and will"" not that every game is going to benefit from it.  Point of discussion was to prove that games are using more than 16 threads or they're not - it seems like you overestimated Starfield, UE5 reliance on core count/threads and most games care about 16 threads at most.   >core count matters less  Yes, that's why I replied to that person and asked him to provide me with modern AAA games which are more ""multi-threaded"" now than AAA games a few years ago, I feel like what he ""observes"" is what he wants to believe, and not something that actually happened with optimization in games.  >X3Ds are not just ""whats important now"".  AMD sells 9950X3D, best productivity and gaming CPU, if you really need both(workloads/gaming) it's the optimal way, for an average user, only benefit of E-cores is lower power draw when idle - but they do matter more if you need those workloads, I agree, but we discussed gaming performance and core reliance.",Neutral
AMD,Has AMD made any RDNA 4 GPUs for laptop?,Neutral
AMD,"AMD has to actually ship product (and support it) for OEM to use them.  There are other considerations, yes, but multiple manufacturers have publicly stated that AMD just doesn’t have enough available for them to warrant making more than a few models.  Nvidia on the other hand has more than enough capacity to guarantee deals so they can easily just stick with them for an entire product stack and simplify procurement.  (For dGPU products. Future of Nvidia’s business direction withstanding at the moment.)",Neutral
AMD,I imagine AMD don't commit the volume necessary to supply the OEMs   Look at the supply issues on the 9070/XT for most of the year. And the DIY market is small,Negative
AMD,"1. Nvidia has brand recognition and simply sells better.  2. Nvidia can guarantee OEMs as much supply as they can move. AMD discrete GPUs for laptops are as good as vapourware.  3. Nvidia GPUs are more efficient and that really matters in a laptop. AMD GPUs really struggle with idle power draw especially, so even before you start to run anything on them, you’re on the back foot  4. Nvidia has their entire laptop stack (with the exception of a couple of SKUs) available at launch. AMD drip feeds its launches so the hype doesn’t remain.  5. A massive proportion of gaming laptop buyers buy them to do work. Almost none of that works on AMD hardware. Their GPUs are straight up not supported in V-Ray and Corona. They absolutely suck in Blender. A 7900 XTX chugging 350W gets is arse handed to it by a 14 inch MacBook running on battery for example. Nvidia GPUs are better for video editing, especially with the new NVDEC of 50 series.   6. The value argument doesn’t hold for AMD laptop hardware. They have worse features and don’t tend to cost much less. And they all have the same VRAM anyway so that is also not a selling point.  TLDR: Because they are worse products and there is more nuance to the laptop market than there is to the DIY gaming desktop market.",Positive
AMD,"It's more likely due to AMD.    They simply don't ship that much volume and unlike nvidia who is constantly pushing volume of desktop GPUs, AMD seem to prefer the much more profitable AI/datacenter market.   Why integrate a worse, less efficient GPU at almost the same price?",Negative
AMD,"They don't. AMD just hate the market for some reason and have never been good at supplying it.  Remember that a lot of laptop brands are largely made by a relatively small number of ODMs. Producing a modern laptop mainboard with discrete graphics is where an ODM leans heavily on their knowledge and relationship with Intel and Nvidia for circuit designs etc. Both companies are very focused on mobile and working with these ODMs. Both have a large lineup of products specifically engineered for mobile, AMD... not so much.  AMD hasn't been focused on mobile for a long time and ODMs have complained about the lack of relationship with the company, being left to figure stuff out themselves etc as far as putting together a mainboard featuring AMD products.  This generation they also just plain do not have the competitive products, there is no RDNA4 mobile chip which is strange. In the past there has also been complaints about inadequate supply of mobile dGPUs.",Negative
AMD,"AMD isn't able to guarantee the same amount of supply as Nvidia (same situation with CPUs and Intel, although that's been improving)  And AMD has also been behind in terms of performance/watt since at *least* 2014, which is the most important metric by far when thinking about laptop GPUs.",Negative
AMD,Power efficiency is everything in a laptop and AMD GPUs require a higher power draw to get a similar performance.,Neutral
AMD,"I think that the APUs are pretty good for laptops, the RX6600M didn't really take off, I haven't seen any laptop with 7600. If they don't come with a 9060 non-XT for laptop now, then they never will.",Neutral
AMD,Amd never released newer lineup this gen thats why.,Negative
AMD,Hard to ship something that doesn't exist,Negative
AMD,"Wondering the same thing, and I'd love to see an explanation.",Positive
AMD,Supply Issues is most likely   \#1 Reason  \#2 They don't sale.   Its the perfect combo of why bother. R&D isn't cheap you need to recoup cost at the bare minimal for what you put into the device being sold. Companies aren't going to do each other any favors anymore then they do a consumer a favor.,Negative
AMD,Cause they like money and amd doesnt make them money,Negative
AMD,"Likely more profitable to go with NVIDIA, whether that is in better prices on components from NVIDIA, or a lack of supply from AMD, or simply better brand recognition that attracts customers.",Positive
AMD,"The key reason is just that Nvidia sells better. For an integrated product such as a laptop its very expensive to give consumers the choice, only for 90% of them to go with nvidia.   The technical aspects are not super important, because AMD did price some of their laptop GPUs aggressively to offset a hypothetical inferiority and almost nobody adopted it other than minisforum I think.   Its not a god situation for the market, its really nice to see framework making it flexible, but for many OEMs it seems just not worth it. The nvidia + Intel bundle situation might make it even worse for AMD in the laptop space in the future...",Neutral
AMD,"AMD makes limited laptop level GPUs. Additionally, for laptops the market is a bit more split. If people want a gaming laptop, they generally really want a capable machine, and Nvidia has the more significant brand recognition there. If they don’t want that, they generally explicitly don’t want a dGPU. So as an OEM, it’s hard to justify the volume necessary to support an AMD SKU rather than just consolidate around an Nvidia line",Neutral
AMD,"Thats why I bought a Framework 16.  To have a Radeon GPU cuz Nvidia is utter dogshit under Linux, especially since most laptops don't use a fuckin MUX switch.",Negative
AMD,because market share is leverage and OEMs don't want to risk their nvidia partnership,Neutral
AMD,"You already got your answer from other users here, but AMD and Intel seem to not even really be trying to unseat Nvidia dominance in the laptop dGPU space anymore. Their currently strategy is to try and undermine it by beefing up iGPUs to the point where some more budget oriented people may question if they even need to step up to a x50 or x60 class dGPU.  AMD's first crack at this with Strix Halo may have been a flop, but the strategy is sound. Intel's releasing their X line of PTL chips in a few weeks. I'm sure AMD is gonna double down next gen with this plan as well.",Neutral
AMD,Probably the same reason all the laptops are intel despite them getting clowned on. It's because their agreement says so,Neutral
AMD,Probably the same reason all the laptops are intel despite them getting clowned on. It's because their agreement says so,Neutral
AMD,"1. AMD only has enough current mobile dgpu skus to count on 2 hands at most 2. AMD doesn't commit to actually shipping high volume of mobile parts 3. AMD tries to force ""AMD advantage"" with various hardware requirements that makes it a pain in the ass for OEMs to ship either cheap base models or use Intel cpus as an alternative if there is a shortage of AMD APUs.  No sane OEM is going to sell amd dgpus when they have min spec requirements for CPU, Memory, Storage, Screen, and wifi/bt.",Negative
AMD,I know it’s bs. Yet virtually every handheld pc maker uses AMD 🤦🏻‍♂️,Positive
AMD,"As an oem manufacturer, why would I want to carry 2 sets of inventory?",Negative
AMD,"All these choices are done via the GPU maker’s business development team going and getting it done by selling into the laptop OEMs. The BD team is enabled by leadership committing manufacturing resources. It’s not like the laptop OEMs just go decide what to buy and put in the systems. I could also see the laptop OEMs going to the silicon companies and sharing “we have consumer demand for an AMD GPU, can you please make some available to us?”  In short, AMD either hasn’t made it a priority or they don’t have the capability. Or consumer demand doesn’t exist such that either party makes it happen.",Neutral
AMD,Because Nvidia makes 95% of all GPU products… so laptop makers need to be ensured that mass product will be available. AMD likely doesn’t have the capacity that some SI’s are looking for.,Neutral
AMD,I have a last gen asus a16 advantage edition with a 7700s gpu. Rougly 4060M performance minus the nvidia rtx/ftame gen which i never use anyways,Negative
AMD,"Because they are not good and AMD is well know for making half ass bios and then never fixing it, especially on the GPU side of the house.   For example we are all still waiting for AMD to release the Linux nvme raid driver for the built in raid, for ALL AMD CPU platforms. Been what? 10 - 12 years now?",Negative
AMD,The industry has conspired against AMD for decades it's literally documented. Intel and Nvidia pay system integrators to use their own hardware over AMD's.,Negative
AMD,"They don't hate them, AMD GPUs simply SUCKS for what Laptop requires most  \#1 Reason: EFFICIENCY, Nvidia destroys AMD gpus in terms of efficiency, not even close, and efficiency is everything in laptop, even 10W higher could become a problem. Nvidia just draws so much less power at the same performance level for many generations already ever since the first RTX. The 5700XT draws the same power as 2080ti while being way slower and has no feature, 7900XTX consume more power than 4090 while being destroyed in everything, even newest 9070XT consumes about 100W higher than 5070Ti even though it's still slightly worse, this might not be relevant in PC at all, but for Laptop this is super crucial, and AMD completely failed at this. Not to mention idle power draw is a problem for AMD gpu.     \#2: Laptops are always meant for WORK and PRODUCTIVITY, not just gaming. If you want gaming only just build pc or get a console, but laptops have to be able to do as many productivity tasks as possible, that's what laptop is made for, a portable pc for work, even gaming laptop is made so that people can work and game on them, and AMD failed this aspect as well, Nvidia not only offer far better feature sets, but also simply work way better and compatible with everything, never need to workaround or tinkering, everything works with Nvidia gpu, and this is massive for laptop     These are the 2 biggest reasons, it's never about stocks or lack of volume that some clueless comments here suggest. That's why AMD thrives with Console instead, because consoles don't need neither of the reasons I mention above, console doesn't care about efficiency since they always plugged in, and console is for gaming only, never for anything else, so AMD has no problem fulfilling the massive volume of console Gpus, because for console use case AMD gpus make sense, unlike laptop",Negative
AMD,NVIDIA optimus and better efficiency,Positive
AMD,"AMD doesn't have a mobile GPU lineup that has matched NVIDIA's for a while now (at least in the last 11 years), and what doesn't help is the efficiency at idle/low loads isn't great either which isn't a big deal in a desktop but much more important in a laptop, as you don't want heat to stick around in the laptop shell.  Also whilst the Nvidia mobile gpu shennagians weren't great with naming, they are generally more power efficient and performance ""per watt"" is better , especially if you are pairing it with a high end cpu or a thin laptop shell.",Negative
AMD,"I actually saw this talked about somewhere. I think the people said the manufacturers have found that there is just huge demand for nvidia gpus from consumers. Dlss, framegen etc and the fact that they deliver very good performance with reasonably small power, it is a no brainer for them. Having an nvidia gpu in the laptop does free advertisement for them.",Positive
AMD,"Why do most people buy Nvidia?  OEM's follow what sells, Nvidia sell units. Just look at all the posts we still see asking if AMD GPU's will burn up, the number of times a AMD GPU was lower cost but OP ends up going Nvidia as it's the brand that everyone buys.  It's just sales, Nvidia moves laptops.",Neutral
AMD,"Why do orange basket manufacturers hate bananas?   I noticed that, while looking for a new purse.",Negative
AMD,Simple numbers game.  Nvidia - 92% market share   AMD - 7% market share   Intel - 1% market share    Which one are you going to go with if your goal is to sell the largest number of gaming laptops possible?,Neutral
AMD,because intel still lobbying  one exemple: [https://www.reddit.com/r/Amd/comments/16q44ar/eu\_fines\_intel\_400\_million\_for\_blocking\_amds/](https://www.reddit.com/r/Amd/comments/16q44ar/eu_fines_intel_400_million_for_blocking_amds/),Neutral
AMD,"They don't make a dGPU for laptops, just iGPU. Though their iGPU is pretty good for most tasks besides gaming. AMD probably thinks if you wanna game, buy a gaming system like a PS5 or Steam Deck which hosts an AMD chip.",Neutral
AMD,"It's old practices, even when AMD started dominating in CPUs you look at the market place and it's NVidia/Intel everywhere... That eventually changed due to the lunacy of not having the best CPUs ""because reasons""... The GPU side of things is going to be very hard as the stigma that AMD is a budget brand is what drives these companies to avoid good AMD GPUs as they fear the budget stigma will rub off on their premium products.",Negative
AMD,"AMD sold their mobile Radeon brand to Qualcomm when they were at their lowest, hence why Adreno is just an anagram of Radeon. Because of that deal, they can't have dedicated mobile GPUs under their Radeon branding...",Negative
AMD,Nope. They wont launch a 9070M,Neutral
AMD,question answered,Neutral
AMD,"They made RDNA, RDNA2, RDNA3... almost no laptops.  Backroom deals, pressure or even manipulation from Nvidia might not be the whole story but I suspect it is going to be a significant factor...",Neutral
AMD,"Chicken and egg, AMD hasn't been shipping product because laptop manufacturers do not buy them. They're finding success in their strix halo APUs, and that's likely their future in the laptop space.",Neutral
AMD,"Yeah, this is by far the biggest issue discouraging adoption by laptop makers.",Negative
AMD,Well they supplied the entire console market pretty well and their cpu market too so their track record is actually good,Positive
AMD,"AMD doesn’t actually make anything, do they?  All their chips are made at other fabs.  That means they’re competing with everyone else for the same production capacity.",Negative
AMD,"True points.  Adding to them that Nvidia is more reliable in hardware and software for decades.  AMD drivers for RDNA used to be problematic and they had higher RMA rates. For a DIY enthusiast this is less of an issue, but for a system integrator/laptop manufacturer it means costs. Even the basic support request is a cost and you may have to service an entire system in case of a RMA, instead of just the dGPU for a DIYer. Combined with AMDs notoriously low margins, a single support phone call can mean a financial loss.  Meanwhile Nvidia is generally stable. While sometimes the drivers cause issues in specific scenarios, there was no general issue with them as their was with early RDNA.",Neutral
AMD,"7 Historically AMD's software stack had been so clownishly bad they were basically unusable. Until a couple of years ago you genuinely couldn't install the normal driver on their mGPU on many laptops, instead you'd get some branded locked down driver with missing features from the laptop maker from around the time they released the model and stuff just wouldn't run on a 2, 3, 4 year outdated driver. Sometimes they'd even have the older driver UI which hadn't been in use for literal years.  AMD being bad at software is not a meme.",Negative
AMD,> Nvidia GPUs are more efficient  This is not really true. RDNA4 is just as efficient as Nvidia. rx9070 topped GN's efficiency charts for instance. RDNA2 was also more efficient than Ampere.  AMD also has Radeon Chill which is another tool you can use for power efficiency Nvidia doesn't have.  I agree with your other points.,Positive
AMD,Who uses a laptop for blender or video editing? Literally cherry-picked use-cases to make AMD look bad.,Negative
AMD,Efficiency at low loads is also poor too: https://tpucdn.com/review/sapphire-radeon-rx-9060-xt-pulse-oc/images/power-video-playback.png,Negative
AMD,Not since Vega,Neutral
AMD,And they barely released anything last generation either.,Negative
AMD,"I've been debating recently on which GPU to get alongside a mobo upgrade, what do you use to have it switch to the dedicated GPU?",Neutral
AMD,As someone that’s works with corporate GPU applications this is funny. If they were so bad why would they dominate the industry?,Negative
AMD,"the idea of a mux switch in a laptop is hilarious, hardware fix for horrible drivers",Negative
AMD,nah intel mobile cpus are just better rn,Positive
AMD,"Actually, you could count AMD’s current generation laptop dGPUs on 0 hands, considering they don’t have a single RDNA4 laptop part this generation. We’ll see if that changes at CES next year (in ~2 weeks), but I wouldn’t hold my breath.",Neutral
AMD,LOL,Neutral
AMD,"I call it the *OEM-factor*™ … Always gets immediately DENIED as non-existent, of course, since years.  Yet somehow we had *a completely heathy and quite balanced laptop-market up to the early 2000s*, where there were loads of potent AMD-powered offerings with AMD's *PowerNow!*-technology (Dynamic frequency scaling and power-gating for power-saving), and it was almost like fifty-fifty AMD vs Intel …   That was when Intel was still utter sh!t in that department and had even horrendous power-draw in mobile.  Until it all of a sudden all changed the precise moment Intel brought their infamous *Centrino*™ program to OEMs (paying system-integrators for equipping notebooks with Intel-chips)  — It has stayed as such since (+90% Intel).  Then the Intel-exclusive *UltraBook*-brand was the next, which a while ago just rolled over to be Intel *Evo*.  > Intel and Nvidia pay system integrators to use their own hardware over AMD's.  Yeas, and very handsomely at that. Still, *»Nothing to see here folks, just move along!«*",Neutral
AMD,"And despite the absolute surge of burned connectors since Blackwell, it is still AMD cards that get questions sbout catching fire",Negative
AMD,"These numbers aren't relevant. They look like dedicated desktop GPU market share. In reality, I bet Intel is first in laptops, followed by AMD just due to the fact that most laptops don't need, nor want dedicated GPUs. As for gaming space idk",Neutral
AMD,"Lmao Intel isn't lobbying OEMs to choose Nvidia GPUs.  And also literally read the first paragraph of the article: ""between 2002 and 2007"".  And thirdly, the biggest proof Intel isn't lobbying OEMs (with what money?) Is how many design wins Snapdragon got",Neutral
AMD,"Right... Intel is lobbying OEMs to use Nvidia products..  Maybe every once in a while you should actually read the articles you use as ""proof"".",Neutral
AMD,"AMD had to consistently deliver a better CPU than Intel for several generations before mindshare in the public caught up.  Its going to be the same for GPUs: until AMD offers the better *all around* GPU, several generations in a row, and at a lower price, it's mindshare just isn't going to change in GPU.  And dont forget the impact Halo products have on their downstream products. The fact that the 5090 is the undisputed best GPU gives huge brand prestige to the whole product stack.",Positive
AMD,what?,Neutral
AMD,They’ll put a 9050m in laptops after they release RDNA6 in 2030,Neutral
AMD,Why would laptop manufacturers do this to us?!,Negative
AMD,"Chicken and egg doesn’t really work for B2B. The seller has to provide the product and push the deals, demand doesn’t just appear out of the air for products with (edit: particularly established) competition. The buyer doesn’t particularly care which egg it buys as long as it has enough of them to make what it wants to sell.  (And in this case the consumer doesn’t really care either, the overwhelming majority of any form of prebuilt including laptop is just “it’s in my budget, available, and the page says it’s good”.)",Neutral
AMD,"I fear ""success"" in Halo Strix is related to its limited supply: that is, the Strix Halo large APU price is so high, the demand is also pretty low → AMD can make enough dies to satisfy the small market.  Large APUs will find it tough to crack into the laptop gaming market in terms of market share; the only other large APUs are 1) consoles, which only survive on extreme optimisation, hundreds of millions of units, and they *still* sell at a heavy loss, and...  >As [IGN reports](https://www.ign.com/articles/microsoft-loses-between-100-and-200-on-every-xbox-sold), Spencer confirmed that the loss on each Xbox console is between $100 and $200 dependent on the model.   2) Apple's M-series Pro / Max / Ultra. These are also very expensive and kind of what one would expect a large APU to cost (+ the customary Apple tax).",Negative
AMD,"Strix Halo has been a commercial failure.  There is only one commercially available laptop of it. No major OEMs are even interested in it, and it is now being sold to Chinese brands at a discounted price for mini PCs and handhelds.",Negative
AMD,"Most people talk about volumes and while that certainly does contribute, the main problem is that OEMs basically expect cpu vendors to handhold them to designing most of the system. Yes you read that right, designing the system as in mobo reference designs, cooling solution and beyond just what you imagine to be normal SW support. When qcom started to double down WoA to promote snapdragon elite, the higher ups complained about how they had to provide massive amount of support to OEMs because they were to used that and they have to massively ramp up on that for OEMs to take them seriously and not just put their SoC in some half assly designed gimped models",Negative
AMD,"I think APU's will be a big deal for AMD in the near future, they will be fast enough for everything and will keep getting better for the limited power envelope of laptops",Positive
AMD,Supplying the console market is probably one of the reasons they don't have much stock left for laptops.,Neutral
AMD,"Right, now look at their GPU track record, the relevant market here",Neutral
AMD,The consoles are using old nodes tho,Neutral
AMD,"Desktop CPUs sure, but mobile CPUs they don't seem to supply a sufficient volume of either. Look at Strix Halo, a high margin performance leading part and a year after launch you can still count the number of laptops using it on one hand.",Neutral
AMD,Cost wise its not effective either. Its the same reason why AMD fail miserably in the Pre Built space.  9060 xt and 5060ti. 350 v 420. $80 and 20% right?  But when you build a whole PC with the 9060xt for lets say $1000. The same with a 5060 Ti wil cost $1080. Only a 8% difference. A difference many would pay  And the difference gets smaller the higher your base pc is  Same thing applies in laptops where the sum of all parts makes the RTX GPU only slightly more expensive.,Neutral
AMD,"I also agree in everything but that point as well. Don't know why you're getting downvoted, though not surprised since everyone here is speaking anecdotally. Also, while you posted an example from TPU there are other outlets such as [computerbase](https://www.computerbase.de/artikel/grafikkarten/amd-radeon-rx-9070-xt-rx-9070-test.91578/seite-9#abschnitt_leistungsaufnahme_gemessen_spiele_youtube_desktop) that backs it up (pretty much on par on idle, and daily usage/multi monitor)  I've actually made a post here: [https://www.reddit.com/r/hardware/comments/1l2vjuo/mostly\_positive\_reviews\_rx\_9070\_xt\_vs\_rtx\_5070\_ti/](https://www.reddit.com/r/hardware/comments/1l2vjuo/mostly_positive_reviews_rx_9070_xt_vs_rtx_5070_ti/)  Where I shared someone's finding on the matter with RDNA4 vs Blackwell (9070 XT vs 5070 Ti). At that time, when capping FPS, AMD by default simply does a better job with freq to hit a locked FPS target to save power, even doing as well or beating the 5070 Ti. In those comments you'll see (obviously so) that FPS capping saves you power anyways, but the matter is, RDNA4, especially when binned and power constrained (+drivers) can be effectively used in laptops.  Oh and btw,  >RDNA2 was also more efficient than Ampere.  IIRC pre-RDNA3 AMD only reported GPU power and not TBP (it was an [igors lab test](https://www.igorslab.de/en/graphics-cards-and-their-consumption-read-out-rather-than-measured-why-this-is-easy-with-nvidia-and-nearly-impossible-with-amd/))",Neutral
AMD,"Are you speaking to efficiency under load? Or while idle?  Idle power draw has been a weak spot for AMD GPUs for a while. I haven't looked into it since launch, but at the time initial reviews seemed to confirm this was still an issue for RDNA4.  High idle power draw is bad for laptops, obviously.",Negative
AMD,"That’s like the majority of professionals who need a GPU in their laptop, that’s not that cherry picked.",Neutral
AMD,You can’t think of a reason why someone may want to do video editing or Blender work on the go?   Or have a device capable of being a video editing platform anywhere to allow for working from home?,Neutral
AMD,basically everyone using a Mac.,Neutral
AMD,"no Blender pros do buy them, as new laptop 40 and 50 series do very good in blender  new laptop “5090” GPU with 24GB VRAM almost matches a 4080 super desktop in blender performance at 175 watts   https://opendata.blender.org/benchmarks/query/?compute_type=OPTIX&compute_type=CUDA&compute_type=HIP&compute_type=METAL&compute_type=ONEAPI&group_by=device_name&blender_version=4.5.0  CAD professionals, with Recent Ai boom a laptop with CUDA support for ML and Ai it's an awesome buy  In steam charts 4060 laptop is the top……  Unlike what reddit thinks,   do you think companies pour millions in for gaming laptop manufacturing R&D to waste?  Gaming laptops do insane number of sales.",Positive
AMD,"They still do. RX 7600 is slower than a 4060 and needs 40-50W more power to run. To compensate, AMD had to reduce CUs from 32 to 28 and reduce power draw. On the other hand, the RTX 4060 is identical on laptop and desktop, down to the CUDA core count and power consumption. It’s the better product. And given that the 60 series is the most popular laptop GPU series, that really adds up",Neutral
AMD,They are less efficient than nvidia and suck way more power at idle which is a big deal in laptops,Neutral
AMD,"I dont need to switch it myself, since it works in handshake with the AMD DGPU and IGPU with integrated MUX switch.   So basically, desktop idle and browser stuff runs off the IGPU, everything else, DGPU 7700S handles it.   Also, if ya have a 7745h cpu, not worth swapping to the HX since its half the full core size, plus compressed Zen 5c cores.  Dont like that alot",Neutral
AMD,Open source community distros running GUI desktops are a bit of a different experience than headless Linux servers running with enterprise licensed packages or Windows/Mac desktop.,Neutral
AMD,"Qualcomm showing up in more and more laptops. ARM is the future, Apple proved that. And Qualcomm is the company best poised to make the ARM chips in the quantity needed, because they’ve been doing it for smartphones for decades.",Positive
AMD,I was talking tech wise... also no. A quick gander on the passmark puts the top 10 laptop chips as 80% AMD with Intel taking the number 2 and 3 slots. So I'm going to gather that the intel mobile cpus are at minimum on par.. I'm sure theres a some variance in benchmarks but I'm lazy  https://www.cpubenchmark.net/laptop.html,Neutral
AMD,"I know right! So pathetic what those companies are willing to do to put down AMD, but shoe is on the other foot now and AMD will stomp out intel in the CPU space and is gaining ground as Nvidia leaves gaming to propel the great AI scam.",Negative
AMD,"I gave up pointing that out, people want to buy Nvidia and you cant change there mind even if it's lower cost or not setting on fire.  GPU's are like SSD brands, you just plug one in and it works yet a lot of normal/light PC users still think it's dark magic to change GPU brand.",Neutral
AMD,These numbers are the *only* thing that's relevant.  We're talking about gaming laptop GPUs.  Manufacturers are going to sell what people want to buy.,Neutral
AMD,"Exactly. My biggest piece of advice for tech illiterate people buying a laptop until about 7 years ago was ""make sure you get an Intel CPU"". AMD was producing some absolute donkeys that had no place in the midrange laptops they were going into. It took a while for that sort of advice to cycle out of people's minds, well after AMD started making decent mobile CPUs.",Negative
AMD,Do you mean AMD?,Neutral
AMD,"No, GPUs aren't direct replacements in laptops. They have to develop two different boards for each model, if they want to offer both manufacturers. Customers are also very picky in gaming laptops. Almost all of them will pick an Nvidia version over AMD at a similar price. Just ask yourself how much cheaper a $1400 laptop would have to be to pick a 9060XT over a 5060ti? If the answer is >$100, then AMD would have to basically give away the chips for free.",Neutral
AMD,This is so massively ignorant to talk about in the PC space. People do care... It's why Nvidia owns 92% of the PC graphic card market.  AMD has been struggling to sell it's products for ages in the GPU space... Both to end users and to board and laptop partners and it's not because they can't produce product...   APUs are AMDs future in the laptop market because AMD CPUs are unmatched in the x86 space. Strix Halo sells because the okay GPU is bolted onto world class leading CPUs.,Negative
AMD,"Next gens RDNA lineup is fully dual use. The top spec goes into the workstation market and high end desktop, the second chip is the 9070XT replacement and a cut down version goes into the next xbox. Third chip goes midmarket gpu and Halo gpu die, and the small one for the ...point mass market apus.",Neutral
AMD,AMD has been betting on that for years but still have issues giving good supplies to OEMs  And they also take a long time to actually launch their products to OEMs,Negative
AMD,It's hard to judge rn because of AI dram production which definitely is gonna cut into some production for some of the process nodes. Ideally though you'd have thought they'd be at least well suited to gaming laptops with their lower power console focused development.   Especially considering their APUs are almost best in class at this point so you'd thought they'd at least have it down in the lighter laptop selections.   I think its mostly just legacy volume contracts and risk adverse manufacturers keeping intel and nvidia dominating,Neutral
AMD,"I am, I'm talking about their console GPU manufacturing. I'm saying that they have good track record for chip manufacturing its at least a half decent indicator.",Positive
AMD,Yeah they are but uh track record is kinda always gonna be old. Plus I wouldn't say that the current situation is exactly a good example.,Negative
AMD,"AMD uses a single universal tapeout/photomask set to satisfy the vast majority of their entire desktop and server catalogues. Even a substantial proportion of AMD's available laptop offerings over the last 2 years have been -HX ones, which are just the desktop SKUs made into a solderable package. That's why it's always been relatively easy for them to maintain relatively consistent inventory, despite having far far fewer wafers coming in compared to what Intel can get.  The actual monolithic mobile SKUs are fundamentally different die designs to the other market segments. Strix Halo also uses a fundamentally different chiplet packaging method to server/desktop Zen, so neither the CCD or I/O die are compatible and makes it its own third, separate limited production assembly line.",Neutral
AMD,It's that actually production problems or do manufacturers just not adopt strixx halos for what ever reason they dont adopt AMD CPUs or GPUs in the laptop market?   Are there volume problems with those laptops? I can imagine strix halos packaging makes it hard to produce,Negative
AMD,RDNA4 idle efficiency is good too. You might be getting confused with RDNA3 big GPUs which were chiplet based. They use more idle power because they are chiplet based.  https://www.techpowerup.com/review/asus-radeon-rx-9070-tuf-oc/41.html,Positive
AMD,"Laptops are for convenience. Video editing and blender require some amount of time sitting down to do what you want that it's better off being done on a dedicated workstation.  ""I'm going to do a little blender and video editing on my laptop at the library today"" - said no one ever",Neutral
AMD,"Nah, idle power on Nvidia is tragic if you more than a single monitor unless they have identical resolution and refresh rate. My 5090 goes from ~40W to 90W by just plugging in a second display while doing nothing",Negative
AMD,I have the 7940HS and was playing on going to HX 370,Neutral
AMD,That’s where it’s being used mostly. I know this your hobby but surely you can see the bigger picture.,Neutral
AMD,yes tech wise is why intel cpus are better. [https://imgur.com/a/CyxoUZM](https://imgur.com/a/CyxoUZM)  On paper the 9955hx and moreso the 9955hx3d look very good but in practice they fall short.   The 9955hx3d and 9955hx are simply lackluster in terms of battery life with the 275hx giving double the battery life and when unpluged the performance of the 9955hx/hx3d tanks which causes the 275hx to pull ahead when not plugged in. Additionally ryzen cpus have more sleep state issues which again hurts the battery life although that could be considered a windows problem and not on amd.   On top of this the intel chips have thunderbolt support and come with better wifi cards which amd refuses to use.   Then on the ultrabook side of things intel lunar lake is just better in terms of having lower powerdraw and a stronger igpu compared to AMDs lower power options although both intel and amd get smacked by ARM chips so not really that important unless you need x86,Positive
AMD,Does AMD make things that go “woosh” when you need them to?,Neutral
AMD,"And the consoles are a completely different market    AMD do not commit volume to PC  gaming, look at the GPU shipments, a much better indicator    I didn't say they couldn't, I'm saying they dont",Negative
AMD,"Unless you are doing one specific job, which is LLM inferencing, no matter what the hype says, Strix Halo is very poor value performance wise. RTX 5060 laptops are faster and generally half the price.   Nvidia GPUs are then also way better supported in professional applications than AMD GPUs.  And if you really wanted to do LLM inferencing on a ""budget"", a MacBook Pro is more efficient and faster anyway.",Neutral
AMD,"That’s a very valid student usage of laptops though, and you very conveniently leave out the work from home usage which does involve sitting down somewhere for some amount of time, while still needing the portability to bring the device between home and the office if needed.  A workstation would mean giving each employee two device for hybrid work, or forcing remote usage, neither of which would be an amazing option compared to just giving each employee a laptop to commute with",Neutral
AMD,Never worked on the creative/film/event management field I guess?,Neutral
AMD,">My 5090 goes from ~40W to 90W by just plugging in a second display while doing nothing  Yea that's not normal, 2 displays have been fine on nvidia cards for... well as long as I've had an nvidia card so 2017, to stay in idle memory speed state and it's only couple of watts more. Maybe if you go high enough it changes like two 4k240 displays or one/both being 1440p500, but 4k240 + 4k144 brief moment i had that combo didn't do it on a 4070ti and was fully idle state. Or you have some power settings cranked up maybe those effect things. Obviously interacting with like browser stuff with 2 high refresh/res monitors does have more brief spikes than a single panel would, but that's not idle.    3 is a bit of mixed bag, but even that it much better than it used to be like ~4-5 years ago, when 3 monitors of any kind was max memory speed state and can still get idle at 3 panels if the refresh rates of all panels isn't high and even then it might go to half memory speed state instead of full speed on some specific combo. I do remember some talk about how the specific model also might affect things due to... something, rather than just pure refresh/res combo. thought that was mainly for 3(+) panels where it mattered, but maybe that could be what's happening even with just 2 panels.",Neutral
AMD,"Meh, not worth it tbh.  If it were a X3D version, yeah, but basic 370 is basically 3% faster",Negative
AMD,"I'm confused... Are you suggesting that this person should sacrifice their own personal laptop's usability for ""the bigger picture?""",Negative
AMD,"I’m not sure what you mean by “bigger picture”, but the desktop GPU experience is why people in this sub would have the opinion that Nvidia on Linux is a poor experience.   It’s only recently (within three years) that Nvidia started releasing open source kernel drivers for their cards on Linux (because they moved their proprietary code into the device firmware). Prior to this (and even still) it wasn’t uncommon for Linux users to boot into a black screen after upgrade due to an Nvidia related problem.  This is different from large ML or HPC clusters which will use licensed drivers and CUDA libraries curated by someone like SUSE or RedHat for a hefty fee.   With regards to procurement, Nvidia has been popular because of how good CUDA is, how powerful/efficient  Nvidia cards have been since Maxwell, and how good their marketing is.",Neutral
AMD,"Cant believe i needed to turn on my vpn for that. I did think there would be variance in the test, is there a reason the amd chip doesnt ever reach its max tdp or is it limited? I'd also like to point out the x3d chip isn't on your graph and they behave wildly different.   I guess the thunderbolt is okay ish? And idk what the wifi card thing is about considering how many laptops just use a separate WiFi chip.   >although both intel and amd get smacked by ARM chips  Not surprising its like the whole point of RISC to be specialised it'll only keep getting better.",Negative
AMD,"Laptops are closer to consoles in alot of ways, tight oem integration, semi custom is also more common, power envelopes and form factor are much similar too.   Also why should we not include AMD's largest consumer GPU segment? Surely if you want to look at whether they can do the volume for an oem you would look at their total track record for oem gpu shipments? Of which the largest will definitely be the shipments to Microsoft and Sony.",Neutral
AMD,You wouldn't put the strix halo in a laptop that could fit a 5060 in it i guess then. You'd be putting it in thin and light laptops where you don't really have the power envelope or space for a discrete gpu. So it doesnt really matter what its perf is vs a discrete gpu cause that laptop chassis isn't gonna fit one.  Also on a side note wasn't it really designed for Microsofts god awful Copilot+ PC rollout that flopped heavily i can imagine the inference based APU coupled with another discrete gpu is why?,Neutral
AMD,"I’m saying that the drum beat of bad driver support isn’t the whole picture. If they want something free with no official support they are gonna have issues.  But saying because of that NVIDIA is bad when they have 100,000s of Linux system running the world at the moment is short sighted.",Negative
AMD,[https://imgur.com/a/PmUFBnX](https://imgur.com/a/PmUFBnX) just over double the battery life compared to the 995hx3d and that is a best case scenario for the 9955hx3d because if you are turning the laptop off many times in the day the disparity will grow due to sleep state issues.  The wifi card thing is a hardware thing because intel makes the better wifi cards and while they do have multiple chipset versions(one CNVio that only works with intel cpus and one non-CNVio) Amd still doesn't use them probably due to cost but also there are some driver/hardware issues as even the non-CNVIO Intel BE200 is known to have issues with AMD cpus,Neutral
AMD,"They haven't released mobile RDNA 4, that's tells you all you need to know about commitment   If they actually competed on volume, don't you think it would make sense to actually launch some products?  AMD would rather sell them bundled with a CPU for a premium, why their APUs are so prolific",Neutral
AMD,"Even then you wouldn't though. AMD says the Max+ 395 can sustain 120W and boost upto 140W. That's not really much less than most 5060s do paired with a Ryzen 7 or Ultra 7.   I have an Omen Transcend 14 with a Meteor Lake Ultra 7 and a 4060. The total power budget for them is \~85W because the laptop is powered via a 140W USB-C power adaptor. The 4060 can pull upto 65W. The newer 5060 model can pull 75W because the CPU is more efficient. As a result, my laptop is slower than a full-power 4060 laptop.  But the same is also true for the ZBook Ultra G1A. Also a premium, high-performance 14"" HP. Also runs through a 140W USB-C power adaptor. Also has the same 85W power limit. Is also slower than the ROG Flow Z13 tablet that has a 180/200W power brick with 20-25W higher power budget. It is also not really faster than my laptop for the GPU.  Of course, the ZBook's CPU is a lot faster. I guess a more fair comparison in that sense would be the 385, but that has a slower GPU.   All of that would be fine until you realise that the ZBook costs a LOT more than the Omen and the laptops otherwise are very similar. Similar battery life, same OLED display, very similar keyboard, trackpad and speakers, similar build quality and general handling.",Neutral
AMD,"> Thats why I bought a Framework 16.  > To have a Radeon GPU cuz Nvidia is utter dogshit under Linux, especially since most laptops don't use a fuckin MUX switch.  I think you've misread the initial statement somehow, because that's definitely not what they're talking about. They're talking about their personal laptop.",Negative
AMD,Oh what laptop was this test ? I assume it was amd vs Intel versions of the same one.   Also the wifi card thing doesnt matter all that much anyways the perf of intel vs qualcomm vs cnvio is really not going to matter much considering they are all compatible with the same standards so you can get a better one if you want,Neutral
AMD,"Yeah what happened to that? Was it power?  >If they actually competed on volume, don't you think it would make sense to actually launch some products?  >AMD would rather sell them bundled with a CPU for a premium, why their APUs are so prolific  I don't want to be cynical and blame AI but I think they were putting their production into datacenter stuff instead",Neutral
AMD,"The tdp for the 5060 in the transcend 16 is 140W which is more than the strix halo by itself which is what I really meant. I mean its the whole point of a integrated GPU less power, less space, less engineering complexity unified memory so on and so forth.",Neutral
AMD,yes same exact laptop the XMG Neo apart from ofc the superior wifi card on the intel laptop and thunderbolt.,Neutral
AMD,">but I think they were putting their production into datacenter stuff instead    As is Nvidia, who are committing more   But despite that, Nvidia still commit enough volume to supply the gaming market",Neutral
AMD,You seem really hung up on single port type and WiFi card huh,Neutral
AMD,"I mean for a company having volume problems 73% year on year revenue increase in that segment seems pretty damn good. I mean looking at their financials I'm finding your point hard to believe, they had record radeon sales and increased gaming revenue from 0.5 to 1.3 Bn yet they're having volume issues...",Neutral
AMD,? no I was just saying they are the same exact laptop spec wise except those two parts although those two parts aren't really important in terms of power draw which would make the comparision accurate.   If they instead had different screens like oled vs ips that would cause a difference in power draw.,Neutral
AMD,"Not impossible, but very unlikely. AMD is currently focused on upcoming Zen 6, and I guess their engineers are working on next gen CPU / chipset / AM port. Plus there are probably constraints with TSMC schedule, I think you have to ""reserve"" quite some time in advance for any order.",Neutral
AMD,"Probably the simplest would be to manufacture 5800x3d again.   (Newer cpus have ddr5 controller, they would have to redesign architecture.)",Neutral
AMD,No.  Considering the tech news from the last few months apparently there isn't much money in the consumer market for something like this.,Negative
AMD,"Maybe, but I'm starting to think they want people to sell off their old stuff. Just a theory of course.",Neutral
AMD,"the thing is you can still get 32gb of ddr5 for two or three hundred bucks. it won't be a great bin, and definitely won't have the overclocking potential of high end hynix a-die but frankly the vast majority of people can/will just run JEDEC and not be able to see the difference. That may be easy for me to say sitting on 64gb of a-die running tight timings, but you/I seriously don't need high end ram to build a pc and run games flat maxed out. Prices have gone up but generally if you could afford to build a PC before the rampocalypse you can still do so now, you're just going to have JEDEC speeds. Which are actually, totally fine if you're using your pc and not a hardware snob (which I am admittedly). ​",Negative
AMD,"Last AM4 cpu that got released is the Ryzen 5 5600F, which happened september. So only 3 months  I wouldnt be surprised if AMD comes up with something new or reintroduce 5800x3d and 5700x3d (these cpus only existed for a year or year n half. Which is very short)  Its been 9 YEARS since AM4 got released. AMD still releasing cpu's for it.. crazy",Neutral
AMD,No  Not enough volume for amd to restart production,Negative
AMD,"The production of the X3D variant of the core chiplets has been discontinued. Presumable it was done so that they can use those lines to produce Zen 5 X3D variant. So unless you want them to discontinue the current gen to put old already somewhat obsolete generation back into production, it's not viable idea.",Negative
AMD,5950X3D? Or even 5950X3D2?,Neutral
AMD,"If I were a top AMD executive I'd be focusing on these things  1. Getting Radeon THERE for AI purposes   2. Making the EPYC line amazing for data centers   3. Finding ways to optimize costs and cut risks     Targeting budget customers is fairly low on any list I'd have.    There's probably some value in keeping Zen 3 dies in production but they'd get minimal priority for anything new or cutting edge. Minimal development efforts. Zen 4 is already getting ""old"" by industry standards. There's not much point to getting anything newer to work with AM4 IODs either.",Neutral
AMD,"First of all, never trust inflated prices on eBay. It's well known that scam artists who have hordes of collectibles or scalped/limited items who will orchestrate sales of items at inflated prices to drive up the market. Yes, they have to eat the cut that eBay takes, but if all of a sudden they have 10-50 items that they can sell for 20-100% more, it's worth it.   Like what people have been doing with VHS tapes on eBay for the last 6 years.   Secondly, AMD hasn't stopped releasing CPUs for AM4. They recently released the 5500X3D for the Brazilian market in June, and the 5600F in others.   The biggest issue is there's only a handful of AM4 boards still for sales and DDR4 RAM in the retail channels is almost entirely gone. Sure you can find used RAM, but minting new products that requires parts that are no longer in production is suicide.   Micron recently said they'd keep making DDR4, but I ain't seeing much available in the US. I assume much of that is still being [shuffled to the enterprise market.](https://prerackit.com/memory-markets-in-turmoil-how-chinas-exit-from-ddr4-manufacturing-triggered-a-server-ram-pricing-crisis-in-2025/)",Negative
AMD,Before the AI bubble I would have said no way. With the AI bubble making new unaffordable for the next 12-18 months at a minimum I'd say maybe.  People on AM4 aren't going to buy AM5 if they have to spend 200% of their budget to get it. DDR5 RAM prices are really going to cripple the PCbuild industry for the next year.,Negative
AMD,"There was some rumors that Zen4 was originally going to come to AM4. They could still do it. Maybe a lot of that work is already done. I'm skeptical there is still much DDR4 in production, and it'll soon all be gone. What we got on desktop was mostly left over stuff that servers didn't want. I don't think they would make these CPUs just for consumer when there is so much more profit in putting that silicon towards servers. If Mircon abandoned Crucial memory for consumers, that to me says a lot about where the real focus is for hardware makers. AMD right now just doesn't care much about desktop anything.",Neutral
AMD,"Yes, they have actively done so in the last year and are being pushed to re-release the 5800x3d by retailers.",Neutral
AMD,"Very unlikely and most probably something on the lower end I’d think.   But a 5950X3D would be incredibly cool though, and a nice upgrade for my 5700X3D. 😄  On the other hand, there are a lot of people out there, me included, on AM4 that aren’t planning to upgrade to AM5 any time soon. So that would be a way for AMD to keep selling CPUs to existing AM4 users of whom they otherwise wouldn’t make money from.",Positive
AMD,"From one side Zen 6 has to sell but RAM/(SSD) prices may block AM4 to AM5 upgrades (how many still on AM4?). Then if going AM5 is to expensive then people may wait till Zen 7 which may be AM6 and if it's revealed early that Zen 7 is AM6 then it will decrease AM5 interest even more.  Zen 6 will launch ""late"" 2026, so 2027 is the year it will be in mass availability. RAM prices may be more sane at that time. Especially when people will be waiting for X3D (if the base variants get rekt again by existing X3Ds).  Refresh of 5800X3D wouldn't hurt but I doubt they would be offer more without a longer development cycle (a new design) after which it may turn out it's all for nothing.",Neutral
AMD,Wafers aren't just sitting on shelf to be bought when in demand. They're order well in advance. It's unlikely they resume production of the 5700x3d which old got disconnected a few months ago (new AM4 cpu is completely put of the question) as it's a step backwards and a gamble that people would still be interested in AM4 several months time instead of shelling out a bit more for significantly higher preformance gains from their next generation of cpus in November.,Neutral
AMD,they do release 5000 series x3d chips but using leftover chiplets that were binned too low for the expensive parts. ie the 5500x3d. its not in production anymore.,Neutral
AMD,No - production has already stopped for the 5000 series. It would be prohibitively expensive to have TSMC manufacturer last-gen chips in another run.,Negative
AMD,"Not only is it not likely for reasons of moving forward instead of back, you'd also crash the ""market"" the minute there's any supply, and it would likely spike prices on AM4 boards and DDR4 ram. The lack of demand is the only reason AM4 boards and DDR4 are relatively cheap right now. Can't get good AM4 CPUs, so the rest of the linked prices slump. Bring back competitive AM4 CPUs and the prices shoot up to match, and now AMD is stuck trying to sell ancient and EOLed designs into a market that's already buying every AM5 CPU they make.",Negative
AMD,"Would be nice when the 5700x3D and 5800x3D would be produced again, but very unlikely.",Positive
AMD,">It got me thinking, is 5000 series AM4 on an old enough node that AMD could restart production cheap? Cheap enough to sell a high end x3d chip to satisfy people holding on to their old platform and RAM while the shortage is happening?  Why this will probably never happen:  * The vast majority of people do not upgrade their CPU. The entusiaste market is probably less than 1%. There is not enough volume. * Selling this processor will cannibalize their own current product line. * The stacked L3 cache could be better utilized on their current CPUs. * All fab production is concentrated on storage, ram, new CPUs and GPUs. New and Old.  The echo chamber that is PC enthusiasts has seriously distorted their perception of the PC market.",Negative
AMD,Why would they do that when they can just sell a new x3d chip at inflated prices and have every single one of them sold before they've even been shipped?,Negative
AMD,they just released the 5600F in September.,Neutral
AMD,"AM4 is just too good for me to ditch. My 6800XT holds it back in 4k gaming, 64GB 3600Mhz CL16 was dirt cheap and both single threaded and multithreaded performance with my 5950X is more than I really need.",Positive
AMD,"Considering they launched the Ryzen 5 5500X3D only 6 months ago, i would say they can, the problem is if they will. Unfortunately it can take a long time to design or adapt existing designs, test and then manufacture them in sufficient numbers for release, if they started now it would probably take a year or more to see them on the market. A 5950X3D does sound cool as hell, maybe i would upgrade from my current 5700X3D, though this CPU is perfect for my needs and the gaming i do.",Neutral
AMD,Imo the 5950X3d is the most likely option as the very last cpu on am4. dual 3XD ofc and with people starting to switch to more expensive and advanced packaging then Cowos there will be enough cowos allotment that amd can buy for older designs,Neutral
AMD,"Restart? They never stopped releasing them, with the latest being the  5500X3D released in June this year.",Neutral
AMD,"DDR6 is coming out soonish in 2027, so when that happens the cutting edge ai shit will all switch over to that. So ddr5 availability should go up permanently after that. It's gonna be a dry ass desert until then but it won't be forever. DDR5 came out in 2020 so it's already fairly old, making a switch back to 4 even temporarily quite unlikely.",Neutral
AMD,"The only one that would *maybe* make sense, and I stress it's a biiig maybe at this point, would be a 5950X3D. After that, no more AM4 anything. Otherwise, probably best to keep on cranking 5800X3Ds.",Neutral
AMD,"You can get a 5700x3D, which is a lot cheaper.",Neutral
AMD,"They could mix tiles, but would be a hard sell for motherboard partners. And why buy a am5 x3d when am4  x3d gives almost the same gaming performance?",Neutral
AMD,"They can restart Zen 3 X3D production, with a dual CCD V-Cache 5950X variant and should do so IMV.",Neutral
AMD,AM4 Zen 6 Fan Edition backport manufactured on an Intel node,Neutral
AMD,"Dont think they're advocating for proper new designed chips, just respinning up old ones.    As for TSMC, I'm sure if AMD really cared to do this, they could find some way to give up some more leading edge capacity for older node capacity.  Perhaps via agreement with some other company who would love to bump themselves up the waiting list.    I think bottom line is that AMD isn't gonna be overly concerned with things.  They're still gonna be making lots of money on all this AI stuff themselves selling CPU's and GPU's, they can take hit on the consumer side for a bit.    I also think paying high prices for AM4 processors is very stupid.  While DDR5 has certainly ballooned building on AM5 platform, the reality is that total system costs are still only gonna be like 15-20% higher.  Small enough difference where it will probably still be worth it to go with AM5 in the big picture.",Neutral
AMD,"Two years in advance, I believe...",Neutral
AMD,"There are actually older Zen3 parts with the DDR5 controller, too. As I understand it, Zen cores are largely decoupled from the wider system with their IO die interfacing with the various external components, so that may be the only bit which really needs changed to support one platform it another.",Neutral
AMD,"Part of me wants to believe it would be possible for AMD to use the old DDR4 IO die and pair it with newer compute dies with Zen 4 or 5, but even if that was true they have no financial incentive to do it.",Neutral
AMD,Or variants of it we never got (5900X3D or a 5950X3D).   It's not like TSMC 7 and 6 have companies fighting each other for their wafers at this point.,Negative
AMD,Im down,Neutral
AMD,"Also, the current price explosion of ddr5 is a bubble by an overleveraged market with inelastic demand that will likely be mostly gone by the time any of these new models would make it to the market.",Negative
AMD,The consumer market is waiting for 8k monitors with high refresh rates. And cpus and motherboards with connections that support that.,Neutral
AMD,They might want it but the rest of us want DDR5.   Nobody's getting what they want in 2026 except the billionaires.,Neutral
AMD,"They want money, and they will do whatever it gives them. Old stuff is sold, makes no profit, so yeah they want more sales. Anyways, stick to ddr4 and if ddr5 demand falls then they will have to adjust.  W11 EOL+ Crypto+ AI ... That is tiny compared to 9.000.000.000 humans that use a pc.",Neutral
AMD,And X3D doesn’t have too much to gain from “good” RAM.,Neutral
AMD,"What about people who just want an upgrade? I would pay a few hundo to get best in-slot CPU on my current platform for a quick bump to give me another year or two.  Right now that's not possible, 5800x3d is no longer manufactured and 2nd hand is prohibitively expensive...  Why not fill that gap with brand new silicon?  The question is - is there fab capacity to make it?",Negative
AMD,It was pretty clear they were dumping and clearing out stock when the 5500X3D launched during the summer.   EPYC with Zen3 and X3D is probably EOL now. Which is the main reason why AMD kept the desktop parts around as well.,Neutral
AMD,"5100x3d in 2 years, trust",Neutral
AMD,"Gonna be plain, with IBM and Cisco starting to pivot, avoiding too much waste of Radeon dev time on AI crap may end up being wise.",Neutral
AMD,5500X3D is just a stockpile of chips that couldn't be sold as better chips.   AMD likely was building that stockpile ever since launch.,Negative
AMD,Assuming that the AI crap doesn't crash before then.,Negative
AMD,ddr5 is unavailable becuase dram manufacturing is not miang the ddr5 chips that go into desktops and increasingly not even ddr5 chips that go into servers. why do you think ddr6 would be any different?,Negative
AMD,Even these have gotten insane. Used 5700x3Ds used to go for 150-175 eur. Now they're 300..,Negative
AMD,u/bobalob_wtf there isn't enough demand to justify a new chip SKU for better raw performance or performance per watt.  It is more likely that they'll do a new manufacturing run of an old SKU.  Will it be cheaper? They may pass on the savings of the older process node to you if it is a competitive advantage.  If now stocks of current SKUs are unavailable then buy used?,Negative
AMD,"I think you're right. AM4 was a great, long-lived platform--I'm still on it myself--but I don't think anyone building now should really be looking at AM4.  Maybe build with 16 GB RAM if you're really on a tight budget, and start with an R5 7600, and upgrade to Zen 6 and more RAM when prices come back down out of the stratosphere.",Neutral
AMD,">As for TSMC, I'm sure if AMD really cared to do this, they could find some way to give up some more leading edge capacity for older node capacity  Is the 7nm node fully booked?  Would AMD have to give up anything to get more of it?",Neutral
AMD,"Nostalgia is one hell of a drug. We saw the same with people exaggerating the longevity of Sandy Bridge during the pandemic. Both SB and Zen3 have been absolute champs, and the latter is mostly great if you already have it, but some people are close to deluding themselves because of desperation over prices and lack of realistic options to build new in the current situation.",Negative
AMD,"Those 15% are a 200 dollar difference for a 32 GB computer. That can be a very significant fraction of disposable income, and the benefits of a DDR5 CPU aren't that big of a deal in this price range anyway.",Neutral
AMD,"the mobile chips have a bunch of zen versions (2,3,3.5,4,5) running ddr5/lpddr5",Neutral
AMD,"> There are actually older Zen3 parts with the DDR5 controller, too.  Ye and Zen 4 with DDR4 controller from Zen 3 as well. They both tested the new IO die with the old architecture and Zen 4 with the old IO die during development.",Neutral
AMD,"For the silicon itself no, but if you want X3D parts packaging is the bottleneck.",Neutral
AMD,The average consumer doesnt even have a gpu that can run 2k. What are you talking about,Negative
AMD,"Not really, it's closer to less than 2billion people that have a pc and almost all of that is gonna be a basic pc that isn't upgraded. Like with everything a company makes way more profit on a overpriced ""pro"" ai chip than a cheap user version and when you have a trillion dollar order for ai gpus you will have that 50k dollar ai gpu be prioritized over a 1k consumer gpu.  The real problem is that these orders are for speculative demand and so they are just selling all of their future product without regard to how much will actually be needed.",Negative
AMD,they definitely help a lot.,Positive
AMD,"where were you last year when they were practically giving away 5700x3d on ali express? it was very clear at the time it was a very limited time deal as the chips were out of production and AMD was just using up all the silicon that didn't bin well enough to be a 5800x3d. if you were ok with your current performance and it wasn't worth the time when fantastic AM4's were only $150 shipped, why FOMO and panic now? In any case anyone building \*now\* can still get a 7500F which will out perform any AM4 CPU for $150, paired with whatever mobo is cheapest and JEDEC tier ram, then upgrade the CPU and ram in the future and be sitting pretty. Fab space is booked up for years solid, scheduling a brand new run specifically for people who missed the boat on a cheap drop in AM4 life extension isn't going to happen and wouldn't be economical for anyone if it did.  Also, are you aware you can currently just get a 5900x from ali for $250 all in? That's your one step AM4 life extension solution right there, they're available, and quite good chips. 12 core, 4.8Ghz boost, they're more than enough to get you a few more years.",Negative
AMD,"> Right now that's not possible, 5800x3d is no longer manufactured and 2nd hand is prohibitively expensive... >  >  >  > Why not fill that gap with brand new silicon? >  >  >  > The question is - is there fab capacity to make it?  And would it be profitable for AMD to release it at prices that you and others wouldn't consider prohibitively expensive?  I took a quick look at ebay, and it looks like the 5800x3d is going for just under $500, and the 5700x3d just under $350.  What price would those CPUs have to sell for new for you to consider them a good buy?",Neutral
AMD,It was pretty clear they were making brand new silicon when the 5800XT and 5900XT dropped last year. They might have stopped this year despite several new AM4 releases but availability makes me think they never stopped at all.,Neutral
AMD,"If AMD handled 10% of nVidia's output, they'd basically 2x their market cap.   There are crazier gambits to take.",Neutral
AMD,"Yeah exactly, they were all the ""bad"" 5600X3D yields which are all just 5700X or 5800X bad yields too slapped with an 3D V-Cache die on top. In no world is AMD going to TSMC and purchasing wafers to make exclusively 5500X3Ds or any other old chip. They especially wouldn't bother purchasing wafers for some transient rise in DDR5 and to give customers on older platforms upgrade options. It also takes years to do that sort of stuff and thats using old process nodes and architectures.  Even backporting a new architecture to an old platform would require re-validating for a new platform and for DDR4 memory now, getting partners to release BIOS updates and to get them on board etc. It's not that simple to just move Zen6 to AM4 for instance by replacing an I/O die and calling it a day. I can't imagine AIBs being happy about losing new motherboard sales either.",Negative
AMD,"Hey Admirable_Bid2917, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Negative
AMD,"a used 5700x3d is 300€ and a used 5800x3d is 350-400€. It is just not worth it, you can buy a brand new 14600k+a decent mobo for that amount.",Negative
AMD,"The exception is if you already have solid DDR4. In that case, it can be a pretty good idea to make a new AM4 build.",Positive
AMD,"> I don't think anyone building now should really be looking at AM4.  You're joking, right? As if anyone aside from Brad Pitt's, Elon Musk' kids or some wealthy politician could even afford actually *newly* released stuff these days, given all the price-increases …",Negative
AMD,"Maybe, maybe not.  But I definitely think they could figure out something to get it no matter, given leading edge manufacturing tends to be more desirable overall.",Neutral
AMD,"Sandy Bridge's longevity wasn't exaggerated, that arch absolutely slapped till 2020. Basically if you had an OC'd 2600K or something you were sitting on that till Intel 12th gen for a great improvement in ST perf. Or you bought Zen3. If you needed MT perf from Sandy Bridge, you likely upgraded to Zen2 or Intel 10th gen for a nice leap in performance. If you really needed MT perf gains, you were likely on Ryzen or Intel Extreme chips anyways almost every new gen.",Positive
AMD,>We saw the same with people exaggerating the longevity of Sandy Bridge during the pandemic.  Did we? Personally I was on Ivy Bridge until 2022,Neutral
AMD,"$200 difference in a $1000-1200 build isn't insignificant, but it's also probably not worth hobbling your system over, either.  Going AM5 gets you better performance for the entirety of its life, it gets you upgrade options in the future, and it also gives you better longevity so that no matter what, better stuff will be available by the time you actually do really want/need to upgrade again.  $200 over a five year ownership period is just $40/year(or like $3/month).  I think people should remember to consider this kind of perspective on things when buying a PC, at least for anybody who cares about getting good overall value.",Neutral
AMD,"I mean....that's pretty much true for everything now though, isn't it?  Given how Sammy's more or less fucked the entire market, not just DIY, and some hyperscalers are finding it pretty damn hard to jump to newer EPYC platforms, I can see there being enough demand to justify firing the Zen 3 X3D line up for a little bit.",Negative
AMD,"I am talking about upgrading from current consumer tech.   Current tech can easily run 2k monitors just fine, even with APUs. My point was not about GPUs specifically, nor about gaming, it was about 8k monitors. Dual resolution monitors are a thing, you know.",Positive
AMD,[TechSpot/HUB tested with 6 different RAM kits](https://www.techspot.com/review/2915-amd-ryzen-7-9800x3d/)  There was less than 2% variance between them.,Neutral
AMD,"5900x sounds perfect for my needs, thank you :)  Why are 5800x3d going for silly prices then? Just random market insanity?",Positive
AMD,People are gonna cry and want $200 5800X3D forgetting it had an MSRP of $450...,Negative
AMD,X3D is a separate production line with its' own constraints. There is limited packaging available for X3D that is not shared with the normal SKUs.  X3D being EOL is not tied to the normal Zen 3 SKUs. The normal SKUs they can shurn out as long as there is demand from both server and desktop.  X3D however might very well be supply constrained. And making lower margin Zen 3 variants might cut into scaling Zen5 X3D SKUs.,Neutral
AMD,"They'd be wise, in that scenario to focus on GPPU stuff and only have AI gains that come with overall uplift because, well, bubble.",Neutral
AMD,"Yep this is exactly the case some people find themselves in. They have something like an i5 6600K or 7600K which are 4c/4t CPUs, completely outdated in 2025. Yet they have 16GB or even 32GB of DDR4 that is still usable! Upgrading to something like a Ryzen 5500, 5600 or 5700X on a cheap B450 or B550 board is a huge upgrade for very little cash, and they spend $0 on RAM as they're reusing what they already have.",Negative
AMD,"Sure, but the majority of consumers for most of AM4's life cycle were on 16GB of RAM. [Go look at Steam Hardware Survey in May 2021](https://web.archive.org/web/20210512095214/https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam?platform=pc) at the height of AM4's popularity and performance leadership (well into Zen3) only 12% of Windows systems had more than 16GB and people were still rolling with Intel too back then. You'd be hard pressed to find anyone with 32GB of RAM on AM4 really in their old systems if they're still using them. 32GB really only became very prolific with AM5 and Z690 thanks to DDR5 density. I guess if you're okay with having 16GB of RAM it would be okay, but at that point might as well just sit on one stick of 16GB DDR5 or 2 8GB sticks of DDR5 till this whole AI memory shortage blows over.",Neutral
AMD,"There's a whole 30%ish of the American population (loosely college educated several years into their career and moderately successful small business owners) that are doing better than ever.   The median person (not highly educated, minimal to moderate career development) might be feeling financial pressure, but there's enough of the top third (100 million people) RIFE with cash to prop up entire industries.      There's a lot of people for whom a new computer every few years is something like 1-3% of their disposable income.",Positive
AMD,"And I'm still on Ivy Bridge today with my 3570k.    But I'm also under no illusions that my system is massively outdated and has been for quite a while and that playing most of the latest heavier hitting games is basically a total impossibility.    You and I were just being cheap/patient bastards.  It had nothing to do with our CPU's genuinely being great CPU's up through 2022, much less today.",Negative
AMD,"> I mean....that's pretty much true for everything now though, isn't it?  Not for stuff made with traditional packaging tech",Neutral
AMD,"You are vastly overestimating the prevalence of ""spec out the CPU socket type for the PC I'm building"" hobbyists as a percentage of the consumer market.  Most of 'the consumer market' buys common PCs off the shelf at big-box general retail store, and 8K dual-resolution monitors have never once entered their mind.",Neutral
AMD,"you can definitely squeeze an extra 2-3% by fully tuning hynix a-die, EXPO profiles are \*really\* loose in all the subtimings. But most people won't bother. As someone who spent a lot of time tuning ram the juice definitely isn't worth the squeeze for normal people.",Negative
AMD,"Just supply and demand. X3D's are hype (with good reason), the AM4 ones  are out of production, and people who have them aren't upgrading/selling because they are still very capable. The price isn't proportional to performance, the 5950x or 5900x for example are within 5% in performance but half the price since the x3d hype is so powerfull. Not that they don't deserve that hype, but other excellent options are overlooked as a result of how they dominate the discussion around cpus for gaming.",Neutral
AMD,"I bought a 5700x3d last fall for $135 including shipping and tax, and at that price it was a no-brainer to upgrade.  But I had already decided that $450 for a 5800x3d was too much when I already had a 5600x.",Neutral
AMD,"I’m not talking about blanket recommendations, please keep in mind steam hardware survey is not representative of commentators here.   Many of us may have picked up 32GB of fast DDR4 during the over supply for really cheap. Or even 64GB like me.",Neutral
AMD,Yup. I got into pc building again this year. Comfortably afforded a 5090/9800 etc etc. the poor are worse off than ever but the PMC middle class- especially the child free are doing just fine.,Positive
AMD,"> I’m not talking about blanket recommendations, please keep in mind steam hardware survey is not representative of commentators here.  You never said that lol.  >Many of us may have picked up 32GB of fast DDR4 during the over supply for really cheap. Or even 64GB like me.  Okay and you're the minority of gamers/AM4 users.",Neutral
AMD,No one is buying this for 800  Stop the sensational headlines  You can get a 9800x3d and 32gb ddr5 cl30 kit at MARKET PRICE for under 800,Negative
AMD,"Another clickbait article, yeah no shit if you order by highest price sales on ebay it'll look like this. If you instead look at recent sales this month they're all around $390-530.",Negative
AMD,Trash fake news. Delete this garbage,Negative
AMD,Glad I bought my 64GB of RAM months ago. Cost less that $100,Positive
AMD,How many people on am4 actually made the leap to 5800x3d than just the normal 3600 > 5600x/5700x and thats it or 1600x to 3600x to just 5600x?,Neutral
AMD,Why do we still allow Tom's Hardware articles in this sub at all?,Negative
AMD,There was a brief period these chips were an excellent buy and very cheap but that ended a while ago. Last time I checked at the middle of this year they were £350+ and now I see they are £450+.  I get that it allows you to max out an AM4 system and if you have a bunch of DDR4 that may be a wise investment still but the days of it being the undisputed price:performance king are long over.,Negative
AMD,"Hello I_Love_Cape_Horn! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,Honestly I want a 5600x3d with the lower power draw and TDP. My mATX system is somewhat small and I’m not playing the most demanding games on it but I’d like to get a few more years out of AM4,Neutral
AMD,"oh ye, i am selling my one for $69420...now go write another article",Neutral
AMD,"I upgraded my AM4 AB350 system from first gen Ryzen 1600 to 5700X3D and the gaming results lined up with the reviews. But these chips falter with high RT usage games that have even higher demands on the CPU and RAM.    For example, in Stalker2 CPU-limited scenario, 9800X3D was almost 2.5X of its performance. I doubt that non X3D would be more than 20-30% slower, so it'd be better to get AM5 instead.",Negative
AMD,"What? I mean if you'd actually wanted to spend 800$ on just the CPU, why not just get a cheap AM5/1851 one and then get yourself some overpriced RAM instead?  This makes no sense at all.",Negative
AMD,"A 7600X is faster than a 5800X3D in most games, if you are buying a 5800X3D for $800 you are an idiot.  A 5600x will still be GPU limited at the resolutions and settings people actually play games at.",Negative
AMD,No shit.  This happens with all older hardware as companies want you to buy new stuff.,Negative
AMD,I was thinking to sell my old DDR4 memory....,Neutral
AMD,This crap of supply and demand can stop anytime now.,Negative
AMD,The RAM I own will appreciate faster than gold in the next year or two woohoo,Positive
AMD,What enthusiasts are on 3 generation old hardware? I consider myself a pretty big enthusiast and never skip more than 1 generation.,Neutral
AMD,Jumping from am4 to am5 is impossible though because of insane prices too,Negative
AMD,I just bout a Ryzen 7 7000 series drum 220 brand new.,Neutral
AMD,Someone wanna give me $1000 for my... DDR3!,Neutral
AMD,Am4 is dead.   LGA 1700 is where it’s at rn. Having compatibility for either ddr4 or ddr5 ram is much better than am4.   And the lga 1700 CPUs especially the 13th and even more the 14th gen intel CPUs like the 14700k are way faster than any am4 cpu as they compete with am5 CPUs anyways.,Positive
AMD,"Maybe I should sell mine, I'm using it in an HTPC that I rarely game on anyway.",Neutral
AMD,I just sold mine for 375$ yesterday.,Neutral
AMD,"Hey I’ve got a 3800X, I’ll sell it for 3.8 million USD if anyone is interested.",Neutral
AMD,"and Ryzen 5 5500 is only $75. sure, having two more cores and 6x the L3 cache is nice, but not 10x the price nice.",Neutral
AMD,This is more sad than anything,Negative
AMD,something something... turntables.,Neutral
AMD,"Pleased my 5950x, 3090ti and 64gb of ddr4 are still keeping up just fine….",Positive
AMD,"There are still places that do sales on AM5 stuff. You can buy a 7800X3d , 32GB ram and motherboard for $580. There is literally no need to spend $800 for a 5800X3D",Neutral
AMD,Gamers would rather pay 800 for this than get Intel. The gamer brain rot is real.,Negative
AMD,Computers are quickly becoming a luxury like in the 90s.,Neutral
AMD,"they are trying their best to milk the market. Push prices up...   Fucking Tech websites, all in the pocket of other big corpo's making artificial scarcity.",Negative
AMD,"A couple of sales, (literally just a few) on ebay is no big deal...there's wacky people doing wacky things out there all the time.  The two or three people are pointing out on ebay are dwarfed by the normal sales flow from normal websites...     ""a thing happened ***once***!  let's all discuss it like it's a regular occurrence and changing the status quo! ""    it's all so tiresome.",Negative
AMD,"Check eBay.  [https://www.ebay.com/sch/i.html?\_nkw=5800x3d+cpu&\_sacat=0&\_from=R40&rt=nc&LH\_Sold=1](https://www.ebay.com/sch/i.html?_nkw=5800x3d+cpu&_sacat=0&_from=R40&rt=nc&LH_Sold=1)  $800, no not quite.  Over MSRP?  Yes.",Neutral
AMD,"Yep.   https://www.microcenter.com/product/5007092/amd-ryzen-7-9800x3d,-asus-b650e-e-tuf-gaming-wifi-am5,-gskill-flare-x5-series-32gb-ddr5-6000-kit,-computer-build-bundle  CPU, memory and motherboard $679.00",Neutral
AMD,> at [MARKET PRICE](https://youtu.be/5KXrQYWbbIs?t=17),Neutral
AMD,What motherbard ?,Neutral
AMD,"Yeah this headline is stupid. I see *new in box* 5800X3D selling for $500-600 on ebay, and plenty of used sales in the last few days between $380-500. The price has risen a bit, but not that much. It's been expensive ever since production of the 5700X3D ended.",Negative
AMD,"And even then, sales VOLUME is what matters. There will always be people who can't do basic arithmetic, who will then buy at these prices, instead of selling their old system and getting something brand new. That doesn't mean that $400 for a 5800X3D is sound market pricing, or that it is worth this much.",Negative
AMD,I built my PC a few years ago and since the cost difference was negligible in the overall build I went with 64gb of ram to fill up the 4 slots on the motherboard. When I was asking build opinion on a PC sub many people called me an idiot for wasting my money on all this ram.   Just kind of goes to show there’s no sense of asking Reddit for opinions.,Negative
AMD,"I bought a pair of 16 gig sticks in an attempt to beat the first round of tariffs. At the time I thought they were DDR4 and I wasn't paying much attention. They came in, I see they are DDR5 and think ""ok, future rig then."" They came in around 120 bucks. Fast forward to today, I check the price of the same pair of sticks, it's over 400. Genuinely considering selling the sticks for a better AM4 CPU.",Neutral
AMD,"I went from 2700X to 5800X3D and upgraded ram from 16GB to 32GB, so I'm good until Zen 6 at least. I got it for 340€ like 5 months after release in germany. If I sold it now I would make 50€ profit, its going for around 390-400€ used on ebay. Not bad.",Positive
AMD,"A 5600x will play games just fine, at the resolutions and settings people play games at you will still be GPU limited on a RTX 5070.",Positive
AMD,I went from the 5800x to the 5800x3d. Stuck the 5800x in my daughters PC instead of buying her a 5600x like I did for my son's PC.,Neutral
AMD,"It really only makes sense if you already have an AM4 motherboard. Otherwise, if you're doing a DDR4 build Intel 14th gen outperforms it.",Neutral
AMD,You can limit the power usage on the 5800x3d or 5700x3d as well if you find a deal on one of those.  I have the 5800x3d and the low power usage is great.  In a lot of games the chip is only drawing ~50W.,Positive
AMD,Because no one sane would do that... they simply took the most expensive offer on eBay and made a clickbait title around it... I may list my old PC parts for 10k; maybe they shall made another such article...,Negative
AMD,"Or it's discontinued and a sketchy seller is hoping to con someone who doesn't know any better. I've seen Ryzen 5 2600's going for 200+ on Amazon from 3rd parties while official channels were selling Ryzen 5 5600's for 130 and 3600's for 90.  Honestly, whenever I see a price on Amazon that's not remotely close to a whole number or 25 cent increment (eg. 137.53), I pause and see if something's fishy.",Negative
AMD,Could also be used by sellers to fulfill insurance replacements.,Neutral
AMD,"Seconding this, a lot of flagship CPUs released in the past 20 years are still absurdly expensive. QX9770, FX-60, P4 EE, 6950X, 9900K, etc. some of them more expensive than others, but the top CPU for a dead socket is still a pretty penny",Negative
AMD,I thought we all loved Capitalism?,Neutral
AMD,"You can still buy laptops for $200 on Amazon, they good enough to do most computing tasks.",Positive
AMD,I had a PC in the 90s didn't know it was a luxury back then guess I was a lucky kid,Neutral
AMD,"Compute has never been cheaper and fairer. You can get a Mac Mini right now for $450, which will do all you need, including light productivity, sans gaming and heavy 3D rendering etc, for the better part of the next decade.   All the harder compute that you need you can rent and get the best value, instead of shelling out for hardware that becomes obsolete within 2 years:  * for inference either get the subscription or pay as you go on Replicate, OpenRouter etc.   * for gaming get GeforceNow or similar service for $10/$20 a month. On MacOS it runs natively in AV1 with Cloud GSync, there's virtually no lag and even in very dark scenes you can barely tell it's a stream. Hardware in the back gets upgraded every 2 years, and 5 years of the service cost less than the GPU it's running on right now.",Positive
AMD,"I get your whole point, but I just want to pinpoint the wacky part, maybe they have a reason for it. Like, one quick example, they have the whole built already working, and the CPU dies or gets fried, or god knows what, and they want the same CPU, idk. Which goes hand in hand with your other point of it being 2-3 sales. The CPU is not being made anymore, so Ebay is the only alternative. Maybe even a collector. There are wacky people out there thought :P",Neutral
AMD,">$800, no not quite. Over MSRP? Yes.  MSRP was $450 and in your link there are a bunch listed at $399  What am I missing?",Neutral
AMD,Still too expensive   Still the price of a non x3d zen4/5 or raptor lake + ram kit which outperforms this,Negative
AMD,Yea I sold mine for $425 two weeks ago after upgrading to 9800x3D for $440,Neutral
AMD,"People are insane, in what world does that purchase make any sense.",Negative
AMD,"A fair number of these purchases are probably going to end up with fraudulent charges (item not as described, etc) because eBay tends to side with the buyer.",Neutral
AMD,It's worth whatever it consistently sells at. It seems to consistently sell for >$400 used.,Neutral
AMD,"It's a lot like the ""car is worth $2000 but needs $2000 in repair"" work making it worthless but they're low credit high APR folks and have cash and $2000 in repairs makes sense to get it going another 2 years",Negative
AMD,Did you ever use more than 32? How often do you need more than 16 really? I'm asking because I stuck with just 2x8 and never had any issues yet.,Neutral
AMD,And they were right.,Neutral
AMD,"Unless you play heavily CPU bound games, which many BRs, and simulation/factory games fall into.",Neutral
AMD,"""Games"" are not a uniform performance load. Tons of games will be CPU bound with *any* CPU, including the 9800X3D, even at 1440p.  As always, know your workloads and purchase accordingly.",Neutral
AMD,"For productivity yes, but not for gaming.",Neutral
AMD,"If you filter on Sold listings, people *are* paying $4-500+ for these.  It’s still insanity even if the headline is sensationalist.",Negative
AMD,Oh? I have a 6950X sitting in a closet…,Neutral
AMD,I had a 9900K in my workstation at the office back in the day.  Solid performer for the money.,Positive
AMD,Adjusted for inflation it’s around $7000. Yeah they were luxury,Neutral
AMD,"Mac's can run games  (native ports) decently too, considering their loss power draw.",Neutral
AMD,Stop being logical   We need 64gb and a 5090 to browse reddit and watch youtube,Neutral
AMD,"Okay Sam, no need to advertise your shitty services.",Neutral
AMD,"Even then, it would be cheaper to get a whole new mobo, CPU, and RAM on AM5 and end up with better performance.  It's an absolutely ridiculous price, no matter what. You'd have to just be a blithering idiot to go for it.",Negative
AMD,"If that situation forces a sale at $800 though, that does suggest that the market is very shallow.  Generally one goes on eBay and picks the cheapest or 2nd cheapest that doesn't look shady.",Negative
AMD,"One just sold for $600  Sold Dec 19, 2025    Brand New  [51 product ratings- AMD Ryzen 7 5800X3D Processor - NEW in Sealed Box](https://www.ebay.com/p/4053561416?iid=168015596458#UserReviews)  **$600.00**  or Best Offer  \+$7.50 delivery  Located in United States  [View similar active items](https://www.ebay.com/sch/i.html?_nkw=AMD+Ryzen+7+5800X3D+Processor+-+NEW+in+Sealed+Box&_id=168015596458&_sis=1)  [Sell one like this](https://www.ebay.com/sl/list?mode=SellLikeItem&itemId=168015596458&ssPageName=STRK%3AMEWN%3ALILTX)  gangster1234484 100% positive (522)     Being kind and logical, that's what you're missing :) <3",Neutral
AMD,"The RAM kit alone is more than the 5800X3D.  People aren't buying the 5800X3D over a new AM5 processor. They're buying it over a whole new platform, which is ~$1000 including board and RAM.",Neutral
AMD,What?  I'm a Top Rated Seller and very happy,Positive
AMD,14700k slams a 5800x3d both in gaming and especially on productivity tasks.,Neutral
AMD,5800X3D is [13% slower than a 14700K with DDR5-6000 in gaming](https://www.techpowerup.com/review/intel-core-i7-14700k/18.html). Unfortunately I can't find a direct comparison with the 14700K using DDR4.,Negative
AMD,"For some upgrading to AM5 would indeed cost a lot right now, but I also feel like a lot of people don't realize that the slowest AM5 CPU (7600X) is just as fast as the 5800X3D. Including for gaming.",Neutral
AMD,Nice! Those go for around $125+ on eBay regularly. People want to max out their old rigs,Positive
AMD,">Being kind and logical, that's what you're missing :) <3  Great, but that still doesn't explain the dozen other listings for new in-box chips at under MSRP prices",Neutral
AMD,"Pretty sure it's just not showing the ""Best Offer"" price it *actually* sold at.",Negative
AMD,What's so rude about what he said?,Neutral
AMD,No its not  Newegg has combo deals ram and good board for 400ish dollars  A crappy 7600 beats the 5800x3d,Negative
AMD,"There are edge cases where no CPU can touch the X3D chips (e.g. Factorio, Baldur's Gate 3).",Neutral
AMD,"Hardware Unboxed's 5800x3d vs 12700kf (with ddr4) comparison was pretty much a wash, particularly if you are >1080p. Nominal win for the 5800x3d for gaming if you are being charitable. (I have both, but haven't done any real comparison testing, particularly because I run 4k.) So add on top some clock speed gains and there you go.",Neutral
AMD,Yeah I don't really get it. When they were sub 200 or even sub 150 for a 5700x3d it was an awesome buy but now it just feels like people panic buying.   Maybe they were waiting for zen 6 and just are afraid that ddr5 will be screwed for many years but I don't see the point in paying that much when you can still buy a faster 7700x microcenter bundle  even with 32 gb of ram for 500.  Even with ram like this I don't see why you would pay over like 250 tops for a 5800x3d.,Negative
AMD,Something to understand anout the average /r/hardware poster is that they believe that X3D chips are magic performance improvers that are instantly better than every other CPU ever produced by impossible margins.,Neutral
AMD,"Not sure why you're getting downvoted. Micro Center too has brought back their bundles with RAM for around $500. And, yes, a 7600 beats a 5800X3D. A 7500f goes toe to toe with it.",Neutral
AMD,"Even in factorio once you go to [high spm comparisons](https://factoriobox.1au.us/results/cpus?map=9927606ff6aae3bb0943105e5738a05382d79f36f221ca8ef1c45ba72be8620b&vl=1.0.0&vh=) rather than the the low SPM ones, raptor lake/non X3D zen4/5 does fine.",Neutral
AMD,"Radeon subreddit is in shambles because it doesn’t support RDNA3, they’ve got pitchforks out and are claiming they’ll go nvidia next gen, lmao",Negative
AMD,Well that’s not cured my impotency.  Damn.,Negative
AMD,"I wonder if GN or HWUnboxed will roast AMD for their [misleading ""performance"" charts](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-5_videocardz.jpg) like they did for NVIDIA and MFG. I have no problem with Upscaling performance, but once you start introducing Frame Generation [like AMD has here](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-4.jpeg), you're muddying the waters of what is ""performance"".",Negative
AMD,Launched without 7000 or earlier support despite the leaks.  lol?,Neutral
AMD,AMD never misses an opportunity to miss an opportunity.,Negative
AMD,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,"Marxist-Leninist-based Upscaling. No wonder they're called Team Red. /j  This is an improvement, but I'm more excited for the hardware of UDNA.",Neutral
AMD,The Ray Regeneration thumbnail is legit just a contrast filter lmao,Negative
AMD,Would they be using their ML/AI cores of e.g Strix point/Krackan point for this too?,Neutral
AMD,Hopefully this leads to them FINALLY being competitive with nvidia in the high end again sooner than later,Positive
AMD,"It's really obnoxious that they released this but didn't roll it out to Adrenalin yet. I had to DDU and reinstall it twice, since Windows overwrote it immediately with 25.10.30 the first time.  AMD owes me 10 minutes of my life back, is what I'm saying. /firstworldproblems",Negative
AMD,does this work on RDNA 3.5? (890m specifically?),Neutral
AMD,"This wasn't the road I wanted AMD to pursue, the ""fake frames"" like Nvidia currently getting a lot of heat from.  But it just shows that AMD has no guidance except copying everything Nvidia does. Grow a pair and just make your technology better because it **IS** good right now, just not in the test metric Nvidia wants to push on consumers which is basically a big fat lie in promises and practical performance.",Negative
AMD,"It'll probably at least partially support rdna3 eventually, but it's pretty obvious that AMD just needed to get this out into the wild with at least rdna4 support asap.",Neutral
AMD,"I mean yeah, RDNA3 doesn't have the physical hardware for this ML-based stuff. If they bring Redstone features to RDNA3, it'll be entirely different.",Neutral
AMD,"I was on AMD for my last 3 GPU upgrades.  I had a Radeon 6800, then i saw AMD announcing that they'll put that card series into legacy support, which slightly pissed me off. They still make and sell 6000 series cards.  Then there's the fact that partial FSR4 support is possible on older cards, but not released or enabled by AMD.  I don't really care about the upscaling part of it, the 6800 chewed up any game i threw at it at 1440p without RT. I wanted it because TAA or FSR3 are horrid when it comes to image quality when you use them as AA solutions.   Playing something like Final Fantasy 7 Rebirth was a travesty if using TAA or FSR 3. Such a beautiful game that looks like a smudged mess with those solutions.  So i got an Nvidia 5080 for black friday sales. I basically just don't trust AMD's GPU division to not abandon even their 9000 series once they release a new series. And i'm done giving money to a  company division that is content in merely keeping their cards in ""Nvidia -50$"" price range for much lower feature sets.  I HATE AND LOATHE Nvidia as a corporation and hate that i gave them money, but i ultimately just picked the better product for my needs.  Their CPU division is banging and my 5800x3D looks like it'll keep chew anything i throw at it for a good while still, but AMD's GPU division can go fuck itself for now.",Negative
AMD,Anyone with a brain could look at RDNA3 and realize it wasn’t a major architectural shift over RDNA2. People read about a couple of low-precision math instructions and assumed RDNA3 had closed the gap with Turing. Honestly I don’t expect AMD to truly lock their feature set in until UDNA launches.,Neutral
AMD,Not so Fine wine now eh? Lol.,Neutral
AMD,"My problem whit the 9070XT, which isn't a cheap card by any standards, is the issues I came up against given the supposed 2.1a ports. The ports are not full bandwidth, I have multiple 4K monitors connected and get stuttering, freezing, and timeout issues constantly. I have to manage the displays as if I purchased a cheap card, lowering Hz here, color range there, etc. I can't run all my monitors at full specs at the same time! RIP I should have purchased a 5070ti...  Having to run my LG C5 and Alienware AW2725Q at 60Hz is crazy. Switching settings every time I want to play is such a pain.",Negative
AMD,I just build a amd pc after many years with just laptop and I saw Radeon subreddit. Is AMD really deserve that hate or Radeon subreddit is just that toxic :D?,Negative
AMD,"They shouldn't worry too much, as even RDNA4 doesn't support it in almost any use cases except for very specific games.",Negative
AMD,I went from 7900XTX to 5080 exactly because of this. And VR,Neutral
AMD,Some of us refuse to use FSR lol,Neutral
AMD,Still balding here....,Neutral
AMD,HUB is saying it sucks.   https://www.youtube.com/watch?v=LpAZF_-qsI8,Negative
AMD,"What's misleading about this? Nvidia specifically marketed their GPUs as ""4x"" performance or whatever and compared GPUs with FG/DLSS off with DLSS and FG on. These charts from AMD are specifically performance charts for FSR which means the point is FG performance.",Neutral
AMD,Gn have published a video but I have yet to watch it,Neutral
AMD,AMD's GPU market share has gone so down that most people actually don't care.,Negative
AMD,"I don't think they will do a ""AMD IS LYING!!!!"" with a stupid thumbnail like they did to Nvidia.",Negative
AMD,oh NOW frame generation is bad when AMD gets their hands on it  give me a break,Negative
AMD,"amd repeatedly said before this launch that it was exclusive to rdna4, people's own fault if they decided to assume they were lying",Negative
AMD,"I actually thought it wasn't leaks, but AMD's own statements that claimed something like them wanted to make this adoptable for multiple older architectures, and things out there. Maybe I misunderstood that. Either way, I'm glad I went with Nvidia last generation.",Neutral
AMD,"It looks dissapointing, so you didnt lose much",Negative
AMD,"Leaks from where? If it was some of the typical suspects, I'm *shocked* that they would make shit up.",Negative
AMD,They are RDNA 3.5 and ML Redstone is only for RDNA 4,Neutral
AMD,How can they be competitive with nvidia in the high end if they don't have any high end RDNA4 graphics cards and Redstone is exclusive to RDNA4?,Neutral
AMD,Lol. It’s not even 4x frame gen. How would they compete with a mid level card like the 9070xt?,Negative
AMD,Actually that Windows owes you time...,Neutral
AMD,"Nope, RDNA4+/UDNA (we presume will also support this) exclusive.",Neutral
AMD,This is what AMD has always done.  It’s part of their origin story.  AMD literally copied Intel’s silicon to break into the CPU market back in the 70s,Neutral
AMD,"FSR4 INT8 already runs on RDNA1-2 and 3. Even some RDNA1 and Radeon 7 on GCN5.  XeFG also runs on DP4a or SM 6.2 path on the same GPUs.  If Intel can, certainly AMD can. They just don't want to.",Neutral
AMD,"Yeah. I think many people have unreasonable expectations. Still, we know there's an int8 version of fsr4 upscaling out there which works pretty well. If AMD just officially published that I suspect a lot of RDNA 2&3 owners would be pretty happy. (Maybe with the added promise of trying to achieve something similar with FG and ray reconstruction)",Positive
AMD,Yes they do. Shader cores are more versatile than matrix cores and can do everything they can at a lower efficiency/performance. AMD gating FSR's newer versions and NVIDIA gating DLSS behind never hardware with that excuse is bullshit.,Neutral
AMD,Yeah but AMD claimed “Architectured to exceed 3.0Ghz”. It was a major architectural shift since even RDNA4’s boost clock did not exceed 3.0Ghz,Neutral
AMD,AMD Vinegar™️,Neutral
AMD,This myth is so heavily reliant on the R9 290X surpassing the GTX 780Ti its not even funny.   Probably users younger than those cards in here lmao,Negative
AMD,But muh drivers,Neutral
AMD,"not sure how old your card is, but is just got it recently and im running 120hz on lg c4 and lg ultrawide monitor(not sure exact model) on 240hz no issues.",Neutral
AMD,...at 60Hz? That don't sound right. You sure your cables are up to spec?    Even RX 6000 could already do 4K120 10b HDR.,Negative
AMD,Holy shit my teeth are straightened and I think one grew back,Negative
AMD,"Yes, it was a good video, discovered what other outlets did not and [Tim also did mention that Frame Generation does **not** increase performance, just perceived smoothness.](https://youtu.be/LpAZF_-qsI8?t=793) Kudos to that video, they (or Tim) did great work.",Positive
AMD,It's still presenting the FPS increases without any context for how compromised the experience is compared to regular frames. But I agree that using it as a tool to lie about performance uplift of a new product compared to the old one is considerably more dishonest.,Negative
AMD,"> What's misleading about this? Nvidia specifically marketed their GPUs as ""4x"" performance or whatever and compared GPUs with FG/DLSS off with DLSS and FG on. These charts from AMD are specifically performance charts for FSR which means the point is FG performance.  What's misleading is they're making out that the FPS you're getting is more performance and making the game ""faster"" (says it in the top right of the chart, their words not mine), yet the latency is increased and delays inputs, that's anything but faster or more responsive gameplay. If this was just upscaling I would have no problem, but as I said, once you start adding Frame Gen as ""performance"" and making the game a less responsive experience, it's not faster, you're delaying inputs for perceived smoothness in the image. Both practices are dumb and misleading and I do not advocate for either what NVIDIA or Radeon have done with marketing their products. What NVIDIA has done is worse, but go to the root of both marketing strategies and both AMD and NVIDIA are pretending like Frame Generation = more responsive and more/faster performance.",Negative
AMD,"I saw it, he doesn't even mention the graphs being misleading from the slidedeck (it's clear from the video he has access to the slides), but GN does look at latency results. In the end, I wish he was harsh like he was with NVIDIA because honestly, AMD is just copying NVIDIA's homework and using the same BS playbook, pretending Frame Gen is increasing frame rate and [making the game's performance ""faster""](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-5_videocardz.jpg) (their words not mine).",Neutral
AMD,"DW bro [we got you caught in 4K calling them ""fake frames"" just earlier this year.](https://www.reddit.com/r/hardware/comments/1n0qtts/nvidia_geforce_rtx_5090_and_the_age_of_neural/navh6bm/) Before you say ""that's just one post or is a joke, [here you are doing it again in a separate thread](https://www.reddit.com/r/hardware/comments/1n67v2u/steam_hardware_software_survey_august_2025/nc3jscw/). Enjoy!",Neutral
AMD,"Bro lmao, every time I see you defending AMD like their white knight. Redstone is clearly DLSS 1.0 and AMD rushed to release it because they're getting stomped in software features.  Though in AMD fashion, they manage to incredibly disappoint as always and reviewers are just letting you know.",Negative
AMD,"There is a leaked INT8 path for FSR4 that works on older hardware and has been implemented by others (i.e., on Linux in Proton-GE).  It works pretty good already, which makes AMD's reticence to put it out officially *baffling* -- especially since they aren't putting RDNA4 into APUs for a while yet, and want to sell a bunch of those in gaming-focused handhelds and Steam Machines.  AMD has a good software product here for once and they need a broad installed base to drive developer adoption, but they don't seem to care and it's infuriating.",Positive
AMD,"No one cared about the new framegen (on older cards), what people wanted is FSR4 upscaling on RDNA3/2, which already has been proven to work well on Linux",Neutral
AMD,"They aren’t even RDNA 3.5, they are XDNA 2. That’s why I asked whether they will leverage XDNA 2.",Neutral
AMD,"4x frame gen is really niche, there are so few cases where it actually makes sense and doesn't cause an unacceptable amount of artifacting and/or latency. You pretty much need a 240hz+ monitor, for one thing. I don't imagine that being a major factor in almost anyone's purchase decision.",Negative
AMD,"4x FG is a gimmick at this point. Maybe FG progresses to the point that it's viable in the future, but it really isn't right now.",Neutral
AMD,Yeah that's fair. Windows loves to replace new drivers with shitty old ones against your will.,Negative
AMD,"The fact that the INT8 version got leaked the way it did says...something.  It is pure speculation at this point by anyone except AMDs management as to why they have not released drivers that include INT8 for older RDNA versions but I think based on the independent testing from that leaked version that not officially releasing it is bad.  It is one thing to want to sell new cards but it is quite another to have something like the INT8 be out in the wild and then try to ignore its existence for the owners of cards not really that old.  And given how they recently they tried to put cards that they still sell into ""maintenance mode"" it really does seem like some parts of AMDs management is not making good decisions for their customers.  Now maybe their data shows that such decisions are better for their quarter to quarter bottom line but I really do question if that is the case.  I'd have to see some *hard* data to prove to me that whatever additional profits they are making but implementing these decisions are adding value to the company/brand over these anti-customer moves.",Negative
AMD,"It's pure market segmentation. Even if it doesn't work the same, they could still allow older cards to get it.",Negative
AMD,People use fsr3 to play at higher frame rates. So why would they release something that almost entirely fails to do that on the most sold gpus which are the low end 6600s and 7600s?,Negative
AMD,"It runs, but performance impact is considerable and inconsistent and quality isn't on par with FP8 either, no?   Could be they decided against releasing it in this state because of it.",Negative
AMD,The unreasonable expectation that cards that are still being sold get feature support,Negative
AMD,My 280X survived until Overwatch 2.,Neutral
AMD,"I have had enough discussions with them, and the thing they call driver is adrenaline software which is driver control panel, clearly not understanding what a driver is.",Negative
AMD,The only benefit I can think of is maybe better motion clarity by pushing the FPS to 200+ or something if you have a really good monitor (i.e. an OLED). But even then the hit on latency means it would only work well for games where latency is not hugely important.,Neutral
AMD,"If I recall, the main harshness on Nvidia was the way they were trying to push reviewers to only review performance with frame generation enabled, an inherently dishonest take.  To be fair, I also haven't seen the video on this yet either, but I think ""frame gen generally sucks in these ways"" is pretty well established and those factors are unlikely to change drastically",Negative
AMD,"I saw first 3 minutes, he did said he didn't had much time for this video  but still doesn't excuse the ""less harsh"" opinion on amd.  Will update this after watching whole video",Neutral
AMD,"Both are bad, at least this is being used to show \*gains* from having the feature on or off, rather than presenting it as \*gains* over the previous generation of cards for the purpose of representing a new product's performance as higher than it really is",Negative
AMD,Proton-GE doesn't implement the INT8 model it's the FP8 model running through the cooperative matrix extensions added to Mesa.,Neutral
AMD,"I mean, why did you expect this? They never said they will do it.",Negative
AMD,Another reason to dump Windows,Neutral
AMD,"I wish they would.  Isn't the NPU in the 90x0 an XDNA also?  That's what is doing this upscaling.  But, who knows if Windows is locking it down for CoPilot, or if it is accessible in the same way as the one in the GPU (since it is technically part of the CPU).  I bet they will figure it out though.  With Intel about to release their own competitor to it (maybe... no GPU benchmarks yet), they will want to use all they have to combat it.",Neutral
AMD,Yes everything is a gimmick until AMD releases a shittier version of it and then gets praised sky high.,Neutral
AMD,With a 4k240hz oled (or any other ultra high refresh rate oled) mfg 4x is literally transformative despite the 2 Steves telling you otherwise.,Neutral
AMD,"Seconding what the other comment says, it's great.",Positive
AMD,The market segmentation is stupid.  Why NOT want to sell more RDNA3 GPUs besides RDNA4? WHY make consumers reluctant to buy more AMD products? It's so stupid.,Negative
AMD,"Path Tracing, DLSS Transformer and Ray Reconstruction Transformer can run on laptop 2050. Or MX 570.  What's your point exactly?",Neutral
AMD,Performant impact isn't that considerable.  It's consistent.  Quality is almost identical.,Neutral
AMD,"Because it was already developed, exists, and all they had to do to get the easy win was launch it officially.   But they chose not to do so while still making every APU including $1500+ Strix Halo products that could use it their only option.",Neutral
AMD,"It’s a bit more complicated. Krackan point/Strix Point are APUs, is everything on the same chip so… why not?  Also I’m on Linux, firmware/drivers are there but there is no program/frameworks using them… So it’s purely a drivers issue from amdgpu to actually schedule compute on the NPU.",Neutral
AMD,You guys say this but people still consider FG a gimmick despite FSR3 supporting FG for years now (and doing a decent job with it too tbh unlike the upscaler).,Neutral
AMD,Not every negative Nvidia comment is a pro-AMD comment. Stop promoting tribalism for 2 giga-corporations that don't give a shit about you.  I genuinely don't think 4x FG is a valuable feature at this time. The latency hit and the image degradation are not worth the smoothness.,Negative
AMD,"I really expected them to announce an MFG, you know damn well that they must have been working on it since the moment they caught wind of Nvidia having it.  It must be a lot harder to do as well as it's already being done by Nvidia than people expect - everyone seems to think everything these days is practically just checking a box off.",Negative
AMD,"You think shareholders are going to like that old product is cannibalizing new? This is exactly what happened with the 1080 Ti.  Sure, it's irrational from a consumer and engineer perspective, but nobody cares about them. They only care about the shareholders.",Negative
AMD,Doe dlss transformer only give 5 extra fps on a 2050?  And path tracing is designed to look good not help games run better. Your example would make sense if there was somehow less light bounce than in rasterized modes.  Fsr4 on rdna3 fundamentally fails at the thing its most used for.,Negative
AMD,"The xdna npu is a completely different architecture (based on xilinx IP) than the rdna4 ml extensions, which are a set of new shader instructions.  They don't really have anything in common in terms of architecture, I'd be surprised if the amdgpu driver ever ""supports"" both, as it'll be effectively adding an entire new driver stack beneath that interface for the npu, and much of that interface would simply be not relevant to the npu (and likely the npu will need new interfaces that aren't relevant to the GPU side of things either). It'll just be functionally 2 different drivers sharing a name.",Neutral
AMD,"Neither of the things you mentioned are even remotely true. Latency hit is negligible if you're close to base FPS of 60 or higher, Nvidia Reflex is far far better than AMD's Anti lag. And Reflex 2 will kill the latency debate with FG once and for all.  And personally I haven't noticed any image quality issues either. The FG model they trained is very good. It's an amazing technology for me, I have a 360 Hz OLED monitor and it's sublime to game in the  200-360 FPS range.   Also let's not pretend to not know fans of which corporate, Intel, Nvidia or AMD represents a literal cult.",Neutral
AMD,"Lol, the 'shareholders' do not care about Radeon consumer products. Most AMD shareholders probably aren't even fully aware of these DIY discrete GPUs, they are a rounding error in the business.  The amount of sales lost to people buying RDNA3 over RDNA4 due to FSR4 is essentially $0 in the grand scheme.",Negative
AMD,It would be hilarious if a Samsung chip has newer RDNA IP compared with AMD’s own APUs.,Neutral
AMD,Is Juno expected to be a mobile version of RDNA4?,Neutral
AMD,">PhoneArt reckons that the prime core will reach a maximum clock frequency of 3.9 GHz. A commenter, Erencan Yılmaz, reckons that this figure should be reduced to 3.8 GHz, due to power consumption considerations when looking at a 2 nm GAA-based design.  What does this even mean? Are they implying that GAAFETs have poor power frequency scaling at the highest end of the V/F curve or something?   Anyway, interesting to see the % Fmax gap between TSMC and Samsung based designs for the P-cores remain around the same.",Neutral
AMD,Very interesting. I would like to know more,Positive
AMD,"Like Google, Samsung has already taken steps to move away from ARM GPUs.  The end game is to replace the ARM CPU with a RISC-V one. These steps are just for readiness.",Neutral
AMD,It's actually quite probable since it actually features AI upscaling and frame generation and rdna 3.5 doesn't support it,Neutral
AMD,"> What does this even mean? Are they implying that GAAFETs have poor power frequency scaling at the highest end of the V/F curve or something?  If you look at the actual comments, they supposedly have their own leaks of the specs showing 3.8. The power consumption comment was a reply to some rando comparing clock speeds to Snapdragon.  Just lazy reporting.",Negative
AMD,"Money  Making GPU tech from scratch is stupidly hard. And it’s going to be hot garbage for a few generations until you polish it.  Look at Intel, despite making iGPUs for decades, still had tons of trouble making decent architecture for performance.   Qualcomm bought their GPU tech from AMD (was ATI back then).  Apple managed to do it, but I bet they were working internally for a few iterations before it was good enough to ship.   Just licence the GPU and not reinvent the wheel if you don’t need too.",Negative
AMD,"Apple didn't make their own gpu from scratch tho. It's more like a custom power vr gpu in it's first iterations. The first ""custom"" apple gpu is a power vr gpu with ""image block"". Which is actually a feature that no other tile based gpu had, only the adreno 840 released this year has this feature, imagination and Mali don't have it yet.",Neutral
AMD,"I think people forget a fully featured GPU arch is not just a collection of ""dumb"" SIMT compute elements.  There is a lot of arcane knowledge. Modern GPUs have so many specialized parts that are domains onto themselves - display drivers, video encoders/decoders, TMUs, RT engines, schedulers, memory & cache management....",Neutral
AMD,Yea I wasn’t even confident they did it either. Just further showcases how difficult it is.,Negative
AMD,"If you saw the similar Hardware Unboxed video from a few days ago, this one agrees with it and presents the info in a different way.  AMD urgently needs to fix this.",Negative
AMD,Very clear Redstone needed more time in the oven. Also it's going to struggle to gain traction unless they add support for older cards.,Neutral
AMD,The frametime issues are truly catastrophic. Looks worse than when I would force Crossfire on games that didn't support it even. I don't understand why they thought it was a high-quality release to represent the brand. WTF man.   At least it looks good and they can theoretically fix the frame pacing. They never did on FSR 3.,Negative
AMD,Maybe Nvidia was up to something with their Flip Metering stuff. The frame pacing of DLSS FG/MFG is flawless.,Neutral
AMD,FSR 3.0 all over again. Their framegen was also unusable on launch. They really never miss a chance to miss a chance,Negative
AMD,"V-Sync on driver level, cap frame rate -3 of your refresh rate   It mitigates the frame pacing in CP2077 and completely solved it on different games  I know it's a crutch but at least it's something until they solve it.",Positive
AMD,"It seems fixable by software (driver), so I do not despair as much as many others.  They needed something out to show their progress.  I'm hopeful it will get much better in the coming months.",Positive
AMD,"What confuses me about this is that the framerate and frametime graphs displayed by MSI Afterburner in many games tend to not be flat with DLSS-FG on my 4090. In fact, FSR-FG often appears flatter. However, the DLSS-FG tends to subjectively feel smooth to me (so long as it's not inheriting stutter from the rendered frames).  Does anyone have any explanations for this in light of the HUB and DF videos? Could the flip metering hardware of the 50 series be playing a significant role here (I think both HUB and DF used 50 series cards to compare FSR Redstone to)? Is there an issue with using MSI Afterburner's framerate and frametime graphs for this purpose (I can't seem to post a screenshot unfortunately)?",Neutral
AMD,"With nvidia bowing out of consumer GPUs next year, AMD is lining up the fine wine perfectly.   When they fix it next year, I hope the media covers it equally as positively as they were critical.",Positive
AMD,"Yeah, I think in Digital Foundry's podcast they called out the Hardware Unboxed content as excellent and basically said ""I am not sure we even need to make a video now, but we will"".  And I think it's good that they did, more attention on this can only be a good thing.",Positive
AMD,"They will fix it in RDNA5, just like how adding frame generation was a “fix” for FSR3 and it being limited to RDNA4",Neutral
AMD,It might be an unfixable hardware flaw.,Negative
AMD,Why urgently? It's an optional feature.,Neutral
AMD,"The ""time in the oven"" is releasing the feature in RDNA5, not in RDNA4. Just like how frame generation was a demo feature for RDNA3, Redstone is a demo feature for RDNA4 with the real version in RDNA5.",Neutral
AMD,Nvidia did it by adding ML hardware support to cards well before they were needed with the RTX 2xxx cards. It's surprising AMD waited so long to do it. They must have thought traditional algorithms would work just fine.,Neutral
AMD,"Yeah seems so. However, if they have to spend more engineering time/power on improving advanced features I would expect porting them to the older gens is pushed further down the timeline.",Neutral
AMD,What did you expect though? It’s AMD. Their software is always a couple years behind.,Negative
AMD,Don’t they have flip metering on RDNA 4?,Neutral
AMD,"it's not flawless, unfortunately. for some games though. yes, it's miles better than FS FG, but there is still room to improvements  take Indiana Jones with path tracing for max GPU load, take RTX 5090, run it with 4xFG without frame cap and check msbetweendisplaychange with capframex. it will have the same sawtooth graph with some short lived frames, but to a lesser degree ofc. it will be very noticeable to the naked eye on OLED monitors because they will flicker due to those variations  reflex by itself (and reflex is forced on when FG is used) also adds not so perfect frametimes that can be seen with msbetweendisplaychange in some heavy games (Cyberpunk 2077 would be another example)",Neutral
AMD,"Has anyone done some god quality testing on the frame pacing of no flip metering vs flip metering? The only such coverage I recall finding is this is [this Gamers Nexus clip](https://youtu.be/Nh1FHR9fkJk?t=1922), but they only tested this on two games, and only one of the two showed an obvious framepacing improvement from the 4090 to the 5090.",Neutral
AMD,"Someone should remind them of that old adage: ""Better to remain silent and be thought a fool than to open your mouth and remove all doubt.""",Neutral
AMD,"Unfortunately, this only solves the tearing issue, but doesn't completely address the frame pacing issue. It also deprives us of the low latency benefits of Antilag2 and adds a sync delay, although not as significant as without the frame rate cap. The increase in latency can be easily verified using the reflex monitoring built into the Optical scaler.",Negative
AMD,"There are different statistics that you can use to populate your frame time graph, each of which are valid depending on what you’re trying to show.   Pure frame time measurement, as in “this is how long it takes to process each frame” is valid. But so is ‘ms between presents’ and ‘ms between display change’, the first being the timing of the frames being presented to the render queue, and the latter being the rate that the actual display updates and shows the new frame. Both of these measurements are captured by presentmon and frameview. I’m not sure exactly what measurement afterburner uses, but I suspect they are measuring pure frame time. But if you looked at time between display change it would probably show the issues that hardware unboxed and digital foundry showed.",Neutral
AMD,Its rumored they will reduce production however nothing is confirmed. Nvidia still makes A LOT of money from gaming and wil lnot be giving it up. They were about $100 million short of a new record in gaming revenue last quarter.,Neutral
AMD,People said they will bow out of GPUs since 40 series just launched,Neutral
AMD,"With nvidia bowing out of consumer GPUs next year   People has been saying this for YEARS now and their gaming market share/revenue has only gone UP and UP. I don't know why anyone with enough sanity would believe this stupid narative   And let me ask you, if shortage hits Nvidia and forces them to reduce gaming GPU production, why would you think AMD will be safe from it?",Negative
AMD,Nvidia aren’t bowing out. They’re just reducing production on the 5000 series. They’re also going to be launching the 6000 series.,Neutral
AMD,lol,Neutral
AMD,"[amd isn't nvidia, they don't release slop. they want to make sure they realease quality products for gamers](https://www.reddit.com/r/hardware/comments/1nw18md/comment/nhdqwi7/)     \- you",Neutral
AMD,"> With nvidia bowing out of consumer GPUs next year, AMD is lining up the fine wine perfectly.  If they don't fix this ""optional feature"" (and future ""optional features"") AMD will be lining up a miraculous market share loss against no competition.",Neutral
AMD,If they leave things broken developers will ignore Redstone. Redstone is supposed to be AMD's answer to Nvidia's suite of DLSS features.,Neutral
AMD,"because all of this is hurting their brands even further, as if gating FSR4 (ML) behind RDNA4 didn't piss people off enough.",Negative
AMD,"There doesn't seem to be anything preventing RDNA4 from running this correctly, it has support for hardware flip-metering. AMD engineers just fucked this implementation up and need to fix the software.",Negative
AMD,"If I had to choose between them supporting my card fully and them fixing up and keeping Redstone competitive, I'd take the latter.  I bought my card for the features it had at the time of purchase. I didn't expect future new stuff beyond maybe FSR4. Making an official WMMA / INT8 version for games to fall back on would be more than enough, but I don't expect that to come.",Neutral
AMD,remind me who had gpu driver issues this gen again?,Negative
AMD,"Hadn't heard about it but they support ""Hardware Flip Queue Support"", which I think is the same thing?  But they [advertise it](https://www.notebookcheck.net/fileadmin/_processed_/2/e/csm_RDNA_4_Architecture_Press_Deck_page-0005_768d67dd27.jpg) with the following benefits:  1. Offloads video frame scheduling to the GPU 2. Saves CPU power for video playback  I don't think it has a role in frame generation, or even gaming, I think it mostly has to do with video playback.  Maybe it is the same thing and Redstone is just bad at frame pacing anyways?",Neutral
AMD,"As someone who came from a 40 to 50 series GPU, I can tell you it's amazing. It literally fixed VRR flickers for me and the pacing is flawless. The effect is exacerbated if you have an OLED display as it has near instant pixel response time.   The end result is a flicker-free smooth gameplay. It's hard to explain but it feels like I'm playing games on a thin fabric, it's that good.   So if anyone's on an OLED and hates bad frame pacing with VRR flickers, upgrading to a Blackwell GPU is the way to go, thanks to its HW flip metering logic.",Positive
AMD,I can only talk from personal experience but I have a 40 and 50 series gpu   I find Frame gen literally unusable on the 40 series card whereas I can literally not tell it is on with the 50 series   It felt like fucking magic to me   The 50 series is also a lot faster overall but I went up to 4K at the same time and am getting less frames so it’s not just more performance,Negative
AMD,You should apply this to yourself instead of typing it out bro,Negative
AMD,"> But so is ‘ms between presents’ and ‘ms between display change’, the first being the timing of the frames being presented to the render queue, and the latter being the rate that the actual display updates and shows the new frame.  So I take it that the former is the time between frames entering a queue of frames to be sent to the monitor, while the latter is the time between those frames actually being sent to the monitor?  Anyways, I installed PresentMon, and using various metrics:  - FrameTime-Display - FrameTime-Presents - FrameTime-App - Ms Between Display Change  I couldn't notice any difference in these graphs between DLSS-FG and FSR-FG in Cyberpunk and Avatar (This is on 40 series, so no flip metering). With MSI Afterburner, the lines for both framerate and frametime appeared much flatter for FSR-FG in both games (even though it didn't subjectively feel smoother than DLSS-FG to me).  At times, FSR-FG has felt noticeably _less_ smooth than DLSS-FG in Avatar, but they both felt about the same on this particular occasion.  Others are reporting that DLSS-FG felt much smoother to them after upgrading from 40-series to 50-series, so I wonder if DLSS-FG isn't much smoother than FSR-FG on the 40 series. Also, I wonder if some of the FSR-FG framepacing issues are inconsistent, getting okay-ish frametimes on some occasions, but othertimes getting awful frametimes in the same game.",Neutral
AMD,so what does that mean for consumers if nvidia is going to reduce production but still wants similar gaming revenue?,Neutral
AMD,Wym they're clearly bowing out right now. They only shipped 11 million GPUs this quarter compared to amds 900k.,Negative
AMD,> They’re also going to be launching the 6000 series  That's a mid 2027 launch at best,Positive
AMD,"40% is significant, and you know they're going to be top-heavy too, they're not going to waste hardware on budget 6060s. $3k GPUs is inaccessible for 99% of gamers that it's basically bowing out of the segment.",Neutral
AMD,Perfection,Neutral
AMD,"To be fair, if it's true Nvidia is cutting GPU supply by 40% soon, AMD will probably have no problem clearing their inventory if it's the only thing available at a reasonable price.",Neutral
AMD,"You realize most people pick up Radeon GPUs because they're incredible value for money. It's the raster and VRAM that is the attraction, Redstone is just a cherry on top.  Seriously, this is textbook example of loss aversion. Had there been no Redstone everyone would have been fine, now we get something as a bonus and although it's not quite ready yet it somehow diminishes the original value of the product?",Positive
AMD,"Don’t worry, I don’t think its an either/or. Its just an order of priority. Ignore the doomsayers, Radeon’s given every indication they intend to bring FSR4 to RDNA3. They aren’t even putting RDNA4 in their APUs in 2026, so supporting it going forward is pretty much a necessity for those lower power devices.",Neutral
AMD,"Literally right now, as we speak, NVidia drivers are mostly great while AMD struggles with whole host of problems introduces by fresh branch.",Negative
AMD,Adrenalin has been crashing so much on my windows clean install that I just said fuck it and installed a Linux distro to see if the problem is software or hardware lmfao  What a horrible purchase I made with my 9070 XT. I regret it SO MUCH. By the way I bought it because of FSR4 support specifically.,Negative
AMD,Dude the current 25.10 drivers are the worst they've been in years and some people are even using 2024 May drivers because for some reason later drivers cause hard pc shut downs in the Spiderman trilogy.,Negative
AMD,"Nvidia, AMD, Intel. You just hear about Nvidia's more because they sell like 95% of all GPUs",Neutral
AMD,Oh right i forgot AMD has the best software. Their upscaling tech is years ahead of everyone else.   I heard they give out free handjobs with each GPU purchase. They’re just that good! That’s why everyone owns one right?,Positive
AMD,"I can't remember if it was Digital Foundry or Hardware Unboxed, but someone mentioned it was the same thing. The video frame scheduling on GPU.",Neutral
AMD,"wow, that's good to know. honestly this makes me want to downgrade from a 4090 to a 5080, lol. frame gen on 40 series is almost unusable due to VRR flicker, i had no idea 50 series fix it.",Negative
AMD,On a 4070 Super I can't tell when frame gen is on. I used it in Cyberpunk to get above 60 FPS. The base framerate was in the 50's with all the cool path tracing stuff and I couldn't tell it was starting in the 50's.,Neutral
AMD,">This is on 40 series, so no flip metering  I believe both Hardware Unboxed and Digital Foundry showed the issues specifically on Radeon GPUs using FSR FG, No?  There are going to be differences between how AMD and Nvidia handle frame pacing, even outside of the FSR/DLSS conversation. There could be something about how Nvidia handles frame pacing that is better for FG, but is not a part of DLSS FG specifically.",Neutral
AMD,Nvidia will increase prices.,Neutral
AMD,So then you agree they’re still making GPUs then?,Neutral
AMD,Everything after your first sentence is pure speculation. Nvidia has like 95% of the GPU market. They aren’t just going to sell 6090s lol,Neutral
AMD,"Friend - they'd reduce production on top tier as  * AMD doesn't compete on that level * their OEM numbers are probably 60-65% of their volume  If anything, they will pump out 6060s to keep AMD out of Steam Surveys since the die will be miniscule thus high yield per wafer, in consumer eyes, and most important - in affordable products thus increasing their user base and indirectly influence.  They can afford for 6090 tier buyers to eat the cost - as they've done willingly for halo GPUs since reviews existed.",Neutral
AMD,Chip yields increase exponentially as area is reduced linearly. They will try to sell traditionally 50-tier sized chips in 70-tier cards and be not much less profitable than the huge AI chips while hedging against the bubble popping.  They could also do another generation of dual fabs where Samsung or even Intel produces consumer chips while TSMC fabs for their data centre designs.,Neutral
AMD,In 2021 Nvidia GPUs were all sold out and going for 2-3x MSRP (when you could actually find one) because of crypto and AMD still couldn't take advantage of the situation.,Negative
AMD,"And they are 'incredible' value for their money because they offer similar features. No one is going to buy an AMD card without FSR, FG, etc... in todays market.   Raster time is over",Positive
AMD,"Hmm no, If I wanted raster I would buy last gen used or on sale as usual, this gen was different because it's supposed to be the one that gets upscaling, ray tracing and frame gen right so NVIDIA tax becomes unjustified.  I'd get a new GPU to step up to 144fps and that requires both upscaling and frame gen, actually I could do without ray tracing but the reason to have a >60fps display is that frame gen is supposed to be ok at higher frame rates. Redstone is not.",Neutral
AMD,"I always love that response in a literal thread about AMD's driver/software issues/bugs.  I'm surprised bro didn't just say  ""I have a 6600 XT and have no issues.""",Positive
AMD,Sounds like a console fits better for you,Positive
AMD,This is no surprise. The 5070 Super 16GB will be the new 6080,Neutral
AMD,"you realize the dram shortage plays in AMD's favor right?  game devs aren't going to optimize their games, your best hope is nvidia figures out their fake vram neural texture compression just so you could have the privilege of paying $800+ for a xx70 gpu with MAYBE 6gb of VRAM in 2026/2027  and by then AMD will have ironed out Redstone, maybe even be on UDNA and have it backported to GPUs with 16gb+ of VRAM  raster will prevail",Negative
AMD,8GB at any rate with how difficult it will be to acquire VRAM,Neutral
AMD,"Most NVIDIA cards except for the 5070 have the exact same VRAM as the AMD counterpart, so I don't know how this plays into AMD's favour 😂  Also Redstone wasn't important according to you, now it's suddenly important",Neutral
AMD,"talking mid to long term, when nvidia cuts 40% of gpu production, they will recoup costs by selling $1000 midrange gpus while AMD will continue to provide GPUs with more vram at better value and feature parity  redstone will be fixed, that's the point.",Neutral
AMD,"Ah yes, because AMD is obviously not affected by a GLOBAL shortage and definitely doesn't need to cut production and raise the price  The fact that Samsung just reported that they have no stock at all definitely won't affect AMD but only NVIDIA",Neutral
AMD,"Last quarter shipping numbers were 94% to 7%  NV cutting it be 40% is only ~38% drop, still flooding the market >6:1 over AMD.  These people are over dosing on the kool aid.",Negative
AMD,But have you considered that AMD is our lord and savior?,Neutral
AMD,Venice is the next release. This would have leaked ages ago unless they are going for dual supplier.,Neutral
AMD,"AMD already announced they taped out Venice on TSMC 2nm back in April.  [https://www.amd.com/en/newsroom/press-releases/2025-4-14-amd-achieves-first-tsmc-n2-product-silicon-milesto.html](https://www.amd.com/en/newsroom/press-releases/2025-4-14-amd-achieves-first-tsmc-n2-product-silicon-milesto.html)  If they use Samsung at all, It is probably going to be something else.  That said, there are going to be Zen 6 EPYC SKUs that use different chiplets (there are at least two chiplet variants).  So it is possible that ""Venice"" is referring to a family of CPUs and there could be some Samsung based SKU in there.  But I think it is more likely that this headline is wrong, and AMD is contemplating Samsung for a different product.",Neutral
AMD,If i had to guess AMD is probably looking to offload lower end and mobile SKU's to Samsung to save on costs and get more TSMC production dedicated to more important datacenter/mainstream silicon. so Samsung might take over APU silicon for desktop and mobile along with motherboard chipsets and maybe some entry level desktop GPU's and most laptop GPU's.,Neutral
AMD,"AMD and TSMC have been pretty intimately close ever since AMD ditched global foundries. Also TSMC isn't too kind on companies that are unreliable partners, AMD just hopping to Samsung would be pretty shocking",Negative
AMD,Moving away from TSMC is always a good thing,Positive
AMD,"Ram shortage.. You need a deal for quota package memory and GPU, if not.. Who's gonna buy a new CPU.",Negative
AMD,TSMC ?: [https://www.tomshardware.com/pc-components/cpus/amds-first-2nm-chip-is-out-of-the-fab-epyc-venice-fabbed-on-tsmc-n2-node](https://www.tomshardware.com/pc-components/cpus/amds-first-2nm-chip-is-out-of-the-fab-epyc-venice-fabbed-on-tsmc-n2-node),Neutral
AMD,Why would AMD hamstring their very populair and performant server line by switching to a subpar node? Delays or price hikes at TSMC?,Negative
AMD,So they were unable to secure capacity at TSMC?,Neutral
AMD,I thought Venice was old news a long time ago.      [https://www.techpowerup.com/review/amd-3800-plus-venice/](https://www.techpowerup.com/review/amd-3800-plus-venice/),Neutral
AMD,"Probably not for launch, but filling up capacity later on.",Neutral
AMD,Possibly Samsung will be building the IMC,Neutral
AMD,Maybe lower end skus?,Neutral
AMD,Maybe iodie? Gpu?,Neutral
AMD,"Moving from TSMC's 2nm process to Samsung's 2nm would likely be a big downgrade. I believe AMD would have to compromise on frequency, power efficiency, or yield and even in the worst case it has to tweak the architecture.",Negative
AMD,The power efficiency throws itself out of the window,Neutral
AMD,"It goes both ways. if TSMC provides capacity to their latest nodes to every company that asks for it, giving AMD less allocation than they want, then AMD has to look elsewhere to fill the gaps. Can't rely on any one company for everything.",Neutral
AMD,They are business partners not spouses in a relationship,Neutral
AMD,"Unless you want to compete at selling the best high performance accelerators, then it makes sense why it is AMD and not Nvidia",Neutral
AMD,"Yea, at best it's dual sourced, or maybe it's going to be used for the IODs, if Venice actually uses Samsung (which I doubt).   FWIW, another prominent (though I'm very dubious about how accurate he is) twitter leaker, jukon, thinks it is for the PS6.   IMO, this rumor ends up going nowhere. We went though similar things with Samsung 4 and 3nm.",Neutral
AMD,"Because AMD has significantly more understanding and insight regarding the processes involved, than random gamers?",Neutral
AMD,"Gives them more leverage in negotiating prices with TSMC, also could be looking at Samsung as a second source; AMD is probably doing enough volume now in the datacenter market that they can justify the costs of adapting their design to Samsung's fabs, and then use the Samsung chips for lower end datacenter products.",Neutral
AMD,GPUs more important?,Neutral
AMD,Vertical integration in CPU/GPU/memory is now the hottest thing. Amd wants one contractor to provide all the comments and and can have a streamline input in the entire stack without delays in communication,Positive
AMD,Because TSMC has too much demand and can’t keep up? They have also raised their pricing a absurd amount as they had no real competition,Negative
AMD,"Why would Nvidia hamstring their own very popular GPU line by switching to a subpar node with their Ampere generation?   Not saying I fully believe the rumor, just stating that it's not a totally implausible suggestion.",Negative
AMD,"Writing is on the wall with China and Taiwan, AMD won't be the first to move either.",Neutral
AMD,Capacity for Venice was secured eons ago. They're already sending samples to hyperscalers...,Neutral
AMD,wow TPU is way older than I thought lol,Neutral
AMD,"I remember that specific article! What a trip down the memory-lane … Thanks for this!  > Advanced Micro Systems (AMD) has released a new revision of their Athlon64 S939 […]  *Oh dear, the glorious socket 939 and its Athlon&nbsp;64* — Makes me a bit melancholic already.   *That was the time to be alive*. That was true journalism at heart from enthusiasts!   Explaining every short and abbreviation with the respective written-out long version and bringing out pieces for actual hardware-hits (instead of today's hit-pieces over the next refresh-cycle), where you could readily feel the joy of the editors themselves writing the article, describing new hardware between the lines.  Not the nonsense clickbait sh!t we have to day, which fabricate news around a single ~~Twitter~~ *~~X~~* *Twix*-post from some leaker no-one knows anyway …  Back then, you could go days, not seldom even a week without a single hardware-news, and no-one bat an eye, as it was only fueling anticipation and building up anticipatory excitement.",Positive
AMD,Athlon 64 3000+ was my first CPU that I got brand new. I upgraded from a very ageing Pentium 133MHz. 8 years of new releases brought 13.5x of clock speed increase as well as a lot more instruction sets. Now my current PC is on an over 9 years old platform...,Neutral
AMD,"Doesn't really make much sense given it'll go against their super successful scaling strategy with their CPU products.  They cant just swap TSMC for Samsung chiplets and have everything all work out the same because they're designing these Epyc packages based on a very specific die size for the CCD's.  This makes scaling super easy for them in so many ways.  Adding in some new CCD based on a wholly different process tech seems like it would throw everything out of whack, no?  They've never done anything like this before.  They've always had the same process tech for all CCD's of the same type, for all ranges of CPU's of that architecture.  Dense/C versions are a bit different, but those are also a strategically produced different line.  You wouldn't do that just for a different process technology alone.",Neutral
AMD,"I/O die only makes sense if they are getting wafers cheap as they don't benifit from the leading node density as much, but they may like it for low power.  Midrange GPU would make sense.  One of the mid/low end APUs would make sense too.  PS6 handheld or even the main console APU could potentially work as well, but that would require consistent parametric yields since they don't have opportunities for performance binning.",Neutral
AMD,Versus current gen 5/4nm?,Neutral
AMD,"Venice isn't their AI GPU lineup, and AMD already confirmed they will use TSMC N2 for their MI400 series IIRC.",Neutral
AMD,"This is about CPU's, where AMD is currently the top dog.  Much like how Nvidia used Samsung for Ampere GPU's while still being competitive, it's possible AMD could utilize Samsung for Zen 6 and still be very competitive.  Especially because AMD's whole chiplet scaling strategy is still a lot more cost effective than Intel's messy bullshit.  Epyc CPU's are also not typically being pushed to higher limits, so efficiency sweetspots matter a lot more, and that's not always gonna be such a huge difference in terms of performance per watt.  It might hurt them a bit more in consumer, but overall they're probably pretty confident in their architecture teams to maintain improvements without relying entirely on process node gains.  Intel's P-core team definitely has plenty of question marks surrounding it since Alder Lake.",Positive
AMD,"> Because AMD has significantly more understanding and insight regarding the processes involved, than random gamers?  Dude, don't you think such a assessment is a 'lil bit too much to drop casually?! o.*0*  *This is Reddit!* 90% of users here are armchair-generals or virtual CEOs, who actually know their sh!t …",Neutral
AMD,That and Nvidia and Apple bought up all of TSMCs 2nm production time.,Neutral
AMD,"This is just a meaningless appeal to authority with no logical backing. Moreover, any decision to make chips in Samsung, whether they turn out true or not, will also involve non-technical factors like cost. They could very well choose to go with a subpar process if it is way cheaper. Beancounters often hold significant decision-making power.     And historically Samsung has had a number quality issues versus the having the same architecture made at TSMC, so these concerns are pretty fair.",Negative
AMD,Perhaps this would make more sense if this wasn't:   * just a rumor  * no mention on whether this was for CCDs or IODs  * being fabbed at Samsung which has a horrendous track record   But sure.,Neutral
AMD,"that's not a reason to choose Samsung over TSMC, try again",Neutral
AMD,"FWIW that is not necessarily how prices are negotiated between designers and foundries.   The roadmap has more effect, and TSMC, Samsung, or Intel being in the picture is rarely used to drive prices lower.   Sure competition among fabs and packagers may set the ballpark of costs. But designers rarely use the threat of going with a different process to get a lower/better price from TSMC, Samsung, etc.   If AMD is going w Samsung it is likely because Samsung's roadmap may align w AMD requirements for a specific design.   The reason why this news may be unlikely is that AMD does not have an stablished silicon team with Samsung (that I am aware of). And that usually is how you can tell any type of significant volume from a large design team is going to be on a given foundry.   However Samsung does have a very nice roadmap for the type of large dies AMD has in their pipeline for their monolithic SoCs (mainly their APUs and GPUs). So there could be some alignment there.   For the CCDs and IO chiplets + 3D cache, it seems AMD is very aligned with TSMC and the packaging flow there.",Neutral
AMD,"I understand dual sourcing for lower end SKUs, but the article makes it sound like the entire new EPYC line is moving to SF2, a very likely worse node than N2. This would impact the performance and potentially allow Intel to close the gap further, so I’m struggling to understand why. If this article is true, there could be something off about N2.",Negative
AMD,"Most analysts have AMD as a top 5 TSMC customer, and Venice is a flagship product from AMD.   It's hard to believe Mediatek can tap TSMC N2P for their next smartphone chips, but AMD couldn't for their high margin server CPUs?",Neutral
AMD,"Every year there's new rumors about AMD using Samsung for their 4nm and 3nm node, and then people like you always say the same comments like this, and then it never ends up happening. But then a new Samsung node gets announced, and the cycle repeats.   *Sigh.*",Negative
AMD,"Yeah, the Chinese overtaking Taiwan we're told to happen the next Monday morning since the 1970s …  Even announcing another round of *»This is the year of the Linux-Desktop going mainstream«* has more credibility to it, and actually is becoming reality rather sooner than later, thanks to Valve's Steam now, also their SteamDeck.  ---- Intel just dug up that age-old Taiwain-spectre, to scare investors and governments into submission for subsidies, and y'all damn fools all bought readily into that nonsense and manifested a threat, China really couldn't care about less.  *China has a host of other problems of greater importance* — Taking out Taiwan, will inevitably result in vaporizing it for EVERYONE, which wouldn't help Peking/Beijing one bit anyway to begin with. It's a futile undertaking and they know it.",Neutral
AMD,I still remember being excited about getting a Denmark (Opteron 165 - basically dual core Venice) and OCing it like crazy.  Those were the days... when dual core felt like overkill and a 50% OC - clocking higher than the fastest available SKU - could be reached with the stock cooler on an entry level part.  today's CPUs a way better and way more boring. Almost zero point to OCing.,Positive
AMD,"> They cant just swap TSMC for Samsung chiplets and have everything all work out the same because they're designing these Epyc packages based on a very specific die size for the CCD's.  Hasn't they've done so before already? AFAIK a bunch of CCXs were dual-sourced (TSMC, Samsung) and AMD even opted for *a three-pronged strategy* on sourcing (TSMC, Samsung, GlobalFoundries) for a single design of their CCXs (Ryzen, Threadripper, Epyc), no?  > They've always had the same process tech for all CCD's of the same type, for all ranges of CPU's of that architecture.  You think that Samsung's and GlobalFoundries' 14nm back then were identical down to the last bits?  Yes, they were the same process, but GloFo surely made quite a bit of custom tweaks on their own, don't you think?",Neutral
AMD,"> as they don't benifit from the leading node density as much, but they may like it for low power.   AMD has been wanting to jump up to DDR5-9000 for a while if you've been reading the tea leaves/what comes out of their PHY guy via AGESA update, but the I/O die simply isn't up to the task as currently built. Jumping up to 2nm for a faster switching frequency and just adding a <profanity> load of transistors to make it less fragile solves that problem adequately. It's not elegant, but what's the point of more advanced materials (or transistors, as it were) if you never use them?",Neutral
AMD,"> Especially because AMD's whole chiplet scaling strategy is still a lot more cost effective than Intel's messy bullshit.  Still baffles me how Intel can't let go of their big-die philosophy, tanking their margins and destroying yields that way since years while even crippling their overall volume — Still times higher manufacturing-cost than AMD.",Negative
AMD,LOL. They did not. Both AMD and Intel are also doing bring up of high volume SKUs on N2.,Neutral
AMD,Nvidia has no products on 2nm. And AMD was first to tape out on 2nm with this CPU. Meaning they get all the early production to themselves. Think.,Neutral
AMD,"Interesting. Yet, I am quite certain the people at AMD and Samsung (or TSMC for that matter) still have significantly more understanding and insight regarding the processes involved than random terminally online gamers. Just a hunch.",Positive
AMD,">I understand dual sourcing for lower end SKUs, but the article makes it sound like the entire new EPYC line is moving   Which makes no sense because AMD already confirmed that Venice will be fabbed, at least for some part of it, on TSMC 2nm.   >SF2, a very likely worse node than N2  You are probably right, but the article also mentions SF2P, which very well could be Samsung's next real node jump after SF2 having very minimal PPA benefits over SF3 GAP.   So maybe they could catch up to TSMC 3nm rather than being decently worse than it, as they currently are.   >This would impact the performance and potentially allow Intel to close the gap further, so I’m struggling to understand why.  TBF I doubt Intel and Samsung are going to be in much different places if Venice is fabbed on SF2P and DMR on 18A, and AMD's design side also just gaps Intel's, so they could still win at what could be considered node parity.",Negative
AMD,It definitely feels like an ego thing that they didn't just copy Ryzen's super successful and ultimately quite straightforward chiplet scaling strategy.,Neutral
AMD,Who goes first? :),Neutral
AMD,Late 2026,Neutral
AMD,Your own recent posts are out of touch even if you claim authority because you're a director.  These Samsung foundry rumors are like Intel foundry rumors. I'll believe it when I see it.,Negative
AMD,">Interesting. Yet, I am quite certain the people at AMD and Samsung (or TSMC for that matter) still have significantly more understanding and insight regarding the processes involved than random terminally online gamers. Just a hunch.  Maybe. However, that may be why u/heylistenman is asking why AMD is rumored to be switching foundries, to get insights from people more involved in the industry than random gamers.",Neutral
AMD,We’re not allowed to discuss this rumor as mere enthousiasts frequenting a forum about hardware? Why do you think AMD would move EPYC to SF2 instead of N2?,Negative
AMD,"> It definitely feels like an ego thing that they didn't just copy Ryzen's super successful and ultimately quite straightforward chiplet scaling strategy.  Well, I think Intel at least *tried* to copy AMD's chiplet-paradigm helplessly for the last couple of years …  It seems that Intel was taken totally by surprise on anything Chiplets and got caught with their pants down (again), when AMD brought it this quick to market, when having worked on them for a decade plus.  Though IF Intel would already worked on anything *disintegrated silicon* for said 10 years by then (like Intel claimed when announcing their heterogenous '*Mix and Match*'-approach around 2018), *Intel would* ***not*** *have needed +6 years for finally reaching a fairly comparable design-approach* only half a decade later in 2023 with Meteor&nbsp;Lake (also Arrow Lake, Sapphire Rapids, Ponte Vecchio).  Since sure enough, all these disintegrated designs \*somehow\* brought Intel truly massive troubles engineering-wise and they had tremendous difficulties to overcome those for years — The years-long horror-stories over validation and dead-on silicon past tape-outs on *Sapphire Rapids* and *Ponte Vecchio* for example are testament to that … *Meteor Lake* was also everything but a performer.  ---- The issue at hand for Intel was quite many-layered …  * Intel hardly knew how to do it (despite claiming otherwise for years). *Shocker!*  * Intel had no real equivalent to AMD's SuperGlue aka *Infinity&nbsp;Fabric*™   That's for sure the most striking one, obviously — The reason they had to wait for PCi-E 5.0 to become a thing, for ""Intel's"" *CXL* to finalize upon it (which in itself is basically just AMD's former *CCIX* in disguise as Copy-pasta anyway done out of spite).  * Intel haven't remotely had adopted a design-strategy by then, which would've offered so to speak ""intelligent"" chiplets/tiles, which could be freely thrown together randomly at will on a (PCi-E) bus (which AMD actually evidently can with their CCX and I/O-dies since ages).  So Intel's claims before the press in 2018, of already working on disintegrated silicon since years already (and that AMD *weren't* actually as spearheading as they became), was pure virtue-signaling, a blatant lie.  As a result, Intel's IP-blocks still remained virtually *dumb* (in the sense of *head*|*less*) for years on out afterwards and most of it had to be *re-engineered from scratch* in all this other trouble like lay-offs, down-sizing or Intel's road-maps being constantly thrown out again (only to start over once more).  The reason why AMD had a years-long edge on chiplets from the onset, was since Intel just couldn't place or handle the stuff as ***independent*** *IP-blocks* — Everything was grown intercoupled and -linked together.  AMD on the other hand already had a decade-long headstart, due to their already well-tuned modular concept of IP-blocks (building-block principle with IP-blocks to freely chose from) attuned for the console-era from back then for the console-contracts (those, Intel always made fun of since).  So Intel allegedly been working on chiplet-esque designs for years already, was utter bullsh!t, and that's actually the sole reason WHY Intel struggled so hard for years with anything Tiles — They started at 0.",Neutral
AMD,"> It definitely feels like an ego thing …  Well, yeah. It definitely IS a ego-thing for Intel — They called them *Tiles* purely out of spite.  Since you can't just call basically the very same what your competitor has, the same name, can you?   That's not how Intel rolls nor would have ever done anyway, even if doing so, would've saves them a lot of engineering-pain in the arse, even more teething-problems and cost them years of falling back behind their only lone competitor …  Truth be told, Intel's *Tiles* are basically in essence just Copy-Pasta: *A botched Copy'nPaste-job from AMD's chiplets*, yet relabeled as 'Tiles', just for Intel trying to pretend to have their ""own"" chiplet-esque implementation, even if it's basically the very same …  Wanna hear a joke? When Intel back then around 2018 out of the blue announced their heterogenous *Mix and Match*-stuff (pretending, they'd already worked a decade on this by then), [the actual effing PowerPoint-slides actually didn't even incorporated anything called 'Tiles']( http://web.archive.org/web/20210923105815/https://newsroom.intel.com/wp-content/uploads/sites/11/2018/08/monolithic-vs-heterogeneous-infographic.pdf) — *Called them* ***chiplets****!*  Yet some big weak weasel's ego was hurt in Santa Clara, and they renamed it *Tiles* shortly afterwards.  > … just copy Ryzen's super successful and ultimately quite straightforward chiplet-scaling strategy.  They joke is, due to both of them share a cross-patent agreement, Intel even would've had access to AMD's patented stuff over everything chiplets — Chances are Santa Clara chose not to over license-fees, or spite …  *That's how you're f–cked over by your own ego* — Rather throw years of potential lead into the gutter, instead of even *thinking* about having to share some meager percentage of profits of yours with others.  That's Intel for you. A bunch of braggarts and loud-mouths, wo always think they can do and know better, yet fail nigh every single time and still can't bring themselves to be humble for once …",Negative
AMD,AMD is actually rumored to have the first TSMC N2 products out.,Neutral
AMD,"I believe  For N2; APPL A20/M6, NVDA Feynman, INTL Nova Lake   For N2P; APPL A21, MTEK 9600, QCOM SDE8G6, AMD Venice  AVGO, MRVL, AMZN, MSFT, GOOG have also volume contracts for n2/n2p production with several SKUs in bring up already.",Neutral
AMD,Late 2026 is Rubin and it's on 3nm. 2027 is Rubin Ultra also on 3nm.,Neutral
AMD,"You're absolutely allowed to discuss the rumor. I am also allowed to point these discussions are way beyond the pay grade of a lot of people in this sub.  FWIW Designs for 2nm generation are already in production, so that ship has sailed. If AMD was to be planning on doing EPYC on SS 2nm, they would be already be at the bring up stage or close to it. Which does not seem to be the case.  Edit: However Samsung's foundry roadmap aligns with some of AMD's monolithic die roadmap, so it would make sense for AMD to at the very least explore their foundry options there. But for 2nm nodes, those negotiations should already have happened a while back.",Neutral
AMD,"AMD only ever claimed N2, which is interesting because Mediatek had no problem specifying N2P, though IIRC their original press release *also* only had ""N2"".   Given that AMD seems like they will launch their N2 products the earliest, I think they might be on N2 rather than N2P, but who knows. The differences between the two nodes seem very minor anyway.",Positive
AMD,"Is it strange that i feel like 3nm is just old tech now ;)  Google AI says  ""Nvidia is heavily invested in 2nm chip technology, planning its next-generation ""Feynman"" AI architecture on this advanced process node via TSMC, targeting mass production around late 2026 or 2027, following its current ""Rubin"" (3nm) chips, with its cuLitho tech aiding the shift for improved efficiency and speed, aiming to maintain leadership in AI hardware despite rivals also targeting 2nm""  Google AI wouldn't lie would it? ;)  Watch Nvidia 2nm get delayed until 2029 because no ram available ;)",Neutral
AMD,">You're absolutely allowed to discuss the rumor. I am also allowed to point these discussions are way beyond the pay grade of a lot of people in this sub.  I'm just baffled why you brought it up if you seem like you *agree* with the premise of the original comment anyway, other to just dunk on gamers? Lol.   >FWIW Designs for 2nm generation are already in production,  Only 18A and Samsung 2nm, both of which are likely not on par with TSMC's 2nm node. Meaning calling them part of the ""2nm generation"" is a stretch.",Neutral
AMD,Google AI is telling me the correct info: https://i.imgur.com/HPg7fhN.png,Neutral
AMD,"> I'm just baffled why you brought it up if you seem like you agree with the premise of the original comment anyway, other to just dunk on gamers? Lol.  It's their pastime.",Negative
AMD,Another AI miracle.,Neutral
AMD,"AI has fuzzy memory like humans. The way attention works in AI is based on context. Certain keywords increase or decrease certain likelihoods.   Vague prompts increase the likelihood of a hallucinated response.. In my screenshot you can tell it also provided the picture of Nvidia's official roadmap. This is called grounding.   Can't trust AI 100% time just how you can't trust humans 100% of time. But there are ways to increase accuracy via grounding, or prompts. We're still figuring out how to extract the best out of these models. They aren't perfect.",Neutral
AMD,"The rumor that Samsung will manufacture the PS6 APU doesn't hold up when you look at standard semiconductor timelines. If we assume a late 2027 launch, mass production needs to start by early 2027. In the semiconductor world, you don't select a foundry 12 months before production; that decision is locked in 2–3 years in advance during the design phase because the physical design is tied to a specific process node. At this stage, AMD should already be in the 'bring-up' phase with early working silicon in the labs. Switching to Samsung now would require a massive redesign that fits neither the timeline nor the logic of a major console launch.",Neutral
AMD,"*— Article itself is in Korean —*  The article states that according to South-Korean media reports, AMD is in talks with Samsung's Foundry-division about chip-manufacturing …  While AMD will expressively **not** leave TSMC behind, especially not in the imminent advent of their N2-ramping (the majority of AMD's setup is scheduled to move to N2 later on), AMD may adopt a two-pronged strategy for possibly having Samsung's foundry manufacture the chips for the upcoming PS6.  So AMD could eventually use Samsung's SF2-process for PlayStation&nbsp;6-chips, for the console which is scheduled for end of 2027, to split and shoulder the load of manufacturing, for ease of availability.",Neutral
AMD,"Samsung: ""Get an ambulance... but not for me!""",Negative
AMD,"RAM might be three times as expensive and rising, but at least we get to reap the benefits; for instance we all get to marvel at whatever the fuck this AI generated thumbnail is trying to convey",Negative
AMD,"Has Samsung ever made x86 chips? Is there any reason they couldn't, in partnership with AMD? Or is this also suggesting that Sony could be moving to ARM?",Neutral
AMD,From the article it seems they’re not actively replacing TSMC.   AMD is choosing to probably dual-source chip manufacturing to both Samsung and TSMC and treat Samsung as a secondary supplier.   Most likely it is because TSMC allocation is too full for them to make the amount of chips they need.,Neutral
AMD,You're assuming they will use Samsung at launch.,Neutral
AMD,"You don't have to lock in your foundry choice 3 years in advance, but 2 years is pretty close.  Three years in advance, you are kicking the tires on the PDK and making sure the node will achieve your goals for your design.  That is not so expensive you cannot do it for multiple foundries.  The next step, is finalizing the layout for masks, which culminates in the tape-out.  You could do that for multiple processes, but it does not make much sense, as you should have known which process you prefer before that step.  Masks are very expensive so you don't want to make them for two different processes.  So tape out is the last chance you have to make a foundry decision, but you should make it months before that.  As an example, AMD announced back in April that Venice chiplets taped out on TSMC 2nm.  Venice is going to be included in Helios racks that are being delivered in Q3 next year.  So maybe a little less than 1.5 years from tapeout to product.  Add in the time for doing the physical layout, and you get pretty close to 2 years.  So I'd expect that right about now is when the final decision on where to manufacture PS6 is being made.",Neutral
AMD,"> In the semiconductor world, you don't select a foundry 12 months before production; that decision is locked in 2–3 years in advance during the design phase because the physical design is tied to a specific process node.  I understand where you're coming from and the reasoning of year-long lead times for a given process is pretty much self-explanatory, but it's not *that* much like 2–3 years. It's 1–2 years in advance, if it takes that long.  Also, it's not that AMD (just like others like nVidia, Broadcom, Qualcomm et al) aren't used to foundry-hopping since decades due to the short cadences of GPU- and CPU-release cycles — *They all have extremely well-attuned process-tailored teams for such foundry-stuff, speeding up things massively*.  Taking GPUs as an example, these are AMD- and Nvidia-teams of hundreds to thousands of process-engineers, VLSI-magicians and all kind of specialists for EDA-tooling and whatnot else is needed, working closely on-site directly at foundries like TSMC, Samsung or GlobalFoundries and interoperate with the foundry's respective specialists for stuff like that …  So given the current time-line at end of 2025, it would be *virtually two full years* up to it, until a launch scheduled for end of 2027, which ought to be pretty sufficient and timely fitting — Pretty realistic if you ask me.",Neutral
AMD,"> At this stage, AMD should already be in the 'bring-up' phase with early working silicon in the labs. Switching to Samsung now would require a massive redesign that fits neither the timeline nor the logic of a major console launch.  Nah.. Remember the launch of Zen&nbsp;5 (Ryzen 9000), which AMD suddenly postponed, due to unspecified 'quality-control' issues at early production-runs? *They recalled and canned ALL already shipped batches prior to launch*.  Who knows what it really was, but it's possible, that it was comparable to Intel's oxidation-issues or comparable process- or design-flaws — It still didn't took them years to correct that, but just a couple of *months* …  I'd say, over the years no other companies became as time-efficient and extremely attuned to toss a design, fix it and redesign it ASAP, only to bulldoze through the whole initial pre-manufacturing process once again in a speed-run-like fashion all over again, as much as as AMD and nVidia are by now — I'd bet, that a complete design-change might take them barely more than 3–6 months total.",Neutral
AMD,"You're joking, but considering how much contracts and design-wins Samsung's foundry-division got over the months from others, I'd consider them the *»Foundry-winner of the Year«* for 2025.",Positive
AMD,I don't see why they could not make X86 chips. But this is suggesting that the PS6 chip could be manufactured both at TSMC and at Samsung to ensure that they don't have availability problems.,Neutral
AMD,The process is architecture agnostic.   They're probably doing the switch to free up volume for the best TSMC processes for AI and CPUs.   Samsung might also be offering a price competitive option.,Neutral
AMD,"Samsung can fabricate whatever their customers bring them. So yes, if AMD pays them to fabricate x86 chips, there is no reason they can't do it.",Neutral
AMD,Global Foundries 14nm node was licensed from Samsung. Their 12nm and 12nm+ was just refinements of that node. So their nodes have been used for x86 at least.,Neutral
AMD,"> Has Samsung ever made x86 chips?  Yes, they manufactured a bunch of Intel-designs over the years.  In any case, *processes don't care about architectures*. These are architecture-agnostic, as others pointed out.  > Is there any reason they couldn't, in partnership with AMD?  Nope. A foundry manufactures what the client asks them for, or don't, depending on contractual incentives.",Neutral
AMD,More like everyone wants tsmc chips so much they became too expensive and Samsung made a half decent chip at a lower price,Negative
AMD,"And even relatively inexperienced teams can pull off faster timelines than 2 years. I was following the crypto space early on when ASICs started being taped out.   And you had relatively small teams that manged to design, raise funding and get shit out the door within not much more than a year. Granted a Bitcoin miner is some of the easiest chips out there to design, so that part was not very time consuming. But you had some companies like KNC that launched on at the time leading edge nodes like 20nm planar back in mid 2014, so not like they launched on some ancient/quick node. And they were formed and started taking pre-orders in early 2013.",Neutral
AMD,Are you really going to write every comment using AI?,Neutral
AMD,"> But this is suggesting that the PS6 chip could be manufactured both at TSMC and at Samsung to ensure that they don't have availability problems.  I'd just see it as a repeat/extension of the first Ryzen-launch — AMD not just adopted a two-pronged approach back then with the original-Ryzen in 2017 (TSMC, Samsung), they even opted for *a three-pronged strategy* (TSMC, Samsung, GlobalFoundries) for a single design of their CCXs (Ryzen, Threadripper, Epyc) …  Still wasn't enough to satisfy demand anyway — Limited availability, as markets just gobbled up ALL of it in months.",Neutral
AMD,"That's not how it works.  You don't just bring a design to a silicon foundry the way you bring a blueprint to a factory.  You have to create the design using that foundry's software tools.  If AMD uses Samsung as a second source, that means they re-created the low-level design for Samsung from scratch.  None of the people who created the TSMC design would be legally allowed to take part.  That leaves Samsung-specific employees and the broader design team that creates the higher-level design.  It is true, of course, that there's nothing special about an x86 chip versus some other chip.  The point is, Samsung just runs the process.  AMD has to design the chip on that process, with assistance from Samsung being limited to issues well outside the scope of the high-level chip design.",Neutral
AMD,Pre 10nm Intel was generation ahead of Industry by a full node shrink they introduced FinFet before everyone was on HKMG,Neutral
AMD,"I'm reasonably sure Samsung has never manufactured any x86 chips.  Intel did use them for chipsets in the past, but not processors.    Actual Intel x86 processors have always been made by Intel in-house, with the exception of their recent use of TSMC.    AMD x86 processors have been made in-house, by Global Foundries (which was spun off from AMD), and by TSMC.",Neutral
AMD,Taiwan says China will invade in 2027.  That's why 2026 will see a massive shakeup in TSMC orders.,Neutral
AMD,"Yup, I think 2 full years is actually a ample amount of time for a experienced and well-attuned team, and being tightly integrated at given foundries on top of that, tremendously helps to cut months of time.  AMD, Nvidia and others are used to short, highly stressful stints like that, since it's virtually their daily work.",Positive
AMD,"is it really AI? using "".."" and space after ""—"" makes me think it wasn't AI generated.",Negative
AMD,"Good Lord, what have you people with your accusations of using AI for comments every time?!  You really become a pain in the butt with this sh!t over AI-stuff — *Just because I use hyphens?!*  You can browse my comment-history, and you'd see, that I'd quite likely already used hyphens on my first comments ever made here on Reddit about 8 years ago (a time, when AI wasn't even existing). It's a style of writing for me ever since I use since like two or three decades, as I got used to it, as many are from the scientific/technical-driven realms.  ---- And talking about y'all imbecile reasoning over em-dashes (or en-dashes for that matter); Did it ever occur to you, that using em-/en-dashes is actually quite common, actually often mandatory in the scientific field of applied since?!  Ever opened a effing book? Or read a news-paper the last decade? Em-dashes (—) are *usually used to bind two sentences of the same scope*, which belonging together analogously. Meanwhile a en-dash (–) is used for date-formats in everyday life and money-values et al and so forth.  In any case, in print media (newspaper, books, scientific works), dashes are heavily used, in the field of typography even as a stylistic element … and if you would've cared to look at the way I wrote my post;  I usually try to write while trying to maintain a pleasing appearance for *ease of reasoning*, and dashes tremendously help with that, that's also why I use horizontal rulers within posts (differentiate separate issues).  You can see the source of my posts using RES (Reddit-enhancement suite), and discover, that I often also use proper non-braking white-spaces (`&nbsp;`) for when it's needed, given proper HTML-entities like &times; (`&nbsp;`) for formulas or given specific brand-nams and whatnot (Intel**_**Core, Intel**_**ARC; AMD**_**Ryzen, AMD**_**Radeon; nVidia**_**GTX/RTX).  In short; *Shut up and learn about [Quad](https://en.wikipedia.org/wiki/Quad_\(typography\)) and that sort of stuff!* -.-",Negative
AMD,> None of the people who created the TSMC design would be legally allowed to take part  Really? I would be shocked to learn that people who worked on the physical design of a chip fabbed at TSMC couldn't also work on a design for a chip fabbed at Samsung.,Neutral
AMD,"> I'm reasonably sure Samsung has never manufactured any x86 chips. Intel did use them for chipsets in the past, but not processors.  That's not true. Just because Intel never \*admitted\* it publicly (so save face and protect their golden cow, their stock), doesn't mean, Intel never outsourced before — They did in fact, and they did in fact *en masse* for years …  Since back then during the self-inflicted 14nm-shortages (Meltdown, Spectre, Foreshadow; also 10nm-issues), Intel's CEO Bob Swan reshuffled much capacity and brought over almost all chipsets to Samsung being fabbed there.  Their low-end CPUs like i3s and Pentium were later manufactured at Samsung as well.  TechPowerUp.com – [Intel Turns to Samsung in Order to Resolve CPU Shortage on the 14 nm Process](https://www.techpowerup.com/256613/intel-turns-to-samsung-in-order-to-resolve-cpu-shortage-on-the-14-nm-process)  (2019)   TechPowerUp.com – [Samsung Scores PC CPU Manufacturing Order from Intel](https://www.techpowerup.com/261641/samsung-scores-pc-cpu-manufacturing-order-from-intel) (2019)  > Actual Intel x86 processors have always been made by Intel in-house, with the exception of their recent use of TSMC.  Just goes to shows that you ain't aware of the actually situation and not really informed. No offense though!  No, with all due respect, but that's pure nonsense — All throughout the time of Intel pumping the mobile market with Atoms (2007-2013, fighting the myriad of ARM-licensees like Samsung, Qualcomm, MediaTek), these very Atoms (especially each and all for the mobile market), have been made *virtually exclusively by TSMC* for several years in a row (and NOT Intel itself) since at least 2009.  True be told, I'm not even aware if Intel ever manufactured their own Atoms ever again past first Gen Atoms (Silverthorne) in 2008, when they basically showed over their whole Atom-business to TSMC for several years.  ArsTechnica.com – *Atom can’t feed fab monster;* [Intel outsources chips to TSMC](https://arstechnica.com/gadgets/2009/03/atom-cant-feed-rd-monster-intel-outsources-chips-to-tsmc/) (2009)  Remember, up until recently with TSMC after them announcing ""outsourcing-possibilities"" (to keep pace) in 2021, Intel **never** had ever admitted publicly upon any outsourcing, except back then as mentioned above.  This was done fully *strategically*, as it would've signalled an actual *admission of struggling and falling behind* (and necessarily having to contract outsiders), which Intel has been desperately trying to avoid for years on end since 2012 or so and through-out the whole fiasco of their 10nm™ …  Ever before, these were all 'rumors' Intel either vehemently disputed publicly (despite doing *the exact contrary*), or just let pass without comment, despite everyone involved knew from day one, that such rumors were 100% true.",Neutral
AMD,Is there a source for this claim?,Neutral
AMD,"It's laughable, as em-dashes (or en-dashes for that matter), have been used especially in typography for at least *the whole of the 20th century*. Actually, around 1850–1990 was the height of such usage in anything print.  Look at any newspaper-archive; Virtually every other older printed text (and even handwritten ones) the last hundred years, uses dashes for *separating the principal clause from the subordinate clause* (En-dash; –, HTML:`&ndash;`) like a counter-argument the writer dropped mid-sentence (for not trying to disrupt the readers' current train of thought), while embedded sentences either \*within\* a principal clause or between principal and the subordinate clause, is ought to be signified with a Em-dash before at the beginning and the end afterwards (Em-dash; —, HTML:`&mdash;`).  **Edit:** Specifically full dashes are also used in writing (directly at the point of breakage), to signify to the reader a sentence or written (direct) speech, which ended abruptly mid-sentence. The same goes for suspension marks (ellipses; …; `&hellip;`), if the recipient is supposed to fill in the blanks by himself.  I even often type them directly using the num-block (Alt+0150 for – and Alt+—). … but I'm using AI! -.-  I just despise wrong spelling or the complete absence of punctuation marks, that's all.",Neutral
AMD,"The use of ""—"" strongly hints at AI, since virtually nobody types that character.  99%+ of people type "" - "" instead, because ""—"" is a special character that can't be simply typed on any normal keyboard.",Neutral
AMD,"Bro, you wrote a novel in this post with pixel perfect formatting and bunch of formatting flourishes. Nobody does that. You are using AI. It's clear as daylight.",Negative
AMD,"There are a lot.  https://news.sky.com/story/taiwan-to-prepare-for-combat-by-2027-president-says-as-he-warns-china-is-preparing-to-take-the-country-by-force-13475504  Also, I'm seeing that China may surround Taiwan and quarantine it.  No matter how you look at it, this will damage TSMC.  TSMCs chip future is likely to come to an end around this time.  Samsung and Intel need to start ramping.  They have about a year.",Neutral
AMD,"Option+Shift "" - "" is pretty easy to type on a mac—although people usually don't bother.",Neutral
AMD,"For many linux users AltGr + Shift + - results in ""—"".",Neutral
AMD,Long press the - on an iPhone and it pops up as an option.,Neutral
AMD,"> The use of ""—"" strongly hints at AI, since virtually nobody types that character.  No, it does actually **not**. If anything, it just shows that LL-models (for training AI in the first place) were crawled (illegally) upon huge libraries of digitized text-corpuses to begin with …  I mean … *Where do y'all fools think, these AI-bots got those dashes from in the first place?!* Exactly.  In any case, if you happened to put open any decade-old newspaper, you'd know, since newspaper and articles (at least the ones being written by actual humans) to this day are full of hyphen and dashes.  Anyway, at least I do since decades (and many I know too), and so do many in the scientific field in general.",Negative
AMD,">Nobody does that.  At least one person does that, and did that years ago already. Seriously, look at the comment history",Neutral
AMD,"Well, no kidding. The U.S. constantly try to push it, no? I mean, aircraft-carriers in the Taiwan straight?",Neutral
AMD,"Or *Option*(+Shift)+*Minus* for different dashes (`⌥`+`-`), while *Option*+*Period* for ellipses is another (`⌥`+`.`), and (`⌥`+`+`) for the plus-minus-sign (±) — I picked those up on a Macintosh IIfx back then in the Eighties at the university and have been using those ever since …  Though it's not just Mac&nbsp;OS&nbsp;X which has been incorporating special typographic characters into keyboard-layouts since decades even under Mac&nbsp;OS&nbsp;9 and below (e.g. proper typographic quotation-marks [“…”] or Guillemets [»…«]), most Linux-distributions' default keymap are also filled to capacity with loads of special characters, of which most you can't even reach under Windows without Alt-Codes via Unicode through the num-pad — Just see Ubuntu's default keyboard-layout …  That's why the likelihood of layouters, writers, editors and content-creators (in the *classical* sense at least, journalistically speaking), often use Mac OS out of principle — The chaos and annoyance to even reach special typographic characters under Windows, is a complete mess and no-go for many.",Neutral
AMD,"Because sure, why not.",Neutral
AMD,Yeah because one cannot pay more for ram than cpu.,Neutral
AMD,"As someone who doesn't have a single piece of Intel silicon in my build, I've never understood people cheering on their downfall. We need competition, people, or shit like this happens.",Negative
AMD,Bought 64GB memory and 4TB storage in June. Have bought a Ryzen 7 9700X and RX 9070 this Friday.  I feel quite lucky.,Positive
AMD,"I don't think this has anything to do with the memory shortage anymore, it's just pure greed.",Negative
AMD,"Of course they do. Black Friday prices aren't supposed to be the new normal prices. I'm sure Intel and Nvidia are doing it too, this is not newsworthy.  In the Videocardz article about it they even admit this is not raising the normal prices, it's just prices returning to normal after Black Friday:  https://videocardz.com/newz/amd-rumored-to-raise-ryzen-9000-and-older-cpu-prices-tonight",Negative
AMD,Won't someone think of the (checks notes) 355 BILLION dollar company!?!?!?!,Neutral
AMD,At this point I can see me running my 4670k GTX 970 build from 11 years ago until 2040 and beyond. Got my duct tape ready.,Neutral
AMD,A decent PC about to cost the same as a new car,Negative
AMD,"“Hey people are still buying ram at these prices, let’s raise our prices too!”",Negative
AMD,"They did not get the memo, that people aren’t building PC due to memory prices?  Good luck AMD, I managed to get 9800x3d for 399€ brand new and I was still on the fence about it. Like really on the fence, it was a stretch. Cause they blow up!  Not to forget I am an enthusiast. I upgrade GPU every gen and always get the latest platform.  If this was a strech for me, then 90% won’t even look at those if you increase the price, especially now.",Neutral
AMD,they’re ryzen the prices  i’ll go now,Neutral
AMD,"AMD: ""What are you going to do? Buy Intel?""",Neutral
AMD,"Nice, now it's gonna be rising prices for RAM, GPU, and CPU!",Positive
AMD,"RAM makers are not going to like this, they rather prefer CPU prices being low because now when they ship DDR5 memory to margin highs  they want consumers to not second ask themselves regarding PC upgrades like they already do.",Negative
AMD,people who built a pc at summer/spring must be happy af,Positive
AMD,"This is what happens when there is no serious competition around the horizon, it's not the first time that AMD has done this btw.",Neutral
AMD,I think AMD should reduce prices instead of increasing.,Neutral
AMD,"Intel is back, AMD should lower their prices. It would be better for their future.",Positive
AMD,"Just bought mine, sorry everybody. Hopefully RAM and motherboard prices will crash for everyone else to make up for it, now that I ovepayed.",Negative
AMD,man i was JUST thinking of upgrading from my 3900x....,Negative
AMD,I guarantee you they wont raise prices of their datacenter CPUs because you bet your ass their customers would switch to arm,Negative
AMD,"Proof that not having competition is bad.  Fast RISC-V chips soon, hopefully.",Negative
AMD,"If AMD holds the commanding lead in the retail market share with their Ryzen CPUs, there board and investors will probably be demanding more profit be taken.",Neutral
AMD,"Damn, didn't know CPU chips had tiny ram slots in them",Negative
AMD,Good thing I got my 9800X3D before this happened.,Neutral
AMD,And they will not drop them until Intel gets their shit together,Negative
AMD,"Hehe, glad I got my Ryzen 9 a few days ago.",Positive
AMD,I'm so glad I didn't wait longer to build my PC,Positive
AMD,"when intel was the top dog, everyone excused it by saying they had the best performance and stability, amd bulldozer was bad blah blah blah don't buy AMD  now that AMD is the industry leader all of a sudden competition is important  so obvious.",Negative
AMD,I mean sure why not . Their gpus finally hit MSRP 8 months after launch so something gotta give,Neutral
AMD,Glad I bought a 9800x3d last week I guess,Positive
AMD,"Instead of reporting on your lack of information, be a farking journalist and actually research the answers before writing about it. Even AI can match this level of journalism.",Negative
AMD,"Depends how much but I like to think this is my final amd product at this point. They have not been making the best decisions the past few years...   Like if intel is cheaper and more powerful at this point, might as well get that then.",Neutral
AMD,Because AMD is a damn corporation aiming to keep its margins while learning from the giants about consumer exploitations.,Negative
AMD,"The setup looks clean, but the price news definitely stings.",Positive
AMD,"No, they're not:  https://www.tomshardware.com/pc-components/cpus/amd-isnt-increasing-prices-on-cpus-at-least-for-now-ryzen-appears-to-be-safe-from-the-ai-hysteria",Neutral
AMD,I think it’s way more likely rx 9000 has 0% than it does like 0.1% I mean it just makes sense 0 people bought it,Neutral
AMD,Cache memory is going up in costs so AMD has to make up for it. /s,Neutral
AMD,"You've been clickbaited. Yes, prices go up again after a sale. This is a non-story.",Negative
AMD,"This is not AMD raising prices. This is prices going back to normal after Black Friday is over.   The Videocardz article on this ""news"" says this is a return to normal, not the normal prices being raised:  https://videocardz.com/newz/amd-rumored-to-raise-ryzen-9000-and-older-cpu-prices-tonight",Neutral
AMD,"""Eeh, might as well"" -Lisa",Neutral
AMD,Very good example being nvidia vs amd. Nvidia can keep their prices outrageous just because there’s no meaningful competition.,Neutral
AMD,Intel pricing is pretty competitive these days. As soon as gaming isn't your top priority they aren't a bad choice at all.,Negative
AMD,What model wifi card or Ethernet controller do you use? I've had really good luck with the Intel ones.,Positive
AMD,"But you see, good guy AMD would never do such a thing!",Neutral
AMD,People cheer on the downfall of Intel for the same reason they'll cheer on the downfall of a rival sports team.,Neutral
AMD,"last time after amd 64 intel got back swinging, This time it looks really sad",Negative
AMD,"The people that cheered for Intel's downfall were probably computer users from the time when it was Intel or nothing, that wasn't a great time because competition was low.",Negative
AMD,intel was a bad actor and actively tried to sabotage AMD and shun them from system integrators. sorry can't sympathize with intel.,Negative
AMD,"I bought about 80 in the last ten years, I'm doing my part.",Neutral
AMD,"I hate to break it to you, but the existence of Intel does not prevent sales being ended and prices returning to what they were previously.",Negative
AMD,"> I've never understood people cheering on their downfall  They were almost a monopoly for a long time, excepting the AMD K8 era.  Intel's business practices and strategic decisions have been extreme short-term profit motivated since.. even before 2010? When they were dominating, they would just hold back from bringing tech to market to maximize profits. Deliberate decisions to rest on their laurels, to not invest into real R&D. It's been a slow motion train wreck, competitors with much more growth R&D momentum catapulting past them, leaving them in the dust.  It's kind of like a desire for justice? Cheering on an org reaping what they sow? Of course, in America companies like Intel, or certain industries, like in 2008, get bailed out anyway. No justice; moral hazards aplenty.",Negative
AMD,"Well just read a article about intel making their own version of x3d cache for their next cpu linup, which will have lots of cache memory. If they actually perform for once and not take 300 watts to do it, could be enough competition to push AMD to lower prices again.  A big will see though.",Neutral
AMD,"Jealous. I bought 5x20TBs two months ago from Amazon, and they finally acknowledge it was lost somewhere and gave me a refund. I can only buy 3x20TBs now.",Negative
AMD,I bought 64gb of ram 2 years ago for $130. 6000mhz,Neutral
AMD,"I bought my 7600X for $150 and DDR5 32GB kit for $80, and additional 1TB SSD for just $40. Now I see both the SSD and the RAM being 2 - 4x the price of what I paid for nowadays when I am browsing our local online marketplace. I can say that I feel the same.",Neutral
AMD,Damn.. I bought 64GB memory last week and a 9800X3D. Paid $380 for the memory.. at least I got the CPU before it increased.,Negative
AMD,"Yeah, I jumped a little earlier and I'm still sitting on AM4, but when it looked like tariffs were gonna blow up the PC component market I made sure that I had a 5950X, 64GB of Samsung B-die, an 8TB flash drive, and a 9070XT.  Planning to coast on this for the next few years and hope there's still a hobby on the other side.",Neutral
AMD,"I bought a 9950x3d and 192gb like 2 months ago, i am happy i did so ahahah",Positive
AMD,The memory shortage is just one symptom of AI chewing through supply and increasing prices.   The DRAM still needs a CPU…,Negative
AMD,"""Black Friday sales have ended"" is not in fact a sign that we have been overtaken by greed.",Negative
AMD,"its... capitalism, from a publicly traded company. When was it ever not about greed??",Negative
AMD,"Nope, there isn't. It's simply a power move by AMD Ryzen because they know exactly the consumers will still choose them anyway over Intel that is already dragged on the mud by the reviewers and tech enthusiast community in general.",Neutral
AMD,"Please boost this, the entire thread is being ragebaited by an absolutely garbage article writing about a complete nothing burger. In the article it reads  ""The timing follows Black Friday and Cyber Monday discounts.....return to standard pricing.""  The article puts in complete rumor gibberish, and baits with RAM drama to confuse the reader because its literally just saying ""AMD cpu's went on a discount for Black Friday, they are now losing the discount."" Except the college student who wrote this had to pad the word count to 1000 on a 50 word article.   Completely nothing burger written for interaction bait.",Negative
AMD,"I mean, their market cap is only a little higher than the GDP of Portugal.  They're only slightly too big to fail.  I'm pretty sure the other tech giants throw fries at them at the lunch table.  It's actually kind of sad, really.  NVIDIA could find the cash to acquire them between its couch cushions at this point.",Negative
AMD,Nah don’t worry new car prices are through the roof as well,Negative
AMD,"Yep, Ive been planning on building my dream PC since I can finally afford to.   Not going to happen if the whole industry decides to fuck us. I'll watch and laugh at them when AI pops and the market is flooded with their existing and planned hardware.",Negative
AMD,"It will be sooner than later I can feel. These companies will go extinct, they are too greedy to exist.",Negative
AMD,"Intel isn't back right now... Arrow Lake isn't getting much traction,  the refresh isn't getting much traction (though it's honestly pretty good), so desktop DIY seems to be AMD's market.  They hence have pricing power.  For mobile AMD isn't really a big player but you don't buy DIY laptop chips so it isn't relevant.  Lunar Lake is pretty good (it's efficient, though ARM solutions still beat it in performance per watt) and Panther Lake is sounding better (ARM still wins, but it's a good gap over AMD and most people ex-Apple want x86).  That's a different market than desktop though.  When Nova Lake comes out it'll be a much better chipset than ARL, we'll see if they have giant cache chips or not which is what a lot of DIY people want.  If that happens then maybe Intel can compete better and exert more competitive pressure on AMD.",Neutral
AMD,Intel is back in what? Panther Lake is only M2 level,Neutral
AMD,"They're not raising them. In fact, the 9800X3D is cheaper now than ever.",Neutral
AMD,This article is complete clickbait about Black Friday sales ending. It applies to all of their competition as well.,Negative
AMD,There are people that prefer their cabling doesn't spontaneously combust.,Negative
AMD,not to mention 3D in the title of the CPU...,Neutral
AMD,considering how many reviewers pick a sales price and base their performance/dollar graphs based on that you may as well see this as raise compared to the misinformation you are fed.,Negative
AMD,I guess we will know in less than 24 hours,Neutral
AMD,"prices going up above msrp 18 months after release isn't ""normal""",Neutral
AMD,"This is just factually wrong. Look at GPU price history from MSRP to market price for the last year. GPU launches were incredibly inflated until these last 2 months, so what you think is ""black friday deals"" and ""cheap"" is what was supposed to be its original MSRP lol. Now this price hike is just to artificially raise the price to maintain the same profit margins they've had from the past year out of pure greed. No reason for AMD to raise prices on existing products on shelfs except for pure greed. No reason for RAM manufacturers to not offer long term contracts to large customers and OEMs when they are purposely causing a shortage, except for pure greed. No reason for Nvidia (who does not produce a large quantity of their FE GPUs themselves and relies on AIBs) to not bundle Vram with their dyes to the AIBs that have a smaller profit margin than Nvidia, except for pure greed.  20% of this is speculation of a AI bubble pop, the other 80% is companies seeing blood in the water and hopping on the price gouging train to artificially raise the prices for the whole industry. When all the businesses collude and work together to raise the price of computing power, which is now an essential commodity now that it powers the world, who stops them? I can't see it being government as they are apart of the majority of the demand for these AI datacenters.  Source you can check out: GPU Prices Crater Before Inevitable Opportunity to Screw Consumers - Gamer Nexus",Negative
AMD,"Counterpoint: Competition only works if there’s checks and balances to prevent price collusion. The stupid SSD mafia colluding and keeping prices high (DAE remember the great fire sale of SSDs in late 2023?). There’s no reason why a 2TB 990 Pro should be that close in price to a 9100 Pro. Were they losing money then? I am hard pressed to believe they were.  The skeptic in me however is willing to bet the RAM prices are never going to go down, and this will become the new normal, and they’ll just pocket the difference (unless there’s something major that happens like upstart Chinese suppliers flooding the DRAM market forcing them to).",Negative
AMD,"and the solution to that isnt shitting on Nvidia, its for AMD to make better cards.",Neutral
AMD,"I mean, there is no competition for the high end. When it comes to medium-high performance, the competition is absolutely there. It's really just the 5090.",Neutral
AMD,"this is partially incorrect.  as there were lawsuits and settlements about amd/ati and nvidia price fixing.  if price fixing is happening, there is no competition.  it is a fake competition, just like the memory industry, where a memory cartel sets their prices through price fixing and unified supply control (let's all massively reduce production and increase prices for example)  BUT it can look to the average consumer to still be ""competition"" then.  is amd and nvidia rightnow price fixing?  well there sure as shit won't be an investigation into it rightnow. hell nvidia can triple down on fire hazards without a recall. and the pricing between nvidia and amd are surprisingly almost always very aligned.  what a coincidence.  amd is also not interested to sell anything aggressively, despite wrongfully claiming they would.  so there is no meaningful competition going on at all here anymore.  and it is reasonable to expect, that price fixing is going on  as well of course.",Negative
AMD,"Yeah, for productivity Arrow Lake currently offers better performance than similarly priced AMD competition. They also don't seem to be cooking themselves (so far) and don't suck back stupid amounts of power the way Raptor Lake did.   Intel's lack of platform longevity is still a pain point, but if that doesn't matter to you and you just care about getting the best bang for your buck right now I wouldn't fault anyone for going Intel.",Positive
AMD,"Even with gaming, I just picked up a 225f for $155 on Amazon.  While not exactly on par with a 9600, it is close enough to save $40 on the CPU and saved $50 on the same brand's Intel vs AM5 ITX board.",Neutral
AMD,just purchased an i5 14600KF after selling my Ryzen 5 5600 just because I had DDR4 ram to utilize it with.  I believe there is a lot of misinformation online on how the 14th gen is still messed up to this day but I'm having zero problems (updated BIOS to be sure). Ran BF6 earlier (CPU demanding game) and CPU was running 70% - 80% at 58 to 60 degrees at stock lol,Neutral
AMD,"I've been out of the loop with pc components for the last 3/4 odd years. My build is primarily music production-focused but I do play games on it quite a bit (primarily PvE, I don't really need anything beyond 4K @60fps): AMD Ryzen 3600, Nvidia GTX 1660Ti, 32 GBs of DDR4 RAM, Asus TUF B450M pro-II. What should I be looking at if I wanted to upgrade without dumping half of my wage for DDR5 sticks?",Neutral
AMD,"For homelabbing, Intel seems almost purpose-built for this. Lots of cores for cheap (never thought I’d say this about Intel), decent enough single-thread, and that excellent Quicksync.",Positive
AMD,Isn't Intel in the middle of divesting themselves from their networking business?,Neutral
AMD,"I dunno, I just use the sharkfin antenna that came with my motherboard. Works fine.",Positive
AMD,"The article is clickbait. Yes, black friday sales are over, so prices will rise again. This also applies to their competition.",Negative
AMD,"Amd was better than intel around 2000s, then intel bounced back, now amd is top again, intels gonna bounce back again",Positive
AMD,"You forgot the (brief) period where AMD K7 was dominant.  Also, INTC did not purposely hold back development for years. It literally fell apart when 10nm was delayed. The chip and fab business was so tightly bound that any delays in the fab (10nm) caused the chip design business to stall and make silly workarounds (e.g., Coffee Lake, Rocket Lake, Ice Lake, etc).  You can point to the massive $100B in stock buy backs, but INTC during that same time also spent more on R&D than AMD and TSMC combined.",Neutral
AMD,> not invest into real R&D  [$8-14B/yr](https://www.macrotrends.net/stocks/charts/INTC/intel/research-development-expenses) on what?,Neutral
AMD,"In a similar situation just on a smaller scale. Bought a 20TB disk for 300€, the store sent it in just a cardboard box and it was obviously DOA.  The return department dragged their feet with the replacement for 2 fucking months until one day they just randomly closed the case with a refund. The price of the same drive is now 450€.  The store is Senetic btw, any European shoppers avoid it. Shit packaging, completely unresponsive for any support apart from the initial (legally mandated) return ticket and fucked me over in the end when it was in their financial interest to do so.",Negative
AMD,"Unless you really need them new, I'd check ebay refurbs for 20tbs. I got my 14tbs for like 180 a couple months ago. you might get lucky, however they were WD white enterprises.   i can send a link if you'd like",Neutral
AMD,"I bought 48gb 6000 cl30, Trident Neo Royal Z (the silver with sprinkles) and felt outrageously lavish for paying 320€ this September.    Now they're more than 500€. Jesus Christ. At least the 9800X3D just hit record low with 440€ where I live.",Neutral
AMD,"I'm glad I grabbed a 2x32GB DDR4-3200CL20 kit for my laptop earlier this year, paid $90 brand new",Positive
AMD,for what do you need 64gb?,Neutral
AMD,Did you read the article or just comment capitalism bad immediately when you saw the headline,Negative
AMD,They can control the diy market all they want. They still don’t have the real important markets of pre built computers and laptops.,Negative
AMD,"Genuinely, who is buying this shit? Average new car price is ~$50k now.  I make 6 figures and the most I've ever spent on a car was $22k, and even then I kinda regretted it (until I was able to sell it at a profit during the pandemic...)  Apparently several manufacturers are straight-up discontinuing base trims next year in an effort to boost that average sale price even higher.  This can only end in tears.",Negative
AMD,What do you mean intel isn't back? Intel has a 75% market share on CPU's.,Neutral
AMD,Thats a stretch but yes they aren't back yet,Neutral
AMD,Unfortunately I couldn't open the article. Thanks for clarifying!,Negative
AMD,Extra 3 dollars at least just for that.,Neutral
AMD,Source? There is nothing to suggest that's what's happening.   The Ryzen 9950X MSRP is $649.99 at launch (August 2024). I bought it in November 2024 for $599.99. It's currently $539.99 on Amazon. It would take a $110 increase to hit MSRP. This would be a MASSIVE increase and doesn't seem very likely.   My prediction: It'll hit $599 again at most,Neutral
AMD,"What's the MSRP? What's the current price?  That's all that matters.  Not whatever you imagine is ""supposed to be its original MSRP lol"".",Neutral
AMD,"Look, AMD is way behind Nvidia in market cap. By paying these increased prices, we're helping AMD stay competitive in the one arena that really matters. I think I speak for all gamers when I say that increased competition benefits us all.   /s",Negative
AMD,"RAM prices have to come down. The entire client market, especially OEMs, will be at risk of collapse otherwise. There's no reason to think a rapid 500%+ increase in memory prices due to a shortage (and the resulting panic buying) is permanent.   I dont believe we're about to witness the collapse of the client PC, smartphone, and tablet markets.",Negative
AMD,"The RAM situation is going to kill the entire PC market entirely if it stays this way.  That might be somewhat acceptable if AI demand simply never goes down whatsoever, but this really cant last.  Literally everything like PC's, laptops, smartphones, consoles, etc will all have to go up in price quite a bit.  It's not sustainable.   Also, SSD prices have been very reasonable overall for a while now.  And yes, the latest SSD's will cost more, but come down in price fairly quickly all told.  This is really not an issue.",Negative
AMD,I fully support China eating the revenue of these companies,Negative
AMD,"...so you're describing the lack of competition, aka anti-competitive price collusion. 😅",Negative
AMD,"Lay persons are going to be priced out of the market, we're going to all be on thin clients eventually as our hardware dies, paying monthly for a resolution/framerate package that doesn't meet it's advertised performance.",Negative
AMD,"> Counterpoint: *Competition only works if there’s checks and balances to prevent price collusion*.  That's what I'm saying since years now; Gamers blindly buying nVidia for the sake of it, ruined the GPU-market.",Neutral
AMD,"Even in medium-high segment, there is no competition, it's an absolute dominance of the 5070 and 5070ti against the 9070 and 9070XT. It's not even close.",Positive
AMD,"Brother, that's a whole lot of stupid.",Negative
AMD,"> Nvidia doesn't keep their prices high because of ""no competition""... they do it because they are greedy as fuck and have always been that way.  No, Nvidia keeps the prices high, because they CAN — People buy their cards, without thinking, no matter what.  > Even back in the day when AMD/ATI was crushing their shit GPUs by huge margin they still charged more because they handed out bribes to game developers for years and always got their BS software gimmicks like Phys-X in new games. They also spent way more on marketing because AMD had to spend huge amounts on CPU.  There you have it, the answer on why people buy Nvidia-cards. Not because those would be (always) necessarily better, but mostly out of the market's *brand-perception* — nVidia spent billions to fabricate their leader-image.  Same reason forwhy so many people still stick to Intel and answer questions about their CPUs with *""It's a i7!!""*.",Negative
AMD,I got another 3+ years out of my old PC simply by upgrading from 1700x Ryzen to 5600 Ryzen. That system is still viable I just wanted to upgrade to 9800x3d. Platform longevity should not be underestimated.,Positive
AMD,"Mmm I’m a bit confused about this, I thought due to AMD being on a smaller node they were more power efficient…",Neutral
AMD,"> Intel's lack of platform longevity is still a pain point […]  Kind of blows one mind, how Intel still sticks to their idiotic 2-CPUs-per-socket mantra no matter what …  As if it didn't cost them already a good amount of users switching sides, due to AMD have the better cards here.",Negative
AMD,And with the price of RAM right now those sorts of savings do mean a lot. 32GB+ of fast RAM has become as expensive as the GPU for a new computer.,Negative
AMD,If Intel comes out with something that is like the x3d chips then I'd consider them. That extra cache is too awesome.,Positive
AMD,"A big upgrade would be something like a Radeon 9060 XT 16GB, which is about 2x as fast as your current GPU. You can also get a used CPU, something like 5600X or 5700X should be available for not much money and will work, just update your BIOS first.",Positive
AMD,Ryzen 5000x3d is a drop in upgrade.,Neutral
AMD,Not sure. But I still use and have seen many Intel networking chips in the wild.,Neutral
AMD,Well Intel may be the one who made the chips that handle your wifi. They make a huge portion of networking chips.  So you very well might have a partial Intel system without knowing it!,Positive
AMD,"the situation looks dire for intel, they just sold everything off, a lot of engineers are leaving, the money cushion is gone",Negative
AMD,"I run multiple VMs, some docker containers, gaming, software development, etc",Neutral
AMD,"I was playing some Clair Obscur last night, had a few chrome tabs open (youtube etc) to look up certain builds or whatever, discord, etc. I don't even rice my desktop experience with widgets. But after a ~4hr gaming session my RAM was at 27GB. I have 64GB.  One thing I noticed in the past from my years of PC experience is that your system will generally use less RAM if its going to be approaching the limit. This suggests that theres algorithms that will use disk space instead and/or do memory reallocations. There is a performance cost to this.  That said I think 32GB is a good amount of RAM, I rarely approach it.  However you never know what apps or games are leaking memory so having a lot is nice.",Neutral
AMD,Consumerism just never stops.  It boggles my mind how completely thoughtless most consumers are with how they spend money.,Negative
AMD,> Average new car price is ~$50k now  Because of the high end dragging the average up. Entry level cars have been pretty consistently priced after accounting due inflation for quite a while now,Neutral
AMD,"A lot of people think they deserve a treat, the treat being a $75k new car. To the point that “paying off the car” is a common financial milestone.",Neutral
AMD,the carbrain insanity is very sad to see. People buying cars multiple times their yearly income in costs then wondering why they are getting fleeced. An average person spends close to half of lifetime income on a car.,Negative
AMD,"You say this as if their market share hasn't been steadily decreasing over the past couple of years.   What's even worse is that Intel's market share shrink has largely been slowed down by them pushing a bunch of high volume, low margin chips. If you look at revenue share in desktop, AMD is already at 40% share.   Intel's competitive position in desktop has only been deteriorating since X3D came out. Something which Intel's own executives have acknowledged, multiple times, in different conferences and earnings calls.   It's a lil insane people are still denying what Intel's leadership themselves have admitted is a problem.",Negative
AMD,I think they mean amongst youtubers.,Neutral
AMD,"They did something AMD can't yet, ARM level idle but in PPA,PPW and raw performance both AMD and Intel need to work on it.",Neutral
AMD,You get an extra fiddy from the 9850X3D,Neutral
AMD,">The RAM situation is going to kill the entire PC market entirely if it stays this wa  ""Hi, I'm Michael Dell""  ""and I'm Tim Apple!""  ""And together we're excited to announce our new industry-wide pricing initiative!""  ""To help alleviate the burden on consumer PC pricing, we're pioneering the launch of a new initiative that has been in the works for some time: 30-year PC Mortgages with fixed-rate APR!""",Positive
AMD,"> The RAM situation is going to kill the entire PC market entirely if it stays this way. That might be somewhat acceptable if AI demand simply never goes down whatsoever, but this really cant last.  We're lucky that internet infrastructure is so terrible in North America, otherwise Big Tech might succeed by sheer force in a pivot to replace local PCs (and ownership) with pure streaming clients.",Negative
AMD,"Yeah effectively. Competitors != competition. When the industry has super high R&D and setup costs (like CPU/GPU design and manufacturing) it's very difficult for competitors to enter and disrupt the market. Just look at Intel ARC, they are a billion dollar company and yet they are still having difficulty in GPU development.",Neutral
AMD,Its literally close according to every single benchmark but whatever.,Neutral
AMD,"Platform longevity has to have some kind of financial value placed on it.   The maximum value of long platform longevity is the price of a motherboard. And you maximize this value if you buy into the very first generation on that platform. So every subsequent generation you enter in, the value of platform longevity goes down.   You also have to consider what impact generational improvements have on platform longevity. Zen 1 -> Zen 3 was a huge jump. Assuming Zen 7 on AM5, I don't think we're gonna see that level of improvement with Zen 4 -> Zen 7.  Let's say someone upgrades from AM4 to AM5 with the launch of Zen 6. They receive less value from platform longevity than someone who entered at Zen 4. And someone who bought in at Zen 4 will likely receive less value than someone who bought in at Zen 1.  So yes, platform longevity does have value. It's better to have platform longevity than to not have it. But best case scenario, that value is the price of a motherboard, and we could argue on the whole that you subtract from that value based on the circumstances.",Neutral
AMD,"99% of users do not even know what a CPU is, let alone know how to upgrade one.",Negative
AMD,"1700x was already obsolete when it was released, my 4 cores non ht 4670k was superior for gaming with a basic overclock and it was released almost 5 years before the first gen ryzens. I see Soo many people braging about upgradability but all of them could've bought a 8700k a few months after the release of Ryzen and had a system stronger than 1000, 2000, 3000 series and slightly weaker than the 5000.",Neutral
AMD,"They were comparing to Raptor Lake. Also, ARL is on a smaller node than Zen 5.",Neutral
AMD,"AMD isn't on a better node. And power efficient is a difficult metric to measure because it varies wildly on context: ISO-Performance, ISO-Power, ISO-task that isn't time sensitive (web browsing). It depends on power profile (high-performance will sacrifice efficiency for performance). It depends on where in the efficiency curve you are. It depends on how high in the product stack you are (a 7500F is more efficient in gaming than a 9950X, but may be less efficient in highly threaded productivity apps, etc.)  edit: Downvoted for being factually correct. You don't just run CB24 at full power and divide scores to determine efficiency. That's way too simplistic.",Negative
AMD,"sure, the x3d makes a huge difference, but for my needs I didn't need the extra power or cost associated with an x3d chip for this more budget build.",Neutral
AMD,"Forgot to mention I edit video in DaVinci Resolve so CUDA cores are kind of a must for me, but yeah, the GPU and CPU are definitely getting swapped before anything else",Neutral
AMD,"Oh absolutely! The latest generation 10 gig chips from them are really good, very power efficient compared to prior offerings. But Intel networking stuff has (mostly) always been solid.",Positive
AMD,"Ah, I didn't know that. Not sure why I got downvoted for answering your question, though.",Neutral
AMD,"Wouldnt say that too early, amd suffered same fate like current intel and bounced back",Neutral
AMD,Yeah but everything seems to be more or less tracking with inflation except wages for some odd reason.,Negative
AMD,"Losing market share isn't great.  But they still have ~75%.  Saying a company with a 75% market share ""isn't back"" is dumb.",Negative
AMD,AMD's PPA is the best on the market. Just not for light workloads no one cares about. Run some database benchmarks.  Nobody cares what Geekbench gamers think.,Positive
AMD,Ninety-eight fiddy ex three dee,Neutral
AMD,"more like  ""Introducing out new lineup running Windows/MacOS SE, designed for just 4GB of DDR3! And it still starts at only $999!""",Neutral
AMD,"you could buy computer parts with a fixed rate loan for a very long time. You shouldnt though. If you cant afford it without a loan, you cannot afford it period.",Negative
AMD,you need to solve communication at faster than light speeds (good luck) to make everthing a streaming client. Anything that isnt local server streaming is absolute ass.,Negative
AMD,"Only if you ignore all the features. You know, the features buyers DONT ignore.",Neutral
AMD,I was talking about sales though?,Neutral
AMD,"Time is also a factor, the amount of time I spend swapping a motherboard is far more than the time I spend swapping a CPU.       Also potentially getting a bad motherboard if I was also swapping a motherboard (has happened twice).       That means loss of more time and effort to reinsert the old motherboard, RMA/return the new board, and then swap the board again again if I manage to get a replacement. Getting an RMA repair/replacement again isn't a 100% guarantee in this day and age with how stingy companies are. So I mean depending on how far you want to take this. By the end of this I could be out an extra $100-$200 in time.",Negative
AMD,"IDK about anyone else, but I had some bad experience with intel and wanted to support AMD. Helps I got the x370 Taichi motherboard as a gift. I still have 2x 32GB Kits of DDR4 that I upgraded to 64GB on two machines. I wonder if I should sell those. I think I should sell my 64GB Kit of DDR4 as well.",Neutral
AMD,Sorry I didn't mean to say it is a one size fits all solution. I'm just hoping that Intel figures something similar out to really push AMD down in price or get them to innovate further,Neutral
AMD,"In that case a 5060 Ti 16GB or something used, e.g. a 4070. I also use Davinci for editing on a 5060 Ti, for Fusion I'd welcome even more power tbh, but everything else works great.",Positive
AMD,"I guess it's the smugness of ""not a single piece of silicon from Intel"" when most likely you and most people do in fact probably have something from intel.   It's r/hardware anyway, most people here are teens that worship youtubers, dont sweat it too much.",Neutral
AMD,"Bulldozer era AMD was truly awful, their GPUs weren’t good, their CPUs weren’t good, and their server CPUs weren’t good either. Their turnaround is crazy.",Positive
AMD,[Inflation adjusted incomes have been trending higher for decades and are up about 5% in the last 5 years](https://fred.stlouisfed.org/series/MEPAINUSA672N),Neutral
AMD,"How can a company ""be back"" if you are literally losing share?   At best one can claim a company is ""back"" if they *stopped* the bleed, but even that hasn't occurred.",Negative
AMD,Geekbench is the opposite of gaming benchmark   And AMD wins ofc in benchmarks that rely on fat caches,Neutral
AMD,and your specific device will be abandoned by the OS in 3-5 years,Neutral
AMD,"Good. I wasn't. You can clearly read ""performance' in my message.",Positive
AMD,I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old. Although I don't really have the data.  What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?,Negative
AMD,I went from a Ryzen 1700 to now a 5800X3D on a x470. The value of platform longevity can not be understated!,Positive
AMD,"Yeah, I'm thinking that 5800XT + 5060Ti 16GB might be the best combo I can aim for until DDR5 is back to normal prices again.  How about ""higher tier"" cards compared to the 5060Ti? 5070s, 5080s etc.?",Positive
AMD,"No worries, it's just internet points at the end of the day, I was just a little confused when I went to respond to his comment and saw the vote count.  I wasn't even trying to be smug, it was more of a ""I don't own anything Intel related (so I have no reason to be going to bat for them), and I still think them failing to compete with AMD sucks for everyone"".",Negative
AMD,7970Ghz Edition was pretty beastly,Neutral
AMD,"game cpu king, productivity king, server? performance king but the industry is shifting to computer per watt",Neutral
AMD,"Intel isn't ""back"" because it never ""left"". They have always retained an extraordinarily high market share.",Neutral
AMD,"I didn't say geekbench was a gaming benchmark. But that people treat geekbench like a gaming benchmark and also that geekbench scores are easily gamed (benchmark runs so quickly that the CPUs can easily go beyond their thermal throttling point before the benchmark completes)..  Also plenty of workloads Zen excels at that don't get the benefit from cache. Like I mentioned database benchmarks, where SMT can give you 50% more IPC. Meaning better perf. and better perf/watt.  You know workloads that pay the bills. Not Geekbench. Of course Geekbench also pays the bills by misleading consumers.",Neutral
AMD,Weird. Considering we're in a price related thread. Whatever I guess.,Neutral
AMD,> What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?  With all the stuff you have to disconnect/rehome/recable? I think for the average home builder closer to an hour.,Neutral
AMD,"Maybe for an experienced builder, but to me (someone who's only built 1 PC) the prospect of just changing the CPU seems like a far less daunting task.  If you just want to replace the CPU, you unscrew 4 screws to remove your cooler/AIO pump head, pop open the socket, put your new CPU in, repaste and remount the cooler, plug some fans back in if necessary, and you're good. Maybe remove the GPU if you're really pressed for space.  When changing an entire motherboard, you have to unmount the cooler, remove the GPU, unplug every cable plugged into the motherboard, possibly remove some AIO fans/the radiator (or just fans in general) from the case if they're getting in the way of the motherboard screws, remove the motherboard, and remove any SSDs you want to reuse.   Then you have to put everything (CPU, SSDs, RAM) on the new motherboard, put the new motherboard in, repaste and remount the cooler, remount any fans you had to take off and plug them back into the motherboard, plug all your PSU cables again, then put your GPU back into place.  You basically have to redo a big chunk of the entire build process. Not to invalidate what you said, maybe you are proficient enough that all that would only take you 10 additional minutes. But I would not bank on the average DIY PC owner being able to do all that so quickly.",Neutral
AMD,"> What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?  Maybe if your build has basically nothing in it, mine has an AIO I would need to remove, PCI-e cards, GPU, 2 SSDs that would need swapping. I think it's more like an hour.  CPU, you just pop off the AIO, swap it out and put it back on.",Neutral
AMD,>I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old.  You might be surprised. Reliability of these things can very much follow a bathtub curve,Negative
AMD,>I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old.  The failure rate for a brand new board is thousands of times higher than the failure rate of a three-year-old board.,Negative
AMD,"QC for PC parts seems to have been rather lax lately, I received a board with multiple damaged pins, and another one with a bad bios, I also got a bad CPU. This was all on the build I did around this time last year. Because of these issues it wasn't up and running till February. I also remember when I was a young teenager I got a bad board from Frys in the late 90s. My friend also received an Asrock board a few months into 2025 that basically melted with stock settings.  For some on compact builds multiple hours. Multiple factors go into this. Swapping a motherboard can be a major PITA. Not to mention when dealing with multiple thousands of dollars of components I have to take breaks to deal with the stress.",Negative
AMD,"5070 has a reasonable price, but it's somewhat limited by 12GB of VRAM. 5070Ti is a great card at a still somewhat reasonable cost and 5080 is probably the worst offender, it's way too expensive for what it is.",Negative
AMD,"Productivity king is pushing it, intel still has the lead in that",Neutral
AMD,"productivity is clearly in Intels court right now. Unless you need AVX512, but the vast majority never will.",Neutral
AMD,"I agree, currently they’re the best in the very same categories they were awful at a decade ago. Productivity can be argued that Intel is still better for some things. Still, in a good number of use-cases, AMD is still fast than intel per watt. Most of the Core Ultras are using a shit ton more power to match AMD’s performance.",Positive
AMD,And are going bankrupt while doing so,Neutral
AMD,The term ‘Is Back’ refers to a positive directional change and not a current status. Their current status is collapsing market share and margins.,Neutral
AMD,"With a bunch of lower end, cheaper chips. They simply aren't competitive.   Here's a quote from the CFO of Intel:   >""As you know, we kind of fumbled the football on the desktop side, particularly the high-performance desktop side. So we're -- as you kind of look at share on a dollar basis versus a unit basis, we don't perform as well, and it's mostly because of this high-end desktop business that we didn't have a good offering this year,""",Negative
AMD,"10 more minutes vs a CPU swap, so not counting that time or cooler installer.   Move RAM over, move GPU over, move storage over. Cables are already routed and sitting right there.    Maybe 10 minutes is optimistic, but I think if its not your first build, an additional hour to swap the Mobo vs just the CPU seems pretty long.",Neutral
AMD,"I wouldn't want to do it that quickly, I like my cables nice and neat and shit.",Positive
AMD,"but ARM is winning in compute per watt, AMD is only a leader in x86 in that category",Neutral
AMD,"I was thinking swap as in same case.  Cable locations vary by mobo, although generally not by a lot.",Neutral
AMD,"ARM isnt winning in compute per watt, Apple design team is winning in compute per watt. Other ARM offerings arent any better than say arrow lake.",Neutral
AMD,Laptop CPU naming scheme is a real mess,Neutral
AMD,I LOVE LAPTOP CPU NAMING SCHEMES THAT MAKE 2 GEN OLD CHIPS SOUND MODERN!!!!,Positive
AMD,4 core/4 threads vs 8 cores/12 threads.   AMD really went light on this CPU. Only one full Zen core.,Neutral
AMD,"I don’t understand why they pit them together: sure 10-30% more performance, _at 50% more power consumption_.  Why not comparing it to an AMD with the same power consumption? It may give better performance…  Edit: I also like that they say “Intel consumption is _slightly higher_”, and on the next sentence “…would draw 68W or _almost 50% more_”. Article is clearly written by Intel PR…",Neutral
AMD,Yeah sure not in a similiar configuration.,Neutral
AMD,Don't see any reason to buy anything on intel 7 unless at a large discount.   The intel stuff on tsmc 3nm is much better,Neutral
AMD,So there are two laptops at same price just one has the Ryzen AI 5 330 the other has the Core 5 210H which one is better? I just saw this review of a laptop with the intel [https://www.youtube.com/watch?v=q5OFCLvxA6U](https://www.youtube.com/watch?v=q5OFCLvxA6U)  and it looks very very bad for battery life not even very light gaming.,Negative
AMD,"I would like laptop without AI please, especially CoPilot, that product is hot trash",Negative
AMD,"What a silly comparison, they aren't even close in price... The AMD only has 4 cores, of course it's going to take a huge hit in multithreaded...   Also, I just realised this is a repost from 9 months ago, maybe prices were different then? Why post something so old now though that's not even relevant anymore?",Negative
AMD,The real budget performance winner is an m1 macbook air.,Neutral
AMD,"Hello Balance-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,Yup. Now it seems like Apple is the only company with good CPU names.,Positive
AMD,"Was gonna say price, but looking at Lenovo's website and I couldn't even find the Ryzen AI 5 330 in my country, but the option with Ryzen AI 7 350 is still cheaper than the option with the Intel Core 5 210H.   So this is a really odd choice for comparison.",Negative
AMD,50% higher power consumption for *50% higher performance (in nT benchmarks),Neutral
AMD,"68W is only the peak at the start, otherwise the chip stays at around 50-58W during gaming. Even the AI 5 330 can peak at 58W and 43W in games too according to their review so it isn't that much of a difference either way.",Neutral
AMD,The better comparison should be 210H vs AI240  13420h reskin vs 7640HS reskin  But that would be boring,Neutral
AMD,"And probably on par in consumption, since the node is smaller.  This comparison right here is like comparing an i3 to an i5 and saying the i5 has better performance…",Neutral
AMD,That's Alder Lake. Can't remember anything later on the Intel 7. There used to be some decent 12-14th gen CPU for it.,Neutral
AMD,"Eh, I'm not a fan of the Pro Max Ultra shenanigans.",Neutral
AMD,Well you can configure the cpu and GPU cores to be different and still have the same name so no they're not better,Negative
AMD,"My gripe with Apple is it’s TOO simple. M4, pro, max, ultra.   Doesn’t really say how may CPU or GPU cores, as it varies by device SKU (some got the binned cpu like the air got 1 less gpu core)",Negative
AMD,I see a $100 price difference in the US with the ryzen at $500 and Intel at $600 through Lenovo. However the thing is the core 7 240H model is only $1 more than the core 5 and the ryzen 7 with 860M graphics is only $525. So why bother with either model at that point?,Neutral
AMD,"Where I'm from, 330 is very sparse and for some WILD reason more expensive than similar 350 options.  At the same time, 350 is more expensive than 225h... (Edit: re-checked, 225h isn't cheaper anymore it seems)",Negative
AMD,"Yeah. That’s like comparing an i3 with an i7. An i7 will consume more, and will have better performance. That’s why the comparison is odd to me: why comparing a Core 5 with an AMD that has to be compared with a Core 3?",Neutral
AMD,yeah i've never even heard of the ai 330. Crazy they gave it the 5 specification when it has 4 cores and three of them are C cores with only one full zen 5 core.  [https://imgur.com/a/lMajlf6](https://imgur.com/a/lMajlf6)  generally speaking amd is better efficency wise although intel does make up for it with the lower idle draw and lower draw in low-medium loads. The higher binned hx stuff have better efficency but you have to power limit them bc out of the box they go like 100w+     edit: from what im seeing both intel and amd have given up on the i3/3 naming in the laptop space but I still wouldn't say it is a fair comparision bc u should compare it to the 226v which has the NPU like the ai 330,Neutral
AMD,Go ask why amd names it a ryzen 5 and not a ryzen 3? Hmm?,Neutral
AMD,No the 210h is on intel 7 same with the 240h,Neutral
AMD,"The 210H is ""Raptor"" Lake(Alder, since the P Cores have 1.25MB L2 instead of 2MB like actual Raptor Lake)   12450H -> 13420H -> 210H are the same CPU, with clock speed bumped up with each iteration",Neutral
AMD,Well they always list the core count directly in the full product title on their site,Neutral
AMD,"It's not.   The Core 210 is Intel's budget chip.   Both chips are priced similar. It's a Core **5** vs a Ryzen AI **5**  Both are positioned as ""5"".  Intel adding E cores to boost the nT performance on their products has been giving them the nT lead in the lower end segments for a few generations now",Neutral
AMD,Whatever that has to do with technical specifications…,Neutral
AMD,"That and this comparison also saw the Intel version having better gaming percent, but it is only like a 5-10% difference across the board, with more games favoring Intel and some other favoring AMD",Neutral
AMD,Why did YOU bring up i3 vs i5 then? Huh?,Neutral
AMD,"Because they are both from Intel, I was trying to convey the fact that they are not on the same category. I need to refer to the same family somehow…  Now, saying Ryzen 5, 7, 9, etc. makes only sense within AMD, same for Intel within Intel. But different manufacturers have different nomenclature, and even different ways to measure TDP.  TDP on Intel is true TDP at minimum power, for AMD you can go lower than the TDP they publish for PL1.  So, at same power, AMD is also competitive, just that they have different nomenclature.",Neutral
AMD,"They *are* the same category. The two companies just approach that category differently. Intel decided to have 4 extra E cores to boost nT performance, but at the expense of higher power consumption if you load those cores.  This is the lowest end """"""current"""""" gen CPU H series CPU from Intel.",Negative
AMD,Bought an XT last week cause stock has largely been sorted in the last few months and quite a few models have been under MSRP and that situation could change in the coming months because of the DRAM situation. I also wanted to go AMD since I moved to Linux so I've been pretty pleased. For as much as we complain this isn't really a bad time to buy a gpu.,Positive
AMD,TLDW:     **7 Game Average (Low/Medium):**         1080P:       RX 9070 16GB is:     ~28% faster than the RX 7900 GRE 16GB    ~24% faster than the RX 7800 XT 16GB    ~33% faster than the RX 7700 XT 12GB    ~80% faster than the RX 6700 XT 12GB       ~159% faster than the RX 5700 XT 8GB     1440P:     RX 9070 16GB is ~96% faster than the RX 6700 XT 12GB       4K:    RX 9070 16GB is ~113% faster than the RX 6700 XT 12GB       **7 Game Average (High/Ultra):**         1080P:       RX 9070 16GB is:     ~25% faster than the RX 7900 GRE 16GB    ~30% faster than the RX 7800 XT 16GB     ~60% faster than the RX 7700 XT 12GB       ~100% faster than the RX 6700 XT 12GB       ~160% faster than the RX 5700 XT 8GB,Neutral
AMD,I like these types of videos that compare against previous gens.  It's very useful for potential buyers to see generational gains and determine if it's a justified upgrade,Positive
AMD,I'm happy to see 9070XT is doing well. Having said that my 7900XT Nitro+ ain't going anywhere any time soon.,Positive
AMD,"Got a 9070XT on Black Friday, now that they're so cheap I couldn't resist. Really impressed with it, performance is great, runs pretty cool and Adrenaline is really nice to play around with undervolting and overclocking. Got a real nice undervolt on it.",Positive
AMD,"Adjusting for inflation doesn't really help, a lot of the older AMD cards dropped below MSRP pretty shortly after release (excluding 5700XT). I ended up getting my 6700XT for ~$290 USD.  Also in Australia it's currently possible to get a 9070 for a theoretical $425 USD excluding tax if you are able to use the $60 AUD store credit, $50 AUD gift card (easy since it can be Amazon), $20 USD Steam credit and ~$10 AUD cashback.",Neutral
AMD,It's the best value for money gpu,Positive
AMD,"As a day 1 RX9070 owner, I can say I'm quite happy with it. I had some stability issues in windows early on, though those went away with driver updates and probably would've been completely avoided if I did a clean install, and it does suck when games only support the Nvidia features even though AMD has an alternative. Aside from that, performance has really solid. I play at 1440p, so regardless of the game I'm playing I know I can crank basically all the settings to the max, not bother with any of the upscaling, and still get FPS numbers well above 100.  Also, it does hurt knowing the ""best value GPU"" in 2025 is more expensive than *my entire rig* was back when I started. Not sure how anyone can afford to get into PC building nowadays if they're starting from zero.",Positive
AMD,Best graphics card for linux.,Positive
AMD,"Just bought the XT version of this card for 4K gaming on my TV and it’s an absolute beast.   Loving it, especially with a slight Undervolt to lower to power consumption.",Positive
AMD,"After going from AMD to Nvidia, raw perf doesn’t really mean anything to me anymore. Transformer DLSS is so good it beats AMD at native res.",Positive
AMD,plenty of cards in stock at your local Micro Center...,Neutral
AMD,Bought a 6800 reference card back in December 2020 and just got the 9070 today to replace it. For me the biggest draw was power to performance 220W vs 304W of the XT,Neutral
AMD,"I've been rocking my 6700XT for years, it might be time for an upgrade, holy cow!",Positive
AMD,"Do they talk about the value with ""maintenance mode"" or ""extended support"" at 3.5 years, and without it?",Neutral
AMD,While the raw performance is good I am still concerned about the adoption rate of FSR4 and Redstone.  I'm waiting another gen to see if it catches momentum.,Negative
AMD,Buying the 7900 XTX for $889 a couple years ago was the best decision I made. Card kicks ass,Positive
AMD,Been having driver problem lately with my Rx 9070. :/,Negative
AMD,"For people that havent been following since 2019:  5700XT 2019 launched at $400 (adjusted from $500), rivaled the 2070 super but had bad drivers for at least a year. Overall, underwhelming  6700XT launched at ??? But once available in 2022 it was generally under $430. Good drivers, and value. Overall, great GPU crushed under pressures of pandemic.  7700XT, trash. Dont even wanna talk about it.  7800XT launched $500 late 2023, delivering around 25% more perf than the 6700XT. Overall good launch/drivers.  7900GRE launched at $550, 2024. This GPU was an absolute mess. It launched super late, had weird underclocked memory issue at launch (despite it being around for a year in China already) and was priced too closely to the superior 7900XT.  9070 launched at $700 in 2025. Good GPU but completely crushed by the 5070 by value/availablility. FSR4 launched with limited implementation and heavy RT perf was only about a rtx 4070. Path tracing not really possible.",Negative
AMD,guessing it not worth upgrading from the 7900xt looking at those GRE numbers,Negative
AMD,"Any explanation for why the 7800XT beats the 7900 GRE at 1080P low/medium?  Is it that the greater amount of MCD:s in the 7900 GRE vs 7800 XT puts more pressure on the CPU, which results in a bottleneck on the 7900 GRE? Or is it just more MCDs = more latency = worse performance with high FPS due to latency?",Neutral
AMD,Cannot do sound now but at no point in graphs i saw mention what RT settings they used.,Negative
AMD,"That's basically the same difference in their memory bandwidth, could have saved themselves a lot of testing and just used those stats.",Neutral
AMD,"And with how idiotic MS is liable to get with Windows, with the AI bubble visibly approaching explosion, the better Linux driver sit only makes a good deal better.",Negative
AMD,"Since you've not had it long, let me pass on some unsolicited advice:  Leave the undervolt off when not gaming. You won't eat up any extra wattage or anything, but it will ensure that you don't run into any issues with stability at low power states. Not that you're guaranteed to have issues, just that it sure makes things easier to diagnose when you KNOW it's not your ""OC."" Adrenaline makes it so easy that you might as well.   Additionally, when starting a new game and facing crashing, the absolute first thing you should do is cut your undervolt in half. It's astounding how much variation in stability you'll face with undervolts and different games.   For example, my 9070 XT is stable at -70mv in any synthetic benchmarks I can throw at it. But it wasn't 100% stable in Horizon Forbidden West until I dropped it down to -65mv. Then in Tiny Tina's Wonderlands I had to drop it down to -45mv to avoid the occasional crash.   But if I load up Horizon again, -65mv for 10 hours straight no problem.   Play around and figure it out. It's fun.",Neutral
AMD,The 6700xt was around $350-$400 when the 7800xt released for $600 in my country.,Neutral
AMD,"Yup. Bought my 5700 non-XT well under $300 (MSRP 349). The 6700XT did eventually drop below MSRP, and stayed there for a while. But do remember that it took a long while for that to happen.",Neutral
AMD,6700 XT was going for 900 during 2021. It didnt start getting cheap until LONG after it's release.,Neutral
AMD,"Which is amusing, because at launch it was the worst - clearly positioned to be an upselling device to make the 9070XT look better.",Negative
AMD,"I mean, technically true but boy what a lame way to win. Only 20-25% faster than the 7900gre it replaced. I guess Moore’s Law really is dead",Negative
AMD,Except for when it isn't: [https://www.youtube.com/watch?v=mPQWoSEYMLs](https://www.youtube.com/watch?v=mPQWoSEYMLs),Neutral
AMD,"Really? The last time I looked, the 5070ti, 9070XT and 5070 are in the top sellers list. not the 9070.",Neutral
AMD,Okay but like how about just the best GPU?,Neutral
AMD,>Not sure how anyone can afford to get into PC building nowadays if they're starting from zero.   Have 9th or 10th gen Intel office PCs flooded the market yet? Those types of desktops used to be good for an low budget gaming / workstation PC.,Negative
AMD,"> Not sure how anyone can afford to get into PC building nowadays if they're starting from zero.  Before the RAM/Storage prices went insane last month, people just accepted that you either build a budget rig (still stronger than a console) or expect to spend >1000 on it. Now though i would suggest agaisnt buying a new build until memory prices get better.",Negative
AMD,When consoles are $600+ what people consider good value changes. Welcome to modern electronics. Hell welcome to modern everything. A carton of eggs is over $5 now,Negative
AMD,"Which is more important then you'd think with the total estrangement from reality down in Redmond nowadays. Even if you're not immediately planning to switch to linux or go dualboot (like me), with how loony toons shit is getting in that operation, being able to as painlessly GTFO of the MS ecosystem as possible on short notice is a capability worth planning for in a build for if and when MS finally does a dealbreaker for you.",Neutral
AMD,"been extremely pleased with my 9600x/9070 system on Linux for at least 8 months or so now.    Only somewhat miss Adrenaline, and setting up fsr4 being slightly less annoying on Windows",Positive
AMD,I had the oppotunity to test out DLSS vs FSR native implementation by developers in same game a few times and DLSS is just so much better. I always use DLSS quality if its an option now. It has better AA than native.,Positive
AMD,It doesn't help that it is now a felony to play games that don't have PT or DLSS.,Negative
AMD,"> 7700XT, trash. Dont even wanna talk about it.  Agree in general but it was on sale multiple times for ~$360 with 2 bundled games, if you wanted or sold those it was a solid buy, arguably the best value card on the market at those discounts. 9060XT 16GB is marginally slower for the same money, albeit with better RT, upscaling and more VRAM.",Negative
AMD,Man what is it with you people and needing to upgrade every damn generation?,Negative
AMD,"Both actually have the same number of active MCDs. On paper, the only spec that's worse on the 7900 is the memory bandwidth due to running the VRAM at lower clocks.",Negative
AMD,"> It's astounding how much variation in stability you'll face with undervolts and different games.    I've  benchmarked with as low as -125 on my 9070    -77 seems to be the limit for Helldivers, and seems to be rock solid for everything else as well so that's where I stay",Positive
AMD,"I bought 12/07/2023 so before the 7800XT release, the 6700XT was quite a bit cheaper after that release, they went down to ~$263 USD before tax but I pulled the trigger early because it came with Starfield which was... let's just say I haven't even bothered trying to play it because the stories and gameplay I saw didn't seem that interesting...",Negative
AMD,"Oh right, I think maybe covid was still affecting things back then, not sure.",Neutral
AMD,"Yep because it's using same die as 9070 xt, it's benefiting from driver performance boost",Positive
AMD,"Ehhh, yes and no. Worst of Radeon, still less of a turkey then that 12 gig 5070.",Negative
AMD,"Getting 30% gen-on-gen in pure raster, with an even bigger uplift in RT and ML workloads on a smaller die is pretty good. To do it at a 40W lower TDP is really impressive. To get those uplifts purely from architectural improvements, with no real node shrink to speak of is completely absurd.  Also there are still very big gains to come with RDNA 5/UDNA.  Next gen will almost certainly be produced on N2P, skipping N3 entirely. It should yield performance uplifts almost as great as Maxwell -> Pascal, assuming AMD/Nvidia don't decide to shrink the dies massively.",Positive
AMD,"Well if we think of it in terms of release in the US it comes across great. 6 months later, same price, less power, +28% is decent",Positive
AMD,Thats the sad reality train we are on now. Pray that we actually even get these when quantum tunnelling gets worse.,Negative
AMD,"Probably more like 40%+ in ray tracing. It is still dead, but  also this architecture is more forward thinking, and spending its transistors on next generation stuff.   I'd imagine RDNA5 will be another big leap, and this will age more poorly in some regardless, compared to the RTX 5000 series from Nvidia. If Nvidia ages worse, it's because of artificial implanted software limits. Like they blocked frame generation on the RTX 3000 series.",Neutral
AMD,"If we knew the actual margins AMD and NV were taking, we'd have a much better idea of the state of Moore's Law. That and the cost of designing more and more complex ICs creates it's own growing cost outside of ""cost of number of transistors per sq mm."" And the more and more complex software environments.",Neutral
AMD,Top seller and value for money are two entirely different and unrelated concepts,Neutral
AMD,"Do you not understand the difference between ""best selling"" and ""best value""?  The 9070 gives you the most fps/$",Negative
AMD,"Really, where do you see top seller list for all GPU SKUs combined, comparison?",Neutral
AMD,Because of the recent performance boost 9070 is a lot closer to 9070 xt there's only 8-10% performance difference,Neutral
AMD,"That's the 5090, and it's not even close. No one is cross shopping a 5090 and a 9070 though.",Negative
AMD,"Maybe, I haven't checked.  Talking about office PCs, post covid IT spend at companies shifted massively towards laptops, now that work from home / hybrid work is a common thing. Buying a cheap, old office PC will be far harder in a couple years time, simply because supply is lower than it used to be. I doubt we'll see floods of 12th+ gen office desktops anywhere near the scale we saw with 5th/6th gen.",Neutral
AMD,The PS3 (60GB) launched at $599 in 2006.,Neutral
AMD,Eggs is a very specific US issue due to chicken pandemic. A carton of Eggs (10 units) here in eastern europe is 2.15€.,Negative
AMD,"LOL, its not a crime but if you are looking for the best, its objectively better.",Positive
AMD,"The problem with the 7700XT wasn't necessarily that it was a bad card.  The problem was that the 6800 non-XT offered basically the same performance, basically the same efficiency, and was very often cheaper throughout most of that card's effective life.  I know someone who snagged a 6800 brand new for $330 more than a year into the RDNA3 era. Also, supplies didn't dry up for a **long** time in many/most markets. It was basically a no-brainer. It was an equivalently-priced (or cheaper) 7700XT with more VRAM. There was zero reason to even consider a 7700XT throughout most of its run.",Neutral
AMD,"Retroactively, the 6800 has been $340-$380 as well. The 6800XT was $400-450.",Neutral
AMD,It will depend on your wealth level. If GPU is not a meaningful expense for you then performance will be the only question.,Neutral
AMD,"So I guess my theory that it's latency limited would be correct, then? Memory bandwidth likely isn't an issue at 1080P low/medium, but high latency very much could be, especially with the RDNA 3 MCD designs that I assume come with a latency penalty due to the memory bus and compute die not being on the same die..",Neutral
AMD,"-75, you mean? Because AFAIK, it still only works in .05mv increments.  I mean, I've benchmarked as low as -95mv on Steel Nomad. Didn't mean it would pass 2 hours of benchmark loop. And to me, stable requires hours of testing minimum.    If you can get -75mv stable in a handful of games, you're over the 50 percentile line in the silicon lottery.   But I maintain that you'll find a game that doesn't like -75mv at some point. Or maybe not, and other games you could get even more than -75mv. Shrug.",Neutral
AMD,"You bought like 1.5 years after release, that's not exactly ""shortly after release.""",Neutral
AMD,Stop basing your opinions off other people's.   Form your own. Starfield is a great game and offers a fantastic experience.   I got my copy with the 7800xt when it released and I remember it still being quite close in price. Much lower than what it was when I got my 6700xt for $900 because my laptop decided it was time to go to Laptop Heaven,Positive
AMD,"Cyptomining 2: Crypto-bro boogaloo.  Late 2020 through 22 was prime crypto-bro and scalper hell. Even 6700XTs were going for $800 for a while there because you couldn't get anything above a 6700XT at all. They sold out to bots immediately, and some humans. Within minutes of going into stock.   6700XT's were less susceptible because the 192bit mem bus made it less profitable for eth mining assho... folks. But someone, whether it as AIBs, retailers, someone, was still jacking prices sky high on them. Not talking about scalpers, but cards you could buy directly from Newegg.",Negative
AMD,yep the 9070 is just defective 9070XT dies being put to use.,Neutral
AMD,Wafer pricing is going from $16-18K 5nm to 30K 2nm so don't expect the same die sizes.,Neutral
AMD,"RDNA 3 was a undercooked bad gen. RDNA 4 fixing all the issues, chopping LLC and Mem phys by a third and going monolithic makes perfect sense. N5 -> N4C helps a little as well.      N2 is highly unlikely while N3P is pretty much confirmed by Kepler\_L2 for GFX13 across all products using it.  Maxwell -> Pascal level perf gains is overly optimistic. Node progress doesn't permit another 30 -> 40 series perf uplift.  But insane gains can be expected in ML and PT over RDNA 4 fs.",Neutral
AMD,"And that's before whatever perf is shaken out of RDNA 4 by Redstone and other software upgrades to come, which may be significant.",Neutral
AMD,"ML is irrelevant. You aren’t getting this card if you want to run LLMs. RT is borderline irrelevant. If you want to run RT in the latest games you will need something much more powerful than a mid range card like this. And lastly it isn’t 30% raster, its 20% in 1440p and 25% in 4k which is pretty disappointing.  The only thing that is pretty good about this card is that everything else is worse. It wins by virtue of being just disappointing in a market flooded with absolute garbage",Negative
AMD,"Fs and I can't wait to hear more about RDNA 5, but unfortunately it seems like it'll be radio silence until well into 2027. Look at how AMD handled RDNA 4 as well, nothing until one week prior to launch. Maybe a Cerny teaser for µarch at GDC 2027 with Road to PS6?  Yeah and FG limiting egregious especially after the new model got rid of OFA entirely. Fingers crossed they unleash it when 60 series launches as a good gesture similar to DLSS TF across all RTX cards.",Negative
AMD,"Are you talking about the 26.6.3 drivers performance boost or is there a newer one recently? If it's really within 10% of the XT, then it makes sense to buy the 9070 yes.",Neutral
AMD,Sure but people were cross-shopping a 4080/90 and a 6900/7900 tier card,Neutral
AMD,The only desktops we have left at my job are incredibly cheap mini pcs or insanely expensive dual quadro towers.  Nothing really in the middle that would be viable for upgrading for home use in the future anymore. :(,Negative
AMD,And the PS4 launched at $399,Neutral
AMD,my best friend just went to leavenworth for playing at native  how dare you,Neutral
AMD,exactly right.,Neutral
AMD,"Even if you're rich, you don't need to upgrade every generation. That's just mindless consumerism. And judging from the fact that this guy is even asking the question, then he's not loaded.",Negative
AMD,"> -75, you mean? Because AFAIK, it still only works in .05mv increments.    LACT in linux allows .01, on windows I did have it at .75     Never had it crash on anything at that, did pretty good in the lottery it seems.",Negative
AMD,"Yeah, I remembered wrong with the 6000 series, I thought they dropped faster (way before I bought but not to this extent), 7000 series dropped pretty fast though.",Negative
AMD,"Dies are not, and have really never been the main cost driver in GPUs.   Dies were absolutely a bit cheaper back in like 2017, but it's not by the huge amount that people believe. These days the die is the most expensive individual piece of a GPU, but not anywhere near the majority of the cost.  The GB203 in a 5070Ti or 5080 costs roughly $100 assuming 95% yields and 18k per wafer (95% viable for 5080 or 5070Ti). A 1080Ti die (GP102) would've cost about $50 adjusted for inflation.   So yes, the dies are more expensive, but they have gone from like 7% of the total price the end user pays for a mid tier to high end GPU, to about 10-13%.   The main cost driver for GPUs is not the dies, but the fact that Nvidia can make so much more money elsewhere, and need to jack up prices in order to justify the continued existence of their consumer GPU business. A wafer used to produce GB203s is a wafer out of Nvidias wafer allocation that can't be used to produce much higher margin GB200s. Every wafer of Nvidia's allocation not used to produce GB200s basically costs Nvidia to lose tens, if not hundreds of thousands of dollars they would have made in additional profits. By raising consumer prices they can somewhat soften that blow, hence why they have continued to go up.  N2 is probably where the wafer costs will actually start being a significant cost driver if Nvidia/AMD want to maintain margins, but we should still only be talking consumer price increases of perhaps 10-15% to maintain those margins. Obviously Nvidia and AMD will shrink the dies and jack up their prices much higher anyway, then shift the blame onto TSMC and their wafer prices. But there is really no reason they need those huge price increases for the move to N2 to make sense financially.  Shrinking the die on lower tier cards may be more justified though, as die costs do actually make up a large component of the manufacturing cost there.",Neutral
AMD,"Yeah, great proof of concept of GPU semiMCM and heralds great things for shit after RDNA 4, but the silicon itself was borked.",Neutral
AMD,"ML is very relevant, and is only becoming more so every day.  You want enough horsepower to  be able to do neural radiance caching, upscaling, frame gen, RR and potentially neural texture compression, all on the fly at the same time. A modern ML feature set with a lot of horsepower becomes very relevant then, as you want to avoid a huge latency penalty for doing all of these things.  Also they do rt well enough as long as you're fine iwth upscaling.",Positive
AMD,"It did Control at 1440p perfectly acceptably. RT isn't the prime determinant yet, but RDNA 4 is pretty competent at it.",Neutral
AMD,https://youtu.be/YWUqsqcM4Hs  Watch this,Neutral
AMD,You mean a 4070TI and 7900 cards? These perform about equally.,Neutral
AMD,"The [Tiny / Mini / Micro](https://www.servethehome.com/introducing-project-tinyminimicro-home-lab-revolution/) PCs have their fans because you can cluster them together and use them as compute nodes for a home server setup, but they're in no way good for gaming.  Just to make the situation even more dire, low profile GPUs are mostly dead now. The best half height, slot power card you can buy is an RTX 3050. If you have an 8 pin available, your only modern option is a Gigabyte 5060. Even if you managed to find a middle of the road 12th+ gen office desktop, you're going to struggle buying a GPU to go with it.",Negative
AMD,"You don’t need a gpu if we’re going to go down that road. Worrying about how other people spend money, now that’s a waste.",Neutral
AMD,if you're rich youll upgrade for best performance irrelevant of price.  You know what is mindless consumerism? playing videogames.,Neutral
AMD,"Damn, thwarted by the limitations of windows bullshit. Didn't know you could adjust in smaller increments under linux.   Yeah, certainly sounds like you've landed a winner.",Negative
AMD,Control is also a 6 year old game,Neutral
AMD,"Playing video games is not mindless consumerism. Now if you're constantly buying games and not playing them, then yeah.",Neutral
AMD,Just wait  2̶ ̶m̶o̶r̶e̶ ̶w̶e̶e̶k̶s̶ ̶ 2 more years and prices will drop again 🥳,Neutral
AMD,"I bought a 9070 XT just after the RAM price increase was announced. I already missed a good time to buy GPUs in the past, because I didn't react to industry news. But not this time.",Negative
AMD,"Oh boy, I love when things become more expensive!",Positive
AMD,"So, has anyone considered switching to hiking? Definitely  more affordable than gaming these days.",Positive
AMD,"I'm honestly surprised it's so little, although I suppose this reminds us just how little 8 or even 16GB of GDDR6 was costing AMD and Nvidia until very recently.",Negative
AMD,"If this turns out to be true, I would be shocked that AMD raises prices before Nvidia.  Nvidia is addicted to money, and I feel like they will exploit any excuse they could to make more money.",Negative
AMD,Not really anything AMD can do about RAM prices. They were probably priced with low margin to begin with,Negative
AMD,Well the 8GB cards were not good value and arguably AMD and Nvidia can stop making them to use the same VRAM chips for higher margin SKUs,Negative
AMD,I grabbed a 9070 2 weeks ago in off the price hike hype bubbling up,Neutral
AMD,"Hello JohnSteveRom2077! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,And I thought I was an idiot for building my PC 3 months ago and not waiting a few more months for black Friday deals.,Negative
AMD,"yeah snagged prebuilt 9800x3d w/9070 XT and some newegg bundles to build a second, tried for the 499.99 9070xt today that failed lol, bot scalped in less than second lol.   So got the 599.99 asrock challenger.  My 2600x and 1080ti are still playing stuff surprising well at least on lower setting at 1440p, but time for an upgrade and teenager needed something he's been using an old 1660ti but playing mostly older stuff.  ram and vram are not going to get cheaper anytime soon and chip producers have little incentive to increase capacity when they can still rake in profit at higher per unit profit vs. more volume and without the risk of building out capacity.",Neutral
AMD,"Well se them next time, I'll buy a new TV or a 3d printer, no need to give them money.",Neutral
AMD,"I've been watching the prices over the last couple weeks since I was in the market for a 9070 XT. Prices went down over Black Friday/Cyber Monday, but they're now back to pre-BF/CM level.  Maybe, just maybe, there's a hint about the reliability of this source that he claims there'll be no new graphics cards from AMD until 2028?",Neutral
AMD,steam deck is looking better and better every day,Positive
AMD,20-40 dollars right now\*,Neutral
AMD,"20-40 isn’t a lot compared to ram prices. does anyone know if cpu price will increase, am like 98% done with pc and I plan on getting r7 9800x3d with rx 9070 xt gpu i will get in two weeks.",Neutral
AMD,AMD making sure it beats Intel's 1% market.,Positive
AMD,"I love this Nvidia does nothing... AMD we are starting to come into line with peoples price expectations. We are starting to slowly claw back sales from the grips of Nvidia.  AMD Video team ""Can't have this we need to full stupid into the wall. Consumers can't forget we are the under dog and if we eat 20-40 dollar increase that just isn't happening""",Positive
AMD,"Another proof that AMD Radeon also doesn't give a damn about their consumers and GPU market share and will happily just pass the price increase of the components to the consumers instead of taking this as an ""Opportunity"" to gain marketshare because Nvidia is also expected to have price increase on their GPU soon.  Speaking of Nvidia I wonder why they still haven't increased their GPU prices yet? Could it be that they are less affected by this dram shortages? I have heard that GDDR6 had a higher price increase over GDDR7 for this very reason, because there are a lot more products out there that is using GDDR6, compared to the GDDR7, which is what the RTX 50 is using.",Neutral
AMD,Buying the recent dip in prices to below RRP might have proved to be the right call.,Neutral
AMD,"AMD just got their products to MSRP, and now if we go back into rebate part trois - woof!  AMD just can't catch a break - well shouldn't hurt much they only shipping like 15 GPUs per week (hyperbole).",Neutral
AMD,"This makes no sense.  AMD supplies the GPU die not the VRAM.  ""Hey AIBs, you are paying more for your gddr, so I better raise the prices of the GPU for reasons"".",Negative
AMD,"Woah.. at this pace, a RTX 5090 may actually be viewed as the card for future-proofing and investment into future price hikes..  Yikes!!!",Neutral
AMD,"I always hated the conversation “should I wait for this?” “Should I wait for the next gen?” No one knows what the future holds, even the companies releasing these products!  I always say just buy what you need when you need it, do your research, and enjoy!  Edit: not throwing any shade to you, just mentioning a related topic that I come across a lot!",Negative
AMD,Yea like finally buying an affordable 6600XT in the year of our Lord 2023 when RDNA3 and Ada are already out.,Positive
AMD,"Yup same, got a 9070 XT the other day for around €500ish. Had to get it for that price, and now I’m not going to be worried about “missing out” just like when the Crypto phase popped in and fucked us over.",Neutral
AMD,Because it is better right?,Positive
AMD,"As the old joke goes, the graphics are amazing but the gameplay sucks.",Negative
AMD,"Went outside, wasnt impressed with the graphics. Lack of DLSS support in 2025 is ridiculous.",Negative
AMD,"I'll just stick to 5+ y/o games, or non first person shooters",Neutral
AMD,"Jogging around local park, assuming your local park is safe can be decent alternative? Maybe riding some cheap bikes also work.",Neutral
AMD,"as far as hobbies go, PC gaming still one of the cheapest. hiking is certainly up there in cost comparisons though.",Positive
AMD,"NVIDIA's rumored (heavy pinch of salt) to be decoupling VRAM from their GPU trays. If that's true, you're going to see a steep price increase as well because the AIBs are going to be fighting with one another to get their memory modules from vendors who aren't keen on expanding GDDR7 production beyond their initial roadmaps.",Negative
AMD,Nvidia has much better margins on gaming GPUs so they can absolutely absorb the costs if they desire.,Positive
AMD,Pretty sure Nvidia doesn't ship memory out to board partners.,Neutral
AMD,Every corporation is addicted to money and stock value. AMD isn't our friend.,Negative
AMD,AMd didnt have a load of GDDR6 piled up.,Neutral
AMD,nvidia isnt providing dram anymore. its up to the partners to raise prices.,Neutral
AMD,"Nvidia has a FAT margin on GPUs,AMD not as they try to reclaim parts of the market",Neutral
AMD,8GB is likely higher margin than 16GB.,Neutral
AMD,Five days ago someone see $560 9070 XT on microcenter.,Neutral
AMD,"They currently have the #1 spot on Amazons best selling GPU's list. Both in the US/global and in Europe (Amazon.de).  The 9070 XT in particular is very popular with DIY'ers and enthusiasts. Nvidia is doing well in the OEM and laptop markets, but for us DIY'ers and enthusiasts, AMD is offering very competitive products. Still behind Nvidia for sure, but doing better than it used to.  And for people rooting for Intel, I know this is an Intel sub mainly, but if you wish more competition, we shouldn't look at Intel. Intel is only eating at AMD's market share, not Nvidias. AMD is the only credible competition for Nvidia, but buying Intel only weakens the only credible competition, and makes Nvidias dominant position stronger.",Positive
AMD,"Do you seriously think NVidia is not going to raise prices at all?  At this point it is a matter of when, not if.  (unless this all comes crashing down shortly, which is unlikely).  We already know they have postponed the 5000 series ""super"" release due to VRAM availability and cost.  (Their 6000 series workstation/server Blackwell GPUs use 96GB of DDR7 each).  They make a lot more money per RTX 6000 Pro Blackwell than a 5090, so they'll prioritize those and hike consumer prices in order to reduce demand there.",Negative
AMD,"AMD (and Nvidia) care about their data center consumers first because that’s where the money and the high margins are. Every wafer allocation and memory module that’s allocated to a gaming GPU is one less that could have gone into a data center GPU. When the AI bubble pops and data center demand plummets, prices will go down across the board.",Neutral
AMD,Nvidia is just [passing the cost](https://www.tomshardware.com/pc-components/gpus/nvidia-reportedly-no-longer-supplying-vram-to-its-gpu-board-partners-in-response-to-memory-crunch-rumor-claims-vendors-will-only-get-the-die-forced-to-source-memory-on-their-own) to board partners instead of being forthright about it.,Neutral
AMD,Not just Radeon look at their CPUs they dont give a two things about us. Their rather deprecated and sell us expensive piles of scrap while making Apple now seem like a value proposition. Truly preposterous times for us with no proper competition and a lack of market interest due to high prices of everything.,Negative
AMD,They supply both.,Neutral
AMD,"Has pretty much always been the case. What was the longest lasting 1000 series card? The 1080ti. The longest lasting 2000 series card? The 2080ti. The longest lasting 3000 series card? The 3090...    Etc. etc.    They're the fastest card in the stack and also tend to have more vram. They've always been the most future proof, but they also cost and arm and a leg.",Neutral
AMD,[I certainly regret not buying ram a couple months ago like I was considering](https://imgur.com/a/iVsxlWV),Neutral
AMD,There are definitely times where that question can be easily and usefully answered,Positive
AMD,"In general hardware has aged like open yoghurt out of the fridge.  Sometimes anomalies happen, it's not the first time.  I remember major price hikes and poor performance increases after the 9800pro ( x800 and x1800 sucked, hd series was little better until the hd4000) nvidia sucked after geforce 4 until the 8000 series.   People who bought gpus during the crypto boom at insane prices got fleeced. People who bought a 3060 ti or 3080 right before got lucky, people  who waited for a 4070super also got a better deal.  By the time the prices have started rising its too late.  If a gpu is available at msrp at launch you can rarely find better value later in the gen as prices dont drop anymore until the next gen. So yeah either buy at launch and msrp or wait for the next cycle   Imo if you are on a complete potato gpu now and need a cheap upgrade then second hand 3060ti are very cheap (190 euros) , else a 5070ti is the only thing id consider to have any performance/price right now. Everything else is either way more expensive for little gain, or has too little vram for the price. Or is too small an upgrade for the money over a cheap second hand 3060ti, or has poor second hand pricing.   5060 would be more interesting if the base model came with 12 GB and was priced at 250",Negative
AMD,"I remember 2 to 4 weeks before the 9070xt launch people were telling others to ""just buy a 7900xt"" for $700. I guess they got 4gb of extra VRAM out of it, but 95% will never use it. But I think there is a time to actually just wait, instead of buying $700 product that will be worth $500 in less than a month.",Negative
AMD,"500€? New? God damn thats a steal.  There used to be a general idea for buying hardware, *Just buy it when you need it, there is always the next big thing coming*, which still is in part true, but man with the volatility of the market since Corona Im 100% certain you can time your buys.",Negative
AMD,so you got it for two thirds the price it actually costs?,Neutral
AMD,Gotta redo your skill tree but it can get boring pretty easy without the right setup,Neutral
AMD,r/outside is leaking,Neutral
AMD,Is it? Books are cheaper. Movies are cheaper. TV is cheaper. What’s more expensive?,Neutral
AMD,"Could be a pretty massive difference. Each AIB fighting to source memory on individual smaller contracts instead of NVIDIA just making one big deal for the entire production run, taking advantage of the scale to get it cheaper.",Neutral
AMD,Sure they are expanding GDDR7.  But for servers mostly.  RTX 6000 Pro Blackwell (96GB GDDR7) is a hot server GPU for inference use cases that aren't just the giant LLMs.  It is much more cost efficient than the B200 HBM offerings for all sorts of use cases that don't need to access too much RAM concurrently.  Several NVidia competitor hopefuls have also gone the GDDR route for inference focused AI accelerators.,Positive
AMD,They're already sacrificing margins on making consumer GPUs instead of AI GPUs,Neutral
AMD,"There are rumors that Nvidia will no longer be bundling VRAM with their GPUs in the face of rising memory prices, and AIBs will have to source it on their own.  While Nvidia may be ""absorbing costs"" on their FE models which have very limited production, for the vast majority of consumer graphics cards they're going to be passing on costs to the AIBs and thus consumers.   They have absolutely no incentive to absorb costs, anyway. Their entire consumer gaming graphics division is already being run at a massive opportunity cost to their real money maker, datacenter GPUs. It's frankly a miracle that they haven't decided to entirely scrap it yet when the same dies they're selling to consumers for a few grand at most can be sold to AI datacenters for tens of thousands.",Neutral
AMD,"\> absorb the costs if they desire  What makes you think they desire? They have a massive fraction of the market, and from a profit/investor POV raising prices is win/win here unless it somehow risks significant market share loss.",Neutral
AMD,"Agreed.  I’m just sort of surprised they would absorb the cost at all, tbh.",Negative
AMD,>Nvidia has much better margins on gaming GPUs so they can absolutely absorb the costs if they desire.  Which won't happen. Nvidia doesn't want to be a charity.,Neutral
AMD,"True, and I recommend people get cards based on their needs/desires, never blanket recommend 1 company over the other.  I’m just surprised it’s AMD who might raise prices before Nvidia, even if they can absorb some of the cost.",Neutral
AMD,That's only a rumor and has not been confirmed.,Neutral
AMD,"Why are AMD's margins slimmer?  They're manufacturing at the same place using very similar technology. Their equivalent product is actually a little smaller than Nvidia's in terms of die size... they're a big TSMC customer, while obviously not as big as Nvidia they're not some small timer who can't negotiate a favourable deal either.  On this basis I would assume AMD and Nvidia have very similar margins on GPU sales to board partners.",Neutral
AMD,then why are AMD cards more expensive?,Negative
AMD,Still seeing $579.,Neutral
AMD,"When you've been there for 20 years and kept losing market share in the last decade continuously, you are in deep problem. But for AMD, right now they prefer being the SOC for gaming and have little interest in putting effort in dGPUs.",Negative
AMD,"I doubt Nvidia will raise prices they will most likely just stop producing FE""s. Which isn't the same as eating the cost or raising it.   Then AIB's will most likely raise pricing. That isn't due to Nvidia thats AIB's doing what they gotta do.   As far as priority gaming has never been the priority for Nvidia this generation. There priority is AI chips. Everything else is secondary to that.   AMD CPU division is basically printing money now. They want there GPU division to go anywhere. There going to need to be more competitive, right now there not.   They don't offer a better software stack, Ray tracing or anything to match the 5080 in performance. They also have a reputation of dropping support a LOT sooner then Nvidia  That means they need to become a value leader and why in the world would you pay the same or more for a 9700xt over a 5070 Ti?   Specially if the two are with in 50 dollars of each other or even 100 dollars of each other.",Negative
AMD,"Whelp if that rumors become the truth, then it is definitely right to expect the Nvidia GPUs will also get a price increase, i just wonder when it will happen and why AMD did it first with their RDNA 4...",Neutral
AMD,"Usually it's best not to buy during the post-launch rush when prices are way inflated, unless one can get one of the few actual MSRP offers.  Other than that, I totally agree that trying to predict the future is largely futile. Especially now that generational improvements have become so slow and low. Hardware holds its value for longer than ever before and the risk of buying a product only to see it depreciate it soon after is therefore also low.  People who bought any decently reviewed GPU at MSRP since the RTX 20-series have gotten pretty good value by now.",Negative
AMD,"9070 xt shouldn't be too bad either, considering it can be had for less than $/€600. Unless you're a big fan of AI upscaling.  Waiting for delivery now, I guess the shop has a bit of backlog after BF/CM.",Positive
AMD,Either way the days of the affordable 200-300$ value king are long gone.  Anomalies like the Nvidia 8800GT or the 1080 Ti will never happen ever again.,Negative
AMD,>95% will never use it  And the other 5% could use 4 or 100? Or is the XTX too cheap for that?,Negative
AMD,"Sure, but how often can you predict something will drop that much? When the 5090 was announced didn't people sell their 4090s for as much as they bought them or slightly more? Also, if I'm going to use a GPU for 3 months, I'm not going to wait for it to drop in price.",Neutral
AMD,"I like to buy every other year now, while my current GPU still holds some value.   GTX 970 -> 980 Ti -> 1080 -> RTX 2070 Super -> 3080 -> RX 9070 XT",Positive
AMD,Brand new in the EU I’ve seen them go brand new for as little as €609 actually. He had it listed for €540 but I managed to haggle it down to €500. So roughly €100-€110 off MSRP?,Neutral
AMD,and proper setups cost quite a bit too.,Neutral
AMD,"Books are cheaper? as an avid reader i would say that in terms of enjoyment hours/dollar books are more expensive unless you stick to library and flea sales. Movies are terrible value. 2 hours per movie ticket price is nowhere even close to gaming value proposition. TV shows, i suppose if you got netflix subscirption and watch it a lot its competetive.  As for more expensive, look up how much good fishing gear costs, let alone if you want a fishing boat. Treking gear also costs quite a lot if you want to keep doing it and it not to fall apart after first use or be so uncomfortable you wont be doing that as a hobby anymore. Plenty of expensive hobbies around.",Negative
AMD,"Also means there's probably going to be a lot more variability in performance from vendor to vendor than we're used to, due to the sourcing of VRAM potentially being from different places.",Neutral
AMD,They have 92% of the gaming sector. It's easy money and a failsafe if the AI bubble goes bust.,Neutral
AMD,Nvidia is also likely in a position where can get better prices on memory than AMD. So they might not be facing reduced margins yet.,Neutral
AMD,Im surprised that people even consider AMD its not priced well against Nvidia and Intel is coming hot on its heels. Their attitude towards consumers is frankly appalling considering that they are just fighting for scraps in this space,Negative
AMD,"Why is it ok to talk about AMD rumors but not NVidia ones?  The seemingly official quote in this article is clearly at least partially wrong:  It states no new products through 2027.  Do you really believe AMD won't be producing any new GPU SKUs for over 2 years?  I think in both cases these rumors are likely true, but may be a bit off given that both are sloppy third hand leaks without any confirmation from the source or quality multiple sources.",Negative
AMD,A 9070 XT's die is almost as large as a 5080 but are selling it for far less. Also nvidia is doubt able to just get a better price on components.,Neutral
AMD,"They have far lower market share, they most likely price their stuff with a lower margin. Nvidia can do whatever they want.",Neutral
AMD,"Nvidia has a 378 mm2 GPU that has a MSRP of $1000 and $750, while AMD for the same die size has $550 and $600 MSRP. In this case, nvidia makes between 50-80% more $$$ per die.  Nvidia has a 260 mm2 GPU has that a MSRP of $550, while AMD has a 200 mm2 for $300 and $350.  And so on....  A wafer of chips produced at TSMC generates nvidia much more money that AMD's one, even though the prices per wafer must be similar.",Neutral
AMD,"The 9070 XT is priced closer to an RTX 5070 right now, performs like a 5070 Ti but is much closer in terms of hardware to an RTX 5080. So while AMD does use similar manufacturing, they have to price lower than Nvidia for roughly the same hardware (similar GPU die size, 256-bit VRAM bus with 16GB of VRAM).",Neutral
AMD,RDN4 is on a more expensive and smaller node.,Neutral
AMD,"Their market share just went up, not down. But it's not where it should be, I'm just happy they have success in the DIY/enthusiast market, since that's where I am. I don't care much about the OEM or laptop markets.",Positive
AMD,Nvidia has about $50 billion more cash on hand to stretch it out and not be a first mover if they wish.,Neutral
AMD,"Apple has insanely good CPU hardware. But they have zero value, because it comes tied to a mac, so thats an automatic nonstarter for most.",Negative
AMD,"Macbook Air M4 is $750 most of the time now and there's no Windows laptop at the price that can compete on build quality, battery and experience, if you don't plan on doing any laptop gaming.",Negative
AMD,Unless the AI bubble pops and chipmakers find themselves sitting on a huge pile of chips that they can't sell.,Negative
AMD,"The usual trick is to list it as a business expense and claim the VAT back, but it wouldn't knock more than 5% off the price, else there'd be no profit for the seller.  It was either used or fallen off a truck.  Legit brand new was €589 in Germany around BF and CM.",Neutral
AMD,>It's easy money and a failsafe ~~if~~ when the AI bubble goes bust.  FTFY,Neutral
AMD,Google’s TPUs are already showing a lot of promise and its been used to train the best AI model currently,Positive
AMD,I switched to AMD mainly because of Nvidias shit drivers on Linux. Which is funny because I had previously switched to Nvidia because of AMDs shit drivers on Windows.,Negative
AMD,"Not everyone lives in the USA.  In my country it's priced competitively with a 150€ price difference to the 5070 Ti.  If you don't care about the RT difference and CUDA, going with AMD is a no brainer",Neutral
AMD,"The 5080 die is the same one as the 5070 ti, which is the direct competitor of the 9070xt and sells more than the 5080. Average margins will be slightly higher for Nvidia, but not by much. Especially considering AMD uses GDDR6.",Neutral
AMD,GDDR6 and GDDR7 do not have the same cost.,Neutral
AMD,"Ah, I was comparing 9070XT to 5070Ti.  They're all remarkably small dies but I guess that's progress.",Neutral
AMD,"Certainly NVidia has notably higher margins here, but you're likely over estimating it somewhat.  GDDR6 and GDDR7 do not have the same cost.   The overall board cost is also influenced by cooling and complexity of the board traces / power circuitry (though, not by that much).    Additionally, Its not clear how much of the actual sale (vs MSRP) NVIdia earns vs AMD -- there are various people making profit between them and the consumer.  We also know that NVidia sells the vast majority of its gaming GPUs not directly to consumers but to OEMs for pre-builds.  We don't know if NVidia sells GPUs to those OEMs at the same price they sell to D2C retail brands.  Do the big OEM system builders (NVidia's largest customers) get bulk discounts others do not?",Neutral
AMD,Oh they're about £100 or more different in price where I am.,Neutral
AMD,Yearly it went down.,Neutral
AMD,"I wouldn’t put it past people doing that but there are plenty of used 9070 XT’s in Europe selling for under €550 on some sites, just seen one sell for €530 a few hours ago and another for €550. Not uncommon.",Neutral
AMD,outside of US AMD is way too expensive to even consider for a GPU.,Negative
AMD,I dont also live in the USA. 150 pricce difference with no guarantee when they will cut first day game driver updates. Please id rather spend that extra 150 and get better support lol.,Neutral
AMD,">GDDR6 and GDDR7 do not have the same cost. The overall board cost is also influenced by cooling and complexity of the board traces / power circuitry (though, not by that much).  Hope you're not trying to say that GDDR7 is responsible for the increase of $400 of board costs. Come on! A little research showed that there's just a difference of $2-3 per GB, which comes around **$30-50 for 16GB**. Please don't try to say the PCB circuitry is $300 because PCB's is one of the cheapest components of a GPU.  LE: You say the costs are influenced by cooling. FYI, the TDP increase for 5080 vs 9070 is just 45W (350W vs 304W). That's $10 **max** in cooler costs if you've been closely following the BOM cots for the industry.",Neutral
AMD,"That doesn't mean AMD isn't selling well, it can mean that Nvidia happens to sell even better. But mostly in the OEM and laptop market, not the DIY and enthusiast market that interest us.  The 9070 XT is the best selling video card on Amazon in the US and in Europe. And the best selling card on Mindfactory and various price aggregators. So it's actually looking pretty good for AMD, the 9000 series is a pretty big success.",Positive
AMD,I literally listed an European country where it is cheaper and the other guy did too while being way cheaper,Neutral
AMD,Cheapest RX 9070 XT in my Amazon (Ireland) is 702eur  Cheapest RTX 5070 Ti is 963eur (+261eur),Neutral
AMD,Not sure how that can be considering neither AMD nor Nvidia has publicized their contracts and we know they arent buying GDDR off the market,Neutral
AMD,Either your country is an anomaly in pricing or you got things wrong.,Negative
AMD,"Tip: some German shops ship to Ireland.  [https://www.alternate.de/listing.xhtml?q=\*&filterCategoryPath=PC-Komponenten%2FGrafikkarten%2FAMD+Grafikkarten%2FRX+9070+XT&s=price\_asc](https://www.alternate.de/listing.xhtml?q=*&filterCategoryPath=PC-Komponenten%2FGrafikkarten%2FAMD+Grafikkarten%2FRX+9070+XT&s=price_asc)  Add €18.90 for shipping and maybe €20 for VAT adjustment, it's still €628. Quick, it's almost sold out.",Neutral
AMD,"No, my country is normal like a lot of others",Neutral
AMD,"That listing is gone, so now the cheapest from there is    €634 ->**€655.31 + p&p**",Neutral
AMD,Check other retailers. There are multiple storss in Germany selling 9070XT's for 600€,Neutral
AMD,"Bad luck, it was there an hour ago.  Here's this one, but they'd deliver in January  [https://www.amazon.de/dp/B0DTT7CPWV?linkCode=xm2&camp=2025&creative=165953&smid=A3JWKAKR8XB7XF&creativeASIN=B0DTT7CPWV&tag=geizhalspre03-21&language=en\_US](https://www.amazon.de/dp/B0DTT7CPWV?linkCode=xm2&camp=2025&creative=165953&smid=A3JWKAKR8XB7XF&creativeASIN=B0DTT7CPWV&tag=geizhalspre03-21&language=en_US)",Neutral
AMD,"I like that they used the term ""Machine Learning"" instead the infamous two letter acronym, starting with ""A""",Neutral
AMD,"Now that you mention it, I also observe that in their RDNA slides they barely mentioned AI at all, just 1 or 2 slides and that's it: https://www.techpowerup.com/review/amd-radeon-rx-9070-series-technical-deep-dive/  But their Ryzen presentation is complete other way around, just AI AI AI all the way.",Neutral
AMD,A.R. may be a little gimicky but I wouldn't call it infamous.,Neutral
AMD,No reason to buy a new cpu when you have to sell a kidney for ddr5 ram,Negative
AMD,I've been holding onto my used ddr4 for no reason for the last 2 years. Turns out maybe being lazy and waiting actually benefited me.,Negative
AMD,If the demand for these in data centers was actually there wouldn't the prices have raised already?,Neutral
AMD,I don't believe they'll keep that promise because their sales will drop when the memory situation impacts all their sales channels.,Negative
AMD,Can we just use banks of CPUs as RAM?   It would only take 333 copies of a 7800x3d to make its 3D cache add up to 32GB!,Neutral
AMD,Honestly just relieved AMD didn’t bump the prices. I can finally upgrade without crying.,Positive
AMD,"None of this is helping anyone. All it's proving is that customers must buy on initial release date, as the future could skew product pricing at a later time. Which doesn't help anyone and encourages scalping even more.",Negative
AMD,"They never were going to. I told you so. The source that said they would, also said AMD is not going to release a new GPU until 2028.",Negative
AMD,"Hello kikimaru024! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,"Good to see AMD hasn't forgotten its roots. Also, Zen6 is probably going to come in higher",Positive
AMD,they better not. if they get greedy and start raising prices just because they can then I'm done with them.,Negative
AMD,I was born with two for a reason.,Neutral
AMD,\+300 euros for a 32 GB Ram kit.  Absolutely insane,Negative
AMD,I'm so glad I bought 64gb of ddr5 for £170 right the price hike.,Positive
AMD,Unless you have socket compatibility... Intel better get with the program here.,Neutral
AMD,"Sure DDR5 went up, but is a couple hundred more really gonna make or break say a $1500 computer that now costs $1700?  I think people are way overthinking this or worrying way too much about it.",Negative
AMD,For the past 1.5y I couldnt get rid of my two Aliexpress Corsair 16gb sticks that i bought for $30 a piece but were not a kit.   Nobody wanted single 16gb sticks - they all wanted 32gb kits and who can blame them when they were as low as $79.  Now I got $230 in total for them.  Crazy world.,Negative
AMD,I built a media server around my old ddr4 and boy am I fucking glad I did,Positive
AMD,sir this is a casino,Neutral
AMD,"yes, but it's also about manufacturing capacity. Data centers aren't using consumer grade ram, but they have booked nearly all the capacity at the fabs that manufacture consumer grade ram.",Neutral
AMD,"The DRAM cartel sees the bubble coming and wants to make huge profits by making the bubble bigger. They are going to scam these companies (other than OpenAI) by refusing to increase production and charging insane prices even though they could easily (and were originally planning) to raise production and keep prices reasonable. That was when these scumbag ""AI"" companies weren't trying to corner the market to screw over their competitors  OpenAI's Sam Altman is the one who started all of this. If he can buy up 40% of the DRAM supply he thinks he can stop his competition from being able to source ram at reasonable prices. The fact that everyone else on earth gets fucked over doesn't matter to him at all. The DRAM manufacturers are playing along because they'll get insane profits. When the bubble finally bursts they will go back to normal operations and prices will crash but they think they'll walk away rich first",Negative
AMD,"There's already separate CPUs specifically for HPC / HBM / whatever feature is required for server style ai computing, so idk why there's worry for regular CPUs, especially when they can be very inefficient for server computations.",Neutral
AMD,That is the good news about chiplets. Server CPUs will just absorb the slack.,Positive
AMD,Even less if you use 9950X3D2's. Probably cheaper too.,Neutral
AMD,"I bought a 7800X3D (tray version), 2 years after launch for 50% of MSRP so not sure what your argument is?",Neutral
AMD,People have needed to buy on release date for 7+ years now. People got ass mad when I was saying those about the 2080 Ti because the premium models always have more limited availability.   The best time to buy is almost always at launch now.,Neutral
AMD,Buy on initial release date? When companies release said like raptor lake? No thanks.,Negative
AMD,"I think right now you buy a ""justifiable"" MSRP day 1 or wait for the end of the line platform to build and buy around.",Neutral
AMD,"Indeed, false reporting of price increases that don't exist do not help anyone.",Negative
AMD,"Roots? You people have the memory of a goldfish.  AMD does the same shit as everyone else, charge the most the market will bear.",Negative
AMD,"It's not about roots. The PC market makes up a huge portion of both Intel and AMD's sales in a way that it doesn't for Nvidia, Micron, or Samsung.",Neutral
AMD,"They happily would if the demand were there. We've seen that across their GPUs.. they're not being benevolent, they're just spinning.",Neutral
AMD,"I seriously hate this argument.  If you had a chance to get a 200%+ higher paying job, would you? Hypocrites.",Negative
AMD,Roots of charging 300+ dollars for a ryzen 5 with 6 cores since zen 3?,Neutral
AMD,"So you were done with them since 2004, right?",Neutral
AMD,How many ram sticks you would like to buy? /s,Neutral
AMD,"Intel doesn’t change RAM with every socket either, so socket compatibility isn’t really required to keep your RAM",Neutral
AMD,A couple hundred? The exact kit I'm using now cost a little under $300. Now it's $1100.  Don't downplay this shit.,Negative
AMD,"I had to settle for 32GB DDR5 at the start of November at $259 AUD, that same kit today is $739 AUD.  At todays prices, RAM alone would have cost more than the rest of my build and I wouldn't have been able to justify it.",Negative
AMD,Not everbody got rich parents bro,Neutral
AMD,you're talking to the same people who contemplate whether it's a good idea to save $50 by not buying an nvidia gpu,Negative
AMD,Same here for my homelab.. 64gb for ~£70. Wild to think how much money I have in ram just in my room alone,Neutral
AMD,"Isn't the chip is the same ?   They have one more for ECC (on DIMM, not the on chip bullshit) and a controller (maybe for RDIMM ? )",Neutral
AMD,"yea they are scooping up all the LPDDR too, we are so fucked lol",Negative
AMD,"Problem as with the Crypto it takes longer to prices to go down than prices to shoot up. I mean it took ages for GPU pricing to get back to ""normal"" and arguably never did. RTX 40 had a huge markup with the 4080 being a 3070 successor with a massive markup.",Negative
AMD,"I mean, can you say that isn't the right call.  if everyone knows its a bubble, why would you start making new capacity that in a few years will at best ride the end of the bubble, if not completely burst by then.  so yeah, doubly smart",Neutral
AMD,"Fabs take 3 to 5 years to make     They arent increaseing production becose they cant without the wait, they bet that the buble would pop before they could payoff their new fabs",Negative
AMD,"There's an insane doomerism bandwagon going on over hw spaces on  reddit, for a while now. From bad reporting to bad reading skills, people are doubling down on the worst possible interpretation and outcomes as guaranteed truth.",Negative
AMD,* RTX 50 series are cheaper now than at launch * DDR5 was much cheaper around mid-2023 to mid-2025 than it was at launch in 2021,Neutral
AMD,"I genuinely can’t believe the conclusions (more like delusions) people come to on this subreddit, like AMD has been the good guy any time in the past several years.",Negative
AMD,Their price increases are due to the memory increases.,Neutral
AMD,"> If you had a chance to get a 200%+ higher paying job, would you?  depends on a job. I could do that right now, but i know too many people in that job that end up with permanent disability before 40 from job requirements.",Neutral
AMD,"The argument I'm making is that instead of raising prices, AMD is not cutting them and Zen 6 will come in higher.   Not sure what you're rambling about",Neutral
AMD,"More like [$195](https://pcpartpicker.com/product/4r4Zxr/amd-ryzen-5-9600x-39-ghz-6-core-processor-100-100001405wof), the launch prices are just inflated.",Neutral
AMD,1 kidney for 1 ram is only fair,Neutral
AMD,I'd settle for a chip.,Neutral
AMD,Rich kids got not sense for prices under a couple grands; therefore they can't understand the problem since their parents will buy them everything to make them shut up,Negative
AMD,the exact kit you are using is irrelevant. A typical 32GB of DDR5 can be got for 300 dollars now. Only a few extremists need the best frequencies and timings.,Neutral
AMD,You can definitely find a kit under 500,Positive
AMD,"You don't need to look at the exact same kit...  Look for a current kit with similar specs...  Most people don't need more than 32GB (and even that's more than most really need), and that was $100 and now it's about $300 for a similar kit.",Neutral
AMD,a 300 dollar kit im assuming is 96gb  Most people dont need that much   Motherboard prices are coming down a lot to offset some of the increase in ram price,Neutral
AMD,Are these the same people who spend a bunch extra on RGB lighting and noctua or maglev fans then too? lol,Negative
AMD,I suddenly feel rich with my 7900XT.,Neutral
AMD,"For server DDR5 yes, for HBM no.",Neutral
AMD,"I admit that I don't know the difference between ECC and non ECC DDR5 memory chips.    I assumed that it was the case since that's how it goes over at TSMC for logic chips. I don't have time to look into it now. If that assumption was wrong, then I look dumb and I'll ""take the L"".",Negative
AMD,Part of that markup is from the die area getting more expensive. The other part is Nvidia further increasing margins and implementing more expensive memory for what is arguably a net negative. The extra bandwidth or some ten watts of power saving does basically nothing on most of the stack and instead you get gimped amounts of memory at higher cost.,Negative
AMD,"OTOH, SSD prices did take a sharp dive for a glorious moment.",Neutral
AMD,new nodes are more expensive than old nodes nowadays. So prices for GPUs will keep increasing just from basic manufacturing costs.,Negative
AMD,"Its not just HW spaces, reddit is very doomer outlook in general.",Negative
AMD,"""Forgotten its roots.""",Neutral
AMD,Dual channel and dialysis it is then.,Neutral
AMD,Under $500 is still absurd for 64 GB of DDR5.,Negative
AMD,Even that's a three fold increase. You get low end 32 GB kits for $300 whereas for the same price I got a Corsair Dominator Titanium 64 GB kit.  The market is fucked and it's entirely the fault of the corporate AI fad.,Negative
AMD,Nope it's a 64 GB Corsair Dominator Titanium kit.,Neutral
AMD,My 64GB kit went from $135 at the start of this year to $780 now.  https://old.reddit.com/r/buildapcsales/comments/1i21ta2/ram_patriot_memory_viper_venom_64gb_2_x_32gb/  https://www.newegg.com/patriot-memory-viper-venom-64gb-ddr5-6400-cas-latency-cl32-desktop-memory-matte-black/p/N82E16820225335?Item=N82E16820225335,Neutral
AMD,Fun fact: RGB fans are so popular now that they are usually cheaper than non-RGB ones. So you can buy the cheaper RGB variant and just turn the lights off.,Positive
AMD,"No ""L"" to be taken, it's greed from manufacturer any way \^\^",Negative
AMD,"True but AFAIK Nvidia could've sold a full AD103 die with a cheaper board and cooler at 300W priced at 800 USD. But Nvidia slipped a pricey cooler, board, pushed wattage and charged 400 more for that.  Crypto IMV really did a number because they thought they could get away with it. Ofc the OG 4080 sold poorly and never sold out.",Negative
AMD,"Yeah, 3x increase for 1 part, one of the cheaper parts, making a $1500 PC using a typical 32GB kit be about 13% more expensive now than before. Hardly breaking the bank IMO.  I am curious how much difference in spec this kit for example is vs the one you got for $300 for 64GB before.  https://www.amazon.com/gp/product/B0BPHSVVS5/ref=ox_sc_act_title_1?smid=AWD7GDDT7Q2ZT&th=1  You are claiming 64GB is $1100, but this is less than half that price for 64GB 6400 CL34 which seems decent enough to me. I wonder what are the specs of your $1100 64GB ram then that it's that much better.",Neutral
AMD,No one is paying 800 dollars for that   The ask price is not the true market price  There are klevv sticks on amazon that is more reasonable,Negative
AMD,im seeing equivalent specs for under 600 now.,Neutral
AMD,"And yet despite selling poorly as you claim, the 4080 sold more than the entire RDNA3 lineup combined. So i guess it offered something enough people wanted.",Positive
AMD,"This is just the start of the price hikes as well. It's going to keep getting worse until at least 2028.  Plus it's not just system RAM. It's going to affect VRAM too, as well as consoles, phones etc.",Negative
AMD,"Redstone bundles four technologies:  Ray Regeneration (New): An AI tool that cleans up ray-traced visuals for better reflections and lighting.  Frame Generation (Updated): Now uses AI (machine learning) instead of old algorithms to create smoother fake frames.  Radiance Caching (Future): An upcoming AI lighting feature (slated for 2026).  Upscaling (Old): The standard FSR upscaling tech, just renamed and bundled in.",Neutral
AMD,Where’s the critics on fake frame and on [AMD’s misleading graph](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-5_videocardz.jpg) GN?,Neutral
AMD,"Weird, almost no mention of evil terrible industry destroying fake frames, guess framegen is ok now since Redstone released?",Negative
AMD,"Isn't he legally obliged to call them ""fake frames"" as he always does?",Neutral
AMD,No performance cost benchmark ?,Neutral
AMD,Not fake frames this time huh,Negative
AMD,"Steve, why no fake frames ?",Negative
AMD,this is [all you need to know](https://i.imgur.com/E845mYA.png) about this interpolation gimmick,Neutral
AMD,"I haven't been in r/hardware for a long time, the amount of hate and low quality comments make it seem like a bot subreddit. But I do know that you're not bots, you're not that clever.       ""wHy NoT fAkE fRaMeS SteVe?""   Go Jump.",Negative
AMD,"What is a shame is that they went from a numeral to ""Redstone"". So whenever there is another update AMD will have to come up with anouther name or go back to numerals again.   Also, game toggles for FSR will be more confusing. Should a newbie look for FSR Redstone or FSR 3.1 in the settings menu? FSR 4.1 would have been the absolute best option as it aligns with RDNA 4. Next generation of features would align perfectly with the architecture RDNA 5 = FSR 5. My gut feeling tells me that it is 50-50 if we will keep Redstone around as a naming scheme for the next generation.",Negative
AMD,Glazing AMD gets more clicks I guess,Neutral
AMD,"how dare you say a negative thing about tech jesus!! since its not a green logo it means the frame is not ""fake""",Negative
AMD,"New tech gets invented, get attacked by tech outlets until it becomes ubiquitous, then they move on and accept it.",Neutral
AMD,"He's called the whole lot ""frame by fake frame"" at 25:33.",Neutral
AMD,"Maybe it’s the YouTube algorithm at play but most of the gaming related hardware review orgs don’t seem to actually care much about technology. Everything that doesn’t conform to the status quo is treated as if it’s some sort of scam. Frame generation, high quality upscaling, even the concept of ray traced lighting.    Perhaps it’s just the difference between “consumer value” gaming outlets and outlets dedicated more towards enthusiast technology.",Negative
AMD,That's only for Nvidia.,Neutral
AMD,No they're real now,Neutral
AMD,"Ah, but when AMD does it...",Neutral
AMD,Why? The average consumer base has embraced frame generation and if that's the case then AMD is entitled to gain the advantages from what Nvidia has established.,Neutral
AMD,25:33,Neutral
AMD,"Because for somebody who acts constantly ""holier than you"", says they stand behind journalistic practices and is constantly criticising companies, he sure as fuck loves not actually approaching most of the stuff like a proper journalist would and is actually highly biased.  He presents himself as one thing, so he gets judged as such, and because his words are often empty, he gets criticised a lot.",Negative
AMD,"And you really, geniunely don't see a problem with his presentation of that topic?  He shitted on Nvidia for their framegen and their misleading titles literally THREE DAYS AGO, and even in a video not about NVidia specifically. Here he just plays into completely opposite direction, with ONLY thing even remotely comparable being him saying ""but they didn't mention image quality and latency on slides"".  He even completely ignores AMD ""performance"" slides with FG in them, slides included in Redstone promo materials he is reviewing. Something he is bringing about NVidia a year after they did it and in completely unrelated videos.",Negative
AMD,You posted here 6 days ago 😂,Neutral
AMD,If Steve is going to bash Nvidia for their frame generation then he should do it for AMD.   Don't think we need a half dozen comments about it though.,Negative
AMD,"Yep, some people blatantly rush to post things first to set the narrative of a topic too",Neutral
AMD,"It's extra funny because he does call them fake frames, just nobody actually watched the video...",Neutral
AMD,of course it does reddit has proven that.   AMD can do no wrong while Nvidia can do no right.   It doesn't matter there both companies with no moral right or wrong. Only whats in there best interest.,Negative
AMD,"Sure, except GNs last fake frame whinging video was 3 days ago.",Neutral
AMD,Aha so it gets accepted the moment Nvidia isn't the only one?,Neutral
AMD,no you forgot the part where they portray themselves as know it all genius and that they supported the amazing tech from day 1,Positive
AMD,There is a difference when frothing at the mouth from shouting fake frames and meekly mentioning it once at the end of the video.,Neutral
AMD,"In the case of frame generation, people call them fake frames because they are. Frame generation is cool, but it is a *motion smoothing* technology. It is *not* equivalent to increasing the base frame rate.  With Nvidia trying to lie about GPU performance by putting MFG frame rates on the same graph as non-framegen frame rates, calling the data fake is only fair.",Negative
AMD,"> Everything that doesn’t conform to the status quo is treated as if it’s some sort of scam. Frame generation, high quality upscaling, even the concept of ray traced lighting.   Personally I just don't give a fuck about interpolation, like I didn't give a fuck about 180MP phone cameras or 1000hz TVs. I'm interested in raw performance, of which none of these technologies are more than a gimmick, or a facade of performance (""percieved smoothness"")",Negative
AMD,"Seriously, man is biased and doesn't see it, then goes out and calls out others for bias which usually doesn't exist, and if it does is actually way smaller bias than his.",Negative
AMD,He put out a video 3 days ago calling it Fake Frames,Neutral
AMD,Why are people down voting this?,Negative
AMD,"> He even **completely** ignores AMD ""performance"" slides with FG in them, slides included in  https://i.imgur.com/kHtgW3d.png",Negative
AMD,Did he not call them fake frames at 25:33? Or am I having a **whoosh** moment?,Negative
AMD,"nvidia is a trillion dollar company, think about that  give amd a break",Neutral
AMD,"He calls them that once, when there is less than a minute left of the almost 30 minute long video. Meanwhile when Nvidia does it he puts in in all caps in the thumbnail and title of the video, and mentions it over and over all throughout the video. Hell, in the video he released just a few days ago literally the first thing he said was ""fake frames"".     Surely you have to agree that it gets treated differently. You can't just point to him mentioning it once at the end and go ""well there you go, he treats them exactly the same in both cases"".",Negative
AMD,"Idk man, they're absolutely roasting AMD over on the radeon sub. Lot of RDNA3 peeps saying their next GPU will be Nvidia. They were expecting something about fsr4 on RDNA3 and got nothing sadly.",Negative
AMD,"How strange, guess we're in vastly different timezones  https://i.imgur.com/kHtgW3d.png",Neutral
AMD,"Was that way with upscaling, was that way with RT at usable performance, was that way with original FG, will be that way with ML denoisers, MFG, neutral shaders ect ect ect",Neutral
AMD,Usually,Neutral
AMD,I agree with calling out the marketing bullshit but most tech YT channels hate the very existence of anything that’s not 2010 level tech wise,Negative
AMD,If my 4x4 matrices take 48 scalar multiples to compute instead of 49 these are FAKE MATRICES!,Negative
AMD,reddit hivemind got you 😔,Neutral
AMD,25:30?,Neutral
AMD,He called them fake frames in the conclusion of this video too.,Negative
AMD,I have no idea. I tend to ignore the karma system as a whole lol.,Negative
AMD,"Do you honestly not see the difference between putting ""FAKE FRAMES"" in the thumbnail and title, and constantly saying it over and over in multiple videos, vs saying it once when it's less than a minute left in the video?  Surely you have to agree that there is a difference.",Negative
AMD,Do not dare touch my billion dollar corporation ahhhhhh comment,Negative
AMD,And AMD is 70 billion dollar company. They are not poor struggling ma&pa shop on the corner. They are multibillion corporation. And they do exactly same shady shit as NVidia.  WHY THE FUCK should we give AMD a break?,Negative
AMD,"AMD, the small fledgling hardware manufacturer lmao.   You sound like one of those people who whitewash AM4.",Negative
AMD,Don't people just skip to the end on these videos anyway?,Neutral
AMD,Tbf they absolutely got shafted by getting RDNA3,Neutral
AMD,"Wasn't the one commenting on whether or not this video had any complaints, just correcting the guy saying its now ubiquitous and uncontroversial so GN doesn't care about fake frames anymore. Whether or not GN whinges about things more in regards to NVIDIA or AMD I do not care.",Negative
AMD,"Yet when it's Nvidia he just keeps constantly mentioning it, but when it's AMD it's once at the end of the video lol.  Dude is biased as fuck.",Negative
AMD,You forgot about VRR. It was the most useless thing until amd brought their version that was literally unusable in every monitor for years.,Neutral
AMD,"It's not the YT channels, they don't say what they believe they say what they think their audience wants to hear.  When the audience is a large amount of people on older hardware, they want to hear that they're not missing out on anything.",Negative
AMD,"I disagree. At least for the more mainstream channels, they have been fairly consistent in praising the potential of new technologies, while criticizing sub-par implementations of said technologies which offer little to no benefit to the consumer.   For instance, DLSS 1 and FSR 1 were (rightly) criticized for being rather garbage, while the concept of temporal upsampling was at least accepted, especially once DLSS 3 proved it could be done well.   Similarly for RT, early implementations were criticized for lackluster visual improvements while tanking FPS, but I can't recall any of the big channels who seriously claimed that the underlying technology (hardware-accelerated RT) wasn't very useful.",Negative
AMD,"Nah not at all I've been chasing frames since the days of drawing on GPUs with a pencil. At no point have I ever thought ""I'd like my game to be a third less responsive so my graphics are prettier""  Plus this is a cycle I've seen repeat in the tech space for nearly 30 years since I first bought a USB scanner. ""Wow the 3600DPI one is only 20% more expensive than the 600DPI one?"" yeah cos it's a 600DPI sensor interpolted.  ""Wow this 1000hz TV is only $200 more than the 120hz one?"" yeah because it's a 60hz panel interpolated  ""Holy shit this chinese noname manufacturer has a 180MP camera!"" yeah because it's a shit 16MP CCD interpolated  ""God damn this GPU can run gobbler 3000 at 4k120? Insane when all other benchmarks show 45FPS"" yah cos it's 45FPS interpolated up to 120.  All you AI gamers claiming any pushback against frame gen is 'hivemind' / baseless hating can get in the fucking sea and take your shit 2025 version of ram_doubler.exe with ya",Negative
AMD,There’s a difference between a small sentence about it in passing and putting fake frames on the title and video image,Neutral
AMD,"yeah and I sat through 20 mins of AI tech fluff that I couldn't give less of a fuck about, noted every mention of FG and whether it was neutral or not and every ref to AMDs and I got to 7 within 14 mins (that were neutral at best, being generous) - including referencing on screen their ""AMDs FAKE FRAMES COMPARISON"" at 1 mins or something before thinking why I am even wasting my time bothering with these melts?  https://i.imgur.com/howNEZB.png  > Do you honestly not see the difference between putting ""FAKE FRAMES"" in the thumbnail and title, and constantly saying it over and over in multiple videos, vs saying it once when it's less than a minute left in the video?  Correct yes.   Surely you agree that bolding **completely** in my comment was pointing out the difference between saying ""COMPLETELY"" vs saying 'they barely mention 'fake frames' when it comes to AMD'?  Surely you have to agree",Negative
AMD,"blame nvidia for setting the standards. 5070 is the leading gpu despite fake frames and low vram.  if this is what the consumer base wants, it's not amd's fault for following the market.",Negative
AMD,"Oh right yeah my bad, read that the wrong way round",Negative
AMD,"Trust me I'm an expert in whinging about fake frames, I'd know",Positive
AMD,"With regards to RT its more a critical view of the readiness of the tech, mostly due to performance sacrifices.",Neutral
AMD,oh no i meant the hivemind downvoting you. i completely agree and thank you for the rant,Positive
AMD,"I think it's funny that some of your examples of Steve mentioning ""fake frames"" in this video is him saying Nvidia has fake frames.  You can't highlight a link which says ""NVIDIA DLSS4 Fake Frames tested"" and go ""see? He mentioned fake frames in this video"" as if it's evidence of him calling AMD's frames fake. If anything you are just highlighting more times Steve has called Nvidia's frames fake.",Negative
AMD,"So it's not okay when a company you don't like does it, but it's fine if a company you like follows suit?",Negative
AMD,"Holy hell, amount of doublethink happening here is staggering.  NVidia does shady shit - blame NVidia. AMD does shady shit - blame NVidia.",Negative
AMD,"> if this is what the consumer base wants, it's not amd's fault for following the market.  Similar thoughts re lootboxes and gambling in games I suppose? If it's what the masses want...",Neutral
AMD,GN fans seem to think they're smarter than other hardware enthusiasts but comments like these prove otherwise.,Negative
AMD,"Fucking hell twice in one thread lol this shit enrages me clearly. sorry!  Don't worry about the downvotes tho, I wrote a plugin that inserts 4 upvotes between every real one so it all looks grand from my end",Negative
AMD,Exactly my point if I'd finished my original comment you'd've only found some way to dismiss all the examples of him not glazing AMD.  I'm not even sure what point you lot are getting at but enjoy jumping to nvidias defense I guess? lol,Negative
AMD,>I wrote a plugin that inserts 4 upvotes between every real one  do tell,Neutral
AMD,"I genuinely don't understand what you mean by ""if I'd finished my original comment"".  My point is that there is a very clear difference between how he has commented on ""fake frames"" from Nvidia up until this point, and how he handled it in this video. It is as clear as day that there is a massive difference between how often he has brought up ""fake frames"" in regards to Nvidia, and how he only mentions it once in this video near the end (and maybe has it in the bottom corner once or twice in this video but don't say it out loud).     I am not defending Nvidia in any way and it saddens me that you see my comment that way. Just because I point out inconsistency in reporting from a Youtube channel does not mean I am defending some company. The world is not that black and white and I hope you one day realize that. Surely people should be capable of pointing that kind of thing out without being labeled some kind of ""Nvidia defender"". Have I even said anything positive about Nvidia in my posts? Have I even said anything negative about AMD? The answer to that is no by the way. I have not commented positively or negatively about neither Nvidia nor AMD in this thread. I have only pointed out the inconsistency in Gamers Nexus' reporting style in regards to this particular video and the previous videos.",Negative
AMD,"Well you see you have this value that represents something - let's say the performance of how well my comment's doing in numerical terms.  What I do (cos it's my preference) is I pull my actual upvote count from reddit and then use an AI subroutine to estimate what my upvotes would've been inbetween the actual votes and then my browser rewrites the page display to show me these additional upvotes.  It's currently at 4x upvote gen which is cool - every 20 upvotes I get per hour actually get it shows me 80 on my comment page. Works really well and it saves me the energy, cost and effort of actually making my comments better. Says I'm doing better than people who are getting 30 or 40 actual upvotes per hour.  Future of commenting, I'll tell you that.",Positive
AMD,"Ok I'm less wound up today - here's my train of thought and what I think's happened is me misinterpreting (and thats on me) the initial argument  1) Original comment is from Nickwhatever > ""wHy NoT fAkE fRaMeS SteVe?"" etc. A common thing I've noticed on hardware that winds me up as I'm very much against any sort of frame interpolation in general, and I've been surprised over the past few years how accepting the gaming community / high end gamers / r/hardware users are in general.  2) The detracting reply which I rush read and did the usual idiot thing of picking up on part of it ""Completely"" rather that the gist of the argument the guy, looking back, is stating - GN are harsh on nvidia and _unfairly_ charitable to AMD.  > And you really, geniunely don't see a problem with his presentation of that topic?  > He shitted on Nvidia for their framegen and their misleading titles literally THREE DAYS AGO, and even in a video not about NVidia specifically. Here he just plays into completely opposite direction, with ONLY thing even remotely comparable being him saying ""but they didn't mention image quality and latency on slides"".  > He even completely ignores AMD ""performance"" slides with FG in them, slides included in Redstone promo materials he is reviewing. Something he is bringing about NVidia a year after they did it and in completely unrelated videos.   2a) What I should've done is watched the video from three days ago and pointed out that it's 1) a video specifically about frame gen, and not the release of a batch of AI shite technology - you'd expect the focus (and resulting evaluation / criticism / praise) to be around the topic. In my eyes, I'd expect any negativity around a certain subject to be more prominent on a video specifically about that topic.  2b) what I DID do was immediately jump on the first (what I saw as) quantitive flaw and go ""ha dickhead, he literally refers to AMD's shit as fake frames within a few minutes of the video, thus proving no bias about AMD"" - which tbh still seems the right conclusion in my head, but it's not adressing the point he was making - **nvidia is fake frames 100% of time, AMD is when we feel like it**. Still don't think that's true but that doesn't excuse the shit reasoning on my part  > I genuinely don't understand what you mean by ""if I'd finished my original comment"".  So this was I had a notepad doc open after going ""the fuck? this guy hasn't been generous to AMD at all, in 7 or 8 mins there's been more instances of 'fake frames/shitting on AMD and the tech' (3) than there has been situations where he's pointed out this new FG is an improvment (2 - one during cyberpunk flittering back and forth between normal and rear view, other one I can't remember but it's minor).  I started quantifying that with timestamps etc etc then realised ""Why the fuck am I even bothering? I've had this exact (sorta) debate before and it ends up with the effort you've put into quantifying your point being a waste"". That was the ""if I'd have finished my original comment""... you (nvidia glazers) would've just found a way to dismiss it - I was talking about *my* comment and you referring to the opposition in general.  > I am not defending Nvidia in any way and it saddens me that you see my comment that way.  In a thread of people saying ""Hah, this proved nvidias fake frames ARENT shit, it's just this warlock loves AMD"" it's gonna be taken as that implicitly, but my bad - you were the subject of my ire towards folk who share the same argument as you for different reasons.  Hope that clears it up, I was just on one yesterday for some reason and I really, really fucking hate frame gen.  Any wrath incurred was completely collateral, even if I disagree with your stance so please accept my apologies - a jumped gun argument pointed at what sounds like a gunshot. Hope this assuages your sadness lol",Neutral
AMD,"That is an interesting approach. I will have to check how that can be done, and what needs to be done by me before anything. Thank you.",Positive
AMD,"9950X3D doesn't really make any sense for a gaming PC, just get a 9800X3D.  Any NVMe would offer you the same experience as the 990Pro, get a cheaper one.   $335 is silly for a cooler, a $35 air cooler would work all the same.",Negative
AMD,"Save some money by going with a 9800x3d, same gaming performance",Neutral
AMD,This system wastes a good amount of money.  * The 9800X3D is actually faster for gaming * The cooler is really expensive for the purpose of having a tiny screen * There are cheaper RAM kits on the market. * There are cheaper 9070 XT models,Negative
AMD,"I don't quite agree with that - unless the 9800X3D has much better heat dissipation than the 7800X3D which I currently have with a Noctura NH-D15 - and even that idles at 58-50C. Plus under load, you can definitely hear the fans spinning up and down. That's why I was thinking AIO. I could get another Noctura, but wanted to try something different and potentially more efficient (and quiet).",Negative
AMD,Switching to 9800X3D.,Neutral
AMD,"Gotcha; I'll switch to the 9800X3D. You recommend a Noctura NH-D15 then? That's $140. Question is does the AIO cool better overall? I don't mind spending if it does.  Which RAM kits that are RGB and 6000 CL30 do you recommend?  I don't necessarily need ""cheaper"" on the video card; I want reliable quality with a good name - same with the RAM.",Neutral
AMD,Final update posted in body - thanks a ton for your help! Only thing now... where to get a legit copy of Win 11 pro without giving Mucusoft $200 \*sigh\*,Positive
AMD,"The 9800X3D moved the 3D v-cache to under the compute cores. This puts the compute cores directly on top of the stack, in direct contact with the heat spreader. This makes it easier to keep cool.",Neutral
AMD,"You have a 7800X3D and you want to upgrade already? But why? Upgrading every gen is a waste of time and money.  There's nothing to disagree with. 9800X3D runs under 100W for most gaming workloads, same as 7800X3D. You could run this with a stock cooler if you wanted to.   Idle temps mean absolutely nothing. Your CPU could run at those temps indefinitely, and idle isn't even an established state. Just because you aren't doing anything doesn't mean your PC isnt' doing anything. Max temps under load is what actually matters. Your idle temps also seem just fine.  The fans will spin up and down regardless. AIOs use the same fans, and run according to the same fan curves. In fact, you're adding a pump to the noise equation.  https://gamersnexus.net/cpus/rip-intel-amd-ryzen-7-9800x3d-cpu-review-benchmarks-vs-7800x3d-285k-14900k-more  https://www.tomshardware.com/pc-components/cpus/amd-ryzen-7-9800x3d-review-devastating-gaming-performance/4  Objectively, you're paying hundreds for only the aesthetics with that cooler  Also in case you weren't aware, there are air coolers under $50 that compete with the Noctua NHD15 these days. IE thermalright PA120 or PS120",Negative
AMD,I recommend the Thermalright Phantom Spirit. It will be more than enough cooling for the 9800X3D and costs $36.  This RAM kit is way cheaper and you will never notice the performance difference between CL30 and CL36. https://www.bestbuy.com/product/crucial-pro-overclocking-32gb-2x16gb-ddr5-6000mhz-c36-udimm-desktop-memory-black/JX8PSKC864?cmp=RMX&refdomain=pcpartpicker.com,Neutral
AMD,Ahh nice! Yah the heat of my 7800X3D always bugs me heh. But the system works great. No question.,Positive
AMD,"I'm giving my current system to my wife, and her system is going to a friend (11900k with a 3090RTX). Doing it now because all indications are RAM and GPU prices are going to continue skyrocketing in 2026.  What combination of ""perfect parts for optimal performance and cooling"" would you recommend for a 9800X3D RX9070XT buildout?",Neutral
AMD,Updated build posted in body.,Neutral
AMD,"Actually, there is a performance difference up to 5% depending on what you're doing between CL30 and CL36. I have the money, so I'm just going to stick with the CL30 for now for that ""up to 5%"" difference heh.  Updated build posted in body.",Neutral
AMD,"It's your money. Just know that for a similar budget you can get an RTX 5080, which is approximately 20% faster than the RX 9070 XT. Plus, 64GB of RAM.  [PCPartPicker Part List](https://pcpartpicker.com/list/zDyzDj)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 9800X3D 4.7 GHz 8-Core Processor](https://pcpartpicker.com/product/fPyH99/amd-ryzen-7-9800x3d-47-ghz-8-core-processor-100-1000001084wof) | $444.99 @ Amazon  **CPU Cooler** | [Thermalright Phantom Spirit 120 SE 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/GpbRsY/thermalright-phantom-spirit-120-se-6617-cfm-cpu-cooler-ps120se) | $35.90 @ Amazon  **Motherboard** | [\*MSI MAG B850 TOMAHAWK MAX WIFI ATX AM5 Motherboard](https://pcpartpicker.com/product/vjpD4D/msi-mag-b850-tomahawk-max-wifi-atx-am5-motherboard-mag-b850-tomahawk-max-wifi) | $189.99 @ Newegg  **Memory** | [\*G.Skill Flare X5 64 GB (2 x 32 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/6QcgXL/gskill-flare-x5-64-gb-2-x-32-gb-ddr5-6000-cl30-memory-f5-6000j3040g32gx2-fx5) | $659.99 @ Newegg  **Storage** | [\*Samsung 990 EVO Plus 2 TB M.2-2280 PCIe 5.0 X2 NVME Solid State Drive](https://pcpartpicker.com/product/hpqrxr/samsung-990-evo-plus-2-tb-m2-2280-pcie-50-x2-nvme-solid-state-drive-mz-v9s2t0bw) | $176.99 @ Amazon  **Video Card** | [\*MSI SHADOW 3X OC GeForce RTX 5080 16 GB Video Card](https://pcpartpicker.com/product/dQqNnQ/msi-shadow-3x-oc-geforce-rtx-5080-16-gb-video-card-geforce-rtx-5080-16g-shadow-3x-oc) | $1099.99 @ Newegg  **Case** | [Corsair FRAME 4000D RS ARGB ATX Mid Tower Case](https://pcpartpicker.com/product/Mjy8TW/corsair-frame-4000d-rs-argb-atx-mid-tower-case-cc-9011296-ww) | $99.99 @ Amazon  **Power Supply** | [Montech CENTURY II 1050 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/yJkqqs/montech-century-ii-1050-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-1050w) | $99.90 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$2807.74**  | \*Lowest price parts chosen from parametric criteria |  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-12-18 17:34 EST-0500 |",Positive
AMD,"First, thank you for this and the time you took to put it together. A few things; I want a v5 SSD that can do 14,7 on the transfer - that's a 9100 Pro. The case would require at least one additional fan - but do we really need fans up top?: I think 3 in front and 1 in back with the Phantom Spirit is fine. I do agree after really thinking about this - AIO pumps always go bad after 5+ years. Air fans; just replace them as they start to make noise. Air really is the best choice, so that saves a ton of money.  I don't need 64GB. 32GB is fine.  As for the 5080, that would require a 1000w PSU. There's no question the 5080 is faster and DLSS is better than FSR. But is the 15% difference worth $400? Not quite sure TBH. Plus the 5080 you linked has pretty bad reviews. I'd want to pay a bit more for a better reviewed card.  Here's what I put together: [https://pcpartpicker.com/user/SkrunchyCakes/saved/#view=D2FCFT](https://pcpartpicker.com/user/SkrunchyCakes/saved/#view=D2FCFT)  Not sure how to share the list as you did.",Positive
AMD,BTW I meant to ask - why are you recommending 64GB vs. 32GB?,Neutral
AMD,"> I want a v5 SSD that can do 14,7 on the transfer - that's a 9100 Pro.  Why? A gaming system will never make use of those speeds. There's essentially no real world difference between PCIe 3.0 and 4.0. 5.0 is just overkill.  I included a 1050W PSU in my build. You picked an SFX unit designed for small form factor cases that is needlessly expensive.  > But is the 15% difference worth $400? Not quite sure TBH.  You said you were willing to pay $150 extra for faster RAM that improves your performance by 5%. I'm a litttle confused by your priorities.",Negative
AMD,Because a few applications can make use of it and you seemed not to care about costs.,Neutral
AMD,Ahh you're correct on the SFX. The difference is RAM speed impacts everything the computer does all the time. The video card only impacts performance FPS above a certain threshold tied to the refresh rate of the monitor (I'm currently 144Hz). You are correct on the SSD; PCIe 4.0 will work fine.  I'm also hesitant to use a brand (Monarch) I've never heard of and AFAIK hasn't been well established.,Neutral
AMD,Your choice. The Montech Century II is highly rated by professional reviewers.,Neutral
AMD,I assume I'd only need one additional fan to blow out of the back of that case? BTW curious - do you have any of these exact components you recommended?,Neutral
AMD,Why not RX 9060 XT?,Neutral
AMD,5060 due to wattage and features.,Neutral
AMD,Why the fuck is it between the 5060 and 6800?,Negative
AMD,"If he's 1080p, you can still get away with 8GB well on 99.9% of games. Even at 1440p with upscaling only a few you might have to drop a couple settings i.e. ultra -> very high (mainly textures). I personally had a ton of driver related problems on my RX 6000 series AMD card over time, so I'd sooner go for Nvidia or possibly a newer AMD card instead.",Neutral
AMD,"that build no matter a new gpu is quite ""old"" granted probably will still hold up. best to switch if possible to a Am5 socket ryzen at least 7 series cpu. ddr5 recommend 32 gb fuckign windows 11 is chunking my 32gb ddr5 ram under minimal load as is. and i've ran debloat scripts turned off all auto start disabled fast boot i got xmp enabled a slight GPU OC like i got 7 tabs open in browser and discord on the side monitor damn near 35 percent my memory utilized at the moment. although the b650 platform was announced discontinued. well they brought it back. and now it would be a super viable option for your buddy if you have microcenter in your area i highly recommend seeing whats in the holiday bundles atm",Neutral
AMD,what about a 9060xt?,Neutral
AMD,"The 6800 isn't a great choice, since it's 2 generation old and needs at least a 600w psu.  I also wouldn't get a gpu with less than 12gb of ram, even if he only plays in 1080p.  If he's willing to search for it and spend a bit more he could look for a 9060 xt 16gb, it would work with his psu as it only has a tdp of 160w and it's also pretty good in 1440p.  He could look for a used 4070/4070 super, it would work pretty well.",Negative
AMD,I'd guess price,Neutral
AMD,"He suggested the 5060, and I suggested the 6800. It's not between the two, I'm asking if my recommendation is correct and if there's a better fit.",Neutral
AMD,"Yeah, he's running games at 1080p currently. I've heard about the RDNA2 driver issues, I was mainly suggesting the 16GB card because he'll need a CPU and RAM upgrade in a couple of years, and I wasn't sure 8GB would be a good recommendation for a 3+ year card.",Neutral
AMD,Shouldn't it be same or cheaper than a 5060? XD,Neutral
AMD,it should be between  1. 3070 Ti   2. 9060 XT   3. B580   4. 5060   5. 6750 XT,Neutral
AMD,"8gb version is, fair enough. And it does get at least the x16 pcie connector so less of an impact from going over vram. Might be one of the rare cases where the 8gb version makes sense",Neutral
AMD,"These are def more balanced options but the things that sucks is that this is strictly from a gpu standpoint, and having had my own issues with the 9060xt while using a 3700x, id say this removes at least the b580 and 9060xt. It might remove the 5060 too but i dunno how it handles with older amd hardware. I myself ended up upgrading to a 5800XT cause the 3700X couldnt quite keep up.  In this persons case, id get a 6750xt, maybe a 7700xt. Maybe.",Negative
AMD,"I don’t even really see 3070ti for reasonable prices, they are mostly going for like $600+ due to being discontinued based on what I’m seeing, you may want to update your list. You are probably better off buying a used 3080, though I’d generally not recommend buying 2 gen old hardware to better be able to take advantage of new features, efficiency improvements, etc.",Negative
AMD,What kinds of issues have you had with the 9060xt? He definitely can't upgrade both his CPU and GPU currently.,Negative
AMD,7700XT is pretty pricey,Neutral
AMD,who the fuck is selling a 3070 Ti for 600 dollars??? What the hell are you talking about? They go for sub 300,Negative
AMD,"For basic use, nothing. For gaming, i'd get screen tears, the occasional stuck frame in games with a lot of objects (survivor games or really heavy areas in elden ring) and I'm fos games like fortnite and marvel rivals, i'd get occasional latency issues.  Tbf, these went to the point where these games were unplayable. This wasn't like trying to run a 4090 with my 3700X. It wasn't a horrific imbalance, but it was enough that i noticed it and didn't like it.  Realistically if you want to future proof your friend pic, maybe do get a 9060xt first and explain to em down the line they may wanna get a 5600XT for like $150.  One more thing of note: i will say now that i game in 1080p. Gaming in 1080 puts more pressure on the cpu than the gpu. If your friend games in 1440p, the balance shifts more to the gpu, so in that scenario, their 3600X may be ok.",Negative
AMD,"New, yeah. But ive started to see em on the used market for less than $300.",Neutral
AMD,"Now I’m seeing used ones for those prices, not sure why it didn’t show up a minute ago, but new ones I’m finding prices are cranked I guess due to being discontinued for so long. Bit of a compromise buying used with no warranty though. Whatever the case though I don’t think a 2 gen card is a good recommendation. A new 5060ti is faster and has a warranty for like $60 more vs a used 3070ti.   https://www.google.com/search?q=3070ti&sca_esv=709dfd0b212bea1b&rlz=1CDGOYI_enUS1023US1023&hl=en-US&biw=428&bih=751&udm=28&shopmd=1&ei=b95GacP0Lub9ptQP9JH2wQQ&oq=3070ti&gs_lp=Ehxtb2JpbGUtZ3dzLW1vZGVsZXNzLXNob3BwaW5nIgYzMDcwdGkyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgcQABiABBgKMgUQABiABDIHEAAYgAQYCjIHEAAYgAQYCkjeRlC2CFinQXAAeACQAQGYAW2gAYQFqgEDNi4yuAEDyAEA-AEBmAIHoAKpBMICDRAAGIAEGLADGEMYigXCAggQABiABBiwA8ICChAAGIAEGEMYigWYAwCIBgGQBgiSBwM1LjKgB8IesgcDNS4yuAepBMIHAzktN8gH0aoUgAgA&sclient=mobile-gws-modeless-shopping",Negative
AMD,"He understands that it's either a CPU or GPU upgrade right now, and he's currently GPU bottlenecked in games so a CPU upgrade is secondary. He does play Marvel Rivals, though. Depending on how the RAM shortage plays out, he might end up going to a Ryzen 7600x in a year or two. The future proof way with a 16GB card was what I had in mind, it would suck if he got two years down the line and found that he needed both a CPU and a GPU.",Neutral
AMD,"I ain't really be seeing that, all I ever see is like $320",Neutral
AMD,"so they don't make 30 series cards anymore, so if you're buying one new, ofc it will be expensive, that's just supply  but, the 5060 Ti is BARELY faster and ""$60 more"" on budget builds is a LOT of money and it is not ""$60 more"", you can get a 3070 Ti for like $250, a 5060 Ti 8GB is $335, that is $85 more",Negative
AMD,What kind of budget are you looking at?  You can get two identical 27” 200~240hz 1440p monitors for around $200 each. Around $350 for similar refresh but slightly better brightness and response time tuning.  Around $500~$650 for 27” 1440p OLED.,Neutral
AMD,"I wasn't sure on size before but I think I want to go big with 32"" monitors. For budget I don't mind spending $500-1000 as long as I am getting something good",Positive
AMD,5500x3d,Neutral
AMD,5500x3d,Neutral
AMD,"R5 5500X3D  It even competes with the R5 7600 in many games, overall it'll be a tad faster than the R7 5700X in games unless said game can't benefit from the massive L3 Cache, but even then it'll just be slightly slower than the R7 5700X.  And both will be a great upgrade over your R5 3600.",Positive
AMD,I just got the 5700x from the 3600. Got it pretty cheap at microcenter.,Neutral
AMD,both are fine.   5500x3d slightly faster in games. But negligible.   And both are big upgrade from 3600,Positive
AMD,I’ve had my 7 5700x since I built my PC in 2022. It has been strong and has far outlived my 3060. I’m keeping it and getting a 5070 and pushing it to 4K !,Positive
AMD,Can you stretch to get a 5700x3d? You've spent a lot on a graphics card. Might be worth pursuing a little more.,Neutral
AMD,5500x3d. Very similar to 5600x3d which performs very similar (generally) to the 5700x3d. Lucky to be in a region that has that chip available!,Positive
AMD,"your not bottlenecking a 9070xt in any way that matters as long as theres enough ram and the os is in good shape .. i run mine on a straight 5600 and i think actual speed of the processor is as important as anything else for general gaming, and that said id rather have the 5700x than a 5500, doubt that is fully supporting pcie4 .. so yeah, unless your trying to get ultra high 4k performance with a high end monitor i wouldnt worry at all about bottlenecking a 70xt, the whole argument there is pretty lame imo ..",Neutral
AMD,"If prices equal: Ryzen 7 5700X  If you only care about gaming at the lowest cost: Ryzen 5 5500X3D  If you can afford to wait, saving up for a real 5800X3D",Neutral
AMD,What’s the price delta between the 5500x3D and the  5700X?,Neutral
AMD,"At current super-inflated prices on the used market, the 5700x3d  ain't worth it. Hell, you can get a 7800x3D for cheaper most places.",Negative
AMD,"Oh, I could definitely stretch if it was available. But it's literally not in stock anywhere :c     I've scoured every retail store in my country. No one is selling it. I bet that when there's one in stock, it will be worth actual gold.      The only X3D option truly available is the 5500X3d, unfortunately.      Maybe I shouldn't buy a CPU at all and just deal with the bottleneck until prices go down again? If they ever do that is.",Negative
AMD,"Oh wait, for real? Is it doing well on the 5600? Can you tell me how you're running your games? CPU usage?",Neutral
AMD,"oh, it's actually the same price lol",Positive
AMD,Ooft,Neutral
AMD,"If 5500x3d is all that's available that's probably the way to go. I bought my 5700x3d through AliExpress and it's been great. But if you can't get it, you're probably better getting the upgrade that you can rn.",Positive
AMD,"Do not wait for the Super series - they are highly unlikely to happen at this point, as AI/data center demand for memory is screwing the entire consumer market over right now.  Buy a GPU now if you're thinking about it.",Negative
AMD,"There likely no super series for the 50 series. Also could very easily see nvidia cut production of consumer GPU to shift more towards data center customers. Grab a 5070ti for $750 and be very happy for the next few years imo. (Or the AMD card, also great)",Positive
AMD,"There's a very slim chance, if any at all, for super series to be coming anymore",Negative
AMD,Supers might not come. There’s rumors of the 5060 ti 16 gb being discontinued due to ram. If you might move to Linux. 9070xt.,Neutral
AMD,NVIDIA drivers on linux are kinda eh so AMD will be a better fit if you’re thinking about the linux route. Can’t go wrong with either tho since they’re both great cards. Def buy a gpu soon cause they are at a reasonable price right now compared to ram and who knows what 2026 will entail. Good luck!,Positive
AMD,Buddy.  Nvidia is reducing output and production next year. Starting with low end cards.  Get the best you can now or wait to ride out the storm if the card you have now is good enough.,Negative
AMD,"Dude if you've seen what's happening lately I'd buy now! Idk if GPU prices will spike but storage and RAM, rumors of them slashing production on gaming consumer cards, super series cancelled rumors. It's too uncertain, and it can be had for MSRP for now:  https://www.microcenter.com/product/693841/zotac-nvidia-geforce-rtx-5070-ti-solid-sff-overclocked-triple-fan-16gb-gddr7-pcie-50-graphics-card",Negative
AMD,"6 months ago the ""wait for supers"" advice was a consideration. Since then prices on everything has shot up and supply can't keep up with data center demand. Supers are quite likely to be canceled or crippled at this point.  9070xt - rasterization/Linux 5070ti - features/ai creation  Most people say that the 5070ti is worth maybe $50-100 more than 9070xt",Negative
AMD,"I somewhat panic bought a 5070ti over the weekend when I found the one I was after for £700. Prices may not rise but after seeing what happened with ram recently I don’t wanna chance it. Very happy running path tracing at 1440 in cyberpunk with a lil dlss help, looks phenomenal.",Negative
AMD,Rx 9070 xt is much better value. Rtx 5070ti is like 7% faster and costs 30% more. That is like hugh price gap for little performance so would chose rx 9070xt everyday.,Positive
AMD,9070xt is better value if you don't care about ray tracing and as for new video card they will definitely cost more since the ram crisis,Negative
AMD,"I feel like there are 1000 of these posts a day, the threads are pretty redundant..  To answer your question I think the 5070ti is the best price/performance card right now. 9700xt is also probably great but I think DLSS4 and frame gen make the 5070ti worth the extra money.   Before the past couple of months, GPUs were pretty much impossible to buy for a somewhat decent price for a long time. So who knows what the market will look like years from now when you want to upgrade. From what I’ve seen of DLSS4 and frame gen, I think they can easily add an extra couple of years of viability to the 50 series cards and that might come in clutch.   If you have a micro center within an hour from you I would just check to see if they have one open box every morning. I got mine for $675.",Neutral
AMD,"I wouldn’t wait for supers that might never come, and in the UK I wouldn’t pay the Nvidea premium. 9070 xt Saphire Pulse was £540 before Xmas on Amazon. Might go down to that again in the Boxing Day sales, but often around MSRP these days.   5070 ti is usually around £700, but have seen it dip under sometimes. Still too much of a margin though for not enough benefit. At least if you’re primarily thinking about gaming.",Negative
AMD,"If you can deal with DLSS upscaling -- and I don't know why you shouldn't be able to deal with it -- I would just just get a 5070 Ti now if you want to play games now. On top of the other scarcity issues, you'd likely have to deal with the same scarcity-inflated prices that the non-super cards had at first.  And for what, extra VRAM for playing at 4K without DLSS upscaling? Just set aside the money you'd save in a bond fund until you're ready for the next upgrade, or use the money to buy shares of VXUS that you'll hold until you retire.",Negative
AMD,"The two cards are fairly evenly matched; the 9070xt has better raster performance and solid RT performance while usually being cheaper. The 5070ti has slightly better RT and upscaling performance along with lower power draw, but is usually significantly more expensive. In my market (UK), the AMD card is usually around £100 cheaper (depending on sales/offers), making it much better value.",Neutral
AMD,Bought a GPU last week because I'm scared.  I was on a 3080 and I was hoping to wait for the 60 series but open ai has forced my hand.,Negative
AMD,Talked to a large pc builder this week and they said increases from nvidia is certainly coming. I bought a 5070ti Thursday :-),Positive
AMD,DO NOT WAIT,Neutral
AMD,"In the event you haven't see it yet, Newegg has the Powercolor 9700xt Red Devil for $650.",Neutral
AMD,Buy nvidia gpu now as prices will rise when they cut the gpu production by 40%,Neutral
AMD,"If you need a new GPU in the next 1 - 2 years, you should grab either of those cards. With the AI chip shortage, RAM shortage, etc., prices will be through the roof for the super series, if they do come out at all.",Neutral
AMD,The only super series you’re gonna get now is super AI.,Neutral
AMD,super series will most likely not be announced until quart 4 2026 and even may not release until 2027 depending on how things are going with the AI boom,Neutral
AMD,Doubt they announce the super,Neutral
AMD,"If you're going to be running Linux, I suggest going AMD.",Neutral
AMD,"I just replaced my 9070xt with a 5080 after dealing with countless AMD software issues. I’d recommend not waiting as many people have mentioned in this thread, and to go for the 5070ti if your budget allows for it.",Neutral
AMD,"Nvidia is rumored to be discontinuing the 5060 Ti and the 5070 Ti because of the 16GB of GDDR7 they're using. These are the GPUs that comprise the 40% production reduction rumor.  https://www.tweaktown.com/news/109394/nvidia-to-cut-supply-of-best-value-geforce-rtx-50-series-gpus-in-half-over-memory-shortages/index.html  They are also rumored to have delayed any new consumer GPUs till late 2026/early 2027.  https://videocardz.com/newz/nvidia-geforce-rtx-50-super-reportedly-slips-to-q3-2026-rtx-5060-ti-16gb-in-short-supply-soon   Of course, these are all rumors. But I think you're going to be sorely disappointed if you wait.",Neutral
AMD,"If u dont care about ur frames being fake go Nvidia, if you want actual profit from your money go AMD, also the important thing to know is that the best thing to do now is to wait for the AI bubble to burst.",Neutral
AMD,What ever is cheaper,Neutral
AMD,I'm convinced if the mods stickied this question sub traffic would be down 50%,Neutral
AMD,I upgraded to 5070 today from my trustworthy work horse 2070 super,Neutral
AMD,"5070ti if it doesn't cost more than 9070xt, especially so if you plan to experiment with ai workloads especially image and video generation or video encoding, or have certain games in your library that favours Nvidia more or has dlss4 but yet no first party amd fsr4.    If purely gaming then the 9070xt is a much better value proposition.    To answer your question, it's better to secure a card now before the vram price increases affect GPUs, it's likely the super series is delayed at best or even cancelled.",Neutral
AMD,"I went with a 9070XT to upgrade from a 6900XT. I would've preferred to get a 5070ti but the cheapest 9070XT around here was an open-box Reaper for $521 vs. the cheapest 5070ti for $750 (new, no open box inventory available around me). I could've stomached paying maybe 15-20% more for a 5070ti but 44% more was a nonstarter. I've only ran one benchmark but I can't say I'm blown away by [the 9070 XT](https://imgur.com/ZiAVUA0) vs my [old 6900 XT](https://imgur.com/9UDxKey) ([7900 XTX score for reference](https://imgur.com/MVM95xX)).",Neutral
AMD,Two questions.  1. Do you really care THAT much about raytracing?  2. Are you willing to spend probably an extra few hundred next year because prices will sky-rocket.   AMD beats out nvidia this gun in raw performance. RT is obviously nvidia's cup of tea and if you're gonna switch to Linux definitely go amd for the compatibility.,Negative
AMD,"I've been agonising over this decision, too. I pulled the trigger and bought a 5070 Ti a few days ago and I do not regret it at all. Amidst all the rumours and price rises, I thought it's now or never. There might be a Super series or there might not, but either way I can play everything I want to now and for the next few years.  As other people in this thread have mentioned, you should buy a GPU now if you're thinking about it.",Positive
AMD,I just got a 5070ti and it is not even installed yet as I think the pricing will be worse next year when I really need it.,Negative
AMD,"I have the 9070xt on a 1440p widescreen, use bazzite, and really like it. Raytracing (especially path tracing) on linux is not as performant as it is on windows, but they seem to be catching up. Modern nvidia cards should work really well on linux right now. The big issue between the two companies is AMD's linux drivers are open sourced and maintained by a lot of large companies and individuals, ie valve, intel, even microsoft I heard, while Nvidia's drivers are not fully open sourced. If Nvidia decides not to contribute to their linux drivers, well.... you are out of luck. For flexibility I would go 9070xt, for bleeding edge RT and upscaling I would go nvidia. For 1440p at least, I don't feel like I need those advanced features. 4k maybe? If I were really into ray tracing I might get nvidia?",Positive
AMD,"Super series up in the air, worse case you buy the 5070ti or 9700xt and just sell it if the super series comes out.",Neutral
AMD,"Fuck Nvidia right now, I would normally say get the 5070ti, but no way in hell would I support buying Nvidia after their palantir announcement",Negative
AMD,This. You won't lose out much even if you get it now.,Neutral
AMD,"If the rumored 30-40% production cuts do indeed happen next year, it's going to be like the crypto boom and COVID lockdown eras all over again trying to buy a GPU next year - if not worse.",Negative
AMD,"Yea, there have already [been rumors reported recently](https://www.pcgamer.com/hardware/graphics-cards/nvidia-is-reportedly-looking-to-cut-gaming-gpu-production-by-up-to-40-percent-in-2026-due-to-vram-supply-issues-but-its-not-as-bad-news-as-you-might-think-not-yet-at-least/) that they are looking to cut consumer production 40% next year.",Neutral
AMD,"and if there are, you won’t be able to find them for a reasonable price.",Negative
AMD,Only 750 in americas? Sick,Neutral
AMD,"Prices might or might not skyrocket, but they sure as hell won’t go down. I’m thinking of getting the 5070ti too. What made you go for that and not the 9700 amd?",Neutral
AMD,50% slower path tracing nd 4070 level rt performance.  Keep this in mind.,Neutral
AMD,I'm scrambling to get a 9070xt this new year. Its very likely that bad things are heading our way.,Negative
AMD,100 pounds difference actually makes 5070ti better value lol. I'm die hard amd owner yet even I know their gpus are simply better and more premium.,Positive
AMD,What did you get? I’m on the 2080ti currently and trying to decide between the 5070ti or the AMD one,Neutral
AMD,Nvidia are also cutting 50 series production by 40% next year starting with the mid range cards and are already not shipping DRAM to AIBs. Hesitating on a GPU now is adding a few hundred to your purchase next year,Negative
AMD,"It won’t be as bad, cause due to memory prices people won’t be building PCs as much.",Negative
AMD,I dont think it will be as bad tbh. You could build a budget friendly rig for mining so even consumers  dabbled into crypto mining. Not many people will be able to build ai data centers the same way.,Neutral
AMD,"It's only pretty recently (over the past couple of months) that they've been available at that price, lol. Everything was way over MSRP for most of the year.  It's a great time - but a brief window of time - to grab a 5070 Ti or 9070 XT for a fairly sensible price. The 2 best overall cards from this gen IMO, and you can't go wrong with either.",Positive
AMD,"Originally I was leaning heavily toward the 9070xt. I upgraded from a 7700xt and liked the cost performance of AMD but… then I tried cyberpunk for the first time with ray tracing and decided I wanted that wherever I could lol. I think 5070ti is about 5% more powerful on average and the larger market share for NVIDIA feels like I’m less likely to encounter any quirks in games as I get the impression more optimisation is weighted toward nvidia, also DLSS seems to atm be slightly ahead of AMDs but sounds like that gap is closing more and more",Positive
AMD,"Good prices right now actually, check Newegg .. a lot under MSRP including Sapphire Nitro+ that's only $699 new",Positive
AMD,"Try to get one when you can and check your power supply as most of 9070xt require 850w and some 9070xt may draw abit more power.  Since you are on AM4 , I assume you build this PC yrs ago and the wear and tear on the powersupply may not be stable for a 9070xt",Neutral
AMD,"Current price difference between the cheapest in stock cards I can find is £117 in favour of AMD (£557.99 XFX Swift 9070xt at Box Vs £675 for a Palit Gaming Pro 5070ti at Overclockers. And Overclockers also charges £7.99 for delivery, while Box is free delivery). That's a substantial price difference between two cards that are very close on performance.",Neutral
AMD,"5090, got through the queue on Thursday luckily.  Hopefully I can ride it out for 10 years",Neutral
AMD,"Yeah, this is like a stop loss situation here. Worst case scenario you pay a lil more today but you still own a pog card for life.",Neutral
AMD,"Here's to hoping you are right, lol. But I'm still concerned it's going to get pretty bad.  You don't have people stuck at home wanting to build PCs like during COVID lockdowns, but data center memory demand is huge and a 30-40% production cut on the consumer side is a huge supply shock that *is* going to increase prices to some extent no matter what - it's just how much is what remains to be seen...",Negative
AMD,its like 1100 dolars on avg here in eu,Neutral
AMD,Thank you for the response. That’s always been my thoughts too.,Positive
AMD,$750 where i live. Is it worth that much if i have sapphire nitro+ 7800xt?,Neutral
AMD,And if you're near a Micro Center there are [often open box opportunities](https://imgur.com/tbPVoZy) for even cheaper. I'm stress testing it right now and so far not even any coil whine issues to complain about.,Negative
AMD,"Tysm. I plan to upgrade my PSU to a 850W in the near future, while taking it a lil easy till then. There's def some wear and tear as I built it during pandemic.",Positive
AMD,"I bought a 850w at the time to future proof, but many thanks for the heads up about this.",Positive
AMD,"Very close raster performance.  No dlss and almost no path tracing,  dogs hit frame gen , 4070 level rt performance.   If all you do is to play battlefield 6 or cod then yeah, good value.",Positive
AMD,honestly we are probably already at the last reasonable time frame to build a PC. it’s literally only downhill from here,Negative
AMD,In Germany it's also 750 euros.,Neutral
AMD,In the netherlands you can get a 9070xt for less than 700 euro after tax,Neutral
AMD,Any particular 5070ti models you’re leaning toward?,Neutral
AMD,"All are great cards, but there's SOME amount of lottery in those 12v connectors   It's really just the case of is the jump in performance worth it - RT is really solid on the 9 series, and raster is clearly better than the 7800 series so you'd have performance gains no doubt, it's more a question of it's enough of a discount for you   I have a gigabyte 9070xt gaming OC atm, building a PC for a friend and he's buying it from me, I bought an Asus Prime OC to replace it (my case is short, not many options for me)   I was coming from a 1080ti originally so the performance value was a no brainer",Positive
AMD,"Yes, it's a better card, but it's not £125 better.",Neutral
AMD,"Well I just read another post where a guy said to take the windforce if you plan to bios flash it etc. And after that, the Asus Prime was supposed to be the best. He said to ignore the Zotac because it was a pos.  I’ve heard this being said about zotac before, even 5 years ago already. So I don’t have a hard time believing him.   So right now I’m set on the Prime. I don’t want to overclock anyway. Generally I set a low fan curve and undervolt my gpu a bit so.  That being said, I’m still deciding between the gtx and the 9070xt. I don’t know",Neutral
AMD,"I came from 5700xt to 7800xt and it was also very much difference. I dont think its gonna be that much of a difference to 9070xt. I was waiting for next gen but seeing the prices,  idk if i should go for 9070.",Negative
AMD,"Ah cool cool I went with the prime in the end, well prime oc version but only because it was cheaper weirdly.  Dunno about it being one of the best, thought it was more mid but has been decent for me so far, best of luck whatever you decide",Positive
AMD,What about the ventus model for 5070Ti? also considering upgrading,Neutral
AMD,Thanks man. Glad you enjoy yours. I’ll just continue my research for a bit more,Positive
AMD,Im not sure. I found this awesome MSI expert Oc edition I have my eye on. But i really need to do more research hahaha,Positive
AMD,"With your setup and that new graphics card, the biggest thing holding you back will be 8gb of RAM and then your CPU in that order. But 60fps @1080p isn’t a very big ask so you might be okay?",Neutral
AMD,"Okay, the motherboard is fine, the CPU is okay, but the RAM is not. Buy RAM from Micro Center (or wherever you buy the GPU).  I'd go with 32GB if you can afford it, or 16GB if you don't want to pay for 32GB.  After you get the GPU, upgrade the CPU to an R5 5600X or higher, or swap platforms. If you plan to swap platforms anytime soon, get 16GB of RAM for now. If you want to make the most out of AM4, try to go for an R5 5800X3D and 32GB of DDR4. If you do swap platforms, go for an R5 7500F or an R5 7600X/9600X (they are the same price; mainly availability is the difference, and the 9600X uses slightly less power).  Also, for the power supply, ensure your current wattage is enough for the entirety of the build.",Neutral
AMD,"This looks to be the same price as MSRP was at launch, which I would say is a pretty good deal.  Your current build is fine, although I'd consider trying to pick up some more RAM. DDR4 *has* gone up in value, but not as egregiously as DDR5 from what I've seen.",Positive
AMD,"Do you have a preferred place to buy RAM? Also, should I go up to 16 or 32",Neutral
AMD,If I get the GPU and get up to 16GB of DDR4 what motherboard would you recommend?,Neutral
AMD,"Thanks for the advice, what do you think is the best place to pick up DDR4?",Positive
AMD,"If you can afford 32GB go for it, otherwise 16 will do fine for now, definitely an improvement from 8",Positive
AMD,"The motherboard you currently have is fine, unless there's an issue you are having with it; otherwise, keep it as is.  Unless you mean for a platform swap, the Asus Prime B650 WiFi is always my go-to. I've never had an issue with them, and they look good for a good price.",Positive
AMD,"Whichever place has the cheapest price. Don't forget to factor in postage if you're ordering online. Might be best to check out PC Part Picker to see what vendor is currently cheapest, although they mostly don't tend to vary by more than 5% in my experience.",Neutral
AMD,"I wish I thought of upgrading before everything got expensive lmao, I'll have to find enough cash for 16gb",Negative
AMD,"Oh well thats good, btw I was looking for a 5600x on Microcenter (which is where I'm planning to buy my GPU and RAM) and could only find a 5500 for 70, 4500 for 60, and 7600x for 180. Should I look around for other options/ try to find a 5600x or would this work?",Positive
AMD,"A 5500 is a good CPU, but the 5600X is a noticeably better one. Look on Amazon. The stock cooler should be enough for it. If you want some reassurance, the Hyper 212 is a good $19 cooler.  Unless, of course, you already have a sufficient cooler.",Positive
AMD,I found one for 223 on amazon which is a bit of a jump lol. I'll probably go for the 5500 and I have zero idea what my cooler is so I'll most likely pick the Hyper 212 up as well,Neutral
AMD,"Yeah at that price a 5500 is a better option, go for it!",Positive
AMD,"Thanks for the help, dude!",Positive
AMD,Anytime!,Neutral
AMD,"With the current market conditions, especially for RAM, I'd stick to an AM4 motherboard. From the latency issues you're describing, sounds like you may be hitting the memory limit at 16GB. See if you can find an affordable pair of 32GB DDR4 sticks.  If you want to improve your CPU and GPU performance, get a Ryzen 7 5800X and look into either getting a used 7800XT or a brand new 9070 XT.",Neutral
AMD,"Given your mobo problems and the performance improvements you will experience with a new cpu and gpu, I recommend you build an AM5 based pc.  Newegg and Microcenter both have some very good combo deals.    These videos provide perspectives you may find informative: https://youtu.be/vt7VLU-rjV8?si=dZJDyZuTSfPxHtEd https://youtu.be/2ReW5i3gRCs?si=BSyx4ePcGKGJ1Jen  I hope you enjoy your new pc: AM5 or AM4.  Merry Christmas!",Positive
AMD,What do you think about the idea of me upgrading the CPU and motherboard and adding 2 more sticks of the same ram I have and waiting to get a new gpu? Would that still give a considerable bump in performance,Neutral
AMD,What do you think about upgrading to a  Ryzen 7 5800x with a  MSI B550M PRO-VDH And two more sticks of the same ram I have and waiting to upgrade the rest till I have more money,Neutral
AMD,"Yeah, you would get smoother frames and more memory overhead while you're waiting. The RX 6600 works great for 1080p quality if that's your focus.",Positive
AMD,"The 5800x will give you a good increase in cpu performance.  So long as your gpu isn't holding you back, this switch should benefit you.  Enjoy your build.",Positive
AMD,Okay I might genuinely upgrade everything but the GPU and psu right now,Neutral
AMD,"The performance difference between the two is negligible aside from raytracing, and in a few games the 9070xt even performs a bit better. If you don't care about RT that much the AMD gpu is a clear winner in terms of value. You also won't be supporting Nvidia which is a plus.",Positive
AMD,"9070xt the extra $150 does not justify getting the same raw performance, only get the 5070ti if you need nvenc for streaming, dlss, only play raytracing heavy games or do work that needs Cuda Cores.",Negative
AMD,"If you're streaming and doing heavier workloads with Blender and stuff then get 5070Ti. If you're just gaming, get the 9070XT. You really want to make that +100 dollars worth and for gaming the performance alone doesn't justify all that extra money except maybe if you must have path tracing.",Neutral
AMD,"If you use linux or ever plan on using linux, 9070 xt is by far superior",Positive
AMD,Just bought the amd,Neutral
AMD,"9070xt, the extra Nvidia features (path tracing at <30 fps in very few game, multi frame gen) are not worth 100.",Negative
AMD,If they were both the same price: 5070 TI.  If it’s just for gaming and you’re penny pinching: 9070XT.,Neutral
AMD,"I would say the cheap model for the 5070ti is actually better than the recommended ones.. but i might be wrong on that one..   On 9070xt vs 5070ti there is no clear winner you have to consider the facts.   -9070xt is the cheaper card that offers pretty much the same native raw performance depending on the game.. they trade blows.   On the other hand 5070ti has better software features.. like dlss that is better, same for frame gen, latency with reflex, video encoding/enhancement. It's also better for other applications other than gaming. I also liked the fact it consumes even 100W less than the 9070xt so less heat in the room and cheaper electricity bills... Rtx is also better on 5070ti.. pathtracing is much better on Nvidia   The VRAM is also better with gddr 7... Whether that will make any difference innthe future it remains to be seen....   So if you want the cheapest raw gaming performance 9070xt is the answer... If you want the better card 5070ti is what you are looking for.",Neutral
AMD,"On the base MSRP cards, i’d go Asus Prime.   Heavy RT is better on Nvidia, and DLSS is in more older titles. Newer titles seem to be releasing with FSR4 (BL4 and Cronos are the most recent ones I have played) but ray reconstruction is far more mature on Nvidia as AMD is in the infancy of it there.   Something to note is that Nvidia seems to be very overclockable if you are into that.   I own a 9070xt and find it plenty of GPU at 4k, but i’m also fine with software lumen and not going wild with RT/PT. I wanted the Asus TUF OC model, and that version was $300 CAD more for the 5070ti model, which was not worth it after testing both. Also saved me from dealing with Nvidia’s two-app solution which I hated.   What do you have now? Nvidia drivers also have a higher CPU overhead.",Positive
AMD,There pretty much the same except for ray tracing and better work performance on the 5070ti resell holds better on the 5070ti also,Positive
AMD,1. Do you know what Path Tracing is?   2. Do you care about it?  If either is no - 9070XT is pretty safe bet.,Neutral
AMD,"For 100 bucks more it's a no brainer, get 5070ti  EDIT: with the new drivers that Nvidia launched these days it outperformed 9070xt in raw performance overall, and if you add the better upscaler (dlss and Multi Frame Generation) there is literally no reason to not pay 100 more, I'm not saying amd is bad, but with less than 150/200$ difference there is no reason to not get the better one, except if money is an issue of course",Positive
AMD,5070ti,Neutral
AMD,"How are there two top comments, one saying the price difference isn’t justified, and the other is saying it’s only that price difference LOL",Neutral
AMD,"9070XT. Same perf, lower cost.",Neutral
AMD,70ti,Neutral
AMD,DLSS>>>FSR,Neutral
AMD,"i find it funny why people even consider the nvidia alternative to amd when it comes to just gaming. amd doesnt use the disastrous 12vhp connector that catches fire every other day, they put reasonable vram amounts in their cards, priced much lower for comparable performance. only time in my mind to go nvidia is if you are a full time streamer or need 5090 level performance because of rt games, pushing high resolutions etc... other than that its a waste of money to go nvidia. Especially with FSR4 which is not all that bad compared to older FSR versions.",Negative
AMD,"If it is less than. $100-150 difference. Go 5070ti. If the difference is higher (cheaper), go 9070xt",Neutral
AMD,"Thanks, I have a question about raytracing - whenever I see some comparison videos, it looks like the ""sky"" in the 5070tis is much more blue than on the AMD. Example: [https://imgur.com/PM0jL1k](https://imgur.com/PM0jL1k)  Is this because of ray tracing, or is this a feature of the game? I have seen this in several different videos now where the 9070XT shows a significantly more ""gray"" sky than the 5070ti",Neutral
AMD,"Out of curiosity, why do you feel it’s good to not support Nvidia? I’m just recently back into PC gaming after 15 years and I like to know. Thanks",Negative
AMD,Production in general,Neutral
AMD,"i agree that the extra features arent worth that much, but if someone really cares about ray tracing then they are. Full path tracing at <30fps native resolution is something an rtx 2080 does, not a 5070 ti. If someone REALLY cares about ray tracing and/or non-gaming workloads, the 5070 ti is the obvious pick. Otherwise the 9070 xt is way better value",Neutral
AMD,"\>path tracing at <30 fps in very few game,  Lol, good cope. Within last year only there was four major releases with PT (arguably three), next big one is coming within a bit more than 2 months. And there won't be LESS games with PT as time goes on.  As for sub 30fps... Do you yourself believe that?",Positive
AMD,"I know this may get me banned but... is this question asked like every few hours on this sub? 🤣  Also, I appreciate how concise you put the answer.",Neutral
AMD,Is there a longevity conversation to be had?,Neutral
AMD,"\> with the new drivers that Nvidia launched these days it outperformed 9070xt in raw performance overall  It ALWAYS outperformed 9070XT, both raster and RT. ""Fine wine"" meme was literally a single HUB video with 16 games tested, which is very small sample size. All wide scale benchmars have ALWAYS shown 5070Ti as faster card.",Positive
AMD,Are there connector issues outside of 4090 and 5090?,Neutral
AMD,"5070Ti can do PT 1440p ca 60fps avg with DLSS quality. With 9070XT you are getting sub 30fps if not sub 20-fps in: - Alan Wake 2 - Wukong - Indiana Jones  .   5070Ti can actually do 4K 90fps avg with high settings in most newest games with DLSS Performance (as it’s a half a tier better visual fidelity still, equals FSR Quality)  You also get features that you don’t get on AMD or just plainly work noticeably better, like: - Ray Reconstruction + half tier better upscaling - Multi Frame Generation for CPU bottleneck - RTX HDR - RTX Broadcast with AI plug ins for OBS - 3d modeling - longer software support 10 years vs 3 years (RX 7000 series) - holds value better for resale",Positive
AMD,"From that screenshot alone it just looks like a dynamic time of day difference. The cards won't display game visuals any different, raytracing would just run at a lower framerate. The only visual difference between the two would be DLSS vs FSR and at this point FSR4 is just as good.",Neutral
AMD,"Those screenshots are taken in very different time of day, so they are not directly comparable as far as image quality goes.  And nah, RT does not make sky more blue, it's for very different things.",Neutral
AMD,"if you want i can send you some screenshots of rt on vs off in insomniac's spider-man remastered.   You see, ray tracing doesnt make too huge of a differernce these days as most games only implement it for reflections, and sometimes lighting. Games that are full path tracers though (such as portal with rtx) sure do look different (and better) from traditional rasterization, and also are much more demanding  The reason nvidia's rt tends to also look better, is because rtx cards have dedicated ray tracing cores, so cores that are literally engineered for this ykwim? AMD doesnt have dedicated rt cores, instead they have ""ray accelerators"" which are basically tiny add-ons to some of their standard cores. A little add-on pack for a boost in rt performance on a standard core just can't keep up with dedicated cores, especially when these cores are used for everything else too while AMD's solution still leaves the cores with all the other work ontop of ray-tracing",Neutral
AMD,Tbh just for Gaming i say 9070 XT all the way Price difference is not worth it imo. 9070 XT is an amazing card.,Positive
AMD,"Nah, 5070Ti will actually struggle to hold over 30fps at native 1440p and higher, that IS true.  It is just extremely dumb to use it as an argument when upscaling exists AND provides as good of a quality as it does. Cyberpunk at DLSS Quality keeps above 60fps most of the time, with DLSS B heftily above, and image quality is pretty gorgeous on top of benefits PT provides.",Negative
AMD,Amd market share is rising and nvudia is stoping the production of non high end gpus. The only one coping is you,Neutral
AMD,"There is, but it still mostly concerns one's interest in heavy RT/PT. PT won't become universal within lifetime of 5070Ti, at least realisting lifetime. If one wants to stretch it noticeably beyond 5 years - then maybe.",Neutral
AMD,"When they came out and right after amd fixed they drivers it was just about 5% faster in 20 game overall, now it's a lot better I would say, the card got performance boost with the new drivers Nvidia released and up to 20% more fps in some games, but yea it was always faster, now its just faster they it should've been since the beginning",Positive
AMD,"Well yea usually 100 or 200$ more matters only if you are a kid that saved money for it and can't wait anymore, otherwise there is literally no reason to spend more  And yes mainly the futures and stable drivers was the thing that kept me going from Nvidia, all my life I've been using only Nvidia cards and not a single issue, my old gt730 was running 6 years every day overclocked to the max and still going strong lmao  on the other hand 5070ti is the top tier gpu for 1440p gaming, I'm amazed how this thing plays everything on max",Positive
AMD,actually yes. i’ve seen a few 5080s with melted connectors,Neutral
AMD,"Any GPU with 12vhpwr has the potential to melt  NVIDIA GPUs have a higher chance to melt (albeit still low) simply because of the way they are designed (my fathers 5070ti hasn't melted yet, hopefully it never will)  AMD GPUs can still melt, although it is FAR rarer, and I personally have not experienced it (thankfully) on my Sapphire Nitro+ 9070xt  Not all AMD GPUs have 12vhpwr, afaik it is only on some 9070xt models (could be more tho)  All NVIDIA 50 series GPUs, some of the 40 series GPUs, (I'm 100% sure the 4080 variants and 4090 use 12vhpwr, not sure about others), and the 3090ti all use 12vhpwr  The higher the wattage, the far riskier it gets",Neutral
AMD,But isn't FS4 supported only in a handful of games?,Neutral
AMD,"Would correct a bit that plain RT does not look better with NV, but runs much better. It can look better due to that, because RT is *always* temporally accumulated, but that's a moot point.  On the other hand, there is DLSS Ray Reconstruction, which DOES make RT look better. AMD until very-very recently had no alternative at all, they now have it. Somewhat worse working. In one game. That game being Call of Duty. Yeah...",Neutral
AMD,This is just simply false,Negative
AMD,"Damn really? Im basing my assumption off how my 2080 at 1080p performs, thats sure a shame (my only full path tracer was also portal with rtx so far so ots not very extensive)  And also yeah dlss is quite impressively good, and always implemented alongside rt. Ray reconstruction (the not transformer model, forgot what its called) is also good for performance but i personally think it looks worse  Anyways thank you for this info, I'll keep it in mind!",Negative
AMD,"\>Amd market share is rising  From 9 to 12%? What an achievement. Call me back when they are at least at 50%.   Also what's that even supposed to do with GPU pick in the first place? Are we supposed to buy only cards of vendor with bigger market share?  \>nvudia is stoping the production of non high end gpus  Alledgedly presumably according to rumors. And those rumors are like fifth time this year alone. Call me back when something ACTUALLY happens.  \>The only one coping is you   What the hell is this comment even then, LMFAO? You are literally arguing back because I've hurt the feelings of your favorite multibillion company. Pathetic.",Negative
AMD,"\>The higher the wattage, the far riskier it gets  Which is funny, considering two GPUs in question, since 5070Ti routinely sucks 80-100w less under load.",Negative
AMD,"In quite a few games with driver override, but still much less than DLSS, yeah. One can always use Optiscaler, but it's not a universal solution.",Neutral
AMD,"In theory it doesn't look better, but it often does due to games rt being more nvidia optimised. I should have mentioned that in theory it should be the same but just isn't cuz of that, thats my bad  And yeah dlss 3.5 really is great looking i must say",Negative
AMD,Which part?,Neutral
AMD,nvidia is the company that bought all the rams just to make everything more expensive and you tell me they dont want to kill the consumer market? Do you hear yourself?,Negative
AMD,"\>In theory it doesn't look better, but it often does due to games rt being more nvidia optimised.  That does not work that way. First - RT is not really ""NVidia-optimized"". It's just a sampling algorithm, it runs similiarly on any hardware. It just runs much faster on NVidia. Running faster in case of RT actually brings better visuals too, but not just due to ""NV vs AMD"", slower NV card will look worse than faster AMD card all the same.",Negative
AMD,"First - you really think it was SPECIFICALLY Nvidia who caused DRAM and NAND shortages? Like, noone else, only NVidia?  Second - do you geniunely think they WANT to kill consumer market? Like, you think they see some value in that? Or that they are just plain evil or something?  NVidia stayed in consumer market since first cryptoboom, solidified its position, expanded its market and continued supply the shit. They were rumored to ""exit consumer market"" for literally dozens of times. Since start of the year alone this is like fifth round of those rumors.  [https://wccftech.com/nvidia-rumored-to-reduce-rtx-50-gpus-production-in-china-in-favor-of-ai-gpus/](https://wccftech.com/nvidia-rumored-to-reduce-rtx-50-gpus-production-in-china-in-favor-of-ai-gpus/)  [https://www.techspot.com/news/108150-nvidia-could-slash-rtx-5000-gpu-production-up.html](https://www.techspot.com/news/108150-nvidia-could-slash-rtx-5000-gpu-production-up.html)  [https://hothardware.com/news/nvidia-is-allegedly-scaling-back-supply-geforce-rtx-50-series-gpus](https://hothardware.com/news/nvidia-is-allegedly-scaling-back-supply-geforce-rtx-50-series-gpus)  And so on and so forth.  And THE MOST IMPORTANT QUESTION: how the hell is that related to Path Tracing and NVidia cards ability to run it at all?! Does that ability shrink with lower market share? Or with lower supply of cards?  Like, WHAT are you even to argue here, LMAO?",Negative
AMD,Ahh okay! Thank you for explaining!,Positive
AMD,There is no ai without nvidia. Everyone is pumping money into nvidia. Youvcan say that nvidia is the shortage itself at this point,Neutral
AMD,"No problem) There's some astonishing amount of RT misconceptions on reddit, sometimes outright disinfo, which is pretty sad, especially for subs like this one which are created to help people to know stuff (and instead turn into fanboys brawl).  So it's always very pleasant to discuss stuff calmly and with someone who actually listens.",Negative
AMD,"Dude. You are spitting ideological headlines in technical discussion. Like, literally nothing of what you've wrote here is any sort of factual statement. Literally not a thing.",Negative
AMD,"Yeah, there's alot of misinformation on here that im sure everyone believes at least some, it's a shame more people don't listen to being corrected and instead argue over a brand preference   Thank you very much for telling me all that about rt, i feel like I've actually learnt alot!",Negative
AMD,Absolutely solid build!,Positive
AMD,This will certainly play Minecraft or Fortnite no problem.  This will likely play any.expensive game you throw at it.  That GPU is much better than mine even.  RAM should be plenty for the coming years.    This build is honestly over kill for the two games you mention but the computer could be used for other stuff as well.,Positive
AMD,"Solid. I have a similar build and can play anything super smoothly. Kids are lucky :)  a 1440p 27"" monitor would be great with it",Positive
AMD,Way to go mom!,Neutral
AMD,"Looks like a great build, my only concern would be the cooler and memory being a tight squeeze on a micro ATX motherboard. It should be okay though.",Positive
AMD,"Not sure if you need to hear this, but you’ve done well. Really well. This is a fantastic, very solid first build, and this will well and truly last a long while for what they need.  You sound like a lovely mother who has gone above and beyond. Your boys are so lucky to have you. It sounds like you do so well looking out for others, so I just want to ask you to please look after yourself and treat yourself too. I hope you and your wonderful family have the most wonderful Christmas.",Positive
AMD,"For the m.2 SSD, I would recommend getting the WD SN7100 1TB or the Samsung 990 evo Plus 1TB. At the moment of me posting this, Walmart is selling them for $89 and it’ll perform better/more consistently. Bestoss seems to be one of those cheap SSD brands that end tanking in performance or failing early.",Positive
AMD,You are a good parent. Excellent build choices all round.   Everything in your list is compatible and will work well together. The purist in me would go full size mobo and a noctua cooler. But that is just me being picky and personal preference - what you have chosen are premium parts that will work very well together. Good job mum!,Positive
AMD,This is a great build. Has plenty of upgrade room in the CPU and GPU in the future too.,Positive
AMD,"If you got the money to order 2 more, you could and just return the ones that'll be arriving late. Or if your kids are reasonable, you can tell them the situation.",Neutral
AMD,That build will play anything in 1080p and is very much capable in 1440p. Good job!,Positive
AMD,"Amazing build. Would get this for myself no problem.  Only thing i would change if needed is a smaller psu to save some money or upgrade the cpu. A 750w psu with this setup is way more than needed, let alone 850w.   That aside every piece is well selected and this should give very good performance for some years.",Positive
AMD,"If the purchases haven’t been made look into the Microcenter 3-1 deals. I think you can get a slightly better one with wifi while it being cheaper through there.   The rest of the parts are great, hope you and your kids have a merry christmas !",Positive
AMD,"Great and great value builds, only component I'd have spend a little bit more on would be the motherboard. The IO is very lackluster and the power delivery, while fine for that specific CPU, is almost as low end as it gets. The nice thing about AM5 are upgrade options in the future, so having that solid base would have been nice.  These will perform great though. Are you going to build it with them? :D",Positive
AMD,"Looks like a solid build, the systems should play the games they like no problem and allow them to play even more demanding games. The only thing I might recommend is that if you can find motherboards for the same price or even cheaper that are ATX rather than micro ATX then the systems will be easier to build and upgrade over time. Micro ATX (or mATX) are pretty small and can make working on them cramped. You can usually get better upgrade options (more RAM slots, more SSD slots, more connectors, I/O ports) with an ATX board so the systems will be more upgradable / flexible down the road.  To be clear your builds will work totally fine, mATX motherboards should be compatible with your ATX case, as well as all the other parts.  If the parts don’t all show up by Christmas something you could do is print of the PCPartpicker build list and put them in cards, wrap up the parts you do have on hand, and just tell them that the rest of the parts on that list are coming. They will be absolutely stoked and I bet the anticipation of building their first PCs will be palpable. While they are waiting on parts they can watch build guides etc.",Positive
AMD,"Honestly, you've done a great job here.  Everyone is going to give you opinions on what they would've done differently, but I would stick with what you got.  You've got a really good balance of all the components and these will be very easy to upgrade in the future (although you certainly won't need to for many years)  Only thing to keep in mind is that these PCs won't support WiFi out of the box. You can always add WiFi to it later though.  Also I would recommend making the Windows 11 Install USB beforehand. Nothing worse than finishing the build and having to wait for windows to download.",Positive
AMD,Solid build,Positive
AMD,1tb ssd might be short depending of the games installed  I would try to get 2tb if budget allow it,Neutral
AMD,It's  a better setup that my fairly expensive gamer laptop. I'd say you did good.,Positive
AMD,"A reasonable build with room for expansion in the future, good job!!",Positive
AMD,I've just built an almost identical system for my youngest. Your kids will be thrilled!,Positive
AMD,"I have a very similar build to this, and I can confidently say this will handle any Minecraft stuff they want at 1440p even, let alone 1080p, fortnite I've not tested personally, but I get over 100fps on 1440p ultra settings in every game I've tried, so I think fortnite will be zero issue at all.",Positive
AMD,I’ll be honest it’s good but it’s also very much overkill for the games and applications you listed.   Also should be noted that buying parts by themselves isn’t always cheaper than buying prebuilt PCs. Sometimes you can get a steal on a pre built.   Overall though they’ll be happy and the PC will last a very long time.,Positive
AMD,"I was able to grab the same CPU from Walmart for $125, if you have one close may be worth a look.",Neutral
AMD,"It looks great. Eventually someone might need more ram, if they get into things that use more, but by then hopefully the price comes down. 1tb is a little tight for storage, but you can get more later when it is an issue also.",Positive
AMD,"This is a great build, I like it",Positive
AMD,"keep in mind the motherboard doesn't have WIFI so you will need to plug in an ethernet, which is better anyways but might be inconvenient depending on where the router and pc are located",Neutral
AMD,that looks pretty solid...should be able to handle minecraft java with some shaders.  That is what makes computers sweat.,Positive
AMD,"Looks solid beside the CPU and mobo (already addressed this!). Im not to say the 7600X is bad, I have one, its fantastic. But for $20 more… buy the 9600X. I bought my 7600X as part of a bundle when the price gap between these two CPUs were $50+,and I would have bought the 9600X instead by itself when it was $180. (Its onsale currently for $194 (amazon) but id still say thats a good purchase!)",Positive
AMD,Cooler might be an issue. But in an year you can upgrade it if it is getting hot. Otherwise good build.,Positive
AMD,"I knew it was for minecraft, didn't really want to figure it out for Fortnite, but was aiming for a setup that will last into their teens and allow them to do more complex things besides gaming. One mentioned Blender the other day and I was like buddy I don't think your laptop can even handle that install file lol.",Neutral
AMD,"Very helepful for me, thanks",Positive
AMD,"They have 27"" Curved 1080p monitors right now hooked up to their laptops but maybe for their birthday, which lucky for them is at the end of Jan. Is it really that big of a difference?",Neutral
AMD,"There is generally no physical difference between an ATX and mATX board if it comes to dimensions where RAM, GPU and cooler are plugged into. It is just that this particular mATX board is tiny, but this particular size is very rare - though nothing strange in terms of compatibility and since it has only 2 rather than the more usual 4 RAM slots - OP will be completely fine.",Neutral
AMD,"I'm gonna be honest, I didn't even realize I had bought a micro lol. I was sleep deprived and so overwhelmed, there's so many models and versions, I said screw it at like 3am and never looked at mbs again. I can go try to find different ones, any suggestions?",Negative
AMD,Take your time have the web available. Yoda always consuls Patience and review and before quit check the plug. Such a great gift.,Positive
AMD,"> a noctua cooler  Huge waste of money on a PC at that budget at low powered CPU. OP's is already overkill.  Full sized mobo is just for aesthetics, though I'd have spend a little more here for a better featured motherboard (could be mAXT, just not this one).",Negative
AMD,"Thank you! I'm willing to get a full size mobo, just not sure which one and good grief there are so many that seem exactly the same to me.",Positive
AMD,"That was my plan but of course the same Sapphire ones won't get here by Christmas now and I'm not sure what to try getting instead. Was reading that the gigabyte ones aren't as good as the Sapphire ones, not sure how true that really is though.",Negative
AMD,"Hi there, your comment has been removed due to our no grey market / piracy rule (**[Rule 3](https://www.reddit.com/r/buildapc/about/rules)**).  It seems your comment mentions something that seems to be a known gray market or piracy resource. Due to this, we are unable to allow your comment on r/buildapc.  Thanks for understanding.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",Negative
AMD,"I almost did get the 9600x but I'll go look at it again now, thank you!!",Positive
AMD,That cooler is more than enough.,Neutral
AMD,"Ok good to know, I just kind of picked one, any suggestion on what I should be looking for spec wise compared to this one?",Positive
AMD,It should not have problems with Blender or Video Editing software like Adobe Premiere Pro or any other productivity software.,Neutral
AMD,"Great build. This is what I would suggest for any friend or family member trying to keep a computer for years. the CPU can be upgraded in a few years IF/WHEN they get into 3d modeling or video editing, ect.... no reason to go above what you have now for gaming. the GPU can be upgraded years down the road and these rigs will be just as good as new. You should be proud of yourself. We are all very impressed, these kids have a great mom who cares.",Positive
AMD,"For Fortnite and Minecraft.. not really if you need the money. A majority of players for any type of games still use 1080p, and it still looks good. The build you’re getting them can absolutely handle 1440p, so if it won’t be a big hit on your wallet then upgrading their monitors would also be a good gift",Positive
AMD,"For ""professional"" e-sports, fast paced shooters 1080p with high refresh rate (Hz) is often still preferred over 1440p. Usually much higher framerates can be achieved on the 1080p setup. So they will be fine with their 1080p for now!.  1440p high hz is starting to catch up. For any non e sports games 1440p is the sweet spot!   In any case I would go for 120Hz as a minimum.  Good luck!! Merry Christmas. You are an amazing mom.",Positive
AMD,"Good info, thank you",Positive
AMD,"Looking at pictures and other threads it should be totally fine. I wouldn't worry, it's a bit cheaper too.",Positive
AMD,It's not an issue. The area around the cpu is laid out the same on micro and full atx boards anyway.,Neutral
AMD,"Agreed. As I said though, that'd be my personal preference. Think I was pretty clear that the parts they've chosen would suffice and they didn't need to alter anything no?",Negative
AMD,I did a pc build last week for my nephew which used the exact same CPU and GPU as you. I went with the Gigabyte B650 Eagle AX if you wanted a full size motherboard that isn't too expensive and has WiFi included.,Neutral
AMD,"Any that mention ""B650"" will be fine with what you've got. It's the latest generation of mobo. More expensive (and unnecessary) model above that is the B850 - you don't need this. Don't go for a Z790 or Z690 or 'Zanything' as these are older gen and no cheaper and can limit upgrades down the line / compatability.",Negative
AMD,"The same GPU is going to perform the same across brands. Anything about factory overclocks or upgraded cooling is mostly just gonna be marketing bs. The main reason to pick one brand over another is going to be warranty/after sale support. And even then, if I'm being completely honest, most brands in the PC hardware space are absolute trash in that regard.   When it comes to AMD hardware, Sapphire and Power Color are considered the best when it comes to warranty and overall value.   If you're really planning on buying more graphics cards and returning what you've got coming, it's worth mentioning that Best Buy and even Walmart carry a limited selection of PC hardware these days. Your mileage may vary when it comes to selection. If you have a Micro Center within driving distance, that'll be your absolute best brick and mortar store.   Since you've never done this before AND it's not really YOUR thing, I'd also recommend looking into hiring some help to do the actual build. Its not all that hard to build a PC, but since you're on a bit of a time crunch I could imagine a scenario where you encounter a small hiccup (it happens) and get overwhelmed/stressed out by the whole thing. The aforementioned Micro Center, if it's an option for you, will offer build services, but a mom and pop shop or even just an enthusiast in your area should be able to do it as well. The best case scenario would be finding someone that would walk you through what they're doing as the build it, just in case you have to troubleshoot in the future. That being said, if you're handy and can follow along to a tutorial on YouTube, you should be able to handle it. Linus Tech Tips has a ""last guide you'll ever need"" video that walks you through building a PC. It's a few years old, but the point was to produce an evergreen guide that would remain relevant for years to come.",Neutral
AMD,"No worries, despite its low price, that cooler is surprisingly efficient and already overkill for that CPU. As a matter of fact, that cooler was designed for board-roasting Intel chips so if it'll cool an Intel, then it'll freeze an AMD.",Positive
AMD,Consider 240. One more fan and rad space. But i wouldnt worry about it now. Just make sure there is good airflow to get rid of the heat that gets trapped in the case and it should be fine.,Neutral
AMD,"Awesome, thank you so much!!!!!!",Positive
AMD,Zanything would be an intel socket anyway.,Neutral
AMD,"GPU is going to be the biggest issue for most modern games, then CPU. I don't know what you are looking to spend but an RTX 3060 12GB or RX 9060 XT (preferably 16GB, but if budget means 8GB ok) would be the options I would look at.  CPU wise you can update your motherboard BIOS and then slot in a 5000 series CPU, 5600/5600X/5700X/5800X/5800XT are all good options.",Neutral
AMD,"While 32GB of RAM is recommended these days, 16GB is still enough for most gaming. I would say the weakest part of your system right now is the GPU, followed by the CPU.     What motherboard and power supply do you have? That will limit what your GPU and CPU upgrade options are.",Neutral
AMD,"Not sure what speed your RAM is, but the 16gb should be totally fine. The weaknesses here are the CPU and GPU. I’d start with the GPU.",Neutral
AMD,"You don't have anything that isn't overdue for an upgrade, but I think the clearly weakest link in this pretty weak chain is that GPU.  I would say second-choice should be storage, third CPU, fourth choice RAM.  Your GPU literally can't play a \*LOT\* of modern games. If you could pickup a second-hand 12Gb 3060 your whole rig would be \*massively\* improved or if you have the budget for it the 9060XT 16Gb (avoid 8Gb graphics cards, they're all universally terrible value).  After that upgrading to a 5600 or something like that would also give you a huge boost and at that point you'd have actually a quite decent PC.  Then there's storage - playing games from a HDD is just not a good idea anymore but your SSD is so small you're only going to fit one game on it at a time.  You could do with more RAM, but actually 16Gb isn't all that big of a problem. It does hurt you at times, but if you're running games from the SSD it shouldn't hurt you a lot - and certainly it's \*MUCH\* less of a problem than everything else.",Negative
AMD,Listen here sonny boy...  *speaks in 6700k and R9 fury*,Neutral
AMD,Rtx 5060 / 5060ti or a rx 9060xt. Then a cpu upgrade to a 5000 series later or now if you can squeeze in a 5600 / 5700x depending on budget.,Neutral
AMD,AM4 and 3000 series GPU,Neutral
AMD,"RAM is stupid expensive right now, I wouldn't recommend upgrading that because it'd be very expensive, and assuming your motherboard uses DDR4, you wouldn't be able to transfer it to newer systems that use DDR5.  The main question is where is your system lacking for your needs? The size of the SSD is a bit small, so if you're using the HDD for a lot of things, getting a larger SSD will be a noticeable improvement. Otherwise you can open up task manager and see if the CPU or GPU are maxed out in the stuff you run, and update the thing that's the bottleneck.  Another option from someone rocking a 5 year old build is to upgrade your peripherals/accessories. A nicer desk, monitor, chair, mouse, and keyboard made a huge difference for me.",Negative
AMD,The best course will be a cheap 6 or 8core 5xxx series CPU ( avoid g series since they dont perform as good due to lack of L3 cache ) which means either 5600/5600x/5700x/5800x/5800XT whichever fits your price ( dont go spending more than 150-160 for one though  Gpu wise a 9060Xt will be a massive upgrade. You can find cheaper 8gb gpus but be aware of the limitations of 8gb vram   A bigger  nvme drive is also a necessity since that 238gb drive will fill after 2-3 AAA games and installing a modern AAA game on an HD is asking for trouble   With Ram you can still get away with 16gb but barely and that is if you are not multitasking while gaming so a move to 32 is still recommended though not necessary,Neutral
AMD,GPU will have the most impact because my brother in christ it is damn near 2026 and you're playing on 4gb card made almost 10 years ago.,Neutral
AMD,"Honestly, unless you do a ton of multitasking, streaming and heavily modded games, RAM should be the lesser of your concerns. Sure, 32GB are better than 16, but as someone whose lazy ass has still not installed on my main PC the 32GB kit I have on my desk since summer... yeah, 16GB are still more than enough for most uses.  The GPU is probably the weakest part in your rig for modern gaming, especially considering it has just 4GB VRAM. Then next step would be the CPU, which while still fairly decent, could fall short for more recent games. I had the same 2600X until three years ago upgraded to a 5800X3D I got on a good deal. That's around the best thing you can get for your current AM4 motherboard (5700X3D, too).   Honestly, changing GPU you should already notice a nice improvement. Then CPU will nail the rest of the jump when you can afford it. About RAM... well, considering how stupid prices have become, you can always try and look for deals, but don't get crazy for it if you are on a tight budget. I'd totally prioritize GPU first.",Neutral
AMD,"Ryzen 5600 and intel arc b580.  Yes an ssd for bulk storage would be nice, but I wouldn't prioritize it in the budget.  32gb of ram would also be nice, but ram is super expensive right now and 16gb is generally enough.  As someone else pointed out. Upgrading the cpu would depend on the motherboard model. Some prebuilts don't have a bios compatible with newer gens(though some do if you know where to look) it would be helpful to know.",Neutral
AMD,"Ram is absolutely ok for this pc Just buy new gpu, something like secondhand 3060(ti)/4060",Negative
AMD,I would grab what parts you need asap. Ram prices are skyrocketting as are data storage like ssd and cpus and gpus. Every component for computers are now essentially going to ai and it has the supply/demand matrix running overtime and fucking us normies over big time,Negative
AMD,"Depend on the mother board, if the mainboard support Ryzen 5 5600 it (which usually is for AM4 system), it will be substantial upgrade, Avoid the low end Nvidia 5000 series though, they are anemic with only x8 Pcie, which is fine if your mainboard is PCie 4.0 or 5.0, but its looks like your system is 3.0 AMD GPU (6600,6700 or 9060 )is the safest bet for upgrade",Neutral
AMD,"What’s your motherboard?  Can it support 5000 series CPU’s?  Your single best upgrade will be the gpu, it was quite literally the bottom of the barrel even when new, and is 100% your biggest bottleneck.  After that, upgrading to something like a 5600x would produce a noticeable bump if your mb supports it.  Otherwise slap a gpu in it, and start saving for a platform upgrade and hope ram prices stabilize.  Your ram capacity is fine currently, so I’d only consider that as part of a whole platform upgrade.",Neutral
AMD,"I built my gaming PC in 2016. CPU i5 6600, DDR4 16GB with 1050Ti.   2 years ago, instead of building a new rig I upgraded my GPU to RTX 4070.   Most of the struggling games issues are resolved.   Even though 16GB RAM is fine, I upgraded to 64GB (because VM/Containers) early this year.   I don't have much option to upgrade my CPU without replacing mobo.",Neutral
AMD,5060ti/9060xt 16gb and a 5700x3d and you're good for a good while,Positive
AMD,"While GPU, CPU upgrades are recommended, check motherboard for what PCI-E lanes are available for GPU and M.2 slots. If you are using a low end motherboard from 2018, you might neeed to change mobo to get the most out of your new GPU. Also check PSU compatibility beefore buying GPU",Neutral
AMD,"Graphics card will be the most expensive to upgrade, but probably make the most difference - a 16gb 5060ti or 9060xt  Ram is fine, I'd leave it.   Doing the cpu isn't a bad idea as your thermal paste is probably about due to be redone anyway- a 5600x would be good. The 5700x3d and 5800x3d are now getting wild prices even on ebay so I'd just not bother.   A 2tb ssd to replace both your current drives would also be good, quite a lot of games assume they're installed on ssds these days.",Neutral
AMD,"GPU always first. Get what fits your budget,. Either new current gen GPUs (minimum 8GB, preferably 16GB) or used (cards like the 3070, 3060 12GB, RX 6600, RX6700XT). These would all be a significant step up from your current GPU.   Next step would be to upgrade the CPU to an RX 5600 or highter. This would also be a noticable improvement, but not as big as the GPU.",Positive
AMD,"Upgrade your GPU you can get a budget GPU that'll beat the pants off what you're running now.,",Neutral
AMD,"Upgrade your cpu to a 5700X, 5700XT or 5800XT. Depending on your current cpu cooler you might have to upgrade that as well, but you can get inexpensive but very good coolers like the Peerless Assassin.  32gb ram would be great, but you can still manage with 16gb. Upgrade this if you have the budget for it.  The gpu needs updating that’s for sure. If you are strapped for cash I would suggest a used Radeon 6600. Otherwise moving up from there an intel ARC B580, next GeForce 5060 ti 16gb or Radeon 9060 XT 16gb, or higher models if you have the budget.  I either one of these things would keep you going for a good while. But keep in mind you might be forced to upgrade the cpu along with a new gpu because otherwise you would be cpu bottlenecked.",Neutral
AMD,"What's your budget? Everything in your system is outdated yes, but actually RAM is the least of them all.  You could drop in a used 5800x CPU for kinda cheap, or get a new SSD for kinda cheap. A new GPU would be more expensive, but arguably more bang for your buck for games.  Or if you have a giant budget and are near a microcenter, I'd say just build a brand new computer with their 9800x3d combo - but this all dependent on your budget.  P.S. And what is your power supply?",Negative
AMD,"Sell the whole thing,  amd start fresh.  Go to microcenter.com  but at a physical store, pickup 7600X3D combo wirh motherboard and ram.  That computer would need everything new, and upgrade money can be spent better on awhile new platform.  It all depends on your budget",Neutral
AMD,What's your current budget for your upgrade,Neutral
AMD,"How much are you willing to spend?  For games, you'd want a better GPU to start with. The 16GB 9060XT Gigabyte Gaming OC is currently $379. Really good 1080p and 1440p performance, and even some fairly decent 4K performance if you compromise on graphics settings (also using FSR).   After that, the CPU and RAM will be your next restriction - which can be easily done by upgrading to a 5700 or 5800XT and find yourself a used 2x16GB 3200CL18 kit (or even 3600CL16)  Just that combination will keep your PC going for the next few years until you're really really looking to build a completely new rig.",Positive
AMD,It does depend on how much you want to spend but I would get a 5700x and a 9060xt 16gb.  If you can swing it get at least a 1TB NVME drive but they have jumped in price the past few months.,Neutral
AMD,"Depends on your budget. If it's somewhere around £300, i'd suggest going for a 5600X + ARC B580   If £200, a used 3070 or 3060 Ti is also a solid shout.",Neutral
AMD,"Well... Depending on your Mainboard, I would certainly think about a CPU update. Most pressing issue would be gpu. I do have 16 gigs of ram as well, and not yet experienced a problem due to that. Therefore I would maybe wait with ram.",Neutral
AMD,First the cpu to a i7 14 gen with integratet graphics should be good then a like 4070 on used market,Positive
AMD,Sell it for $200 ish bucks on FBM and buy a $500 used PC on FBM. Total cost to you is $300 and you can get a decent upgraded pc,Neutral
AMD,Sell it for $200 ( local prices may vary) on FBM and buy a $500 used PC on FBM. Total cost to you is $300 and you can get a decent upgraded pc,Neutral
AMD,"I did something similar to this over black Friday;  b350 pcmate MOBO, 2200g APU/CPU (no GPU build) to a $89 5600x off of aliexpress and a $330 9060xt 16gb.",Neutral
AMD,"I’m rocking a 3060 12 GB with a 5800X and 2 1080p monitors rn, it’s definitely great for a low budget!  I can play most older single player games on High graphics settings, and newer ones with low-medium settings and they still look great. Stellar Blade for example I can run on decent medium to high graphics settings because of the extra VRAM.",Positive
AMD,"5700x3d if used and for a decent price is a good option too. Facebook marketplace is good for that, depending on region.",Positive
AMD,How do you find out what type of motherboard you have? I bought a prebuilt and am pretty terrified/against opening it up and taking it apart. I got a i5 with geforce rtx 3060 is all i know. Not sure how to see my ram info either. Thanks in advance,Negative
AMD,"Dude, ram is an easy upgrade lol.  But I agree that 16gb is still generally enough. Especially for an entry level machine.",Positive
AMD,"16 GB of RAM is still enough to get by get by with for most gaming. You won't see a huge improvement going to 32 GB DDR4 in most games. With your current GPU/CPU, more ram will do almost nothing for you.",Negative
AMD,"The exact type of motherboard you have probably doesn't matter much, the main things that would matter is what type of RAM it supports and what CPU socket it supports. I believe your CPU only supports DDR4 RAM (so that's what the motherboard supports), and the socket for that CPU is AM4, so if you wanted to upgrade the CPU and keep the same motherboard, you'd just have to make sure the new CPU has the same socket.",Neutral
AMD,"RAM has become expensive as heck, tho. If OP is in a tight budget (literally said right now can probably upgrade one thing only), I wouldn't worry about RAM atm.",Negative
AMD,"I agree, I was responding about you having a ram kit sitting on his desk for half a year without putting it in.",Neutral
AMD,"Oh it was about me, damn, lol.   Yeah, I've been really busy with shit for the last months, and since I use that computer a lot I just haven't found the time to disconnect everything and do proper maintenance (clean dust, change thermal paste... you know) which I want to do altogether with the RAM upgrade. It's been a kinda hard year, so whenever I have free time with my computer I just evade myself from responsibilities and plunge into videogames or something instead. For better or for worse, that's why I know 16GB are perfectly fine for most uses tho.  Now I also need to replace my last secondary storage HDD with a SATA SSD, too, so I'll do it soon, for real this time lol.",Negative
AMD,I get you man. Especially since I have kids.,Neutral
AMD,"That's how it's intended to be powered.  The 3rd 6+2 pin connector is unnecessary anyway, most 9070 XTs only have two, since the TDP is ~320 watts.",Neutral
AMD,"I'm currently using 2 x 6+2 PCI cables with one of them daisy chained for my Asus 9070 XT Prime. No issues whatsoever, even though the card itself has 3 x 8 pin connectors.",Positive
AMD,"daisy chains are fine and have been forever  you just get more stable power delivery with individual cables, but that rarely matters.  Like the 3000 series with their insane transient spikes is the first time it ever became an issue",Neutral
AMD,"Been using a 7900XTX for over 2 years on daisy chained 3x6+2. Been wanting to change those for custom cables for ages, just for looks. Never got around to it. No problems, ever. And that card is pulling constant 450W during gaming.",Neutral
AMD,"My PowerColor 9070 XT has 2 8 pin spots. I use 2 separate 6+2 PCIE daisy chained cables, with each daisy chain part just kind of dangling. Not great for looks, but idgaf, and its working fine. Also worked fine on my Vega 56 card before",Neutral
AMD,Yeah you can use a daisy-chained cable and just let the leftover header hang loose. It's very common.,Neutral
AMD,"Thank you for a quick reply.   I trust that the cables are usable, yes, but do you suggest that I use 2 individual 6+2 cables and 1 daisy-chained 6+2 (using the 1 connector without the daisy-chained addition)?  Or should I go 1 individual 6+2 and 1 daisy-chained 6+2 + 6+2?",Neutral
AMD,"So you did the thing, right? 1 standard 8 pin (6+2) and then another one that is 6+2 daisy chained to 6+2 right? 2 cables for 3 connectors?  Everything looks stable?",Neutral
AMD,"Thank you for a quick reply, too.  Same question: Should I use 2 individual 6+2 cables and 1 daisy-chained 6+2 (using the 1 connector without the daisy-chained addition)?  Or should I go 1 individual 6+2 and 1 daisy-chained 6+2 + 6+2? I am not an expert regarding this and it's bothering me a lot...",Neutral
AMD,"That's good to hear, 7900XTX is a beast of a card. I hope all will be good in 2-3 more years!",Positive
AMD,"How much power can the daisy chained cable put out, though? 150W or less?",Neutral
AMD,>  1 individual 6+2 and 1 daisy-chained 6+2 + 6+2  Just do that.,Neutral
AMD,"Yep, that's exactly what I did. Everything is rock solid.  A single PCI cable is rated for 150W but can do closer to 300W in reality (depending on wire gauge). So two cables with one of them in daisy can easily do 500W sustained if you count the PCIe slot itself.",Positive
AMD,"Oh yes, me to. Can't afford an upgrade to that card.",Negative
AMD,"As far as I know, the connectors are rated for 150 W, so, barring defects or improper connection, at least that.",Neutral
AMD,"Ooof, I can breathe easily now! Thank you for the math there, I will proceed to order the card tomorrow.",Positive
AMD,"Sell a kidney, buy a 5090 :P Nvidia won't mind if you do... Apparently, even AMD will increase the card prices of the 9000 series from January 2025 :/",Neutral
AMD,"Oh yes, we PC hobby types live in grand times. I am lucky, my 3440x1440 old school monitor only does 100Hz. I will just keep using that. No need to get better PC hardware in that case.  Good luck figuring out your path forward. ;)",Positive
AMD,"This is all pretty neat :) I'd say the value and synergy was very good, especially that 7900xt. I'm honestly most surprised that you were able to get the cpu-mobo-ram for less than the cheapest Micro Center bundle with at least 32 gb of ram (500$).",Positive
AMD,where the heck did you find 32 ram for $190?,Neutral
AMD,Could have just gone to Microcenter,Neutral
AMD,"I did have to go used on a majority of components, albeit most in pretty much open box condition. I’m kinda bummed on the fact I could’ve gotten a Ryzen 5 9600x for 15$ more.",Neutral
AMD,"Found a dude selling mobo and ram combo on Facebook, 290 total.",Neutral
AMD,"True, but even a CPU/RAM/MOBO bundle that is closest to my specs would have increased my budget by at least 70 dollars, still amazing deals tho.",Positive
AMD,New?,Neutral
AMD,"Ram used, Mobo open box.",Neutral
AMD,Okay that's a great deal man. I paid 300$ for one stick recently 32gb 😹,Positive
AMD,Since it's ur first build did u use standoffs ? If the board is touching the case it will be shorting out. The board can't be touching the case at all.,Neutral
AMD,Is the CPU fan on the correct header?,Neutral
AMD,Does anything happen when you try to short the power pins? Are you certain you are shorting the right pins?,Neutral
AMD,"Is the cables that run from your case into your motherboard inserted properly? Like the power switch, reset switch etc.   It's a bunch of singular connections you gotta plug in for your case buttons to work  Also known as Front Panel Feature cables.",Neutral
AMD,"What do you mean it wont power on, exactly? Is it not booting, or no power at all? Check the power switch on the back of the power supply. Your troubleshooting steps seem to be non-boot though?  If there is power, but not booting, how long are you waiting? AMD systems do memory training and can take a few minutes sometimes to finish.  Speaking of memory, check and see if your ram is seated properly. Even LED ram can have the leds lit but still not boot if they aren’t fully seated.",Negative
AMD,We need a picture or video on that one,Neutral
AMD,Make sure psu cable to motherboard is fully plugged in. I had to press it in surprisingly hard,Neutral
AMD,Did you test your power supply to make sure that it's not the problem?,Neutral
AMD,Standoffs and not switching on the power button on the PSU. Happens to us all.,Neutral
AMD,I did not use them but it does not appear to touch the case .,Neutral
AMD,Which one would that be ?,Neutral
AMD,We put it in CPU fan 1 and it didn't work,Negative
AMD,Nothing happens and yes I checked the manual .,Neutral
AMD,No power at all no leds turn on for any component,Negative
AMD,Not having standoffs is like trying to drive a car with no tires... It simply won't work... unless like the case is made out of plastic or cardboard...,Negative
AMD,It's a must it's not optional. The board is shorting out most likely. You can take the board out and try turning it on on like a cardboard box. If it turns on than you have your answer.,Neutral
AMD,"Can you share photos of the build, general photo of it all, then close ups of the status LEDS, all power connections both at the PSU and components",Neutral
AMD,Looking at the mobo probably above the RAM slots. It'll say CPU fan or something like that. Not pump or opt,Neutral
AMD,I'm very interested in seeing if we can get this working... Please make a video or pictures at the very least showing everything for us please. Show the wires and ram,Positive
AMD,Just tried it on a cardboard box with CPU and cooler and it did not work,Negative
AMD,Isn't it possible he's fried his mobo if he powered it on while screwed directly into the case? Seems to me that there could be all kinds of shorts.,Neutral
AMD,I have currently disassembled the build for trying to short the motherboard on a cardboard box but I have to sleep. Tell me what pictures or video would be helpful and I'll try to upload some tomorrow,Neutral
AMD,Iv grounded out boards a cupple of times and after fixing the ground (like a screw that fell under) it worked again but ya it's possible it's fried but normally that's not the case..... But if it is id try returning the board if it's in the return window lol...iv seen this same mistake with new builders not using stand offs quite a few times but nowadays most new cases have the standoffs already installed so it's not as common anymore..,Neutral
AMD,5800x3d but its like 300-400 dollars   5800xt for the money like 150-180 dollars,Neutral
AMD,"For gaming 5800X3D, 2nd best 5700X3D.",Positive
AMD,"Get a 5800xt, 5950x or 5900xt depending on your budget.  All great CPUs.  What do you have now?",Positive
AMD,"I think the 5800x3d is the best for gaming, not for production or stuff like that but thw massive cache size is great for gaming.  And you can save a mortage buying ddr4 instwad of 5.",Positive
AMD,"5800x3d is the best, by a good margin.  It's also $400 - 500.",Positive
AMD,5800x3d. Good luck picking one of these up these days though. They are no longer in production.,Positive
AMD,Just got a great deal on a 5800x3d comming from a 3700x and damn that was worth the upgrade,Positive
AMD,Best am4 you can get right now would be a 5800x3d,Positive
AMD,"I have r9 5900x, quite a big difference from r5 3600. Which motherboard you have? Make sure to update bios for latest cpu support before",Neutral
AMD,"If it's for gaming, 5800X3D is the cream of the crop and will decimate any other CPUs in AM4.   X3D variants are a must have if you're gaming oriented and especially if you play MMORPGs and/or games with mods.",Positive
AMD,Go with a 5800XT.  It'll be a huge upgrade for you and is still very affordable.    The 5800X3d is the best gaming CPU but they haven't been produced in a while and their prices are pretty high these days.,Positive
AMD,"5800X3D/5700X3D, but they've both been discontinued and prices have skyrocketed. They're not remotely worth what people are charging now. 5800X/XT if you can find that.",Negative
AMD,"LOL, ""ride your unicorn to a magical castle where they sell 5800x3d without having to consult with a loan officer.""  I personally went w a used 5900x, they can be had for about $160-180. Plenty of cores for any computing tasks or games that can use them. At worst you get the same performance as a 5600x, and in some titles as much as a 10% boost",Positive
AMD,"I was at microcenter today for a very similar question. They didn’t have any x3Ds. I ended up with a 5800XT that came with a heat sink for $180. They had a 5700X for $160 without a heat sink, but the guy said he couldn’t think of any reason to go that route.   I currently have a Ryzen 5 2600X so I think it will be a nice upgrade.",Neutral
AMD,"Well thanks for everyone’s recommendations! I got a 5800x3D! It’s unopened and new off of eBay so let’s hope it’s not a repackaged ripoff, how can I tell if it’s real when I get it?",Positive
AMD,cant find them in europe,Negative
AMD,Right now I have a Ryzen 7 3700x 8 core,Neutral
AMD,I’m pretty sure I have a Msi B550 tomahawk,Neutral
AMD,"Make sure that you update your motherboard's BIOS to the latest version.   When installed, the 5800X3D should show 96MB of L3 cache and run at the default clock speed of 3.4GHz. And have 8 cores and 16 threads running.",Neutral
AMD,"Probably not, with ram prices so high, AM4 is in high demand",Neutral
AMD,"dont get the XT ones, a 5800XT is like only 5 fps better than a regular 5800X and they usually costs more as well, if youre purpose is gaming a 5800X and the XT ones’ performance difference is literally unnoticeable. I would recommend getting a 5700X.  If you are willing to pay for the 5700X3D or 5800X3D, they are the best on AM4.  Note : a Ryzen 5 5600X has the same *(not exactly but their difference is sooooo small its literally unnoticeable)* gaming performance as the 5700X 5800X 5900X and 5950X on 95% of games  Lets say for example = if your only purpose is gaming and you decided to get a 5900X instead of a 5700X or a 5600X, you’ll just be paying more for almost no performance increase. Get a 5700X or a 5600X",Neutral
AMD,Thank you so much!! I appreciate it!,Positive
AMD,"what is going on out there? why are ram price so high? i wanted to build a new PC but now it cost 4x more, has it do to with AI?",Negative
AMD,The higher stock clock speed on the 5900 doesn't make a difference?,Neutral
AMD,Seconding all of this. I just did an AM4 upgrade to a 5600X. Everything I found indicated it was the best price to performance for gaming.,Positive
AMD,RAM manufacturers are prioritising making memory for AI datacenters so they’re making less consumer ddr5 ram or pulling out of the market entirely like micron. There’s less supply so it’s more expensive,Negative
AMD,"If you already have a 5800X, forget it. The X3D variant has been out of production for a while and is highly sought after. Those that are available have a huge price tag that is simply not worth it. With all the hardware crisis going on, you better pucker up because you are stuck with that pc for at least two years. You are better of upgrading your gpu, or something else in your pc to increase longevity, like a new psu, better cooling. Or undervolting. The 5800X aren’t very happy with undervolting, but even a tiny bit goes a long way.  EDIT: just saw you have a 3080ti. Just keep using that. If you wanna spend money on your pc, get another 16gb ram while you still can. Or another SSD before they explode in price.",Negative
AMD,"it would be a bump up in performance, but not worth it. Grab another 16gb of ram and rock your system throughout the ram crisis.",Negative
AMD,"I don’t think so man. Unless you have $300 to burn for a used CPU with marginally better performance, that’s really the only case it makes sense.",Negative
AMD,I have a 3080TI and 16 GB DDR4 3200mhz just to add that,Neutral
AMD,What sort of performance are you looking for? I imagine you're hitting around 150 FPS on near-max settings.   Do you need more than that?,Neutral
AMD,"When you say ""optimal performance"" what do you mean exactly?  Does the game run smoothly at 60fps or above?",Neutral
AMD,"I’ve got a Ryzen 7 5700x and an rtx5060ti 16gb, depending on how much vram you have it could be that. Before upgrading my gpu I had 12gb 3060 and even with that extra vram I close all other applications when playing bf6 otherwise my vram usage was maxing out on mid-low settings. I found that upgrading the gpu fixed that issue and the 16gb vram is enough to keep my browser open when playing bf6 but watching yt at the same time does halve my fps so I stick to music",Neutral
AMD,Save up for a new system. The bump would be small.,Neutral
AMD,"At 1080p High with a **5090** the 5800x3d gets 142fps average and 106fps 1% lows. Meanwhile, the 5800 gets 111 fps advantage and 83pfs 1% lows.  How ever since a 3080 gets an advantage of 106fps at 1080p overkill with a **9800x3d** i would imagine it would be a negligible performance increase unless you play at 1080p low.   https://youtu.be/nA72xZmUSzc?si=NlbhpUm7OzP-G1Kt  https://youtu.be/RP0rfOP5iAk?si=7jt6MTWCiiU_zyQs",Neutral
AMD,"Get 32gb of ram, a 5800x should be plenty for BF6 at a solid 100fps",Positive
AMD,At the price they sell 5800x3d used now no not worth it at all.,Negative
AMD,"I just upgraded from a 5800X3D to a 9950X3D after finding a decent deal on ram on Marketplace...  There's an improvement sure, but it wasn't the game changer I was expecting for gaming. (I do a ton of video and photo editing though so it's absolutely worth the upgrade for that).  The 5800X3D is quite a chip. Ran it with my 5090 for a bit and didn't really feel very bottlenecked.  I'm throwing it up on marketplace soon as they're getting quite high prices, will offset my upgrade cost.",Positive
AMD,"5800x3d wont give you optimal performance, and its a bad time to upgrade to AM5. So you will have to sit this one out.",Negative
AMD,where do you even get it for a reasonable price,Neutral
AMD,Just turn up your graphics settings and toilet your GPU do the heavy lifting,Neutral
AMD,Not really bro.  Start saving your pennies for an am5 upgrade!,Neutral
AMD,"Idk  what these replies are saying, going from a Non-X3D to an X3D, especially in CPU intensive titles will always be a reasonable upgrade. Yes the 5800X3D is expensive right now on the Used market but keep in mind, its still waaay more cheaper for you to get the 5800X3D than to upgrade to AM5 platform right now.   You have 3 options 1 . Keep the 5800X  2. Upgrade to a 5800X3D (you will spend some money)  3. Upgrade to AM5 (you will spend a SHITLOAD of money)  Choice is yours👌",Neutral
AMD,"Can you drop your thermals while gaming and how much ram is being used under typical load? ""double your ram double your ram!"" imo is terrible advice until you've ruled out undervolting your cpu/gpu. Are you worried about frame rate or latency when you're talking about ""optimal performance"" ? Do you have a gsync monitor and what resolution are you running",Negative
AMD,"Not OP but I have a 5700X with a Thermalright Phantom Spirit 120. CPU rarely goes above the 50s celcius, probably peaks at 62 degrees under high load. I’ve applied an auto overclock via Ryzen master, would I be better off undervolting instead (or alongside a clock speed increase)? I thought undervolting only helped if you were reaching your thermal ceiling and boosts weren’t maxing out as a result. Are their other benefits to it that I’m unaware of?",Neutral
AMD,"> The X3D variant has been out of production for a while and is highly sought after  Oh for real? I sprung for it awhile back because I heard the larger cache helped poorly optimized games which feels like most things these days.   Between that and me buying 64gb of ram about 8 months ago, my PC purchases are surprisingly well timed.",Positive
AMD,Double your RAM and leave the CPU alone,Neutral
AMD,"I got 32 GB RAM and the 5800X, the CPU is rarely ever at full capacity with exception of Ultimate Epic Battle Simulator. Another 16 GB of RAM would do you better probably, and it’s also cheaper than the 5800X3D.",Neutral
AMD,"I will say, since 90% of my gaming is sim racing in VR, moving from the 5900X to the 5800X3D was like a generational leap. All the stutters disappeared and the CPU frame times dropped and stayed drastically more consistent. The X3D chip is absolutely a worthy upgrade, but not at current prices. And if you aren't playing in VR and have a G-sync enabled display the frame inconsistency is way less of an issue.",Positive
AMD,"but then i gotta upgrade the cpu i gotta do the ram which is crazy expensive, motherboard and cpu :( im a student my pockets not that deep",Negative
AMD,might just wait until AM6 to buy AM5 parts lmao,Neutral
AMD,"It uses less power for starters, its not just lower temps. So that gives it more room to boost higher. Usually you would ""just"" enable PBO in the bios and a negative curve optimizer, then go down with 5 each increment until its not stable anymore, then go back up another five and thats it. The X3D cpu's can be undervolted a lot, the regular ones not as much. Most should support -5 just fine. Everything else less than that is pure luck of the draw.",Neutral
AMD,Just let it do its thing you wont gain much.  Edit: fixed game to gain.,Neutral
AMD,"With ramaggedon though it gives you plenty of time to save, the longer the ai bubble lasts the more you can save for a new system",Positive
AMD,"This is really helpful, thank you for this. Any particular cpu stress test you recommend to test stability?",Positive
AMD,How much more performance would that get you?,Neutral
AMD,But I will game much,Positive
AMD,"Personally I would just use cinebench, but there are other more demanding and more thorough stress tests out there like OCCT. Most would recommend that.  It’s just enable PBO, set all core negative curve optimizer, start with 5, test, and keep incrementing with 5, stress testing each time. The moment it becomes unstable decrease with 5 and test again. And that’s it. You eventually settle on a number and if it ever starts to misbehave go back another 5 but at this point it SHOULD be rock solid.  You can tweak and push it even further but this is the most simple variant and it will still yield good results.",Neutral
AMD,"It’s nothing groundbreaking but it’s the easiest and safest way to gain more performance and on top of it less heat and power. The speed you gain is dependent on a lot of factors like cooling, cpu binning, PBO settings and etc.",Neutral
AMD,Gain* sorry typo.,Negative
AMD,"Back in the day, you could get a Celeron to run almost as good a a Pentium with overclocking.  Early Athlons were also pretty good when overclocked.  And it wasn't that hard to do.  I overclocked a bit back then and it paid off.  But today, from what I've seen, it seems to be a bit more complicated with a lot less return.  But I have some free time over the holidays, so maybe I'll look into it more.",Positive
AMD,"Its not really a case of “new thing bad”, it’s that the philosophy behind the releases that’s changed. If we take a part like the ryzen 5800X, you don’t get a lot of overclocking headroom because it’s basically already running into the ceiling of the architecture out of the box. That’s why you have to go the route of undervolting to push it further. Heck, I’ve heard of some unlucky folks who had to OVERvolt their CPU’s to get the system stable.   If you take the Athlon 1700+, a legendary cpu for its overclocking potential, it could overclocking so good because it was perfectly good too end models that was simply downclocked. The demand for low and mid end cpus was so high amd (and Intel) had to resort to this.   Now on the other hand, it’s more controlled and low to mid end CPUs are almost always high end CPUs that didn’t muster it, and here is the most important part: if it’s a good high end that has to live as a 5500x for example, then the extra cores etc are fused off on a hardware level.",Neutral
AMD,">If you take the Athlon 1700+, a legendary cpu for its overclocking potential,  I do believe that was one that I overclocked.  So easy and so much more performance.  I had a Celeron before that that was equally overclockable.  Ah, the good old days.  Yeah, I understand about the binning and stuff.  That aspect hasn't really changed that much, but your options for over-volting and under-volting and setting power curves and frequency manipulation, most, if not all, of which can be done in BIOS.  We had a jumper that gave you two or maybe three options for the base clock and another jumper for frequency multiplier and after that it was just having a cooler good enough to keep the CPU from overheating.  I knew one guy that built his own liquid cooler and drilled holes in his case for the tubing and mounted a radiator on the top of his case.",Positive
AMD,Very happy to see this ship. This also brings life to the GPUs in the old Mac Pro trashcans as they share the same architecture. So good to see.,Positive
AMD,Linux devs doing sidequests again,Neutral
AMD,Old is selling it short.,Neutral
AMD,"I have a R5 240m, worth switching it from windows 10?",Neutral
AMD,ive hd7950 and i feel noticed.,Neutral
AMD,Neat!,Neutral
AMD,It'd be nice if the R9 M70X in the Macbook Pro 2015's also end up being supported. They are GCN 1.0/Southern Islands too.,Positive
AMD,"Yeah, it seems like somewhat of an understatement.",Neutral
AMD,"It's good news all around, especially in this era where hardware shortages and the accompanying price hikes make people instinctively want to hold onto what they have for as long as possible.  Moreover, having viable alternatives to Windows when Microsoft seems dead set on forcing its user base to sunset perfectly good hardware for nebulous benefits is also a good thing.",Positive
AMD,"Definitely, with windows 10 being end of support and if the computer is unsupported on windows 11, you don’t really have a better option",Negative
AMD,"There is essentially a distro for any needs. I wanted something newer and gaming focused, ended up getting Pop!_OS rather than having my PC running Windows 11 when I bought these parts earlier this year… never going back",Neutral
AMD,What if I play counter strike?,Neutral
AMD,"You shouldn’t have any problems, it’s been supported natively on Linux for a while.",Positive
AMD,"Runs natively on Linux a lot better now than at the launch. A lot of it is down to the poor nvidia driver performance at times causing problems for folks with their GPUs, but there are plenty of additional optimisation guides available for this now (because yay for being able to customise your system). One other reason why I like AMD GPUs is the driver stack works so well on Linux.",Positive
AMD,The 5060 by a fair margin because it’s more recent and has access to newer technology. What price are they though? What about something on sale like this  https://www.walmart.com/ip/iBUYPOWER-Element-SE-Gaming-PC-Desktop-AMD-Ryzen-7-8700F-NVIDIA-GeForce-RTX-5060Ti-8GB-32GB-DDR5-RGB-RAM-1TB-NVMe-SSD-ESA7N56T01/17048165781,Neutral
AMD,"Intel + RTX Combo, i don't trust longetivity of RX Radeon driver update unless it's Radeon 9000 series",Negative
AMD,"I don't like both of them. If the Case is the one pictured then im pretty sure the PCs are overpriced.  On the Ryzen CPU side i dont like the Desktop 8000 Series. So like it doesnt exist for me😂 Oh and i never heard of the RX 7700 16GB, i only know the RX 7700XT 12GB but could be me who missed out on that.  On the Intel side i dont like the RTX 5060. Would be to weak for me. When i buy a PC it should last a bit longer than 2-3 years in terms of upgrading.  And on BOTH PCs i dont like the 16GB RAM. I know it expensive atm but 16GB is nowadays the Office PC recommendation and 32GB+ for everything else.  Would love to see the prices tho cuz you cant recommend without knowing the price.",Negative
AMD,If you have some selfrespect you would choose AMD and your going to start using Linux as operating system.,Neutral
AMD,Have you considered this? its a $899 but you get the 5060 ti and 32gb ram. [https://www.walmart.com/ip/iBUYPOWER-Element-SE-Gaming-PC-Desktop-AMD-Ryzen-7-8700F-NVIDIA-GeForce-RTX-5060Ti-8GB-32GB-DDR5-RGB-RAM-1TB-NVMe-SSD-ESA7N56T01/17048165781?classType=REGULAR&athbdg=L1103&from=/search](https://www.walmart.com/ip/iBUYPOWER-Element-SE-Gaming-PC-Desktop-AMD-Ryzen-7-8700F-NVIDIA-GeForce-RTX-5060Ti-8GB-32GB-DDR5-RGB-RAM-1TB-NVMe-SSD-ESA7N56T01/17048165781?classType=REGULAR&athbdg=L1103&from=/search),Neutral
AMD,The Ryzen was 849.99 and Intel was 779.99,Neutral
AMD,The Ryzen was 849.99 and Intel was 779.99,Neutral
AMD,"Okay for only 70 more i would definetly stick with the Ryzen System. CPU is ~10% faster based on what you play, sometimes the Intel is faster, but overall Ryzen +~10%. The RX 7700 and RTX 5060 should be on the same performance level. But the RX 7700 has double the VRAM of the RTX 5060.  If you want to buy one of the 2 PCs:  -Raw Performance + VRAM Headroom: Ryzen 8700F + RX 7700  -If you care about CUDA, more modern Upscaling (DLSS 4, Multi FrameGen) and better RT: i5 14400F + RTX 5060   Edit: On both PCs i recommend upgrading to 32GB. Both PCs come with only 1x 16GB Stick of RAM. Thats a bottleneck in itself for the CPU.",Positive
AMD,"Given DDR5 is driving consumers away from AM5, it'd be an interesting way to get money out of AM4 owners who need an in place upgrade.",Positive
AMD,"That would make me feel very dumb for recently buying one on the used market 🥲 but holy shit, would be nice if it happened.   Problem is how the process is and how the chips are binned, could they really get enough yield to make it worth their while vs paying to flood the market with lower binned chips nobody wants to buy",Negative
AMD,Hell yeah. This and 5700X3D too.,Positive
AMD,That would be great/hilarious.  Keeping am4 as the all time GOAT platform.,Positive
AMD,Just give is the 5950x3d or 5850x3d.,Neutral
AMD,I'm here just waiting for a 5950X3D.,Neutral
AMD,Me with a 5700x from last year  https://preview.redd.it/6d5l1z4lh38g1.jpeg?width=640&format=pjpg&auto=webp&s=805759206ded804d65b7757c9c01b50b673c9a8a,Neutral
AMD,"Just crazy though they make new chip 5950x3d, make am4 great again.",Positive
AMD,I have the 5900x. Wonder if it’s worth it to get a 5800x3d if the release more,Neutral
AMD,What's that going to do outside of busy fabs that are already committed to other products?   The issue is that we don't have enough chip making things. Going back and making a line that's already retired doesn't solve the issue of not enough chip factories.   All it does is repurpose already strained to meet demand factories to meet some new demand that wasn't planned on.,Negative
AMD,Can’t believe I got $120 AliExpress 5700X3D a year ago,Neutral
AMD,i think that's why they launched the 5800XT. but yeah if they re-release a AM4 x3d chip would be cool. would be nice for my secondary system.,Positive
AMD,Here I am stuck with a 5600x. Next time I upgrade I'm buying the top fucking cpu. This bs will never happen to me again.,Negative
AMD,Fabs already moved on.. why would they go backwards.    They won't  Deal with it.,Negative
AMD,"AMD, don't fumble ffs! You have a golden opportunity.",Neutral
AMD,will there even be capacity to make new ones?  At best maybe they can find a few pallets of those saved up for warranty and sell them.,Negative
AMD,Are we/they?,Neutral
AMD,The 1080 of the CPUs.,Neutral
AMD,I couldn't find a 5800x3d or 5700x3d last year. Not sure if it's worth putting money into a new cpu if i already have a 5800x.,Negative
AMD,"In a normal world where business make products well so that people will buy them, this is a no-brainer. As a chef, if we don't have enough of one ingredient but a lot of another, guess which ingredient I'm running a special on.   However, this is not a normal world. Surely there is some reason why this option can't be done.",Neutral
AMD,this is not happening lol,Negative
AMD,It's not gonna happen. Buy a 5800XT while you still can.,Negative
AMD,that’s like the struggle of my life trying to upgrade without going broke as hell,Negative
AMD,"I doubt it. That would also mean reversing their directive to wind down AM4 motherboards while the ram cartel winds down DDR4 production altogether.  They already reversed the wind down of the B650, so the win-win scenario for them, board makers, and the ram cartel would be for everyone to still switch to the lower AM5 CPUs.",Neutral
AMD,And to think i was gonna go with a 5600 before is said fuck it and pulled then trigger on the 5800x3d,Negative
AMD,"Damn, what number they calling?  I got some grievances of my own",Negative
AMD,Best chip of all time.,Positive
AMD,Well I guess it's more worth it that I still use 5800x3d which I bought close to 2 years ago?,Neutral
AMD,I would buy one tomorrow if they came back out,Neutral
AMD,"Dear AMD, bring us a 5950x3d for Xmas. Thank you",Positive
AMD,BAHAHA AM4 WILL NEVER DIE,Positive
AMD,"I mean, from AMDs perspective they could, but how long would it take to actually get them back in production again? Will there still be demand for them in that time, and would it not be more cost effective to just keep making AM5 chips, or make an AM5 refresh that can use DDR4?",Neutral
AMD,Ddr4 stock will be out by  the end of the month... this is just false hope imo.,Negative
AMD,"No, please don't, because I want to schlep my old one on Facebook for a thousand bucks.  After I do that then yeah, start manufacturing them again.",Negative
AMD,"That chip is fucking immortal :D   Glad I switched my 2600 to 5800X3D back in a day, and held myself on jumping to AM5",Positive
AMD,They could make a 5850x3d but instead of being a binned version like 9850x3d make it like the 9800x3d with the cache on the bottom and unlock overclocking.  Would probably sell for the price of a 9800x3d,Neutral
AMD,Ryzen 9 5900X3D would be Fire XD,Neutral
AMD,I was called a madman when i bought a 5700x3d so late into am4's life,Neutral
AMD,"It’s good and all, but to do that would take time, and their yields might be questionable too. It’s not like they can just click a button and there’s a million 5800X3Ds 😅",Neutral
AMD,I thought about buying a 5800X3D last year so I could upgrade my main CPU and put my current one in a machine still running a 1700 but I figured I'd wait a little longer for the price to go down. Boy that sure was a mistake.,Negative
AMD,would be nice if they could shove zen4/5/6 while they are at it.,Neutral
AMD,"Yeah with RAM/GPU and other prices I might upgrade to this for the near future, but not at silly prices.",Neutral
AMD,Man I sure did want an AM4 X3D chip but... My 5600x is doing me good anyway.,Positive
AMD,"Would be a good move, RAM makes upgrading very very difficult.",Positive
AMD,I don’t know if there is enough demand for AMD do this.   It’s like wagons in the us a small group love wagons but not enough to justify production,Negative
AMD,I just ordered a 5500x3d hours ago since there are no 5700x3d in my country... Would be kinda unlucky if it happens but I'll just enjoy what I can I guess,Neutral
AMD,"So happy I decided to splurge on a 5800X3D, it's gonna keep me afloat through these turbulent times",Positive
AMD,"No, no they aren’t.",Neutral
AMD,Hahahaha,Neutral
AMD,It's not like there's spare capacity at fabs: why go back to making 5000-series CPU chiplets when they can keep cranking out modern EPYCs and Threadrippers instead?,Neutral
AMD,"Meanwhile I’m trying to sell my 5800x3d, 32gb of ram and a rog Strix motherboard for around $500 and am getting absolutely zero interest lol",Positive
AMD,I sure could use an update to it. This cpu broke XMP on my machine,Negative
AMD,"Three years ago, whenever I brought up chip makers reviving old models I got downvoted to hades for it.  “iT’s iMpoSsiBle whY wOulD they do tHaT?”  Funny.",Negative
AMD,"Ja, in den Rechnern der Kids werkeln aus Kostengründen auch noch 5800x3d. :D",Neutral
AMD,"Ich habe einen 5600x und hatte eigentlich vor, auf AM5 aufzurüsten, weil meine AIO jetzt auch schon 7 Jahre alt ist und ich gleich ein neues Gehäuse haben wollte. Durch den DDR5 Wahnsinn setze ich das jetzt aus. Jetzt hätte ich schon gerne zumindest ein weniger umfassendes Upgrade mit neuem Gehäuse, AiO und einem 8 Kerner.",Neutral
AMD,Ich hab einen Ryzen 5950X aber halt auch eine Asus Rog RTX 4080 Super und da ist die CPU leider schon wieder voll ein Bottleneck :(,Neutral
AMD,Gamer und reviewer fordern. Das ist in etwa so wie wenn der Steuerzahler etwas fordert.,Neutral
AMD,"Würde für AMD keinen Sinn machen. Bei den meisten AM4 Prozessoren wurden die CCD zwar von TSMC gefertigt, die IO DIEs aber von Globalfundries in Amerika gefertigt und anschließend in Dresden im BTF fertig gestellt. Da AMD erstens kein Kunde mehr von Globalfundries ist (sie müssten die Konditionen neu verhandeln) und Globalfundries auch nicht mehr das BTF in Dresden hat (wurde Ende letzten Jahres geschlossen und die Tools verkauft so wie die Leute in der Abteilung gekündigt oder in andere versetzt) wäre der Aufwand sehr groß für CPUs von einer ""toten"" Platform wo die Leute ja auch für eine neue CPU trotzdem nicht extrem viel Geld ausgeben wollen. Das Risiko wäre viel zu groß daß man sehr viel Geld investiert um am Ende als die gute Firma wahrgenommen zu werden. Es gibt keine Garantie das AMD damit so viel Geld verdienen würde das Investoren und Manager zufrieden wären. Klar würde ich das auch cool finden um meinen 3700x zu tauschen, aber es wird leider nicht passieren.",Neutral
AMD,Mir würde der abgespeckte 5700X3D schon reichen.. oder das der 5600X3D und 5500X3D in Europa verfügbar sind.,Neutral
AMD,"I am on i9-9900 with Z370 chipset and 32GB of 3600 MHz DDR4 RAM. Wanted to upgrade to Intel Core Ultra 7 265K (yes, I wanted Intel with all my will power) but bad luck stops me obviously.   PC market is always messed up. It seemed to be stable after the GPU thing - but naah, les go with the RAM now.",Negative
AMD,Ça ne serait pas plus simple de produire une carte mère am5 avec des slots supplémentaires en DDR4 !?,Neutral
AMD,"Why? Nobody is making ddr4 anymore.  Once inventory is out, it will. E more expensive than ddr5.",Negative
AMD,I feel that it makes more sense to just buy ddr5 and a mid tier am5 chip than build on outdated sockets. Seems like a short term fix for a long term endeavour. It takes 3 core components being outdated for relatively minuscule savings overall.,Negative
AMD,I honestly cannot fathom how people are paying 400+ for this when you can just sell your current AM4 cpu/mobo and buy a 14600k/Z690 DDR4 board for way less than the 5800x3d.,Negative
AMD,"Yeah DDR5 prices are still pretty brutal, makes sense why people want to squeeze more life out of their AM4 builds instead of doing a full platform jump",Negative
AMD,I already have a 5800X3D but i think i would upgrade to a 5950X3D,Neutral
AMD,DDR4 is also stupid expensive and not that much cheaper though,Negative
AMD,Seems to me that chips that don’t make the 5700/5800x3d bins will still be in demand considering the tech shortages could last years.,Negative
AMD,The 5700x3d's are just 5800x3d's that didn't meet the performance benchmark. If they revived the 5800x3d they'd also be reviving the 5700x3d as a result of that.,Neutral
AMD,Ddr4 is really not being produced at scale anymore. Once people start buying DDR4 systems the price will explode just like ddr5.,Negative
AMD,why not both?,Neutral
AMD,Or even take a play from AM5 and make a 5850X3D,Neutral
AMD,I had a 5950x and went to a 5800x3d and it improved my 1% lows in most games and some games got a boost of average FPS too. It is just for gaming not for any other tasks so it was absolutely worth it for me.,Positive
AMD,If only there was a free search engine you could type that into?,Neutral
AMD,Demand for ryzen 9000 series will drop off if ddr5 ram is either prohibitively expensive or not available for a good while,Negative
AMD,That was a good price. I paid around $165 in January. Crazy what they are going for now,Positive
AMD,"I almost bought one myself. I hesitated and then the price started going up.  Oh well, my 5600x is good enough for me I guess. If AMD wants to make more at that price range, I'm totally in, however.",Neutral
AMD,I am doing my first PC build and I went with the 5800XT. It would be awesome to see more AM4 X3d chips with how RAM prices are.,Positive
AMD,Are you having issues with it? My 5600X still runs everything I can possibly throw at it without any issue.,Negative
AMD,"Me too, might have to find a discounted pre built",Neutral
AMD,I'm just here on a i3 12100F. I'd love to go AM4 but the cost of doing so puts me at i5 13400/i5 14400 prices just for a sidegrade to an R5 5500 + B550.,Neutral
AMD,Because ddr5 ram shortage is going to eat into demand for am5 processors in new builds... Meanwhile there's a healthy amount of am4 boards that already have ddr4 ram in them and could use an upgrade.   The design work is already done and the nodes they are made on are still in operation.,Neutral
AMD,"I did too, but prices had already went stupid and not worth it for a fractional upgrade.",Negative
AMD,"Kann ich mir vorstellen, im gebraucht bereich liegt das interesse dort eher im 350€-400€.",Neutral
AMD,"It's about getting some cash out of existing AM4 users. They largely wouldn't be buying new or more memory, just replacing an existing CPU because upgrading to AM5 and DDR5 is going to be incredibly expensive for the next few years at least.  So they can get no money in the next few years from people like myself where I will opt to sit on my 3600 because DDR5 prices are insane, or they could manufacture some AM4 chips, and maybe it's worth upgrading to that. Re-starting manufacturing isn't cheap though so I'm sure they'd really have to run the numbers to decide if it's worth it. Maybe once AM5 sales start to plummet they will.",Neutral
AMD,Because a LOT of people are still on AM4. I'd bet more than AM5.,Neutral
AMD,"Oh just buy vastly overpriced DD5, oh and a whole new motherboard. Do you even know how PC building works?",Negative
AMD,Because the real world does not work like that.,Negative
AMD,I'm trying to squeeze more life out of my Intel I7-11700k!,Positive
AMD,"if you are primarily gaming not worth it, technically a downgrade for gaming if memory serves right",Negative
AMD,"There are rumors of a 9850x3d with dual v-cache. They could make a dual v-cache am4 chip as well and cash out on the am4 renaissance.  I don't know if it's possible, but if they could do bottom-side V-cache on an am4 like they do on am5, it could let the am4 x3d chips reach 5ghz instead of their current 4ghz. Would be a MASSIVE boost to everyone in am4. Honestly I think a 5850x3D with bottom side cache could probably match a 7800x3d at least.",Positive
AMD,Prob 5900x3d would be more practical.,Neutral
AMD,"Yeah, but people who already own AM4 systems also already own DDR4 memory.",Neutral
AMD,"Not necessarily.  The 5800X3D became economically irrelevant for AMD. It was expensive *and* it was being beaten by the 7700X on AM5, which was both easier to make, and outselling it.  From analysing assembly dates, AMD appears to have ordered two, possibly three, batches of Vermeer-X/Milan-X. AMD used pre-tested, known good, Vermeer dies for the chip stacking process: **Every last one of them** was 100%, high clocking, the best dies TSMC had made.  I bought this 5700X3D in June 2025 for just over £200, it went end of life not long later. The date code on it was week 9 **2024**. AMD was making these in batches, not in serial production. Go google up some pics of the CPUs, you'll see the date codes cluster in two major groups with a possible third one.  Given that they were pre-tested dies, all of them able to clock and scale voltage at the very best of what Vermeer was ever capable of, the choice of whether it was sold as a 5600X3D, 5700X3D, or 5800X3D was a purely economic one.",Neutral
AMD,"5700x3d is exteme hard to overclock even to 4,2ghz.",Negative
AMD,"It's really for people who already have an AM4 system with a slow CPU, and already have DDR4 RAM who can no longer afford an AM5/DDR5 system. It's a big performance jump over the the 1000-3000 series and 5000 non-x3D",Neutral
AMD,Consumer DDR5 also isn't being produced at much of a scale anymore either so may as well get what performance is possible from the socket people already have memory for.,Neutral
AMD,"Reviving the 5800x3d would offer a good upgrade for people on AM4. No new DDR4 required. Additionally, there's still a decent supply of used DDR4.",Positive
AMD,Im on a 5800x3d. Id welcome a new am4 with open arms. Riding the am4 wave till 2030….,Positive
AMD,"All I can contribute is the 5700x3D is the most consistent and stable system I've ever had, love it.",Positive
AMD,Certain games really benefit from the extra L3 cache - Escape from Tarkov and Star Citizen being two that instantly come to mind.,Neutral
AMD,Go check out some of the charts form Gamers Nexus from when it came out. You can find charts with both chips.  I have what you have and found out about the x3d from it being announced they were stopping production.  It's a clear gain. I think people say around 20%. Which was pretty nice at the pre-scarcity price.,Neutral
AMD,Same. Bought the 5900x before x3d even exists. FPS wise it doesn’t look much of a difference but I think the 1% improves which I’m hoping improve monster hunters wild,Neutral
AMD,"I went from a 5600X to a 5700X3D - average FPS was a little bit better, but nothing eye opening. However the 1% lows improved massively and general frame time was much more consistent.   I especially noticed this in VR performance for sim racing where inconsistent frame rates and frame times are immediately noticeable.   While the 5600X is technically a faster CPU, in both base and turbo clock frequencies, the additional cache of the 5700X3D made a huge difference for the games I play. The fact that I also gained two cores really isn’t that much of a factor. I originally went from a 3800X to a 5600X and the reduction of cores, from 8 to 6, had no negative impact on gaming performance.",Positive
AMD,"The issue is bringing back 5700x3d on say the back of the current running 5500x3d would change binning.  And it might not come back at a price point people are willing to purchase.  It really depends on the facility.  If the fab they're doing 5500x3d can't produce enough yield to bin a decent amount of 5700x3d, there's no point, supply would be so low that price wouldn't match consumer expectation.  It really depends on where each line is being fabricated, but if the machine they're doing the 5500x3d can't hit yields high enough to produce enough bin for another SKU, it's just not going to be smart to start binning on something that the final price to consumer will just drive them away.",Negative
AMD,"https://preview.redd.it/sm4hfifukg8g1.jpeg?width=4212&format=pjpg&auto=webp&s=93f5a9b2614c7c7c6c1a86784826781e80618fac  Six months ago, at exactly the right time...",Neutral
AMD,"Got my 5800x3d for $300 last month and that was a fair price on used market. Still expensive for last gen but less then nee cpu, mobo and ram",Neutral
AMD,"Outside of workstation tasks and some specific CPU-heavy games, 5600X is just fine. I mostly game, so I made the small upgrade to a 5700X3D to make my AM4 build relevant for a little while longer, but it’s not that big of a lift over a 5600X, 10-20% lift at most, not worth it unless you’re just trying to eek out some more performance and you can justify the price over swapping to AM5, which is different for each person.",Positive
AMD,I listened to all the tech tubers saying a 5600x is all you need for gaming like 5 years ago. Now I realize cpus have a longer shelf life and I would be in a much better place if I had at least gotten a 5800x or 5900x.  It just sucks wanting to upgrading but not being able to because of ram. Sometime in the next 2 years when ram prices drop I'm building a beast.,Negative
AMD,Same. I’m actually on the regular 5600 and it does just fine with my 9060 XT. Hardware Canucks actually has a couple of videos comparing how older CPU’s hold up with modern GPU’s and the results lean towards pretty well with the 5000-series.  [Against the 9060 XT and 5060 Ti](https://youtu.be/NqRTVzk2PXs?si=rPbJ63sy1vGa2GUr)  [Against the 9070 XT and 5070](https://youtu.be/TXKyQYiLro8?si=lJhigcH8-o2E5NtI),Positive
AMD,Nah bud save your money. I also got a 5800x on release before X3D you won’t get massive gains. Just ride the 5800X into the sunset and pray the market cools.,Neutral
AMD,"No huge gains outside of some asset-heavy games like simulations and anything that needs to preload large textures or other intensive processes, so some capability gain for workstation tasks. The downside is a lower TDP and a lower overall clock speed.",Negative
AMD,"I did the same, and upgraded to an x3d the day it came out; For me it's night and day in MMO's, where developers can't fully control scenes, and thus, there's an increased chance of dips.  It's a bit slower on top the top end, but much, much more stable in the 1% and .1% lows.",Neutral
AMD,I'm sure hoping since Im standing on AM4 board and I'm planning on upgrading CPU and GPU to get through these rough times.,Positive
AMD,Wafers costs fuckton more than any newly produced AM4 would sale.,Neutral
AMD,"Yep, I’m talking about people building from scratch.",Neutral
AMD,It very much does,Neutral
AMD,I’m still rocking my heater/ i9 9900kf from 2019!,Neutral
AMD,"If we're talking about the regular X version, yes. The 5950X3D doesn't exist but one could dream.",Neutral
AMD,Exactly. I’m sitting on 6 or 7 sets of DDR4 from when it was cheap cheap. Also my itx portable is still AM4 so an upgrade for it would be nice.,Positive
AMD,"It is, considering how long and in quantity the 5800X3D was made. The 5800X3D launched first, and was produced for months before the 5700X3D hit the market",Neutral
AMD,"Cache affects game performance kinda like not having enough RAM does. In some games it makes almost no difference, but in a few games it's like 50-70% improvement. If there are specific games you play a lot, try looking up benchmarks for the X3D chips for those games specifically.  If you play a lot of different games, then I'd say it's probably not worth worrying about it.",Neutral
AMD,Even if am4 chips would cost same as am5 people would buy it cause they don’t need new ram and mobo for it. And for instance if you go from 3600 to 5800x3d that’s a huge boost without platform change.,Neutral
AMD,"It's not a problem if the 5600X still does everything you need it to, which for me it does. Obsessing over tiny gains and upgrades isn't worth it.",Neutral
AMD,Same! Tho I have regular i9-9900. It's still a pretty decent CPU to be honest.,Positive
AMD,"A 5950X3D would essentially be one 5800X3D and one regular 5800X glued together. For gaming, you'd get the best performance by disabling the non-X3D die which would turn the CPU into a 5800X3D.  The only advantage of such a part would be that you get 16 cores for productivity tasks, while still getting access to 8 cores with 3D V-Cache for gaming.",Positive
AMD,"> Even if am4 chips would cost same as am5  I mean that's a big if.  It really depends on what kind of yields they can get on the machine that does the 5500x3d.  If they're getting low yields for 5700x3d on the the machine, there's not much point to binning as such.  Given the 5500 74mm² and the 800 standard wafer, that's like ~890 dies per wafer if none are bad.  If each wafer is only yielding 10% or less dies that can be binned 5700x3d, it's going to be more than the AM5 equal in cost.  So at least 90 or more of those have to meet the 5700 testing.  And my guess is that they settled on 5500x3d because they were hitting high 90% yield for that chip and not much more for anything else.  The Zen3 is arrange in 2x4 for the cores, so they must be hitting high for 6 of 8 coming through and really, really low for all eight.  The difference between 5700x3d and 5800x3d binning is the later hits higher stable clocks.  The 5700x3d is just functionally a 5800x3d that's got a few issues so running it at lower clocks keeps it stable.  But the 5500x3d is two cores died during production.  So it might be hard for them to hit 8 for 8 enough times to bin 10% of the wafer into a 5700x3d SKU on the machine they have the 5500x3d on.  And the 5500 isn't being produced on the main money maker fabs, that's always the latest gen that it's tooled for.  So 6 for 8 is maybe as best this machine they're running on can do and 8 for 8 is pushing it too hard.  I don't know, depends on the location and what they've got going.  But to retool one of the big boys that have great batting averages to run an AM4 platform is silly talk.  It'd take a ton of time to do so and disrupt so much we'd have another disruptive chip stocking issue.  I don't know AMD's specific numbers, but I can only imagine that they'd need some high 8 for 8 yield for it to be equal in price to the current AM5.  There's likely a specific reason they selected the 5500x3d as 6 for 8 and at the price point they've put it at.  My guess is that they're getting high yields on the machine they're using for two busted cores, high enough that the 5500 can be at the price it's at and it make sense to keep an AM4 around still.  Again, if the DDR5 makes an issue for the 9000s and AMD actually cares about the consumer platform, then it's just easier to use some dollars to have some of the higher demand consumers for DDR5 to ease up a bit and let some flow into consumers.  A whole lot easier than trying to push a machine to justify binning the 5700 SKU.  And vastly easier than trying to retool one of the machines it used to be on, back to doing that platform.",Neutral
AMD,"A perfect machine for programing + gaming, *at the same time*",Neutral
AMD,Is disabling the cores a simple process?,Neutral
AMD,"Ngl id buy a couple of those. Have shit tons of DDR4 RAM and a few server rack cases I could use to add a few more units to my home lab.   X3D CCD for cloud gaming VMs and the regular die+ for productivity workloads  Edit; not a hardware Eng, could be misusing CCD as a term and I apologize.",Neutral
AMD,Awesome reply. I don’t believe it’ll happen but if it would and would give folks a bit of a boost until ddr drama ends.,Positive
AMD,"Something something ""Screeps"".",Neutral
AMD,"Restart, head into bios, make change, boot back up again",Neutral
AMD,"Yeah, it really depends on the length of the ""DDR5 drama"" which that's what I'm going to call this now.  I've taken it, you can't stop me.  But the thing is, given what I've heard with folks like Micron, Samsung, and SK Hynix, these aren't temporary changes, these are structural shifts in production.  *Money into data centers has crossed some sort of threshold that things like consumer PCs aren't profitable any more.  There just simply isn't enough sales in consumer PCs to justify the production before.*  Honestly, this gets into the economics part that I have very little knowledge on.  So, take anything I have to say in the above with a massive grain of salt.  But the main makers of DDR5 RAM are doing things that are big shifts away from the way it used to be done.  Reasons for why they're doing that I can only guess, but those may be bad guesses.  So the DDR5 drama may have a much longer duration.  I don't know, but the kinds of shifts they're doing aren't common.  But at the same time, it may mean a massive shift for the AM5 platform and software developers.  Chip makers issue a thing called ""Last Time To Buy"" LTBs.  The main producers of DDR4 have LTBs already.  The last ""raw"" DDR4 chips will be sold by major makers up to Dec. 31st.  After that, there's no more being made by the major makers.  But the big three aren't the only ones who make these.  There's Nanya, Winbond, and the Chinese CXMT.  They have yet to announce LTBs for their chips.  Again, I'm guessing here.  Just pulling shit out my ass here.  *But we all know how like Chinese CPUs are like a gen or two behind the leading gen?  I have a feeling that's going to be the entire consumer PC market.  Like what they're using in data centers is like two gens ahead of everyone else.  I think the DDR5 drama is just the correction to that state, but I have a sinking feeling, the situation that China has with their homegrown CPUs, will be what everyone who is a consumer will be in soon enough.*",Neutral
AMD,"I would say okayish, not a steal",Neutral
AMD,I have thesame specs but all brand new with 6700XT. Built it for around 800-ish USD  https://preview.redd.it/fmbjsu3pkb8g1.jpeg?width=4080&format=pjpg&auto=webp&s=67640a84ce338c300098709e1a6a9363e70f6e6c  You might have overpaid a bit,Neutral
AMD,Considering ram pricing it’s alright. Can easily upgrade the GPU later for a chunky performance boost,Positive
AMD,Yeah i asked  if he would sell without cpu and gpu and i woulda got a 7700x and 6800xt sadly not tho,Neutral
AMD,Yeah I've noticed it across all of my games since switching GPUs,Neutral
AMD,System specs including power supply model,Neutral
AMD,Cpu - 5700x3d Gpu - Asus 6700xt  MB - Msi b550 a-pro  Ram - Corsair ddr4 3600mhz PSU - cooler master mwe 750v2 gold,Neutral
AMD,Are you using separate 8-pin PCIe cables for each connector on your 6700XT or are you using one cable with the pigtail?  Don't rely on pigtails.,Neutral
AMD,using one cable with pigtail should i change it? it was working fine before maybe not now,Neutral
AMD,"It's not just good, it's great!",Positive
AMD,"Swap the MSI MAG for an MPG or a Be Quiet Pure Power 13 M.  Otherwise very good.  You will grow out the SSD in no time, consider 2TB.",Positive
AMD,Not bad,Negative
AMD,"Is this a prebuilt or are you putting together?  Great prebuilt no worries about the performance.   If you're building I think an air cooler for the CPU, then put the saved money towards a better power supply.",Positive
AMD,"I'm giving you the benefit of the doubt that you got this as a gift and don't know much about PCs.  You have one of the best gaming PCs you can buy.  Other than upgrading to a 5080/5090, and marginal improvements getting a 9800X3D, there's nothing better.   So yeah..I'd say you have good ""first"" PC.",Positive
AMD,Thanks! I really thought this was a mid PC lol,Positive
AMD,Putting it together. I was just saving my money and picked the parts last month and I am getting it this summer,Positive
AMD,I don’t actually have this and I have been saving since 2022 to buy a gaming pc and I am gonna buy this PC this summer. I picked the parts like a month ago.,Neutral
AMD,Nearly the best gaming CPU on the market and a [pretty high end GPU to go with it.](https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-nitro/34.html),Positive
AMD,"Depends on what you want to play and how much you're paying. Those specs are quite low, and you're probably better with a steam deck if it's comparable in price or look into one of the consoles",Neutral
AMD,No. I have a PC with that CPU's little brother the 3200G and 3 years ago it could play some light games.  You would at minimum need to add a decent GPU.  You don't mention price but 549 BAM pops up on google and if that's the price it's not worth it. Maybe if it had a GPU included,Negative
AMD,"Best buy actually has some good last minute pre Christmas sales going on. I usually wait till the return rush to get the good deals but after the first of the year things are gonna jump cuz ""tariffs"" so. Best buy also does the 4/8 payments through PayPal.",Positive
AMD,This is more of a Netflix streaming PC or business use PC than a gaming PC. This won’t run bannerlord or any newer AC games.,Negative
AMD,Plenty of good games to be had with this.  * Dead Cells * Blasphemous * Darkest Dungeon * FTL * Balatro * Hades * INSIDE * Slay the Spire * Inscryption * Crow Country * Outlast * Underrail * Quake 1+2 Remastered  ...and a lot of others as well. But these alone are some of the best and plenty to be occupied for years on end.,Positive
AMD,"My main motivation was to play Mount and Blade Bannerlord, but also I want to play some of the newer Assassin's Creed games.",Neutral
AMD,Thank you. I appreciate the advice.,Positive
AMD,Appreciate the honesty.,Neutral
AMD,This is how a 5800x should look:  https://preview.redd.it/p6jt2g5chc7g1.png?width=997&format=png&auto=webp&s=69b2669d3b0ad0332044a35a688e373c0e0d5d48     100% got scammed.,Neutral
AMD,"That is an AM2 or AM3 era CPU, possibly as old as Socket 939. I hope you didn't damage the mobo when you forced that thing in.",Negative
AMD,"Thank you everyone, I'm requesting a refund ASAP",Positive
AMD,Yes you've been scammed. No doubt about it.,Negative
AMD,"OP, I suggest you check tweakers for the best Dutch pricings/specs etc for this specific CPU > [https://tweakers.net/pricewatch/1618234/amd-ryzen-7-5800x-boxed.html](https://tweakers.net/pricewatch/1618234/amd-ryzen-7-5800x-boxed.html)  With Bol more being a sellers platform at this point I suggest you steer clear of purchasing PC parts from them.   If you need some help you can DM me :)",Neutral
AMD,That's an AM2 CPU. Sorry bud.,Negative
AMD,"It's an AM2 CPU, so yep, you've been scammed. Wait how the fuck have you put it and CPU light came on? Does this CPU pattern match your motherboard's pattern?",Negative
AMD,Absolutely. Ryzen 5000 have a middle that's not populated with pins. I might have a very old Sempron nearby that I think might be the same as this... will check and get back.  Yup: https://prnt.sc/NTE8FWqgZSZA  What you have there is a CPU for AM3 slot.,Neutral
AMD,"What does the top look like?  Just curious because yes, counterfeit Ryzen heatspreaders have become quite common but they used to only be faking the X3D chips.",Neutral
AMD,Yeah the ‘missing’ pins in the 5800x are parallel and symmetrical.  You should put in for a refund if that’s possible.,Neutral
AMD,"On the other side it is literally engraved in the metal what it is. The pins are definitely from an older socket tho. Also Bol is not an official vendor, it's a marketplace just like Amazon",Neutral
AMD,"You managed to shove an AM3 CPU into an AM4 socket? How? And uh... Don't you have eyes? All it would take is a single look at the socket and CPU to know they aren't compatible... Like you don't even need common sense to see that, you just need working eyes...",Negative
AMD,"Yes, thats an AM3+ Pin layout. Refund it and buy from actually legit sites next time.",Positive
AMD,"Yep 100% scammed. This is an AM3 CPU, pre Ryzen era FX.",Neutral
AMD,"Yes, luckily its official vendor and RMA it if you have unpacking video",Neutral
AMD,Why show the bottom? what does it say on the top?,Neutral
AMD,This kind of stuff happens too much.  I'm driving to the closest MicroCenter (3 hours away) and buying in person.  I plan on opening each box there in store to make sure theres no funny business.  I don't plan on driving back.,Negative
AMD,"I'm curious, what does the top of the cpu look like, any photos?     It's clearly a different CPU, but I wanna see if it's a fake or if they just put the wrong part in the package.",Neutral
AMD,So why not just googling yourself how that CPU is supposed to look like?,Neutral
AMD,It appears to gave 940 pins - vintage am2 cpu,Neutral
AMD,![gif](giphy|wPb0Er6MG6d9K),Neutral
AMD,That is not am4 dudee,Neutral
AMD,"You did get scammed but, if you squint your eyes hard enough it looks like am4.",Neutral
AMD,"OP: since you're in the Netherlands, the only two stores I recommend buying pc parts from are: [azerty](https://azerty.nl) and [megekko](https://megekko.nl). Two reputable online stores that have yet to let me down in the 10 years I've bought pc parts.",Neutral
AMD,Hopen dat bol.com die verkoper ff een goeie tik op de vingers geeft,Neutral
AMD,I refuse to believe this is not ragebait,Negative
AMD,"Does the cpu have its model etched into the lid? Should have its SKU number on it at least. It looks like an AM3 socket cpu, definately request a refund!",Neutral
AMD,Looks like an AM3 socket CPU so yes.,Neutral
AMD,"That’s an AM2 or AM3 chip, probably the cheapest Athlon they could find and slapped an AM4 5800X IHS on it because it’s the same size, definitely got scammed brother. Don’t know where you got it from or if it’s a reputable site which I’ve never heard of BOL, I’d get your money back. Tell them this: “I bought what specifically said a Ryzen 7 5800X which is an AM4 CPU but what I got is clearly an AM2 or AM3 CPU with a 5800X IHS slapped on the top and I demand I get my money back.” And if they say there’s nothing they can do then tell them “I’ll get the law involved for false advertising and fraud.” Then they’ll give you your money back.",Negative
AMD,Did it come with lube?,Neutral
AMD,Looks like a phenom II,Neutral
AMD,yes. how did you even get it in the motherboard? 💀,Neutral
AMD,"Go back, kiss him as hard as you would the love of your life",Positive
AMD,How much did you pay for it?,Neutral
AMD,"siorry bug, you got scammed     once you refund it buy yourself a 5800xt or a 5800x3d",Negative
AMD,Ryzen 1000 / 2000 / 3000 / 5000 series (AM4),Neutral
AMD,that looks similar to the socket to my old ass athlon 64x2 in my retro computer,Neutral
AMD,His pins bend out the box 😂,Neutral
AMD,I have a ryzen 7 5800x and mine didnt look like that,Neutral
AMD,That looks like some kid ordered the AM4 chip and then claimed it didn’t work and returned his old AM2 chip. The good old return scam “free upgrade thanks to Bezos”,Negative
AMD,Poor you,Neutral
AMD,"At best it could be an FX, but could also be some older Athlon II or Phenom II.  You might have been third-party scammed where someone bought a 5800x, but returned it with an older cpu and retailer did not check the return properly (or returner did a great job making it look unopened). Were any of the seals questionable/not present?",Negative
AMD,I'm afraid you damaged your mobo as well.,Neutral
AMD,Yeah fr you lost thatttt,Neutral
AMD,Lmao it’s always so strange to me when people make a post answering their own question IN the question.,Negative
AMD,"Yeah mine when I was on am4 didn’t have any missing pins, sorry to say bud looks like you were scammed",Negative
AMD,Yes. That will never work.,Negative
AMD,Dropshipping company.,Neutral
AMD,"Go back, kiss him as hard as you would the live of your life",Positive
AMD,"Go back, kiss him as hard as you would the love of your life",Positive
AMD,"Go to bios and check what CPU it sees.  I have myself bought a 7800X3D which doesn't look like in photos but was told on PCMR Discord that multiple places make these and that they look slightly different. BIOS sees it, windows sees it and gaming is amazing as expected so I don't care 😁",Neutral
AMD,![gif](giphy|hpAMh2sBYpsmFhSRPI),Neutral
AMD,"Just fyi bol.com isn't really an official vendor for anything, it's more comparable to amazon so you should check the original source where the CPU came from.   Still sucks you got scammed like that, hope you get that refund quickly.",Negative
AMD,"OP if you can’t get a refund and paid with ur credit card, challenge the charge with ur bank with the pretense that you’ve been scammed.",Neutral
AMD,They are good to order from but you need to cueck who is selling it. If bol it self is selling it 99% good to go if any other seller it really depends on which one but then it is a 50/50.,Neutral
AMD,"saves 17 bucks over Bol as well.  Bol is really good for more mainstream things like controllers or consoles, but for pc parts, rather anyone else, unless sold directly by them",Positive
AMD,Oh God I'm picturing the mashed pins underneath 😭,Neutral
AMD,Unless someone swapped the IHS,Neutral
AMD,Common sense? In this economy?,Neutral
AMD,"If his motherboard is AM4, there is no fucking possible way he can go to bios to check this CPU as this one will not ever fit in AM4 socket, this one won't fit AM3 socket. You've been told wrong or you think of wrong thing.",Negative
AMD,Yeah that's just not true,Neutral
AMD,![gif](giphy|RkuynGGVvthiV8cUMK),Neutral
AMD,"Yeah, but bol does handle as a middle man when you ask for a refund, you should contact Bol and not the seller directly.",Neutral
AMD,"That's not true. Bol is very much an official reseller it just depends on product category. You have Bol.com retail ( direct)  and Bol Plaza ( marketplace).   But for the 2 resellers selling the product currently, I'm surprised you received a bad product as they are official retailers ( Paradigit and Informatique).",Neutral
AMD,"This. Its called a ""chargeback""",Neutral
AMD,Bol really isn't really that good for anything anymore. Almost everything is dropshipped nowadays. It's even worse than Amazon.,Negative
AMD,His pins are 10000% fucked,Neutral
AMD,Nope I am just a retard not reading it properly...my case was completely different from OPs,Negative
AMD,"While the OP image isn't a 5800X3D - this poster is correct in that it is manufactured (or assembled) in different places, so the PCB colour may vary slightly for example...",Neutral
AMD,Sorry I am just a retard not reading it properly...my case was completely different from OPs,Negative
AMD,This is not always the case. Depends on what the seller is paying for.,Neutral
AMD,"The funny thing was that at the time of writing my comment I blanked on what it actually was called, I sat there for a while going hmmm.",Neutral
AMD,"Damn dude, I know the feeling, happens to me often hahahaha",Negative
AMD,"Definitely always, contact third party first - right? Like there is so much possibility, into the positive if contacting the deller directly as well. But the rule of thumb is to always go to the company you have a direct contact to. When you buy anything in a shop, you don't go to the brand for most problems, right? Especially if it is about being scammed, you would go to the shop and explain to them what happened so they can contact their delivery, which is gonna do their own inspection etc rtv etc etc",Neutral
AMD,'tis but a clock cycle.,Neutral
AMD,"Congrats on the great new build! What benchmark do you use, I am planing an upgrade soon and would love to quantify it.",Positive
AMD,"90% of the time i only use 3Dmark and cinebench. 3DMark has a huge libary for benchmarking the whole system or specific parts of your PC, it also offers stresstest etc. Its great.",Positive
AMD,Great thanks,Positive
AMD,It should be much more powerful than the 2070 in the ballpark of 70-80% faster with more vram. If it's not faster then something is probably wrong.  A couple of basic things: Are you plugged into the GPU? Do you have any other programs eating up GPU or even cpu/ram? When opening task manager in a game (control + shift + escape) is the GPU 100%?,Positive
AMD,Are you using a hard drive or ssd,Neutral
AMD,"Try rolling back to older drivers if you are using new one, cause sometime new drivers do bring improvement for new games, but messing up on older titles, try 25.9.1 version, don't forget to DDU from safe mode just in case",Neutral
AMD,"Hey, what's the PSU rated for?",Neutral
AMD,"Thanks for the replay!!  - I believe I am plugged into the gpu, as it's directly plugged into where the gpu is anchored into the back plating.  -only steam and discord are open  - nothing appears to be background processing  - while running nighrein my gpu is running at 67% and my internal gpu is 0, further confirming I'm plugged into the right one!  Hope this helps!",Positive
AMD,ssd,Neutral
AMD,"Okay, well if the GPU isn't 100% it means something else is limiting it, I'm not familiar with Nightrein, so I'm not sure what it could be, an FPS cap, cpu or ram being the limiting factor etc. But if you're GPU is the limiting factor it should be over 95%, usually 99/100%.",Neutral
AMD,"That's great to know, I will check it out. So generally we want the gpu to be maxed out?",Positive
AMD,"Yes, if it's not maxed out then something else is limiting performance, since your CPU is so high end it's most likely something software related.",Negative
AMD,"It is a terrible motherboard, potentially its throttling the CPU, very unlikely though    The 5500 isn't a power hungry chip   If the bios is outdated, update that and see how it goes. It won't be your PSU",Negative
AMD,the A320 doesnt support OC. only B and X series motherboards support OC.,Neutral
AMD,"Yes I know this isn't the issue, the base speed of my cpu is supposed to be 3,6ghz but it's only running at 3,2ghz",Negative
AMD,I thought I was the luckiest guy in the world when my lady bought me a PS5 two years ago. Turns out we are both the luckiest men in the world !,Neutral
AMD,N0ice,Neutral
AMD,She wants you to spend less time with her. Sus... /s,Neutral
AMD,"You know, at this point the same would stand if it was just sticks of RAM",Neutral
AMD,One of T H E coolest cases my eyes have seen so far. U got good taste friend. Happy Holidays / surfing / gaming  🙃,Positive
AMD,"the radiator is going to have air bubbles in it pretty soon, you have to change the orientation of the pc.",Neutral
AMD,It's the same lady!,Neutral
AMD,https://preview.redd.it/q8l83qvl4a8g1.jpeg?width=3024&format=pjpg&auto=webp&s=8268a0820139be05355049116ad463e845836869  Exactly the aquarium look I was going for!,Neutral
AMD,"![gif](giphy|QiCCluutpecs8)  Why would she want that, when this is what I look like. /s",Negative
AMD,Can you please explain further? Do you mean to turn the radiator upside down?,Neutral
AMD,![gif](giphy|t6cn3lRhDZtBjdAjKN),Neutral
AMD,[https://www.youtube.com/watch?v=BbGomv195sk](https://www.youtube.com/watch?v=BbGomv195sk)   i know its long but its the best explainer out there.,Neutral
AMD,"You'll be OK the way it is now. Jayztwo cents made a video that greatly simplifies the GN video and explains what you've done is just fine but not the best. [The Classic](https://i.redd.it/b5i5e59f7k861.jpg)  Also,   https://youtu.be/DKwA7ygTJn0",Neutral
AMD,Her vagine hang like sleeve of wizard,Neutral
AMD,"Thank you, I’ll watch it.",Positive
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"I don’t understand why they won’t make one with 6 cores and 16cus with no npu for gaming handhelds. We don’t need 12 freaking cores in our handhelds, it’s actually detrimental considering it pulls power from the tdp starved gpu.",Negative
AMD,Rebadged ryzen 5 340.,Neutral
AMD,"This doesn't deserve the AI7 naming with 2+4 cpu cores and 4 cu graphics.  I think this is the uncut Krackan Point 2 die. If it's Gorgon Point, then they could have given it 3+3 cores, just like the 340 predecessor, and not make it worse than it.",Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,Soo rdna3.5? How much the improvement from 740m?,Neutral
AMD,a 6/16 with RDNA 4 (for the FSR4) would make a fantastic successor Steam Deck 2,Neutral
AMD,"Isn't this the Z2E? It's an 8 core IIRC, but 5/8 are Zen 5c cores (granted, it's cut down from Strix Point).",Neutral
AMD,"Not a ton, but measurable. Per-CU rdna3.5 isn't much better in typical gaming, but it is better. Check out 880M vs 780M benchmarks to see the top end of each.",Positive
AMD,They will def skip rdna4 for some stupid reason.  I get it’s a stop gap before rdna 5…but it’s also a 2 year gap.  Maybe longer now with memory prices.  I think steam deck 2 will be 6 zen 6 cores and 12-16 rdna 5 CUs in 2027.  But again…memory :/,Negative
AMD,In 2028.,Neutral
AMD,"Z2E is rdna 3.5, which is just a refined rdna 3.  Rdna 4 is significantly more efficient than rdna 3.  Example: the 9070xt is a 64CU chip that outperforms the 84cu 7900xt at similar TDP (~300w)  If that holds true at lower tdp’s, then an RDNA 4 16cu chip would perform around 25% better than the Z2E, which is already basically 80% better than the steamdecks apu.  So a hypothetical rdna4+ 6core 16cu handheld would have >2x performance to the OG steamdeck.",Positive
AMD,"AFAIK Valve said they're not going to make Steam Deck 2 until there's a very noticeable advance in APU technology. IIRC even +50% performance is not enough for them which is a shame on one hand, but on the other, I totally get it.",Negative
AMD,My comment was not about graphics.,Neutral
AMD,Rdna 5 would be well beyond 50% considering rdna 4 would deliver almost 50% uplift.,Neutral
AMD,"You can't push for devs to make console-like optimization for your device if you keep releasing new devices everytime there is +10% power available !  SD2 should be like Switch 2 vs Switch 1, something that really feels like a different generation, that will run games that the first one can't even hope of launching.",Neutral
AMD,"Isn't it like there would be no RDNA 5? AMD is planning to merge RDNA (consumer branch) with CDNA (professional branch) into UDNA. Nonetheless, the rumors suggest that APU based on Zen 6 (which would be the next generation) could power the Steam Deck 2 with its launch in 2028 ([article](https://www.linuxjournal.com/content/steam-deck-2-rumors-ignite-new-era-linux-gaming)). Fingers crossed for Valve",Neutral
AMD,">You can't push for devs to make console-like optimization for your device if you keep releasing new devices everytime there is +10% power available !  The problem with Steam Deck is that it's just a PC packed into a handheld. It uses the same games library as your ""normal"" PC, someone's else ""normal"" PC, my ""normal"" PC, so there's no chance we'll get the console-like optimization since it's impossible. PCs are just too different.   Although I totally agree that there's no point in releasing a new handheld every year or two because there's a new APU with +15% more performance.",Negative
AMD,I read that as zen6 being 2028 and got a little concerned. I’m planning to upgrade to zen6 when “AM6” drops and that timeline was putting it at 2032 lol.,Neutral
AMD,"Yes it will likely not be called rdna 5, rumors have said udna for a while.  I think 2028 is way too late.  Next gen consoles and PlayStations portable will be 2027 which will all be zen 6 and udna.  Unless memory pricing pushes that back.",Neutral
AMD,"I think AM6 will be revealed in late 2027 or in 2028 as it'll be around 5 years since AM5 release, so it's at least 2 years of waiting.  I also wait for AM6 to upgrade my PC, but if the current situation will stay with us for longer (or, hopefully not, become even worse) mg rig will have to stay with me for a while.",Neutral
AMD,I’m rocking an AM5 platform now and my plan is to upgrade when they EoL the socket to an end stage processor.,Positive
AMD,GCN 1.0/1.1 - HD7000 and R7/R9 200,Neutral
AMD,"Wow, really old huh...",Neutral
AMD,I just want kernel level anticheat as an option for Linux.  When that happens I’ll switch.,Neutral
AMD,So is this bringing it in line with Windows performance or improving beyond?,Neutral
AMD,"And in the usual news, my 30% decrease of caring for linux has happened again. Still 0%.",Negative
AMD,Cannot play anti cheat kernel games such as EA FC or any competitive games = useless.,Negative
AMD,There is a larger gap between today and when GCN1.0 launched than between when GCN 1.0 launched and the first ever Radeon launched.,Neutral
AMD,If that happens companies will make use of it. Without it those who release on Linux will have to bend the knee and release their games without kernel level access.  I'd rather not play a game than let it touch my kernel level.,Negative
AMD,Kernel level anticheat is something the game companies choose to do not something the Linux ecosystem needs to implement. Eac for example works on Linux as an anticheat officially . I have over 4k games in my steam account and none of them need kernel level anticheat. I can play all of them under linux. That is more than enough without giving kernel level access to a company like riot(for example) to have literally more privileges than me on my own pc at all times. People have accepted some extremely invasive measures from companies to play their multiplayer games. Cheating sucks I get it but the way they have implemented the solution is unacceptable imho,Negative
AMD,"No, that shit doesn’t belong in either Linux or Windows",Negative
AMD,"And when that happens something else will be a hard stop for you that will be ""and then I'll switch""  let's be serious for a moment, you can just say you won't switch. It's not bad inherently bad, it's only bad if you don't have a valid reason which this excuse gives you until the next one",Negative
AMD,"You just want to run rootkits on your machine, huh? Better you never use Linux than we get that garbage implemented.",Negative
AMD,Don't you have a Battlefield 6 match to lose or something?,Neutral
AMD,check windows sub and look for updates fpa drops almost every year🤣,Neutral
AMD,Stop,Neutral
AMD,"12 vs. 13 years, checks out. Why do you mention this though?",Neutral
AMD,yup unfortunately i still have a dual boot system with windows so i can play counterstrike on faceit (with their anticheat) its the only reason i still have windows on my PC honestly.... like there is literally no other reason to have windows now.,Negative
AMD,"The problem is that it does exist in current reality, and I can't play with my friends unless I use windows.",Negative
AMD,"Yet another ""gaming"" laptop with 16 core 9955hx with 9955hx3d option, but the max gpu is 5070ti. Because making a 6-8 core x3d with laptop 5080 or 5090 would make too much sense.",Neutral
AMD,Yeah it happens. Its why I went with the legion pro for the x3d CPU at 100w allowance and 5080 at 175w tgp.,Neutral
AMD,Amd has refused for two gens in a row to make a mobile x3d chip with less than 16 cores.,Neutral
AMD,prolly because intel cant compete,Negative
AMD,Amd should try to compete to get their x3d cpus into more than 4 laptops first,Neutral
AMD,"Thats the point, intel is so behind that they simply dont need to.",Negative
AMD,They are vastly ahead in laptop presence.,Neutral
AMD,Plan to rock mine until AM6,Neutral
AMD,got my 5800x3d years ago best purchase ever. I was ever so hesitant from coming from the 3700x cpu but the magic that was 3d cache can not be underestimated. now if we can just get a big amd gpu i wouldn't have to worry for a while.  sad fact that pc parts are getting artificial scarcity cause ram makers well we don't want to scale growing demand we just want to price gouge.   Just hope this ai bubble pops in all of these companies faces,Positive
AMD,"I feel like they won’t considering production has been shut down for over a year. The 5700x3D, 5600x3D and 5500x3D are made from already existing 5800x3D chips that failed QA. I don’t think any AM4 x3D silicon has rolled off the assembly line since last September.   Maybe they could do a Rocket Lake style back port and get a newer Zen running on AM4. Again it’s probably not worth the effort unless next year’s market is catastrophic. Intel allegedly still has Bartlett Lake in the pipeline which would be a 12 P-Core DDR4 CPU with no E-Cores.",Negative
AMD,Long live AM4. AMD is slowly turning into Intel.,Positive
AMD,Can AMD make a 5950X3d while they're at it too?,Neutral
AMD,I've been sitting on a 5800X for a while since I upgraded my old rig to the 3d version. Guess I should pick up an AM4 board and some DDR4 for some home server needs.  ...or maybe I should just sell the 5800x3d because apparently these things are still going for over $400 on ebay used? wtf?,Neutral
AMD,"Even if AMD wanted to, this would have a lead time of easily 6 months, and nobody knows if the RAM price problems will persist until then – or if DDR4 will stay affordable until then.",Negative
AMD,The PC market is trash—and will stay trash for a while yet.,Negative
AMD,Guna ride my 57003dx and 4090 all the to AM6. I’m gpu bound at 4K anyways,Neutral
AMD,My 5800x3d still isn't the limiting factor and I've had it ages.,Neutral
AMD,I'm glad I got a 5700x3d while they were still affordable. I'm gonna be set on my x370 launch day board for the next 4 years lol.,Positive
AMD,"I want to see 5950X3D with two 3D cache or nothing.      the true AM4 behemoth, the beast that was never born",Neutral
AMD,"I got mine when they came out, its a champ. The 3dcache makes it perform like its a gen newer in most games. I plan to ride it out until am6 at least lol",Positive
AMD,"It’s the 1080Ti of CPUs, no chance AMD brings it back",Neutral
AMD,"Zen3 X3D Production has been closed for over a year, everything sold now is just parts that failed QA and are binned down.",Negative
AMD,I'd rather see the 5700x3d. Like 2% less performance for a notably lower cost.,Neutral
AMD,With the price of RAM these days this would be a smart move. 5800X3D DDR4 totally still viable these days.,Negative
AMD,Honestly? Im for it.  Rocking the 5800X3D/507012gb/32gb DDR4 and I can confidently say this is a system that'll last me until 2030 and maybe even beyond.,Positive
AMD,"I've got a 5800X3D, 4090 and 64GB ram. I think I'll be holding onto this setup for a LONG time.",Positive
AMD,So thankful I got my hands on one of these when I did,Positive
AMD,Mine's still going strong along with a 7900 GRE. The GOAT.,Positive
AMD,Going to rock the AM4 and 5900x until AM6.  There’s no need to upgrade a socket change when you don’t really need it,Neutral
AMD,"Ever since i got mine and i forgot what bottleneck is. seriously lol, pairing it with a 9070 xt and they both fight like champs. love it!",Positive
AMD,It’s crazy that these things are talked about like they’re an ancient relic. Grabbed mine at the initial release and it feels like I just got it the other day.,Neutral
AMD,Dies in my 5950x,Neutral
AMD,If they did return it to production I would snatch one so fast. Looking to upgrade and with RAM prices jumping to AM5 doesn’t exactly look appealing,Negative
AMD,If it _really_ came to it I don't think there's any actual technical reason they couldn't slap a Zen5X3D chiplet on a package with the AM4/DDR4 IO die.  That's probably a product they tested internally and found it just didn't make sense (Zen5's are kinda bandwidth starved with DDR5-6000... there'd be a lot of wheel spinning with DDR4-3200) but that'd have been before ram costs went to Narnia.,Negative
AMD,"I miss the days of legit £125 5700x3d's off sellers on AliExpress, now you gotta drop around £250 for a 5700X3D  I don't see AMD reviving the line now tho",Negative
AMD,"5800X3D , 32GB ddr4-3600  and 7900XTX  This will need to last me till AM6 arrives.   Might grab a 5800x3d for my sons rig if I can ever find one  .",Neutral
AMD,That will help only those who already are on AM4 and want to upgrade CPU and already own DDR4 ram. But prices of DDR4 have already been increasing in fast pace and are already almost 300% what they have been just in October here in Norway and I believe similar situation is in other places too. Anyone who thought that stable DDR 4 prices will stay down or not get extremely expensive just wait. Everyone is now trying to make money on desperate PC people. Look just the prices of standard HDD. Everyone who thought of going over from SSDs or M.2 to HDD for mass storage should know HDD prices are also going up in incredible pace. Yeah PC users are FUCKED big time.,Neutral
AMD,"I have had this same thought the last few days after seeing X3D prices skyrocket. There is obviously demand, but is it worth it enough for them to restart production, even if they have the capability. Not to mention the risk of said chip instantly selling out if the batch is too small.  I personally would not be willing to pay more than $200 for an AM4 X3D upgrade, especially since I have a 5600 so it feels like a 5700X3D restart or refresh would be most likely.",Negative
AMD,my fear is that the industry aims to remove the consumer (us) of having his own HW and to force subscriptions on us,Negative
AMD,"I've seen the 5800XT still being sold. I have a 7950X3D, and I barely use the cache specific cores for anything but gaming. Is it worse to just install the XT as an upgrade in an old build? A friend of mine installed it last week on his PC because he couldn't find the 3d version and he doesn't want to upgrade everything yet (too expensive for him). I think he was quite happy with it even though it wasn't the 3d version",Neutral
AMD,Gamers and reviewers must not really understand business operations then. I cannot think of a single time anyone has spun up foundries(which are already very competitive themselves) because customers were facing a hard time.,Negative
AMD,"I have 5600, 9070XT with 1440p display. Is 5800X3D is good upgrade? I won't upgrade to AM5",Neutral
AMD,"I kinda wish I had gotten a 5700X3D or 5800X3D. They are the best AM4 has to offer, and would have been an affordable upgrade to my old 2700X. That being said, I have recently upgraded to the 7800X3D and it has been amazing.",Positive
AMD,"I would unironically buy on day one. I'd love to buy a used one too, but as far as I can tell, nobody is selling their Zen 3 x3D CPUs on the used market yet (because why would you).",Neutral
AMD,Went for a 5700x3d from a 3600 about 9 months ago. I'm happy I made that decision. The x3d chips are just so impressive in terms of gaming performance. It was almost never what was limiting my system in games.,Positive
AMD,Is my 5600x new again?,Neutral
AMD,I would love it if they made a new higher tier AM4 x3d chip. That would be amazing.,Positive
AMD,"i gave my workstation 3700x/x570 to my niece. tbh it was pretty power-thirsty and it didn't age as well as i thought '8 cores' would outside productivity, but it's a nice rig.  currently using a 12700k ddr4 setup, which is fine for me but... eh.  it's not exciting at all.  it wasn't interesting when i built it.  it'll carry me through the AI horror show though.",Positive
AMD,I will buy it 200% if it becomes available.,Positive
AMD,"I did make the step to upgrade from a 5600X to 5800X3D, it was the best decision years later , combined with PBO and a peerless assassin, I have a beast running better and cooler , as I play with a 4070Ti @1440p I feel like I'm in the best sweet spot",Positive
AMD,"Buying myself a 5800x3d a couple of years ago, even at Corona prices, might have been one of the best decisions i made, tech wise  This CPU just...goes. I play games, i work from home, i dabble with AI stuff. I've changed out a total of 4 GPUs since then, a 1660 super, an Rx6700xt, an Rx 6900xt and now rocking an RX 7900XTX. Still the same CPU, never saying no to me, no matter what i throw at it.",Positive
AMD,I miss the good old days of $120 5700x3d on AliExpress,Positive
AMD,Wait until people learn you can game on non x3d CPUs,Neutral
AMD,"Please do this, AMD.",Neutral
AMD,"if AMD doesn't capitalize on AM4's resurgence in popularity and the general reliance on DDR4, then Intel will, because LGA1700.  While it wasn't looking very impressive against the 5800X3D, the i9-12900K was still a contender and right now you can get an i9-12900KF for slightly less than an i5-14600KF. Obviously doesn't work for people who already have AM4, but people who are in the market for a cheaper platform than AM5 can get somewhere with LGA1700 and the performance ceiling is still higher than AM4 as long as AMD doesn't raise it.",Neutral
AMD,God I hope so,Positive
AMD,Plan on upgrading mine sometime in 2027. Don’t really see the need. Especially with all future games being made with handhelds in mind.,Neutral
AMD,still rocking with R7 5700X3D,Positive
AMD,"I check eBay and Facebook daily, it’s a miserable time",Neutral
AMD,If they did return it to production I would snatch one so fast. Looking to upgrade and with RAM prices jumping to AM5 doesn’t exactly look appealing,Negative
AMD,"I’ve been using that chip for a while now, it’s a great work horse",Positive
AMD,Please let this happen 🙏 when I finally went to get one the price had skyrocketed so I ended up buying a used 5950x because it was far cheaper.,Positive
AMD,Best purchase I ever made back in 2023,Positive
AMD,"5600 with 4080s, x3d tempts me",Neutral
AMD,"YES PLEASE. I’d upgrade in a heartbeat. Hell, try and revive that 5950X3D",Positive
AMD,My 3600 will have to truck on until well into AM6 probably 🫠,Neutral
AMD,The GOAT,Neutral
AMD,"I am kicking myself for not picking one up when they were readily available.   Would be the biggest boon for my rig, going from a 5600x to a 5800x3d",Positive
AMD,Had a 5700x3d in my hands returned it for a 7600x    Worth it yea but at the time ram prices where good    Kinda wish I keep both I was expecting to upgrade my kids PC with a 5700x3d I doubt that will happen now,Positive
AMD,Here I am still rocking my amd 3950x happy as a clam with no plans to upgrade any time soon!,Positive
AMD,I remember knocking this chip when it came out because it wasn’t unlocked for overclocking and now I look like a doofus,Negative
AMD,Sad day when tech enthusiasts ask for almost obsolete tech due to current tech prices.,Negative
AMD,"Best CPU I've ever purchased by far. Paid 300, 3 years ago and there's still nothing it struggles with, not even a bit.",Positive
AMD,Would the effort to improve upon the Zen 3 chips (Zen 3+/Zen 4) be worth it over simply restarting the production of already existing designs like the 5800X3D? Or using a more advanced node like 6nm or 5nm?,Neutral
AMD,Yep would really like to buy one atm.,Positive
AMD,Got 5700x3d this year and it’s awesome. It’s essentially same speed as modern CPUs gaming at 4k,Positive
AMD,Was thinking of am5. But with all these BS. Ill rather stay am4 and get a 5800x3d,Neutral
AMD,"I upgraded from a 3600 to a 5800X3D that I got on sale, best upgrade ever. I then upgraded to the 9070XT, which I got for $30 over MSRP, so I will be skipping AM5 altogether, especially now since RAM pricing went insane. Fuck..., maybe I will die using this setup if the tech bros just keep fucking over consumers.",Positive
AMD,I fully expect the RAM market to return to normalcy by August,Neutral
AMD,"Very happy to have grabbed my 5700X3D when I did, upgraded my GPU to 5070Ti shortly after. Yesterday my APC Smart-UPS made a very loud pop sound and threw up an error code as my system immediately went dark. Smelling burning I scrambled for a power bar and all I could do was pray my computer didn't get fried because hitting PC Part Picker is NOT very fun these days!",Positive
AMD,Please just 5850X3d or maybe 5955X3d for us. I will even buy it original price,Positive
AMD,5950x3d would be fucking sick,Negative
AMD,*revive ddr4,Neutral
AMD,Hail the king baby.,Neutral
AMD,I'd buy two for my kids.,Neutral
AMD,I had to settle for the 5700x3d…. I actually swapped from a 5800x because I mostly do 90% gaming 10% everything else /sad,Negative
AMD,I would absolutely get one. I'd give my kid the 5600x3d,Positive
AMD,"I would get one. And if there are bundle deals with an AM4 mobo, I would too. Not that my current mobo can't handle it but I broke the USB 3.0 header on it and I can't use the front panel USB ports on my case. Stupid reason I know but well.",Negative
AMD,"5800X3D the GOAT, right after it is 5700X3D",Positive
AMD,My old 5800xed and my old R7 make a good steam machine.  In a small ITX box.,Positive
AMD,"I actually upgraded at the beginning of the year, from a 3700x to the 5800x3d, 16 to 32gb at around 65€ and got the 9070xt, don't regret it at all seeing what again happened, next step will be a monitor upgrade :)",Positive
AMD,"I have a 5900X, I would ‘downgrade’ my cpu to one of these and move my current chip to my server (currently an i5 6500)",Neutral
AMD,Or make somehow make the new processors/mbs work with DDR4 lol. That's the only way I'd upgrade. I'm not an engineer so I'm not sure if that's even possible.,Neutral
AMD,5**9**00X3D.  Dr. Su literally had one in her hands when she unveiled 3D V-Cache,Neutral
AMD,OMG please!,Neutral
AMD,seems gamers are not paying attention to market forces   AI makes more money to sell to,Negative
AMD,Looks like we're going the distance buddy.,Neutral
AMD,But I just got a 5700x...,Neutral
AMD,"went from 3700x to 5800x3D 2 years ago, best investment ever  I am planning to keep this and skip AM5+DDR5 going directly to AM6+DDR6 :P",Positive
AMD,5700x3d would be cheaper for 10% less perf max. But yeah the idea is good!,Positive
AMD,I'd love to see a 5900X3D or 5950X3D.,Positive
AMD,"Release a 5950X3D with 3D cache on both dies, and I'd buy it.",Neutral
AMD,Be careful what you ask for. Seems like a perfect opportunity for AMD to come out scalping with it costing $600 or more.,Neutral
AMD,"Rocking 5800x non 3D, i won't replace it anytime took tbh",Neutral
AMD,"I just sold a 5800x3D for more than I bought it 3 years ago, which was also more than it cost me for the 9800x3D lol",Neutral
AMD,I bought a 5700x3d off aliexpress years ago for $120. Feelsgoodman,Positive
AMD,On paper it sounds like a good idea for them to spend some time backporting new archs to AM4 if this memory shortage will last a while. But I still doubt they have enough time to do that if they were to start today.,Neutral
AMD,"I regret not getting a 5800 or 5700x3d because I thought I’d have enough saved up to do a whole mobo, RAM, and CPU upgrade for my rig.  Waited too long then the RAM shortage hit and now I’m stuck with my 3700x for the next few years.",Negative
AMD,"I upgraded mine a while back, keeping it until AM6 at least",Neutral
AMD,"Been trying to find one, but $400+ for a used, 5 year old CPU from a random source on eBay is just a terrible deal. Fuck this bullshit.",Negative
AMD,"I don't see this happening, because the current memory shortages will not be fixed by switching back to DDR4. DDR4 production will be ending very soon and memory manufacturers will be more than happy to shift all production to DDR5.   The current memory shortage problem is a result of an artificial surge in demand while manufacturers remain cautious about ramping up production too hard, lest the bubble bursts and they have to drop prices through the floor. It's going to be a while before either side gives in.",Negative
AMD,"I would love that. By the time I got my savings in a better state, I only found a store selling the last of the stock at premium price. And switching to AM5 is too expensive for my taste. So yeah, it would get in line to buy one.",Positive
AMD,No 5800x better more ram performance gain if the produce them with higher quality they could all run 5ghz allcore and 2x16gb 4000 1:1 mode,Positive
AMD,"AM4 changed the game but none of us has predicted for how long yet! Went with a full custom watercooled AM4 platform since 2017 on 8xcores CPUs on a Crosshair VI Extreme paired with a GTX1080Ti and a 2x8GB kit of B-Die DDR4 (@3800C16currently)  : 1700X 4.1GHz / 2700X 4.2Ghz / 3800X 4.3 Ghz / 5800X3D Undervolted 4.45 Ghz... And with the addition of an RTX3080Ti recently, that 7 years-old AM4 platform feels like it's gonna keep taking it for years to come like a champ!",Positive
AMD,I hope they make more 5800x3d. They stopped by choice.,Neutral
AMD,Where can I get one nowdays? Seems sold out everywhere.,Neutral
AMD,"Yes please!!... Currently I'm still rocking a 5800x3d and have 2 other AM4 pc's waiting for an upgrade. At that time the original 5800x3d had a restriction on clocks and temps due to the 3d v-cache being on top... With the current 9800x3d using the v-cache at the bottom to take advantage of better coolers and temps, I would definitely buy at least 2 more and know some partners that would make the final upgrade for their long forgotten AM4 still rocking ryzen 3000 cpu's.",Positive
AMD,"5800x3d is starting to show it's age, but I'll run it till AM6 I reckon. Same with my 7900XTX. CPU bound on BF6 is kinda annoying. 5800x3d can't really deliver more than 120-130FPS in BF6, and makes me CPU bound which is annoying.",Negative
AMD,"5700x3D (on my Ryzen 1 cheap motherboard) and RX 9070, I'm ready for the years to come.",Positive
AMD,Which gamers and which reviewers exactly? 🤔,Neutral
AMD,yes please,Neutral
AMD,Releasing 5850X3D (with the compute die on top of 3D cache just like 9800X3D) would be better.,Neutral
AMD,One of the best buys ever for me,Positive
AMD,It's definitely the 1080 Ti of CPUs.,Neutral
AMD,300 well spend bucks 3 years ago...,Neutral
AMD,Yeah Id be willing to pay a pretty penny if they would be willing to sell it for a slight mark up.,Neutral
AMD,"Same, i'm skipping AM5 all together",Neutral
AMD,"Bring back the 5800X3D, please!",Neutral
AMD,"Made the jump from 1800x to 5800x3d, was incredible. Not sure how much the older x370 motherboard chipset is a limiter at this point.",Neutral
AMD,Picked up a brand new 5800X3d for $280.00 two years ago and sold my 5800X for $160.  Social media haters said it was a side grade and not an upgrade.  Look who is laughing now.  Ha ha ha.  My 5800X3d paired with my rtx 5070 ti is playing everything on Ultra and high with no bottlenecking.  Will probably be still doing the same for the next 3 years.  And I was also able to pick up 64Gb of G.Skill tridentZ (4X16) 16 18 18 39 for $124 in June this year.  So happy I purchased before this rampocalypse,Positive
AMD,"If they would now come with a 5850x3d with slightly higher clocks, everyone and their mom would buy it.  Companies often underestimate the demand for the ""last best option"" on last gen platforms that are still being used.",Neutral
AMD,Bummed to not have got one of these. My 5600x is fine but I know I’m missing out,Negative
AMD,"Yes! I really hope they do! I'd buy a 5800x3D immediately!   I started to upgrade my AM4 system with the goal of transitioning over to an AM5 build, but the RAM issue has me holding off on things.   I currently am running a 5600x, 32GB RAM(3600), 2x 2TB NVMe, and a 5070ti on a 27"" curved 1440p Asus monitor.   The 5070ti was an upgrade from the 3060ti I was running. And I upgraded to a Corsair 850w psu at the same time.   All I need to complete the new build is a mobo,cpu,and RAM. Possibly a new cpu cooler.   But, if I could get a 5800x3D for a good price, I'd probably buy that and stick with AM4 for awhile longer.",Positive
AMD,Upgraded my R5 3600 to R7 5800X3D like a year ago. Best decision ever. Think i can survive 3-4 more years with it.,Positive
AMD,I almost bought one of these a year ago to upgrade my old system that could take it.     regretting that.,Negative
AMD,"Yeah, I won't be changing mine out any time soon.",Neutral
AMD,I got that cpu 3 days ago. Decided to upgrade laterally and keep ddr4. It's really a good cpu.,Positive
AMD,Intel is looking pretty good these days if you're on a budget and want to do ddr4.  I just grabbed a 14600k for my dad's build for only $210 CAD and a 32gb kit of ddr4 was only $200 CAD.I would've built him an AM5 system for sure if DDR5 prices weren't insane.,Positive
AMD,I feel dumb but why exactly? Seems like linked to ram price but ddr4 is also getting more expensive is it?,Negative
AMD,I sold my used 5800X3D a few weeks ago for more than what I paid for my 9800X3D 😅,Neutral
AMD,"Hopefully amd bean counters are looking in to it...  If they figure revenue is greater than cost then they would go for it.  The big factor there is estimating how big the market is for a 5850x3d or maybe a 6800x3d or something where they find another foundry to kinda make what they had or jank a zen4 into am4.  The likelihood of the same sku would be very low because of these factors.  So how big is the market/potential revenue and is it more than the cost of janking something together.  That market size question falls each day we get closer to the end of the ram shortage.  I gotta think it is at least being kicked around.  Maybe it will die, but it has to be a thought.",Neutral
AMD,"Upgraded my 3600X to 5800X3D kind of on a ($600) whim. I didn't really have any good reason to. I guess my logic at the time was to get the 'last, and best version' of AM4 and ride out this build. Other than paying full price for it, I've been happy with it. Can't say the same about the buggy Radeon driver experience however.",Neutral
AMD,"this is stupid af, i'm for progress and more performance my 9800x3d is light years better than my old 5800x3d",Negative
AMD,"I've had my 5800X3D for just over 3 years now. It's going to my wife and I'm upgrading to a 9800X3D. I bought 64gb of DDR5-6000 for $500 CDN less than it is today. Still more than at its lowest, but I knew I had to pull the trigger while I had the money.  Edit: Canadian dollars not US dollars",Neutral
AMD,"Well, instead of reviving the 5800X3D, isn't it just easier to make a DDR4 AM5 motherboard?",Neutral
AMD,Ditto.,Neutral
AMD,I will rock it forever  if they bring it back,Neutral
AMD,"Mine died under warranty, after waiting 6 months for a replacement I gave up and had to settle with a 5700x. Pretty sad and annoyed by it as this was my plan too.",Negative
AMD,Same. Still going strong.,Neutral
AMD,Plan to rock mine until it can no longer play MMO FFXIV. I'll leave to the Japanese to support the game on a GTX 970.,Neutral
AMD,5700x3d til ATLEAST Second gen AM6.,Neutral
AMD,"Yep, not gonna start considering an upgrade until long enough into AM6 to not be an early adopter.",Neutral
AMD,Same here. Getting in on the end of AM5 isn’t anywhere as appealing as the start of AM6. My next motherboard is gonna get a long time of use…,Negative
AMD,Same brother !,Neutral
AMD,Same!,Neutral
AMD,Yeppers,Neutral
AMD,When is AM6 expected?,Neutral
AMD,"I was thinking about getting a 9800x3d at some point but it's just not even worth it. At this point I have a 9070xt, 5800x3d, and 32gb of 3600 ddr4 ram. I mainly game at 4k, so I'll rock the 5800x3d for a while and just upgrade my GPU in like 2-3 years.",Negative
AMD,"I have a 5700x3d, 32 GB DDR4 4000, a 2 TB SSD, and a 4080 Super.  I’m well situated to outlast the AI boom, figure I should be able to get playable framerates on new games for the next 5 years.",Positive
AMD,Rocking mine til AM7,Neutral
AMD,The chip makers are making so much money off of AI that they don't even want to serve the consumer market at all anymore because the margins are lower.,Negative
AMD,"Bought a 5700X3D for $120 USD brand new and sold my 3700x for $75. Such a solid upgrade. Paired with my 5080, I should get another few years on my system.",Positive
AMD,Likewise mate! Also switched out the 3700x for the 5800x3d. Also switched a 2080s out for the. 6800XT at the time.   The 9070XT has been cheaper here as compared to what I paid for the 6800xt. Still can't decide if it's worth it now or worth waiting it all out,Neutral
AMD,"Same!  I got it in a discount bundle from microcenter.  Like 350 for the chip, a new mobo and 32 gigs of ram.  Crazy amazing deal",Positive
AMD,"I don't even blame the RAM manufacturers **this** time.   I think the RAM manufacturers know AI is a bubble. It takes multiple years to build new manufacturing plants to expand production. Meanwhile, the AI bubble could pop in like 6 months.  I blame Sam Altman buying up all the RAM.",Negative
AMD,"I rotationally upgraded my current PC from the 1800x on a 370 platform, up to a 3800x & finally a 5800X 3D & 570, glad I got 32GB of RAM when I did, because the G.Skill modules and latency haven't been available for quite a long while.",Positive
AMD,Same. Got my 5800x3D when rumors were that it was a very limited run so paid £337 in January 2023. It’s nearly 3 years old and it’s been a beast. Was an upgrade from a 3600,Positive
AMD,"Likewise, still feel no need to upgrade from it.",Negative
AMD,I made the same upgrade last year. Amazing performance along with a 4070 Ti.,Positive
AMD,">got my 5800x3d years ago best purchase ever. I was ever so hesitant from coming from the 3700x cpu but the magic that was 3d cache can not be underestimated.  I made the same upgrade and I totally concur, going from a 3700X to a 5800X3D definitely felt like a bigger upgrade than going from an i5 4670k to a 3700X.  >now if we can just get a big amd gpu i wouldn't have to worry for a while.  As far as I'm concerned my 6900XT still delivers on 1440p on most games, but 16GB of VRAM is beginning to feel a little cramped for the newer games.",Positive
AMD,Exact same 3700x -> 5800x3d for me,Neutral
AMD,How much is the difference now ? And what gpu you have?,Neutral
AMD,"If anything prices on CPUs will drop thanks to lower sales due to increased memory prices, people will keep from updating for a while.  DDR4 production has been completely shut down since 6 months so AM4 parts availability will drop anyway and production plans has already been phased down in favor of other platforms.",Negative
AMD,"Switched to the x3d while using a 3700x as well. 330 at microcenter on the first black friday after its release and I was floored at how much more performance I got. Double the fps in wow was completely unexpected, spikes in fortnite were nowhere to be found.",Positive
AMD,Why would ram manufacturers scale for artificial demand tied to an industry everyone knows is a bubble,Negative
AMD,"We just need to wait for the fabs to catchup on production and eventually there will be some available for consumers and bring prices back to more reasonable levels. This ai bubble popping will just hurt gamers and consumers because it's basically fueling future technology development and advancement. If we hold through the expensive price hikes, we'll benefit from rapid growth in the end.",Neutral
AMD,"I kinda regret my purchase of the 5800X3D. The 5700X3D got announced within literal weeks for hundreds of $ less for like \~3-5% less performance. Couldn't even return it as I was not in the country at the time, and by the time I came back I was well beyond the return window.",Negative
AMD,Also went from 3700X to 5800x3d early on. The difference was significant even with a 1080Ti at 1440p.  Eventually I swapped it for a 6800XT and now an 9070XT since release. Works great.,Positive
AMD,"Managed to buy a new 5700x3d , the last one , few months ago.",Neutral
AMD,"It takes time to scale up, guaranteed they're already in the process of doing so",Neutral
AMD,The scarcity isn’t artificial at all.  The memory makers found buyers willing to spend far more than those that make consumer memory.,Neutral
AMD,"> I don’t think any AM4 x3D silicon has rolled off the assembly line since last September.  SKUs haven't, but AMD extended Milan availability to 2026, which means the silicon is there.",Neutral
AMD,"The beauty of it is they wouldn't need to do a Rocketlake style backport, just put newer chiplets alongside the older IO die on the package.",Neutral
AMD,Almost definitely easier to release an AM5 motherboard that supports DDR4 than a newer Zen on AM4.,Positive
AMD,"Bartlett Lake is allegedly for the embedded market only, no?",Neutral
AMD,"Even if next years market is catastrophic, I just dont see how spinning up an entirely new hybrid SKU (So eg Zen 4 or 5 but with DDR4 Support) would make any economical sense when looking at the cost and also the fact theyre already 2 Gens in on AM5.",Negative
AMD,"I'm up to 4 AM4 machines in my house now.   3600x, 3700x, 3900x, and my 5800x3d.  I'll be running AM4 for probably the next 10 years until I retire the last of them   Edit:  also have a laptop with a 5600h, but that's not Am4",Neutral
AMD,"Oh man, I fucking wish. I’d be all over it.",Positive
AMD,THIS!!!  I really enjoy my 5950x but it could use a little more oomph in the gaming department.,Positive
AMD,Pfft just have them make a 5950X3DX edition with a 32GB HBM MALL cache with DDR4.     Problem solved!,Neutral
AMD,"It was in the labs, Lisa Su even showed a prototype at Computex 2021 lol.",Neutral
AMD,"They actually did...but it was just an engineering sample that never made it to market 😞. They maxed it out with 3DV-cache for each CCD, too.  https://youtu.be/RTA3Ls-WAcw?si=\_6NcQQG798fTH8aR&t=1072]  How awesome would that be tho, if they finally crowned the AM4 platform with a 5950X3D (with the cache on both CCDs, flipped in the right position this time for higher clocks, and windows cache optimizer chipset drivers).  Do it AMD!!!",Positive
AMD,Yes  And maybe give us 2 x3d dies,Neutral
AMD,If they did that it might straight up cannibalize themselves for how too good it would be,Neutral
AMD,I’ve been waiting for this since X3D has been a thing.  I will unironically pay $400 or more for that chip so I don’t have to upgrade my entire PC when my 5900X is no longer cutting it. Getting an extra 3 years or so would be worth it.,Positive
AMD,Gimmi.,Neutral
AMD,Or bring back dual CPU motherboards to the DIY market,Neutral
AMD,"Did you check the new 9950x3d-2?... It has v-cache over each CCD, instead of only one of them... A 5950x3d-2 would be the ultimate and final upgrade for an AM4 setup... I will definitely do the jump to AM5 when RAM prices get closer to normal next year and with ryzen 10000 jump in performance.",Positive
AMD,They don't make them anymore and they're one of the best DDR4 cpu's available. I bought a 5700x3d at the beginning of the year and it had already risen to $250. I imagine it's higher than that now.,Positive
AMD,Man i would sell my 5800x3d now if it's fucking $400 so I can upgrade but ram prices are ridiculous. Maybe I'll wait til AM7 now looking at hardware prices... Was previously gonna do AM6 upgrade.,Negative
AMD,RAM price is expected to come down in 2028.,Neutral
AMD,"> stay affordable  lmao, even bottom of the barrel ddr4 is like triple the price it was a few months ago.",Negative
AMD,Got a 5700x3d on cyber Monday from AliExpress and they’ve shot up since. Can’t wait to slot it in to replace my 2600. My 1080ti will be happy,Positive
AMD,AM4 Boards a true troopers.,Neutral
AMD,"The reason the 5700x3d released so late, compared to the 5800x3d, is that it's just 5800x3ds that didn't make the cut in clockspeed.  I got one towards the end of last year for a great price, and the performance difference is usually a little bit more than 2%, but it was literally less than half the cost of a 5800x3d.",Neutral
AMD,Same as me. I bought 128GB RAM but two of the sticks stopped working. 64 is enough though.,Negative
AMD,Thankful about being two generations behind?,Neutral
AMD,They will or they will starve for sales and their Mobo partners will as well.,Neutral
AMD,I noticed a difference going from a 5600x to 5800x3d. Mostly in the 1% lows.,Neutral
AMD,Unless you're playing cpu bound games it's probably not worth it. If you can get it for cheap then sure.,Negative
AMD,"Yes, if you can find it for a reasonable price. I went from a 5600x to a 5700x3d and it was worth it to me, but I paid like $130 on AliExpress a bit over a year ago.",Positive
AMD,DDR4 production is over. This product would only be for people already on the AM4 platform.,Negative
AMD,"The AM5 Chips only have ram controllers for DDR5. You could make a motherboard with DDR4 slots but I seriously doubt it would work. Before, the ram controller was on the motherboard but now it's inside the CPU itself so we'd just be back to square one",Negative
AMD,"Not going to happen, Zen 3 X3D Production ceased like a Year ago and every x3d product based on Zen3 sold are just downbinned 5800X3Ds that failed QA Testing. Even if they now decided to buy both 7nm production capacity and Front side Hybrid Bonding it would take at least a year until you would be able to buy new 5800X3Ds, and all of which would be pretty expensive for what to them is a last last gen niche product. I mean yeah it would be pretty nice but economically this wouldnt make any sense even if they tried to be nice.",Negative
AMD,How’d it die?,Neutral
AMD,If this happens to me I'll probably just sell the rest of my PC at this point lol,Negative
AMD,I just got my hands on GTX 970 to replace 660Ti on my second scrap PC and it's surprisingly capable card running Valheim at 1080p max settings,Positive
AMD,"Same here, upgraded my 5600x to a 5700x3D before they became expensive AF. Upgraded my 6800 to a 9070XT a few weeks ago, initially wanted to do a full upgrade somewhere in early '26, but at the moment I can play everything on ultra with 150+ and beyond fps on 1440p, so I'm good for at least 3-4 years, I reckon.",Neutral
AMD,"Same, with my 5700x3D and 4070 Super, I think I’m good for some years",Positive
AMD,"I bought in 2020, at what I thought was the end of AM4–but I had the money (and time, lol) to upgrade. I got a 5600x figuring maybe down the road I could get a 5900x or 5950x if I wanted something faster. Instead, I got a 5700x3D cheap last year which has been a nice upgrade!  I’m hoping AM6 has more cores per CCD, I’ll probably start with an x600 and then upgrade to x3d on the 2nd or 3rd gen.",Positive
AMD,Not anytime soon. 28 or 29 or something .,Neutral
AMD,I have a 5800x3d and 4090.  I built a newer pc with a 9800x3d and 5090.  Performance feels incredibly similar.,Positive
AMD,It would use fabs that aren't currently being used to make AI chips (for the most part I would assume) so it would be more additional revenue than one eating into the other.,Neutral
AMD,greedy bas\*\*\*\*s. But nothing new,Negative
AMD,"For AMD their AI gross margins are actually below their corporate average. Meanwhile, gaming CPUs for AMD have always been above their corporate average margin.   There's also nothing that stops AMD from doing this. There's no conflict of wafer supply or other packaging shortages that overlap with AI. The hardest part might just be trying to start back up those old packaging lines and old wafer starts. It could be pretty difficult to do it. And as soon as they could have any products out would be in 6 months which would probably still be worth it. But who knows.",Neutral
AMD,yeah i came off a 5600x for a 5700x3d and paired with a 9070xt its really a solid combo. I got so lucky doing a minor refresh on my pc earlier this year.,Positive
AMD,"I swapped my 2700x for 5700X3D and Vega 56 for RX 6800 XT, works wonders :3",Positive
AMD,I bought my 5700X a year ago for 140€ new. I hate the market.,Negative
AMD,Same cpu paired with my 5070 ti. How much longer will this config last?,Neutral
AMD,"If you can afford the 9070xt now, do it and sell your 6800.  Shits only going to get more expensive the more you wait and it’s a solid card.",Negative
AMD,"I upgraded my CPU 3 years ago and my GPU 2 years ago. Went from 3700X to 5800X3D and RX 5700XT to RX 7900XTX. Managed to sell my old CPU, GPU and some other stuff and got enough to upgrade my RAM few months ago. I started doing a lot of 3D work and game dev stuff, so 16GB wasn't cutting it, got an upgrade to 64GB for €250. Seeing the current shit show, im so glad I made the jump to 64GB",Neutral
AMD,Supposed leaks put it at 2028 before prices come down due to backlog demand even with increased production.,Neutral
AMD,Why scale up? If the bubble pops. They would be stuck holding the bag and ram would drop even more than if it just pops with less equipment,Negative
AMD,"Except it is artificial.  Both Samsung and SK Hynix have just come out and said it lol  https://wccftech.com/two-of-the-biggest-dram-suppliers-are-skeptical-about-increasing-production/  This is all due to OpenAI buying insane amounts of ram production to keep others from getting it, artificially inflating prices.",Negative
AMD,"The extension was over 2 years ago already with availability up until 2026 theoretically only for certain CPUs. AMD likely ended manufacturing this year.  The peak run for TSMC N7 was 2019 - 2021, with most capacity dropping off by 2023. This is a 9 year old node now.  By now TSMC N7 is a small shadow of what it was with most remaining capacity bought by 2nd tier chip designers like Huawei.",Neutral
AMD,It’s a bit of a mystery. Probably should have been released by now but they haven’t officially cancelled it,Neutral
AMD,"Kinda same, as I'm now on AM5 but my bf inherited my 3900X and my server runs a 1600X lol",Neutral
AMD,"Got 5950X, 3800XT, 2700X and 3200G systems in our home. And my old 1700X laying in a box.   I just today was looking for 5800X3Ds to replace my sister's 2700X and living room 3800XT. Guess I will just get some 5800Xs instead, as the X3Ds on eBay are over 400€ while the 5800X is everywhere for around 160€ new... The 3200G will also be replaced with a 5700G soon.",Neutral
AMD,"Right there with you. I kept buying all of my co-workers ""old"" PC parts for my home servers and wife and kids machines. Up to 6 AM4 machines including mine.",Neutral
AMD,"3 here with 5800x, 5700g, 1600x in systems and a 2400g laying around.",Neutral
AMD,"I tried switching out my 5950x with a 5800X3D for a little bit. The difference in gaming wasn't all that noticable for me and the games I play. I ended up giving the 5800X3D to my nephew and going back to the 5950X. I need all the core for blender, code compilation and other multi-threaded workstation stuff, so I found the 5800X3D too slow for embarassingly parallel workloads.      Guess I'm waiting out the next few years before it makes any sense to upgrade. If I were to buy anything it would be the 395+ AI MAX.",Neutral
AMD,Process lasoo,Neutral
AMD,BLACK EDITION,Neutral
AMD,I wonder if anyone watched that video... He said they did the 7950x on AM4 before they made the 5800x3d... https://youtu.be/RTA3Ls-WAcw?t=1101... I would guess that is the 7950x3d... Which means they can just duplicate the 7950X3D on AM4 using the 7000 architecture.,Neutral
AMD,That would hurt productivity while gaining nothing for gaming.,Negative
AMD,"If it is too expensive to upgrade AMD will starve... They can support newer features on new MOBOs for the AM4 line while pushing newer CPUs for the AM4 line. Let people reuse the DDR4. Helps customers, helps mobo manu and helps customers. Win/Win/Win in an otherwise screwed situation that is expected to last until 2028 at the earliest.",Neutral
AMD,That doesn't make any sense when there are 16 core consumer CPUs. Most people don't need that much and anyone who needs more can get threadripper or epyc.,Negative
AMD,For gaming latency would get worse. Trust me we've already had problems with cross CPU memory talk in big data apps. Gaming would be much worse.,Negative
AMD,I definitely am not used to my hardware appreciating in value.,Negative
AMD,Yea they’re like $330 now. I’m just rocking a 5600x. Not much point upgrading to non-x3d chip and the x3ds are too expensive =/,Negative
AMD,Check ebay. Nothing goes for under $400 without damage like bent pins.,Neutral
AMD,By whose estimate?,Neutral
AMD,"Nobody knows when the prices will come down. They are expected to NOT come down in 2026 because that DRAM manufacturing capacity is already sold out.  When prices come down depends on when demand growth from AI slows down. Some people think it's a bubble and will soon pop. I don't think it's a bubble. RAM, electricity and permits seem to be the big bottlenecks for data centers. Financing could become an issue if the markets turn bearish on AI.",Negative
AMD,"I fully expect it to come down by August next year, as the AI bubble starts bursting",Neutral
AMD,You upgraded your CPU before that old ass GPU?,Negative
AMD,"You should check the warranty on your busted sticks--a lot of manufacturers give a lifetime warranty.  I had two sticks die, and they were replaced under warranty...but I also only had 2 RAM sticks, so I had to shell out for replacement RAM while I waited for them to come back, which was annoying--but it beat just not using my PC, lol.",Negative
AMD,Yeah because I would have needed to build an entirely new system to upgrade past a 5800x3d dipshit,Negative
AMD,Couldn't they just create a conversion chip within the motherboard to make the CPU think it's DDR5 ram?  There might be a minute drop in speed from the conversion but it's better than paying half a grand in Ram.,Neutral
AMD,It would make plenty of sense if AM5 sales are way down due to the RAM situation.,Neutral
AMD,I had two 9950x die on me for no reason. Just happened I guess,Negative
AMD,Just went to turn on the pc one morning and it wouldn’t boot,Negative
AMD,I had two 9950x die on me for no reason. Just happened I guess,Negative
AMD,Same here with the 5600x to 5700x3d upgrade when it was cheap on alibaba last year. So glad that I made that upgrade.,Positive
AMD,"That's what I was worried about if I did make the jump, glad to hear I'm not super crazy",Neutral
AMD,"You can't easily refit a fab for an entirely new and very different process node. You basically would prefer to build a whole new one. In particular, the most advanced chips have very specialized processes / devices, EUV litho and Gate all around FETs.",Neutral
AMD,It’s the same fabs tho.,Neutral
AMD,What kind of increases did you see from the 5700x3d?    Ive been scouring marketplace trying to find one of the x3d chips and ive begun offering to swap my 5600x plus cash so they can put it in their build and still sell a complete system. No dice yet.,Neutral
AMD,When I saw it at that cheap of a price I bought 3 of them and gifted them to people that were on am4 still in my friendsgroup. I knew it was too good to pass up,Positive
AMD,At 1440p? 4-5 years at decent settings probably!,Neutral
AMD,Or keep the 6800 as a backup. Who knows what availability looks like for the next 12 months?,Neutral
AMD,"Money.  Just because the bubble pops doesn't mean the technology will be going away. Homes and the internet still exist.  Samsung, micron, sk hynix have all already reportedly begun to scale up and it's going to take years. Probably be 2029-2030 before these new production lines are ready.",Neutral
AMD,I can see industry wanting to use their existing DDR4 and upgrade to high core count EPYC 7003 high core count SKUs. That might be incentive enough for AMD to restart production.,Neutral
AMD,My server is the 3900x.   I picked it up from Facebook marketplace.  Came with a x570 motherboard and 64gb of ram for $250.   Couldn't pass it up.     It replaced my old Fm2+ motherboard with an athlon X4 860k,Positive
AMD,Thanks for that. I got the 5950x when I thought it was gonna be the halo chip. This helps allay my fomo.,Positive
AMD,I went 9800x3d and threw my 5950x in another PC and the difference in gaming can be massive but it depends on the game and how CPU dependent it is. The heavier ones are going to see the biggest differences. 5800x3d was basically on par with 1st gen AM5 non x3d stuff.,Neutral
AMD,I use that when I game and stream simultaneously.  It's why I bought the 5950x.  Like two PCs in one.  :D,Positive
AMD,Depends on the game and depends on the productivity  There are reasons to want a 16 core 3d vcache CPU,Neutral
AMD,"Youre ignoring the fact that backporting newer Zen Architectures to work an AM4 and support DDR4 would require completely redesigned silicon for a product that would cannibalize their newer chips, for an publicly traded company, this just doesnt make any economical sense.",Negative
AMD,Yea its a weird feeling. My ram went from 200 to 700,Negative
AMD,"If you haven't been paying attention, EVERYONE in the industry.  https://tech4gamers.com/memory-shortage-till-2028/  https://www.tomshardware.com/pc-components/dram/the-ram-pricing-crisis-has-only-just-started-team-group-gm-warns-says-problem-will-get-worse-in-2026-as-dram-and-nand-prices-double-in-one-month  Mircon wouldn't leave the consumer market if it was just a flash in the pan..  It's just price fixing at it's finest..  https://www.tomshardware.com/pc-components/dram/memory-makers-have-no-plans-to-increase-production-despite-crushing-ram-shortages-modest-2026-increase-predicted-as-dram-makers-hedge-their-ai-bets",Neutral
AMD,Mine!,Neutral
AMD,Yes it is old ass but it still kicks ass. The cpu and 32gigs ram will keep it running. Next upgrade will be gpu when i see the need,Positive
AMD,"Thanks. Maybe I can get them replaced on warranty, sell them and retire early :)",Positive
AMD,chill down pal lmao,Neutral
AMD,I really don't think that's how it works. Ram is electrically incompatible between generations. The fastest DDR4 is also slower than base DDR5 meaning that it probably couldn't run at all (3600MT/s for DDR4 highest VS 4800MT/s for DDR5 lowest),Negative
AMD,Which currently they aren't and even then I think it would be highly unlikely for that to happen on a scale which would be noticeable enough to show up on AMDs Radar since Intel's current Plattform also is ddr5 exclusive.,Neutral
AMD,"I had my first 5800x3d arrive DOA, Amazon thankfully replaced it for me after a 5 min chat",Positive
AMD,Did you upgrade the Bios? Apparently MOBOs are overvolting.,Neutral
AMD,"At first I wasn't sure if it was a big enough upgrade, but it certainly was. Again, good for at least 4 more years.",Positive
AMD,"Yeah I made a machine to play PoE in 4K in 2024 summer with a 7900XT and an 5700X3D. It delivers. And it'll deliver for quite some time. Although I am upgrading the video card in a roundabout way: I will move from Europe to Canada next summer, I bought a 9070XT in Canada during Black Friday and sent it to a friend, will sell the 7900XT locally next summer and that's how I will get a practically free upgrade. I will take my CPU, RAM and SSD with me -- small, lightweight and by now super valuable.",Positive
AMD,"5800x3d are 7nm, those fabs aren't pumping out bleeding edge AI chips.   There is still use of those fabs for other purposes, but I hardly see them at capacity",Neutral
AMD,"I wish I had friends like you 😭  That CPU here was like double the price I spent on the normal one, and I had to save for RX9060XT so...  It is what it is.",Neutral
AMD,Much like how I kept the 2080s as a backup.   If the 9070XT is going to fail within a single year it's not a good buy though.,Negative
AMD,Maybe vene hopefully update them as well while they are at it?,Neutral
AMD,Nice! Daddy has the 9950x3d and it is nice having so many threads for sure,Positive
AMD,"The ccd to ccd latency kills any benefit for gaming.   Some games doesnt even benefit from the extra cache and would rather run on the non 3d cache ccd.  And because zen3 has the cache on top of the ccd, there is a frequency penalty which hurts alot of productivity. There are ofc some that can use the extra cache, and in that case it would benefit. But that starts to get very niche on a desktop cpu, and gives up performance in most other use cases.",Neutral
AMD,"You are ignoring the fact they already mated the 7950X to am AM4 platform before they released the 5800x3d. It was an engineering sample.;)  [https://youtu.be/RTA3Ls-WAcw?t=1101](https://youtu.be/RTA3Ls-WAcw?t=1101)  At least the AMD Engineers claimed they did it. But what do I know, I am ignoring facts. I am also ignoring the fact that the 7000 series chips are still being produced.  ""Key Details from AMD Engineers  During a visit by Gamers Nexus to AMD’s testing labs, engineers clarified several points about these ""hybrid"" samples:  * **Internal Proof of Concept:** AMD engineers explicitly stated they had the  **Ryzen 9 7950X**  (Zen 4) running on **AM4 boards** internally. This was done primarily to test the Zen 4 compute dies (CCDs) and architecture using the existing, mature AM4 infrastructure before the AM5 platform was fully ready. * **The ""Hybrid"" Design:** To make this work, engineers likely paired the **Zen 4 CCDs** (5nm) with a compatible **Zen 3-era I/O die** (12nm) that supported DDR4 memory and the AM4 socket. * **Decision to Cancel:** Despite having functional 16-core Zen 4 samples on AM4, AMD chose not to bring them to market. They instead focused on the  **Ryzen 7 5800X3D**  as the final high-performance gaming upgrade for the AM4 platform because the 3D V-Cache provided a more significant gaming benefit on the older socket than a core-count increase."" * [https://www.youtube.com/watch?v=iM1NXHQ8YTA&t=2s](https://www.youtube.com/watch?v=iM1NXHQ8YTA&t=2s)",Neutral
AMD,"The shortage lasting until 2028 is probably the best case scenario here, unless the bottom falls out of the AI market entirely. Chip manufacturers might be able to catch up with current demand by 2028, but as long as the AI companies have effectively infinite money to throw around they can just keep buying up the new production capacity",Neutral
AMD,"And that's assuming OpenAI doesnt buy even more, or their competitors don't scoop up more to make sure they have enough.  The only thing that's fixes this is the AI bubble popping.",Negative
AMD,There are people who went to jail for the last price fixing scandal. I doubt anyone in power has forgotten. It was a pretty big deal,Negative
AMD,"Another ""fact"" you know nothing about. [https://www.pcgamer.com/hardware/motherboards/the-dominoes-are-falling-motherboard-sales-down-50-percent-as-pc-enthusiasts-are-put-off-by-stinking-memory-prices/](https://www.pcgamer.com/hardware/motherboards/the-dominoes-are-falling-motherboard-sales-down-50-percent-as-pc-enthusiasts-are-put-off-by-stinking-memory-prices/)  [https://www.tomshardware.com/pc-components/ram/bewildered-enthusiasts-decry-memory-price-increases-of-100-percent-or-more-the-ai-ram-squeeze-is-finally-starting-to-hit-pc-builders-where-it-hurts](https://www.tomshardware.com/pc-components/ram/bewildered-enthusiasts-decry-memory-price-increases-of-100-percent-or-more-the-ai-ram-squeeze-is-finally-starting-to-hit-pc-builders-where-it-hurts)     You point to the year overall and claim all is well. You do not bother reading the whole situation and ignore that there is a time lag between prices of ram going up and its effect being reported. It will get worse for Am5 mobo and CPU sales.",Negative
AMD,"TSMC upgraded several of the 7nm fabs to 4nm tho, so those production lines don't exist anymore.  Edit: and they are converting more currently: https://www.tomshardware.com/tech-industry/semiconductors/tmsc-ponders-upgrading-2nd-japan-fab-to-4nm-could-pave-the-way-for-more-advanced-chips-for-japanese-customers",Neutral
AMD,x3D production is not limited by lithography process but packaging which is a bottleneck right now and will be for at least several years.,Neutral
AMD,Anything capable of 7nm is probably capable of manufacturing ddr5. Ddr5 is 10-12nm lithography  I imagine it's backwards compatible and lower nm manufacturing is capable of producing higher nm components still.,Neutral
AMD,"I'm just paranoid about my tech failing when replacements aren't readily available. Also, any random piece of tech can fail regardless of the overall quality of the product line.",Neutral
AMD,"Yes. We all are aware of that  Keep in mind, there are reasons why some epics have 3d vcache. Simulations and AI aren't ""very niche"" on desktop. They're not common, but it would be nice to have the option.  I don't know why anything optimized outside of gaming and streaming gets so many of you guys bothered when people say they want it.  Also, there are even games that want the cores AND the cache",Neutral
AMD,"First of all, please spare me your AI written SumUps, second of all there is still a difference between prrof of concept and production silicon. You can bet this combination was probably full of architecture bugs. Like they literally said in the snippet you send that they didnt go through with that for a variety of reasons, not only the narrow statement of the 5800x3d being better like you said.",Negative
AMD,There is a possibility of the bubble popping on AI.,Neutral
AMD,"We know it's nothing but bullshit. they knew about these contracts for quite some time, but still slowed production because of ""oversupply"".   Anyone thinking otherwise doesn't have a functioning brain.",Negative
AMD,"Doesn't matter as the current administration in the USA is letting them do whatever they want. If we want any action, the EU will need to bring them to justice.",Negative
AMD,"thats not how it works. the companies making DDR5 are SK Hynix, Samsung, and Micron (and a bunch of smaller players). TSMC's fabs aren't designed to produce DRAM or NAND, just more specialized ones like MRAM or RRAM",Neutral
AMD,> I imagine it's backwards compatible   Might want to look into things instead of using your imagination when facts are involved. Too much nonsense being spread online as if it's reality because people assume. Imagination should be left for creating things.,Negative
AMD,"No it isn't. TSMC 7nm afaik is an entirely new node and not a refresh thus it will need a redesign instead of just use the same design that they use for 10nm.  Also you typically don't use 7nm or lower for RAM since there is a lot less benefit vs making it on the bigger nodes. Basically RAM doesn't scale as well as logic (the density improvement for making RAM on smaller node is not great) thus why you don't see companies making DRAM on 7nm.  So when if someone want to make RAM on 7nm, they need to design it first and do all of the testing and validation process and then potentially have a product that is a lot more expensive. I will say that with the current price and assuming this kind of pricing last long, it might be viable (as in not losing money) to sell RAM that is made in 7nm process but still doing that would only add very little available RAM to the pool and the product itself is not going to be better.",Negative
AMD,"You’re imagining wrong, ram manufacturing uses completely different processes and TSMC doesn’t produce ram at all, it’s made in specialized factories by different manufacturers.  Samsung is the only manufacturer who has both logic foundry processes and memory manufacturing but in different fabs.",Neutral
AMD,Confidently incorrect,Neutral
AMD,Wat,Neutral
AMD,Cores AND the cache is exactly what you get with 5950x3d...  Which games btw?,Neutral
AMD,"I knew odds were high you would be too lazy to watch the video and the AI summarizing the video would prove I was not pulling it from my arse. But I think the real problem is it pissed on your oversized ego.   ""(W)ould require completely redesigned silicon"" is the problem with your claim. I was proving you were wrong. It does not need to be ""completely"" redesigned, but repurposed and bug stomped. Maybe figure out the meaning of the words you are using?  You also did not bother addressing the fact in my original statement that there is nothing to cannibalize if people cannot afford Ram for it. ""If it is too expensive to upgrade AMD will starve...""  The people with money that can afford the expensive DDR5 setups will benefit from those and the performance they offer while the potential customers will be able to afford AM4 setups that cannibalize old DDR4 builds. Think Ryzen 1000 series, Ryzen 2000 series and Ryzen 3000 and even Ryzen 5000 series that wanted upgrades but were waiting for a little longer and now cannot afford them because of DDR5 shooting up over 300%. Well they cannot afford to upgrade anymore, but they want to. This would give them a reasonable path. Think of the $100 steak vs the $500 cheeseburger. The $100 steak is reasonable and logical  when compared to the $500 cheeseburger.  It also builds customer loyalty and customer good will.  I really hate explaining logic to arrogant people that have proven there is nothing to be proud of. AS it stands I am probably not breaking it down small enough and I am sorry for that, but I am only willing to do so much. I hope you have a good night and please ponder on what I am saying without flying off the handle or accusing others of ignoring facts when they are things that are not even facts but illogical opinions you have.",Negative
AMD,This.  If the bubble pops before 2028 prices will normalize faster.,Neutral
AMD,If the AI bubble pops the US economy is screwed seeing how it's driven by tech companies as the majority of it's valuation,Negative
AMD,If the ai bubble pops we’re going to have bigger problems then buying ram. GPD growth is like .2% when you don’t count AI investments.,Negative
AMD,"In wich scenario AI bubble ""bursts""? all of a sudden millions and millions of people simply stop using it?  AI is here to stay, prices will level down, but it is going to be gradual.",Neutral
AMD,"> TSMC's fabs aren't designed to produce DRAM or NAND  DRAM is much simpler than what TSMC's fabs are designed for.    I think the gotcha would be the economics. DRAM might not necessarily benefit from TSMC's 7nm process very much, and it could be significantly more expensive to manufacture.   Those 10-12nm manufacturing processes used for DRAM don't need to be as advanced, but probably do need higher volume, and would therefore be more optimised for the cost of production over having the smallest features, or the highest performance.",Neutral
AMD,"I'm no expert, but I'm not spewing nonsense.  Perhaps you should take your own advice.  [This is basically confirming what I've said. New nodes have some amount of backwards compatibility built in to enable old designs to be migrated to new nodes more easily.](https://i.imgur.com/Q5zXzXp.png)",Neutral
AMD,"SRAM (used for cache on logic processes) is the least scaling part of a logic focused process that’s correct, but Dram doesn’t use SRAM its much simpler but slower and still scales down rather well.  DRAM manufacturing however is quite different from logic manufacturing and focuses on another set of characteristics.",Neutral
AMD,maybe ram prices will crash then :D,Negative
AMD,"""US economy"" is a bit more complex than market indexes. The money will just distribute to other industries",Neutral
AMD,"oh no the rich people number that doesn't actually go to anyone not in the top 5% will go down, how horrible",Negative
AMD,"It's worse than that, it's 0.2% when you exclude only data centers, not even the full AI bubble.  So even just stopping the production of data centers would send the US in an atrocious recession.",Negative
AMD,the money will just move from AI to gold/bonds to other stocks allowing better industry/market development outside AI,Neutral
AMD,"Which is still not how this works, producing dram is a completely different beast from producing CPU/GPU/whatever chips that actively compute.",Negative
AMD,Using AI summaries isnt reliable. It told me that Amari Cooper didn't retire and pulled Facebook articles as reference while linking an ESPN article where Amari Cooper stated he was retiring. It said there was no reliable source. Those AI summaries pull bad information and make shit up all of the time,Negative
AMD,"Holy crap, I'm so embarrassed for you right now.",Neutral
AMD,Your source is AI generated text? For real? WTF,Neutral
AMD,"Yes, because the crash of a critical sector has never led to a catastrophic recession before. Oh wait, that’s exactly what happened in 2008 with the housing market crash genius.",Negative
AMD,"Except those rich poeple/companies employ everyone else. And what do yoit think all those mutual funds, pension funds, 401ks are investing in right now?  It will be worse than 2001 and 2008.",Negative
AMD,"Google's AI is also so insanely bad, feels like all it does is hallucinate.",Negative
AMD,"AI is not critical to anything, housing and (""too big to fail"") banking is",Negative
AMD,"I'm OK with it being worse than 2001 or 2008. It's not like the boom economy right now is helping anyone - rents are at all time highs, food costs too much, people are still getting laid off regardless anyway, and we can't even afford distractions like PC gaming because they're stealing all the wafers for Sam Altman's ego",Negative
AMD,sure we'd all like to see amd pay lots of devs to improve / fix rocm but    I think they also need to do stuff like this to get people interested.    After all you'll always need people to tinker with it because they want to and    not because they are forced to in the end,Neutral
AMD,Smart move,Neutral
AMD,"AMD will do anything but hire more software developers to fix their stuff. Running a goddamn lottery here, this is beyond embarrassing.",Negative
AMD,Its cheaper than hiring actual developers.,Neutral
AMD,You do realize Nvidia did something quite similar to this right? It's the reason they're so crucial in all areas of AI now.,Positive
AMD,"but they have?  [https://github.com/ROCm/ROCm/graphs/contributors](https://github.com/ROCm/ROCm/graphs/contributors) there's a clear uplift in work being done  they've also been working on the build system to get better, quicker, smaller builds of ROCm. This isn't ROCm itself, but does enhance the user experience. An entirely different, parallel project. Sounds likee ""more software develpers to fix their stuff"" to me  [https://github.com/ROCm/TheRock?tab=readme-ov-file](https://github.com/ROCm/TheRock?tab=readme-ov-file)  also, you need proper data to know what's wrong  have you already complained about tesla cars providing training data for the self driving features? I'll write it for you  >Tesla will do anyhing but drive the miles themselves and create the training data to fix their AI model. Instead they're stealing MY footage so the car will learn to drive in MY area. Can't believe they're not just driving here themselves, this is beyond embarrassing.",Neutral
AMD,"This has been their MO for awhile. They release something way after Nvidia that isn’t as good, then hope the community does free work to make it useful.",Negative
AMD,Also gets people to use ROCm and see if it's a viable alternative for their use case. Might get some people to actually write code in other areas for Rocm.,Neutral
AMD,They're crucial in all compute areas because they bet on cuda two decades ago and started including it in every GPU. Not because they ran a lottery to fix bugs in cuda.,Neutral
AMD,"\> there's a clear uplift in work being done  An uplift that's 20 years too late.  \> have you already complained about tesla cars providing training data for the self driving features? I'll write it for you  No, why would I? That's a completely different issue. Tesla isn't dangling FSD above people's heads to entice them to improve it by working for Tesla for free. Your analogy makes no sense.  I am glad that ROCm is improving, but running a lottery is a shitty way to attract developers. As you stated in a different commit: run programs with unis and companies, give away stuff periodically, provide uni courses and pre-uni programs, etc.",Negative
AMD,"I hate to break it to you but most free software was done for free, at least they are offering quality barter.",Negative
AMD,But why would you waste time and resources on rocm when cuda exists.,Neutral
AMD,The only reason CUDA gained such prominence was because Nvidia literally ran programs giving away free GPUs to colleges and universities to get people familiar with the software and write extensive libraries for it for many use cases. The whole CUDA ecosystem relies on the work the regular Devs have done. Something similar to what AMD is doing now. You think this program is just to fix some bugs? It's to get people to actually use Rocm and learn it as well. The laptops are an incentive.  CUDA like you say has been around for 2 decades. And it still wasn't until the past decade that it's been so prominent. Part of that is because of the initiatives that Nvidia took.,Neutral
AMD,"\>Tesla isn't dangling FSD above people's heads to entice them to improve it by working for Tesla **for free**.  \>I am glad that ROCm is improving, but **running a lottery** is a shitty way to attract developers.  Nvidia really is cheapening on bots nowadays",Negative
AMD,That's a stupid question. You do it so you aren't dependent on one company with a black box technology.  ROCm is open source. It's got far more potential because it's customizable beyond CUDA. It's already very usable for large companies.  On the other hand nobody outside of Nvidia knows what's going on with CUDA. If they decide to pull the plug you're done.,Neutral
AMD,"There's a bit of a difference between a lottery with 20 laptops for fixing bugs and giving away free GPUs to familiarize people with the software. And by ""a bit"" I mean a fucking world of difference.",Negative
AMD,"I wish this was a truly low power version, 75w Max. Then we could have slot powered, single slot/short but single fan or low profile cards.  I still hold hope that AMD will make whatever a modern RX6400 would be, RX9400? LP and slot powered options are few and far between, a compelling 12+ gigabyte option would be a winner in that market.",Neutral
AMD,"I hope this increases the chance of having a low profile design if it ever releases outside china. Would be enticing for <10L case builds to have a small card with 16GB VRAM but I am not seeing any partner AIBs, I can see first from Gigabyte and Yeston that can provide smaller low profile designs for AMD cards.",Neutral
AMD,"> The standard RX 9060 XT is rated at **180W** TBP and requires a 450W minimum power supply.  False. RX 9060 XT is rated at **160W** TBP: [https://www.amd.com/en/products/graphics/desktops/radeon/9000-series/amd-radeon-rx-9060xt.html](https://www.amd.com/en/products/graphics/desktops/radeon/9000-series/amd-radeon-rx-9060xt.html)  >Chinese media reports suggest a 140W TBP  The source link that was provided confirms ""up to **140W**"" TBP: [https://www.amd.com/en/products/graphics/desktops/radeon/9000-series/amd-radeon-rx-9060xt-lp.html](https://www.amd.com/en/products/graphics/desktops/radeon/9000-series/amd-radeon-rx-9060xt-lp.html)",Neutral
AMD,"amd should release a 9400 and 9500.  sure nobody wants to see just 8gb, but it would be fine for this use case.  9400:1 slot low profile slim card like the old 6400, no added power connectors. 9500:2 slot low profile card, extra power connector but overall low tdp for thermals.",Neutral
AMD,Gamers can rest assured that at least from AI it won't have demand lol,Neutral
AMD,"so more or less a factory UV 9060XT, guess they have produced enough silicon and can use the better one with better UV capability here.  If that's the case, these cards would actually be best fit for a BIOS mod that unlock the power limit.",Neutral
AMD,Will this have 3 DisplayPort ports? Or just 2?,Neutral
AMD,I love low power consumption with same performance,Positive
AMD,"So they had some silicon not quite up to par to hit advertised boost clocks, but they didn't want to cut it down to RX 9060 (non XT)",Neutral
AMD,How about a laptop GPU 😡,Neutral
AMD,Honestly that's impressive uses much less than my 6750xt lol.  I'm just using a 550w PSU for my setup.,Positive
AMD,"Did anyone even ask for this? My 9060XT really doesn't want to touch 180W even with the power limit maxed out, temperatures are more than controlled and any half decent PSU can handle it",Negative
AMD,What? I already have the 9060XT,Neutral
AMD,"Damnit, Chinese market only again.",Neutral
AMD,Any laptop GPUs in development AMD?,Neutral
AMD,Cool. It's about time.,Positive
AMD,9050 XT somewhwere down the line once most of the binning has been done,Neutral
AMD,Yeah would love to have a RDNA4 single slot GPU for a tiny Bazzite build.,Positive
AMD,"I honestly think AMD should release the RX 7400 to consumers, it's 43W & will work well for SFF systems as it can just be a single slot card a-la the GT 1030/RX 6400.",Positive
AMD,"At least a 5060 lp competitor, 75w looks a bit constraining for what most people would want",Neutral
AMD,"well, you can limit the power yourself if that is the interesting part. i'm running my 9060XT with a 40W limit right now as that's enough for the game i'm playing. though i have the 8GB version, so the power consumption is likely a bit lower. most of the time the fans don't spin when gaming.",Neutral
AMD,I’m getting decent 1080p performance from a B50 pro. Not good. But decent. I don’t normally game on it but I had to try.,Positive
AMD,"[9060 XT LP](https://www.amd.com/en/products/graphics/desktops/radeon/9000-series/amd-radeon-rx-9060xt-lp.html) will be an international release because the  product page is available on the English page for AMD, while all China-only products are listed just on the Chinese page (like the 9070 GRE, 7650 GRE,  6750 GRE 12GB,  6750 GRE 10GB)  English: [https://www.amd.com/en/products/specifications/graphics.html](https://www.amd.com/en/products/specifications/graphics.html)   Chinese: [https://www.amd.com/zh-cn/products/specifications/graphics.html](https://www.amd.com/zh-cn/products/specifications/graphics.html)",Neutral
AMD,or plopping in SFF office PC builds. ...or maybe i just get Gabe Cube this time.,Neutral
AMD,"Very helpful, thanks",Positive
AMD,Anything wrong with gigabyte? Their builds for 30 series cards which were notoriously power hungry and spike heavy were well prepared for the task. Don't see why they would be bad for lp cards.,Negative
AMD,"If Yeston is able to, I wouldn't be surprised to see a [Cute Pet](https://yestonstore.com/collections/cute-pet) 9060",Neutral
AMD,">False. RX 9060 XT is rated at **160W** TBP  To provide more context for anyone reading this, the partner models can have three different TBPs, 160, 170, 182, some information on which card uses which is provided in this google sheet: [https://docs.google.com/spreadsheets/d/111K2xkO2-ExNq8NgQsPULDEShBoBqFf-9WQFeorjFHY/edit?gid=755628141#gid=755628141](https://docs.google.com/spreadsheets/d/111K2xkO2-ExNq8NgQsPULDEShBoBqFf-9WQFeorjFHY/edit?gid=755628141#gid=755628141) so it's both right and wrong depending on which model is being talked about since AMD has no 'reference' board/card. Although obviously 160W is the official number with it being on their website.",Neutral
AMD,"easier to sell one of each variant than several of one variant, in other words their sales team probably asked for this, operations be damned lol",Neutral
AMD,"I didn't ask for it, but I'll take it. It'll hopefully stay even more under TBP compared to other cards – I have my doubts though. My 1660 Super could use an upgrade, but I refuse to install a card that'll double the power draw of my entire apartment, just to play video games :D",Positive
AMD,But why buy such a card? It has so little performance that some APUs have more power. If you wanna go SFF go for a G suffix CPU and you will save space and power and still have more performance.,Neutral
AMD,I agree. AMD really missed out on the gold mine that is the low profile gpu market here (that Nvidia ignored). Lots of enthusiasts will be glad to grab a RX7400 for SFF or mini ITX builds. I will certainly buy one in a flash to upgrade my RX6400 that I have installed in my Lenovo M920x Tiny. I also have a few HP & Dell sff PCs that will appreciate a low profile GPU upgrade!,Positive
AMD,"This would require being able to boot though, right? I don't know of a way to change the power limit of a GPU outside of windows.  A lot of GPUs simply won't boot without the pcie power connectors plugged in",Negative
AMD,:),Neutral
AMD,"RX 7400 seems to be around RTX 3050 8GB - RX 6600 performance though. It's not actually a bad performing gaming card at all (especially for 43 watts), especially if the MSRP is right.    It's a good option theorectically if you are on something like a old Dell Optiplex SFF, where the best option is a 3050 6GB or RX 6400.",Positive
AMD,Home Theater.,Neutral
AMD,"yeah i at least assume it won't boot without the connector, if that is what is limiting your use case.",Negative
AMD,"If it is just for video output: all AM5 CPUs have an integrated GPU that can drive 4k60 no problem (not in 3D, but 2D stuff like video playback is no problem at all). Only those with a 'F' suffix have the GPU disabled.  I.e. a Ryzen 5 7400 goes for 130€ (tax included, so about 120 USD without tax), which is more than enough for video playback. For 140€ you can get a Ryzen 5 8500G, which has a little less CPU power (like 5% less) but has a way more powerful integrated GPU. You can play most games on low settings on it, if it is an older game you can probably max it out no problem. It is about on par with a GTX 1050. So if your home theater should also function as an emulation machine for everything PS2 and older, even some PS3 era games, go for this CPU.  A dedicated GPU wouldn't be cheaper than about 100USD, so there is just no room to sell those if the cheapest CPU+GPU combo goes for 120USD.  Nvidia is still selling the GT 710 for 50USD, but the integrated GPU in the Ryzen CPUs is about 3 times as powerful. The G version is more than 40 times as powerful, just to compare them. They did have the RTX 1650, but you can't get it new anymore, was 160SUD at launch but went up to over 500USD towards the end of its life cycle. So nothing people that wanna be cheap would buy.",Neutral
AMD,"Is anyone really doing HTPCs still? Even with all the issues Windows and Linux have with HDR and stuff? I feel like a Plex server capable of AV1 transcode plus a box that supports AV1 like Fire TV 4K, onn. 4K box, or even your TV's built-in chip would be better than an HTPC.   Hell the Plex machine could have the RX 7400 because that has AV1 encode and decode. That would justify AMD releasing it.   Then use a PS5 or something for UHD Blu-Rays.",Neutral
AMD,That is pretty much the exact use case for most of these low power GPU requests unfortunately.   The ability to get power from the PCIE slot only makes for a VERY high amount of compatibility.  Makes you wonder why a lot of the mobile GPU chips aren't released as low power desktop GPU's too,Negative
AMD,"yea AMD's low end gpu devision has been entirely absorbed by APU's and while i like that the apus have good gpus, sometimes i wish they would release more low end cards for systems that are already established and just need a gpu      i always wanted AMD to release a 12 or so CU Vega thats passive cooled and single slot or low profile back in the day when vega was relevant",Negative
AMD,'existing' home theatre.,Neutral
AMD,existing setup.,Neutral
AMD,"There isn't a market here, this is a few thousand possible customers, not enough to create a product.",Negative
AMD,I don't understand. Like a Windows Vista PC with Windows Media Center?,Negative
AMD,windows 10 with an Rx460.,Neutral
AMD,Right but how do you use it? There's no remote control so you need to always have a mouse and keyboard ready? How do you have the audio routed?,Neutral
AMD,"K/M works great, Amp.",Positive
AMD,I use Arch linux BTW,Neutral
AMD,Nice! Planning to achieve Rx7600 + 5 5700x using Pop! Os,Positive
AMD,didn't go 9070xt due to vram?,Neutral
AMD,I’ve got an XTX sitting in my closet. I’m just waiting for a good time to sell. I hope you enjoy your new build!,Positive
AMD,"Op, is that a skywatcher refractor telescope on a eq 1 tripod?",Neutral
AMD,"Hold on to it dearly, the next couple of years are gonna be rough",Negative
AMD,don't forget KDE Kwin Wayland btw,Neutral
AMD,"Thanks :)  popOS is really nice!      I chose Arch because I need the native ROCm 7.1 package, and Arch supports it. It's a truly bleeding edge distro.   Obviously, with its pros and cons, I believe each distro has its own features for each type of user.",Positive
AMD,"Hmm... Several reasons:  1. VRAM 2. I'm planning on getting a liquid cooler for the GPU soon and installing the 7900XTX Aqua Elite BIOS on it for a massive performance boost. 3. I have a soft spot for hardware architectures, and RDNA3 is one of my favorites after the cell broadband engine architecture. In fact, I think the 7900XTX will be one of the cards  I won't sell, but will sit on my lab shelf. 4. I get for now better desktop support, as well as better productivity and AI support.",Positive
AMD,Because the 7900XT out performs the 9070XT in raw performance,Neutral
AMD,"Yes, unfortunately yes. We hope the tech world can be accessible to everyone again",Negative
AMD,"Arch is a nice choice tho, might as well try endevour OS one day!",Positive
AMD,Why RDNA3 and Cell specifically?,Neutral
AMD,tell hs about your opinions of rdna3 and cell!   seems like an interesting topic :),Positive
AMD,What does it take to install the aqua elite bios on a reference card?,Neutral
AMD,"I tried to flash the Aqua Bios to my 7900xtx Pulse but couldn't get that to work, but the Nitro+ Bios gives you about 60w more to play with, got me an additional 6fps in Cyberpunk on Ultra graphics.  Anyways, good luck flashing :)",Positive
AMD,"cell is trully interesting but 7900xtx not so much lol, weird choise he made",Negative
AMD,"I’m glad to see that I’m writing to curious people.   u/SeventyTimes_7    u/Ambitious-Ad8725    u/Sora_hishoku   I’ll simplify the discussion to make it more interesting and less technical/daunting.  CELL was conceptually the father of NVIDIA’s success.   For the first time, the focus was on specialized parallel computing, and the engineers at Sony, IBM, and Toshiba were visionary and extremely bold in pursuing that path.   They only made one mistake: such a CPU couldn’t function properly without adequate software support. That’s where NVIDIA came in and realized it needed to create CUDA for its GPUs, offering what CELL did, but with an easier language and an intuitive framework for programming.   Without CELL, history would have gone differently.   For me, preserving that CPU is like preserving a piece of computing history.  RDNA3, on the other hand, is one of the boldest, most audacious, and futuristic choices in the entire industry. It's the future today, where everything will be chiplet   It shifts towards horizontal development, not just vertical, even in the consumer sector. It allows interconnections between multiple chips with record bandwidths of over 5 terabytes per second, eliminating many bottlenecks that are inevitable with manufacturing nodes and monolithic chips.   This isn’t just a simple play; AMD has managed to overcome the latency barriers that are extremely severe on GPUs, unlike CPUs.  They also redesigned the Compute Units with FP32 SIMD, bringing intelligence inside the chip for the first time, not just size and raw power.   In magazines, you’ll read that the 7900XTX has 6,144 shader units, but that’s a partial figure it depends on the scheduler, the compiler, and how well the code is written.   6,144 represents the minimum, where each instruction is incompatible with another. In real practice, however, code often contains many compatible and similar instructions, so the count can theoretically reach up to 12,288, within reasonable limits.  The key detail is that, for the first time, the chip communicates with software at the lowest level, working in perfect harmony. If the compiler improves in the future, RDNA3 itself improves, because instructions per cycle are reordered and scheduled more efficiently, leveraging far more than the declared 6,144.   Other companies simply increase physical ALUs without addressing modularity and software-driven management.  RDNA3 has an extraordinary pipeline combining general purpose and specialized tasks, modularity in its Compute Units, and many other clever innovations.   Its display engine is phenomenal and still reigns supreme today.   It was a bold and risky choice, but an incredible engineering success.   They could have taken the easy route with a high-wattage monolithic chip, but instead, they pushed the limits physically and mathematically, creating an amazing chip for the consumer market.  It remains a prime example of the validity of the chiplet design, as the first consumer GPU in the world to use a chiplet.   It is capable of leading in performance and efficiency (350W) in theoretical computation following linear math principles.  I want to preserve these two pieces of engineering, as early examples of the future :)",Positive
AMD,"First of all, a gpu with dual bios is recommended in case of inconvenience.  Liquid dissipation is also mandatory as the GPU will have a very high energy consumption by pushing the chip a lot.  After that for the guide I’ll send you a link later",Neutral
AMD,To say such nonsense you don’t know anything about RDNA3.,Negative
AMD,Wow I had no idea of all this. Fascinating. I started amd with RDNA1 with a 5700xt then bought four more..moved up to the 7900gre and now the xtx. People keep telling me to sell the old ones but something about collecting the different types and testing to workings off all of them keep my attention more then the actual gaming does now a days. But this really does sound like an important step in computing history. Also planning on picking up a rdna2 and rdna4 card just to have the while family...😁,Positive
AMD,"The xb360's GPU is often slept on in it's position in GPGPU history - it was the first true modern consumer unified shader architecture, and there were a number of GPGPU tools developed for it - lots of games used that for things that *would* have been done on a cell SPE on on the ps3 equivalent. It was a big reason why gamedevs didn't ""miss"" the power of the cell - and it was arguably prescient as GPGPU then became the dominant paradigm for those sort of tasks.  A lot of the stuff developed for that then got rolled into ATI's Close To Metal SDK, which then likely influenced CUDA when that was released later.",Positive
AMD,This has to be AI generated,Neutral
AMD,"I am a young computer engineer, specialised in software for microprocessors.  seeing the history of compute parallelism is so interesting in that there is such obvious benefits to parallelism but so much complexity when you design a system around it.  This might be sending me off on a bit of a research rabbit hole this week, there are a few questions that have popped up for it.",Positive
AMD,I dont mind the convenience of it. And already on a custom loop. Ive reflashed gpus with unofficial bios back to normal but not one to another type of bios.  Do I need a flasher or is the process the same with the ATI software,Neutral
AMD,... do you?,Neutral
AMD,"No, it’s only translated with google, the text is mine. But English is not my first language so I chose to translate it. I have no need to take information from others when I know very well what I’m talking about. Too bad, if we were on Steam I would have given you 200 clown points.",Neutral
AMD,"Yes, I am very pleased to have been a stimulus for some research. I wish you the best ❤️❤️",Positive
AMD,Very well. It gnaws at you so much that you’re still here commenting instead of going to share news about Intel,Negative
AMD,"Most of this is just repeating AMD marketing, or just glazing a mediocre product.   Separating the IO from the compute isn't exactly anything as revolutionary as you are making it out to be, nor is the type of packaging AMD used, and the die size of the 7900xtx is so small that they likely didn't even benefit much cost wise from going chiplet anyway. There certainly were no perf/watt or perf benefits like you seem think from that.   And architecturally RDNA 3 was so mid that people were seriously asking if there were issues during the product development.   The fact that AMD then backtracked from chiplets and didn't even release a high end RDNA 4 would make one think that RDNA 3 didn't inspire much confidence in the validity of their chiplet design for client.",Negative
AMD,What?,Neutral
AMD,"We are on two different levels of thought. I point to the moon and you look at the finger. I’m not wasting time, good evening.",Neutral
AMD,That’s what you do all day. Share news about Intel processors,Neutral
AMD,"No. More recently it's been overwhelmingly about process tech, not Intel processors. My profile isn't private lol, you can check for yourself.   What does it matter though? You just regurgitated RDNA 3 marketing all over this post.",Negative
AMD,They don’t have a high end slim model anymore? Stuck at 5060 when zephyrus goes up to 5080/5090?,Neutral
AMD,Wake me up for zen6 mobile,Neutral
AMD,Next year,Neutral
AMD,Going back to individual integer schedulers makes me think the 88 entry unified scheduler in Zen 5 might be suspected as a root bottleneck for why Zen 5 didn't do better against Zen 4 despite being so dramatically wider.,Negative
AMD,Looking forward to building an EPYC Zen6 with 8GB of RAM in 2026,Positive
AMD,Original article: https://www.phoronix.com/news/AMD-Zen-6-znver6-GCC-16,Neutral
AMD,time for AMD to make a new memory standard that doesnt need the memory makers input and amd can make it all in house,Neutral
AMD,For what? Everything is still gpu limited with my 7800x3d,Neutral
AMD,I doubt availability of consumer zen 6. Wafer availability on the newest nodes is getting tight and expensive. Totally wouldnt put it past them to re release the old archs for a few years.,Negative
AMD,I can't wait for the inevitable meltdown from gamers if Zen6 requires DDR6 RAM lol...,Neutral
AMD,Nobody will buy this shit if it's 700 USD bro,Negative
AMD,"I thought it was well considered that the IO die was the main culprit? Hence why the x3D parts were so much better than the standard, and why even the 9800x3d has serious issues in heavy RT workloads once the cache runs out and it has to dip into main memory.",Negative
AMD,"Zen 5 did better in most non gaming tasks,  in Games, we already seen multiple times that wider cores dont really help with gaming performance if you dont increase the cache/get a faster IMC and memory  Rocket lake is another good example, its much beefier than Skylake, but Skylake had lower latencies so the 10900k/10700k outperformed the 11900k/11700k in games  you can also take ""gimped"" Zen 3 APUs that have 16MB of L3 per CCD  and compare them to Zen 2 with 16MB per CCX, the difference is quite small in games     i suspect that Zen 6 with 48MB per CCD (if rumors are true) will do 10 - 15% better than Vanilla Zen 5 in games, but still slower than a 9800x3d  and 10800X3D is where Zen 6 is going to shine  also, if the 7GHz rumors have any truth to them, we might see bigger gains but i doubt 2nm can reach 7ghz",Positive
AMD,Performance profiling didn't show that though. Being backend bound is overwhelmingly the smallest bottleneck for perf in almost all the specint2017 subtests.,Negative
AMD,I just grabbed the $159 Newegg bundle for a future/spare parts build just to have the 16gb ram on hand. It happened to come with a pretty solid motherboard so can’t complain. Very tempted to throw a 6 core x3d cpu in and and be off to the races.,Positive
AMD,Just need 8gb of cache,Neutral
AMD,I can smell those 4 slots with 2 GBs RAM each,Neutral
AMD,"If they had stupid amounts of money and fab capacity, which they don't, that'd still be a bad idea.  Basically Rambus all over again but from AMD this time around.",Negative
AMD,"Would be cool if they made a threadripper sized socket that has the CPU die, GPU die, and HBM dies for unified memory all next to each other. The Vega cards already had the GPU and HBM2 dies organized this way years ago, and the APUs AMD has been putting out have been great, so the pieces are all there. Would admittedly suck for upgradability, but would probably have pretty big efficiency and performance gains",Positive
AMD,Yeah we don't need another competing standard..  https://xkcd.com/927/,Neutral
AMD,"Man, I have flashbacks from the mining boom and overpriced gpus. It just isn’t fun to build a rig rn",Negative
AMD,People use CPUs for things other than gaming,Neutral
AMD,AMD has confirmed that it is for AM5,Neutral
AMD,DDR6 isn't coming for years.  Earliest dates you can find are sometime in 2027.  And that is primarily for server use since the cost for it is supposed to be REAL high.  Its probably coming to desktop much later in 2028 or so and Intel will probably launch it first in PC just like they did pretty much DDR2 onwards (IIRC they actually supported RDRAM before DDR).  AMD also tends to delay adopting the latest memory standards by a year or more.  Been that way since before DDR3 came out so they're pretty consistent here.  If they follow that pattern again you won't see AMD requiring DDR6 support until sometime in 2029.  That is why rumors having been coming out that Zen7 will also be on AM5.  Either DDR6 won't be ready in time or AMD won't be ready with a new socket in time for Zen7.  Given the caches they're putting on their chips these days its not like it matters much anyways what RAM they use past a certain point.  You can get more benefit from cranking the IF bus and improving latency then going high bandwidth on AM5 anyways.  And that is for the non-X3D chips.  The X3D chips care even less about main system bandwidth.    Where it would matter is for a high(er) performance iGPU but everyone seems to be going for soldered RAM of some sort with those anyways so its of questionable benefit any which way you look at it for AM5's expected lifespan.,Neutral
AMD,Top-ish end Zen6 could easily be 700USD+.  But its also rumored that 32C/64T is coming Zen6 to very top end so its hard to judge what is really going to happen price + core-wise here.  For reference a 16C/32T Zen5 9950X is about $540 right now though so I'd expect prices to drop a bit lower baring tarrifs or other market BS.,Neutral
AMD,"Depends on the workload.  Games and various other apps are memory bandwidth/latency sensitive with low/maximized ILP (instruction level parallelism) potential due to stalls (waiting for data).  Those are being restricted by the IMC/IOD and fabric interconnection limits.  However, most simple benchmarks happily fit within the core caches and even those showed much lower than expected gains.  Zen 4 has four integer schedulers (96 entries, 24x4), Zen 5 has 1 unified scheduler (88 entries).  There are scenarios where each should have an advantage over the other, but the unified scheduler really should win more often than not... but it seems it wasn't able to maximize the additional execution units as fully as smaller individual schedulers could in Zen 4.  Zen 6 has now, it appears, gone back to individual schedulers, so it seems like an area AMD couldn't make the unified scheduler (which Intel uses) work as well as desired.",Neutral
AMD,">Hence why the x3D parts were so much better than the standard  They weren't though, the gains were coming from the higher frequencies new X3D parts had relative to previous X3D parts, while standard Zen 5 vs standard Zen 4 didn't see that.",Positive
AMD,"The 3D parts don't seem relatively any better between Zen 3, 4 or 5. Gaming wise 7800X3D and 9800X3D seem almost identical",Negative
AMD,"> Zen 5 did better in most non gaming tasks  For context, it posted a very respectful average 18% gains on wide assortment of Linux workloads (i.e., mainly server focused):  https://www.phoronix.com/review/amd-ryzen-9950x-9900x/15",Positive
AMD,">Zen 5 did better in most non gaming tasks,  A \~10% gain in specint2017 is not impressive at all.",Negative
AMD,I would hope the improved IO die latency and faster ddr5 support with better timings would help also. AMD is so far behind apple right now and with their 2 year generation cycle they are going need a lot more uplift to stay competitive till zen 7 in mid 2028,Positive
AMD,RAM BUS go vroom vroom,Neutral
AMD,"I think that you are very unlikely to see HBM memory in the regular consumer market again unless the AI industry collapses.  It's too valuable for use in consumer products when they need all they can get for compute oriented products.  There's a reason they went back to GDDR relatively quick.  Granted you can get something sorta like that with the Ryzen AI chips where they're using DDR5X like the Gmktec Evo 2, or the Framework desktop with 128GB RAM.  The price is a bit breathtaking though.",Neutral
AMD,"competing standard?  the memory makers are already taking their ball and leaving, who cares about their standard if they are just gonna up and leave",Negative
AMD,If you want to upgrade. I would not wait for next year. Honestly. I cant see the ram and gpu situation getting better in the upcoming next 3-4 years.,Negative
AMD,Gaming has hit the mainstream is what happened. You know it's real bad when consoles are getting price hikes every year now.,Negative
AMD,"Yeah, there needs to be a reason for things to be faster. We have ideas and standards for all kinds of things, but only really start moving when it's actually required.   Take pcie for example. We were 7 years on pcie 3.0. Because it was enough. 16 lanes was enough for basically everything.   Then we got 2 years on 4.0 and 2 on 5.0. Tho consumer is still using 5.0. Because suddenly GPU's and SSD's were able to saturate it and we needed more for our highspeed networking.   Networking is also funny, we had the ability for 400 gigabit for ages. We just didn't need it so it was barely relevant. Same with Gigabit. Man we are JUST barely getting 2.5g devices. But suddenly you can get 400g for under 2k€.   If there is no need for ddr6, then there will not be a ddr6. Some maybe even sidestep ddr for hbm. Not entirely impossible. Just like gddr5 was around a long time, then suddenly we needed more so hbm, dddr6 and gddr6x came around really quickly.",Neutral
AMD,I sincerely hope zen7 lands on am5. I went super budget with my build (7900x for $200) and while it works fine I want to throw an end stage cpu in it from the last support generation when we know for sure that is.,Positive
AMD,"So, $999 for the 32 core cpu.",Neutral
AMD,What sort of rumor is that... They are expanding to 12 core CCDs so I imagine the top would be 24. How do you expect to make up 32 cores with 12core CCDs??? That would mean having 4 CCDs and disabling 4 cores per CCD which I don't see happening... Maybe we'll get 36 cores with 3 CCDs though?,Neutral
AMD,What's the point of this comment lmao,Neutral
AMD,"This is probably true, but believe it’s also the case that they have leapfrogging design teams, e.g Z4 team will have moved onto develop Z6, which might have a bearing on the outcome here.",Neutral
AMD,"yeah thanks, Zen 5 is very good  the ""gaming"" side is just AMD not changing anything in the cache subsystem and IMC being the same  the 48MB L3 on the vanilla Zen 6 is going to be better than Vanilla Zen 5 for sure, if the high clocks rumors turn true, then we might even see really nice gains (unlike wider core, higher clocks have more effect on games)",Positive
AMD,"It’s purely a price to performance issue for me. I’m already using a 3090 so it’s hard to justify those fully integrated boards, but if I was building from scratch today it would probably be a different story.",Negative
AMD,"It's literally not going to matter as AMD can't get fab space/time.  The only way to fix it is to have everything integrated on the processor with HBM, which again, won't happen as AMD can't get fab space/time.  Intel has sadly gone the correct route with their process of integrating the ram into the processor, but it does suck as you can't upgrade it.",Negative
AMD,I bought the 7600 on release and planned to “max-out” the socket once I get a new GPU but I’m still happy-ish with my 7900XT.,Positive
AMD,It will be better in 18 months,Positive
AMD,Zen7 is the first one that might be DDR6.  How is Zen6 not enough of a performance uplift that you need to wait two generations?,Neutral
AMD,Possibly.  If they do a X3D version it could easily go for $1k or more I think.  Few would buy it but that is normal for these halo prestige parts.  Its more or less a HEDT chip anyways.,Neutral
AMD,Consumer 32 core CPUs are around $2500-3499 right now so 999 would be a huge discount.,Positive
AMD,i imagine them trying to compete with Nova lake on core counts  i expect to see Zen 6c used in desktop as a 2nd die   maybe 12C Zen 6 X3D + 32C Zen 6c die,Neutral
AMD,"12C/24T is rumored for the ""big"" Zen6 but something like a Zen6c (I dunno what it'll actually be called) will fit more cores in the same die space.  Note that I mentioned rumored in this post and in the post you were originally replying to.  No one knows for sure what AMD is planning here, and I don't claim special insider knowledge, but nothing I'm stating is outlandishly impossible either.",Neutral
AMD,To flex lmao,Neutral
AMD,That's absolutely something I didn't think about and could absolutely be related... maybe a split between what the teams think is the better solution...,Negative
AMD,Aren't there actually 3 teams working on Zen?,Neutral
AMD,How so?,Neutral
AMD,The 7900x works fine for me is the thing. 4-5 years when we see am6 might be a different story though,Positive
AMD,"I think AMD need to return to lower the HEDT entry price. AM5 platform is fine with topping out at max 16cores.  HEDT used to be $549-749 only, if you inflation adjusted it is still no way near the base price of the lowest HEDT CPU now. The extra PCIE lanes is even more relevant now thanks to nvme ssd.",Neutral
AMD,We’ll find out if this is the case when Zen 7 releases.,Neutral
AMD,Because demand will catch up.,Neutral
AMD,"Intel may get entry level ish HEDT next time around. But probably not. Buying a cascade lake or old TR is still all that’s available for most people’s budget. Which, they still work ok. Just slow on single core and missing 512",Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,the higher rated cpu is more than the lower rated cpu? shocker...,Neutral
AMD,"I don't usually do this, but: ""Obviously"" - Professor Snape",Neutral
AMD,"In other words, grab the 9800x3d on sale while you still can before AMD increases the price of 9800x3d to make 9850x3d more appealing.",Neutral
AMD,About to get 9800x3d for 325 at Microcenter.,Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"Will this one cook Asrock boards, or will Asrock boards cook this cpu?",Neutral
AMD,Calling it now they're discontinuing 9800X3D and only selling this to raise prices in a less obvious way.,Neutral
AMD,you telling me it's more expansive than the 9800x3d? whaaaat?!,Neutral
AMD,Kinda wish I held out for a 9800 or higher instead of getting the 9700X,Neutral
AMD,"if 14900ks is to the 14900k, is this the same between 9850x3d and 9800x3d?",Neutral
AMD,"""No duh"" - Minerva McGonagall.",Neutral
AMD,"> before AMD increases the price of 9800x3d to make 9850x3d more appealing.  That's not how that works...  If 9800X3D goes EOL because 9850X3D has good-enough yields, then the price increase is from **retailers** artifically inflating prices of remaining stock due to high demand.  And again, why would AMD do this?   CPUs that can't reach 5.6GHz but reach 5.2 are still valuable.",Neutral
AMD,I have 4 of them lol,Neutral
AMD,325?!!! I’ll sacrifice my pp to be micro for one to open in my city,Positive
AMD,How the fuck is that possible? Insane deal! Open box?,Negative
AMD,Not normally how the 850 class chips work   This is nothing new. AMD has done this for nearly 10 years now,Neutral
AMD,"Considering 7800x3d is still for sale, doubtful.",Neutral
AMD,9850X3D clocks higher than 9800X3D.  That means any binned chips that can't reach 5.6GHz will still have value as 9800X3D if they can reach 5.2GHz.,Neutral
AMD,When have they ever done this with previous x800x3d chips?,Neutral
AMD,"Ehh, you saved like $200 and unless you have a really high end GPU on a relatively low resolution the difference is very minimal, besides in two years or so you can just throw a zen 6 chip in there.",Neutral
AMD,Maybe in games because of the clockspeed increase. but even the any 9000 x3D chip will be marginal of error when it comes to games,Neutral
AMD,"So you're telling me it was the retailers that increased the prices of 7800x3d couple weeks/months before the 9800x3d released? Because that's certainly not what anybody is saying.   9800x3d is most likely not going away or ending production. They'll just increase the price of 9800x3d because well they can. This will bridge the pricegap between 9800x3d and 9850x3d and make people think ""Oh well 9850x3d is just 50-80usd more, might as well get that.""",Negative
AMD,Sounds like open box.,Neutral
AMD,![gif](giphy|TbfII8ChaSS6PlZFvv),Neutral
AMD,"399 is the price without a bundle with the motherboard, but the good thing is that my friend needs a new PC so I'm just going to give her my 7800x3D for about 175 (bought it for 325 in August of 2024) + the motherboard in the bundle.",Positive
AMD,So you get one at no change to you? Doesn’t seem very fair,Negative
AMD,"Combos with the motherboard, and then have my friend build a PC on the motherboard + 7800x3D that I'm going to give to her.  399 is the price without combos.",Neutral
AMD,"Yeah, that is the price without motherboard bundle.",Neutral
AMD,"It is kinda of how it works when you consider this chip is a perfectly functional 9800X3D that they withheld from the market to apply an unnecessary overclock so they can sell it a higher price. Tho obviously they won't stop production of 9800X3D completely.     They do that all the time tho to their credit this time the overclock is somewhat substantial which is not usual, let's wait and see. Still i think most people would rather this be a normal 9800X3D that they can try overclocking themselves than a pre-overclocked one for 20% higher price, I'm calling it right now this thing will NOT be 20% faster than 9800X3D, probably not even 5%.",Neutral
AMD,"I agree that it’s money saved. I’m just FOMOing a little bit, plus the whole “well I’ve spent a ton of money on components anyway…”",Neutral
AMD,The difference comes in cpu heavy titles ofc in non cpu heavy games the difference is margin of error since you aren’t using it,Neutral
AMD,The 7800x3d went ballistic the second reviews came out to show that they are still a really good CPU choice. Supply and demand did the rest.,Positive
AMD,"AMD didn't increase the MSRP on their store, so yes - it was retailers.",Neutral
AMD,"It's not. There is a Combo deal right now. I got the same thing. 9800X3D, motherboard, and 32GB CL36 DDR for $680",Neutral
AMD,"As a bonus, they're running a special right now that they'll throw in 32GB of DDR5 for $200 with any CPU.  If you need RAM for one of those systems buy it with the CPU, otherwise you're looking at more like $350 for the RAM.",Positive
AMD,As of now it’s not fully even reached 5% lol only 4.7% margin of error likely in most benchmarks. I would assume at most in 1080p and select games they’ll be a 3-5fps improvement maybe. Sometimes it will prolly be the same.,Neutral
AMD,"I was thinking the same thing as you are now, but I gave in. Build a new PC in February with a 9700X and 9070 XT when 9800X3D prices where almost double of the 9700X.   During Black Friday I upgraded to the 9800X3D as it was it was a good price and I couldn't fight my FOMO any longer.   In 1440P I don't think I would notice a difference if you would swap my 9800X3D back to a 9700X. If it makes you happy and you got money to spare go ahead and get the 9800X3D, but the 9700X is still a great CPU. The 1% lows are somewhat better in some games (that  I play) with the 9800X3D, but that's about it. The biggest upgrade is the name of the CPU and ""bragging"" rights.",Positive
AMD,"Now imagine how ballistic it would have been if the price of 7800x3d wouldn't rise to it's original MSRP, but stayed where it was during summer of 2024. 8% (or something I don't remember) worse performance for over 40% less (If my math is right).   How AMD managed to raise the price, if it was with slowing down the production, leading to decreased supply or with just selling it for more is kind of irrelevant in this context, as we're just interested in price. The point is that they can do the same here, making 9850x3d artificially more compelling product by simply raising the price of 9800x3d just before 9850x3d launches.    The price of 7800x3d reached an all time high just about when the 9800x3d was released. I don't think that's a coincidence. Looking at price history in December 2024 when the 9800x3d was barely available the price of 7800x3d went back to it's original launch price (at least here in Sweden) with the price creeping up since August of 2024.   Sure it's how market works, but It's not *all* market. AMD did something, and they might do it again, that's my point.",Negative
AMD,"Ok, but did they ever lower it?   The price listed on their website is not the price wholesale or retailers pay. Just because they didn't raise or lower it on their website doesn't mean they didn't change the price they sell at to wholesale. It's not like many consumers buy directly from AMD anyway.",Neutral
AMD,I have been eyeing a pretty  budget upgrade for sub 500 with 7600x3d wondering how dumb an idea  that would be.  Right it’s  between 400 and 420 at micro center. Would be for a mostly gaming setup would be upgrading from 3800x.,Neutral
AMD,"That doesn’t mean he’s not getting one open-box. I’ve seen them at Microcenter for around the price previously mentioned. There was no mention of a bundle, just the processor.   Even if this is the bundle offer, they’re not getting a 9800x3D for $325, they’re getting a bundle offer for $680.",Neutral
AMD,"when did that start?? i just bought a whole system, cpu memory ram gpu board, i didn’t get 32 gigs for 200 bucks!",Negative
AMD,Currently playing things at 1440p. And yea I suppose in the end it’s not a huge difference,Neutral
AMD,Just remember bragging rights only last 18-24 months lol,Neutral
AMD,"Correlation is not causation, you’ve no basis for your conclusion.",Negative
AMD,Pepperidge Farm remembers when microcenter used to give 32gb ddr5 for free with purchase of any 7000 series CPU,Neutral
AMD,"This week, I guess.  I bought a CPU last Thursday and the offer wasn't there at the time, and so I bought the cheapest ram I could find for $250 (Crucial DDR5 5600MT/s, not even a heat spreader!).  That same kit today is $300.99, and probably was $75 3-4 months ago.",Neutral
AMD,Haha i was talking about fall 2022.,Neutral
AMD,Oh…back then the 32GB of RAM was way cheaper anyway.,Neutral
AMD,AMD still not shipping their discrete graphics in laptops in 2025 is an own goal.,Negative
AMD,"bring back the z16 line, with haltic touchpads and no numpads. Hx 370 or strix halo",Neutral
AMD,Still no Stix Halo 128gb laptop other than that single 14 inch HP one. I'd call it a phantom launch but there are many mini PCs with that chip. It makes no sense.,Negative
AMD,"How is this the most powerful?  It doesn’t have the most powerful apu, it doesn’t have the most powerful mobile cpu  from amd, and it doesn’t have a gpu.  It doesn’t even have a high end screen.",Negative
AMD,Because Lenovo refused to make:  - P1 - P16v - P16s/P14s with the better chassis and dGPU  - P16 no suffix  with AMD.,Neutral
AMD,"2.4k euro 2.6k usd is that because its got 64gig ddr5 and has ""AI"" in it? dont see the point in that high of a price other wise",Negative
AMD,"Screen resolution 1920x1200. Is it a machine from 2015?    Crippled AMD versions as usual, thanks Lenovo.",Negative
AMD,"The ""16-inch"" is doing a lot of work here, as ASUS and HP already have 13"" and 14"" models with Ryzen AI Max+ 395  Also a 16"" model with that processor was [previewed by ETA Prime](/r/Amd/comments/1medko7/this_laptop_has_amds_most_powerful_igpu/) a while back, I understand those were supposed to launch in time for the holiday season but the memory market threw a spanner into these plans.",Neutral
AMD,And dumping their unwanted last gen 7600m dies with absolutely zero mainstream oem demand en mass (steam machine),Neutral
AMD,"And no 6-8 core x3d laptops with 5090, and no AMD 2-1 OLED laptops, etc.",Neutral
AMD,AMD never misses the opportunity to miss the opportunity.  It's actually unreal how consistently good they are at fucking up what seems like an obvious move.,Negative
AMD,If they make a strix halo laptop it won’t be a Z16. It will be a different much thicker lineup. AMD also doesn’t have anything to succeed the 6550m and the Z13/Z16 was overall a thinkpad nobody asked for. I predict the same fate for the X9.,Negative
AMD,>there are many mini PCs   Even this is a massive overstatement because practically ALL of them (sans the high priced HP Z2 mini) are from small local OEMs and many of which in weird obviously non-mainstream form factors.  I would be very surprised if total shipment of all of these mini PCs combined even reached a few hundred thousands worldwide.,Negative
AMD,"Yeah, it's strange there is no 16 inch laptop with strix halo yet, I have seen online from multiple sites that gaming laptops with strix halo are coming as part of CES 2026 and there was the sixunited leak of a possible Ryzen AI max 388 and 392, the ram shortage will of course sting here as they still need at least 32GB ram to be viable but hopefully will be offset by the longevity of having more than enough ram.  My guess is it's timing, the mini pcs and handhelds have been coming from small venders that just release when they are ready while big OEMs like Asus or Lenovo like to wait for Computex or CES to launch laptops, especially high end laptops like the kind that strix halo will be powering.",Neutral
AMD,"It’s the most powerful 16”, AMD Lenovo laptop.",Positive
AMD,As I bitterly pound away at my hot & loud corporate provided intel lenovo thinkpad,Neutral
AMD,If only there was a P1 with Strix Halo...,Neutral
AMD,Its 1.8k usd when you buy this specific model from Lenovo directly,Neutral
AMD,"The thing I hate about thinkpad is the shitty starting screen resolution. Then different screens are locked behind arbitrary hardware configurations, or just randomly not available in your country for some reason.",Negative
AMD,"[theunknownforeigner]   > Screen resolution 1920x1200. Is it a machine from 2015?   > Crippled AMD versions as usual, thanks Lenovo.  &nbsp;  Please read this before you make yourself look even more stupid:  https://psref.lenovo.com/Product/ThinkPad/ThinkPad_P16s_Gen_4_AMD?tab=spec",Negative
AMD,"> ASUS and HP already have 13"" and 14"" models with Ryzen AI Max+ 395  The headline is specifically about Lenovo",Neutral
AMD,"> ASUS and HP already have 13"" and 14"" model**s**  Drop the plural.",Neutral
AMD,I think at this point it's worth them spinning up a small company to contract manufacture laptops from Pegatron or Tongfang and sell those into the market.   Make some kind of Surface-like halo product.    There are so few AMD Advantage systems today that I can count them on one hand.,Neutral
AMD,They don’t even make any X3D chip for laptop less than 16 cores. Laptop vendors always want to force bundle the highest priced combos and not allow sensible options.,Negative
AMD,"I was the owner of z16. I agree they made so many stupid decisions with that laptop and no wonder why it didnt have many sales. First of all that 6500m was barely faster than 780m igpu, 60hz screen, small battery. But with all those flaws it was actually so so close to being proper macbook competitor. Haptic touchpad  really nice, no numpad, awesome build quality, full amd system meaning linux experiece was 10/10. I would pay any amount to buy fixed z16.",Negative
AMD,"> while big OEMs like Asus or Lenovo like to wait for Computex or CES to launch laptops  This is just not true. They launch intel models left and right at any time in the year. They only ""reserve"" model launch for big events when new chips get unveiled at those events.",Neutral
AMD,"Interesting, specially since my AMD Lenovo laptop has a ryzen 9955HX3D 16 inch oled laptop is far stronger, but I guess it doesn't count because its got an Nvidia GPU instead of being an APU.",Neutral
AMD,low bar lmao,Neutral
AMD,Tbf I don’t think you got a lunar lake one.,Neutral
AMD,"Forget P1, Lenovo has zero strix halo laptop across their entire lineup.",Neutral
AMD,Tbf that’s a corporate laptop thing not a Lenovo specific thing. Elitebook/Probook and Dell Pro are the same.,Neutral
AMD,"Read the review, you're stuck with the 1920x1200 if you want Ryzen 9. Same with the P14s Gen 6 AMD.",Neutral
AMD,There is not a single zen 5 amd advantage system. Zero. Not one. I’m pretty sure even the zen 5 framework 16” isn’t an official advantage laptop anymore.,Negative
AMD,"Yet for some reason there is no laptop 5080 or 5090 with even the 16 core x3d, which is insane considering that is the best gaming chip, and the extra efficiency of x3d would benefit power and thermally limited laptop dgpus.",Negative
AMD,"I agree it looks great, and I actually think the faux leather feels nice. But “fixing” it requires a hypothetical die smaller than Navi 44 (9600 desktop die) or powergating Navi 44 to 50w TGP. Amd is not showing any care in laptops atm. They still don’t even have any mobile rdna4. 2026 is all coast 0 new things.",Positive
AMD,"That’s mostly true, you’re correct. The difference is that flagship laptops, like Strix Halo systems, RTX 5080/5090 models, or more extreme designs such as the Zephyrus Duo, are usually saved for CES or Computex announcements rather than dropped randomly.  More budget‑oriented devices, like ASUS TUF or the recent Gigabyte A16 Pro, tend to launch whenever. But I’d be surprised to see “budget” Strix Halo laptops in 2026, especially with the current RAM shortage. There will probably be more options than in 2025, but I wouldn’t expect them to lose their premium pricing since Nvidia faces the same memory constraints with their competing products.",Neutral
AMD,"They should’ve just said most powerful AMD Thinkpad laptop instead of this, because it is confusingly worded. I actually have this P16S laptop for work, and it’s honestly a pretty solid workstation with excellent battery life, but yeah the 890m isn’t exactly going to win any awards.",Neutral
AMD,Exactly.. which is a shame,Neutral
AMD,There are 3 in total,Neutral
AMD,I would prefer to not have dgpu. Strix point or strix halo. I currently. Have strix point tuxedo laptop with slow slotted ram i stead of lpddr5x and it runs games like pubg and cs2. Cant even image what good of experience it would be with halo.,Negative
AMD,"Lenovo dropped the AMD variant of the Legion Pro 7 randomly a few weeks back.   I have a different prediction.  I would post nudes on main if there is a mainstream (by mainstream I mean mainstream form factor, NOT mainstream budget) 16"" clamshell laptop with Strix Halo from the big three vendor.  There are usually leaks. There aren't any.",Neutral
AMD,"I actually like the P series for workstation, so honestly its easier to recommend them rather than trying to use catchy titles from their side honestly.  I agree with you",Positive
AMD,"> most powerful AMD Thinkpad laptop  How many hoops until the title is basically ""The most powerful AMD ThinkPad P16s launched in the USA in 2025""?",Neutral
AMD,Halo is never happening in Z16-like chassis.,Negative
AMD,Due to thermals? Why not? They put it in a tablet,Neutral
AMD,"An ROG is not the same philosophy as a thinkpad at all. Thinkpads have never tolerated high temps or high fan noise and have always been extremely conservative with power limits relative to size. The only thing comparable is HP's Zbook Ultra 14, and that is thick for a 14 incher. A 16 incher (you want to be able to hold 80w) would be much thinker than a Z16.",Negative
AMD,Power your GPU with two separate 8-pins.,Neutral
AMD,Thanks everyone for the feedback! Definitely gonna be switching to separate cables for the GPU power.,Positive
AMD,Are there any intake fans?,Neutral
AMD,Nice 👍🏼,Positive
AMD,Use two separate 8 Pin cables ffs.,Neutral
AMD,Where are your kables? Mines a hot mess next to the gpu,Neutral
AMD,How’s the cooling in that case? I’ve been considering this one once I have the motivation to tear my build apart and swap to a smaller MB 😅,Neutral
AMD,"I have a very similar build in an NR200 that I put together this past October. 7800x3D and 6950XT coming from an i7 6700k platform. The performance is night and day, its not even close. Even with the same GPU, I've almost doubled my FPS in some games and increased the 1% lows immensely.  I love that case, its like a Fractal North Mini",Positive
AMD,Love that case! I have one too. I went from 8700k to 9600x.,Positive
AMD,"This is awesome, great little case! I just downsized to this case from a North XL. Added a B850M-Plus and Arctic LFIII Pro 240mm, reused 9800x3d, 4090, and A850GS (fits in highest position with 240mm AIO). Now I have a leftover case + B650-E (non E) + LFII 360mm AIO. I found a paper bag full of DDR4 at work, maybe that will finance a whole new refit (/s)",Positive
AMD,How many Fans can you put in that case? I always wanted to try LianLi Cases but they are so expensive in my country.,Neutral
AMD,"Ooh, you too, huh? Yeah, the case tore up my fingers and all on Friday night, but it's great! Alas, now I'm running Windows 11, but how's Bazzite Linux?",Positive
AMD,just curious why 1000w PSU ?,Neutral
AMD,My advice could be to consider some slim 120mm fans as intakes underneath the GPU.,Neutral
AMD,"If your GPU draws more power than one 8pin allows, you run the risk of damaging the cable, the GPU, and your PSU, in that order",Neutral
AMD,How come?,Neutral
AMD,Good thing mine only has one… smh 😂,Positive
AMD,this PSU has only 2 power rails; you're correct in this scenario.    2 power rails for a 1000W gold-rated PSU sounds soo cheap,Neutral
AMD,"pigtails are fine, that's a great psu from a reputable brand, if it's designed like that, it's meant to be used like that.  beside, the 7800XT barely pulls 250W.",Positive
AMD,The GPU and PSU act as intakes and the panels are mesh all around so plenty of fresh air for all the components,Positive
AMD,Honestly been way better than I expected. The mesh side panels are great. I have the GPU and PSU setup as semi intake fans then exhausting through the top and back,Positive
AMD,"Theoretically, you could fit like 10 to 12 all at the same time.",Neutral
AMD,Bazzite has been great. Everything worked pretty well out the box. I dual booted windows to play the Marathon play test and might reinstall Escape from Tarkov if I get bored with Arc Raiders.,Positive
AMD,Just because it was a good sale and gives me wiggle room for future. Only cost me about $10 more than a 750 or 850 would right around black Friday.,Positive
AMD,https://i.redd.it/tmym6kfy5m8g1.gif,Neutral
AMD,"A cable is rated for a specific load (of a six/eight-pin connector), you don't really want to put the stress of two connectors going through it. Two separate cables sharing whatever the GPU is drawing from the PSU is safer and one of the most common fixes for random crashes and system shutdowns.",Neutral
AMD,It's the correct way to do it. there's a lot of information online that people like to lead a front thinking it's fine(if not better) to do it that way but there's actual scientific information as to why running two separate cables is better.,Neutral
AMD,Jayztwocents did a video where a graphics card did not perform the same with pigtails as with two separate cables.,Neutral
AMD,"Pigtails are absolutely not fine. Please don't spread misinformation.  A standard PCIe 8-pin connector is officially rated for **150W** (12.5A at 12V), per PCI-SIG specifications. The motherboard PCIe slot adds up to **75W**, so one 8-pin cable theoretically supports up to **225W** total for the GPU.  Modern GPUs often exceed this:  * Many (e.g., RTX 30/40 series or RX 6000/7000 series) have TDPs of **300W+**, with power spikes reaching **350–450W** or more during heavy loads/overclocking. * For a dual 8-pin GPU, the intended max is around **300W** from cables + 75W from slot = **375W** total. * Higher current generates **more heat** (power loss scales with current squared). * Over time or under spikes, this can cause **cable/connector melting**, **voltage drop** (leading to instability/crashes), or in extreme cases, **fire hazards**. * Cheap or thin-gauge cables (e.g., 18AWG) exacerbate this; even quality ones from reputable PSUs (like Corsair, Seasonic) have limits—many manufacturers warn against splitters for GPUs over \~225–300W.",Negative
AMD,"GPU and PSU are not considered intakes, you should get some real intake fans in there bringing in cool air.",Neutral
AMD,"Yeah, I'm playing *Escape from Duckov*, its cuter, ah, cousin. I just died a few times. I guess it's bedtime.",Neutral
AMD,Then it makes sense to buy 1000w then,Neutral
AMD,Where is this from? There was nothing like this in the PowerColor pdfs,Negative
AMD,"The top right-most looks like daisy-chaining, and I think this is frowned upon.",Negative
AMD,"OP please post this as a comment if you may, some people might not see the importance of this information if it gets burried with other sub comments.  This is also the reason why I use two separate cables instead of pigtail for my 6700XT. I was undervolting and overclocking in previously with the pigtail and it was unstable as hell. Two Separate cables made one hell of a difference especially when I am playing Hogwarts and Battlefield 6.",Neutral
AMD,"in my comment I explicitly state that it's a good PSU, and the card itself pulls 250W. Your whole comment is as if I am telling OP to use pigtails with 350W+ cards or something.  corsair themself says it's fine: https://www.corsair.com/us/en/explorer/diy-builder/power-supply-units/individual-8-pin-vs-pigtail-connectors-for-gpus/",Neutral
AMD,"This case is a bit tight for fan spaces. It looks like OP might have enough exhaust fans to create negative pressure (4 ""exhaust"") in conjunction with how ventilated this case is, I don't think it will be an issue. OP should monitor temps though!",Negative
AMD,Can't exactly remember.. I've just kept the image,Neutral
AMD,"it's probably fine either way, I have the same case and use 3x arctic p12 slim fans as bottom intakes, the ones you can daisy chain, they fit under my 4080 so I believe they would easily fit in here if he wanted to add intakes",Positive
AMD,"If I were them I would consider making the back fan an intake even if it is a bit of an odd configuration. Really need some cool airflow in there, counting on GPU and PSU to pull air in is not a great solution.",Negative
AMD,"The temps have been more than fine so far, but I've really only played Marathon and Arc Raiders. Gonna stress the system soon with Cyberpunk. Haven't seen in game temps above about 56c yet.  Most builds I've seen in this case on r/mffpc use a similar airflow setup. I have some space below the GPU to add more inflow but not sure if I'll need it.",Positive
AMD,Too bad it has the firestarter connector,Negative
AMD,"Got my 9070 XT Pure a week ago, this Asrock card looks nice as well",Positive
AMD,Aorus Master cards have competition now,Neutral
AMD,"I'm just imagining when it catches on fire, the display changes to the dog meme ""This is fine""",Neutral
AMD,"I was sceptical at first until I realized if it get burnt within 5 years I'll have a free upgrade with RMA 🤷‍♂️ Yes we have 5 years by law here regardless of what companies might want.   Already got a free up/side grade from a 6900 XTX to a 9070 XT, which broke right within 2 years.   I'm going full out with the firestarter connector, ASRock AM5 Mobo with 7800X3D. If anything breaks within 5 years I'm golden, if not it's outdated anyway 🤷‍♂️",Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"Ok everybody, say it with me:  it’s just a slightly higher clocked version of the 300 series with the same Zen4 and RDNA 3.5 cores. There’s not anything to get excited about.",Neutral
AMD,Their naming conventions are dumb.,Negative
AMD,Gonna be a slaughter by Panther. AMD not crossing that 20-25% threshold on laptop market share they've been at since Renoir era.,Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,It’s just a slightly higher clocked version of the 300 series with the same Zen4 and RDNA 3.5 cores. There’s nothing to get excited about. How’d I do daddy?,Neutral
AMD,It’s not going to be a slaughter because of the price difference. Panther is not going to be priced just as Ryzen 400. It’s going to much more expensive because of the new high tech,Neutral
AMD,"Nothing gets slaughtered by anything. If current ram and ssd prices continue to soar, people will be looking at the cheapest CPUs to offset that 16GB.",Negative
AMD,It’s just a slightly higher clocked version of the 300 series with the same Zen4 and RDNA 3.5 cores. There’s nothing to get excited about. How’d I do brother?,Neutral
AMD,"Somehow the supposedly expensive lunar lake with on package ram can be found in laptops with similar price to kracken point, not just strix point. Whether that is intel eating a lower margin or laptop oems eating a lower margin, I don’t know, and isn’t for consumers to worry about.   There will also be 10x as many laptops using Panther than rebrand strix point just like there was way more laptops using meteor than phoenix rebrand (hawk point) in 2024.",Neutral
AMD,The volume will once again be corporate bulk purchase via long term contracts as usual.,Neutral
AMD,"Ok everybody, say it with me: it’s just a slightly higher clocked version of the 300 series with the same Zen4 and RDNA 3.5 cores. They should've named it RDNA 3+.",Neutral
AMD,"Can you find me a laptop with Lunar Lake and Krackan, with exactly the same specs, and at the same price? Used Geizhals and did not succeed.",Negative
AMD,And it will have to compete with long term contacts set by AI data centers backed by trillion dollar companies,Neutral
AMD,It’s just a slightly higher clocked version of the 300 series with the same Zen4 and RDNA 3.5 cores. There’s not anything to get excited about.  Are we… are we still doing this?,Negative
AMD,"https://www.dell.com/en-uk/shop/laptops-2-in-1-pcs/dell-pro-14-plus-laptop-or-2-in-1/spd/dell-pro-pb14255-2-in-1-laptop/gcto_pb14255_emea?redirectto=SOC&configurationid=8c519bdc-ad7a-4b90-8b7a-01e5a3bc99bc   https://www.dell.com/en-uk/shop/laptops-2-in-1-pcs/dell-pro-14-plus-laptop-or-2-in-1/spd/dell-pro-pb14250-2-in-1-laptop/gcto_pb14250_emea?redirectto=SOC&configurationid=d4e36ddc-2337-4a13-9372-6f34a11838c0  These two Dell Pro 14 Plus with Ryzen Pro 340 or Core Ultra 236v both with 16GB ram, 512GB SSD, same display and all other options as closely match as possible. Difference came out to be £25 cheaper on the LNL side.",Neutral
AMD,"And just like in the past years, the worldwide PC shipment had never dropped apart from post-Covid adjustment in 2023. Businesses on a scheduled upgrade cycle will eat the cost amortised over the useful life cycle of hardware as usual.",Neutral
AMD,I rather have the Miku version,Neutral
AMD,That thing should be at least $50 cheaper than the regular version.,Neutral
AMD,Perfect for when something goes wrong ASUS will be of no help in the customer service department,Negative
AMD,What a total miscalculation to make this instead of a battlefield 6 or arc raiders 9070xt.   No one's gonna buy this.,Negative
AMD,The 9070 XT is a $599 card.,Neutral
AMD,"ROG 9070, but nvidia have ROG brand only for them.",Neutral
AMD,Ew,Neutral
AMD,This isn't the most exciting game to base a custom edition card off of. Blacks Ops 7 is far from what BO1 and BO2 were.,Negative
AMD,I also keep by memory at 8008mhz so it burns itself to death,Neutral
AMD,The sales are down bad for activision this year,Negative
AMD,My basic reaper 9070 will do thanks,Positive
AMD,"They are doing everything now just to get people playing this shit game, because nobody wants to play it lmao",Negative
AMD,sry i wont put that thing on my desktop. A cod version? ??,Neutral
AMD,"I am building a Linux mini-ITX PC, and that card would not fit, and also the software mentioned doesn't work on linux. It looks really cool though. I don't play CoD, but I would still buy it if I was in the market for a mid-tower ATX PC",Positive
AMD,"Instead of fixing the fidelity CAS upscaler where it’s blurry when you move in game, they make a new COD branded gpu. Fidelity CAS was the best graphics option for competitive play. It was a good blend of performance and visibility. The only way to see in the game now is to have DLSS transformer (which requires an NVIDIA gpu) or FSR 4 (which requires a new 9000s series AMD gpu). The rest of AMD gpu users are stuck with a broken graphics option which is weird because BO6 it was working fine.",Neutral
AMD,Two brands gamers are growing to genuinely despise: ASUS and Call of Duty. This was a major miscalculation,Neutral
AMD,There’s a Miku version woot,Neutral
AMD,We have REVA  https://www.powercolor.com/news-detail71.htm,Neutral
AMD,it should been included in the game pass Subscription,Neutral
AMD,it’ll sell out,Neutral
AMD,It’s almost like releasing a Madden 2026 edition.,Neutral
AMD,Nobody? It’s COD bruh 🤣,Neutral
AMD,"I used to work at a 4-letter company that made a killer COD Black Ops card back in the day, but I would also say that this is a really nice-looking design for our TUF Gaming models, which typically cater to solid tones. Of course, if it doesn't work for your system in terms of colors - or a preference towards franchises, then I also understand that too.",Positive
AMD,"The Miku version is on the NVIDIA side, unfortunately. Personally, I still really like the color design of this model as it's used as a colorway on the TUF Gaming design.",Negative
AMD,Im just saying B07 is shit and gets a lot of hate cuz of it,Negative
AMD,I think its more a judgment of a steaming pile of a video game that had been universally panned and having that represented on a video card is a tough sell. Its like when Apple got blasted for including the U2 album on all of their devices and people couldn't delete it.,Negative
AMD,Powercolor has you beat with the power of waifu  https://www.powercolor.com/news-detail71.htm,Neutral
AMD,"COD gets tons of hate annually and it isn’t due to “quality” (people would have to play the game to assess that). It’s due to being the highest selling franchise of the past 2 decades.   The same reason Justin Bieber got hated on, the same reason LeBron gets hated on, the same reason Tom Brady gets hated on, etc.   People hate to see the same thing succeeding over and over and will pray on its downfall.  Black Ops 7 players love it very much, non-players hate it very much. A tale as old as time.",Negative
AMD,"I get where you and others are coming from, but I'm still skeptical when I hear that a game is universally disliked - especially franchises like COD and Battlefield. I gamed on the original MW for a long time and remember the build-up for the so-called ""boycott"" of MW2, which barely lasted beyond the launch of the game. Many people still didn't play the game, and I'm sure that's true here as well, but a lot of the most public and loudest critics were still queueing up as soon as the game launched. It was a fairly instructive moment in how the Internet works.  However, I don't think the analogy works here. We still offer our standard TUF Gaming Radeon RX 9700 XT without the COD tie-in and design, whereas the Apple situation involved everyone using the service and didn't have an opportunity (initially) to choose if they wanted the album or remove it.",Neutral
AMD,Oh wow that looks really good,Positive
AMD,That doesn't seem as exciting as Yeston's waifu cards.,Negative
AMD,"Since bo7 came out cod has regularly had on average around 45 thousand players on steamdb losing at least 100,000 players. They're now doing literally anything they can to get people to play their game hence all the promotions and free weekends",Neutral
AMD,It'd hard to beat ocean water scented weeaboo power,Neutral
AMD,"that’s Steam. COD consistently does poorly on Steam.   According to Circana, COD is #2 on both consoles just behind Fortnite (was #1 on launch month though).   COD is on 3 PC clients: BattleNet, PC Game Pass, and Steam.   Steam has always been COD’s smallest platform.",Neutral
AMD,Finally someone saying what I've been saying for days. Steam players are convinced that Steam holds like 90% of PC player base. They don't know that some games are played much more on other plaforms. For example Diablo 4 has like 10X more players on [Battle.net](http://Battle.net) than on Steam. CoD is played more on GamePass AND [Battle.net](http://Battle.net) than on Steam. They just can't accept that we are not all Steam glazers.,Neutral
AMD,Steam does have most of the pc player base on it without question though. The one point you made here is invalidated by the fact that diablo 4 isn't a game that was doing well anyway so when it shadow dropped on steam it naturally wasn't gonna have a huge player base and the few that are left on [battle.net](http://battle.net) are all people who got really into it or people who spent too much on it to quit.   Ironically you just did the exact thing that Throwaway was initially talking about by hating on something just for the sake of it not to mention that glazing [battle.net](http://battle.net) of all launchers is just crazy work. What next? Ubisoft Connect?,Negative
AMD,"Some games are just not played on Steam,  reason doesn't matter. Saying that Diablo 4 have only ""few"" players is so disingenuous. The game is really popular, especially now more than ever with current season and Paladin class added.",Neutral
AMD,"Does intel 10A still come out as scheduled in 2027? I googled it and found out intel said the 10A will come out in 2027, but this was old news in 2024.",Neutral
AMD,"GFHK also has 14a for Razor and Coral Rapids in 2H 2027, so I'm taking what they are saying with very little credibility.   Plus, we had very similar rumors during 18A, and that went nowhere. Fool me once...",Negative
AMD,I wonder how intel and other companies are going to manage for next year? Prices for memory and SSD’s are predicted to go even higher putting off many buyers from getting a new PC build or laptop.   This makes me concerned Nova Lake won’t sell as well because of this.,Negative
AMD,It's shameful to see LBT posing with 14A wafers when all the groundwork for this was setup by Pat Gelsinger. The entire Intel board should have been sacked instead of Pat.,Negative
AMD,Unbelievable till official announcement,Neutral
AMD,can't they use it to make more ram ?,Neutral
AMD,good news,Positive
AMD,"Lisa So Sue Me wants a taste of the Lip? Am I living in a different dimension? I callled out So Sue Me on X, is she jumping on Big Blue’s Back?  Is anyone Dollar Cost Averaging INTC? It will still be awhile before IFS is firing on all cylinders. The Lip said he would stop high end chip production for external customers (If No One Took A Byte) in order to get $$$ to build out Ohio Fab.   Let’s get it done. I’m driving distance from the Ohio Fab, any chance Intel will give me a tour?",Neutral
AMD,"If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  [https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots](https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots)  *""Intel's previously-unannounced Intel 10A (analogous to 1nm) will enter production/development in late 2027, marking the arrival of the company's first 1nm node, and its 14A (1.4nm) node will enter production in 2026.*  ***\[Edit: to be clear, this means 10A is beginning development, not entering high volume manufacturing, in 2027\]*** *The company is also working to create fully autonomous AI-powered fabs in the future.""*",Neutral
AMD,"14A probably won't be ready for 2027, much less 10A.",Neutral
AMD,10A & 7A are in R&D phase,Neutral
AMD,It's gonna be 2026 soon and 18A is launching at the very start of 2026. A double node shrink in like 2 years doesn't exactly sound very possible.,Neutral
AMD,"Remember, these are just names/nicknames. 10A? The difference between 14A and 10A is probably equivalent to the difference between 14nm and 14nm+",Neutral
AMD,And yet here you are.,Neutral
AMD,"Brother, don't hint at your place of employment when you have your full face in your profile as well as you commenting in NSFW subs.",Neutral
AMD,There will probably still be another of layoffs next month 😂,Negative
AMD,"Yes, perhaps it’s better if you post it on the r/intelstock subreddit instead 🤪",Neutral
AMD,"Ram should be at a more reasonable price in 2027 according to Moores Law is Dead. Maybe not $100 for 32GBs, but maybe below $200 🤞",Neutral
AMD,"The entire Intel board probably should have been sacked, but Gelsinger as well. He failed at his main mission and drove the company into a crisis. That kind of thing should have consequences.",Negative
AMD,Who was it that decided to exit the SSD business.  They sold off a cash cow for pennies on the dollar.,Negative
AMD,Nvidia is at least some what believable. AMD though?,Neutral
AMD,"I thought that too. At least they'd have some money coming in. But apparently it takes years to rejig the plants to churn out RAM instead of CPUs. And they're heavily invested in getting the next gen CPU fabs working.   Pivoting to RAM just doesn't make sense, unless they magic'd up a new type of RAM that's cheap to make and has super low latency - which is one thing I've always thought they ought to do.   Imagine if external RAM ran with super low latencies like CL1 or CL2 or something. You wouldn't even need branch prediction and prefetch and massive caches in the CPUs.",Neutral
AMD,"""news"" needs a lot of quotes around it...",Neutral
AMD,This isn't wallstreetbets. We don't talk like that here.,Negative
AMD,">If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  Yup, and to make it even more obvious, the same graph also has Intel 14A showing up early 2026, and 20/18A showing up at the start of 2023*,* so clearly it's not the date of when the node is going to come out (or even start HVM).",Neutral
AMD,"Dunno why this is being downvoted, the CEO of Intel himself said that 14A is a 28-29 node in the Q2 2025 earnings call.",Neutral
AMD,enough info about how intel names products exists to know. if it didn't increase in transistor density per mm it would not be called 10A.,Neutral
AMD,"I think everyone knows there will be continued Q1 and possibly Q2 layoffs.   Return to office didn't lead to enough voluntary attrition. Leadership wants to hit a magic number which sounds good for financial reports, not what is actually viable to run things.",Negative
AMD,The thing intel is doing rn is literally pat's groundwork isn't it?,Neutral
AMD,Still a tall order imo unless it's some defense chip for RAMP-C,Neutral
AMD,"If they're following industry standards I'd say it depends on how good AMD's next gen is. Intel doesn't need direct access to AMD designs to etch chips for them, and designers make way more than per wafer than foundries do.  If AMD has superior designs to intel again they could finally ship out some damn chips for laptop OEMs. It would hurt intel more than the revenue would benefit them imo since client has really been carrying intel for the last six years and demand for AMD chips has been high despite the drip feed of strix chips. honestly I'm considering an AIO/NUC/whatever the new name is with strix halo and unified LPDDR5 to upscale old footage without having to use my daily desktop. imagine if it was available at scale.",Neutral
AMD,"they don't have to make faster ram, just make it, right now, some ppl don't really care about speed",Negative
AMD,"So, risk production in late 27/early 28 and HVM in 2029 I suppose?",Neutral
AMD,YEs it is. He did make mistakes. He was hiring like crazy at the beginning of his term. And he should have started cutting sooner. But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.,Negative
AMD,"Nothing they're doing *right now* is a success story. Remember that they don't actually have customers, and that is first and foremost what got Gelsinger fired. As things stand, the foundry as a whole is a failure. If things turn around, that will have to be under Lip Bu.",Negative
AMD,I wasn’t aware 14A is part of the RAMP-C initiative. I thought it was only Intel 16 & 18A that are currently covered by RAMP-C?,Neutral
AMD,I think so. Maybe optimistically we see a 14A product in late 28'.,Neutral
AMD,"> But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.  No, that was just more wasted money. 18A doesn't even use the high-NA machines Intel bragged so much about. It seems they tried blaming their struggles in foundry on the equipment instead of the broader org culture and talent.",Negative
AMD,"As much as I hate to say it, Intel arc was also a mistake.",Negative
AMD,It can expand in future ? My point is how can we believe such stuff at face value without actual proof.,Neutral
AMD,14A does use the High-NA machines. They didn't buy them with no plan to use them That would be stupid.,Negative
AMD,"It can expand in the future but this is a trial, it’s not yet a long term commitment until the outcome of the project is known (final evaluation won’t be until 2026/2027). 14A is not part of RAMP-C, it’s still in phase III trial with 18A. There’s been no additional RAMP-C design calls via NSTXL that I’m aware of",Neutral
AMD,"> 14A does use the High-NA machines. They didn't buy them with no plan to use them  They bought the very first high-NA machines, claiming it was for 18A. Now they won't be used until a node that hits volume in '28/'29, by which point TSMC will have (or rather, already has) much better machines. So what exactly was the point?  > That would be stupid.  Is that not a perfectly apt description for Intel's foundry strategy in recent years? It sounds like they really drunk the coolaid with their attempts to blame the 10nm failures on the lack of EUV.",Negative
AMD,14A will have volume production in 2027.,Neutral
AMD,Didn't Intel say in a presentation that 2027 is risk production for 14A? https://www.techspot.com/news/107736-intel-doubles-down-foundry-ambitions-unveils-18a-14a.html  https://www.youtube.com/watch?v=5Jbj4RQBXbo&t=818s,Neutral
AMD,"Lip Bu himself is saying '28-'29. At this point, there isn't a chance in hell it's ready for volume in '27.",Negative
AMD,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Negative
AMD,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Neutral
AMD,I hope it comes to desktop CPUs,Positive
AMD,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Positive
AMD,Yeah this headline doesn't add up based on my own testing,Negative
AMD,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Negative
AMD,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Positive
AMD,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Positive
AMD,concur  some benchmarks are biased,Neutral
AMD,lateral,Neutral
AMD,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Neutral
AMD,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Neutral
AMD,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Positive
AMD,Answer to strix halo was the partnership with nvidia,Neutral
AMD,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Neutral
AMD,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Neutral
AMD,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Neutral
AMD,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Neutral
AMD,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Neutral
AMD,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Neutral
AMD,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Neutral
AMD,Back in the day you could overclock a 2600k from 3.4Ghz to 4.5Ghz on a $25 Hyper212 cooler. The performance gains were incredible as Sandy Bridge scaled very well at higher clocks.   Now days CPUs come overclocked already.,Positive
AMD,"""It's crazy to think that a cpu from 2009 can be easily overclocked.. 2.9Ghz to 4.1Ghz is crazy !""  You could overclock huge amounts on earlier generations - I used to run Pentium 4 1.6GHz chips at 3.2GHz on air-cooling, more on phase-change cooling.",Neutral
AMD,"I ran my i5 750 2.67Ghz for years at 4Ghz without any issues. I benched it some at 4.2Ghz even, but it was not fully stable.  The X58 CPU are even better tho. And even if you had insane OC potential back in the days it was not as good as it sounds, since the turboboost was higher than the stock frequency that is listed.",Negative
AMD,"Lol a 15 year old computer running Windows 11, meanwhile Microsoft telling people to upgrade 5 year old laptops for win10 being EOL.",Neutral
AMD,X5690@4.6GHz on Rampage III Extreme 😘,Neutral
AMD,it is crazy that intel sold you same technology at downclocked speeds to make a nice model range with different prices.,Negative
AMD,Sick stuff. I still got my i7 930 at 4.2Ghz running just fine. These types of chips overclock like crazy.,Negative
AMD,Be nice. Give it another stick of ram!,Neutral
AMD,"Cool. Glad it worked for you. I have dual xeon server, maybe i should try it. But its production server dont wana break my apps. Lol",Positive
AMD,My 2500k did ~4.8 ghz and my 6950x did 5.2 ghz. Its base clock was like 3.2ghz and this was using 128GB of quad channel DDR4.  It was “stable”,Neutral
AMD,45nm is crazy in 2025,Neutral
AMD,Q6600 G0,Neutral
AMD,500W power draw when,Neutral
AMD,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
AMD,"I used to run my i3-540 at 4.2GHz, air cooled on what is effectively worse than a Hyper 212 Evo. I miss the old days when I could overclock the snot out of them. These days I guess they're binned to almost their max potential out of the factory so most of the time I'm undervolting them.",Negative
AMD,Well done. Still using two H55m machines with OC (x3450 and i5 661).  They also OC decently at stock voltage keeping turbo and all power savings. My X3450 does 2.6 -> 3.3Ghz(3.8 turbo). The advantage is that it idles quite low at 50-60W.   But for gaming and rendering it's better to go all in as you did. Most chips can do anywhere from 3.8 to 4.2 all cores IME.,Positive
AMD,nah my 40 logical processors would smash through it all  x2 xeon e5-2680 v2,Neutral
AMD,"is that better? I dont need to dive into setting anymore, the CPU maker do it for me with warranty.",Negative
AMD,"There is still more to work with, especially if one does not fossilize on static all core OC, but does 2-step TVB fueled dynamic OC, Ecores are Aldo the source of much happiness on arrow",Positive
AMD,I miss overclocking. Felt like you were getting a bargain. Now I don’t even try.,Negative
AMD,"Not as big an OC as yours, but I had a pre-built from FutureShop.  It was their home brand name.  Found a BIOS for the board that wasn’t theirs.  Managed to get 3.2GHz out of a 2.4GHz Pentium 4 on pre-built from FutureShop cooling.",Neutral
AMD,"Wow, soo cool",Neutral
AMD,The motherboard doesn't accept other stick of ram. Only my corsair ram work,Negative
AMD,How did you even get a 6950x to boot at 5.2ghz? Most of them hit a wall around 4.3ghz,Negative
AMD,"Has its ups and downs. Now that I'm older and have less time to tweak things and mostly just want shit to be stable, I see ""pre-overclocked with maybe 5% performance left on the table"" as a pro. The con is that chipmakers just jam a ton of power through it to make it happen, and the option of buying a half-price chip and spending an entire sleepless weekend tweaking it yourself to get 95% of the more expensive chip's performance is gone.",Negative
AMD,"Same - the complexity and heat rose a lot and the gains because less significant - with multi-core chips and turbo frequencies there just isn't much headroom in them.  That and I work fixing issues with computers all day, I just want my own PC to work.",Negative
AMD,"Thats because these older CPUs were surprisingly energy efficient. Also mostly because now modern CPUs are powerful enough where overclocking is pointless. Even my i3-12100 being overclocked would be pointless, even if its only a 80 watt CPU",Negative
AMD,He couldn't without LN2.,Neutral
AMD,"It was short lived, over ~7 years I had to pull back the multiplier from 52 to 44 to keep it stable.  I retired the system this year.  It was a full open loop from EK.  2x Pascal Titan X in SLi",Neutral
AMD,"I used to overclock everything, now I undervolt everything lol",Neutral
AMD,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is “slow” is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already “beating” AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Neutral
AMD,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Positive
AMD,"I've gotten one and honestly it's amazing, easily the best laptop I've ever used so far.   I was skeptical about the battery life claims but I've genuinely found that using it for about 8 hours straight for coding, only drains the battery maybe 50%.  I've set it to only charge up to 80% max for battery health conservation, and I've regularly coded for 12 hours straight on the medium performance profile and haven't needed to charge until I got back home.  (This is for the Ultra 7 258v cpu variant btw)  Also this is while running Fedora with KDE Plasma which makes the battery life even more impressive as it's one of the heavier distros running cutting edge hardware and I've heard that Linux has less battery optimization compared to windows.    Screen isn't anything to write home about but the 100% srgb one looks good enough and is bright, 60hz looks kind of bad but I know that it saves a lot on battery.   Keyboard feels very nice as far as laptop keyboards go, having it be easily swappable is lovely as I wore out the keys on my old laptop, and I want this thing to last.   Linux hardware compatibility is perfect so far, even the fingerprint sensor works out of the box on fedora.   My only real complaint is that the plastic it is made out of is a major grease magnet and if I touch it without having immediately washed my hands, even if my hands weren't dirty, it'll leave dark patches from oils. Also it would be nice to have swappable RAM but I think 32gb ought to last a very long time anyway.   Genuinely seems like arguably one of the, if not the, best laptops for actually getting work done. Maybe it's not as fancy or sleek, but it just works. It's like the 2001 Toyota of the laptop world, it's not winning prizes for looks, but it'll never die, gets good mileage (battery life), and is easily repairable. Maybe not the laptop you want, but definitely the one you need (excluding people who need something like a dedicated GPU or really need super high CPU performance).",Positive
AMD,"Intel beats AMD in software (drivers, firmware) … I got think pad 780M laptop by company I work for. Randomly display won’t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Negative
AMD,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Negative
AMD,lol. Even in the cons it says weaker multicore than AMD….?   This article seems like AI wrote it,Negative
AMD,"Unfortunately Intel abandoned the on-package RAM after Lunar Lake again, which is the primary reason for the great efficiency and low power usage. I kind of understand why, it's expensive and not very flexible, plus apparently the market doesn't actually care that much about long battery runtimes. Only a small minority of people are ready to pay premium for this.",Negative
AMD,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Negative
AMD,Suck at gaming.,Neutral
AMD,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Neutral
AMD,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Negative
AMD,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Negative
AMD,Isn’t the keyboard one of the most important characteristics?,Neutral
AMD,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Negative
AMD,"Hey I’m looking at the exact same laptop that you have. Can you tell me about the build quality and if there’s any keyboard flex when pressing down on it? Please tell me. I’m going to use it for word, excel, reading lots of pdf files and ebooks and watch movies. Will it be enough for that?",Neutral
AMD,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Positive
AMD,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Neutral
AMD,Soldered ram is a lot faster. So no.,Neutral
AMD,Yes but now RAM costs a ton of money,Neutral
AMD,Is multicore performance the only consideration when buying a laptop?,Neutral
AMD,What kind of issues?,Neutral
AMD,What kind of issues with Intel? I thought it was the AMD that had tons of issues,Neutral
AMD,It is not a gaming laptop,Neutral
AMD,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Positive
AMD,It's an enterprise grade product you buffoon.,Negative
AMD,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Neutral
AMD,Build quality.    Thinkpads are solid machines that are easy to fix.    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Positive
AMD,"So build quality will be subjective, from what I can tell, it's got very good build quality in terms of ""real"" factors such as durability. But it definitely feels less ""premium"" than similarly priced consumer grade laptops. The plastic is plastic so it will flex a little bit, but the parts all seem very well put together and it does feel ""solid"" overall.   I haven't really noticed keyboard flex, but I have noticed a slight amount of flex where my palms rest, particularly on the right side, where the smart card reader is, which makes sense as it is just a big hole in the side of the laptop. I plan on getting a dummy smart card to fill the gap and hopefully that should reduce it.   Overall whilst the internal chassis is metal, the outside is just plastic. I imagine that is good for durability, as it ought to be able to absorb shocks, but, as I said, it definitely makes it feel less ""premium"". They key press feel of the keyboard definitely does feel very nice as far as laptops go though. Obviously it's still nothing compared to a good mechanical keyboard but for a laptop it's very nice.   I bought this laptop for longevity and durability, so given that It's only just come out, I can't really say much about that, but the prestige of thinkpads of previous generations kind of speaks to their reliability. Plus it's apparent that they are still quite easy to repair and Lenovo has video guides on replacing loads of the parts.   And for your use case the battery life should be very good. It seems the Intel chip was designed to be very efficient during periods of downtime and something like viewing a PDF or editing a document has a LOT of downtime for the CPU",Positive
AMD,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Neutral
AMD,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Negative
AMD,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Neutral
AMD,All the more reason to make it upgradable,Positive
AMD,Lunar Lake isn’t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Neutral
AMD,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Negative
AMD,This was back when the 14th generation were having issues.,Neutral
AMD,"It's $2,000 so no excuse.",Negative
AMD,Thank you so much for this valuable and comprehensive information! I really appreciate it:),Positive
AMD,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Positive
AMD,"Lunar Lake already beat AMD, nobody buys AMD laptops",Neutral
AMD,i stopped at $2100 for a Thinkpad T14,Neutral
AMD,"Ah okay, got it thanks.",Positive
AMD,wrong  Nobody Supply AMD laptop     There fixed for u,Negative
AMD,"I do, and many of the people I know do.",Neutral
AMD,Nobody pays that much.,Neutral
AMD,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Neutral
AMD,Try the shunt mod,Neutral
AMD,"Cool, errr...  icy",Neutral
AMD,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Neutral
AMD,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Positive
AMD,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Neutral
AMD,did you use dry ice? how did you hit sub-ambient?,Neutral
AMD,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Neutral
AMD,Are you in the US? If so how were you able to get Maxsun?,Neutral
AMD,Oh... for sure 😁,Positive
AMD,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Positive
AMD,Great work dude! Only 200MHz to go 😉,Positive
AMD,Car coolant in the freezer 😁,Neutral
AMD,That's the way! Let us all know the results.,Positive
AMD,I am in Australia.,Neutral
AMD,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Neutral
AMD,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Neutral
AMD,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Positive
AMD,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Neutral
AMD,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Neutral
AMD,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Negative
AMD,Oh! You put the car coolant to run through a freezer? Wow! Nice,Positive
AMD,And largely against the non-x3d lmfao.,Neutral
AMD,Aren't they just showing that AMDs CPUs are better for gaming?,Neutral
AMD,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Neutral
AMD,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Neutral
AMD,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Negative
AMD,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Neutral
AMD,I assume they compared with CPUs in a similar price range,Neutral
AMD,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Negative
AMD,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Positive
AMD,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Neutral
AMD,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Neutral
AMD,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Negative
AMD,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Positive
AMD,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Neutral
AMD,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Negative
AMD,"Now install windows 11, lol",Neutral
AMD,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Negative
AMD,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Neutral
AMD,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Neutral
AMD,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Neutral
AMD,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Positive
AMD,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Neutral
AMD,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Neutral
AMD,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Neutral
AMD,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Neutral
AMD,I did same performance on all processors.,Neutral
AMD,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Neutral
AMD,That sounds like an AMD Stan argument circa 2020,Neutral
AMD,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Neutral
AMD,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Negative
AMD,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Neutral
AMD,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Neutral
AMD,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Neutral
AMD,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Neutral
AMD,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Neutral
AMD,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Negative
AMD,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Neutral
AMD,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Neutral
AMD,Expand ?,Neutral
AMD,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Positive
AMD,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Negative
AMD,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Negative
AMD,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Neutral
AMD,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Negative
AMD,did you do it,Neutral
AMD,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Neutral
AMD,can you reset settings then choose ray tracing ultra preset.,Neutral
AMD,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Neutral
AMD,because they exclusively exist in DIY build your pc enthusiast bubble,Negative
AMD,Pricing was aggressive. A 12 core 3900x was 400 usd.,Neutral
AMD,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Neutral
AMD,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Neutral
AMD,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Negative
AMD,"Okay, I did it",Neutral
AMD,"No, I didn’t remember good",Positive
AMD,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Neutral
AMD,Thanks for solidifying opinion that your benchmarks are fake,Negative
AMD,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Neutral
AMD,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Neutral
AMD,"What’s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If you’re comparing score screenshots to OBS-recorded videos, you shouldn’t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. I’ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think they’d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since I’ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Negative
AMD,Cam someone confirm or is this gas lighting?,Neutral
AMD,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Positive
AMD,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Positive
AMD,Intel comeback real?,Neutral
AMD,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Neutral
AMD,3D v-cache has entered the chat.,Neutral
AMD,Take it as a grain of salt. Intel marketing LOL,Neutral
AMD,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Neutral
AMD,Thats cool ...but lets talk about better pricing.,Positive
AMD,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Neutral
AMD,Tech Jesus has entered chat :).,Neutral
AMD,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Neutral
AMD,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Positive
AMD,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Neutral
AMD,What do you mean by gaslighting in this case?,Neutral
AMD,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Neutral
AMD,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Neutral
AMD,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Negative
AMD,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Negative
AMD,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Negative
AMD,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Negative
AMD,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Neutral
AMD,Nova Lake bLLC about to ruin Amd X3D party.,Neutral
AMD,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Negative
AMD,I always wondered if Intel marketing budget is higher than the R&D budget,Neutral
AMD,Intel Arrow Lake is much cheaper than Amd Zen 5.,Neutral
AMD,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Negative
AMD,only an AMD fan would worry about replacing their shit CPUs under 3 years,Negative
AMD,Hardware unboxed isn't a reliable source.,Neutral
AMD,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Neutral
AMD,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Negative
AMD,Telling people that its performance is better than it actually is?,Negative
AMD,The ones with similar pricing not performance,Neutral
AMD,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Neutral
AMD,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Negative
AMD,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Positive
AMD,Quite common for AM4 in my experience.,Neutral
AMD,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Neutral
AMD,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Negative
AMD,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Positive
AMD,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Neutral
AMD,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Neutral
AMD,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Neutral
AMD,Sooo they are in the YouTube space for the money not for the love of tech,Neutral
AMD,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Negative
AMD,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Neutral
AMD,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Negative
AMD,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Negative
AMD,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Negative
AMD,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Negative
AMD,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Negative
AMD,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Neutral
AMD,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Neutral
AMD,Sure but charts seem about right to me,Positive
AMD,APO is game specific. I'm referring to what has changed overall.,Neutral
AMD,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Negative
AMD,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Negative
AMD,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Negative
AMD,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Negative
AMD,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Negative
AMD,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Negative
AMD,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Neutral
AMD,"Hi, I’m at my wit's end with my build and would really appreciate some advice.  My PC has been plagued by random crashes, CRC errors, and installation failures for months:  - Random application crashes, often citing `KERNELBASE.dll`, `ntdll.dll`, or `ucrtbase.dll`. - Frequent CRC errors when extracting large ZIP or RAR files. Retrying usually works. - Software installations fail with data corruption or unpacking errors, only to succeed when I try again. - Games crash or stutter randomly. - Very rare BSODs.  Specs:  - **CPU**: Intel Core i9-13900K - **Motherboard**: ASUS ROG STRIX Z790-H GAMING WIFI - **RAM**: 32GB (2x16GB) G.Skill Trident Z5 RGB (DDR5-6400 CL32, Model: F5-6400J3239G16GX) - **GPU**: MSI RTX 4090 Gaming X Trio - **Storage (OS Drive)**: Crucial P5 Plus 2TB NVMe SSD - **OS**: Windows 11   This is what I've already tried (everything passed):  *  **Memtest86:** Completed multiple passes with no errors. *  **Prime95 & Intel XTU:** Seems to be stable. *  **FurMark:** GPU stress test is stable. *  **Storage Health:** All drives pass SMART and manufacturer-specific self-tests. *  **System Integrity:** `sfc /scannow` and `DISM /RestoreHealth` complete successfully. *  **Updates:** All drivers, firmware, BIOS, and Windows are fully up-to-date. *  **Physical:** Cleaned the case, re-seated components, and replaced thermal paste. * **XMP**: I've disabled/re-enabled XMP multiple times, doesn't make a difference.  I have trouble finding a consistent way of reproducing the issue. Today I tried a 7-Zip benchmark which failed once with a ""decoding error"", but I wasn't able to reproduce it afterwards. I couldn't get Intel Processor Diagnostic Tool to fail after multiple hours. So...  *   **Is this a memory instability issue?** Could the RAM be faulty?  *   **Or, could this be a faulty CPU core?** I found [this post](https://www.reddit.com/r/intel/comments/15mflva/tech_support13900k_problems_when_using_multiple/) where a user had identical symptoms (CRC/7-Zip errors) that were only resolved by replacing a faulty 13900K, even with XMP off.  Thanks in advance for any help.",Negative
AMD,"# Alienware 34 - AW3425DWM resolution issues  [](https://www.reddit.com/r/ultrawidemasterrace/?f=flair_name%3A%22Tech%20Support%22)  I just got the AW3425DWM, and my laptop is a Dell Inspiron 15 5510, which is not a gaming laptop. I'm not a gamer.  When connecting through the HDMI port on the laptop and monitor, I can't set the resolution to 3440x1440; I can go up to 3840x2160, but not the monitor's native resolution. However, when I connect through the USB-C port on the laptop to the DisplayPort on the monitor, I can set the resolution to 3440x1440 without any issues. The downside is that I've lost the only Thunderbolt port I have available.  Is there a workaround for this issue? If I use an HDMI to DisplayPort adapter, will I be able to set the resolution to 3440x1440?  I understand that the HDMI 1.4 port usually can handle 21:9 resolutions, but with the latest Intel drivers, it isn't giving me the option for 3440x1440",Negative
AMD,"Hi, can someone help me please? I'm trying to generate a video on AI playground after installing it but it just keeps loading for videos and images don't show up. That's my it it'll specs:   Processor: Intel(R) N95 (1.70 GHz) Installed RAM: 32.0 GB (31.7 GB usable) System Type: 64-bit operating system, x64-based processor Graphics Card: Intel(R) UHD Graphics   -That's a part of what's in the console:    No key found for setting negativePrompt. Stopping generation oa @ index-fOc02QH8.js:61 w @ index-fOc02QH8.js:61 await in w V @ index-fOc02QH8.js:22 pt @ index-fOc02QH8.js:61 await in pt V @ index-fOc02QH8.js:22 c @ index-fOc02QH8.js:61 await in c Il @ index-fOc02QH8.js:14 Sr @ index-fOc02QH8.js:14 n @ index-fOc02QH8.js:18 index-fOc02QH8.js:61 uploadImageName b99fb28ea4440a2f51ce53cd5c529554e5965b66f5f5a8506b9b973b66e754bd.png index-fOc02QH8.js:251 [comfyui-backend] got prompt (anonymous) @ index-fOc02QH8.js:251 (anonymous) @ VM5:2 emit @ VM4 sandbox_bundle:2 onMessage @ VM4 sandbox_bundle:2 index-fOc02QH8.js:61 updating image {id: '6eb88f1b-f433-4541-a421-40619ac9fdc2', imageUrl: 'data:image/svg+xml,%3C%2Fpath%3E%3C%2Fsvg%3E', state: 'generating', settings: {…}, dynamicSettings: Array(3)}       With: RuntimeError: UR error     (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:251 [comfyui-backend] Prompt executed in 116.86 seconds   (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:61 executing Object",Neutral
AMD,Will XeSS 3 and Intel multi framegen be available for Iris xe graphics igpus?,Neutral
AMD,"Putting together a 4k gaming 5090 machine, deciding between the 285k or 265k. Does the extra L3 cache of the 285k make any difference, or does the ~5ns less latency of the 265k make more of a difference?   I plan on a 2dimm board to push memory and OC slightly, but nothing crazy.",Neutral
AMD,"Hi all,  I ordered a contact frame from Thermal Grizzly because the temperature of my CPU wasn’t great, and I was tired of the fan noise.   I installed the contact frame in August, and everything was fine until two weeks ago, when the desktop suddenly stopped booting. The fans were spinning, but there was no POST, no debug LED — nothing. I thought the motherboard was dead.   However, when I removed the AIO head and tried to boot again, it worked!  Since then, I’ve been experiencing intermittent no-POST issues, especially after gaming sessions.   To get it to boot, I always have to adjust the screws on the AIO head. If it’s too tight: no POST. If it’s too loose: CPU temperatures are high. For example, in Hogwarts Legacy, I get around 60 FPS with an RTX 4090, while the CPU runs at around 18% usage @ 85°C. Which is not normal at all, as I used to run the game at around 100–110 FPS.  I tightened the screws on the contact frame using my thumb and index fingers.   I’m looking for advice from anyone who has encountered a similar issue, because at this point my only idea is to remove the contact frame and reinstall the original one.  For context, my setup is from November 2022, and I installed the contact frame in August 2025. I have never removed the CPU from the socket since the first installation.  Thank you all.",Negative
AMD,Anyone know why the performance of my 13700k is so much better when using a pre micro code bios on a z790 board? This even after applying the latest micro code update through windows,Positive
AMD,"My hp elitebook 830 g8 notebook, had some problems booting up and it shows blinking lights (caps key) and it keeps trying to boot up without success, but i after getting it repaired now it boots up but sometimes it still does the same thing and boots up after awhile  I was already using the latest versions of the BIOS and ME firmware, and trying to download and update them again did nothing. However, avter updating the BIOS (to the same version), it showed a message saying ""HP Sure Start detected that the Intel Management Engine Firmware is corrupted"", but it only did so once.  What it does consistently is it restarts once or twice when i boot it. It shows the logo, then turns off, and then boots normally.  In the BIOS, it shows in system information that ""ME Firmware Mode: Recovery Mode"".  Windows also takes much longer to boot than usual, can take up to a full minute.  I can recall that the whole system was super slow at some point, but that was before I started diagnosing any of these details. It works fine now after boot.  Tried all sorts of things, installing ME drivers doesnt seem to do anything, and IDK what to do now.  help",Negative
AMD,"My processor with integrated 11th Generation Intel graphics (Intel Core i3-1115G4) completely lost Vulkan support after installing the latest version, 7080 (coming directly from the previously installed version, 6987). I'm testing the drivers one by one, but it looks like Vulkan support has been completely removed. Did this only happen with my processor, or with all 11th Generation processors?",Negative
AMD,"I'm on Win11 with Killer WiFi 7 BE1750x 320MHz Wireless Network Adapter. PC keeps crashing/restarting. Here is an error:  The computer has rebooted from a bugcheck.  The bugcheck was: 0x00020001 (0x0000000000000011, 0x0000000000210720, 0x0000000000001005, 0xffffe700010059a0). A dump was saved in: C:\\WINDOWS\\Minidump\\121225-18625-01.dmp. Report Id: 189a9fba-5969-4b6c-8199-d8b6a03a1a34.  Copilot had me disable all Killer services except Killer Network Service, but the issue persists. Now it tells me to uninstall the drivers and block them from reinstalling and just use the generic drivers.",Negative
AMD,"I recently bought a 2025 LG Gram 17 (Intel 258V + 140V). Great laptop for productivity tasks which is its primary use for me. I might occasionally game on it if I'm traveling. I only really play Final Fantasy XI these days which was released 2003 so this processor has no problem running it at max settings and \~60 FPS even in Silent mode.  That's when it is plugged in though.  As soon as it is unplugged the performance drops like 80%. I don't mind the performance hit when I'm doing productivity tasks, but would like control over it when I want.  I've tried various things to improve this while on battery:  * Making sure it is designated as ""High Performance"" in Windows. * Making sure Advanced Power settings are the same between plugged-in and battery. * Making sure in Intel Graphics Command Center that Display Power Savings are off. * Making sure in My gram that performance is set to high. * Changing anything anywhere I can find that might be limiting performance on battery.  I haven't started digging through the BIOS yet and don't know that I will. At that point it is too much of a hassle as opposed to some quick toggle(s) in the OS.  Any other places I should be checking or suggestions to address this related to the CPU itself?",Positive
AMD,"Hey everyone, looking for second opinions because this behavior doesn’t seem normal.  Specs:  CPU: Intel i5-14600KF  Cooler: Scythe Fuma 2 (dual tower, dual fan) with oficial LGA 1700 mounting kit.  Motherboard: ASUS TUF Gaming B760M-PLUS WiFi II  Case airflow is fine.  Darkflash DLX case with 9 fans.  Ambient temp \~25–30°C (Brazil)  Before this build I had a Ryzen 7 5800X on an ASUS TUF board with the same case and airflow, and temps were completely normal. No overheating issues like this.  but now, I'm facing the following:  Idle temps sit around 60–75°C  Any moderate load (opening anything, even the bios) causes instant spikes to 95–100°C. When acessing the Bios, the temp shown is 78-88°C  CPU thermal throttles immediately  Heatsink stays barely warm, even when CPU reports 95–100°C  Fans ramp up correctly  I’ve tried:  Re-mounted the Scythe Fuma 2  3 times (check [https://imgur.com/a/ehBnxLn](https://imgur.com/a/ehBnxLn) for how the termal paste was when I remounted  the last time)  Re-mounted the CPU.  Checked power limits  Limited PL1/PL2 to 65W → temps drop but CPU becomes extremely slow, 1˜3GHZ, but still hitting 50-60ºC on idle and 80-90ºC on the rest, 88 on the bios as well.  Undervolted –0.06V, didn't solve.  Has anyone else seen this behavior with 14600KF + ASUS B760 TUF boards?  My friend (chatgpt) recommended a contact frame and said that it would fix it. What do you think about this?  Any input appreciated. Thanks!",Neutral
AMD,"This isn't tech support, but my thread got locked and the mods said to post it here.  My 14700k is starting to give instability symptoms, so I opened a ticket with Intel.  The next day, I got an email asking for more information and was told they would call yesterday if they hadn’t received it. I replied to the email with information and my order confirmation, but as of 4:00 yesterday, I didn’t get a reply.  So, I went into the ticketing system and didn’t see my reply. I added a new comment with the information from my email and received almost an immediate email from the support person granting the RMA.  He asked for shipping information and if I’d choose option 1 or 2. For option 2, he asked for my agreement to the process and my billing address, name on the credit card, and expiration. He said he’d call to get the credit card number.  I replied to the email and pasted the information in my ticket. Oddly enough, I didn’t see his second response in the ticket.  Is that really how this goes down? There’s not an order system where I can input my credit card information? I have to wait for a call from some random support person and give him my credit card number over the phone?",Negative
AMD,"Hi,  I’m trying to register my account for claiming master game key card, but when i put my phone number to complete registration it always “phone number unreachable”.   I already submitted the ticket for support, i worry about not get any replies because of xmast and new year holiday.   Can you help me to complete my registration, i just want to claim the game and play in peace.   I tried to look for solution but nothing have same problem with me ? Is there any possibilities that on weekend the system can’t use A2P SMS for Indonesia Region",Negative
AMD,"Intel ARC b580 apparently unable to run games in the snowdrop engine, but in such a way that it's almost impossible for benchmarkers/testers to catch.  I've been trying to run Avatar Frontiers of Pandora on my arc b580 for over a year now, and I keep having the same extremely unusual crash.  If I freshly reinstall the game, or change my operating system, or switch arc driver version, I can play the game normally, at regular framerates, with no issues, for around an hour. MAYBE two if I'm lucky.   The game then crashes, with seemingly no trigger. No thermal issues, no unusual resource usage, no memory leak, no framerate issues, no stuttering, not even a crash report. The game will just close as if I had closed it manually.   I don't crash in the same location every time, it crashes with different game settings, xess enabled/disabled, different monitors, different ram configurations. I can have the framerate capped or uncapped. GPU maxed out or not, power draw high or low. I can't consistently recreate the crash in the same way every time. The only certainty is that it WILL crash eventually.  After crashing once, It will then crash on game launch, every single time. The game opens, the epilepsy warning flashes up for a second or two, and then the game crashes in the same way, with no freeze, no crash report, just the game closing.  Since the game runs fine for an hour or so after I first install it, any benchmarker testing the gpu will likely never see the crash, because they install the game, run benchmarks, do their tests, and then close it before seeing anything wrong.  The following are fixes that I have tried, and have NOT worked.  * Updating graphics drivers (to both the latest absolute driver, and latest WHQL certified driver). * Downgrading graphics drivers (going all the way back to the first game ready drivers for the b580). * Updating windows (latest windows 10 and latest windows 11 versions). * Running a memory diagnostic * Enabling/disabling XMP. * Verifying integrity of game files. * Forcing the game to run in the Vulkan engine instead of snowdrop. * Performing a complete reinstall of the game. * Replacing one of the .dll files with a ""repaired"" one. (this apparently fixed a crash for some people). * Forcing the game to boot in dx11 instead of dx12.  The following are fixes that I have tried, and HAVE worked.  * Running the game on my AMD integrated graphics.  Obviously running the game on an igpu doesn't offer playable performance, but the game will run, and I've yet to find any crash, even leaving the game open for several hours.  Due to this issue, I'm on the verge of selling my b580 and replacing it with a different card from another manufacturer that hopefully won't have the same issue.  Has anyone had a similar issue and knows the fix? Although I suspect the issue has to be resolved with a driver update.",Negative
AMD,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Neutral
AMD,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Neutral
AMD,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Neutral
AMD,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Neutral
AMD,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Neutral
AMD,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Neutral
AMD,"u/vincococka ,Yes you can use that memory kit safely as long as you don’t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of Intel® Core™ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Neutral
AMD,"u/SuperV1234 If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor  need a replacement.",Neutral
AMD,"u/triptoasturias Before I share any recommendations, could you confirm if this is your exact system-[Inspiron 15 5510 Setup and Specifications | Dell](https://www.dell.com/support/manuals/en-my/inspiron-15-5510-laptop/inspiron-5510-setup-and-specifications/specifications-of-inspiron-15-5510?guid=guid-7c9f07ce-626e-44ca-be3a-a1fb036413f9&lang=en-us)? Also, may I know which driver version you’re using and where you downloaded it from—was it from Dell or the Intel Download Center?",Neutral
AMD,"u/triptoasturias Your concern is related to **port and bandwidth limitations**. Most Inspiron models only support **HDMI 1.4**, which is limited to 4K at 30Hz or 2560×1440 at 60Hz. Ultra-wide resolutions like 3440×1440 often aren’t exposed because they’re outside the standard HDMI 1.4 spec. **USB-C to DisplayPort**: This works because DisplayPort has much higher bandwidth and supports ultra-wide resolutions natively. HDMI-to-DisplayPort Adapter, Unfortunately, **passive adapters won’t work** because HDMI and DisplayPort use different signaling. You’d need an **active HDMI-to-DisplayPort converter**, but even then It will still be limited by the HDMI 1.4 bandwidth from your laptop. So, you likely **won’t get 3440×1440 at 144Hz,** maybe 3440×1440 at 30Hz or 50Hz at best.",Neutral
AMD,"u/mercurianbrat This spec can run **basic image generation workflows** (CPU mode or lightweight models), but **video generation and heavy diffusion models will struggle or fail** on this setup. AI Playground’s minimum requirements are currently tied to Intel Arc GPUs with 8GB or more of allocated VRAM.  Currently you can download the installer for discrete GPUs.  We will also publish an installer that will run on Intel Core Ultra-H with built-in Intel Arc GPU (please keep in mind that [Windows allocates half of the system RAM as VRAM](https://www.intel.com/content/www/us/en/support/articles/000020962/graphics.html) for integrated GPUs, so 16GB or more of system RAM are required)  and Intel Arc GPU discrete add in cards with 8GB or more of memory. AI Playground takes up 8GBs of  HDD/SDD requirements: 8GB w/o models,  \~50GB with all models installed.",Neutral
AMD,"u/mano109 As a general corporate policy, Intel Support does not comment on information about products that have not been released yet.  **Visit** our [Newsroom](https://newsroom.intel.com/) for the most recent announcements and news releases.",Neutral
AMD,"**265K all the way.** At 4K with a 5090, you're GPU-bound anyway. The 265K's lower memory latency beats the 285K's extra cache at that resolution, plus you save money for better RAM or cooling.",Neutral
AMD,"u/hus1030  The mounting pressure from your AIO cooler can directly affect whether the system successfully completes POST. When the cooler is tightened too much, it can cause the CPU or motherboard to bend slightly, which may lead to poor or lost contact between the CPU and the socket pins. This prevents the processor from initializing properly, resulting in a no-POST condition. Installing a contact frame changes the pressure distribution compared to the stock retention mechanism, so overtightening the AIO screws can amplify this issue. On the other hand, if the screws are too loose, the CPU temperatures will rise because the cooler is not making proper thermal contact. To avoid these problems, ensure the AIO screws are tightened evenly in a cross pattern and do not exceed the manufacturer’s torque specifications. If the issue persists, you may need to verify that the contact frame is correctly installed or temporarily revert to the original retention bracket to rule out pressure-related problems.",Neutral
AMD,"u/BudgetPractical8748   Intel Default Settings may impact system performance in certain workloads as compared to unlocked or overclocked settings.  As always, system performance is dependent on configuration and several other factors.",Neutral
AMD,Nope got cash back,Neutral
AMD,"u/Any_Information429 Your HP EliteBook 830 G8 is experiencing boot issues due to corrupted Intel Management Engine (ME) firmware, which is a critical low-level system component that manages hardware initialization. This corruption is causing the blinking caps lock light, multiple restart attempts before successful boot, and the extended Windows startup times you've been experiencing. The BIOS showing ""ME Firmware Mode: Recovery Mode"" confirms this diagnosis. Since these issues began after your recent repair, it's likely that the Management Engine chip connections were disturbed or the firmware became corrupted during the service process.  To resolve this, you need to perform a forced recovery of the ME firmware by downloading the specific firmware version for your EliteBook model from HP's support website-[HP EliteBook 830 G8 Notebook PC Software and Driver Downloads | HP® Support](https://support.hp.com/us-en/drivers/hp-elitebook-830-g8-notebook-pc/38216726) and using specialized recovery tools to reflash the Management Engine. You also have check BIOS settings to ensure proper ME configuration and temporarily disable fast boot to allow complete initialization. If the firmware recovery doesn't resolve the issue, this may indicate hardware-level damage to the ME controller that occurred during the previous repair, which would require professional chip-level service or potentially warranty coverage since the problem originated after authorized service work. The good news is that once the ME firmware is properly restored, your system should return to normal boot times and eliminate the restart cycles you're currently experiencing.  USB flash recovery method is definitely worth trying first - it's designed specifically for these types of firmware corruption issues and should get your laptop back to normal boot times without all those frustrating restarts. Check here: [Support Search Results | HP®️ Support](https://support.hp.com/us-en/search/videos?q=BIOS) BIOS Videos",Negative
AMD,"Hi @[Content\_Magician51](https://www.reddit.com/user/Content_Magician51/) Upon checking, there is a new driver version available which is 32.0.101.7082. You may try this and use DDU method to make sure that you performed a clean driver installation. Here are the links of the latest driver and the steps on how to perform DDU.  [Intel® 11th – 14th Gen Processor Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/864990/intel-11th-14th-gen-processor-graphics-windows.html)  [How to Use the Display Driver Uninstaller (DDU) to Uninstall a...](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)",Neutral
AMD,"u/MISINFORMEDDNA  I took a look at your crash error, and here's what's going on. That error code you're seeing (0x00020001) is actually what's called a ""hypervisor error,"" which basically means it's a problem with Windows' virtualization stuff rather than directly being caused by your WiFi drivers.  The real culprits are more likely things like memory issues, BIOS problems, or conflicts with virtualization features like Hyper-V. I'd suggest running a memory test first (just search for ""Windows Memory Diagnostic"" in your start menu), and if you have any virtual machines or Docker running, try shutting those down temporarily, we'll probably need to dig deeper into hardware or system-level issues to really fix this one.",Negative
AMD,"u/strumpystrudel So what you're experiencing is actually pretty normal behavior for your laptop when it's unplugged - that 80% performance drop is totally expected and here's why. When your laptop is plugged into the wall, your CPU can run at much higher power levels (probably around 28W or more), but when you switch to battery, it gets severely limited to maybe 8-15W to preserve battery life. This is especially true for ultrabooks like the Gram that prioritize being thin and light over raw performance. The thing is, a lot of this power management happens at the hardware level with Intel's built-in systems, which is why all those Windows power settings you tweaked aren't really making a difference - the CPU is basically ignoring them and doing its own thing to save battery.  Now, for a game like Final Fantasy XI, you should still be able to get it running decently on battery with some tweaks, but expecting that same smooth 60 FPS at max settings is probably unrealistic given the fundamental power constraints of ultrabook design. Most ultrabooks see this kind of 60-80% performance hit on battery for any sustained workload, so you're definitely not alone in this.   But honestly, this is just how these thin and light laptops are designed to work - they're amazing when plugged in, but they have to make compromises when running on battery to actually give you decent battery life.",Neutral
AMD,"u/Aggravating_Gap_203 I'd recommend running Intel's Processor Diagnostic Tool first to rule out any hardware defects with the CPU itself. Just download it from Intel's website, run the test, and let us know if it passes or fails. While you're at it, try loading your BIOS defaults and make sure your power settings are at Intel's recommended specs - PL1 should be around 125W and PL2 around 181W for your 14600KF-[Intel® Core™ i5 processor 14600KF](https://www.intel.com/content/www/us/en/products/sku/236778/intel-core-i5-processor-14600kf-24m-cache-up-to-5-30-ghz/specifications.html)  [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html)  Your friend is actually spot on about the contact frame recommendation. Before you buy one though, try remounting your cooler one more time - make sure you're tightening the screws in an X-pattern and that everything is perfectly aligned. Sometimes it just takes that perfect mount to get things working right. Let us know what the Intel diagnostic shows and we can go from there!",Neutral
AMD,Hi [RadioFr33Europe](https://www.reddit.com/user/RadioFr33Europe/) I sent a direct message to gather more details for me to review the case and check the status of your replacement request.,Neutral
AMD,"Hi [Designer-Let-7867](https://www.reddit.com/user/Designer-Let-7867/) For issues related to game bundles and how to claim it, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry/issues/error message during claiming.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Neutral
AMD,"u/earwig2000 Let me check this internally. From what I see, you’ve already tried a lot of steps to address the game crash issue. I’ll share an update here as soon as I have more details, and I might need to collect some info from you for further analysis.",Neutral
AMD,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Positive
AMD,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Neutral
AMD,"Hi, I've tested disabling turbo and I still experienced issues. I've created a support request, case number 06728608, please take a look ASAP.",Negative
AMD,"Actually, this is not the case. After spending hours on this issue, I've finally found the 31.0.101.4502\_A14 drive at Dell's website that is working without any problems with the 3440x1440. So your claim that HDMI 1.4 is limited to 2560x1440 is false. HDMI 1.4 can go further to 3840 x 2160 without any problems. It's unacceptable the lack of support for UWM from Intel graphics cards and drives. Many previous drivers let you choose 3440X1440 resolution. Why did Intel stop supporting this resolution in the last graphics drivers?",Negative
AMD,I was going with the 265K over the 9800X3D since the Intel stuff seems to get better 1% lows and smoother experience at 4k and above. But does DLSS change that? Does DLSS lowering the render resolution push the 9800X3D back into the lead?,Neutral
AMD,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Neutral
AMD,"u/SuperV1234 Hi, thanks for the update. I’ve reviewed case number and confirmed that it’s currently being handled by our **warranty team** for replacement. You should be receiving further instructions from them shortly.     [Guide to pack your faulty CPU](https://www.intel.com/content/www/us/en/content-details/841997/guide-to-pack-your-faulty-cpu.html)",Neutral
AMD,"u/triptoasturias this explains, The generic Intel® driver provides users with the latest and greatest feature enhancements and bug fixes that computer manufacturers (OEMs) might not have customized yet. OEM drivers are handpicked and include customized features and solutions to platform-specific needs. Installing Intel generic graphics driver will overwrite your handpicked OEM graphics driver (in your case Dell driver). Users can check for matching OEM driver versions at OEM websites. For more information on how the installation of this driver may impact your OEM customizations, see this [article.](https://www.intel.com/content/www/us/en/support/articles/000096252/graphics.html)",Neutral
AMD,Unfortunately my Dell warranty support has ended and so far the forum there has not been able to help either. The removal tool works but Killer just keeps coming back.,Negative
AMD,"I also went to Dell, typed in my service tag # and cant find any other ethernet drivers.",Negative
AMD,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Neutral
AMD,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Positive
AMD,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Negative
AMD,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Negative
AMD,"i think it's bad for us, consumers",Negative
AMD,Was the team up really to crush AMD or Nvidia's answer to enter China?,Neutral
AMD,AMDware unboxed only cares about AMD anyway,Neutral
AMD,This hurts the arc division way more than this could ever hurt amd.,Neutral
AMD,They will crush user's wallet,Neutral
AMD,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Neutral
AMD,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Positive
AMD,Remember Kaby Lake G? No? This will also be forgotten soon.,Negative
AMD,Yes.,Neutral
AMD,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Neutral
AMD,Foveros baby!,Neutral
AMD,AMDUnboxed on suicide watch.,Neutral
AMD,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Neutral
AMD,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Positive
AMD,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Negative
AMD,Ohh noooerrrrrrrrr,Neutral
AMD,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Neutral
AMD,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Neutral
AMD,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Neutral
AMD,welcome to the Nvidia and amd duopoly,Neutral
AMD,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Positive
AMD,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Neutral
AMD,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Neutral
AMD,a partnership doesnt mean they get free reign over license lol,Neutral
AMD,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Neutral
AMD,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Neutral
AMD,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Negative
AMD,Why would it?,Neutral
AMD,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Neutral
AMD,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Negative
AMD,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Neutral
AMD,Past != Future,Neutral
AMD,"nvidia also used to make motherboard chipset, with mixed success.",Neutral
AMD,FSR 4 looks like the later versions of Dlss 2 did,Neutral
AMD,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Positive
AMD,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Negative
AMD,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Neutral
AMD,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Neutral
AMD,Never said that.,Negative
AMD,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Neutral
AMD,Why do so many people think that this will kill ARC?,Negative
AMD,The market for Arc is the same as for Nvidia.,Neutral
AMD,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Positive
AMD,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Positive
AMD,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Positive
AMD,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Neutral
AMD,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Negative
AMD,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Negative
AMD,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Negative
AMD,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Neutral
AMD,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Negative
AMD,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Negative
AMD,Nvidia does not have an A310 competitor.,Neutral
AMD,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Neutral
AMD,I have not lied,Neutral
AMD,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Neutral
AMD,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Negative
AMD,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Negative
AMD,Intel doesn't have a current gen A310 competitor either.,Neutral
AMD,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Negative
AMD,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Positive
AMD,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Neutral
AMD,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Positive
AMD,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Negative
AMD,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Positive
AMD,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Negative
AMD,The later versions of Dlss 2 look like Dlss 3,Neutral
AMD,Nvidia probably feels the same about their low end SKUs.,Neutral
AMD,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Neutral
AMD,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Negative
AMD,Yeah lol,Neutral
AMD,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Neutral
AMD,"""Everyone I don't like is biased""-ass answer",Negative
AMD,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Neutral
AMD,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Negative
AMD,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Neutral
AMD,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Neutral
AMD,Just hodl until you get the biscuits,Neutral
AMD,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Negative
AMD,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Neutral
AMD,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Neutral
AMD,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Neutral
AMD,I thought Arrow Lake refresh was in the cards for 2025.,Neutral
AMD,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Neutral
AMD,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Negative
AMD,This entire thing is a mobile roadmap so why are you here?,Negative
AMD,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Neutral
AMD,"Yeah if it's Surface roadmap, it's a nothing burger.",Negative
AMD,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Neutral
AMD,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Neutral
AMD,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Neutral
AMD,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Neutral
AMD,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Neutral
AMD,"I like how they just throw random words around to pad their ""article"".",Neutral
AMD,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Neutral
AMD,"""leaks""",Neutral
AMD,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Negative
AMD,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Negative
AMD,Scared of their rumor?  Lets release our rumor!,Neutral
AMD,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Negative
AMD,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Neutral
AMD,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Negative
AMD,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Neutral
AMD,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Negative
AMD,bLLC is not stacked cache,Neutral
AMD,What do you consider random? The article was perfectly clear.,Neutral
AMD,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Neutral
AMD,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Neutral
AMD,"Isn't the latency similar? The advantage of the 3d cache is an even larger cache size, while the advantage of what intel is going to do with bllc is that there is less of a thermal issue so the clocks can be higher.",Neutral
AMD,Probably only on the skus with less cores.,Neutral
AMD,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Neutral
AMD,Yeah. Definitely just you,Neutral
AMD,You could literally make that claim with any CPU performance increase.,Neutral
AMD,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Neutral
AMD,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Neutral
AMD,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Neutral
AMD,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Negative
AMD,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Neutral
AMD,"No, because it would be the same die. Just won't fit.",Negative
AMD,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Negative
AMD,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Neutral
AMD,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Negative
AMD,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Neutral
AMD,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Neutral
AMD,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Negative
AMD,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Negative
AMD,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Negative
AMD,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Negative
AMD,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Neutral
AMD,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Positive
AMD,It's not sorcery. Its just Intel doing the game developers work.,Neutral
AMD,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Negative
AMD,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Negative
AMD,Never count out Intel. They have some very talented people over there.,Positive
AMD,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Neutral
AMD,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Negative
AMD,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Neutral
AMD,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Neutral
AMD,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Negative
AMD,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Positive
AMD,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Neutral
AMD,I will follow all!,Positive
AMD,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Positive
AMD,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Negative
AMD,How is this doing the game developers work?,Neutral
AMD,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Negative
AMD,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Negative
AMD,The intel software team is pure black magic when they allowed to work on crack.,Neutral
AMD,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Neutral
AMD,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Negative
AMD,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Positive
AMD,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Neutral
AMD,Balanced in full load will just do the same thing as High Performance.,Neutral
AMD,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Neutral
AMD,Or... Just process lasso.,Neutral
AMD,Because it’s optimizations on how it can efficiently use the cpu.,Neutral
AMD,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Neutral
AMD,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Positive
AMD,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Neutral
AMD,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Neutral
AMD,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Neutral
AMD,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Positive
AMD,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Positive
AMD,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Neutral
AMD,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Negative
AMD,You need to download the Intel Application Optimization app from the Windows store,Neutral
AMD,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Positive
AMD,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Positive
AMD,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Positive
AMD,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Neutral
AMD,Where to download this APO,Neutral
AMD,Except its THE game devs job to optimize games for multiple cpus and gpus.,Neutral
AMD,It’s literally their job to do so? wtf you talking about?,Negative
AMD,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Neutral
AMD,"Will do, I got a 265K. Performance is already great tbh.",Positive
AMD,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Neutral
AMD,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Neutral
AMD,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Neutral
AMD,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Negative
AMD,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Negative
AMD,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Negative
AMD,That's not simply what APO does.,Neutral
AMD,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Neutral
AMD,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Neutral
AMD,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Neutral
AMD,i am not talking about older games. i am talking about newer games.... really dude?,Neutral
AMD,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Neutral
AMD,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Neutral
AMD,Thanks a lot.,Positive
AMD,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Negative
AMD,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Neutral
AMD,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Negative
AMD,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Negative
AMD,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Negative
AMD,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Neutral
AMD,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Positive
AMD,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Positive
AMD,Did the DP4A version also improve from 1.3 to 2.0?,Neutral
AMD,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Neutral
AMD,"i don't get the performance i got with FSR, but damn, FSR looks like crap in every aspect, i got less performance with xess in comparison, but at least got better framerats than native and get a better image quality",Negative
AMD,Okay but why would I want to use that instead of NVIDIA DLSS?,Neutral
AMD,It’s the least you should get after not getting FSR4.,Negative
AMD,You'd use it over FSR if that's available too?,Neutral
AMD,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Neutral
AMD,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Negative
AMD,And they expect people who bought previous RDNA to buy more RDNA,Neutral
AMD,Not by a significant amount.,Neutral
AMD,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Neutral
AMD,for the games that dont support DLSS,Neutral
AMD,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Negative
AMD,10 series cards will benefit from this,Positive
AMD,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Positive
AMD,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Positive
AMD,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Positive
AMD,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Neutral
AMD,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Negative
AMD,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Neutral
AMD,DP4a is cross vendor.   XMX is Arc only.,Neutral
AMD,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Neutral
AMD,1080ti heard no bell,Neutral
AMD,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Neutral
AMD,where did amd touch you bud?,Neutral
AMD,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Neutral
AMD,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Neutral
AMD,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Neutral
AMD,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Positive
AMD,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Negative
AMD,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Negative
AMD,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Neutral
AMD,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Negative
AMD,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Negative
AMD,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Negative
AMD,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Neutral
AMD,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Neutral
AMD,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Negative
AMD,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Neutral
AMD,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Neutral
AMD,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Neutral
AMD,Thank you :),Positive
AMD,Wouldn't the 300 series actually be Arrow Lake Refresh?,Neutral
AMD,"Videocardz is wrong      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Neutral
AMD,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Positive
AMD,This is pat's work,Neutral
AMD,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Neutral
AMD,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Positive
AMD,Is Intel finally making a comeback with their cpus? I hope so,Positive
AMD,Large L3 cache without reducing latency will be fun to watch.,Neutral
AMD,the specs sure do shift a lot..,Neutral
AMD,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Positive
AMD,it's just a bunch of cores glued together - intel circa 2016 probably,Neutral
AMD,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Negative
AMD,Bout time,Neutral
AMD,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Negative
AMD,Will it be available in fall of this year or 26Q1?,Neutral
AMD,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Negative
AMD,Noob question:  Is this their 16th gen chips?,Neutral
AMD,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Neutral
AMD,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Neutral
AMD,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Neutral
AMD,"What is it with Intel and 8 ""pcores""  how about just do 24 cores 48 threads",Negative
AMD,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Positive
AMD,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Positive
AMD,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Neutral
AMD,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Negative
AMD,"So alder lake on cache steroids ?  If the heavier core count chips don’t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Neutral
AMD,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Neutral
AMD,"""E-Cores"",  Ewww, Gross.",Neutral
AMD,"**Hi,**  I’m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPU’s clearly starting to bottleneck a bit, but it’s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Neutral
AMD,NVL will definitely be the 400 series. PTL is 300.,Neutral
AMD,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Neutral
AMD,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Neutral
AMD,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Neutral
AMD,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Neutral
AMD,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Neutral
AMD,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Positive
AMD,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Negative
AMD,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Neutral
AMD,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Negative
AMD,More like *despite* him.,Neutral
AMD,All the SKUs rumored so far are BLLC,Neutral
AMD,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Neutral
AMD,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Negative
AMD,Why not 8 P cores with HT + even more cache and no e cores at all,Neutral
AMD,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Positive
AMD,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Neutral
AMD,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Neutral
AMD,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Negative
AMD,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Neutral
AMD,Nova lake? More like 26Q4.,Neutral
AMD,Only Pantherlake for mobile,Neutral
AMD,"It doesn’t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Neutral
AMD,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Neutral
AMD,It is really happening. The only question is when? Or can they release it on the next year without delay?,Neutral
AMD,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Neutral
AMD,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Neutral
AMD,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Neutral
AMD,"E cores are the future, P cores days are numbered.",Neutral
AMD,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Negative
AMD,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Neutral
AMD,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.     You can definitely wait a bit.",Negative
AMD,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Neutral
AMD,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Neutral
AMD,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Negative
AMD,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Neutral
AMD,The former ceo,Neutral
AMD,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Neutral
AMD,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Positive
AMD,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. You’re using dated info.,Negative
AMD,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Positive
AMD,No different to 12-14th gen then.,Neutral
AMD,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Neutral
AMD,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Neutral
AMD,E core is 30% faster than hyper threading,Neutral
AMD,HT is worse than E cores,Negative
AMD,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Neutral
AMD,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Neutral
AMD,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Neutral
AMD,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Negative
AMD,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Neutral
AMD,ARM chips regularly do this sometimes on a yearly basis.,Neutral
AMD,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Neutral
AMD,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Neutral
AMD,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Neutral
AMD,"> They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Neutral
AMD,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Neutral
AMD,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Neutral
AMD,"Lmao, some of my games still runs on e cores",Neutral
AMD,People are still disabling e cores for more performance.,Neutral
AMD,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Neutral
AMD,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Neutral
AMD,fym no different they're 50% faster,Negative
AMD,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Negative
AMD,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Neutral
AMD,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Neutral
AMD,"As a advice, Nova Lake will further increase memory latency.",Neutral
AMD,In which gen iteration?,Neutral
AMD,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Neutral
AMD,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Negative
AMD,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Neutral
AMD,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Neutral
AMD,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Neutral
AMD,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Neutral
AMD,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Neutral
AMD,They don't pay attention,Neutral
AMD,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Neutral
AMD,That is more so bc the 9950x3d isn’t really a 16 core cpu it is two 8 cores,Neutral
AMD,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Neutral
AMD,How do you think they're doing 2 compute tiles if the memory controller is on one?,Neutral
AMD,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Neutral
AMD,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Positive
AMD,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Neutral
AMD,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Neutral
AMD,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Neutral
AMD,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Neutral
AMD,That didn't stop Intel with N3B,Neutral
AMD,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Neutral
AMD,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Neutral
AMD,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Positive
AMD,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Neutral
AMD,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix the problem. It doesn't happen very often but when it does it's very annoying.",Negative
AMD,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Neutral
AMD,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Neutral
AMD,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Neutral
AMD,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Neutral
AMD,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Negative
