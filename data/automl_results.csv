brand,text,sentiment
Intel,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? … but Xe3 plus will be enough to call it C series.",Neutral
Intel,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Positive
Intel,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,Neutral
Intel,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",Neutral
Intel,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",Positive
Intel,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,Neutral
Intel,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes Xe³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   Xe³ is a Xe³ based architecture like Xe³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. Xe³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",Neutral
Intel,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",Positive
Intel,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,Neutral
Intel,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,Neutral
Intel,That's not what Peterson was talking about context wise when he addressed this in the video.,Negative
Intel,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entry‑level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",Neutral
Intel,[AMD RDNA 3.5’s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),Neutral
Intel,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",Neutral
Intel,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",Neutral
Intel,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",Neutral
Intel,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",Neutral
Intel,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",Neutral
Intel,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",Positive
Intel,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",Positive
Intel,MLID must have an aneurysm seeing the guy still employed at Intel,Negative
Intel,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",Positive
Intel,Igpus not discrete gpus,Neutral
Intel,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",Negative
Intel,Hahaha I can hear Tom saying “fucking Tom Peterson has been *lying* to you” as you say that,Negative
Intel,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,Negative
Intel,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,Neutral
Intel,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,Neutral
Intel,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",Negative
Intel,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,Negative
Intel,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,Negative
Intel,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,Negative
Intel,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",Neutral
Intel,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",Negative
Intel,You stole what I was going to say... take my upvote.,Negative
Intel,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",Negative
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  One thing seems clear, this solution will be cheaper than standard AI data center products.  Hopefully, Intel can get and maintain footholds in medium-sized AI research market",Neutral
Intel,No way that power usage is at idle. Even if there was only one performance state like Tesla GPUs they wouldn't consume that much power.,Negative
Intel,that's insane vram density,Neutral
Intel,"This thing only has a 1Gb Ethernet port? I don't know much about the use case for this thing, but that seems surprisingly low. Simply from the use case of uploading training data to this I feel like something faster is necessary.",Negative
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Is there a fork of chrome that runs on gpus,Neutral
Intel,"Now do it with pro dual version of b770s, 64gb each. Could be a far more economical inference solution than what AMD is providing",Positive
Intel,but is that faster than a single 5090?,Neutral
Intel,Is this enough VRAM for modern gaming?,Neutral
Intel,Nvidia: ill commit s------e,Negative
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  If they rely on PCI-E for interconnect, bandwidth will be anemic between cards for training purposes. Might be ok for inferencing, but for training, you need the bandwidth.  That's not unclear at all.  This thing is probably not useful for anything, but serving many users with small models for inferencing.",Neutral
Intel,"You think Intel is going to maintain these footholds any longer than they would need to?  This push of their dedicated GPUs into the professional space, is just a mere attempt to get rid of the unsold inventory of ARC-GPUs at the highest possible price-tag right from the beginning – Dumping those into the enterprise- and datacenter-space, in noble hope that someone clueless might bite upon the Intel-brand, and then dip into it at high costs *at non-existing support software-support* Intel pretends to deliver some time in the future.  Since as soon as the bulk of it was sold, Intel will cut the whole division and call it a day, leave anyone hanging dry.  That's what's planned with the whole professional-line of them – Buying this, is a lost cause as a business.",Negative
Intel,"I think they've just taken the 400W per card TDP and multiplied it by 16 to get the 6400W figure, so I think they're trying to say minimum power draw at full load, not at idle. ""Minimum power draw"" is definitely really odd phrasing for that number. Surely you'd think they'd say ""Total GPU TDP"" or something less confusing",Neutral
Intel,I don't think servers are supposed to stay idle for long.,Negative
Intel,"> the 768 GB model uses five 2,700 W units (totaling 10,800 W), while the 384 GB version includes four 2,400 W units (7,200 W)    And an insane power consumption.",Neutral
Intel,You basically always put in dedicated NICs in those kind of servers. The onboard ethernet is rarely used.,Neutral
Intel,Could that make it very cost effective for any particular use cases?,Neutral
Intel,"Even the fancy NVIDIA interconnects are a bottleneck for training performance, as are the memory bandwidth and even L2 bandwidth. One architects the algorithm around that. A tighter bottleneck is obviously not ideal, but it might still make sense depending on the price of the hardware.  As long as a copy of the full model and optimization state fits on the 24GB bank the PCI-E interconnect isn't really too limiting for training. Small models, like a 1B parameters voice model, will naturally fit. The larger the model you want to train, the more complex the architecture might be, but there are ways to train even models over 100B parameters on such hardware, like routing distributed fine grained experts. Not that I think it's a good idea at this point.",Neutral
Intel,It's VideoCardZ so they probably got information from a tweet or Reddit post and copy/pasted without doing any thinking whatsoever.,Neutral
Intel,"I assumed that would be the case, I just didn't see any room for expansion. It's pretty crowded with 16 full sized cards.",Neutral
Intel,"I don't really see any, unless you can find a workload that will fit the card and multiply that workload for 32 users, but as each chip is performance wise less than a 3090, it has to be a fairly light workload.",Neutral
Intel,"1B might work for a voice model, but useful LLM models ARE 100GB+. Smaller models are fun little toys.",Positive
Intel,At least its a human hallucination and not AI hallucination.,Neutral
Intel,Can also be bad translation.,Negative
Intel,"according to the datasheet theres two free pcie 4.0 x16 slots on one version and two pcie 5.0 x8 slots on the other one. Both say ""support high speed NIC"" so I assume there is at least enough room for that",Neutral
Intel,"I wonder if these might make good controllers for ai powered robots. Something highly specialised, like a fruit sorter or something. I don’t know. I’m just hoping Intel can find a buyer because it’s good for the market to have more competition",Neutral
Intel,"It's a thing where the next generation of this card would be a viable competitor for the generative AI crowd, but not this one, where it can't compete with a 5 year old 3090.  For robotics, you just want inference. There are much better options focused on low power, small form factor NPUs custom made exactly for that, and they can be paired with small SBCs like a Raspberry Pi.",Neutral
Intel,No word on OMM so I guess that's reserved for Xe3-P. Wonder what other changes that architecture XE3-P will have.  The new Asynchronous RT Unit functionality in Xe3 is very cool and now Intel also has Dynamic VGPR. Neat.,Positive
Intel,"It's interesting they lump it in with B-series, given the architecture is pretty distinctly different from Xe2 used in Battlemage. Note that BMG/Celestial names have been explicitly used for dGPUs, however. They're not synonymous with Xe2/Xe3.",Neutral
Intel,"It would be nice if Intel released a small, budget friendly, passive cooled, PCIe powered dGPU with multiple QuickSync units, made specifically for AV1 encode and H.265 4:2:2 chroma with 10-bit depth.   It's unfortunate that one has to cough up $2,000+ (at least around here) for an RTX5090 to have three NVENC units at their disposal, and the 5090 still can't compete with a passive-cooled Apple Macbook that draws a couple of watts at peak.   You know things are crazy when Apple is the only company that caters to your specific needs!",Neutral
Intel,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"An ambitious plan, but I’m guessing its capabilities will still lag behind Nvidia and AMD cards.",Negative
Intel,Very bleak chance that Intel will continue with discrete gpu line up. Pretty shit decision tho,Negative
Intel,"So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In&nbsp;card under this precisely defined term.  Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – *In favour of Nvidia-GPUs?*",Negative
Intel,Xe3P needs that feature parity with RDNA 5 and RTX 60 overall since its a 2027 product.,Neutral
Intel,"hopefully that means NVL-AX is not getting axed. at least, the 24 core die. heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch. super weird, not believing too much into it. curious about Druid, people seem optimistic, idk why lol.",Neutral
Intel,yeah super weird -- but that does bode well for the next dgpu being based on Xe3,Neutral
Intel,"Arc Pro B50 was pretty interesting to me, it was a modern 75W card that isn’t gimped.",Positive
Intel,With a board with lots of PCIe x16 slots you can stack A310's or A380s and get along fine. I do 3 transcodes of av1 great on a single one right now.,Positive
Intel,The reason they'll never make this is the same reason they don't put SR-IOV on consumer cards.,Negative
Intel,Can you do this work on a $600 Mac mini? Or do you need something more expensive.,Neutral
Intel,They will if they have xe cores that gen. They'll only fully kill it if they abandon that core development entirely to exclusively use Nvidia graphics tiles,Negative
Intel,">Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  What in the world are you talking about. This roadmap says nothing of the sort.",Negative
Intel,"No, they are saying the GPU in Panther Lake is Xe3, but instead of classifying it as a new-gen architecture, they are saying it's just an extension of Xe2, so they've put it under the B series product family.",Neutral
Intel,100% but I fear this is another botched gen so it will at best be on par with current gen offerings.,Negative
Intel,"Intel buying chips from Nvidia won't be as cost efficient as them developing their own chips. Intel can't abandon GPU development in the short term or it'll be left behind in the long term (Nvidia can easily sell their Intel shares and pivot away from Intel leaving Intel with nothing).    If Intel will stick to making dGPUs and expand to competing in gaming with said GPUs they benefit greatly from the experience and talent that dGPU development yields, this makes dGPU development a no brainer. If AMD can afford to make dGPUs Intel definitely can as well.",Neutral
Intel,"> heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch  Would be very interesting if so, though I'd be surprised if projects get resurrected in this spending climate. Competitiveness might also be tight in '27.",Neutral
Intel,Celestial was based on Xe3p.,Neutral
Intel,"Then that can't just be a minor tweak, but perhaps it's just market segmentation.",Neutral
Intel,> They will if they have xe cores that gen  Why do you believe so? They made iGPUs without dGPUs before.,Neutral
Intel,"They won't Nvidia tiles are for specific market segments, they aren't a one size fits all solution.",Neutral
Intel,That's not what my colleague's say,Neutral
Intel,It wouldn't be a Helpdesk_Guy comment if it didn't include insane made up bullshit to disparage Intel.,Negative
Intel,"Exactly. *Good Lord are y'all shortsighted!* Are you falling for their shenanigans and typical tricks again?  Crucial is, what Intel does NOT mention – That is anything future dedicated graphics, as in *Add-In&nbsp;cards*.  They're purposefully declare PTL as de-facto dedicated GPU, yet any thing of future **d**GPUs is ABSENT.",Negative
Intel,"Congrats, you looked at the pictures. We all did. Now look at it *again* …  Now explain to me, why ARC alchemist is separated (since it's a line of dedicated graphics-cards, duh?!), while Battlemage gets PTL's X^e Graphics iGPU-tiles put alongside, despite such tiles classically does NOT count as a graphics-card but rather a iGPU (no matter how large).  The actual answer is, that this on the slide was surely NOT done slide by accident, but it figuratively puts PTL's X^e Graphics tiles of PTL's iGPU *on par* with their dedicated Battlemage-cards, which (at least in Intel's eyes) shall from now on signal a ""continuation"" of  Battlemage itself. Thus, there won't be any further dedicated graphics-cards like Celestial as classical Add-In cards, which still is way overdue anyway …  Since if not so, then WHY are PTL's iGPU-tiles labeled as ""Intel ARC B-Series""?! *You see the virtue signalling?*    You're also aware, that Intel recently dropped the Intel Iris&nbsp;Graphics theme and now calls all iGPUs ""ARC""?",Positive
Intel,"Care to elaborate where I was blatantly wrong and it didn't actually turned out mostly or even exactly as I predicted prior when popping the prospect of Xyz, often months to years in advance?  A single example would be sufficient here, mind you.",Negative
Intel,"They did not declare PTL as a de-facto dedicated GPU. By definition it's *not* a dedicated GPU and they never said it is.  They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs. You're trying to fill in missing info with a nonsense interpretation.   What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".",Negative
Intel,"So much buzzwords, yet it sounds like a stroke.  You need help.",Negative
Intel,"Alchemist had both desktop discrete and mobile discrete. Intel's roadmap isn't a 'graphics card only' chart, it's an IP roadmap. That's why Panther Lake's Xe3 iGPU shows up under Arc B-series, they are saying Xe3 is an extension of Xe2, where as Xe3P (Celestial) is the next Arc family.",Neutral
Intel,"> A single example would be sufficient here, mind you.  Sure, here you go:  > So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In card under this precisely defined term.  >Their non-show is basically code for: “We're canceling everything dedicated, and our eGPUs we consider now dGPUs.”  >So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – In favour of Nvidia-GPUs?",Neutral
Intel,"> They did not declare PTL as a de-facto dedicated GPU.  Except that's exactly what they did – look at the damn pictures my friend, Intel purposefully group it into the same category as Battlemage's dedicated GPUs and Add-In graphics-cards and thus, iGPU-tiles as on par as dGPU cards.  > By definition [e.g. Panther Lake's iGPU-tile] it's not a dedicated GPU and they never said it is.  \*Sigh I *know* this, well all do! Yet Intel wants to pretend, that PTL's on-die eGPU X^e Graphics tile is now basically declared, what *Intel* now considers a **d**GPU. The implication is strikingly obvious here.  > They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs.  Ex-act-ly! You really don't get it, don't you? They deliberately leave everything what *classically* would count as a dGPU open and without ANY path forward, meanwhile their X^e 3-tiles are now considered dGPUs by Intel.  > You're trying to fill in missing info with a nonsense interpretation.  No, I don't. I'm just one of those being able to decypher Intel's typically backhanded tricks and try to explain that they're virtue signaling the worst: ARC as a line of dedicated graphics-cards is dead and won't get a follow-up, instead, Intel now considers it replaced by iGPU X^e Graphics-tiles (like the one in Panther Lake).  > What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".  Their trickery here, that they basically announce the discontinuation of ARC graphics-cards through the back-door.",Negative
Intel,I wonder why they're using such naming. If Xe3 is just Xe2 enhanced then why not call it Xe2.5 or Xe2+. And if Xe3P is a new architecture why not call it the real Xe3?,Neutral
Intel,"Just wait and see. I sadly have a horrific on point streak to remain in the right with anything Intel, even if I call things months to years in advance.",Negative
Intel,"The implication here is incredibly obvious:  ""Intel Arc B-Series consistes of Battlemage discrete graphics *and* PTL iGPUs"".  That's the implication here. It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL",Neutral
Intel,"> It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL  Of course not, since it would instantly tank their reputation and will kill all further ARC-sales?!  Do you actually think, that Intel is that stupid to plain announce the death of ARC-series graphics-cards? Do you think they'd even sell a hundreds cards after such a announcement? Who would by a dead-end line of graphics-cards, which won't even get any further driver-support and still have disastrous drivers like Intel's ARC? NO-ONE!  Of course they virtue signal it through the backdoor here, to preserve the sell-off the of the rest of pile of shame of their B-series graphics-cards as long as possible (leaving users with none support afterwards), and *only then* pull the plug afterwards when they went through their inventory …  Since when are you following Intel? Intel has always done so, and only announced a sudden discontinuation once the inventory was sold (leaving customers in the open with no further support overnight), despite virtue signalling a continuation all the time (to preserve a proper sell-off).  ---- Wasn't this exactly the same story with **Optane** back then? Each and every question upon a future support was answered in the affirmative, only to pull the plug overnight again as usual …  I'm not another piracy of cons here, it's Intel pulling the same Optane-stunt with ARC – Happily selling off their cards, knowing darn well, that they will pull the plug, yet refuse to actually play with open cards again.",Negative
Intel,"> > It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL >  > Of course not  Great, we're all agreed that you're making shit up when you said  > Panther Lake now Intel classifies as a dGPU",Negative
Intel,nice to see that Intel is actually putting resources into fixing cpu overhead,Positive
Intel,"Fixed/improved in some cases, on a game-by-game basis.  Better than nothing, but far from actually *fixed* fixed. Steve says he'll be testing more games and resolutions shortly, but I'd also like to see direct comparisons between a pre-fix driver and the newest.",Neutral
Intel,"Huh, didn't expect that. Good job, Intel.  B580s mostly aren't super-worth buying atm, but this is a really good sign for future Intel products (assuming there will be any of course) as well as obviously good news for people who bought B580s.",Positive
Intel,"While it's great to see improvements, those still seem to be on a case by case basis.   The root cause is still there and if there are more powerful cards coming (big if), then the issue will shift up again.   But hey maybe Nvidia can start prioritising the overhead issue as well while we're at it.",Negative
Intel,How is idle power usage?,Neutral
Intel,"What I took away from the video: more or less fine with 5600 and up, still struggles with 2600. Intel is doing per game optimizations, so your mileage may vary. Space Marine is still struggling, Spider man 1 is running great, SpiderMan 2 does not.  Drivers are constantly improving, HUB has bright outlook. The bigger issue is 9060XT8 within 10-15% price range, but delivering 30% better perf.     Edit: A lot of people talking about how old or slow 2600 is in replies. And that's part of the point, I obviously didn't get across well. The issue is not the absolute performance, but the relative loss compared to AMD. In CPU bound scenarios they both should be close, since the limiting factor is CPU. But in this scenario 9060XT is even further ahead than when they are unconstrained.",Neutral
Intel,"Lol, just in time for the Panther Lake announcement. But credit where it's due.",Positive
Intel,"I thought people were pretty sure it was an entirely hardware problem that can't be alleviated with driver fixes.  Guess they were wrong somewhat. I think the claim that it can't be completely fixed still seems to stand, but it can be alleviated alright.",Negative
Intel,Mildly interesting B580 news! There seems to be some improvements to the CPU overhead problem,Positive
Intel,"Meanwhile Intel Vulkan drivers on Linux are absolute garbage, they provide less than 50% of the performance of what's available on windows. It's so bad, that usng WineD3D (DirectX to OpenGL) gives better performance than  DXVK (DirectX to Vulkan).",Negative
Intel,"TL;DR improved, but far from fixed, clickbait title.",Negative
Intel,"> Fixed/improved in some cases, on a game-by-game basis. Better than nothing, but far from actually fixed fixed.  :(",Negative
Intel,unrelated but whats up with the 1488 in your username,Negative
Intel,"I bought an Arc A380 for $100 when it first launched, have been comfy 1080p gaming ever since, graphics / software maturity and improvement has been amazing.  I can't wait to see the B770 (or whatever they decide to call it) and also Arc C580 based on Xe3-HPG (Celestial)",Positive
Intel,Intc is going to be huge,Positive
Intel,72 up upvotes on a duplicate post when someone posted it an hour earlier. WTH Reddit.,Negative
Intel,Could this have been a bug on Ryzen only? Did they ever test the 12400 or something like it in comparison to a Ryzen 5600?,Neutral
Intel,I'm noticing the 8gb card has better averages & lows than 12gb,Positive
Intel,which version is this,Neutral
Intel,Thats terrific from Intel.  Hopefully they can still improve on it and ofcourse continue releasing & developing GPUs. Maybe two generations down the line we would be considering Intel GPUs over AMD.,Positive
Intel,"I mean that’s ok, but for Spider-Man, which was the most detailed view, it looked more like the 9060 xt performed inconsistent with the 5600.  20% uplift on lower and higher quality CPUs. I would have expected there to been a consistency across all of them if this was the case.   I would still not even consider recommending a 570/580 to a friend over even the 8 gb models of nvidia and amd.",Neutral
Intel,"Intel is really turning things around, looking forward to what they have moving into the future",Positive
Intel,"I can't reccomend Arc to anyone, there's 0% chance the arc driver team still exists in 2 years.",Negative
Intel,"I still remember the day when amd took over ati and finally got to hd 7xxx series aka the first trully new microarch since the purchase and they launched the first multithreaded version of the drivers for the tahiti? One version of the hd 7850xt, then the 7950 and 7970 giving everyone basically a massive perf improvement, less frame to frame latency and like a two digit perf uplift... Sounds a lot like what intel is going through. Who knows maybe in 10 more years intel shows their 1080 or whatever ends up being called and is competing in the high tier. It's been impressive, considering 10 year ago hey were basically on worthless igps only good for quicksync h264 low quality encoding.",Positive
Intel,Should've used RTX 4060 instead. Previous tests also showed that AMD GPUs too can suffer from CPU overhead.,Negative
Intel,Intel: lets make amd cpu work slow with code-compiler tricks  Amd cpu: slow and make overhead in games but only for Intel gpu.  Intel: can't sell low-end gpu for high-end cpu  Intel: removes amd-cpu dampener.,Negative
Intel,"this is so sad,  because intel just officially ended arc :/  remember, that nvidia wouldn't have intel use nvidia graphics chiplets in apus, if arc wasn't dead dead.  so you can't suggest arc anymore, because intel sure as shit won't properly support it longterm at all.  and this SUCKS, because the b580 at least had the barest minimum vram, that works rightnow.  in a different timeline arc would still be cooking and the royal core project didn't get nuked by an idiot.  but well it is what it is.",Negative
Intel,"Probably Nvidia engineers helped them make better drivers, since Nvidia owns them",Positive
Intel,"a lot of people dog on them and spread fear mongering info about their GPU division and ARC support, but they've been doing quite well in terms of support. I got a Claw 8AI+ and they've been pushing updates with performance optimizations for mobile arc as well.",Positive
Intel,The CPU bottleneck didn't happen universally either. Some games others tested didn't suffer from it.,Negative
Intel,There is a direct comparison between driver versions in the video.,Neutral
Intel,"Remins me of the first Alchemist GPUs which were abysmally slow in older games and there were headlines of like ""new drivers bring 30% more performance!"" every few months an then it turned out it got that benefit in some DX9 game nobody play anymore than the rest barely improved...",Negative
Intel,"Once Intel figures out how to make QuickSync use the full potential of these cards, they’ll be unmatched for anyone that does video work.",Positive
Intel,They are worth if they're at MSRP. 12GB for $249 is great.,Positive
Intel,">still struggles with 2600  as someone who upgraded from Ryzen 3800X -> 5600X -> 5800X3D -> 9800X3D, I noticed all upgrades in CPU-heavy games even at 1440p (hello, MMORPGs or Escape from Tarkov) and if you're playing with Ryzen 2600 in 2025(almost 2026) you'll end up with mediocre experience even without considering overhead problem - I agree with your point, I just think that in 2025 Ryzen 2600 is an outdated CPU and considering AM4 upgradability, it should be changed to 5600/5600x3d/5700x3d/5800x3d to get a proper experience without being CPU-limited in some games.  [AMD Ryzen 7 9800X3D Review - The Best Gaming Processor - Game Tests 1080p / RTX 4090 | TechPowerUp](https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/18.html)  If we look at CPU charts at 1080p, Ryzen 2600 is not even on this list because of how bad it is.",Negative
Intel,"Since 2600 is Zen+, I would've been more interested to see where 3600/Zen 2 landed between the Zen+ and Zen 3 CPUs since my old 3600 is in my brother's PC as an upgrade from i5-4460 and his GTX 1060 is starting to fail (one of the vram chips has errors in nvidia mats after running mods so it sometimes runs fine in idle but under load it randomly freezes [can take seconds or an hour] and/or artifacts and then either TDR manages to reset the GPU or not and PC crashes) so he's also now rocking my old GTX 960.",Neutral
Intel,"Yeah, in EU the 9060XT is 20€ more, and for that you get:  * noticeably more perf. * more mature driver, game support, feature,... * no random issues like this (and several more)  IDK whether anyone would even consider the B580 here.",Neutral
Intel,Imagine buying a brand new current gen GPU in 2025 while still gaming on 7 year old Zen 1+ fabbed on that delicious Global Foundries 12nm.,Neutral
Intel,2600 is slower than haswell in games,Negative
Intel,At 1080p.   Now have HWU go test TLOU2 or any other memory intensive stress test they’ve done in the past with any other 8gb card.   B580 scales excellent up to 1440p.   The 8gb cards are going to fall flat on their face.,Neutral
Intel,Don't know about discrete but integrated graphics are fine on Linux with DXVK.  Played GTA IV with Tiger Lake Iris Xe using Proton and got 40-60 FPS at 1080p High settings with the random FPS drops I get in Windows completely gone.,Positive
Intel,Good thing the drivers are Open Source so The Community(TM) can improve them.,Positive
Intel,I’m guessing this is because of the dumpster fire that is Xe drivers,Negative
Intel,I'm Scottish and it's related to [Battle of Sauchieburn.](https://en.wikipedia.org/wiki/Battle_of_Sauchieburn),Neutral
Intel,"Share the post you're talking about, I shared this video in \~5 minutes timeline after it was released on YouTube, and your scenario is not realistic.",Negative
Intel,It's not a bug on Ryzen. Just performance scaling is easier to do on Ryzen.,Neutral
Intel,"Well, that's to be expected if a game doesn't actually require more than 8GB.  But there's also the second issue (at least in some games) that a game simply reduces texture quality etc. automatically without your consent if it would go above the VRAM limit otherwise or some textures actually take a lot longer to load in properly which is not reflected in the framerate comparison.",Negative
Intel,"He's using 1080p medium to lower the stress on the gpu and better reveal the driver problem.  Use a more demanding setting and maybe the vram becomes relevant, but that's a different video.",Neutral
Intel,See for yourself:  https://youtu.be/gfqGqj2bFj8?t=700  7028 and up seem to have that fix.,Neutral
Intel,"What do you mean in two generations? Arc at MSRP is a perfectly viable choice. It has pros and cons, but Arc is competitive.",Neutral
Intel,actually previous tests have shown amd to have less overhead than nvidia [https://youtu.be/JLEIJhunaW8](https://youtu.be/JLEIJhunaW8),Neutral
Intel,"The change happened in August with the 7028 version driver release, odds are they've been working on this for months.  Intel has really good engineers too. Nvidia has nothing to do with this.",Positive
Intel,"I really hope you’re joking here. You know an announcement of future products doesn’t automagically mean what you wrote, right?",Negative
Intel,MLID is a clown,Negative
Intel,8AI+ is an underrated handheld.,Positive
Intel,intel *should* be dominating the handheld space.  their mobile cpus are plenty good and their strategy of offloading gpu work to the cpu makes tons of sense on an igpu.  honestly i was surprised the 1st gen claw wasn't competitive.,Positive
Intel,"But it still does suffer in many games in the video.  The video title is misleading. This should be celebrated, but not as a ""fix"".",Negative
Intel,For one game. I'd like to see which games improved and by how much.,Neutral
Intel,"I mean, that's how drivers mature.  Some game saw 30% boost, but everything else using that code maybe saw a 1% boost.  And then you do the same for another game and so on until you've covered the weird cases and everything else gets a stack of little 1% boosts that collectively add up to their own big boost.  It's a bunch of work and doesn't happen overnight.",Neutral
Intel,I think that was just them bolting the open source dxvk into their driver.,Neutral
Intel,"> abysmally slow in older games  This is still the case in some games. One that Ive had first hand experience with is Resident Evil Revelations 2.  For some reason theres a shader caching issue where for the first 3ish minutes on every level its single digit frame rates. After it ""loads"" everything its perfectly normal.    Never had that issue on my older, far less powerful, system.",Negative
Intel,"They're already the fastest card for AV1 and other codecs outside of industry-made FPGA/ASICs for television studios. Primere Pro, DaVinci, and other editing software have to support it.",Positive
Intel,"Maybe it's region-dependent, but where I live the 8Gb 9060XT is 10% more but it's a \*lot\* faster.  I don't think either of those represents great value. One is crippled because of the VRAM, the other one is just slow for the asking price.  IMO the 9060XT 16Gb is the cheapest GPU genuinely worth buying right now. The B580 and the nVidia and AMD options closest to it have too many drawbacks.",Negative
Intel,"CPU is king if you're looking for a smooth and responsive system - sure, gpu well likely dictate average fps, but fps dips tend to be due to cpu and in the cases where they aren't, you can just adjust graphics settings.",Neutral
Intel,"Hardware Canucks made a whole video on the [5060ti vs 9060xt on multiple CPUs](https://youtu.be/NqRTVzk2PXs)  Even Nvidia and AMD cards to an extent suffer from notable performance losses particularly on more CPU reliant games like spiderman from the 2600's level of performance. The CPU is just too weak to run with midrange and higher cards in 2025. I remember my 2600x struggling to maintain 165fps in valorant of all games. The moment I moved to a 5600 it shot up to 300 fps, and this was just on an rtx 2060.",Negative
Intel,"> and if you're playing with Ryzen 2600 in 2025(almost 2026) you'll end up with mediocre experience  If you were playing CPU heavy games, it was a mediocre experience even back when it released.  Anyone who cared about CPU performance when it came to gaming. Didn't eve look at Zen before Zen 2. That's when they started getting some wins in ST heavy games thanks to the cache. But the double CCX layout still limited performance. Then with Zen 3 is when they finally reached parity with Comet Lake.",Negative
Intel,"I have a 5600x and plan to get the 5800x3D soon, did you notice much performance gains when you upgraded?",Neutral
Intel,>IDK whether anyone would even consider the B580 here.  Media and productivity tasks.  Quicksync is way more faster and more quality especially in AV1. More formats like HEVC 10-bit 4:2:2 are also supported.  Blender score is much higher for B580 than 9060XT.  And 12GB VRAM makes a difference compared to 8GB.,Positive
Intel,"If you have a limited budget, you would upgrade part of the system at a time and whatever limits you hit.  Not everybody have unlimited money glitch to upgrade everything all at once to the latest and greatest.  I prioritize my upgrades to CPU, so but had to upgrade my RX480 earlier this year when game (e.g. Indiana Jones) start requiring raytracing to run.  I upgraded my CPU after fing a good deal on 5800X and also due to Windows 11 won't official run on my 1700.",Neutral
Intel,"Entirely plausible, if you can't afford a rig overhaul but your GPU shits the bed.",Negative
Intel,It's a budget GPU; I'm not exactly sure why you think that's not a valid use case.,Negative
Intel,"It's slower than 6700k and 7700k at max boost frequency in many games but Haswell (core 4000 series) is a stretch. Maybe it's slower than a highly overclocked 4770k in specific titles that are highly frequency dependent. But not overall. There are enough modern games that benefit from 12 threads over 8, back when 2600 launched there definitely were not.",Neutral
Intel,Bullshit. Based on what?,Negative
Intel,">B580 scales excellent up to 1440p.   Despite its VRAM deficit, the 9060 XT 8 GB is 23% faster than the B580 at 1440p with maximum settings according to [TechPowerUp](https://tpucdn.com/review/powercolor-radeon-rx-9060-xt-reaper-8-gb/images/relative-performance-2560-1440.png).",Positive
Intel,"I have an Intel Arc A380, it's supposed to be more or less equal with Radeon RX 6400, but for example Guid Wars 2 runs at around 80-90 FPS on 6400 in some of the older zones (outside of Lion's Arch), but only 38-39 FPS on Arc A380. It's not a problem for me, because I didn't buy it for its Vulkan performance, I just needed a GPU that can run 3 monitors and ideally with hardware encoding and A380 is great for that. That's not only my observation, benchmarks on Phoronix show the same story.",Neutral
Intel,"> Played GTA IV with Tiger Lake Iris Xe using Proton and got 40-60 FPS at 1080p High settings with the random FPS drops I get in Windows completely gone.  Eh, a game made originally for the xbox360 over a deade and a half ago almost getting 60fps in 1080p is not the reassurance you think it is.",Neutral
Intel,"Tiger Lake uses i915, Battlemage uses the dumpster fire that is Xe to put it kindly",Neutral
Intel,https://www.reddit.com/r/hardware/comments/1nua9bm/huge_arc_b580_news_intel_fixes_cpu_overhead/   It's the second one that shows up under new(older).   Maybe it was hidden and a mod approved it?,Neutral
Intel,Is it noticeable by average person? like how switch 2 has bad display? I think more people care about the frame pacing. Wonder why steve doesn't include 0.1% lows,Negative
Intel,Sweet then haha,Positive
Intel,I want more performance.,Positive
Intel,I want more performance.,Positive
Intel,It's a Ryzen problem. On way an i3-10100 is supposed to match a R5 3600. They should've included an i5-10400.,Negative
Intel,That's an insult to clowns,Negative
Intel,">honestly i was surprised the 1st gen claw wasn't competitive.  You're probably the only one...  Meteor Lake was just bad, long before the Claw it was shown how much it trailed behind AMD apus in efficiency.  Lunar Lake on the other hand is great and a well balanced chip for a handheld, unfortunately it is also very expensive to make and not very high-volume (or at the very least Intel is prioritizing shipments to laptop OEMs).",Negative
Intel,It was previously assumed that overhead was a hardware issue and likely not fixable for this generation. Pretty big development I would say,Neutral
Intel,"I don't think so, no. [I'm mostly rewriting what I have written previously regarding this](https://old.reddit.com/r/hardware/comments/1kz272s/lisuan_unveils_g100_chinas_6_nm_gpu_targeting_rtx/mv7g3q0/), but from my personal experience of being on Arc for years, I suspect that only a small number of DX11 games were manually whitelisted to use DXVK.  DX9 performance gains came about from them implementing their old DX9 driver, instead of using D3D9on12 as it did on launch. Native Arc D3D9 has always behaved differently to installing DXVK for me, either being more or less broken.",Neutral
Intel,Progress is progress,Positive
Intel,"Exactly. Once they nail QuickSync on it, it’ll be a no-brainer.",Positive
Intel,"Yes, it's 100% worth it if you're playing at 1080/1440p, at 4K resolution it won't be a big deal.",Positive
Intel,"Yes the 5800X3D is awesome if you can find one for a good price. It's comparable to a Ryzen 7600 in games. Definitely saw a huge upgrade from my Ryzen 3600, even with a relatively low end GPU (I had a GTX 1080 at the time)",Positive
Intel,"Welp my comment was about gaming (as the video)  But even if you consider productivity and media, the Arc still got dunked on by Nvidia card.  * The 5050 (240€) is 20€ cheaper, similar gaming performance, 20% faster blender perf. * The 5060 (280€) is 20€ more, 20% more gaming perf, **more than 50%** perf in blender.  You also get the whole DLSS RTX shebangs, for media NVENC is at least equal if not better than QuickSync. The only thing the arc got going for it is the 12GB.  So yeah unless you absolutely needs the 12GB, this card will not be appealing at all at its price. For 200€ then maybe it might make sense I guess.  Source for blender perf [here](https://opendata.blender.org/benchmarks/query/?device_name=Intel%20Arc%20B580%20Graphics&device_name=NVIDIA%20GeForce%20RTX%205050&device_name=AMD%20Radeon%20RX%209060%20XT&device_name=NVIDIA%20GeForce%20RTX%204060&device_name=NVIDIA%20GeForce%20RTX%205060&blender_version=4.5.0&group_by=device_name)",Neutral
Intel,"Yeah, 8 gigs is an instant disqualification as an option, the price to longevity is not viable.",Negative
Intel,"> If you have a limited budget, you would upgrade part of the system at a time and whatever limits you hit. Not everybody have unlimited money glitch to upgrade everything all at once to the latest and greatest.  If you have a limited budget, (EDIT ADDED: And you're starting from scratch where you don't have a working system) it’s often best to save up instead, and do a complete build when you can afford it. You often get a lot more for your money that way.",Neutral
Intel,"Play the PC version if you can some time. It causes drops to less than 40 FPS on Windows with its DX9 renderer even on a modern, cheap GPU like the 6500 XT.  Drops which disappear when playing with Proton on Linux.",Negative
Intel,"GTA 4 is an infamously bad PC port, being able to run it well is still a challenge for modern systems. It's one of those games that will never run nicely no matter what hardware you give it because it's just fucked. There are certain settings that nuke fps for no perceivable benefit, even on extremely high end hardware.",Negative
Intel,"I'm pretty sure that i915 supports Arc dGPU as well. You need to use certain kernel flags, similar to how you need to disable nouveau for Nvidia, the details of which vary by distro.  [https://dgpu-docs.intel.com/devices/hardware-table.html](https://dgpu-docs.intel.com/devices/hardware-table.html)",Neutral
Intel,"Xe doesn't have poor performance, it's the userspace anv driver to blame. On Arc A series and meteor lake SoCs phoronix reported massive performance gains by switching from i915 to Xe.",Negative
Intel,"I don't know why, but yes, it was hidden/didn't exist back then and when I posted this video there were no other posts visible, which is proven by upvote ratio on that video you shared.  I think it's either mods/Reddit auto-flag system.",Negative
Intel,"Super noticeable when large textures take a million years to load in, or are constantly swapping from low to high resolution as they keep being evicted from and reloaded into VRAM. If they never load in the first place, I suppose some people might not realize the game shouldn't look like mud.",Negative
Intel,Isn't that exactly what the driver update is providing?,Neutral
Intel,"It's an nVidia problem, well-known for years, which exists for both Intel and AMD processors.",Neutral
Intel,"Intel has been putting in work on their drivers. They've been playing catch-up with AMD and Nvidias years of experience.    ...but lets not pretend they're making maracles here. Lots of these driver updates claim +200% improvements when in reality that takes a game from literally broken to ""playable""  I say this as an early adopter of an A770 and have been reading driver patch notes every single time.",Neutral
Intel,"Of course, and as I said, it should be celebrated.",Positive
Intel,"Great ok, thanks!",Positive
Intel,"Quicksync has less artifacts/blockiness in actual AV1 side-by-side comparisons. And encoding speed is faster. And it has two encoders.  Also 8GB is very bad for the price, you could have 8GB RX480 for $220 in 2016. Plenty of games have issues with 8GB already and it will only get worse.",Neutral
Intel,"Not really... If you only game it's worth upgrading the GPU more often, if you mostly work, the GPU can be upgraded every console cycle or less.  Unless you mess up with bad choices, e.g. buying the last 4 threads cpu ever made.",Neutral
Intel,"Unfortunately hardware encoding doesn't work on Xe on Alchemist GPUs, so I'm stuck with using i915, but honestly I haven't noticed any major difference in Vulkan performance between them. I think it's the ANV driver that is responsible for bad performance.",Negative
Intel,"I think they mean the range lacks a high end, or even a midrange.",Neutral
Intel,"Yes but I am specifically looking for a lot more performance, the B580/9060XT isn't enough performance for me.     I'm saying within 2 generations because i'm hoping within those 2 generations they'll be at an affordable price while being similar to a 5090 performance.",Neutral
Intel,Hardly anyone is encoding video with their PC's the few times they do it the CPU works just fine.  Buying a GPU to stream video no one is watching is daft.  Plex transcoding is fast enough on modern CPUs there really isn't a need for GPU transcoding unless you are streaming to the neighbourhood.,Neutral
Intel,"Why even encode to AV1 with quicksync? Streaming? I thought that wasn't supported by major platforms, and on most others it will be reencoded anyway.  For everything else you're going to get higher quality and compression with CPU encoding (svt-av1, x265, whatever)",Negative
Intel,If you're using it in places where quality matters you're using it wrong.,Negative
Intel,"I thought about it and you're right. I was thinking about it in the context of either not having any system, or having one so old that it's useless for your intended use case.. where you would buy parts every few months before finally having a complete system a year or two later. In that scenario, I'm 100% right. On the other hand, if you have a workable system, then sure, it makes sense to upgrade where needed, and in that context, you're the one that's right. I'll edit my comment.",Neutral
Intel,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,Neutral
Intel,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",Positive
Intel,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,Neutral
Intel,I do have to wonder how much we are missing out by these features being proprietary rather than having graphics vendors work with other stake holders and each other to make open cross compatible upscaling and frame generation techniques. It's been great for nvidia but bad for the ecosystem as a whole for everything to be so fractured.,Negative
Intel,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",Neutral
Intel,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",Positive
Intel,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,Neutral
Intel,we are witnessing the downfall of pc gaming in real time,Negative
Intel,Why should they? They are going to buy NVidia GPUs for everything now.,Neutral
Intel,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,This could be awesome more completion The better,Positive
Intel,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,Negative
Intel,"Multi-Post News Generation, with three articles interpolated per source.",Neutral
Intel,I'm excited for Intels new GPU,Positive
Intel,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",Negative
Intel,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",Neutral
Intel,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,Neutral
Intel,Competition *,Neutral
Intel,Obligatory article quoting reddit post quoting another article quoting original reddit post.,Neutral
Intel,"Fake frames, fake articles! /s",Negative
Intel,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,Neutral
Intel,"Yes but FSR support is all over the place. Look at what optiscaler is doing, we could have had an open standard for upscalers that FSR XESS and DLSS could have been built on top of meaning much wider support across games instead of every game needing specific implementation and leaving us with outdated upscalers that we need driver overrides and DLL swaps to get around.  Microsoft is only now working on a directX based upscaler API that solves this problem. We should have had something like that years ago like we did for RT.",Negative
Intel,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",Neutral
Intel,"It makes adoption slower though. Especially for smaller devs and the smaller graphics vendors. Even now with pretty large games we still have software lumen only. If RT was pushed as an open standard earlier we might actually have more games and better implementation across the whole market, not just nvidias pet projects.",Negative
Intel,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",Neutral
Intel,"patents expire after 15 years, they will have to share it then.",Neutral
Intel,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,Neutral
Intel,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",Neutral
Intel,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,Negative
Intel,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,Negative
Intel,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",Neutral
Intel,"> RT in games is now done through DirectX APIs, which are vendor agnostic.  Now being the key word, Nvidia launched their RTX before DXR was even available.  AMD's slow adoption is absolutely one of the problems which might not have not been so delayed if AMD, Nvidia, console makers and Microsoft worked on RT together from the start.   This is all just musing really. Maybe nvidia did us all a favour by breaking the mold forcing everyone else to play catch up.",Neutral
Intel,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",Neutral
Intel,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,Negative
Intel,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,Negative
Intel,"Okay then, what's the Xe GPU roadmap looking like then?",Neutral
Intel,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",Neutral
Intel,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,Negative
Intel,They barely lived on before the deal.,Negative
Intel,"It'll live on our hearts, yes.",Positive
Intel,Lol if you believe that I have a bridge to sell you in Brooklyn,Neutral
Intel,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",Positive
Intel,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",Neutral
Intel,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,Neutral
Intel,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,Negative
Intel,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",Neutral
Intel,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",Neutral
Intel,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",Neutral
Intel,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,Neutral
Intel,"Always selling out actually, they just can't produce that much",Negative
Intel,And I have another if you think nVidia is capable of keeping this deal running for that long...,Neutral
Intel,Intel will hopefully split their fab business from the rest of the company either way.,Neutral
Intel,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",Neutral
Intel,> they just can't produce that much  because they are losing money on them,Negative
Intel,Trade bridges.,Neutral
Intel,Intel's arc is dead with or without nvidia deal.,Neutral
Intel,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",Negative
Intel,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,Negative
Intel,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",Negative
Intel,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",Negative
Intel,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",Neutral
Intel,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",Neutral
Intel,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",Negative
Intel,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,Neutral
Intel,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",Negative
Intel,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",Neutral
Intel,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",Neutral
Intel,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",Neutral
Intel,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",Negative
Intel,"> If using NVIDIA RTX iGPU in Intel SoC, that will leave only discrete Intel Arc designs to be sold independently.  Assumes without evidence that Nvidia's GPU will immediately replace Xe in every SKU.",Neutral
Intel,Oh those sweet promises. How credible it is!,Positive
Intel,I wonder what this will mean for running llms locally. Could I buy a laptop with 128 gb of ram and an Nvidia iGPU and have that memory unified with the GPU to run the models?,Neutral
Intel,"Sure it does..... Ignore all the signs, we are fine, nothing to see here.",Neutral
Intel,If the roadmaps haven't changed does that mean there's still a chance we'll see Celestial gpus in 2024?,Neutral
Intel,Rtx “igpus” are probably in a different power envelope to igpus..,Neutral
Intel,"Xe will live in Desktops and high end HX which uses the same chips. Xe3 is done. Xe4 is probably done too. So i expect them to be used here like HD Graphics and will get very little improvements from Xe5 onwards. Just to keep display and needed functions.  In a way they are still commited to GPUs   H, U and V are almost certainly getting their Arc GPUs replaced with RTX ones.  We all know with RTX tech those Laptop CPUs will sell a whole lot more and make intel way more profit. Anything with Nvidia's tech sells like pancakes now.  So going forward they can't recover Xe development costs through the laptop iGPUs using like they did before. Remember laptop iGPUs are the highest volume ARC sales.  Without those sale it's hard to justify full on Xe spending needed for dGPUs. Especially with Intel's financial situation.",Neutral
Intel,"This deal seems crazy.  For Nvidia that's an easy way to get custom x86 CPUs for datacenters and a way to directly compete with Strix Halo tier products.   If the products suck they can just blame Intel and walk out as if nothing ever happened. They don't really have much to lose here.  Even if they cancel their own CPU cores as a result of this deal, they can still use stock ARM cores in the future when/if the partnership ends...  But for Intel? For Intel this seems like a complete nightmare.  What did it take for them to convince Nvidia? It's hard to believe this won't have any impact on Arc... In the worse case scenario they'll be giving up on high-end graphics, just for Nvidia to abandon them right after when x86 isn't as important anymore...  This sounds like a desperate gamble, and it's difficult to understand where it came from because Intel does have pretty good iGPUs... Why are they so desperate?",Negative
Intel,Intel's market cap is 141 billion.  Nvidia's 5 billion investment doesn't give Nvidia enough ownership to start calling all the shots.,Neutral
Intel,"I wonder why MLID seems to want so much that ARC be cancelled...  How many times has he brought out ""leaks"" about ARC getting axed in the last few years? I stopped counting.",Negative
Intel,Why are we not linking to the original source?  https://www.pcworld.com/article/2913872/intel-nvidia-deal-doesnt-change-its-roadmap.html,Negative
Intel,Remember how well it went when Intel shipped an AMD iGPU?,Neutral
Intel,With the announcement I imagined Nvidia branded products. Mini-PCs where the CPU and GPU aren't replaceable. Nvidia having Intel and Mediatek products. The integrated GPUs for Intel have seemed pretty solid for a very long time. The discrete cards are competitive in price at least. I wouldn't bet on them quitting discrete cards. Don't think they'd want to be caught with nothing again if another major novel GPU algorithm pops up and starts a frenzy again,Positive
Intel,"Intel iGPUs are equal or better than AMD outside of the one on the 395. 140v is performant and efficient, 140t is on par with the 890m but paired with a better CPU.",Positive
Intel,"Well, I hope they will have them, but I have a feel they will only be available for servers.",Neutral
Intel,Yeah right.. this will age well... Nvidia definitely wants more competition...,Positive
Intel,"They just want to clear Arc inventory, development has stopped.",Neutral
Intel,Intel has struggled with GPU for decades which is insane when look what Apple has accomplished with their iGPUs. Intel was decimated from the insides out when the C-suite decided asset stripping via sharebuybacks and gutting employees/R&D etc.. in addition to taking in debt.  Not sure if Intel can recover but interesting to see all this corporate heroic medicine trying to save the patient.,Negative
Intel,Which is also a big reach to assume when for a large portion of intel's market the current intel iGPU is already faster than they need or will need for the forseable future.,Negative
Intel,"Much more likely the intel/Nvidia collab is for a few special new products, not a complete change in how intel has done graphics for decades.",Neutral
Intel,"Not immediately, but Intel knows CPUs with RTX GPUs will sell a whole lot more than CPUs with ARC.. Money is what Intel desperately needs.  So don't be surprised if they do it fast.",Positive
Intel,There are definitely more Intel ARC iGPU users than Arc GPU users. This will lead to a cut in workforce (already happened) and more in the future. This also means less investment into things like XeSS.,Negative
Intel,The thing is that Intel barely sells any dedicated ARC cards. Almost all of their graphics userbase comes from Intel integrated graphics with their new XE tiles on them. These will be replaced by the Nvidia tiles.      The result of this is that basically every Intel laptop will now have Nvidia inside.,Negative
Intel,"Yes, I doubt Nvidia would ""give"" iGPU without any licensing/fees.",Neutral
Intel,If the bandwidth is as low as most PC laptops currently have it will be slow. Even Strix Halo and DGX have this problem and they are better than most laptops.,Negative
Intel,Nvidia already has this with the the DGX Spark and that same chip is coming to laptops.,Neutral
Intel,"That's likely the plan, and they should be able to pull it off. Just a question of whether they can beat AMD and Apple at it.",Positive
Intel,Intel really needs to use a unified memory design like Apple or they will always look like a joke. They really should be buying a company that makes memory.,Negative
Intel,Reminds me of Stadia. All the signs were there that Google would pull the plug but many were optimistic. Google even said they are committed.  Then it got killed.  Edit - found a article that lists how many times Google said they were committed.  https://www.theverge.com/2022/9/30/23378757/google-stadia-commitments-shutdown-rumors,Negative
Intel,"Yes, I think we'll see them next year.",Positive
Intel,"Doubt they’ll remove the option for Arc completely from the H, U and V series. NVIDIA is most likely charging a pretty penny for their GPUs, which will make Intel completely non competitive in a lot of markets if they were to just switch to NVIDIA.  At minimum, it will stick around for the H and U series as a complimentary option.  Not everything magically sells better just because it has NVIDIA on it",Neutral
Intel,I don't see how Intel might be getting more profit from these tbh... Revenue? Yes. Profit? From where?  Especially if Nvidia will continue to use exclusively TSMC nodes.,Neutral
Intel,"Killing Xe3 and Xe4 is so weird to me when Panther Lake is supposed to launch this year, and Nova Lake (which is rumored to to use the Xe4 media block and use Xe3 for the graphics/shaders/compute block) is supposed to launch sometime in 2026.   Like, the volume for both ""launches"" is supposed to be low, actual devices probably won't be available to purchase until the next year, but. Companies are supposed to be receiving samples of Panther Lake. Now. Presently.   For them to say their roadmap isn't affected, isn't changed at all. Unless that's a massive lie, that's only possible if samples of integrated Xe3 have already been fabbed, are already in working silicon.   ...Like, not to get too into the rumor mill, but supposedly the early Panther Lake samples aren't doing great. The CPU side isn't very efficient compared to Lunar Lake, and the GPUs sometimes don't work. But, when they do work, supposedly, the performance is really good, and, again, rumors, but the problem is supposedly more about Intel's drivers than the hardware or architecture itself.   ...I can completely believe that Intel would panic and would rather kill their GPU division entirely than. Invest in their software stack. Developing good technology and then abandoning it instead of advancing, because abandoning it is cheaper and easier is an extremely Intel move.   But if nothing else, the sunk costs for Xe3 at least make me feel like. They sorta have to figure it out, if only because they don't have time to replace it with NVIDIA, in the time frame Panther Lake has to come out.",Neutral
Intel,"There is just simply no way Intel is going to rely on Nvidia as the sole supplier of their iGPUs when their product lines. Theres even less chance that the U series, their low cost volume product line, would switch to Nvidia sourcing.   Nvidia iGPUs are going to be their own separate, lower volume product line, with its own designation. Maybe -G, or -N, or -AX",Negative
Intel,I dont think so.  Why would they cut themselves from hundreds of bilions of dollars market this easily?  It doesnt make sense,Negative
Intel,">We all know with RTX tech those Laptop CPUs will sell a whole lot more and make intel way more profit. Anything with Nvidia's tech sells like pancakes now.  this makes no sense, it will sell more as opposed to what? the igpu needs a cpu anyway and nvidia's gpus mostly get paired with Intel's cpus anyway. Intel controls 80%+ of laptop cpu market and 65% of ALL igpu + dgpu market, why would they give up such a big lead? If anything this has the potential to cannibalize Nvidia's lower end 4050/4060 market since thats probably the performance these RTX SOC's will be",Neutral
Intel,"Intel has already given up on high end graphics, this deal has no impact on Arc cause Arc already has no future.",Negative
Intel,Intel will want the nvidia name on everything regardless as they know it will sell,Neutral
Intel,The actual original source is here: https://events.q4inc.com/attendee/108505485/guest  But youll need to register to see it.,Neutral
Intel,A lot of AMD's are using outdated nodes and architecture.,Negative
Intel,Intel's biggest struggle with GPUs is gaming compatability and drivers. Apple certainly isnt doing great their either.,Negative
Intel,"I love my laptop with an Intel iGPU, it can actually last me through a work call compare to my other laptop with an AMD chip plus a damn 4070Ti in it.",Positive
Intel,Sure but a lot of Intel's laptop chip business has no need for (presumably) more expensive integrated RTX does it?  Of course for gamers and maybe even workstation laptops (which is a pretty tiny market) these chips will be very appealing but everything else... whatever flavour of integrated ARC Intel are currently developing will continue to be the norm.,Neutral
Intel,"I would assume that the RTX iGPUs cover all the ""gaming"" SKUs, leaving the intel iGPUs for all the low-end and business ones.",Neutral
Intel,"Why would they sell more? We already have laptops with RTX dGPUs and they sell less than Intel iGPU laptops.   The Nvidia iGPU versions are going to be more expensive chips and the laptop prices will reflect that just as they currently do. There's going to be a *reason* to pay extra for the Nvidia versions, and thats going to be measurable more GPU performance. Not just low end Nvidia GPUs on-par with Arc iGPUs.",Neutral
Intel,"Eh. 90% of people and especially companies don't have any need for high GPU performance in a laptop, so CPUs with cheaper Xe iGPUs will likely continue to make up the vast majority of their sales.",Neutral
Intel,"> Not immediately, but Intel knows CPUs with RTX GPUs will sell a whole lot more than CPUs with ARC.. Money is what Intel desperately needs.  Among gamers and people who need the power? Agreed!  Majority of users who don't game or need the power/expense.... nope!  What I like about this is that it pushes innovation further. Hopefully it brings innovation to the sub-$600 laptop market.",Positive
Intel,Average person doesn't care.    Most people don't play games on their computers all of the time.,Negative
Intel,Businesses do not care about GPU's in their laptops it will not sell a whole lot more.,Negative
Intel,Intel iGPUs are drastically better than Nvidia stuff for anything except Halo-type parts.,Positive
Intel,"I think it may come down to the fact that the executives within a particular business unit probably ARE very committed to making it succeed and sticking with it, but the ultimate decision to pull the plug comes from higher up",Neutral
Intel,Ahh yes reminds me of this lovely Google Graveyard.   https://killedbygoogle.com/,Positive
Intel,"The reason why Stadia failed was because they never implemented ""negative latency""",Negative
Intel,"Because a Laptop is a sum of its part.  Lets say for $400 IntelArc CPU and $500 IntelRTX.  When a laptop is built, if the IntelArc costs $1000 the  IntelRTX will cost $1100  The higher you go the closer it gets percentage wise.  So when the difference is less 10% which one do you think sell way more?",Neutral
Intel,Nvidia is always looking for more fabs as they seem to be significantly supply limited at TSMC. If 18A/14A turn out good it's a safe bet they'll at least try it out.  Remember how Ampere customer cards were done on Samsung so they could use all of their TSMC allocation for A100. They would probably have continued that Samsung partnership if internal corruption at Samsung didn't bomb their 4nm process.,Neutral
Intel,"Even **if** Intel were to completely cancel Xe and their entire business becomes dependent on Nvidia selling them iGPU chiplets (for some odd reason), the timeline for these Nvidia chiplet CPUs would be after NVL",Neutral
Intel,They've spent the past 1-2 years laying off much of their driver team.,Neutral
Intel,Excuse me but Hundreds of billions?   Since when has gaming GPUs make 100 of billions.  Nvidia makes just over 10 billion on Gaming GPUs a year.,Neutral
Intel,"They don't see themselves having a credible chance of capturing that market (GeForce + CUDA moat is deep) without having to trash their margins, they see short-term gain in being able to have the best gaming laptops on the market for a few generations, and being the choice to provide the CPUs for NVIDIA AI platforms.",Neutral
Intel,If you haven't noticed intel hasnt been doing so hot.  Dell recently added AMD for XPS for the first time. Many OEMs similarly are doing amd versions for the first time. Something unthinkable even 5 years back   This way they losing OEM sales on the laptop space. They are no longer the preferred CPU brand.  So what do you do? Well having their CPUs with RTX would give them a surefire boost,Negative
Intel,Intel is worried that they'll lose this market to the now superior AMD chips.,Neutral
Intel,"Well AMD does make 4 billion each year from their console deals, and Intel cannot enter that space because they don't offer a product that is as compelling as AMD.      With this partnership they could do this.",Neutral
Intel,Oh we know they are trying to sell us outdated modes and architectures. And they will continue to do so for the entirety of 2026 in mobile.,Negative
Intel,Apple GPUs are significantly more powerful than any intel iGPU. I can play cyberpunk 2077 at 60fps 4K and nice visuals. Also I can run large AI LLMs as well. I can run heavy AI workloads all day long and total wattage 150-160w for the entire Mac Studio.  Intel iGPUs would melt/die. Its amazing how far Apple has pushed Apple Silicon while Intel got stuck in the mud.  Sad to see how the mercenary C-suite came in like a plague of locusts and gutted Intel leaving a husk that requires intensive care to resuscitate. I hope they do rise back up because competition is good and pushes progress forwards.,Positive
Intel,"If Optimus is working properly, your AMD laptop should not be consuming any more power than if it were APU-only during normal use.",Neutral
Intel,this is likely more to compete with amd's apu's and less for business,Neutral
Intel,"I can see a consumer marketing reason and a business professional reason for RTX throughout: driver support. Anything with the RTX branding will get more support on both fronts, especially if the integrated parts default to studio level drivers. A large part of Arc’s inefficiencies are due to the lack of software support, so streamlining this aspect would be a huge boon for the “it just works” marketing coming back online for Intel.",Positive
Intel,That could still mean Intel dGPU business will go away.,Neutral
Intel,"It has been talked about a lot but SOCs will be replacing the lower end systems completely. For example, the Strix Halo chip that AMD just released is better than a RTX 4060. That may seem weak, but it's cheaper to provide an APU than a CPU+GPU.",Positive
Intel,"I said profit.  Even if RTX CPUS cost more, they result in significantly more sales resulting in greater profits.  We know from the market, that even if its more expensive, Nvidia GPUs will still sell more than equivalent cheaper counterparts",Positive
Intel,"You mean the business sectors which are most likely to use AI? You know, what Nvidia is famous for.  And low-end don't exist for new CPUs for laptops anymore. Its all older generations rebarded.  Intel use older alderlake and raptor lake CPUs rebarded with UHD graphics and AMD sells Zen 2 CPUs.",Neutral
Intel,Makes me wonder if we will see lower price devices with Nvidia iGPU V vs roughly equivalent dGPU models. There should be some saving I think but will also be interesting to see if there remains a choice or if Nvidia will try and move most of it's mobile lineup to this unified product.  I think on the very highest end like 80 and 90 tier mobile chips they may well be too powerful to adequately cool as part of a combined package.,Neutral
Intel,"I honestly want a work laptop that has a good keyboard, screen, and battery life. (and light!)   I'm mostly doing emails, anything else I'm using a workstation for number crunching.",Positive
Intel,And how do you know this?  Cause last i remember Nvidia doesn't have iGPUs,Neutral
Intel,"While we have yet to see good comparisons, it's provable that Nvidia produces more efficient graphics cards than Intel does. The die size on the Intel chips is massive compared to the similar performing Nvidia ones. Larger the die the more power it's using. Switching to Nvidia will provide more performance at the same power usage.",Positive
Intel,Latency on stadia was actually good. I tried it for a bit with with their pro subscription for a month.  But never renewed or bought anything cause of the awful business models.,Negative
Intel,"And why would Intel do that? Theyre dropping MoP because they act as a middleman, moving MoP at cost to OEMs.  A $100 BOM increase per unit rarely only comes with a $100 product increase. And even then, it's corporate suicide to make Nvidia a critical sub-supplier when you've dont need them to be. Intel would never go in 100% on Nvidia being the sole iGPU option.   The Nvidia option is going to be a separate, lower volume, more premium product line.",Negative
Intel,"Apparently they have been working on this for a year at this point, so a product with an Nvidia tile is likely going to launch by 2027.",Positive
Intel,"Nvidia makes over 10 billion on gaming GPUs in a quarter. But yes, not hundreds of billions.",Neutral
Intel,Arc covers also non gaming segments like B50 product,Neutral
Intel,"Gaming laptops?  Ive watched the webcast with Jensen and as far as I understand this is about datacenter because customers dont want to switch to arm, so partnership with Intel will allow them to get x86 cpus with features Nvidia needs for their datacenter and hpc customers",Neutral
Intel,"Also, the partnership potentially could get Nvidia using their fabs. Nvidia will also now hold a 4% stake in Intel.",Positive
Intel,"LNL(Xe2) solos every AMD offering in igpu out there and matches the AI 395 with lower power, even ARL(Xe+ Alchemist) isnt that behind, igpu perf is the last thing Intel is concerned about, these high perf RTX laptop apus will only damage the lower end offering from nvidia  and here look at the laptop market share, Intel's share has been pretty constant between 75-80% for the past 3-4 years, idk where the notion of Intel struggling in laptop space is coming from, they even clawed some back after the launch of Lunar Lake: [https://www.tomshardware.com/pc-components/cpus/amds-desktop-pc-market-share-hits-a-new-high-as-server-gains-slow-down-intel-now-only-outsells-amd-2-1-down-from-9-1-a-few-years-ago](https://www.tomshardware.com/pc-components/cpus/amds-desktop-pc-market-share-hits-a-new-high-as-server-gains-slow-down-intel-now-only-outsells-amd-2-1-down-from-9-1-a-few-years-ago)",Neutral
Intel,It’s only been five years since people said it when Renoir APU launched. Maybe in another 5 AMD can cross 30% market share on laptop (it’s been stuck at 25% for about 3 years)?,Neutral
Intel,That’s really nothing. In Q2 2025 AMD made 1.1B from gaming (which includes semi custom and Radeon) out of 7.7B.,Neutral
Intel,Asuming AMD tech work properly is not a bet you want to take.,Negative
Intel,"I know, it's just that I think we'll keep seeing Arc iGPUs for the business and lower end consumer market primarily. Cheaper or more power efficiency focused laptops mainly.",Neutral
Intel,They don’t need that to compete with AMD’s regular iGP. 140v/140T do well enough.   Strix Halo is irrelevant as it was near zero mainstream OEM presence and,Negative
Intel,"It's possible.  Arc might be carving a workstation niche though? their B50 certainly seems like that might be the focus, offers a lot of pro features for a low price.  As far as enthusiast goes I wouldn't be surprised, it's a money sink Intel probably doesn't need even if the payoff might be worth it long term. They say it will make no difference though so who knows.",Positive
Intel,Demonstrably false in any currently existent device. Any $1500 laptop with a 5070mobile  blow the pants off Strix halo.   Continue your APU delusion in r/amd,Negative
Intel,"Will they really profit more by adding RTX GPUs where there were none previously? Especially considering that NVIDIA will likely not make it any cheaper.  Like ultrabooks aren’t going to magically sell more and more, or more expensive in all categories just because they have NVIDIA GPUs.  Only place I can see it really make a lot of sense and add more profit is in place that already had NVIDIA GPUs, like with gaming laptops or enterprise, or as an option to complement Arc.",Neutral
Intel,Are you aware that most enterprise computing is cloud based?,Neutral
Intel,"I think client inference is kind of stupid. It's limited in capability by your local memory amount, and it's economically inefficient because you can't batch requests.  No AI system that exists is ""there"" yet, all of them can be improved enough that people would switch over to a better one. The current top of the line commercial models are hundreds of GB for the weights. To use client inference today you need to use a severely castrated model. Alternatively, if you ship all those hundreds of GB of DRAM on every device, they will be very inefficiently utilized because a single user rarely has the token flow to keep them working, and when they do it's all serial so you can't even batch, you just have to do a linear read over the whole model to get a single token out. And when there's an improved model next year that takes twice the ram you have to roll out whole new machines to the whole fleet.  In contrast, centralized inference can fit as much memory as you put on it, there are no power constraints, you can batch hundreds of requests in one go, and you can update the whole system much more easily. Client inference won't even win in latency because even though you have to pay for network latency, the centralized solution is probably much faster.  The only real advantage client inference has is privacy, and that's not a problem in business, they just get their own inference server. For office work, that even makes latency very fast.",Negative
Intel,"I truly think you'll start to see lower tier offerings, like 50 - 60 class because large iGPUs. Maybe even 70, and a separate, discrete graphics chip become increasingly rare of the next decade.   Nvidia has a similar deal going on with Mediatek to bring their graphics there too.   This is Nvidia shoring up their lower end market in client laptops as the APU wars begin",Neutral
Intel,"You don’t have to wonder, laptops with 7600m are already far better deal for gaming than any equivalent system with Strix halo for gaming.",Positive
Intel,"That kind of proves my point. Intel iGPUs exist, and therefore are better.  Trying to scale-down discrete graphics into integrated graphics will take quite a bit of development effort. AMD has done pretty well with it, but it took them years.  PC gaming / PC building hobbyists seem to drastically undervalue how good Intel graphics really are. They're low power, stable, and have sufficient performance to make the entire category of discrete graphics a niche market in PCs. For most applications that currently use discrete graphics, there would be no reason to even consider Nvidia if it were an option.",Positive
Intel,Quicksync and power management if I were to guess. Spinning up a dGPU uses a lot of power.,Neutral
Intel,Quick sync on a $150 Intel CPU is better at transcoding then anything Nvidia offers under $600.   A SIGNIFICANT portion of intel sales are because of features like this. Otherwise the entire industry would just switch to AMD. Which is superior in every single way EXCEPT it's IGPU support.,Positive
Intel,"Meanwhile back in the real world the switch 2 exists with its nvidia SOC and all those SOC's inside cars exist, all those SOC's in Jetson's etc exist.   Lol literally knows nothing out side of PC gaming hardware.",Neutral
Intel,Nvidia actually does have iGPUs. Cars that use AGX Drive will have one.   The Volvo EX90 is using [AGX Orin](https://www.techpowerup.com/gpu-specs/jetson-agx-orin-64-gb.c4085) which has an Ampere-based IGPU with 2048 CUDA cores and [owners are being given free a upgrade to a Dual Orin setup](https://arstechnica.com/cars/2025/09/forget-ota-volvo-is-upgrading-the-core-computer-in-the-ex90-for-free/).  The latest AGX Thor has a Blackwell IGPU with 2560 CUDA cores.  Either way it seems easy enough for Nvidia to package Blackwell into a tile that Intel can replace their Arc graphics tile with.,Neutral
Intel,"Again, you're confusing yourself with current-generation dGPU comparisons. Yes, discrete Arc is behind Nvidia, because it's new.  An iGPU isn't just a dGPU slapped next to a CPU, nor are different process nodes really comparable.",Neutral
Intel,"Pay for a sub, still have to pay again to ""buy"" games that you don't own. What could possibly be unappealing about that?",Negative
Intel,Latency on stadia could not be good. It would defy laws of physics (namely - the speed of light) for latency on Stadia to be good. This is why streaming gaming services never work and cannot work. We need FTL communication to reduce latency enough and FTL remains strictly fictional.,Negative
Intel,You are kidding yourself if you think Nvidia will let their iGPUs be a low volume products.,Negative
Intel,I am pretty sure intel made the arc pro at a low price to sell of most remaining stock and capacity bookings.  This Nvidia deal was in talks from a year ago. And isn't it a funny coincidence they announce the deal after the Arc Pro has sold out in a lot places?,Neutral
Intel,"There were two things announced. A client partnership for gaming laptops (primarily), and a datacenter partnership for custom Xeons with NVLink.",Neutral
Intel,"> LNL(Xe2) solos every AMD offering in igpu out there   Except in, you know, actual workloads.",Neutral
Intel,Well intel is and will still be high in volume since they have very low end on lockdown. They maybe high volume but they are the lower margin chips.  Here is whole range of intel core (not ultra) that use older architectures  https://www.intel.com/content/www/us/en/products/details/processors/core.html  They even revived intel 14nm AGAIN  https://www.intel.com/content/www/us/en/products/sku/244818/intel-core-i5110-processor-12m-cache-up-to-4-30-ghz/specifications.html  Amd cant quite match this yet  However the higher margin products is where they getting hurt by AMD. So that's not good for them,Neutral
Intel,Luner Lake is for handhelds are very lower powered laptops. It's not for work.,Neutral
Intel,LMAO but tbf this time it’s also nvidia,Neutral
Intel,"That's likely the case for now, but Intel probably can't fight against the market.   Now that Nvidia iGPU is an option, OEMs are going to express interest, enterprises are going to express interest. It's the AI era, nobody is going to get fired for buying Nvidia.  Intel's CEO is a customer-pleaser, so if there's demand for mainstream SKUs w/ RTX, I can't imagine he'll say no. And Nvidia winning more of Intel's business? Maybe they'll have to port Nvidia IP to Intel foundry to service the higher volume.  I think Intel Xe will stick around for a while as a legacy platform, Intel does have a bit of a software moat, but I think this goes much further than Kaby Lake-G.",Neutral
Intel,> Arc might be carving a workstation niche though  Not enough to keep it alive. Even smaller market than gaming.   > offers a lot of pro features for a low price  Well that's kind of the problem. They're forced to compete on price.,Negative
Intel,"Thats just more of AMD failing to work with OEMs. Placing a large iGPU tile on a CPU is lower BOM costs than two separate chips with two separate memory pools. And it's easier for OEMs to source a singular chip and cool that single chip.   Strix Halo's failure at using these lower BOM costs to aggressively gain market share, and instead positioning itself as a premium ""high-VRAM"" option is besides the point.   I agree with the poster above the APUs are going to begin cannibalizing the low end dGPU market over the next 10 years.",Negative
Intel,">Like ultrabooks aren’t going to magically sell more and more, or more expensive in all categories just because they have NVIDIA GPUs.  actually, they will. The mindshare aside, the ability to support things like CUDA or ray tracing will make it a much more desirable product.",Neutral
Intel,"Let me tell you why intel rtx laptops wont be significantly more expensive.  A $400 Intel+ARC and $500 Intel+RTX. Same performance, but Nvidia has their features.  Now the Intel+RTX looks more expensive right by costing 25%? WRONG   Cause if the two laptops are built with exact same components and Intel+ARC laptop costs $1000, the Intel+RTX laptop would cost $1100.  Suddenly the Intel+RTX becomes the obvious choice at only just 10% more money.  It gets worse the higher you. $1500 intel+arc v $1600 Inte+rtx. Or $2000 v $2100  This is also why AMD sucks at OEM and Prebuilts  If a 9060xt built desktop costs $900 the equivalent RTX 5060 Ti desktop would just cost $970.  So suddenly instead of the RTX 5060 TI costing 20% more, it becomes only 8% more expensive.",Negative
Intel,Its debatable. There was a big push for putting everything in the cloud. now there is a big push for getting everything back to local because cloud sucks.,Negative
Intel,Which runs on Nvidia hardware.  Nvidia laptop GPUs dont have enough memory to run ai locally.  However an Nvidia iGPU with access high capacity DDR5 and you see where we are going?,Neutral
Intel,"I'm really looking forward to it. I guess ODMs are probably pretty pleased as well, this should simplify laptop motherboard layout, feasibly improve reliability and given it's already so rare for a gaming laptop to have anything other than an Intel CPU + Nvidia GPU they're probably going to find all of this very easy to design around.",Positive
Intel,"Nvidia's been doing iGPUs for *years* in Tegra.  > For most applications that currently use discrete graphics, there would be no reason to even consider Nvidia if it were an option.  Nvidia's IP and drivers are simply better.",Positive
Intel,"You know, i think an iGPU does exist. The switch 2.  And I don't need to tell how efficient that is despite using the awful Samsung node",Neutral
Intel,Intel Xe Media Engine is the SOC tile. Not the GPU tile  So Arc GPU tile etting replaced will not effect that at all.  Edit - here is how it works  https://www.intel.com/content/www/us/en/support/articles/000097683/graphics.html  The display is also there too. So the GPU tile cam be completely idle. Arc or RTX,Neutral
Intel,> Quick sync on a $150 Intel CPU is better at transcoding then anything Nvidia offers under $600.  That's just how they spec the encoders across the lineup.,Positive
Intel,Good thing the Arc tile getting replaced wont effect it.  If anything now you get both Qsycn and Nvenc,Neutral
Intel,Intel sales are mostly laptops and most businesses do not give a single shit about iGPU or quicksync.   Hardly anyone actually buys intel for quicksync lol.,Negative
Intel,I actually did reply the switch 2 just below because i got tunnelvisioned on windows  He never replied after that because the switch is far more efficient than anything intel has and that too using a worse mode,Negative
Intel,Here is the kicker. You didn't need to sub and then buy games.  If you bought a game you can play it without a sub.  But google failed spectacularly to market it that as you didn't even know that.,Negative
Intel,The sub was optional. You could buy a game and play it indefinitely without any extra costs.,Neutral
Intel,"It was good. Not as good GFN cloud gaming but surprisingly good and very playable.  GFN has lower latency than consoles btw on MnK. That's because consoles rely on Bluetooth controllers which adds more latency.  So if anyone is fine is consoles, they will be more than fine on GFN.",Positive
Intel,"well, their dGPUs are a niche product in laptops already. Why would an agreement to co-develop an APU with Intel change that?",Neutral
Intel,"The Arc Pro B stuff hasn't even released to consumers yet.  B50 is shipping the 25th, B60 hopefully not to long after.  If Intel can really ship B60's at MSRP this year they will sell every single one they can make. There's nothing out there that comes even close to being competition for it.",Neutral
Intel,"The deal was negotiated for months and finalized / signed on September 13th (per Greg Ernst on LinkedIn). That means they announced the deal just 5 days after making the deal.  And even then, these parts arent coming out for years.",Neutral
Intel,"what actual workloads? In gaming they are basically tied, and that's what most consumers care about. The workstation laptop market is very small and niche and most people use a discrete gpu there anyway",Neutral
Intel,What “actual workloads” on an integrated graphics would you be referring to?,Neutral
Intel,"What kind of work are we discussing here you think the bulk of office laptop from dell, hp, and Lenovo (which is the majority of worldwide pc shipment) have to do?",Neutral
Intel,Whether MT matters or not seems to depend on if Intel is food or not.  Don't people always talk about how gaming is king?,Neutral
Intel,We will see. For the moment all we can go on is what Intel is saying and they're saying it's not going anywhere.,Neutral
Intel,"and yet Strix is the more expensive option out there. Large, performan APUS are not easy or cheap.",Negative
Intel,"Again, no.      First off, Arc already support RT on iGPU. And unless they start shipping more power hungry, larger GPUs in the place of smaller ones for ultrabooks, the performance isn’t going to magically get that much better for RT to be useable.     Second, if CUDA made ultrabooks that much more desirable, you would think we would see a 2050 or even a 3050 be included in a every ultrabook. That’s not exactly the case",Neutral
Intel,It's really funny how you assume that OEMs and ODMs won't also charge more on top of the component cost.,Negative
Intel,"No I don't, actually. Why would enterprise customers bother running anything on laptops? There are no reasonable use cases for this. The only ""AI"" thing that needs to exist on a laptop is advanced search and windows recall, which the existing Intel hardware is perfectly capable of handling. Plus, you are aware Microsoft copilot+ requires a separate NPU right? Which makes the whole idea of running ""AI"" on said Nvidia IGP quite redundant.",Negative
Intel,"Yea, gamers definitely live in some sort of mirror universe.  From my perspective, Intel drivers are significantly better than AMD drivers, which are worlds ahead of Nvidia drivers.",Positive
Intel,"Samsung's 8LPP was one of their best nodes lmao, what is with this revisionism",Positive
Intel,"90 percent of servers using Intel for Quicksync are not going to waste their time buying an Intel ARC GPU for $300 that they can't even source.   If Intel makes this mistake, literally every single person in the industry would just switch to AMD CPU's. As they are superior in virtually every single aspect of computing. If you are forcing people to buy an Intel ARC GPU, then the CPU they own will not matter.   Contributors to things like Jellyfin already stated they will just spend all their development time optimizing for AMD APU's.   I have worked in Comp Eng for 20 years. I can assure you Intel will not survive if they do this. They will likely be bought like the first company I worked for, ATI.",Negative
Intel,"AFAIK, the 265KF for example, does not support quick sync.",Negative
Intel,"Well, console controllers add about 50ms latency. However if your stream latency is less than that then you must be lucky and live very close to the server (physically).",Neutral
Intel,Nvidia laptops are niche products....HUH????  Have you seen the steam charts? Their laptops are big Even rivaling desktop numbers.  https://store.steampowered.com/hwsurvey/videocard/,Positive
Intel,Its bait to sell off remaining stock and bookings.,Negative
Intel,> In gaming they are basically tied  LNL vs Strix Halo? Absolutely not.,Neutral
Intel,"Gaming, content creation. Your choice, really.",Neutral
Intel,CPU performance was never discussed in this thread only iGP.,Neutral
Intel,"> and they're saying it's not going anywhere  They've said nothing about dGPUs in particular. If anything, likely died before this deal.",Negative
Intel,"Laptops \*With Strix Halo are more expensive.  But I'm failing to see how taking the 40CU's package, making it a separate dGPU chip, giving that chip its own VRAM is cheaper than placing those 40CUs on package.  Strix Halo is low volume with a bespoke 256b motherboard. It's cost structure is negatively impacted by its economies of scale - not because its design is inherently more expensive.",Negative
Intel,"it may not be desirable enough to include a 3050, but desirable enough to slightly increases sales if its part of the iGPU? There is also a thing that you want to avoid dGPUs in ultrabooks due to battery time.",Neutral
Intel,"Of course they will, Because you and i both know which one will be in more demand and sell more.",Neutral
Intel,"> Plus, you are aware Microsoft copilot+ requires a separate NPU right? Which makes the whole idea of running ""AI"" on said Nvidia IGP quite redundant.  That bit seems to be changing, fwiw.",Neutral
Intel,"Why are you of the belief that people care about CoPilot+ at scale?  There are plenty of reasons a company would want local compute options instead of compute in the datacenter. Data security, data ingress/egress costs, local AI compute for appliance-type deployments, edge inference, network constraints, etc.  There is not a one-size-fits-all approach to AI compute. NPUs are not sufficiently powerful enough for the workloads today, let alone where AI compute is heading.",Negative
Intel,Wouldn't change anything as the NPU is im the SOC tiles. Not the GPU tile.,Neutral
Intel,"> Intel drivers are significantly better than AMD drivers, which are worlds ahead of Nvidia drivers  What?",Positive
Intel,Bro what? In what context?,Negative
Intel,It's not even a current *Samsung* node. It's like 3 full gens behind state of the art.,Neutral
Intel,It is awful compared to modern tsmc 5nm. Which all CPU and GPUs use,Negative
Intel,It wasnt good when it was new andt isnt good now.,Negative
Intel,I think the smarter move would be to do what amd did. A VPU for servers  https://www.amd.com/en/products/accelerators/alveo/ma35d/a-ma35d-p16g-pq-g.html  Now that will sell. Nvidia isnt competing there either.,Neutral
Intel,"They aren't saying you'll need a gpu, they are saying the quicksync and other media engine features are on the cpu SOC tile which will still be present in hybrid nvidia igpu tile designs. They could still stuff it up by not supporting it but the hardware will be there.",Neutral
Intel,people doing large scale transcode with Quicksync would never switch to AMD - the worst possible option for transcode.,Negative
Intel,F = no IGPU,Neutral
Intel,Less than 25% of PCs sold have Nvidia graphics at all. Don't care about the Steam hardware survey lol. Intel iGPU outsells Nvidia 2.5 to 1 in client.,Negative
Intel,"Nah.  The AI market is too hype for Intel to just drop it, and Nvidia is exploiting enough market power that there's certainly space to profitably undercut them in that segment.",Neutral
Intel,What content creation are people buying AMD iGPU laptops for specifically? This is the weirdest lie someone has ever told on this website lmfao,Negative
Intel,"140v does not lose to 890m in either of these, what are you on about?",Neutral
Intel,Ask AMD. They are the ones selling the SOC for 600-1000 dollars to the OEMs. Low volume high RnD ammortization may be the reason.,Neutral
Intel,"Yeah, the one that has an almost identical featureset for $500 less",Neutral
Intel,"Companies are not clamoring to get AI compute more powerful than what the NPU can provide, locally, and at scale across their whole fleet.   If they were, P series and Precisions would've supplanted E/T and Latitudes by now.   Like, what's the usecase of that much local AI on individual workstations? Presumably any data being inferenced will he in some shared location, and with it, so will the inferencing hardware.",Negative
Intel,"OK, what kind of workload do you think they will actually run on laptops that is too powerful for existing and future Intel hardware, not too powerful for whatever Nvidia IPG will be bundled, and they don't want to do with an on-prem solution?",Neutral
Intel,You still haven't addressed the why,Negative
Intel,"Being a long-time desktop Linux user with a decent memory.  Intel's the only graphics vendor that takes drivers seriously at all. They write and ship them months in advance of product releases, to the point that I could reasonably expect to buy a pre-release B60 today, put it in my desktop PC, and have it just work with already-installed drivers.  AMD and Nvidia treat drivers like video game releases. They ship at the last possible minute and then need patches a week later for bugs, then they kind of get abandoned. And Nvidia never does open source driver releases, which are the only way to have reliable long-term support or any support for configs that differs from the vendor test setup.",Neutral
Intel,"No shit, but compared to its contemporaries it did pretty well.",Negative
Intel,"Huh, weird, last time I checked Blackwell, Navi 48 and Zen 5 were all on TSMC N4, Arrow Lake on N3, and none of them on N5. There seems to be an awful lot missing from ""all CPU and GPU""",Neutral
Intel,"That's just a lie lmao, the 8LPP was possibly the only good node SF has put out in years. 10LPX and 10LPP were both not great but 8LPP was good.",Positive
Intel,"Of course they are. Even upstream FFmpeg couldn't keep up with the demand and they had to write additional code themselves to drive the hardware transcoding.  https://github.com/jellyfin/jellyfin-ffmpeg/wiki  It's not true that they spent all their time on APUs, they did spend some time, but it was much better than their competitors spending almost no time on AMD.",Neutral
Intel,Right. The person I'm responding to is saying that Intel CPUs would still support Quicksync if Intel removed the Arc iGPU tile.,Neutral
Intel,"Nvidia's gaming division makes over 4 billion a quarter. And its a safe bet more than a 1 billion comes from Laptops alone.  That not a niche market at all. Yes, its not market dominant if you include iGPUs which even exists in celerons, but if you call over 1 billion a quarter niche, we need to rewrite the meaning of ""niche""",Neutral
Intel,Intel is going to get Ai market by being the exclusive x86 supplier of Nvidia.  Both on server and client.,Neutral
Intel,"Intel's too late to the AI market, Arc flopped, Gaudi flopped harder, broadcom is doing custom solutions, AMD is number 2. China is all in with full state backing. What is Intel going to do? Nobody needs a third place.",Negative
Intel,> This is the weirdest lie someone has ever told on this website lmfao  Weirder than claiming LNL outperforms Strix Halo in GPU?,Negative
Intel,They claimed it even beats Strix Halo.,Neutral
Intel,"""identical"" lol",Neutral
Intel,I think you dont realize how many companies both need the local performance and not get it because of budget constraints. Employees having hell of a time trying to deal with that mess? not shareholder headache.,Negative
Intel,"I think you're missing the point because you seem to believe that all users or departments have unconstrained budgets and can just buy whatever the ""perfect"" solution is for their workload. If that were the case, all workloads would already be handled in the datacenter, and no one would ever need a local GPU for compute, graphics, or AI workloads, which is not reality.  It's a hell of a lot cheaper to go buy a dozen laptops than to go buy a single B200.  Look at the entire entry laptop workstation market. There's a reason why those products exist, or else OEMs wouldn't make them.",Neutral
Intel,Won't matter anyway because of how poor NPU support is.  ROCm doesn't support Ryzen NPUs. Don't think Intel OneAPI does either. And CUDA obviously doesn't.  There was good post about NPUs here  https://www.reddit.com/r/hardware/s/JGJ45bpbjN  There is very little reason to support NPUs.,Negative
Intel,"And yet anyone who does more than basic display and maybe some 3d gaming stuff on their GPUs, uses Nvidia on Linux. Be it GPGPU stuff, cryptographic work, ML training or rendering. Intel ARC had driver issues on both Windows and Linux for a long time. They have a good record on Linux before that yes, but only for their integrated stuff. And intel 7th gen igpus still do not have Vulkan support.   Also we are not talking about Linux users anyways. Like 5% of people use Linux.",Neutral
Intel,> Being a long-time desktop Linux user with a decent memory.  so completely irrelevant market niche.,Neutral
Intel,Intel 14nm did not only “pretty well” but amazingly compared to its 2014 contemporaries.,Positive
Intel,N4 is a custom N5 version. They are a lot more similar than people think. They are both 5nm nodes.,Neutral
Intel,I think the F series has both Display engine and Media Engines disabled cause they assume you are going to be using a requirered dGPU for that.,Negative
Intel,"Intel iGPU outsells Nvidia dGPU 2.5 to 1. There's no need to redefine niche, which means ""a specialized segment of the market for a particular kind of product or service."" I think being a high end upsell product that's found in less than 25% of computers qualifies.    >You are kidding yourself if you think Nvidia will let their iGPUs be a low volume products.  That's what you said. They already *are* a comparatively smaller market than Arc in client. The Nvidia x Intel collab product will be the same: A lower volume specialized part that costs more and performs better, that some people will pay extra to upgrade to. But it will not form the bulk of Intel's volume.",Neutral
Intel,They are very clearly not including Strix Halo. You're somehow the only one who is thinking of Strix Halo.,Neutral
Intel,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,Negative
Intel,Far more than I expected them to come out at. Damn.,Negative
Intel,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",Negative
Intel,I’m getting one when it releases in Australia,Neutral
Intel,Thats great and all but when will there be stock? (Canada),Positive
Intel,Pytorch libs are getting better for intel and amd support but there are still a lot of 3090 cards on the used market and those don’t need hoops to jump through to get compute working.  If I was handed a budget this low by a boss I’d just ask them to buy a lot of 3090 cards.,Neutral
Intel,I'm disappointed.  My order was canceled,Negative
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",Negative
Intel,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,Neutral
Intel,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",Positive
Intel,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",Negative
Intel,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",Neutral
Intel,The 3090 does not have ECC on it's VRAM nor certified drivers,Neutral
Intel,Totally not worth it.,Negative
Intel,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,Positive
Intel,"I'm more looking forward to the B50, but obviously local pricing is everything.",Positive
Intel,"It has SR-IOV, certified drivers and other professional features...",Positive
Intel,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",Neutral
Intel,">The gaming cards require significantly less robust driver and application support  Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  It was like that since A770 release. When some games straight up don't work or have unplayable bugs, you are literally paying for own suffering.  That said, B50/B60 are good, though i wish B50 worked with ""Project Battlematrix"" that is Intels NVLink. B60 was also expected to be for 500$, not 600$, but it is what it is.",Negative
Intel,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 1 year!?,Neutral
Intel,"...Do integrated graphics support SR-IOV? Do any of Intel's iGPUs support the ""pro"" version of their drivers,   Like, obviously iGPUs have less compute power and less bandwidth than anything dedicated, but. Part of me feels like iGPUs should actually have the features of the ""pro"" cards, given they aren't replaceable, with the actual pro cards providing performance.",Neutral
Intel,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",Neutral
Intel,Do not underestimate the lack of CUDA.,Negative
Intel,"Bought my 3090 for MSRP 4.5 years ago and honestly would never have imagined that it would hold 50% of its value.  I want to get another one when they drop to $200 or so, so that I can finally be disappointed by SLI and make 14 year old me happy by finally clicking the bridge onto two GPUs and firing up a benchmark, but who knows if/when that day will come.",Neutral
Intel,A used 3090 is only $100 more.,Neutral
Intel,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",Negative
Intel,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,Neutral
Intel,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,Positive
Intel,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",Neutral
Intel,It is an competitive edge and hope Intel pushes on it hard. Same with AMD and its Strix iGPUs. I will buy unified memory on a desktop for a premium too - the M5 chip is going to make all 3 Intel/Nvidia/AMD wake up hard,Positive
Intel,Atlas 300i duo,Neutral
Intel,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",Negative
Intel,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",Neutral
Intel,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,Neutral
Intel,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",Neutral
Intel,It was meant to be a joke. Not so funny I guess.,Negative
Intel,"B580 needed to get out very early to beat RTX 5060 and RX 9060 to the market. Nvidia's and AMD's cards outperform it in gaming raster performance thanks to having been in the market for much longer. They have the more efficient architecture and software for gaming. It wouldn't be great for marketing if Arc came out at the same time with similar pricing, yet got beaten in the FPS benchmarks.  But productivity cards could wait. Arc's always been very competitive in productivity tasks. Maybe because Intel has more experience in this department. So reliable driver is the focus here.",Positive
Intel,">Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  Good driver and application support is not possible to deliver without a significant install base of your product in the wild. PC configurations vary wildly and usually driver issues are about some bad luck combination of hardware, driver versions, and application versions.  The incumbency advantage in this market is immense. Devs will validate that their stuff works on all manner of configurations including an Nvidia GPU, so they get the perception of having bulletproof drivers.",Negative
Intel,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,Neutral
Intel,Why does this matter?,Neutral
Intel,"Some iGPU do support SR-IOV, I think 12th generation and newer. Motherboard, driver, and OS dependent.",Neutral
Intel,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,Negative
Intel,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",Neutral
Intel,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",Neutral
Intel,Used market is freaking insane. It is better to grab new or open box.,Negative
Intel,Over here used 3090s are sold for 500-600€.,Neutral
Intel,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",Neutral
Intel,I hope this advances well for the future hardware and software releases -[MLIR] (https://www.phoronix.com/news/Intel-XeVM-MLIR-In-LLVM),Positive
Intel,Dude that feeling is so worth it though. I did that same with 2 Titan Xps a couple months ago and the build just looks awesome. Great performance on pre 2018 / SLI supported games as well.,Positive
Intel,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,Negative
Intel,the super gpus are not expected to release soon?,Neutral
Intel,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",Negative
Intel,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",Negative
Intel,Used 4070 Supers dont sell for nearly $700+ on the low dude.,Neutral
Intel,Yes running business of used cards is how its done.....,Neutral
Intel,no fee SR-IOV makes this a substantially different offering then AMD or Nvidia.,Neutral
Intel,Typically run at 250W though to be fair.,Neutral
Intel,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",Negative
Intel,not on the Vram like professional cards,Neutral
Intel,Q4.,Neutral
Intel,I thought it was funny ¯\_(ツ)_/¯,Positive
Intel,The b580 is still good at $250 and it’s only recently in the past month the 5060 and 9060xt 8gb dropped near or at $250 in some instances.  It’s a good card at 1440p aswell and it has 12gb of the other 2 8gb cards. Only problem I have it is the windows 10 drivers being abysmal dog shit if you’d want to play any current game with stuttering and crashes.,Positive
Intel,"Yeah, apparently a year’s worth of it",Neutral
Intel,Because they're super late to the party.,Negative
Intel,"Wow, 800-900USD after 5 years is more than I would have expected.",Positive
Intel,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",Negative
Intel,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,Negative
Intel,For what exactly do they need vram without cuda ?,Neutral
Intel,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",Negative
Intel,Gamers are not niche. They are 20 billion a year business.,Neutral
Intel,"False, they’re releasing in december or jan. So about 3 months from now",Neutral
Intel,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",Neutral
Intel,"The 3090 Ti did, but the standard 3090 did not.",Neutral
Intel,"It's a really good general purpose card. Perhaps not the best card if you want to squeeze out the most fps out of games, but it performs really well in a wide range of tasks including gaming. And yes 12GB VRAM really comes in handy.  It can easily be found for under $250. B580 is still cheaper than 5060 and 9060 XT and even RTX 5050 and RX 7600, especially in the UK where you can find a B570 for only 20 pounds more than RTX 3050. That's way cheaper than RX 6600, it's a no brainer. But the prices could only go down thanks to economy of scale mostly. It's been produced for a while, that's why.  Also one of the big reasons it's still selling now is thanks to good publicity. Imagine if it came out as the same time as RX 9060 XT. HWU would tear a new hole regardless of how much of an improvement it is over the Alchemist cards and the impressive productivity performance. Those guys literally don't care about anything other than pushing a dozen more frames in a dozen personally chosen games. Gamer Nexus and Igor too to some extent. It would be bad publicity.",Positive
Intel,"If intel felt they could have released this safely earlier, they would have",Neutral
Intel,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,Neutral
Intel,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",Neutral
Intel,3090s will be much less attractive when the 5080 super and 5070ti super launches.  If the 5070ti super is 750- 800 with 24gb for a new faster GPU with a warranty it hel used market will have to shift.  The 3090s have nvlink but I'm unsure how much that matters for most people.,Neutral
Intel,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,Neutral
Intel,"rendering, working on big BIM / CAD models, medical imaging,...",Neutral
Intel,CUDA isnt the only backend used by AI frameworks,Neutral
Intel,"I use opencl for doing gpgpu simulations, this card would be great for it",Positive
Intel,You mean your 0 profile history because you have it private?,Neutral
Intel,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",Negative
Intel,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,Negative
Intel,Spoken like a true gamer.,Positive
Intel,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,Negative
Intel,You still get about 85% performance compared to stock settings.,Neutral
Intel,"Of course, but that’s the point. It took them an excessive amount of time to “feel” like they could have released this",Negative
Intel,What are they doing that's making them money? Or are theu selling the compute somehow?,Neutral
Intel,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",Negative
Intel,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",Positive
Intel,">rendering  Rendering what? Because if you're talking from a 3D art perspective, all our software is leaps and bounds better with CUDA or outright requires it. You can't even use Intel GPUs with Arnold or Redshift, so good luck selling your cards to a medium scale VFX house that heavily use those render engines",Neutral
Intel,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",Neutral
Intel,>CUDA isnt the only backend used by AI frameworks    It is not the only backend for all AI frameworks; it is the only backend for the majority of AI frameworks.,Neutral
Intel,It is the only functional one.,Positive
Intel,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",Positive
Intel,0 people are using autodesk with an intel arc gpu lmao,Negative
Intel,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",Negative
Intel,"Oh no, im part of 60% of world populatioin. how terrible.",Negative
Intel,Which is 36% higher performance per watt.       ... to be fair.,Neutral
Intel,The reason they didn't is because it would have been worse to release it earlier,Negative
Intel,"Gamers Nexus has a documentary on the GPU smuggling business, where even the 3060 12GBs are being used: https://www.youtube.com/watch?v=1H3xQaf7BFI  The 4090s, 3090s and other cards? There's such a demand for those in China that people will buy them up and smuggle them.",Neutral
Intel,Software Development,Neutral
Intel,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,Negative
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"Depends on what software you use, but your right that NVIDIA with Cuda is safe bet when is comes to rendering. I use Enscape and that works great in my AMD gpu",Positive
Intel,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",Positive
Intel,Which ones?,Neutral
Intel,Private profile = Complete troll.  You're not an exception to this rule.,Negative
Intel,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,Negative
Intel,"Right man, my point is that it shouldn’t have been",Negative
Intel,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,Neutral
Intel,"Hell, one of them was just the cooler without the actual GPU.",Negative
Intel,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",Neutral
Intel,for example every local image and video generation system I've seen.,Neutral
Intel,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",Negative
Intel,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",Neutral
Intel,According to whom? With what evidence?   Intel spends more on r&d per year than nvidia and amd combined   Do you think they are just being lazy?,Negative
Intel,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",Neutral
Intel,Getting good LLMs crammed onto GPUs for coding is a popular topic on /r/locallama.,Positive
Intel,And the one directly under that was the actual PCB...,Neutral
Intel,Im running Comfy UI + Flux on my Strix Halo right now without issue 🤷‍♀️,Positive
Intel,Propaganda is 90% of chinas economic output though.,Negative
Intel,"> Neweggâs  I know this is mojibake, but this kinda sounds like a Lithuanian versions of Newegg lol",Neutral
Intel,The competing product is sometimes slower while also being twice the price.  If this wasn't a success then Nvidia would be  unbeatable,Negative
Intel,"What's the word on the B60? Even more VRAM (24GB), and double the memory bandwidth. I see it listed as ""released"" in various places, but can't figure out where to actually buy one.",Neutral
Intel,"My RTX a4000 doesn't support SR-IOV. I don't know about newer series, but at the time you had to buy the A5000($2500) or A6000 and then there are some crazy licence fees to use it.  For 350 i will buy it when it gets available just for this.",Negative
Intel,L1 techs had a great feature on these.,Positive
Intel,"Profitable product for Intel, wouldn't suprise me if Xe3P and onwards for dGPUs happens because stuff like this can do easy returns.",Positive
Intel,1:4 ratio of FP64 performance is a pleasant surprise,Positive
Intel,"Honest question here: what makes it a ""workstation gpu"" that does it differently than say like a low end 5060/AMD equivalent?   Iis it just outputting 1080p ""faster""?",Neutral
Intel,"Its also just a whole 95 cards sold. (Past month, I’m unsure if its been up longer)",Neutral
Intel,"It will never be in stock again. It’s good for AI, hosting pass through srvio to VMs without licensing and a number of other things outside of gaming.",Neutral
Intel,"Hello 79215185-1feb-44c6! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Let me know when it shows up on the steam hardware survey. That's the only barometer for success that true hardware enthusiasts care about.,Positive
Intel,what's up my Neweggâs,Neutral
Intel,My favorite game  Fallout Neweggas,Positive
Intel,It also sounds like an ikea lamp name or something,Neutral
Intel,Geriau nei Bilas Gatesas,Neutral
Intel,I gotta spell Neweggâs?!?,Negative
Intel,"The B50 appears to be an decent low-end workstation GPU, at least as long as the intended workloads don't effectively require CUDA in any way, shape, or fashion.    My one lingering question is what use cases actually *require* the certifications of a workstation-class GPU (which would rule out something like a relatively similar consumer-tier RTX 5060 Ti / 16GB) but wouldn't benefit from CUDA?  Then again I'm not exactly an expert in the field, so I could be completely off-base here.",Neutral
Intel,"I'm disappointed in Nvidia's inability to put out a stable driver since last December, I'm waiting to see if a competitor card will come out that meets my wants for an upgrade.",Negative
Intel,Intel might be using the B50 as a pipe cleaner for the B60's drivers to prepare it for a retail launch in Q1 2026    IF they're doing this then it's a sound strategy,Positive
Intel,"Double the memory bandwidth of trash is still trash.  Edit: Y'all can downvote me all you want, but 250GB/s is just slightly more than the 200GB/s of my low-profile 70W GTX1650 GDDR6 that I bought for €140 in 2019. Its absolutely pathetic and should be unacceptable for a new product in 2025, let alone a product of $350 !!!. Even double of this (\~500GB/s) of the B60 is less than a RTX3060. Pathetic products.",Negative
Intel,SR-IOV is the selling feature for me and why I have one ordered. Getting a Tesla P4 with nvidias vgpu licensing working is a pain in the ass and expensive.  I'll get it and sit on it until SR-IOV is released in case of scalpers/stock issues. If it doesn't pan out I'll either just sell it on or drop it into my home media server for the AV1 encoding/basic AI stuff.,Neutral
Intel,"Last time I checked GRID licensing can be faked out, but yes, only Quadro/Tesla and Turing/Pascal(IIRC) through driver mods can use Nvidia's vGPU.",Neutral
Intel,"The professional market is smaller than gaming and even more slanted towards Nvidia. This might be a nice side business, but can't remotely justify developing these cards.   Not even clear it's profitable either. The numbers here are negligible so far.",Negative
Intel,"Do people actually need and use FP64 at all anymore? I've got one or two original Titan cards that I haven't thrown out although I've never used them for this purpose either, because they apparently have very high FP64 numbers and if I recall correctly can operate in ECC mode as well.",Neutral
Intel,"iirc, SR-IOV and VDI support in the coming months, toggleable ECC support, and it is ISV certified",Neutral
Intel,"So I spec our PCs at work. We do anything from traditional office work, to intense engineering tasks. On our engineering computers we run MatLAB, Ansys, Solidworks, MathCAD, LTSpice, Xilinx, Altium and other such apps. Lots of programming, VMs, design work, simulation testing, number crunching, and on occasion AI work.   This means we spec systems like with RTX Pro 500, RTX Pro 2000, RTX A4000, A4500, A6000s. The reason we have these rather than cheaper GeForce cards is mostly 3 things. Power/form factor, Driver certification, pro GPU features.   So typically Nvidia keeps the top binned chips for their professional cards meaning the power efficiency to performance is top tier. So we can get high performance single slot or low profile cards, or get some serious GPU performance in relatively small laptops. Drivers usually are validated better than the GeForce drivers, so they include better bug testing, and the apps we use validate performance with the cards which helps us evaluate performance. They also have way more vram like the RTX A4000 has 20GB of vram while being just a supped up 4070. Then from a feature perspective they have better VM passthrough support, or you can enable the vram to run in ECC mode for error correction. Very important when running 24-48 hour simulations.",Neutral
Intel,Software support is a thing. CAD applications like solidworks and inventor don’t officially support the GeForce rtx or radeon rx line of gpus and they’re considered untested unsupported options. You can’t get any tech support if you’re using them. For a business that needs those apps you need a workstation gpu. They also come with ECC vram,Negative
Intel,ECC memory.,Neutral
Intel,"That kind of puts it into perspective.  Also, let my take a guess:  Newegg sells them well because of how dirt cheap they are, people buying actually expensive Pro cards will more likely do it directly via their system integrator.",Neutral
Intel,Oh when they get enough enterprise customers they will definitely charge licensing fees,Neutral
Intel,How many A2000s show on the hw survey? Because that's the Nvidia variant and it has been around for a long time..,Neutral
Intel,Lmao,Neutral
Intel,Komentarą skaitai per tapšnoklį ar vaizduoklį?,Neutral
Intel,I am no expert but don't these gpus have ECC vram. That's a enough to get labs/professionals to buy them.   You don't want the headache of lacking error correctiin in a professional environment.,Negative
Intel,"I seriously considered getting one for my homelab. I would really like some SR-IOV, and giving multiple VMs access to transcoding would be very useful. Ultimately decided against it because at the moment my CPU alone is powerful enough, I have other uses for the PCIe slot, and I would have to import one. But it's something I'm going to check in on whenever I'm browsing for new hardware from now on.",Positive
Intel,"I know in my field of work, solidworks certified hardware is one such application where certain features are gated behind workstation class cards.",Neutral
Intel,Vulcan does fine with inferencing.,Neutral
Intel,"all the professional graphics stuff is where this can matter, i.e. CAD, large display walls, traffic control, video studios etc",Neutral
Intel,people are buying B60's  https://www.reddit.com/r/LocalLLaMA/comments/1nesqlt/maxsun_intel_b60s/,Neutral
Intel,"Most Zen-1 parts had much worse single core performance than Kaby Lake,    People still cheered on the competition anyway despite it's shortcomongs",Negative
Intel,"GTX 1650 has only 4GB of RAM at 128 GB/s; RTX 3060 is only 360 GB/s, and only 12 GB--or maybe just 8 GB for some cards--of RAM. But thanks for playing.   Edit: relevant username. Up voting you for jebaiting the crap out of all of us.",Neutral
Intel,you really dont want to fuck around with software licensing as a business. vendors do inventory audits to ensure nobody's exceeding their license allocations. piracy would automatically invite a lawsuit.,Negative
Intel,Grid licensing can be faked if you depend on a sketchy github driver that only works on Turing GPUs. You certainly don't want to be doing that in a professional setting where licensing costs are not a massive expense anyways.,Negative
Intel,"I believe mobile is the main reason they continue developing ARC IP, highly integrated SoC are crucial for lower power consumption and performance per watt, as more and more mobile designs are becoming more integrated (see strix halo for example) Intel knows it has to continue developing graphics IP that is competitive with competition. As for discrete cards, this is a battle in the long run to win, but it will take serious investment, we can hope that they won't axe as part of cost cutting measure.",Positive
Intel,"The B50 (16Xe cores) is pretty cut down compared to the full G21 (20Xe)die, it has 2600mhz boost clocks instead of the 2850mhz on the gaming cards, it uses 14GB/s memory (19Gbps on gaming cards) and it has a 128bit bus with 8 memory chips (B580 has 192bit bus with 6 memory chips)  The only costly thing about is the 2 additional memory chips.   I'm not saying it's extremely profitable but it can't be too expensive to make since a portion of the volume is likely faulty G21 dies that can't make a B580 or B580.   If Intel can sell the B580 for $250 without too much pain, then the B50 is probably making a profit",Neutral
Intel,"Yes, to the point where I’m considering picking up a Titan V on eBay. It’s a must for scientific computing, single precision floats accumulate errors fast in iterative processes.",Positive
Intel,I recognize those as words...,Neutral
Intel,I think it was obvious I was being facetious.,Neutral
Intel,> You don't want the headache of lacking error correctiin in a professional environment.  I think Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU. That was the explanation that I got back in the university years when the IT department would use the cheapest possible professional GPUs instead of high end consumer GPUs.,Negative
Intel,"Yeah, if ECC is a hard requirement for whatever reason then that would certainly rule out all the GeForce-branded RTX cards.    Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a [RTX PRO 2000 Blackwell](https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-2000/) instead, which fits the same niche as the B50 (i.e. low-profile 2-slot @ <75w with 16GB of VRAM) while being faster and having a far superior feature set.",Neutral
Intel,I’ve been working in a professional aechtiectural environment for 5 years and haven’t seen the need for ECC once.   Can you explain situations where it’s needed? I’ve always wondered.,Neutral
Intel,"SR-IOV for (license-free!) vGPU is IMHO the killer feature here, perhaps along with being able to get 16GB of VRAM per card relatively cheaply and without needing auxiliary power.  Both open up interesting server and workstation use cases that can't be had cheaply from the competition.",Positive
Intel,Have you tried GPU paravirtualization?,Neutral
Intel,"You can buy it off AIB Partners but you can't buy it at retail (i.e. microcenter, newegg] and it doesn't have an official MSRP yet.   The prices you see now are what AIB's want to charge in bulk orders.    If you want to know how much let's say 5 B60's cost you have to get a quote from a distributor",Neutral
Intel,They need GPU IP for two things: client and AI. Anything else is expendable.,Neutral
Intel,"Yes, my point was *if* they have the gaming cards, they can justify the professional line, but it's not nearly big enough to justify making a dGPU to begin with.",Neutral
Intel,"SR-IOV is Virtual GPU (SR-IOV is IO Virtualization used to split PCIe lanes into virtual functions so their physical function can be shared between VMs). No consumer cards support Virtual GPU right now besides Pascal/Turing with driver hacks. AMD's SR-IOV offerings are [very limited](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#virtualization-support), [And Nvidia has a bigger selection](https://docs.nvidia.com/vgpu/gpus-supported-by-vgpu.html) but their budget VGPU options are being phased out (P40).  I believe VDI is Microsoft's implementation. I believe I've done VDI on my RTX 2070 before (I have done seamless sharing between host and VM), but I don't know if it's possible with AMD. Someone please correct me if I'm wrong here, I'm more familiar with the Linux side / vGPU than VDI.  ECC is Error Correcting RAM. I generally don't understand the use case for ECC either, but it is ubiquitous in HPC. All server boards support ECC RAM.  In modern environments most of these features need 16GB of VRAM minimum, but if you ever wanted to try it on a consumer card, you could get an old RTX 20 series and try it out with some driver mods. Optionally, the P40 is still pretty cheap ($250 used) and doesn't need those hacks at the cost of drawing a lot of power, which Intel has solved with their Battlemage Pro platform (by far the cheapest VRAM/$/W you can get).",Neutral
Intel,>Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU.   A380 and A770 is also on the certified gpu list. But otherwise that statement is correct.,Negative
Intel,Can you buy an rtx pro 2000?  If i had to guess what percentage of wafers are for b200 chips i would say 90%.  I don't think there are enough pro 2000s around. I don't think there are enough gpus around in most cases.,Negative
Intel,"> Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a RTX PRO 2000 Blackwell instead  It is interesting. IDK how common it is, but one of my university labs had computers donated from nvidia with nice quadro GPUs for their time.",Neutral
Intel,Pathetic 1:64 ratio of FP64 flops,Negative
Intel,RTX 3090 Ti and RTX 4090 have ecc. Not that they're cheap.,Neutral
Intel,"It's needed whenever the work you're doing matters and a single-bit error could cause significant harm.  Something like audio or video, a single-bit error probably isn't very noticeable.  Calculations, it absolutely depends on what you're calculating and which bit gets flipped; flipping the low-order bit in a number might not matter much and flipping the high-order bit could cause a big error.  Networking, it depends on whether the protocols you use have more error checks at a higher level (TCP does; UDP does not).  If in doubt, you want ECC, but market segmentation to mostly restrict ECC support to ""server"" chips and boards and charge more for ECC memory means you'll overpay for it.",Negative
Intel,"They're so good, I wish there was a single slot variant.   I want to put them in my MS-01s. The Sparkle A310, being the main candidate for deployment in those machines, only has 4GB and its maximum h264 encoding throughput actually drops below the iGPU (although its h265 and AV1 throughput slaps the 12900H/13900H). It's just a little too low to comfortably handle the Plex server usage I have, so the iGPU remains in service until a suitable competitor arrives.",Positive
Intel,"IIRC, that requires a Windows host right? That's a non starter for many people unfortunately",Negative
Intel,my guy sr-iov is a type of gpu paravirtualization.,Neutral
Intel,The Asrock b60s are $599,Neutral
Intel,AI doesn't even need a GPU; it can have its own accelerators - see Gaudi.,Positive
Intel,"One use case for ECC, is when the data is critical and can’t be lost.",Neutral
Intel,>  I generally don't understand the use case for ECC either  Its for when you don't want errors to just be ignored?   How is that hard to understand?,Negative
Intel,ECC should be in literally all memory.,Negative
Intel,"\>but their budget VGPU options are being phased out (P40).     I mean, the T4, L4 , and A16 exists...     I'm also not sure why low end workstation GPU needs SRIOV support.",Negative
Intel,I see them at least available in my stores. although mostly as backordres via remote warehouses but they seem readily available with some shipment time.,Positive
Intel,"but you can still do gpu paravirtualization without sr-iov using Mediated Passthrough, API Forwarding (RemoteFX) or Dedicated Device Assignment",Neutral
Intel,"Where can you get them? And are they for sale yet, or pre-orders, or...?",Neutral
Intel,"The problem with Gaudi (I know, I've written code and run training runs on it) is simply that the programming model is not oneAPI, or whatever oneAPI becomes. Yes, pytorch works, but people care a lot about software longevity and long term vision when buying $5mm+ of GPUs (and these are the purchases Intel cares about that can actually start to offset the cost of development).   The whole purpose behind Falcon Shores (and now Jaguar Shores, if it will even happen) is to put Gaudi performance (i.e. tensor cores) in an Xe-HPC package. Unifying graphics and compute packages is what NVIDIA was able to achieve but not yet AMD, and it's really great for encouraging ML development in oneAPI.  See this post to see where Intel would like to be: https://pytorch.org/blog/pytorch-2-8-brings-native-xccl-support-to-intel-gpus-case-studies-from-argonne-national-laboratory/ (they don't mention the ""XPU"" because it's Ponte Vecchio, which are iiuc worse than A100s).",Neutral
Intel,"Intel can't get people even in an AI shortage. No one wants to deal with an ASIC. That's why their AI solution is GPUs, starting with (hopefully) Jaguar Shores. So it's that or bust.",Negative
Intel,I spit my coffee reading that. Gaudi? The platform that nobody uses that Intel has to revise their sales estimates down each half quarter?,Negative
Intel,"Yup. For example you are doing a structural integrity physics simulation, and a single flipped bit can ruin your 1 week long run (and your liability insurer will reject your claim, a lot of them have standards requiring calculations to be done only on ECC for sensible reasons).",Negative
Intel,"Great example of why certain people shouldn't reply if they don't have knowledge in the area.  - Tesla T4 is $650 Used and has 16GB of VRAM. - Tesla L4 is $2000 Used and has 24GB of VRAM. - Tesla A16 is $3000 Used and has 64GB of VRAM.  Compared to:  - Arc Pro B50 is $350 new and comes with 16GB of VRAM. - Tesla P40 is $275 used and comes with 24GB of VRAM.  If all you care is vGPU / VDI for a small amount of hosts, then no, you're not getting a Tesla A16. What kind of joke suggestion is that?",Neutral
Intel,"Mediated pass though requires big ass license fees vGPU/MxGPU, and isn't FOSS other than Intel's currently broken support that they abandoned for SR-IOV support.  API forwarding only support limited host/guest setups, and even more limited API support. The only FOSS support is VirGL, which only support Linux host/guest and only OpenGL.  Obviously fixed pass though is an option, but even that isn't without issue. NVIDIA only recently removed the driver restriction, they could add it back at any time. Plus you are limited on virtual machine by the physical GPU count. It works with Intel GPUs and is FOSS with them.  SR-IOV on Intel fixes all of that. It works amazingly well with their iGPUs, has no license issues, and is fully FOSS.",Negative
Intel,Hey no need to be aggressive towards the other user. Your comments are very helpful and I appreciated them a lot but keep it constructive please!,Positive
Intel,"LMAO, I actually have quite a bit of knowledge in this area.  If all you care for is VDI for a small number of VMs, then you'd go GPU passthrough. vGPU / MxGPU often requires higher levels of hypervisor software tier (i.e. VMware vSphere Enterprise Plus), requiring more money. For KVM hosts, setting up vGPU is a lot more difficult and time consuming than just straight up GPU passthrough.  Only two groups of people would be interested in GPU virtualization / splitting:  * Enterprise, in which they wouldn't care about the used card prices.  * Enthusiasts, in which they wouldn't want to pay for vGPU prices anyway. So why bother catering to this crowd?",Neutral
Intel,"Full GPU passthrough is not a solution that many people would consider because it is clumsier than using sr-iov (or potentially VirtIO GPU Venus). Plus for each extra passthrough instance I would have to add in another GPU and this greatly increases power consumption, heat output and cooling requirements. The process is not all that much more complicated at least on Turing GPUs with a hacked driver on KVM guests at least. Plus for passthrough, you probably still need an NVIDIA card because last I checked AMD cards still had a random kernel panic issue after being passed through.  My assumption is that sr-iov on the b50 will allow users an affordable way to have multiple guests on one host GPU without increasing power draw and paying for expensive alternatives and expensive vGPU subscriptions.",Negative
Intel,"...first time I heard people prefer SRIOV over GPU passthrough because it's ""clumsier"" lol. I'm sure setting up mdev devices in KVM, finding the correct corresponding GPU instances, making them persistent through reboot, then edit virsh xml for each individual VM is a lot easier than just doing IOMMU passthrough. /s     Again, enthusiasts don't care about power consumption / heat output / cooling requirements for their lab environment. Enterprise that do care about them are very willing to pay extra cost to get a production ready driver. You're creating a hypothetical situation that simply does not exist in the real world.",Neutral
Intel,"It's good they launching this, this card adds some competition to the landscape, but before anyone buys it they should figure out if their drivers are lighter weight.  It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  The more ARC cards out there the more developers get familiar with them, the more XeSS v2 gets added to titles, the more the drivers get matured and the better future ARC cards will be.  I'd happily pick up an ARC card... once they've proved themselves in terms of driver maturity and overhead.",Positive
Intel,This should be a great upgrade from my A750 if B580 performance is anything to go by. I hope it's under $400.,Positive
Intel,"Unless it's super cheap, it's not gonna be sold well at all.  Even here with all the ""enthusiast"" and people are saying ""make sure you have this hardware combo, that driver, these settings,..."". The average buyer would just simply pay 50$ more for an nvidia card and not have to worry about all that.",Negative
Intel,I hope they fixed the drivers CPU overhead problem or that GPU's gonna need a 7800X3D or 9800X3D to feed it fully.,Neutral
Intel,Too late for me I already went with a 9060xt but hell I had dreamt of it!,Negative
Intel,"I wish they'd get the drivers past the point of frequently broken, but also they haven't produced enough cards for any previous launch to make any dent in the market regardless.  It's pretty much guaranteed the upcoming super refresh will make much more of a difference in terms of bang for your buck.",Negative
Intel,The problem with arc is you need the latest and greatest CPU to go with or you lose 1/4 of performance,Negative
Intel,Intel has the ball in it's court   If you released a New GPU..  that is pretty much a 5070.... add on 24gb of ram...  and price it at 399   u will   make  boatloads.  it will play pretty much any game at max settings at 1440p..  They must really be  hating on turning down sony though at making the SOC for the PS6 cause the margins too low..they really would need that money now lol,Neutral
Intel,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Depending on the price I might give it a shot.,Neutral
Intel,Love it going to get one if I can scratch some money together,Positive
Intel,Wouldn’t there be a risk that future drivers will not be supported and that it comes with US government back doors?,Negative
Intel,"LOL, preparing ""box"" packaging   I immediately thought of advanced silicon packaging like CoWoS or whatever",Neutral
Intel,Who is gonna tell them G31 would be celestial die since B580 was G21 and A770 was G10?,Negative
Intel,"Ah yes, finally the 4060ti 16gb/4070 killer, only 1.5 years too late! Ig at least this will force price drops on the rx9070",Positive
Intel,Aren't the driver overhead issues really only seen on older processors that are six cores or less?  Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course,Neutral
Intel,"> It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  Is it really though? Powerful CPUs are comparatively cheap, powerful GPUs are expensive. I know plenty of people who went with a 9700X/7700X or even 9800X3D but 'cheaped out' on the GPU because spending $1200 on a 4080 (at the time) was simply too much.",Neutral
Intel,"I agree with everything you said.  However, I myself will buy one just because I want more competition and so I am just going to give Intel a sale. Sure it doesn't move the needle much, sure Intel's probably not going to make any money out of it and I personally probably won't use it much, but I am just doing it out of principle. I sure am in the minority, but at this point I can't sit idle and allow this duopoly to continue without trying something.",Neutral
Intel,"It's a hardware problem, not a driver problem. The cards have insufficient DMA capabilities and uploads must be handled by the CPU, no driver will fix it, and as a consequence the B770 will have even worse overhead.",Negative
Intel,I have a B580 and the driver seems pretty stable to me at this point.,Positive
Intel,"The super refresh is only for 5070, 5070ti and 5080. I doubt the B770 will compete with the 5070 to begin with so those cards are more upmarket.",Neutral
Intel,my b580 has been stable,Positive
Intel,"I’ve had a B580 for 6 months and have experienced one game-specific issue with Tarkov. Everything else, old, new or emulated has worked fine.",Positive
Intel,"I’ve been using Arc on both windows and Linux since alchemist, it’s powering 3 rigs for gaming, transcoding, etc.   Initial few months was rough but drivers are absolutely serviceable and have been for a while, and continue to get better each release.  I play lots of different games on steam btw, very rarely do I have issues.",Positive
Intel,to our knowledge... i wonder what kind of uplift we'll see it have with next gen cpus,Neutral
Intel,It's BMG-G31.  https://videocardz.com/newz/intel-confirms-bgm-g31-battlemage-gpu-with-four-variants-in-mesa-update  https://videocardz.com/newz/intel-ships-battlemage-g31-gpus-to-vietnam-labs-behind-past-arc-limited-edition-cards,Neutral
Intel,"It isn't the number that determines the generation, it's the prefix.  A770 was ACM-G10 (alchemist G10), while the B580 is BMG G21 (Battlemage G21). The shipping manifests that have been floating around for the better part of a year have been for the BMG G31. Unless new leaks I'm not up to date with are discussing a G31 with a different prefix, everything points towards it being battlemage, not celestial.  Now I pray that Intel have found a way to mitigate the driver overhead. If not, the B770 will be utterly useless for gaming. Nvidia is bad in the overhead regard, but the B580 is damn near an order of magnitude worse.",Neutral
Intel,> Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course  Not even close.,Negative
Intel,"HUB showed the b580 lost like 20%+ perf between the 9800x3d and the 7600 or 5700x3d and actually fell behind the 4060, as the 4060 lost minimal performance on the weaker cpu vs using the 9800x3d. And the 7600x and 5700x3d can certainly power much stronger gpus like the rtx 5070 without bottleneck.  Edit: my bad, I didn't know it was for only a specific game, though still not a good result for b580 overall",Negative
Intel,Where do you find such information?,Neutral
Intel,Source: I made it the fk up,Negative
Intel,"I was going to say, I put my sister in a b580 and she has had no driver issues in 6 months.",Positive
Intel,"That's sort of my point, they'll probably still exert more price pressure across the stack than the b770, despite being a totally different segment.",Neutral
Intel,https://youtu.be/00GmwHIJuJY?si=z4wU05sJx2SeS7K1  How can people get on here and lie and literally no one questions them? The 7600 only lost anywhere near that performance on Spider-Man Remastered,Negative
Intel,"Inference, there was a blog post tracing and comparing what the Arc driver does with the Radeon driver. The radeon driver just sends a few pointers to buffers, the Arc driver sends large amounts of data. Assuming the driver programmers at Intel aren't idiots, it's because something is seriously wrong with the cards and DMA.",Neutral
Intel,"No, I inferred it from tracing what the driver does, and assuming the programmers aren't idiots.",Neutral
Intel,Try Mechwarrior 5: Clans on high and say there are no problems again.,Positive
Intel,"that game runs on my 3080 ti like ass, just like all early UE5 games...  even with RT turned off, to hit solid 4k60 I needed DLSS and if I wanted 90+ fps I need to use DLSS performance / ultra performance.",Negative
Intel,Steve from HUB and Steve from GB both lack the technical knowledge to look into the underlying issues,Negative
Intel,"It doesn't even run, it crashes left and right on an Arc.",Negative
Intel,"that I have no idea, on launch I did have crashing issues on my 3080 ti, but they did get resolved over time.  but if you are now seeing it still then welp  PGI is a small team that may not have gotten help / getting to it themeslves to make their game arc capable.",Negative
Intel,"Yeah, still doesn't work at the latest patch (and latest Arc driver) with anything other than low.",Negative
Intel,SR-IOV at that price. Who cares about anything else.,Neutral
Intel,Intel would be stupid to axe there Graphic card division if this proves to be successful.,Negative
Intel,"Single-slot variant or custom cooler please, my MS-A2 running proxmox is demanding this card.",Neutral
Intel,About 66% overall performance of a B580 it looks like. That's really nice for a 70W card.,Positive
Intel,"This is exciting, definitely looking forward to the b60 as well.",Positive
Intel,"Obligatory ""Intel is exiting the GPU business any moment now"".",Neutral
Intel,how hard are tehse to actually buy?,Neutral
Intel,"Buying one, this is impressive",Positive
Intel,"if compared by price to performance ratio,  ARC B50 is slower than RTX 5060 in terms of price and performance",Neutral
Intel,Its better than a 1.5 year old bottom of the range card....well done i guess.,Positive
Intel,Better than NVIDIA? lol .... oooookay,Positive
Intel,"Haven't seen the video, but I'm already buying one if that's the case",Neutral
Intel,Literally it could be a damn arc a310 or rx6400 and people would buy that card at $350 without licencing bs. For anything VDI related the B50 is huge.,Negative
Intel,Intel’s “MOAAAAAR CORES” in the GPU space???,Neutral
Intel,what is that? SR-IOV?,Neutral
Intel,Super interesting!  Wonder how well it would handle AI tasks like Frigate while another VM uses it for inference or a third doing video transcoding with Plex.,Positive
Intel,16 GB VRAM too,Neutral
Intel,They will eventually axe it.,Negative
Intel,Instead of axing it maybe spin it off like AMD did with Global Foundries?,Neutral
Intel,And if it isn’t successful?,Negative
Intel,They do have 3rd party vendors for ARC PRO Cards this time around so it most likely will happen.,Neutral
Intel,"The B60 is more exciting to me just for that 24GB VRAM. Still, at this price point the B50 is a pretty compelling buy tbh.",Positive
Intel,I think it would be really stupid for them to do so.,Negative
Intel,You can preorder from newegg now. They ship later this month.,Neutral
Intel,"One Swedish retailer I checked has them coming into stock next week (10 September) and open to orders, how much stock there will be however, I have no clue.",Neutral
Intel,Same. I put my preorder in. Plan to put it into one of my sff builds.,Neutral
Intel,Why is this impressive for $350 USD? How will this be useful for you? I’m not being sarcastic. I am genuinely curious.,Neutral
Intel,What did bottom of the range cards cost 1.5 years ago?  How much VRAM did they have?  Did they support SR-IOV?   Just think for a bit sometimes.,Neutral
Intel,It quite literally is. Watch the fucking video.,Negative
Intel,"Wendell confirmed as much in the comments, looking forward to his future testing of the card.",Positive
Intel,What does AMD have in this product segment?,Neutral
Intel,"Ingen av de kortene greier LLM, hvis du tror det.",Neutral
Intel,"Ingen av de kortene greier LLM, hvis du tror det.",Neutral
Intel,"I didn't know either so I looked it up.   ""SR-IOV (Single Root Input/Output Virtualization) is a PCI Express technology that allows a single physical device to appear as multiple separate virtual devices, significantly improving I/O performance in virtualized environments by giving virtual machines direct access to hardware. This bypasses the overhead of a software-based virtual switch, resulting in lower latency and higher throughput for demanding applications by dedicating virtual functions (VFs) to guest VMs.""",Neutral
Intel,Everyone (including Nvidia) is moving toward APUs with large GPUs onboard. Why would Intel kill their chance at competing in that market?  They've already withstood the most painful part of the transition. There's no point in stopping now.,Neutral
Intel,They are keeping their Fabs which is even more expensive to maintain why would they sell GPU not to mention their iGPUs are pretty Damm good nowdays not like meme in Intel HD4400 even though they could play any game /jk.,Neutral
Intel,Doubt they have the revenue to spin out successfully without significant investment from outside sources.,Negative
Intel,Sadly Intel has a recent history of making poor life choices.,Negative
Intel,"Maybe it's just me, but this reads as AI generated.",Neutral
Intel,"I dunno man, I was building a PC for work and the 3050 was the cheapest Nvidia card I can get and the 7600 is the cheapest from AMD. Huge price gap between the two, by about 100 USD. AMD really needs to buck up their APUs to render cheap GPUs surplus or have something cheaper than a 7600 to price match the 3050.",Neutral
Intel,"They're comparing it to an entry-level NVIDIA GPU, the A1000. Saying that Intel GPUs are ""better than NVIDIA"" as a universal statement is flat-out wrong. Let's see some competition to the RTX 5070 Ti, 5080, or 5090. NVIDIA has zero competition on mid-range and high-end GPUs.",Negative
Intel,Radeon Pro V710 and you can't even buy it retail.,Negative
Intel,thanks 🙏,Positive
Intel,Because intel shareholders are super short sighted.,Negative
Intel,this comment is the weirdest version of 'corporations are people' that i've encountered,Negative
Intel,Lmao seriously the formatting and the amount of bolded words just screams AI,Negative
Intel,It's AI generated in your mind,Neutral
Intel,"because AMD has a bad habit of leaving a bunch of their older cards in the channel and having them become the low end...  CPU and GPU, AM4 lives for so long because there are still piles of the stuff in the channel and just gets slow tiny discounts till its gone in full  its like their demand forecast is too optimistic or something but at this point I think its deliberate",Negative
Intel,Because this is not a gaming GPU and thus the A1000 is the correct card to compare with.,Neutral
Intel,Good luck using those super gpus to host multiple gpu accelerated vm with one card. Nvidia won't let you.,Negative
Intel,"Yes, compare an Arc Pro to a GeForce, totally the same market.",Neutral
Intel,"That seems more mid or high tier rather than these relatively low tier gpus, the b50 is a cut down b580...  Also the v710 seems like the kind of ""passive"" gpu that's ""passive"" as long as it's next to several 7,000 rpm fans.  So it would probably not work very well as a retail car because it's rack server focused.",Negative
Intel,"I think the really loud short sighted shareholders have quieted down a bit after it became clear they helped drive the company to where they are. Hell, they're probably not even shareholders anymore.",Negative
Intel,The weirdest version was Citizens United,Negative
Intel,Did you not figure out why they’re bolded?,Neutral
Intel,I'm aware but it's the only current gen GPU for graphics workloads that has virtualization support from AMD.,Neutral
Intel,"The current Chairman of the board, Frank Yeary, is one of these stupid short sighted people. He REALLY wants to sell the fabs, and is probably the reason Intel went through their latest round of layoffs (Lip-Bu Tan wanted to raise money from Wall Street, Yeary apparently sabotaged it).",Negative
Intel,"if corps are people, they should be allowed to vote, right ?",Neutral
Intel,"Not only that, but because they are people, they should also be able to fund gigantic super PACS to get a candidate into office. I love America!",Positive
Intel,"u/michaellarabel It would be super cool to have Molecular Dynamics benchmarks for these kind of cards, since you already use them for CPU testing and a few of them (e.g. GROMACS) support APIs from all three vendors (CUDA, ROCm, SyCL, + OpenCL)",Positive
Intel,"I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other, the leader can change in a generation or two, which hasn't been the case often in recent history.  Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.",Positive
Intel,"Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.",Neutral
Intel,Youtube link:  [https://www.youtube.com/watch?v=Y9SwluJ9qPI](https://www.youtube.com/watch?v=Y9SwluJ9qPI),Neutral
Intel,I cannot wait to see what's in store for 2026 Mac Studios and the M5 CPU. Especially if M5 Ultra makes its debut. AI workloads should see a significant performance boost by 3-4x? I wonder if M5 Ultra will offer 1000GB/s memory bandwidth.,Positive
Intel,6 wide e core holy shit,Neutral
Intel,"A major exciting aspect for me is the massive boost to Raytracing performance. The M4 Max is the closest anyone has ever come to matching Nvidia in 3D Raytraced Rendering, beating out even AMD. In Blender M4 Max performs somewhere in between an RTX 4070M and 4080M.  A 56% leap in RT performance would essentially put an M5 Max closer to a RTX 5090M than anyone before at a fraction of the power.",Positive
Intel,"Is there a link for English? If not, can you summarize how they tested sustained performance and how much is the improvement over previous generations?",Neutral
Intel,"The E core having more improvements than just 50% larger L2 is a nice surprise, but damn the efficiency and performance of it is insane. 29% and 22% more performance, at the same power draw is insane, clocking like 6.7% higher too. They used to be behind the others in performance with the E cores but had better efficiency but now they both have better performance and efficiency.  As for GPU, I always wanted them to focus on GPU performance next and they finally are doing it. Very nice, the expected 2x FP16 performance, which now matches the M4 which is insane(M5 will be even more insane). Gpu being 50-60% faster is a nice sight to see. For RT performance(I still find it not suited for mobile but M5 will be a separate matter) I’m surprised that the massive increase is just from 2nd gen dynamic caching, the architecture of the RT core is the same, just basically a more efficient scheduler which improves utilization and less waste.  For the phone, vapor chamber is nice, them being conservative on having a low temperature limit can both be a good and bad thing which is shown, the good thing is that it means the surface temperature is lower so the user won’t get burned holding the device, and the bad thing is that it can leave performance off the table which is shown. As that can probably handle like another extra watt of heat and performance. Battery life is very nice, the fact that it can match other phones with like over 1000mAh bigger battery is funny. As people always flexing over how they have like a 4000, 5000mAh+ battery, of course having a bigger capacity is better, but the fact that Apple is more efficient with it and can have the same battery life at a much smaller battery speaks volumes about it.",Positive
Intel,"The occupancy characteristics of A19 Pro are quite incredible. 67% occupancy for a RT workload.  Look at Chips and cheese's SER testing. 36-44% ray occupancy with SER in Cyberpunk 2077 RT Overdrive.     Assuming NVIDIA can get this working on 60 series an effective a 52-86% uplift. After OMM and SER this seems like the third ""low hanging"" RT fruit optimization. Anyone serious about a PT GPU architecture NEEDs dynamic caching like Apple. And no this is not RDNA 4's Dynamic VGPR, it's a much bigger deal. Register file directly in L1$ has unique benefits.",Positive
Intel,"Nice been waiting for this. P-core frontend improvements and branch, and a wider E-core with it's newer memory subsystem shows great YoY gains as usual. Though I am not surprised since it's been years leading up to this that Apple has steadily have been increasing power/freq to get the rest of it's performance gains, although IPC YoY is still class leading. The wider e-core takes the stage which is now commonly being focused in the industry (ex. Intel: Skymont etc). Excited for any outlet doing die analysis (I don't know if kurnal has done it yet).  Real generational GPU gains, instead of last year's YoY tick. Supposedly GPU size has not increased and that is impressive. Massive FP16 compute matching the M4, really shows their commitment to ML (as if naming 'tensor cores' wasn't obvious) and this will greatly help with prompt processing if you're into local models. Finally with a vapour chamber in the PRO models, performance overall really stretches it's legs and sustained is really respectable.  Also, since I'm skimming, I'm assuming A19 base like it's predecessor is a different SoC to the Pro. It is also really really refreshing to see the base A19 be better than the 18 Pro, little to no stagnation and a year at that. The base iPhone 17 looks like a reaaly reallly good option, more than ever, wished they didn't drop the Plus model. But man, I feel like waiting another year, hearing rumours about N2 and new packaging tech excites me.  That said, looking forward to QC, MT, and Samsung. SD8EG5 seems to be closing the gap, and that'll be very interesting tho those GB numbers don't tell things like power.",Positive
Intel,"""Generations ahead of other ARMs M cores"".   Uhm we are getting the Dimensity 9500 and 8 elite gen 5 next week    The C1 Pro has 20% IPC improvement IRRC, plus this is N3P   Let's not jump to conclusions before seeing the competition    I also wonder if QC made changes to the E cores",Neutral
Intel,>SLC (Last Level Cache in Apple's chips) has increased from 24MB to 32MB  Really tells you how pathetically stringent AMD has been with cache sizes on their apus (no die size excuse allowed here because they never use leasing nodes specially N4 was already old when Strix point debuted),Neutral
Intel,">A19 Pro E core is **generations ahead** of the M cores in competing ARM chips.  >A19 Pro E is 11.5% faster than the Oryon M(8 Elite) and A720M(D9400) while USING 40% less power (0.64 vs 1.07) in SPECint and 8% faster while USING 35% lower power in SPECfp.  >A720L in Xiaomi's X Ring is somewhat more competitive.  No, Apple's 202**6** E-cores are just **+3% (int perf)** and **+1% (fp perf)** vs Arm's 202**5** E-cores, though at **-22% (int)** and **-16% (fp)** less power.  Note: Geekwan's chart is wrong. The Xiaomi O1 does not use the A72**0.** It uses the upgraded A72**5** from the X925 generation. Not sure how Geekerwan got the name wrong, as they recently reviewed it.  Integer  |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%|  Floating point  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%|  I would not call this ""generations"" ahead.",Neutral
Intel,I always wonder how they plot the architecture map and figure out such as the depth of LDQ kind of things...Is it public somewhere? That kind of detail won't be able to get via regular benchmark right?,Neutral
Intel,A19 Pro GPU is now only 1.62GHz vs 1.68GHz in A18 Pro while having the same number of ALUs (768). Does that mean the increased performance is basically due to memory bandwidth increase?,Neutral
Intel,"At this point all the flagship phones are great. I have an iPhone 14 pro max and a Samsung Galaxy S23 Ultra. But I’ve had iPhones for years and i couldn’t switch if i wanted too i have way too much invested in ios (games, apps, music etc). But I ordered a 17 Pro Max 1tb thought about 2tb’s but have a 1tb now and still have 450gb’s free and i download everything and never delete anything. Im trading in the Samsung and giving iPhone to my mom. Still think of buying a cheap phone that gives me the $1100 trade in because it’s any condition and I hate trading in a phone thats practically new.",Positive
Intel,-21 freezer lol https://browser.geekbench.com/v6/cpu/14055289,Neutral
Intel,">Power however has gone up by 16% and 20% in respective tests leading to an overall P/W regression at peak.  That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade in floating point. The 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm:  |SoC / SPEC|Fp Pts|Fp power|Fp Perf / W|Perf / W %| |:-|:-|:-|:-|:-| |A19 Pro P-core|17.37|10.07 W|1.70 Pts / W|84.2%| |A19 P-core|17.13|8.89 W|1.93 Pts / W|95.5%| |A18 Pro P-core|15.93|8.18 W|1.95 Pts / W|96.5%| |A18 P-core|15.61|8.11 W|1.92 Pts / W|95.0%| |A17 Pro P-core|12.92|6.40 W|2.02 Pts / W|100%| |8 Elite L|14.18|7.99 W|1.77 Pts / W|87.6%| |O1 X925|14.46|7.94 W|1.82 Pts / W|90.1%| |D9400 X925|14.18|8.46 W|1.68 Pts / W|83.2%|  These are *phones*. Apple, Arm, Qualcomm, etc. ought to keep max. power in check. This is *o*n par with MediaTek's X925, a bit worse than the 8 Elite, and much worse than Xiaomi's X925.  I would've loved to see efficiency (joules) measured, like AnandTech did. That would show us at least if ""race to idle"" can undo this high 1T power draw or not in terms of battery drain.",Negative
Intel,"All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P. Apple buys first dibs on the wafers so they always have that advantage, it isn't always about the architecture itself.  It will be more interesting when there are Qualcomm chips out with their architecture on this N3P node, and the Mediatek chips with the usual off the shelf ARM cores on this node too to compare.",Neutral
Intel,"Those GPU stats are false. According to Tom's Guide, in 3D Mark Solar Bay Unlimited, the 17 Pro Max is only 10% faster than the s25 ultra https://www.tomsguide.com/phones/iphones/iphone-17-pro-max-review#section-iphone-17-pro-max-performance-cooling",Negative
Intel,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,">I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Me too. I'd imagine a marginal increase in size over A18 Pro. The P cores have mostly stayed the same. And the E cores despite major changes are unlikely to contribute to a major increase in area (if I'm right, individually they occupy around 0.6-0.7mm2 per core). The extra cache (around 2MB) should increase area slightly. SLC area as well should contribute to that increase.  I'd imagine the GPU with the new RT units, doubled FP16 units, new tensor cores, and general uarch improvements are the major contributor to any notable area increase.  Plus I still don't feel like the approaches these companies are taking are aligned very much in terms of GPU architectures. For eg, Apple's been very focussed on improving compute performance on their GPU. Qualcomm less so.  >Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other,   True that. Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp. But Apple's done extremely well with this upgrade. The E core jump has made them close the nT gap with Qualcomm while using much lower power.  GPU is a case where technically Qualcomm could take raw perf crown. But Apple's RT dominance, Tensor cores and general compute lead might help them in the desktop space.  >Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.  Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs. They consume similar or even more area for their core architecture while lagging in performance while using significantly more power. The x86 ecosystem and compatibility is the only reason they'd survive Oryon.",Neutral
Intel,Having so many SoC makers on arm competing against each other by one upping each other yearly is bearing fruit vs x86. i don't know how Intel and AMD long term can fare at this rate. Arm CPUs are still showing double digit gains yearly.,Neutral
Intel,Most of apples RT gains are from optmsiing how the GPU deals with divergence.  This is not dedicated RT silicon so much as making the GPU be able to maintain much higher throughput when there is lots of divergence.  RT operations have a shit tone of divergence.,Negative
Intel,I can't wait to see die shots and measurements for these chips. The A18 Pro and A18 die shots were really interesting to see what was compacted or lost for the base model chip. I have a feeling that there will be bigger differences for the A19 Pro and A19 with that giant SLC on the former. Die areas will also be interesting. Cache isn't cheap for area and I'd also love to see inside the new E-cores and GPU.,Positive
Intel,">Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  I wouldn't say its a medium core yet tbh. Apple's E cores are still sub 1mm2 in area usage. Compared to other ""M cores"" they are still relatively small. I imagine the A19 Pro E core is fairly larger but the A18 E core was around 0.6/0.7mm2 in size. I'd imagine its not grown a whole lot.  >They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.  I'd imagine they're saving up these to mention Blender perf etc in M5 keynote.",Neutral
Intel,4 minutes ago. Damn. I should have waited. I'll post it!,Negative
Intel,"An M5 Ultra would offer 1.23 Tb/sec of bandwidth scaling from the A19 Pro.  M5 (128-bit, LPDDR5X-9600) -> 153.6 GB/s M5 Pro (256-bit, LPDDR5X-9600) -> 307.2 GB/s M5 Max (512-bit, LPDDR5X-9600) -> 614.4 GB/s M5 Ultra (1024-bit, LPDDR5X-9600) ->1228.8 GB/s",Neutral
Intel,"Haha. I mean they are actually pretty late to this tbh. Most ""E/M cores from other competitors"" are similar in size if not bigger. I'd imagine Apple's E core is stuck between a true M core like the A7xx and a true E core like A5xx in terms of area, although it probably leans toward the A7xx in that regard.",Neutral
Intel,What does 6 wide mean? What units?,Neutral
Intel,"[https://www.reddit.com/r/hardware/comments/1jcoklb/enable\_rt\_performance\_drop\_amd\_vs\_nvidia\_20202025/](https://www.reddit.com/r/hardware/comments/1jcoklb/enable_rt_performance_drop_amd_vs_nvidia_20202025/)  In gaming RDNA4 RT isn't that far behind Blackwell. Other than that raytraced rendering like in Blender AMD has been for a while far behind. It won't be until Blender 5.0 till we see any improvements to HIPRT. Though for the longest time since following HIP it's been rather mediocre and my expectations are low for next release, though their [PRs](https://projects.blender.org/blender/blender/pulls/145281) make it seem they've been doing some work. It's a low priority for AMD which is unfortunate.",Neutral
Intel,>	beating out even AMD  Was that one really surprising?,Neutral
Intel,Is that Metal vs Optix or Metal vs Cuda?,Neutral
Intel,"The video has a mostly accurate English captions option. CPU P core is up 10%, E core is up by 25%, GPU perf is up by 40% and sustained performance is up by 50%.",Positive
Intel,> just basically a more efficient scheduler which improves utilization and less waste.  When you take a look at GPUs doing RT task you see that they tend to be very poorly utilized.  GPUs are not designed for short running diverging workloads. But RT is exactly that. So you end up with a huge amount of divergence and or lots of wave like submissions of very small batches of work (so have a large scheduling overhead).     There is a HUGE amount of perfomance left on the table for this type of task for HW vendors that are able to reduce the impact on GPU utilization that divergence has.,Negative
Intel,Maybe Apple measures occupancy differently in their tools. I wouldn't be too sure comparing these two. But I'd definitely think a combination of SER and Dynamic Caching present in A19 should result in very good utilization compared to other uarchs.,Positive
Intel,"That is very impressive, RT tends to have very poor occupancy as it is a heavily branching workload!",Positive
Intel,>Supposedly GPU size has not increased and that is impressive.   Important to note that the supporting SLC which is a major reason for improvements om the GPU side has increased from 24Mb to 32Mb. Which would increase area a bit.,Positive
Intel,">The C1 Pro has 20% IPC improvement IRRC, plus this is N3P  N3P is 5% faster than N3E. By TSMC's own claim..  Also I can't find a source for a 20% IPC improvement. ARM's claim is 16% IPC improvement. And that is not without a power cost since ARM claims that at similar performance, power reduction is only 12%.  https://newsroom.arm.com/blog/arm-c1-cpu-cluster-on-device-ai-performance  >Let's not jump to conclusions before seeing the competition   I mean I agree. But I don't see how the C1 Pro is supposed to cross a 95% P/W disparity. (4.17 points using 0.64W vs 3.57 points using 1.07W) using D9400",Neutral
Intel,"Tbf, Apple had a 32Mb SLC back in the A15 Bionic. They reduced the size of that afterward to 24Mb. Its not like the size significantly mattered in GPU performance until now.",Neutral
Intel,A 30% lead in P/W is a generations ahead in this day and age. Considering the successor (C1 Pro) is stated by ARM to reduce power by just 12% at iso performance leaving Apple with a comfortable lead for a year. Also I specifically was a bit confused by their choice to compare the A725L instead of the M variant.,Neutral
Intel,There's a guy on twitter who does the microbenchmarking for them.,Neutral
Intel,"No. I'm positive memory bandwidth offers very little in terms of performance upgrades. If you recall, the A16 was essentially the same GPU architecture as the A15 but used LPDDR5 instead of LPDDR4X, yet there were practically zero performance improvements.  I don't think anyone has investigated the A19's GPU microarchitecture thoroughly. But the main improvements seem to come from the increase in SLC size (System Level Cache which serves the GPU) from 24Mb to 32Mb and the newly improved Dynamic Caching. Its very likely there are a lot more changes responsible for that 40%+ improvement that we don't know about.",Negative
Intel,With most GPU tasks you are never ALU limited.,Neutral
Intel,">That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade and the 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm  Technically we need to compare P/W at similar power or similar performance. Peak power P/W is not a very accurate measure to compare gen on gen. And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.   An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.",Negative
Intel,Im having my doubts here. Howis the A19 p core much better than the A19 pro p-core? Aren’t they exactly the same p cores?,Neutral
Intel,">All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P.   N3P is a mere 5% faster than N3E when comparing similar microarchitectures... This is straight from TSMC's marketing slides.  Comparitively, barring the P core (which did see an ok improvement), the E core and the GPU have seen 30%+ improvements. The node has nothing to do with it.  If Qualcomm loses to, matches or exceeds A19 Pro this year it would be because of their updated microarchitectures and have barely anything to do with minor single digit improvements offer by a sub node.",Neutral
Intel,in the end what matters is the sicion you can buy so you can compare them.,Neutral
Intel,Please tell us more about how we can never compare AMD vs Intel chips by your logic,Negative
Intel,"Tom's Guide tested basic Solar Bay. This is the older version of the benchmark with less raytraced surfaces.  Geekerwan tested the modern, updated version of Solar Bay referred to as Solar Bay Extreme. This new benchmark has a much higher raytraced load, with far more reflective and transparent surfaces and much more detailed scene with more geometry.  Please kindly read the benchmark title mentioned in the posts. Or atleast watch the videos. Before commenting.",Neutral
Intel,Bad bot,Negative
Intel,"I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  I just wouldn't call the race so early, but it does seem very likely, that AMD will be behind. I just dont think it is as bad as it seems. AMD was plagued by Zen5% and still on 4nm for client, they might hit heavy with client dedicated improvements and N3, but in the end we have to see, x86 client performance really seems to struggle rn (and whatever intel is doing...).",Negative
Intel,">Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs.  Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.  Also... Let's not forget about ARM. A SoC with ARM CPU cores and an Nvidia/AMD GPU could absolutely ruin Qualcomm's day regardless of how better/worse their custom CPU cores are.",Negative
Intel,>Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp  No,Neutral
Intel,"IMO, Microsoft needs to push hard to switch to Windows-on-ARM, or else they risk an Android-like OS for Laptops swooping in and filling the gap left by those who do not want to go the Apple route. It feels like a crucial moment for both Windows and Intel/AMD, at least in the x86 product space. It retains PC Gaming at this point, but if that nutt is cracked via even half-decent, *compatible* emulation, then ... sayonara!",Neutral
Intel,"Yeah, the E core isn't quite there yet, but it's good to know they're moving in that direction with now stubborn they are with P cores, refusing to include more than what can be run at the equivalent of all core turbo.",Neutral
Intel,"Wow that will be insane and adding the new ""Tensor"" elements added to the GPU cores will make it a formidable AI workstation.  Especially when NVIDIA monopoly is only offering small VRAM GPU cards at absurd prices.",Positive
Intel,That would be amazing. Id love to see them put some hbm there too,Positive
Intel,It’s highly unlikely they will go past a 256 bit bus. You run out of pins and layers to route a bus that wide. Gets extremely expensive. Still neat bandwidth!,Neutral
Intel,The m5 ultra should be on par with a 4090?,Neutral
Intel,Skymont (Intel’s E core) is 8 wide (9 wide at first stage then bottlenecks to 8 I think iiuc),Neutral
Intel,"According to AnandTech, Apple's E cores were based on Apple's Swift core, back when they had a single core type  Previous die shots show Apple's E cores were close to Arm's A5xx in core area (larger, but far smaller than A7xx core only). But in terms of core+L2 Apple's E cores are similar to Arm's A7xx in core+L2 area  I'd argue it's the other way around, Apple's E cores are the true E cores  Whereas Arm's A7xx were stuck between being an M core and an E core  Now Arm has split their A7xx core to Cx Premium (M core) & Cx Pro (true E core)  Arm's A5xx/Cx Nano are a very odd core, almost no else makes a similar in-order core. Arm's A5xx/Cx Nano are more like Intel's LP E cores, instead of Apple's E cores",Neutral
Intel,The decoder,Neutral
Intel,"Amd is very far behind in rt.  You re linking gaming benchmarks, thats not rt thats mixed use.    Just look at path tracing results for a more representative comparison",Negative
Intel,"Hey, they made an effort with RDNA 4. I think that should surpass the M4 Max. I just can't find any proper scores for it.",Neutral
Intel,"Metal vs Optix.  https://youtu.be/0bZO1gbAc6Y?feature=shared  https://youtu.be/B528kGH_xww?feature=shared This is a more detailed video with individual comparisions and a lot more GPUs.  Its a lot more varied. In Lone Monk, it hangs with a desktop class 5070. In Classroom, it hangs neck to neck with a 4060Ti. In Barbershop, it falls behind a desktop 4060Ti. In scanlands, it falls behind a 4060.   If we consider Classroom as a baseline average, a theoretical 60% faster M5 Max, like the jump we saw in Solar Bay, would land hot on the heels of a desktop class 5070Ti, a 300W card. Competing with a 65W laptop GPU.  Edit; The Youtuber is using the binned 32C variant. A 40C variant would surpass the 5070ti.",Neutral
Intel,"Yeah I forgot what was the term before but I remember, it’s just like Nvidia’s Shader Execution Reordering introduced in Ada Lovelace.",Neutral
Intel,I have a query regarding RT workloads. Would offsetting RT performance to the CPU with the help of accelerators help? Or is that not the case and it would be even worser on CPUs.,Neutral
Intel,"Sure it might be different, but I doubt it. Occupancy is just threads used/total threads.  It's interesting how first gen dynamic caching + SER (apple equivalent) is hardly better than NVIDIA in terms of occupancy. Yet only 44%. So only slightly better than NVIDIA (upper end of range). Seems like first gen was more about laying the groundwork while second gen is really about pushing dynamic caching allocation granularity and efficiency. At least so it seems.  Oh for sure. That occupancy is incredibly high. \~1.5x uplift vs A18 Pro. Getting 70% occupancy in RT workload is really unheard of. Apple engineers did a fine job.  AMD might opt for this nextgen if they're serious, but it's a massive undertaking in terms of R&D, but could massively benefit PT and branch code, ideal for GPU work graphs.",Neutral
Intel,Agreed and you can see that by comparing with occupancy numbers for competitors.       Anyone who's serious about RT needs to copy whatever Apple is doing xD,Positive
Intel,"C1 Pro is two generations ahead of the A720 in the 9400. Also, Xiaomi demostrated a much more efficient implementation of the A720 cores in their O1 chip (4.06 points at 0.82 W).  Edit: actually, it seems like the O1 uses A725 cores. Perhaps that is what they are referring to in the video as ""A720 L""",Positive
Intel,It’s been a BIG bottleneck in AMD’s apus since Vega 11 back in 2018. Doubling from 8CU to 16CU in 860m vs 890m gets you only +30%.   AMD is just so damn stringent with area despite jacking up the price on Strix point massively on an old ass node.,Negative
Intel,"You ought to do the math first. Power is the denominator. 12% reduction in power is *substantial*.  Integer: A19 Pro E-core is 3% faster at 12% less power vs claimed C1-Pro.   |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%| |\-12% power|4.06|100%|0.72W|5.64|114%|  Floating point: A19 Pro E-core is 1% faster at 4% less power vs claimed C1-Pro.  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%| |\-12% power|6.07|100%|0.96 W|6.32|113%|  Hardly ""generations ahead"".",Neutral
Intel,Thanks for your reply.  CPU monkey reported 2GHz instead of 1.62GHz. So maybe that's where most of the gain comes from.  I suppose the tensor core like matmul units also boost performance for graphics and AI.,Positive
Intel,Maybe the improvement is from the new matmul units?,Neutral
Intel,"I am deeply suspicious of all of these power measurements. Separating p core power usage from other aspects of the soc is difficult on iOS. Which is not a criticism of your summary, but I’d be wary of drawing anything definitive about the efficiency of the p cores.",Negative
Intel,"It's not a knock on the design, it's how apple is configuring the CPU. It doesn't matter that performance at the same power is improved if the default clocks on the real product put it way past the sweet spot into diminishing returns so bad it regresses efficiency.   On one hand, the power inflation isn't causing problems if the 23% increased battery life per Wh is anything to go by, but on the other, what's the point of chasing peak performance like this if your boost/scheduling algorithms never allow that speed to make an impact on responsiveness?",Negative
Intel,"Geekerwan's results are ***average*** power, not peak power, IIRC. These are real, actual frequency bins that Apple has allowed.  These frequency bins *will* be hit by some workloads, but just not nearly as long as SPEC & active cooling will allow. It would be good to revisit Apple's boosting algorithms, but IIRC, they hit 100% of Apple's design frequency in normal usage.  It's not like users have a choice here; we can't say, ""Please throttle my A19 Pro to the same power draw as the A18 Pro."" Low power mode neuters much of the phone, so it's rarely used all the time.  //  I find avgerage power useful two reasons:  1. *How* performance vs power were balanced; here, performance took precedence while not keeping power stable.  2. It also shows, when nodes are not the same, where the node's improvements went. Here, an N3P core delivers notably worse perf / W versus an N3E core. TSMC claims [up to 5% to 10% less power](https://www.tomshardware.com/tech-industry/tsmcs-3nm-update-n3p-in-production-n3x-on-track) on N3P vs N3E.  I agree 10W is not common and SPEC is a severe test, but it's more the *pattern* that has emerged on Apple's fp power and whether it's worth it:  2023 - A17 Pro P-Core: 6.40W  2025 - A19 Pro P-Core: 10.07W  Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   >And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.  I agree it's rare, but why would Apple allow 10W? Were many workloads ***lacking*** fp performance that users prefer a bit less battery life for +9% fp perf vs the A18 Pro?  Of course, to most, battery life is more important, IMO, which is why core energy is most crucial, but missing here.  //  >An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.  So the question becomes: do users want slightly better perf and 5% more power? On a phone, I'm of the opinion that power is paramount and should be forced to lower levels.",Neutral
Intel,">Aren’t they exactly the same p cores?  Definitely not. See the SPEC perf / GHz: A19 is nearly just the A18 Pro. Thus, this seems to be the final picture:  A19 Pro = new uarch, 32MB SLC, 12GB LPDDR5X-9600  A19 = binned OC of last year's A1**8** Pro, w/ faster LPDDR5X-8533, but smaller SLC (12MB)  New uArch clearly didn't pan out as expected in fp efficiency. A19 Pro may sit at a flatter part of the freq / power curve, A19 Pro may have more leakage, A19 Pro's [faster RAM](https://en.wikipedia.org/wiki/Apple_A19) may eat *a lot* power (Geekerwant tests mainboard power, not purely CPU power), etc.",Neutral
Intel,I'm sorry you had to take time away from ripping off others' content to correct my mistake,Negative
Intel,">I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  M2 and Zen 4 launched around the same period. The desktop chips score around 5-10% faster in Geekbench while using 20W per core power and 30-40W more for the I/O die. Taking the ST crown by a hair's width while using 5-10x more power isn't a win at all imo.",Neutral
Intel,">Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.   I'd agree if they performed as well as their clocks suggest. Whats the point of clocking to 5.7Ghz if a mobile CPU clocked at 4.4Ghz leads you in absolute performance by 15% (Geekbench) while using a tenth of the total power.",Neutral
Intel,This snapdragon era is Microsoft's third time trying arm (windows rt and surface pro x). Hopefully third times the charm,Positive
Intel,Their E-core is faster than something like Tiger lake per clock. It’s about 60-70% as fast as the best desktop CPU from 2020 and probably as fast as a U-series chip from then under sustained loads.  The real takeaway for me is that the rumored A19 MacBook is going to dominate the $600 laptop market segment if it releases before Christmas.,Positive
Intel,"Weirdly enough, I feel like them not letting the E core balloon in size has helped them in the long run. Seems they're focussed on maximizing performance within a limited area. Their E cores have had exceptional jumps in performance since the A12 almost every generation being 25%+ in improvements.  I'd wager that E core dominance is the primary reason why the iPhones are able to match Android manufacturers using super large and dense batteries in terms of endurance.",Positive
Intel,Doesn't mac studio already 500gb vs nvidia workstations of 96gbs?,Neutral
Intel,NVIDIA currently fist pumping the air over their failed ARM acquisition rn,Negative
Intel,Its a bit unlikely. Maybe for a version of the M series dedicated for a Mac Pro. But one of the main reasons they can get away with this design is because its very scalable. All the way from A series to Mx Max series. Adding HBM would probably require a dedicated SoC redesign for a very niche product segment in Macs.,Neutral
Intel,The M4 Max already uses a 512 bit bus. Does it not?,Neutral
Intel,Its technically not a true 9 wide core. I think its 3+3+3.,Neutral
Intel,E-Core is relative. Skymont is more of a C-core (area optimized) than what we typically think of as an E-core (energy optimized).,Neutral
Intel,with a viable width ISA it is better to look at the typcile throughput not the peak throuput as you very rarely are able to decode 8 instructions per clock cycle.,Neutral
Intel,"With the 9070? I don’t think I’ve seen any results showing that either, however all I’ve looked at is the blender benchmark charts",Neutral
Intel,"the shader re-ordering is different. (apple also do this).  Even with shader re-ordering you have the issue that you're still issuing 1000s of very small jobs.    GPUs cant do lots of small jobs, they are designed to do the same task to 1000s of pixels all at once.     If  you instead give them 1000 tasks were each pixel does something differnt the GPU cant run that all at the same time... in addition the overhead for setup and teardown of each of these adds even more void space between them.   So apple are doing a hybrid approach, for large groups of function calls they do re-ordering (like NV) but for the functions were there is not enough work to justify a seperate dispatch they do function calling.    This is were dynamic coaching jumpstart in.      Typical when you compile your shader for the GPU the driver figures out the widest point within that shader (the point in time were it need the most FP64 units at once, and register count).  Using this it figures out how to spread the shader out over the GPU.  Eg a given shader might need at its peak 30 floating pointer registers.   But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time.   If you have a shader with lots of branching logic (like function calls to other embedded shaders) the driver typically needs to figure out the absolute max for registers and fp units etc.  (the worst permutation of function branches that could have been taken).  Often this results in a very large occupancy footprint that means only a very small number of instances of this shader can run at once on your GPU.  But in realty since most of these branches are optional when running it will never use all these resources.  The dynamic cache system apple has been building is all about enabling these reassures to be provided to shaders at runtime in a smarter way so that you can run these supper large shader blocks with high occupancy as the registers and local memory needed can be dynamically allocated to each thread depending on the branch it takes",Neutral
Intel,"While RT does have lots of branching logic (and CPUs are much better at dealign with this) you also want to shader the result when a ray intercepts, and this is stuff GPUs are rather good at (if enough rays hit that martial)   We have had CPU RT for a long time and so long as you constrain the martial space a little GPUs these days, even with all the efficiency  loss are still better at it.   (there are still high end films that opt for final render on cpu as it gives them more flexibility in the shaders they use) but for a game were it is all about fudging it GPUs are orders of magnitude faster, you just have so much more fp compute throughput on the GPU even if it is running at 20% utilization that is still way faster than any cpu.",Positive
Intel,"Its not just useful for RT but also for a lot of other situations, being able to just call out to functions on the GPU and not pay a HUGE divergence penalty (you still pay some) opens up GPU compute to a load more cases were we currently might not bother.",Positive
Intel,I'm a bit confused. You think lagging 15% behind in P/W the competition for an entire year is not being a generation behind?  ARM themselves have managed only a 15% jump this year. So it will essentially be 2 years before we get an E core that matches the A19 pro. And this is just considering the Xiomi's SoC. Mediatek's and Qualcomm's which dominate the majority of the market lag even further behind.,Negative
Intel,I'd advise against using CPUmonkey as a reliable source. They're known to make up numbers. (Reported M1 Pro/Max Cinebench scores 6 months before they launched based on predictions),Negative
Intel,"I agree to some degree; Geekerwan notes they are testing mainboard power, not core power (if you Google Translate their legend).  For me, I assume all the major power draws on the motherboard *are* contributing to the overall SPEC performance, too.   If the faster LPDDR5X-9600 in the A19 Pro eats more power, it's fair to include that power because ***all*** A19 Pros will ship with LPDDR5X-9600. That was Apple's choice to upgrade to this faster memory.    Now, you're very right: we can't purely blame the uArch at Apple. It may well be the DRAM or the boost algorithms (like we saw at the A18 Pro launch last year) and—at Apple specifically—even the marketing overlords.  It's also why I'm a big proponent of JEDEC speeds & timings & volts in desktop CPU tests, much to the chagrin of a few commenters.",Neutral
Intel,Why is separating p-core power usage from SOC power uniquely difficult on iOS?,Negative
Intel,I understand your reasonings. But its the only semblance of comparison we have to date between different SoC. I've learned not to look a gift horse in the mouth.,Neutral
Intel,">Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   >That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   I mean this has been a trend long before the A17. Apple has been increasing peak power little by little since the A12 Bionic.  I remember reading Anandtech articles about it.",Neutral
Intel,Don't be a sour puss now because you didn't check your sources before commenting. Mistakes happen.  >ripping off others' content   Eh? Are you stupid? You're pissed that someone posted a hardware review ON A hardware sub? The entire sub exists to discuss hardware bozo.,Negative
Intel,"While the efficiency for this is BAD, I dont think its 20W per core.  When we look at results by [Phoronix](https://www.phoronix.com/review/amd-zen4-zen4c-scaling/2) we can see \~7-8W per core for this (not great numbers, because its a weird chip and different node), which is still very bad. AMD certainly has some power issues, but many of which, i.e. inefficient I/O dies, are not really dependent on the CPU uArch and could switch at any moment. They certainly have much more inefficient chips at the moment than both Apple and Qualcomm. For Zen 6 we expect a major update to the desktop chiplet architectur which could bring some much needed improvements in terms of I/O though.  They have reasonably fast cores, and I think they are not in a terrible position, even though it is far from good. I think what is interesting for AMD to look out for is that they keep moving fast, instead of intel who didnt move fast since like 14nm, and AMD has strong cores. Additionally AMD has (including from the datacenter) an enterprise need to make the CPUs more efficient.  So yeah, very bad CPUs efficiency wise. A bit behind, but not terribly on perf per node wise, efficiency on the desktop is an afterthought for AMD, clearly, but they are moving constantly and are improving. It might be AMD Laptop Zen 6 has again like 35W TDP, for 3000 Geekbench and be dead, but with some client oriented tweaks I see chances (maybe just from the patterns in the tea leaves in my mug)",Negative
Intel,"I originally saw this on Phoronix' forums, but I can't find the link to the comment so I'll send this one instead: [https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png](https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png)  Zen 5 is now behind, yes, but it isn't really that bad.",Neutral
Intel,"Per clock, the A19 Pro E core is competitive with Golden Cove. Atleast in SPECint.  M4 E core-> 3.53 points at 2.88Ghz (1.23 p/Ghz) or IPC i9 14900k-> 9.93 points at 6.00Ghz (1.65 p/Ghz) or IPC  A19 Pro has a 22% jump in IPC in SPECint (might be some variations due to time difference and lack of knowledge about the subtests ran, but still gives a good picture)  22% IPC jump over M4/A18 E core = 122% of 1.23 = 1.50 A19 P core = 1.50 p/Ghz  Golden Cove in i9 14900K has a mere 10% lead over A19 Pro in perf/clock in SPECint.  https://youtu.be/EbDPvcbilCs?feature=shared  Source: I9 14900K compared with M4 in this review.",Neutral
Intel,It does but the Macs are limited in other ways (memory speed among other things),Neutral
Intel,It wasn't for a lack of trying.,Negative
Intel,Yeah there was some rumors of a server version and thy have a patent for a multi level cache setup but they also patent and prototype plenty of things that never get released.   https://www.techpowerup.com/277760/apple-patents-multi-level-hybrid-memory-subsystem,Neutral
Intel,"Oh huh, it does but it’s 128 bit per channel. So memory on 4 sides of the die. Wild, don’t see that normally except in FPGA for data logging (or GPUs)",Neutral
Intel,"The difference seems a bit drastic in open data benchmarks.   https://youtu.be/B528kGH_xww?feature=shared  Testing individual scenes, the 9070xt and M4 Max seem neck and neck.  The M4 Max at best (in Lone Monk) is 5070 desktop class and at worst (in Scanlands) is 4060 desktop class. On average, I'd say in Blender, it is neck and neck with an RTX 4060Ti desktop card. I think a theoretical M5 Max should be on par with a 5070Ti if we see the same 60% bump in RT performance.",Neutral
Intel,Apparently Cinebench 2024 GPU is not compatible with RDNA4 cards lol. So I can't find any scores to compare.,Negative
Intel,"So does dynamic caching ensure that the total size will ""always"" be the same as whats being called? As in certain cases it is still possible that there can be wastage like for the example you said ""Eg a given shader might need at its peak 30 floating pointer registers. But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time."" on that, there would be 10 registers wasted doing nothing, if it cant find any else thats <10 registers to fit in that.",Neutral
Intel,Yeah you're right. Should've said branchy or low occupancy code.  Combining such a HW side change with change on SW side with GPU work graphs API could indeed open up many usecases and new possibilities that are well beyond the current way of doing things. I can't wait to see what Apple does when Work Graphs arrives in a future Metal release.,Positive
Intel,"Well when you put it like that (13-14% behind in efficiency) saying ""generations behind"" certainly sounds misleading.  All it would take for another arm vendor to beat that is jumping one node(let) earlier than Apple, which is certainly doable given the lower volumes, although traditionally it rarely happens. Or jumping earlier to anything that might give them an advantage really, e.g. lpddr6.",Negative
Intel,"It's not ""generation**s**"" behind as you originally wrote. It's being compared to cores from **a year ago** already, mate.  I fully expect MediaTek to adopt C1-Pro and Qualcomm for sure will also update.  Apple's E-cores are simply nowhere near as dominant as they used to be in the A55 era.  EDIT: before we speculate more using marketing for Arm and hard results for Apple, let's check back in a few months to see how C1-Pro  *actually* performs and how Qualcomm's smaller cores actually perform.",Neutral
Intel,Its a bit harder since we don't have any A19 Pro die shots yet. But Apple's E cores have always been sub 1mm2 compared to A7xx cores.,Neutral
Intel,Yeah. All fair points. I don’t disagree. It’s still fun to speculate!,Positive
Intel,"Because we don’t have the tools. On macOS Apple provides powermetrics but Apple states that the figures can’t necessarily be relied on. On some very specific tests you can narrow down power to a cpu core, kinda. Spec tests often stress other aspects like memory, so I would use the provided figures as a guide. Either as a “p core bad” or “p core good” conclusion.",Neutral
Intel,"Oh for sure. It’s not a criticism of either Geekerwan or yourself. They are doing a great job with the available options and I appreciate your summary. I just find it a little amusing when people dissect milliwatt differences as absolutely accurate. We just don’t have the tools, and people are keen to jump on the “p core doomed” bandwagon.",Positive
Intel,"The power increaseas are definitively (and definitely) accelerating, though, worse than it used to be.  [**Geekerwan P-core power**](https://youtu.be/iSCTlB1dhO0?t=422) **| SPECint2017 floating point**  *% is from the previous generation*  A14: 5.54W  A15: 5.54W (+0% power YoY)  A16: 6.06W (+9% power YoY)  A17 Pro: 6.74W (+11% power YoY)  A18 Pro: 8.18 W (+21% power YoY)  A19 Pro: 10.07 W (+23% power YoY)  //  AnandTech did the *great* work of measuring energy consumption / joules. That really proved that race to idle was working; more instanteous power under load, less overall energy   [**AnandTech P-core Power | SPECint2017 floating point**](https://web.archive.org/web/20240907062349/https://www.anandtech.com/Show/Index/16983?cPage=15&all=False&sort=0&page=2&slug=the-apple-a15-soc-performance-review-faster-more-efficient)  A14: 4.72W, 6,753 joules  A15: 4.77W, 6,043 joules (+1% power YoY, -11% energy YoY)  Average power went up, but energy consumption went down.",Negative
Intel,You copied and pasted someone else's data and now you're acting like a hero,Negative
Intel,"Still very bad used to work in their IO team, and the idle eatt performance is still very bad. Their idle core loading is also not as good as I would expect for their next generation, and its sad to see the future generation lose to even on M3 on benchmarks. The LP core was supposed to help in this situation, but I just can't see past the fact on why the gap is actually widening over time. I hope Intel can come in and close this gap.",Negative
Intel,"The 9950x lags by 8% behind the M4. M5 is another 10% on top of this lead. M series chips use around 8W of power in total to achieve this perf including memory and what not. 9950x is like 20W per core, another 40W for the I/O and another unknown amount for mempry.",Neutral
Intel,I was comparing overall score.  Geekerwan got 4.17 at 2.58GHz which is ~1.62pt/GHz which is a little higher than your speculation and a mere 1.8% IPC difference from that Golden Cove number.  The fact that it did this at 0.64w in such a tiny core is absolutely incredible. It once again raises the question about how much ISA matters. I don't see any x86 E/C-cores getting anywhere close to those numbers.  https://youtu.be/Y9SwluJ9qPI?t=260,Positive
Intel,one will think the massive vram capacity just override the disadvantages.,Neutral
Intel,"dynamic caching would let more copies of the shader run given that is knows the chances that every copy hits that point were it needs 30 registers is very low.   If that happens then one of those threads is then stalled but the other thing it can do is dynamicly at runtime convert cache, and thread local memroy to registers and vice versa.   So what will happen first is some data will be evicted from cache and those bits will be used as registers.   maybe that shader has a typical width of just 5 registers and only in some strange edge case goes all the way up to 30.   With a width of 5 it can run 20 copies on a GPU core that has a peak 100 registers.",Neutral
Intel,"Metal has supported GPU side dispatch for a long time, (long before work graphs were a thing in DX) Barries and fences in metal are used by the GPU to create a dependency graph between passes and this is resolved GPU side (not CPU side).   I don't see the explicit need for some extra feature here as we already have \*and have had for a long time since metal 2.1\*",Neutral
Intel,"Historically, the core only area for Apple's E cores have always been between Arm's A5xx & A7xx cores, but closer to Arm's A5xx  But for core+L2 (using sL2/4), Apple's E cores have always been similar to A7xx cores in mid config",Neutral
Intel,"I agree that the presence of inaccuracies is very likely. And I certainly don't think the P core is doomed for a 10% jump in what is essentially a a very minor node upgrade.  But considering the video does go into the P core's architecture where the only substantial changes were the size of the Reorder Buffer and a marginally better branch predictor, the performance numbers make sense.",Neutral
Intel,This is a combination of the meaningless smartphone benchmark game (95% of users would be perfectly fine with 6 of Apple's latest E-cores) and the need to have a powerful chip in laptops/desktops all sharing the same core design.,Neutral
Intel,"I'm confused. And legitimately concerned about your mental faculties.  Literally in your previous comment, you posted Tom's Guide data, data thats not ""yours"" to try and discredit my post. And checking your post history, you also posted a Mrwhosetheboss video to discuss battery life comparison in an another subReddit.   So are you a hypocrite since you're ""stealing"" data as well? I'm not stealing anyone's data. I'm correcting your stupidly incorrect conclusion with a source to back it up. Just like you attempted to lol.",Negative
Intel,"You see 7-8W increase going from 1 Core to 2 Core, which indicates that AMD has a huge base overhead but the core doesn't use 20W.   So its a problem with the SoC design not with the CPU design that it uses 20W for ST.  Its more like running a ST workload draws 20W SoC + 8W CPU stuff. So if they had better SoC design they could substantially boost efficiency even with the same cores.",Neutral
Intel,"Comparing the 9950X to the M4/M5 is a bit of a stretch... I'm not saying AMD is as good, but if they did a ""Lunar Lake"" with Zen 5C + on package memory they wouldn't really be that far off.  I want x86 to belong in the museum too, but sadly the ISA doesn't really matter (that much) and AMD isn't exactly incompetent... EPYC CPUs are still dominant and this is what they're truly targeting...",Neutral
Intel,"Oh I was referring to this statement in your previous comment.  >Their E-core is faster than something like Tiger lake per clock.  By ""per clock"" I assumed you mentioned IPC.",Neutral
Intel,"I wouldn't recommend comparing IPC from two different runs of SPEC2017. There could be numerous different changes or subtests we don't know about. For eg, compare IPC numbers here vs the test used in A18 review. There are some notable differences in scores and IPC.",Neutral
Intel,Oh it's a huge benefit (I have a M2 Ultra at work) but we still use Nvidia. The cuda ecosystem is far more mature and widely supported with better support for embedded and datacenter scale compute.,Positive
Intel,"I see, so dynamic caching can make it so a shader doesnt have to be 30 registers wide if it doesnt have to do 30 often so it doesnt have to reserve that much space and waste it(such as in conventional cases, if its 5 registers and 30 peak, it will still reserve 30 registers despite it being at 5, which then would waste 25 doing nothing)  Also SER happens first right?",Neutral
Intel,Impressive and thanks for enlightening me.,Positive
Intel,I don’t disagree. The performance figures seem good. The power figures may or may not be. I’m just nitpicking.  Edit: just noticed that they show the A19 having 99% P-core FP performance path 11% less power. That is weird and get’s to my point about power measurement strangeness.,Neutral
Intel,"The needlessly high clocks, agreed: Apple could've still improved perf with lower clocks.  >the need to have a powerful chip in laptops/desktops all sharing the same core design.  They previously kept this in check on the A16 and even A17 Pro, both sharing the same core designs as the M2 and M3 respectively. That doesn't seem too related, as every uArch should scale a few hundred MHz either down or up.",Neutral
Intel,I posted links to reviews. I didn't copy the entire data for my own post,Neutral
Intel,"ISA matters much as well, or mostly the implementation of it. X86 and AMD dont go well together, and AMD is the very definition of incompetency. Both deserve to be sunseted by now. The fact that there is no proper ARM support on consumer platform is the only reason why X86 on consumer still exists. For servers, an ARM server is more power efficient, and only those really legacy stuff requires X86. Companies would really appreciate the cost savings and the ARM ecosystem more than the clusterfk of X86.",Negative
Intel,"Yeah, I guess that wasn't clear. What I meant is that the IPC of the E-core is so much higher than Tiger Lake that it's performance at 2.6GHz is surprisingly close to desktop Tiger Lake at 5GHz.  I know it's not fair to compare with Intel 10nm (later Intel 7) with TSMC N3P as it's 2 major nodes apart, but this goes way beyond that because these chips are using only around 2.5w for just the E-cores. TSMC claimed 25-30% power savings from N7 to N5 and another 30-35% from N5 to N3.  Using these numbers, we get these 4 E-cores using somewhere around 6-7w PEAK, but this is all-core turbo. Intel's CPU at it's most efficient 12w configuration won't hit it's 4.4GHz turbo required to win in single-core performance and is going to hit closer to its 1.2GHz base clock in all-core sustained performance at which point the E-cores not only have their giant IPC advantage, but double the clockspeed too.  All this again to make the point that a macbook with this will be blazing fast for consumer workloads and might finally be the laptop to usher in multi-day battery life.",Neutral
Intel,"This is true, but there aren't a ton of alternatives short of taking a ton of time to compile/run everything myself. I don't think it's worth it for a hypothetical IPC comparison that it sees nobody else would even be that interested in seeing.",Negative
Intel,"Reordering of shaders has a cost, if for a given martial you just hit 10 rays you will not want to dispatch that shader with just 10 instances as the cost of dispatch and scdulaing will be higher than just inlining the evaluation, so you will merge together the low frequency hits into a single wave were you then use branching/fuction point calls.    You will also use this mixed martial uber shader to use up all the dregs that do not fit within a SMD group.      Eg you might have 104 rays hit a martial but that martial shader can only fit 96 threads into a SIMD group so has 8 remaining thread, you don't want to just dispatch these on there own as that will have very poor occupancy (with 88 threads worth of compute ideal) so you instead inline them within a uber shader along with a load of other overflow.",Neutral
Intel,"Are you perhaps blind? The youtube link to the review is at the top of the post? What is wrong with you friend? Feeling a bit under the weather?  The video is in Chinese and before I updated the post with a youtube link, the previous source was from Bilbili, a platform that doesn't even work in most countries.   So I summarised the important points in it for people who didn't understand Chinese and couldn't infer anything from the graphs. No one's gonna ignore the link that is at the top of the post and read my summary before seeing that a link is available.   You're fighting imaginary demons here. Go to your samsung subreddit and whine about Geekbench or something.",Negative
Intel,"But we were discussing CPU uArches, and on the question of ""Does AMDs single core suck up 31W"" the answer is ""NO"", the SoC power is 31W, and that is an issue that can be solved independently of the CPU uArch.  This just means that the massive inefficiencies are not inherint to the CPU uArch, but are a Problem of SoC design.  7W on a single core isnt super efficient, but its far lower than 20-31W. The question is if AMD is able to strip away a large part of those ~20W overhead, but that is not contingent on questions about CPU uArches.  And thats the point of the 1 Core vs 2 Core comparison. To demonstrate that it isnt the CPU that sucks up 20W, but the SoC, which can be solved much differently.",Negative
Intel,Enjoy your scratch-prone iPhone!,Positive
Intel,Your one brain cell is trying really hard. See if you can wake up its buddy.,Negative
Intel,"Same price as B580 with lower performance, 4GB less vram and 128 bit bus.  A round of applause for Nvidia",Neutral
Intel,"GB207 being slower than AD107 is pathetic, what's the point of these x07 dies again? They're not thst mich smaller than recent x06 dies.  They're spaffing design cost on these barely different dues.",Negative
Intel,"Now that we have a third-party review, it pretty much confirms what Inno3D said the other day, it's definitively slower than the RTX 4060 by about 5-7%.",Neutral
Intel,The RTX 5050 is 2.5% slower than the Arc B580.   It's also a 50 series card that costs $250.,Negative
Intel,> the system used a Ryzen 7 9800X3D  If the B580 only wins by 2.6% with this CPU then it's going to lose when you use something weaker because of that CPU overhead problem with Intel.,Negative
Intel,It's actually surprising that it's that close to a 4060 considering the 5050 only has 2 GPCs as opposed to the 4060's 3,Neutral
Intel,IDK but that performance is actually not bad. It should've just been cheaper.,Positive
Intel,The only positive about this is they're likely going to make a 5040 with a cut down chip that should pretty easily fit <75W.,Positive
Intel,"Honestly for people who don't need a lot of GPU power, not the worse.   I have two work computers, one with a 4090 and an old one with a 3090Ti.  These GPUs sit idle just taking up space, would have made more sense to get the something less performant.",Negative
Intel,272mm\^2 for B580 vs \~150 or less for RTX 5050 and perf within 5% lmao,Neutral
Intel,"I think to lose in sales, it would have had to lose a little more convincingly.  Yes, it's worse. But not enough to make up for features and brand-loyalty / nvidias massive reputation with gamers who are not into tech.  This thing will sell. It will sell A LOT. Because Average-Kevin who just wants to play Fortnite and some League will have a great time with it. His YT videos will get upscaling, his shitty mic-quality will get fixed (mostly) by Nvidia Broadcast... and he DGAF why we think it's the wrong move.",Negative
Intel,"Does it actually fall behind the b580 or is it due to vram bottleneck in certain instances?  Either way, this should open up people’s eyes about the b580. The b580 is just marginally above the 5050, which is a terrible product, with 4gb more vram… even at its 250 dollar price point, it’s never been a good deal. Not to mention how it’s going for 400 these days.",Negative
Intel,"People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.  Edit: r/hardware is dumb CPU overhead is a thing and budget GPU buyers need to know which of these two cards is effected the most by it. Everyone already knows that you are going to be GPU limited on top tier CPU's its not valuable information for a budget card.",Negative
Intel,Looks like people already forgot that testing the B580 with a top end CPU gives unrealistic results for actual budget buyers.  Every trick in the book for a nvidia sucks article.,Negative
Intel,Don’t make Jensen sad,Negative
Intel,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Will it also fall behind them in the Steam hardware survey?,Neutral
Intel,the 3060 is still a better value than this card bc its about same speed but 12gb vram and higher bus,Positive
Intel,"Maybe a product for non-gamers who want/need multiple display setup, like me. Even then, if you bother to spend that amount of money for 5050, better add some more to get 5060, and if bother to get 5060 just better get 16GB one. Furthermore, you may feel why not add bit more to get 5070 series rather than 5060 16GB. That's how these things are priced.",Neutral
Intel,Value engineering to extract as much wealth as possible while unable to match performance conditions.,Neutral
Intel,When you gimp the memory bus of course it is not as good,Negative
Intel,"No 75 watt gpu, pin connectior, yeah this is death of arrival.",Negative
Intel,"Trying to convince people to buy a 8gb card in 2025 should be illegal. Yet, people will line up to buy it because of the Nvidia logo...",Negative
Intel,"It's a Nvidia Rx 7600, actually good recommendation over that card if it released for the same price in India. Rx 7600 is awful compared to rtx 5050 in pretty much everything.",Positive
Intel,Humiliation for the RTX 5050 is well deserved. It's good this gets curb stomped by the Intel B580.  More market share for Intel because AMD also introduced a useless 8 GB card.,Positive
Intel,">128 bit bus  Seems like this isn't the main issue with 5050's performance. 4060 has lowest memory bandwidth out of all 3, yet it's the fastest card. The die is just too cut down.",Negative
Intel,Unfortunately it will still sell like hotcakes in pre-builts to people who don't know much about computers.,Negative
Intel,"Their brand name is so strong that despite being worse in basically every way, including having poor drivers which was one of their major selling points in the past, they will still sell super well",Positive
Intel,"I just looked at PCPartPicker and the cheapest B580 on there, in the US, is $299 (and the brand is Onix). The Intel Limited Edition is $340. If you can get the 5050 at MSRP it would have a decent price advantage.  I mean, I wouldn't get a 5050 but in the US at least, the $249 price for the B580 isn't real.",Neutral
Intel,"Where I live none of the main sellers stock the B580, a round of applause for Intel.  The B580 isn't faster on the CPU's owned by the people who are in the market of a budget GPU.  People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.",Negative
Intel,Intel apparently hasn't shipped any GPUs last quarter. The shortage is already been seen as prices of b580 are going up.,Negative
Intel,"> with lower performance,  even on lower end cpu? Holy crap nvidia wtf",Negative
Intel,Because it will sell a shit load. Probably will be one of the highest selling Nvidia GPUs this generation,Negative
Intel,"At these small die sizes, yields can be really high. High enough that if they wanted to have supply for a 5050, they might not have enough GB206 dies that can be cut down to 20/36 SMs.  The 5060M is probably catching the bulk of the bottom-level GB206 dies at 26/36.  There's also the memory difference, with the 5050 having gddr6 while the rest of the lineup uses gddr7. With dies this small, that cost difference in memory could actually make it worth taping out a new chip with controllers for the cheaper stuff. Gddr6 is dirt cheap at this point.  You could absolutely then argue that there's no reason for the 5050 to be so cut down that it needs a new, even smaller die, and that Nvidia can eat some margins on a budget card to give it better memory. And I'd agree. But I'm sadly not anybody who can make that decision.  Personally, I can't wait to see what becomes of the cut-down GB207s. The 5050 is a ""golden"" fully-enabled die. There will be some with only 18, or 16, or even fewer working SMs. Hell it could lose an entire GPC and be down to half-working. Those will be some absolutely sad little GPUs.",Neutral
Intel,> It's also a 50 series card that costs $250.  I'll believe that when I see it in stock for 250. MSRP is meaningless if the product isn't available for that price.,Neutral
Intel,And will be even worse since it's vram is 8gb. In many many games the b580 would Perform even better,Negative
Intel,Yeah it would be interesting to test with a more real-world CPU pairing.,Positive
Intel,"Yeah, that was my question, did that ever get fixed? It seems like nobody remembers that anymore",Negative
Intel,"TBH, they should have made THIS card a 75W SKU. It probably easily can be, without losing much performance. That would make it a very viable upgrade path for low-end systems. And kind of make the price more attractive, because it would require zero additional changes.",Positive
Intel,Margins go crazy,Neutral
Intel,4x the performance with MFG though.,Neutral
Intel,In mkst cases comparison would make sense since you testing gpu's not anything else. But when you toss in b580 complaints about using too powerfull cpu make much more sense,Neutral
Intel,I have a system with a 9950x3d and a 1060. I’m thinking of getting a 5050 to go with it.,Neutral
Intel,> Every trick in the book for a nvidia sucks article.  That's just a standardised testing suite.   Someone will have low-end CPU testing once this gets any distribution.,Negative
Intel,They're using the same cpu for the 5050 too. What a weird comment to make.,Negative
Intel,"Don’t forget how everyone is shitting on the 5050, calling it the worst nvidia card ever, but were praising intel non stop for the b580.  Even using purely stats from the article, b580 is 3% better with 4gb more VRAM than “worst gpu ever”, so I don’t see how this translates to a win for the b580. Being better than a terrible product by a hair doesn’t exactly make it good.",Negative
Intel,Same can be said for the 5050 no? Both have CPU overheads unlike AMD which any mid-range AMD GPU can easily outperform Nvidia's 5090 at 1080p.   The 12GB VRAM however are better suited for people who are stuck at older Gen3/Gen4 PCIE speeds.,Neutral
Intel,"It'll run nearly every game in existence at 1080p with playable framerates with the right settings. IDK why it would be a product for non-gamers. Calling it that means the B580, 4060 and 7600 are products for non-gamers, too.  Only game I could think of it wouldn't run properly would be oblivion remastered, which has terrible performance on anything.",Negative
Intel,3050 released with the exact same specs and msrp. The 70w variant only came out 2 years after. The first variant was in top 10 of steam's hardware chart.,Neutral
Intel,"This is not a card for 1440p, as it would have issues with getting even 60 fps there, and in 1080p 8gb is mostly enough.  There is a difference between choking on VRAM with 80 base fps (5060ti) and 50 base fps (5050). Yes, second case is still annoying, but you might want to dial down the settings anyway, and then you would be fine.",Negative
Intel,Nvidia RX7600? That a new GPU? 😂,Neutral
Intel,"Funnily enough the 8gb 9060 XT is the best brand-new alternative at $300 and below, hands-down, and that includes the B580. It's x16 interface means it's less restricted by the vram compared to the 5060/Ti 8gb. The B580 still has overhead issues, VR issues, game issues and is rarely available at MSRP.  But people trash it anyway and praise the B580 without actually looking at its performance. In the benchmark linked by the OP alone it loses to the 4060 lol",Positive
Intel,"Performance is fine I'm sure it'll perform just as good as the 3060 maybe slightly if you lower the power to 75W, call it a 5030 and sell it for $150. But these are different times and Nvidia doesn't do reasonable things like that anymore.  Bottom line, it's just the price and TDP that sucks.",Neutral
Intel,TBF a pre-built you're just looking at:   * What's the price?  * Does it have a graphics card or not?  * Overall package/size,Neutral
Intel,"or even people who have a baseline knowledge, but think ""dlss and framegen"" will make this abomination better than its competition",Negative
Intel,"It's not so much the brand name but their near monopoly in the prebuilt market. There will be 1 prebuilt with a 9060XT 8 or 16GB for every 20 prebuilts with a 5050/5060. It's so bad that even the prebuilts with the 16GB 9060XT will be outsold by prebuilts with a 5050 at the same price range, as companies like to pair trash GPUs with ""high"" end CPUs like a 13700F to clear stock but the ""i7"" allows them to mark up the price.",Negative
Intel,"It was in stock at newegg monday for 3+ hours at $249.99, I picked one up after seeing a post that was 3 hours old on /r/buildapcsales   I've stopped into microcenter a couple of times and they said they get them, but I just haven't been lucky enough to be there when they were restocked.  EDIT: it is back in stock as of 11:40 EST at Newegg",Neutral
Intel,"In Germany you can find Arc B580 for €269-283 low-end, and B570 for €218-235.",Neutral
Intel,Her in Canada they are $360 CAD. Cheap cheap. And ways to find. Might be a issue in your country perhaps,Neutral
Intel,"Went on Shopee and the lowest I could find is $280, which is not bad considering there's usually taxes which increases prices of GPUs by 10-20% from global USD MSRP.",Neutral
Intel,">the $249 price for the B580 isn't real.  Yes it is, it just gets sold out as soon as it's restocked - https://www.nowinstock.net/computers/videocards/intel/arcb580/",Negative
Intel,Doesn't it just need a 7600?,Neutral
Intel,Not in Canada. Prices are exactly as launch. Could be your countries problem,Negative
Intel,"Eh, they recently shipped a new batch at MSRP to Newegg.  Don’t know what’s going on with the partner cards. Probably the typical shenanigans",Negative
Intel,You forgot about the smaller dies.,Negative
Intel,"The chip is tiny, so a single wafer -> more chips. The demands on power delivery, cooling etc. are nothing to speak of. And the GDDR6 memory is cheap and readily available.  If anything will ever be in stock, it's this card.",Positive
Intel,5060 has been in stock at MSRP everyday since launch. This should be the same. Even the 5060ti 8gb and 5070 are at MSRP.,Neutral
Intel,MSRP has always been meaningless because it stand for suggested retail price.,Negative
Intel,"But if the recent LTT video is halfway accurate, AMD would murder both of them in that scenario. The 9060 XT 8GB might want to have a word with those two cards.",Negative
Intel,negative margins for intel,Neutral
Intel,The B580 showed we need to test in the systems the cards will end up being used in. The restricted PCIe lines on the lower tier cards i.e. only use 8 also means they need to be tested on older systems.,Neutral
Intel,You really believe you are an average 9950x3D owner? Really?,Negative
Intel,"Out of curiosity, what is your use case in such a setup? I find myself cpu bottlenecked quite often on a 7800x3D but i use a 4070S, when i used 1070 GPU was bottlenecking me.",Neutral
Intel,"But Intel Arc drivers have a really bad overhead, while nvidia does not, so in gpu bound games the 5050 will have around the same performance but the b580 will lose a lot of it, so it's not a weird comment, it's a very important point.",Negative
Intel,>in 1080p 8gb is mostly enough.  The narrative was pushed so much that this fact became an unpopular opinion.,Negative
Intel,nvidia Rx 7600 equivalent or Rx 7600 alternative by nvidia. Ig people can't even get a silly joke nowadays,Negative
Intel,The 12 GB of RAM in the B580 will age more gracefully for a truly low end gamers. On purpose of 8 gb is retro gaming.,Neutral
Intel,"Hate to be that guy but to call this a 30-class card is insane. The 1030 was 25% the performance of the 1060, and the 1630 was around 45% the performance of the 1660. This card is 80% the performance of a 5060, which should probably be called the 5050.  To take it a step further, the GT 1030 was 13% of the 1080ti, the 5050 is 20% of the 5090. The 1050 was 25% of the 1080ti, so it would be reasonable to call this a 5040 or I could even be satisfied with 5040ti.",Neutral
Intel,That would be an interesting card,Positive
Intel,I think a big thing about prebuilts are the aesthetics. Does it look good? is the cables managed? Is the RGB controllable?  Slapping a PC together is easy but building something that looks beautiful through the side panel is not as easy.,Neutral
Intel,The 4060 is 2% and DLSS did plenty of work for the even slower 3060. DLSS is going to be great for this card. Frame gen will be good in games this card can run at good base framerates.,Positive
Intel,"cool, it's in stock now",Positive
Intel,A 2-year old CPU ushering a new platform at the time of B580's launch. Far from a budget user's upgrade timeline-wise.,Neutral
Intel,"For a brand-new truly budget build, AM5 is still restrictive pricewise to people eyeing GPUs of this level. The 3050 and 6600 were well-known inclusions in $500 builds, building with a 7500F is going to cost $280-300 just for the cpu, mb and ram alone. AM4 or LGA1700 otoh can be as low as $180-200.",Neutral
Intel,Depends on the game  The 7600 can see some serious performance deficits in some games like Spider Man  https://youtu.be/00GmwHIJuJY&t=521  and Hogwarts' minimum FPS is affected  https://youtu.be/00GmwHIJuJY&t=468,Negative
Intel,I mean tariffs in the US are a thing...,Neutral
Intel,"It wasn't always meaningless. Overall deviation from MSRP used to be smaller, and it wasn't hard to find the basic models at MSRP once upon a time.",Neutral
Intel,more like we need to test with different processors. Like with high-end to see absolute maximum and some lower grade to see how good/bad driver overhead is,Neutral
Intel,I never said I was,Neutral
Intel,"I'm looking into this now, and you are correct. I am just reading this for the first time.",Neutral
Intel,Lol no. For truly low-end gamers the B580 will perform worse because of its CPU overhead that still hasn't been fixed more than half a year after release.,Negative
Intel,"I mean not really. The b580 is low end enough to the point where it'll become obsolete in performance long before it runs out of VRAM.  It's the equivalent of saying buying a 16gb 7600xt in 2025 is ""more futureproof"" than buying a 12gb 3060 in 2025.",Negative
Intel,an 8 GB card is obsolete at day one for modern games.,Negative
Intel,"It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup. You can find those numbers on Gamer's nexus latest shrinkflation video on this topic (https://www.youtube.com/watch?v=caU0RG0mNHg). Considering those things, the 5060 is essentially a 5050 and the 5050 is a 5030.  The GT 730 had 13.3% as many cores as the full GK110 die, with a 50W TDP.  The GT 1030 had around 10.3% as many cores as the full GP102 die, with a 30W TDP though this one was impressive.  The GXT 1630 had around 11% as many cores as full TU102 die, with exactly 75W TDP.  The RTX 5050 has 10.7% as many cores as the full GB202 die (RTX PRO 6000 Blackwell), and around 11.7% vs the 5090 (which is the number you'll se on Gamer's Nexus video, he rounds it up to 12%, I think that was a mistake on his part? I'm not gonna try and correct him though), but it has a TDP of 130W somehow, just 15W less than the 5060, that's absurd.  Hence why I'm saying it SHOULD be a sub 75W TDP card, and priced way below that. My theory is that it was OC'ed past its efficiency curve to make it at least moderately better than the 3060 and still be able to call it a XX50 card.",Neutral
Intel,Nah most people who buy prebuilts just want a good working PC with minimum effort,Neutral
Intel,Yep. That's a problem for that country. Countries with normal leadership have much more viable options,Negative
Intel,Price is only driven by demand and supply. See how the crazy expensive GPU still sell like a hot cake during the COVID lockdown period?,Neutral
Intel,It's well known by Arc owners.,Neutral
Intel,"The 16gb 7600xt  is a giant pile of crap even with 16 GB , so a 12 GB 3060 is the better choice.  If you go to techpowerup you'll see that the b580 is 17% in relative performance than the ""12 GB 3060 """,Negative
Intel,What if you’re playing at 720p?,Neutral
Intel,"> It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup.  That is a absolute dog shit methodology to use.   The 1080 Ti was 471 mm², 5090 is 750 mm².  Those two products are not the same tier. And can therefor not be used as equal reference points. When it comes to die size the top Pascal card is closer to the 5080 than 5090.",Negative
Intel,The point is more VRAM =/= aging better. The b580 having 12gb isn't saving it from becoming obsolete in a few years.,Negative
Intel,"They re not the same price tier either bro... They could have called the 5090 a 5095ti if that makes you happier.  The guy you quoted is right.   Also comparing the performance diff between the 1030 vs 1060 because guess what the 5060 should have been the 5050 looking at die size and memory bus.   Not sure why people keep defending these naming schemes, do you think the engineers use them internally lol? Its bullshit that marketing comes up with",Negative
Intel,"Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍 Doesn't matter if die sizes get bigger, that's up to Nvidia. Just because the halo product got bigger doesn't mean they can get away with moving their product stack (below the XX90) one tier up in prices.   I guess it is a dog shit methodology to use, only Gamer's Nexus uses it and I'm sure #1 Nvidia apologist u/Alive_Worth_2032 knows more than him.   It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree.  And it's a shit methodology to use if you're an Nvidia executive.  If you can't see that Nvidia is putting all the effort into binning high end chips for AI cause it's 90% of their income, then I don't know what to tell you. Obviously it's a good move for them, and it works, but there's 0 reason for a gaming customer to defend them for it. We used to get much more out of their chips in the gaming cards for more reasonable prices, why's it wrong to want that?   You can't just draw a conclusion based on relative performance between 2 underpowered products, you compare them to the ""best"" Nvidia COULD give us (which is the full top die for each generation) and go down from there.  Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm^2. That's not even an excuse.",Neutral
Intel,"> It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree. And it's a shit methodology to use if you're an Nvidia executive.  You can chose one methodology.   You can decide on comparing what hardware you get at a certain price point.   Or you can ignore price/bom and look only at arbitrary model numbers as if they mean something.  You cannot do both at the same time.  Personally I prefer to look at die area. The 5050 today is roughly comparable to the 1050 Ti, comparing it to the top cards that are not comparable is irrelevant.  Pascal did not have a analogue for the 5090, period.  >Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍  That changes nothing. The Pascal Titan are using the same die as the 1080 Ti. The whole tier of die that is used in today's consumer top SKUs and for the RTX 6000 Blackwell, DID NOT EXIST back then.  >Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm2. That's not even an excuse.  What are you even trying to say? Ampere also did not have a analogue to the 5090. The 3090 Ti, even it is sitting almost a tier below the 5090. The 2080 Ti and 5090 are unmatched in other generations.",Negative
Intel,"You’re still missing the point by focusing purely on die area and pretending model numbers are arbitrary. Model numbers mean something, nvidia knows that so that’s why they kept them consistent for so long (unlike AMD that's hella inconsistent). Because they rely on the perception of tier consistency from generation to generation, even if they’ve worsened the specs behind the scenes.  You say you “prefer to look at die area,” but that’s irrelevant unless you’re building the chips yourself. Customers don’t game on silicon real estate they game on actual performance and hardware capabilities. And the cuda core count + bandwidth vs top-die approach directly shows how much nvidia is offering relative to what they could offer if they weren't prioritizing AI margins.  So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  > Pascal did not have a 5090 analogue  That’s just semantics. Every generation has had a full-fat top die, whether branded as “Titan,” “RTX 6000,” or whatever, which where I drew the comparison. Comparing lower-tier cards to the top die is how you reveal how much of the architecture's potential is being offered to gamers.  And yes, 3090 and 3090 Ti still preserved proportionally higher specs vs top-tier dies. What changed now is not just die sizes, it’s NVIDIA reserving most of the silicon for AI and throwing scraps to gaming.  > You can choose one methodology...  Sure, and I chose one: compare the lowest-tier GPU to the top die, which accurately shows how nvidia has been shrinkflating consumer value over time. You're welcome to look at BOM and TDP too the 5050 still loses. It should be sub 75W and priced accordingly.  I think it's okay to call it an XX40 series card even if we keep the TDP at 75W but the XX30 series would need to disappear. What I don't agree with is using the die area excuse to justify shrinkflation just because the 5090 is such a halo product.",Neutral
Intel,"> So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  Do you even listen to your own madness?  You realize that what you are saying. Is that the lower end 5000 series would be ""better"". If Nvidia removed the 5090 entirely. And renamed the 5080 the 5080 Ti, and made the 5070 Ti into the new 5080.  Suddenly, you would praise the lower end cards for being ""better"". Purely because the high end is less powerful. Nothing else changed, no one is getting more hardware for their money.  Jesus Christ the mental gymnastics you people go trough.   The hardware Nvidia gives you for your money, is all that matters. How large the top die is and what core configuration it has. Is irrelevant for the value for cards further down the stack.",Neutral
Intel,Everything,Neutral
Intel,"Upgrade your cpu to 14600k, add 16gb same model if possible and the gpu is up to you bc the 14600k can handle any gpu.",Neutral
Intel,CPU,Neutral
Intel,"A CPU upgrade will do the most for the games you play, even something like a i5-14400 would go a long way all by itself. Upping the ram could help some if you can get the same kit you already have for a reasonable price",Positive
Intel,If you only play esports games then upgrade cpu. Also  ram doesn’t cost too much so you might aswell upgrade that.,Neutral
Intel,"Used 16 gigs of ram, update bios and get 14 gen Intel cpu",Neutral
Intel,"You can get good CPU and GPU for a ""budget"" price if you checkout Facebook marketplace. With these upgrade, you can get more FPS.",Positive
Intel,bottlenecked by cpu. You'll double frames getting i7 12700k/kf for sure. those games you mentioned are cpu intensive,Neutral
Intel,"Jesus, you have like the lowest end hardware for the generation you bought into.... you can upgrade litterally everything.",Negative
Intel,So i Dont have that much budget but i want to be able to not worry about having low fps also my monitor is 165hz so i dont need that much power but i just want to know what can i keep because changing everything is a lot of money for me right now   Thank You for Your response!,Neutral
Intel,You didn't have to brutalize him like that,Negative
Intel,Damn I was gonna say that,Negative
Intel,Ok I would look for a i5 14400 and some ram but when I have more budget what would you recommend me to upgrade later on,Neutral
Intel,"This is the only reasonable comment, why is everyone else acting like FPS games require 1500 dollars of hardware",Negative
Intel,I remember buying my first RAMsticks. Way cheaper about 8 years ago.,Positive
Intel,"It's possible his motherboard is older and only supports DDR4. In that case, the memory will be expensive and not worth upgrading.  \-   Added later, I did notice he included the motherboard name, and based on a quick search, that is a DDR4 motherboard.",Negative
Intel,Just being honest haha not gonna sugarcoat it. Man’s needs a new cpu and ram which requires a new motherboard and a new gpu will of course help a lot.  Literally nothing he can keep  7800x3d/9800x3d  Corsair 6000mhz ram have kits relatively cheap  Hell a 5060 is a lot faster then his Intel gpu but I’d opt for a 5070 minimum Any am5 suitable board,Negative
Intel,I saw that too I was like 🤧,Negative
Intel,I’m pretty new to pc gaming,Neutral
Intel,yes its ddr4,Neutral
Intel,You don't need a fucking 7800x3d and 5070 for fortnite and valorant,Negative
Intel,All good. I wasn't really judging. Just made me laugh for real  Your response will help him. Gotta start somewhere,Positive
Intel,Someone’s awfully angry haha the guy didn’t post anything regarding budget. I also mentioned 5060 was a huge upgrade and just said what I would get.   If he’s gonna go through the trouble of upgrading his CPU and ram which would require a new motherboard anyways why not get something that’ll last him years. No one “needs” half the pc parts they get but if you keep your pcs for 6+ years then yes a 7800x3d would be a much better choice.   It’s already dropped in price and likely to go on sale during Black Friday. Also gives the guy a chance to upgrade his monitor over the next year or 2 if he so chooses.  You see I don’t mind going to work saving my money and spending just over $1000 for a pc I’ll get years of use out of. Others like you might like buying a new mid range mobo/cpu and then upgrade the gpu later when it becomes problematic and then upgrade the cpu and mobo again in this constant cycle or low tier parts. Everyone’s different lol,Neutral
Intel,BUT IT WOULD BE  NICE TO HAVE n why do you think those will be the only games he ever plays? Lol,Positive
Intel,Because they're the primary games he mentioned? You're recommending he drop well over a grand on new parts like it's the obvious thing to do. An A750 with a better CPU will play most games fine at 1080p just fine.,Neutral
Intel,So he should build a system that can play every game? *Every game?*,Neutral
Intel,I have a Sparkle A380 and it barely breaks a sweat doing 4K transcodes on my Plex server.,Positive
Intel,How many are you expecting to do? I think any modern Intel iGPU can handle 4k transcoding. I think even the $200 N100 type mini PCs could manage a pair of streams.,Neutral
Intel,"The P3 Plus uses QLC flash, which has low durability and becomes noticeably slower after a few years. Get a better drive with TLC flash like the Kioxia Exceria Plus G3, WD Blue SN5000, TeamGroup MP44L/G50, Patriot P400 Lite, or Klevv CRAS C910.",Neutral
Intel,"N305, save on that juice bill (:",Positive
Intel,i want the capacity for 6 streams. i think any more than that would get hectic  i am trying to make direct play available on most clients by saving lower quality media options for each show/movie so there's that. I'm also going to eventually test out live sports streams but maybe that will be too much!,Neutral
Intel,"looking into this now, looks great!  do you have one? im a noob, it seems to be limited to 32gb ram? not that im looking to get more but would like to eventually if i run more services",Positive
Intel,Ooo okay then yeah the a380 seems like it might be the way to go then.,Positive
Intel,"I do own one, it's a low power chip which can run a couple transcoded streams just fine. There are a few of those chips, not sure which one's the best choice these days but you've got a couple options. They come with the board itself and are often rebranded by various Chinese brands. I think CWWK was the original manufacturer of those boards.  A lot of people, including myself are running it with 64GB of RAM without issues. I'm also running about 30 different containers of all sorts and while there are certainly some use cases it might struggle with, it will be fine with most. Keep in mind it's physical connections are somewhat limited.",Positive
Intel,I hope your RAM isn't actually that slow. The Microcenter bundles tend to be DDR5-6000 CL36.  There's no faster GPU for that price. Anything faster would cost more.  I would recommend the Western Digital SN7100 SSD over the slow one that you picked.,Negative
Intel,should’ve gotten a ram with atleast 6000mhz.,Neutral
Intel,"I suggest some changes. First cooler, You can get a [Thermalright Assassin X 90 SE ARGB 32.77 CFM CPU Cooler](https://pcpartpicker.com/product/Vz9wrH/thermalright-assassin-x-90-se-argb-3277-cfm-cpu-cooler-ax90se-argb) for 16 to save a few dollars to put somewhere else.   You a bit overkill on the psu as the wattage is nearly double what you might be putting together. Great for later upgrades but overkill for this build.   It is not a bad combo deal but there are better choices to be had. The ram is slow but you can use it for now. I would change the ssd. That is a gen 3 and you wil want at least a gen4 with a higher speed.   Using your combo I would change the cooler, cheaper option with good performance, the ssd, while budget and not the fastest it is 2tb gen 4 with decent speed and fully modular 750 80+ gold atx  or you can stick with your 850 watt which ever you prefer.   That is a good card for the cost but i would be tempted to switch it out with a 9060xt 16 gig or 5060ti   [https://pcpartpicker.com/user/xxSineaterxx/saved/#view=YXkXYJ](https://pcpartpicker.com/user/xxSineaterxx/saved/#view=YXkXYJ)  With the 9060xt 16gb reaper the cost is slightly higher but with that card you should get better performance.",Neutral
Intel,gotcha you're saying the silicon power is slow? or the RAM?,Neutral
Intel,"thanks for the advice i'm considering changing the psu and storage for sure, also your pc list is private i can't see it",Neutral
Intel,Both.  The RAM in your PCPartPicker build is DDR5-5600 CL46. This is incredibly slow and I honestly doubt that's what you bought from Micro Center.  The Silicon Power A60 SSD you picked is also slow.,Negative
Intel,"Ram companies don't matter 95% of the time. What you need to watch for are the specs.   CL is your rams reaction time. CL30 or 32 is good.  And MT/MGHz, are it's actual speed. 6000mt or 5400mt is good.  In your last post, we advised you that your ram was too fast because you had a CL28 ram kit. you might have over-adjusted and picked slow ram, my friend. 5600mt is good speed, but cl46 is slow reaction time.  A CL32 5400mt will probably be faster than your ram.   Honestly, everything else is great👍 this pc will serve you for a  good while, even with the slow ram.",Neutral
Intel,Oh sorry lol I was working on another build private  [https://pcpartpicker.com/user/xxSineaterxx/saved/#view=HJy4Bm](https://pcpartpicker.com/user/xxSineaterxx/saved/#view=HJy4Bm),Neutral
Intel,"thanks, yeah yall connvinced me. i'm returning it",Positive
Intel,Yes you could just swap the gpus,Neutral
Intel,What exactly is your question?,Neutral
Intel,Do ai really nees to invest in a 14700 and or AMD 5 system if I get a 5079ti or can I just got 5070ti,Neutral
Intel,"What's the most economical and logical path. Opinions. Keep the LGA1700 build and just keep the cpu or upgrade to 14700 to not bottle neck 5070ti or would it not matter? 2nd invest in AM5 system way more expensive it seem for a new mb,cpu,ram, cooler for why to ""future proof?",Neutral
Intel,"How much budget are you willing to allocate. If it were up to me, I would go up for the 5060 Ti, for my main uplift. Otherwise, I would say a B580.",Neutral
Intel,"Better overall drivers, 5060. More VRAM, B580.",Positive
Intel,Goes a bit over my budget. Probably going to go with the B580. Thanks!,Neutral
Intel,"You can reasonably assume that there will be driver support for a significant amount of time for the B580, honestly it'll probably be supported for the entire time that you have the card. It's unfortunate in that this probably kills future products in the Arc line, but given Intel's need to shed costs, it's also not all that surprising.",Neutral
Intel,Intel releases a competitive GPU   Nvidia instantly proceeds to buy a part of Intel...,Neutral
Intel,Seems to me like they'll be building SoC hardware for like mini-PCs or laptops.  Maybe integrated graphics for desktop CPUs.  It doesn't seem like it's competing with discrete GPUs.,Neutral
Intel,This is shit. Hopefully they still make the mid range 700 series. I was sort of holding out for that but it's taking too long.,Negative
Intel,"This does suggest that Intel GPU unit will likely get the axe.  It also suggests that ARM doesn't have the HPC ability of x86.  ARM was half as fast, at the same power, as AMD EPYC.  So, Nvidia is likely admitting that ARM isn't close enough yet.  Real question is how long this takes to materialize?  Further, this could violate x86 agreement.",Negative
Intel,They aren't going to sabotage a division of a company they just bought for 5 billion. I think it's a problem for the new year release of the 7 series cards though. Those were supposed to challenge some of the mid range cards.   They aren't dropping driver support for a LONG time. By the time you have to worry about it you wont own it.,Negative
Intel,Wasn't arc already on its way out?,Neutral
Intel,"Alright, good to hear. I’ve been pretty happy with it so far otherwise!",Positive
Intel,"Swatch owns all the watch brands. Meta, alphabet and Microsoft own all the tech brands. Nestle, Unilever own all the commercial food products etc etc",Neutral
Intel,Lol seriously. This is how capitalism in america works these days!,Negative
Intel,"True, my first thought was consoles and handhelds (Steamdeck).",Neutral
Intel,I really hope this is the case because I love the idea and technology behind arc and really want it to become more competitive.,Positive
Intel,"Also consider that even for Nvidia spending 5 billion to just terminate a division of intel isn't why they did it.  They'll probably slap their brand on it and rake in the profits. I'd expect a 10% jump in msrp but still.. They aren't in business to lose money. I think they'll keep production going personally, put their stamp on it, sell it for a bit more and keep the status quo.   I think they were worried about what the new intel card was going to do to their low/mid range lineup. The dev cost for the 700 series has already been spent. They will release it.. Just at what price point?",Neutral
Intel,5 billions is a small price to pay to eliminate a competitor in their cash cow almost monopoly segment.,Negative
Intel,b series is less than a year old,Neutral
Intel,it's always worked that way.,Neutral
Intel,Thats true i guess if were being honest,Neutral
Intel,Save a little more and buy 9060 XT 16 Gb to serve long life without pain in the ass because of potential Intel discontinuing of support or lack of VRAM - this is the best solution.,Positive
Intel,"B580, the 8gb 9060xt isnt a super attractive option. The 16gb variant is a better card but its usually above ~40% more than the b580",Negative
Intel,"If you are not going to be buying a new gpu for a good few years I would go with the 9060xt. There is no telling how long Intel is going to maintain driver support, and how much improvement there will be with the driver support that is left. B580 still suffers with issues from launch like cpu overhead, if there are game specific bugs or optimizations that they need to do for in drivers for new titles you might just be shit out of luck.",Negative
Intel,B580 is significantly slower on weaker cpus like your i5. Get a 9060xt,Negative
Intel,Neither. Get used if those are your only new options. 8gb is not enough for a new card and Intel GPUs are e-waste and are plagued with driver issues. I also doubt Intel is going to be pumping resources into improving that situation because dedicated intel graphics are effectively canceled.  9060 XT 16GB is the cheapest GPU worth buying new.,Negative
Intel,That 9060XT will become obsolete by its own VRAM very very soon. Either save for the 16GB version or go with the B580. Run away from any 8GB GPU if you want it to last more than a year.,Negative
Intel,"It's a solid GPU for the price point. Just keep in mind that it isn't really ""on sale."" That's the MSRP for it.",Positive
Intel,I find the best time to buy a GPU is just after lunch. That way you're not overly hungry and won't spend more than you have to.,Positive
Intel,"It’s unpredictable. GPU prices are trending downwards, but you never know what’s going to happen. My opinion is if you need the GPU, just get it.",Negative
Intel,Huge leap in performance if you can squeeze a little more I would go with 9060XT 16GB.,Positive
Intel,"From the research I've done, a lot of the more widely seeked out GPU'S sell out before black friday. So if you see one you like at a price that fits your budget before then, go for it. Here in the next couple of weeks I'm planning on getting mine (9060xt 16gb).",Positive
Intel,Watch this before you decide: [B580 ($250) vs 9060 XT 8gb ($260)](https://www.youtube.com/watch?v=gfqGqj2bFj8),Neutral
Intel,"With regards to AAA, it's very entry level.  Functional, but that's about it.  A 9060xt 16gb is worth the price jump but if you can't squeeze that, the b580 is okay.",Neutral
Intel,B580 has weird ass bugs so if ur ok with tinkering : troubleshooting weird stuff occasionally it’s good,Negative
Intel,Sometimes they put them on sale towards the end of the day when they are getting close to expiry date.,Neutral
Intel,"I'm wondering the same about the 9060xt 16gb.  they're quite cheap right now so I don't want them to go up in price or go out of stock, but I'm not sure I can afford one right now but maybe in January or so.",Neutral
Intel,"When they are cheap and make sense to buy. There is no set time, prices change all the time for countless different reasons including random sales.  Any decent current gen GPU for $250 in 2025 is not bad. You can get 3-4 years out of it no problem, then think about the next upgrade. And if you're lucky you can recoup about 40-50% of the cost you paid for the card when it was new.  I paid $250 for a 6700XT three years ago to the date. Still using that card now, and it's worth the same.",Positive
Intel,It's pretty random. Sometimes the price of last gen GPUs go down a little when a new one is coming out but demand fluctuates wildly directly affecting prices & availability. Most of the time when a new GPU comes out the inventory gets snatched up pretty quickly by bots online at retail price then people flip sell them for almost double the retail price. Your best chance of getting a new GPU for retail price is maybe Best Buy or Microcenter.,Neutral
Intel,Not really. GPUs are more supply and demand. You never  know what prices will do. Monitors are usually a good buy on black Friday tho.,Neutral
Intel,Tomorrow,Neutral
Intel,Im waiting for UDNA i just bought a 6700xt about a year ago though for 240 usd,Neutral
Intel,Whenever you have the money saved up for a GPU in your budget range  And whenever you find your current GPU is not meeting your needs anymore,Neutral
Intel,Well that's MSRP so yes,Neutral
Intel,"In spring, between March and May. You might also be lucky and find  some sales during black weeks soon.",Positive
Intel,Right now is the time. 50 series is at MSRP. Used gpus aren’t going for insane prices right now except for like 4090.,Neutral
Intel,After you sell a kidney is the ideal time to buy a GPU!  Or after you rob a bank!,Neutral
Intel,"I think you'll be overall happier if you get a 9060 XT 16GB, you'll probably have less driver issues, and it'll have FSR4 support. As to when, it's hard to say. If you need a card now though, it usually makes sense to get a card now.",Positive
Intel,"I bought parts on 2019 blackfriday/Christmas only needed GPU & PSU, wasn't sure what to get and then 2020  With prices being so ridiculous, I just left it, 2023 I got a PS5, 2024 Finally got a 4070 super for £600.   Blackfriday is still a while, buy now, build it and enjoy your PC!",Neutral
Intel,"If not black Friday, cyber Monday has decent sales",Positive
Intel,Generally speaking when you can afford it and it's a good price.,Positive
Intel,"Finally, someone get's it...",Neutral
Intel,What about the 8GB version? It seems like it would be the middle ground between the B580 and 16GB 9060XT.,Neutral
Intel,"Thanks for that video, that was incredibly helpful and informative!",Positive
Intel,Sometimes you can find bags of them in the dumpster behind bestbuy after they close for the day,Neutral
Intel,I just traded in an RX 7600 for a 9060xt 16gb from NewEgg and it has been awesome.  I was on the fence for a while but I’m happy I went with it,Positive
Intel,I've seen several comments recommending it so I actually cancelled my order I had placed of the B580 to reevaluate lol. It seems that even the 8GB version of the 9060XT is a tier above the B580. Would the 8GB version be worth getting over the B580? Or only the 16 GB version?,Neutral
Intel,At 1080P most games no issues but AAA games of the future could limit graphics at higher settings.,Neutral
Intel,Consider a used 3080 too. It's faster than both and has dlss4,Positive
Intel,nice.  I'm on a 3060 and still deciding if I should upgrade for bf6 or not.,Positive
Intel,"No, don't get any 8GB card. Performance suffers, graphics quality can really suffer, on newer games. The price difference is small, get a 16GB card.",Negative
Intel,If it's just about the vram then he might as well just get the B580.,Neutral
Intel,"Why not try BF6 on your current system and see if you're fine with the performance at the settings you prefer? The game comes out tomorrow (October 10), so you'll have an answer soon!",Positive
Intel,It's as if it's not all about vram... Which was the entire point of me linking the video.,Neutral
Intel,"Clearly shows the B580 being the weaker card, **despite having more vram.**  >getting a new 8gb card in 2025 is always going to be a bad idea, it's extremely limiting.  Yet the 12gb card is clearly weaker. Pick a lane.  Where in the planet can you buy the 16gb variant of the 9060 XT for $260?",Negative
Intel,"And here, ladies and gentlement, is a fine example of the dunning-kruger effect in action.",Neutral
Intel,What are you upgrading from? What resolution do you play at?  [https://www.techpowerup.com/gpu-specs/geforce-rtx-5060-ti-16-gb.c4292](https://www.techpowerup.com/gpu-specs/geforce-rtx-5060-ti-16-gb.c4292)  its 65% faster. 5060ti 16gb is like a strong 1080p to entry level 1440p card while 5070ti is a strong 1440p card to a decent 4k card. I'd say it's worth the upgrade. usd msrp is $750 so I'm not sure which country you are buying in.,Neutral
Intel,"Yes it’s definitely worth it. The 5060 are entry level cards, 5070ti should blast through any game as long as you’re not playing at 4K",Positive
Intel,"You get ARC Raiders Deluxe Edition if you buy a 5070 or better right now.   So that narrows the margin by 60 bucks that you’re going to spend anyway, so I’d get the 5070ti.",Neutral
Intel,"100% worth it if you can afford it. i got my gf a 5060Ti 16gb because she plays minimal games and Im a broke student where that extra $120 is really crucial for me. otherwise, the 5070 is better value.",Positive
Intel,"At 1440p, yes.  At 1080p, no.",Neutral
Intel,the 5060ti will probably fulfill your needs but will it sate your wants is the question.,Neutral
Intel,"yes.  especially if you are playing ""competitive"" type first person shooters AND you have a high hz monitor and want smooth gameplay.   there are people who play only older games or ""cinematic"" single player games and having 60hz and 60fps is fine.   but for online multiplayer shooters, higher hz and fps matters more.",Neutral
Intel,"I think if you want to have very high performance for the next few years, yes.",Positive
Intel,Yes for sure.,Positive
Intel,"I've had both (currently on the 5070ti). It was well worth the extra money.   I play a lot of vr, and using the exact same settings,  my 5060ti was almost always running at 100% usage, my 5070ti does not even break 50% usage",Positive
Intel,"Yes. It is faster but the fact you asked this question, if you go with a 5060 Ti, you’re probably going to have serious FOMO right away. Just bite the bullet and get the card you want, especially if you’re planning AAA titles.",Neutral
Intel,Most likely if your current power supply can handle it and you're playing above 1080P.,Neutral
Intel,"very worth it, looking at around a 66% performance jump at 1440p.",Positive
Intel,"I have the non-ti plain old peasant 5070 and it runs any game at 1440p, 120fps with no issues.",Positive
Intel,"I got the 5060ti Good 60fps at 1080p on cybperpunl full ray tracing. Its a good card for 1080p. But had to RMA and got a 5070. Middle ground: way to overkill for 1080 but lacks a punch for 1440p. Then had to rma again (both Zotac twin edge fans) and got a MSI 5070ti. Crushes 1440p as well as 5060ti crushed 1080p. Both are good cards, just pick your resolution and go with it, for me it was worth",Positive
Intel,"Don't believe anybody claiming the 5060TI 16gb isnt capable of 1440P gaming. This card is an absolute unit. Mine is overclocked and the performance is slightly higher than stock 4070/3080. It sips power at 140-170 W, super quiet, runs everything i throw at it at 2k fully maxed out. I play the witcher 3 next gen fully maxed out ray tracing and all and hair works with DLSS Q im getting 55-60 FPS. Throw in framegen and thats an easy 100 + FPS. And yes framegen is amazing i dont care what doomers have to say. Also having 16gb of vram gives me peace of mind with no texture popping whatsoever. Honestly id grab it if the price difference is that high",Positive
Intel,Yeah just get the 5070TI at that point if you have the money for it. Even the 5070 with its 12GB of VRAM is better than the 5060 TI 16GB. Especially if you're gaming at 1440P or 1080P. Right now since it's prime day I'd also take a look at the 9070XT as it's right around the same price as the 5070TI or could be cheaper. And it's raw rasterization it's on average about 5-10% better than the 5070TI depending on the game. But if it's cheaper than might be the better buy.   Obvs if you just want NVIDIA then just stick to them but I'd take a look at the prices to see what makes sense for you,Positive
Intel,Yes,Positive
Intel,1080 - 5060 1440 - 5070,Neutral
Intel,"Just last week I was mulling the same thing.  I bought a 3060 in 2021 because that was the only thing I could get my hands on.  Now that we're back at MSRP, I thought to myself, ""will this last?""  With all the turmoil, I figured it may very well be possible that we don't see MSRP for another 4-5 years, so I just went with the stronger card to, hopefully, last me longer.  My 5070 TI just came yesterday and I don't regret it.",Neutral
Intel,"The 60 cards are mostly 1080p/ultra. 70 is 1440p/ultra. 80 is 1440p/ultra high fps and 4k. 5090 is overkill but good for production jobs.  These are numbers for unoptimized 2025 AAA games. If you play old stuff, 5060ti can probably be a good 1440p card for that",Neutral
Intel,Yes,Positive
Intel,"Only you can make the call, but yes, they are *very* different GPUs. The 5070 Ti is an entire two performance classes above the 5060 Ti, and close enough to the 5080 that people question whether that is worth the extra cost. Do you need all that extra performance? Who knows, but the difference in price is definitely warranted.",Neutral
Intel,"If you can afford (this is the important part) it you should always go for the better card. Trust me, no one ever regretted buying the better card at the end but many did regret not spending a bit more to get the better card.",Positive
Intel,"What's your setup?   Are you playing 1080p, 1440p or 4k?   What PSU do you have?   What CPU do you have?   What do you plan on doing for future upgrades?   These are all relevant in deciding whether it's worth it or not! But fundamentally the 5070ti is an incredible card!   Or, you could get nigh on 5070ti performance, but at least cost, with an AMD RX9070xt. Prices probably vary depending on where you live though!",Positive
Intel,"i would say the middle point between those 2 would be the normal 5070. Its at msrp or lower nowadays. Its the one i use and i like it. It runs any game better than the 9060 xt or the 5060 ti.   But if you want more power and you can afford it, go for the 5070ti. In my case after seeing what i play the 5070 was the perfect fit for me.",Positive
Intel,Why not a 5070?,Neutral
Intel,"Consider your CPU as well. I just bought a 5070ti, and while it's great, any game that demands CPU power is not a big difference. Basically any multiplayer game i've tried doesn't appear 65% better.  Also, at least where I am, the 5070ti is 75% more expensive after tax.",Neutral
Intel,Get the RX 9070 Xt if you do end up going with saving. The Rx 9070 Xt performs much better than the Rtx 5070 Ti. AMD GPUS also work better with Linux (which doesnt necessarily affect everyone but is still nice),Positive
Intel,5070ti and 5080 are only like 250$ apart with all the sales going on... might wanna just get a 5080.,Neutral
Intel,I'm running on a 3060 TI currently and run on a 3440x1440 ultrawide monitor,Neutral
Intel,"lol what? I’ve been using 5070Ti for months now, and had no trouble running anything at 4K. It absolutely crushes 1440p.",Positive
Intel,5070 ti imo is 4k card with some caviat (dlss),Neutral
Intel,"I didn't even know this until AFTER I bought the card, which was awesome because I hadn't preordered arc raiders yet lol",Positive
Intel,"Just curious, why not the 9060 16gb?",Neutral
Intel,What about the non-xt 9070? How/where does that fit into all this? Is it a better buy than the 5070 non-ti?,Neutral
Intel,What about the non-xt 9070? How/where does that fit into all this? Is it a better buy than the 5070 non-ti?,Neutral
Intel,https://www.techpowerup.com/gpu-specs/radeon-rx-9070-xt.c4229  The 9070 xt is on average slower than the 5070 ti in raster although it can be faster in some titles but on the other hand the 5070 ti has some pretty big outliers occasionally  20-30+ percent faster in a few titles in raster,Neutral
Intel,Wdym much better it is litterly slower with worse features,Negative
Intel,"[https://www.youtube.com/watch?v=w94bD3S8K0M&t](https://www.youtube.com/watch?v=w94bD3S8K0M&t)  5080 is the worst price performance card of this gen and this video illustrates the point well. tldw, both cards are strong 1440p and decent 4k level cards, you are getting basically the same gaming experience for worse price performance and same vram.",Negative
Intel,The 5070ti is definitely a better buy than the 5080 a for the price. You get very little more power for that 350-400 price difference. If they were $750 vs $850 then sure but not 750vs1000 or more depending on location,Positive
Intel,"5070 ti, or a used 4070 ti super.",Neutral
Intel,5060ti 16gb is a 30% jump but tbh its not worth it. Id say a noticeable change to your gaming experience would be a 5070ti and it would better handle your resolution,Negative
Intel,"This is what I upgraded from to 5070Ti, it’s another league, you won’t be disappointed.",Positive
Intel,"People subjectively have their own opinions about performance. So, for many a card that does 4K/60 isn't a ""4K card"" because it doesn't do 4K 90, 120, etc... which is wild because not long ago that argument was ""but can it do 4K 60?"". Or, they move the goal post in some manner.  And the market is now flooded is cheap high refresh rate mid quality or worse IPS screens, because it sells better than a quality accurate display at a lower refresh rate. Some prefer visual fidelity and some prefer higher frame rates. It's not like they can't coexist, instead people just project their opinions about resolution, framerates, and panel type.",Neutral
Intel,Depends what you accept as acceptable. I prefer max quality 90+ for single player. 140+ multiplayer.. I also went 240hz and no card does that in new games in 4k without DLSS or frame gen.,Neutral
Intel,I mean it’s like what 4k 50-60 usually? If that’s what you like that’s fine but I personally wouldn’t l call that great,Neutral
Intel,Game looks so good. As a tarkov refugee I can’t wait for it to come out.,Positive
Intel,"$380 at Best Buy when I got it so close in price to a 9060xt, and the bigger reason is because she wants to get into content creation and I opted for nvidia’s better hardware encoder.  otherwise I would’ve gotten her a 9060XT.",Neutral
Intel,"It essentially has the exact same performance as a 5070 at 1080P. But the 9070 is slightly better at 1440P and also has 16GB VRAM. It also is exactly the same price most of the time, should be about $550 for both. Even in Ray tracing the 9070 is actually on par or better than the 5070 in most games RDNA 4 really was a big leap for AMD.   It really comes down to price and if you find either below $550 it's a really good deal on the 5070 or 9070. Finding either the 9070XT or 5070TI at below $750 would also be a really good deal.",Positive
Intel,Overall that might be true I was probably reading an article or viewing a video where the test cases favored AMD looking back at Daniel Owens video where he did 100 benchmarks on both overall the 5070TI was indeed about 3-4.5% faster generally speaking.      I did see a video where the 1% lows were actually quite a bit better on the 9070XT but honestly can't find the source so I could be talking out my ass on that.  But realistically they are more or less the same unless its in RT Performance than the 5070TI is the one to get for sure,Neutral
Intel,Got a guy i know that runs a 4070 and claims they are playing on 8k. Claim kinda confusing to me.,Neutral
Intel,Im sorry I dont speak serf.,Negative
Intel,4K is awesome but I absolutely hate 60 for multiplayer. Loving my 1440p OLED for 5070ti,Negative
Intel,"It can do many games at max quality with more than 90 frames though... Cyberpunk native, absolutely not, but neither can a 5080 or a 5090.",Neutral
Intel,I ran a 7900XT for a while with a 4K 144hz monitor and never felt it was struggling. Many multiplayer games above 200fps native The 5070ti is a more powerful card as well.,Positive
Intel,No? Most games I’m playing surpass 100+ with DLSS,Neutral
Intel,"Appreciate that, thanks!",Positive
Intel,https://www.reddit.com/r/Amd/s/BJXoqoh8WQ  Yes the meta review shows in about 30-40 or so of the reviews the 9070 xt was faster than the 5070 ti in like 6 or so. Averaging the reviews the 5070 ti was slightly faster like 3-5 percent  Basically it is game dependent which is faster but there are a few large outliers for the 5070 ti that push it over on the average  Rt performance is also very similar between the two only(10-20 percent difference)   path tracing is where there is a major difference but that is only in like 3 games,Neutral
Intel,Its possible  The games they play might be easy to run or they are not sensitive to low/choppy fps. I’ve never heard of an 8k monitor being used lately haha,Neutral
Intel,"I have a 4090. But even if I didn't have one, it's just the objective facts lol",Neutral
Intel,Yeah what are you running there big dawg?,Neutral
Intel,Then why are you talking about a 5080 instead of a 5090? Surely you're trolling?,Negative
Intel,Yeah I hate 60fps. Everyone has their own preferences for sure.,Negative
Intel,I have no idea. Had never heard of it being done before myself. I run 3440x1440 34inch myself. Didn't even know 8k monitors existed. Hence my confusion.,Neutral
Intel,Maybe he uses a TV? Most newer TVs are great monitors although if they're too big you need to sit further away.,Positive
Intel,"Ahhh, ok that makes sense. Totally forgot about TV, that's cool! Thank you.",Positive
Intel,"Here's my take, CPU is a bit weaker but I fit a 9060XT 16GB in there and the PSU is higher quality too.   [PCPartPicker Part List](https://pcpartpicker.com/list/Pnmyh7)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 5500 3.6 GHz 6-Core Processor](https://pcpartpicker.com/product/yq2WGX/amd-ryzen-5-5500-36-ghz-6-core-processor-100-100000457box) | $75.98 @ Amazon  **Motherboard** | [ASRock B450M-HDV R4.0 Micro ATX AM4 Motherboard](https://pcpartpicker.com/product/RD97YJ/asrock-b450m-hdv-r40-micro-atx-am4-motherboard-b450m-hdv-r40) | $69.50 @ Amazon  **Memory** | [Silicon Power XPOWER Gaming 32 GB (2 x 16 GB) DDR4-3200 CL16 Memory](https://pcpartpicker.com/product/B8m2FT/silicon-power-xpower-gaming-32-gb-2-x-16-gb-ddr4-3200-cl16-memory-sp032gxlzu320bdi) | $69.97 @ Silicon Power  **Storage** | [Patriot Viper VP4300 Lite 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/zPQKHx/patriot-viper-vp4300-lite-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-vp4300l1tbm28h) | $67.98 @ Amazon  **Video Card** | [PowerColor Reaper Radeon RX 9060 XT 16 GB Video Card](https://pcpartpicker.com/product/fzh2FT/powercolor-reaper-radeon-rx-9060-xt-16-gb-video-card-rx9060xt-16g-a) | $349.99 @ Newegg  **Case** | [Montech X5M MicroATX Mini Tower Case](https://pcpartpicker.com/product/BpQwrH/montech-x5m-microatx-mini-tower-case-x5m-b) | $54.90 @ Newegg  **Power Supply** | [Thermaltake Toughpower GX2 600 W 80+ Gold Certified ATX Power Supply](https://pcpartpicker.com/product/pG6qqs/thermaltake-toughpower-gx2-600-w-80-gold-certified-atx-power-supply-ps-tpd-0600nnfagu-2) | $64.98 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$753.30**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-10-02 01:33 EDT-0400 |",Neutral
Intel,"If you're close to a microcenter you can replace the CPU/MB/RAM with the 7600X bundle for about the same cost. It'd come with a single stick of 16gb though, so you'd have to add another for $50 to make it even.",Neutral
Intel,"Sure. B580 is best value GPU on the market right now, as it comes with BF6",Positive
Intel,Used or new?,Neutral
Intel,"7700 xt, you can find them new at around 320$",Neutral
Intel,Look around Jawa:   Gigabyte Gaming OC RX 7600XT | Jawa https://share.google/drh4RK5YDMkwiXxLS,Neutral
Intel,Used 3080 go for about $350 and under now a days,Neutral
Intel,Try a used or lesser brand 3070. If you're in usa theyre at newegg for less than 270usd,Neutral
Intel,"9060XT with 16GB of VRAM, the only budget card worth buying. Friends don't let friends buy 8GB VRAM cards in 2025.  FSR4 is a game changer and this upscaling and frame gen tech will give you added performance with improved visuals and is quite frankly the future of gaming, and will be implemented into the PS6. Sony helped AMD develop FSR4, or should I say, they co-developed it.  Here you are, by the way....  https://www.youtube.com/watch?v=ApAOw29_QCE",Positive
Intel,"7500F + motherboard + DDR5 for £324:  https://uk.pcpartpicker.com/list/gbYMsp  9600X + motherboard + DDR5 for £354:  https://uk.pcpartpicker.com/list/WzrdLc  I'd go for the 9600X personally, for that motherboard you may need a BIOS update. You could sell your 5600X + mobo + DDR4 as a bundle on FB Marketplace or something. Ali Express may have cheaper stuff if you trust them",Neutral
Intel,"Based on what games you say you play, I'm not sure a platform upgrade is required at this time.    What might be the easiest, cheapest, and most prudent option is to just upgrade your GPU. Then, if you're not getting the performance you desire, upgrade the platform.    What budget do you have? What resolution and refresh rate do you use?",Neutral
Intel,i want to mention as user who has similar pc ( as in i have build of )  ryzen 5 9600x    32 ddr 5    6700xt    etc.  it still pity good pc ( just might want to upgrade later after cpu upgrade you might just want newer gen gpu )  but even then it going be running perfetly fine at 70+ fps stable ( while some optimize 120+ fps easily with ( 6700xt ryzen 5 9600x )   overall it is pity good gpu that serving well even in today's game's ( while idk about battlefield ) but with last playtest and soon other one ( for me it run well in fps wise ) at arc raiders with 6700xt ( it is optimize game ) ( while tarkov 60+ fps at streets while other maps 100+ fps native ),Positive
Intel,The ARC card will probably be ur best bet but AMD and Nvidia have better features in general amd the 9060xt 8gb isnt that bad. It jsut kinda sucks that the lower end of hardware hasn't moved all that much in years .  As for a cooler mean for $20 or less u cant beat the Thermalright Burst Assassin its a single tower air cooler but its build really well and can handle a ton of heat  https://pcpartpicker.com/list/8zXRcx,Neutral
Intel,"Atleast the 12gb option, probably pick the 12gh nvidia over the arc",Neutral
Intel,"this would be my game plan - 9060XT 8gb, 5060 8gb, Arc b580, 3060 12gb  for a tower cooler I personally like the ID-Cooling SE214XT   I added a PCPartPicker list to give some ideas of better PSUs to get over what you listed  [https://pcpartpicker.com/list/MyWcwY](https://pcpartpicker.com/list/MyWcwY)",Positive
Intel,"Dont overpay for your usage. Compare benchmarks between b580,  3060 12 gb, 5060 8 gb and 9060 8 gb and buy what's best for your use (1080p competitive games). It will probably be the 5060.",Neutral
Intel,"If you can afford to step up to the 9060XT 16GB, it's much better. It's about 30-40% faster on average and has 4GB more VRAM. Not to mention better driver support and access to a better upscaler with FSR 4.   Why are you considering going down to the B580?",Positive
Intel,The b580 is a great tinkering machine or amazing if you're building for video editing first but for a PC gamer who has to ask I'd avoid it. It's fine if you literally only play new releases a month after release but for freshly released stuff or anything before like 2022 it can get rough. It's kind of like being in a really cool beta nobody's sure if Intel will commit to,Neutral
Intel,"9060 XT, not even a question imo.",Neutral
Intel,The B580 is great if your CPU can handle it. Judging by the pricing you gave in that other comment i'd say it could be worth it. That all depends on your CPU though.,Positive
Intel,"Don't listen to the other people here. As someone who owns 9070xt, 5060ti and B580, the Intel card is an amazing performer for the price and the drivers keep getting better.    I think with XeSS it is the best budget 1440p gaming card.   The only major issue is UE5 games not performing very well, but the drivers are improving.",Positive
Intel,"i will go with B580, save your money to buy something else, since you are playing FHD then yes B580 is a better value",Positive
Intel,"The biggest reason was the price, I saw an arc b580 for R$1800, compared to the 16g 9060xt which I was only finding for around R$2800",Neutral
Intel,$230 for that would be bad in the states but just depends what country you are in I guess.,Negative
Intel,i heard it was a pretty decent entry level card,Positive
Intel,"The B580 costs 330$, RTX 4060 costs 380$ and 6600 costs 270$.  (Ronded up/down currency conversions)",Neutral
Intel,"It can run beamng drive, My igpu run the game at 20fps on lowest settings in 720p, and my rx580 sounded like a jet engine while running the game on medium settings.",Neutral
Intel,What country are you in?,Neutral
Intel,well i hope it does you wonders friend,Positive
Intel,Poland,Neutral
Intel,I love Poland!  I flew to Krakow to visit my family in Ostrava earlier this year. 🫡,Positive
Intel,Now we need Nvidia to fix its CPU overhead problem.. which it has had for the last 10+ years... here's hoping!,Neutral
Intel,Only for some games though,Neutral
Intel,i'd go with the RX 9070 more VRAM better drivers better performance its more money but should last you longer,Positive
Intel,Bro that thing looks less like a PC and more like it’s about to open a portal to hell,Negative
Intel,9070!!,Positive
Intel,"If you can afford it, the 9070 is way faster than the B580. It's not even close. And I think pairing it with an 11700K is totally fine. That's still a decently fast CPU.",Positive
Intel,"That is legitimately one of the coolest PCs I've seen, such a shame it's nearing the end of it's life...  You BETTER build a worthy successor of it!!!",Positive
Intel,man i playing on 1440p and 4k on a 9070. get that and thank yourself.,Positive
Intel,"Glad to see people actually considering Intel GPUs. I hope they can catch up with Nvidia and AMD in terms of performance and compatibility.  That being said, I'd get the 9070.",Positive
Intel,The 9070 XT is 1000 times better... Intel is dead 5 years ago...,Positive
Intel,"So, put it in my current build first then build the other other one and swap over?",Neutral
Intel,Sick right? 😁 lmao,Neutral
Intel,"Thanks, that's what I was concerned about",Neutral
Intel,"Yes I know, I'm very easy to please just with Star wars stuff",Positive
Intel,Think my current system can handle it?,Neutral
Intel,Understood 🫡 should I consider the 7900 xtx or 5070? Or is the 9070 the best option out of the three?,Neutral
Intel,Rip 🙏,Neutral
Intel,yeah i know some people still rocking 11th gen intel you should be fine with the new GPU now and then a full upgrade later,Neutral
Intel,You and me both,Neutral
Intel,with the 1050? fuck no,Negative
Intel,"Depends on your regional prices and the specific models. If you ignore the prices I would prioritize them like this: 5070ti > 9070xt = 7900xtx > 5070 > 9070. If you need the additional VRAM a 7900xtx is an easy pick, though I think their prices are a bit too high.",Neutral
Intel,"Awesome good to know, thanks 👍",Positive
Intel,No I'm replacing it. The 1050. With the 9070. Can my 11700K and ddr4 ram work with the 9070?,Neutral
Intel,"That's a pretty helpful way to visualize it. I'll probably wait for a deal/sale, I have a Micro Center 20 min away and just grab whatevers cheapest for now 🙏 thanks.",Positive
Intel,"that should be good depending on the game. cpu intensive games can be tough, but if youre planning on 1440p and/or 4k the gpu gets taxed alot more anyway. the 9070 is a great card, ive owned one for about 4 months now and im very impressed with it. theres some OC headroom too.",Positive
Intel,"Sounds great! If you are gaming on 1080p+high settings or 1440p+mid/high settings, consider the 9060xt 16gb. Not as powerful as the basic 9070 but it's an excellent price/performance card, that might get you faster to the 5090.",Positive
Intel,"[Test with an i7-13700 + ARC A770 at 1080p max settings](https://youtu.be/qOhaZFrMbKA?t=592) from a year ago. 65-70fps average seems about right. Not sure why you would run Fortnite at max settings tho, set a few things to medium or high, so you can get around 120-160fps.  And ""CPU utilization"" means nothing, you have to check if the individual cores are maxed out, games don't usually run on all cores. But GPU at 99-100%, means the GPU is limiting performance.",Neutral
Intel,"Trash gpu, expected performance.",Negative
Intel,It’s a pc I built to sell. I was just getting some benchmarks from low to high and noticed that with settings maxed the fps tanked,Neutral
Intel,"Also, I just recently did a Ryzen f5 7600 with a 12 gig 3060 and so much better performance so I assumed the Intel would do better with more chores than Fortnite being a CPU bound game.",Positive
Intel,"Yes, I did recheck that though and above 4g decoding. Also xmp enabled at 6000mhz",Neutral
Intel,"You cannot compare core count, clock speeds, compute units, or anything like that for different architectures, only way to know for sure is by running benchmarks.  An example of that is the recently release Arc B580. If you looked at the spec sheet for the A770 and the B580, the A770 seems much, much more impressive, [but in games, the B580 deliver much better performance](https://youtu.be/aV_xL88vcAQ?t=676) (and even better with the recent driver updates).",Neutral
Intel,Peerless Assassin would cover your needs for $35.... Grey one looks nice,Positive
Intel,"You can add one more fan at the back and top for exhaust and go for an air cooler like the Thermalright Phantom Spirit or if you fancy, a one fan AIO which you can mount to the top as exhaust is cool too",Positive
Intel,spec requirements showing what fps we can expect at what resolution and settings at native make me so hard,Negative
Intel,This is how detailed all system specs should be tbh,Positive
Intel,"Didn't see many complaints about performance issues in BF6 beta, and people were singing praises for optimization, so this is not too bad imo",Positive
Intel,Played the beta at 1080 with a 2070. No issues,Positive
Intel,"Fellas, pray for my 1060 6GB 🙏",Positive
Intel,"This again confirms that the more fps you want, the more you need to have a big CPU in this game",Neutral
Intel,My mobile 2070 was running great in beta so I can't wait to play on Friday lol,Positive
Intel,"Holy shit, reasonable system specs",Positive
Intel,3090 isn't high end anymore? Man,Negative
Intel,My 7900XTX isn’t top of the line anymore 😩😩,Negative
Intel,"I got over 100 fps in 4K during the beta with an RX7600 just by going to low settings, but this game still looks genuinely good at low. Totally worth it for the frames and very well optimized.",Positive
Intel,Isn’t this kinda good?,Positive
Intel,"Ryzen 7 7800x3d, RTX 5070 ti and 32gm 6000mhz ram, i’m so ready!",Positive
Intel,This is how EVERY game developer should put out system spec requirements. This is fucking WHERE ITS AT BOYS!  9800x3d/5090 playing on my msi 32in 4k 165hz qd-oled at NATIVE 4k was getting me 130fps~,Negative
Intel,"Well im good to go.. sadly i wont get the game, i got fired cuz staff reduction, so i should not use money on games till i get a new job   ![gif](giphy|sT1K7rkPJOwh2)",Negative
Intel,I don’t think me and my i7-4790 will be enjoying this anytime soon lol,Negative
Intel,Why does a game care about TPM and Secure Boot?,Neutral
Intel,Cries in 6600xt,Neutral
Intel,I'm more concerned about that kernel-level slop they hook up the game with...,Negative
Intel,"i'm stupid, do i need windows 11 to run the game at 1080p 80fps+?",Negative
Intel,Been waiting 10 years for a good battlefield game. Super excited to play this. Ran the beta at 150+ fps on my current setup. Can't WAIT,Positive
Intel,TPM2 enabled on w10...,Neutral
Intel,"Ok, good detailed system specs, my question is why tpm enabled requirement. The last column there",Neutral
Intel,My 3080 getting me probably 60 fps on 4K ultra is fucking CRAZY,Negative
Intel,My 7700x was really busy in the beta. Locked my frames at 120 at 3440x1440p Ultra DLAA with my 4090. Never really got the feeling of lots of framedrops.,Neutral
Intel,"Based on the performance in the beta, it'll probably run better than the specs suggest. I played with hardware comparable to the minimum spec and still saw +60 fps most of the time. So they either got less optimized since then or they're using worst case performance scenarios for their hardware recommendations.",Neutral
Intel,I have 1080 TI lol,Neutral
Intel,"If accurate, this is exactly how triple A games should perform in the current generation.",Neutral
Intel,Has anyone tried this game on a HDD?,Neutral
Intel,"did anyone play the beta with a ""rufus"" like install of windows 11 that skipped the tpm/secure boot requirements?",Neutral
Intel,ultra++ it is...,Neutral
Intel,"For the sweaty boys out there, we all know we'll run this bitch in the lowest settings possible for ""enhanced visibility""",Negative
Intel,"Well upgrading to 9700x and 3090rtx is great Had rx580 during the beta, while it was playable but had to do fsr to get close to60fps",Positive
Intel,"I upgraded from a 2070 laptop to a 5090 laptop for this, let's go! Can't wait.",Positive
Intel,>UEFI SECURE BOOT ENABLED  oh come on....,Neutral
Intel,"Can you actually use MFG in a competitive online shooter?  I don't have a problem with MFG in casual games but aren't you going to run into, idk, latency or something fucky?",Negative
Intel,Might be the first game I opt to use 1080p on my 4K monitor. Can’t wait,Positive
Intel,Makes sense except for MFG being on the final performance tier. That's not really what mfg is good for,Negative
Intel,got me at TPM 2.0,Neutral
Intel,No linux compatibility added? :(,Negative
Intel,"I have the ultra specs and the game keeps randomly crashing, Otherwise it works flawlessly, but doesn't like to stay open. It crashes with no error  warning or anything and  im not sure what to do",Negative
Intel,Alright how is windows 10 on the minimum.  I couldn't install it on my system because it doesn't have that boot check option.,Negative
Intel,It also requires giving money to Jared Kushner.  Fuck that.,Negative
Intel,"Multiframe Gen? I hope they intend that for the campaign mode, and not for online competitive play.",Neutral
Intel,i personally think tpm and secure boot requirements is actually braindead,Negative
Intel,My poor CPU.  I need to just retire it to the wife's pc and pull the trigger on 9800X3D it seems.,Negative
Intel,"Native means it internal, already upscaled by frostbite engine.  fk the tpm and vbs stuff.",Negative
Intel,"I love it but frankly, I think they're overestimating just to be safe. My 12400/5700xt is lower than recommended even though I ran ultra with really great FPS in the beta. There's no way it drops that much",Positive
Intel,Looks like they “Recommended” 8GB VRAM,Neutral
Intel,This is pretty impressive considering the intentional lack of upscaler for the reported performance metrics; native 144fps at 2k res is amazing.,Positive
Intel,"I played the beta with an rx7900gre on 4k, using fsr I reached 80-90fps. Worked perfectly well.",Positive
Intel,Ahhhh when your hardware is listed as capable of 144hz 4k with dlss and no mfg. i wonder what native will be like with settings tweaks.,Neutral
Intel,Rip. Won’t be able to hit 1440p 144fps with my 4070ti super apparently,Negative
Intel,"I played the beta two different times with an 8gig 3060 (not the TI), 3700x and 32gig ram at 1080.    I couldn’t play on balanced on the recommended. I had to set it to low and tweak some stuff.    Would go from 60fps to 20’s.   I really hope the final version is much better.",Negative
Intel,On my knees praying I can somehow get 60fps from my 1070Ti,Positive
Intel,Anyone try a intel 9700k yet?,Neutral
Intel,my 1080 will run it fine... i hope.,Positive
Intel,"I’m new to all this, so I’ve got a question. During the beta I was able to play and it was pretty stable and I only have a 1660ti. Why is this so if the bare minimum specs are above what I have? Thanks in advance.",Neutral
Intel,Actually impressed with the detail here. Also makes me feel the need to upgrade lol.,Positive
Intel,I hope this game delivers + good support over the years! We as a battlefield community desperately need a new game! Cannot play BF4/3/1 anymore.,Positive
Intel,Can someone recommend some settings for medium +++,Neutral
Intel,my 5600x will cry :(,Negative
Intel,Beta played quite nice so I hope they dont fuck up the performance at release.,Positive
Intel,"Why do all of these system specs sheets assume that you're either playing at low resolution at low fps, or high resolution at high fps? I play with the settings cranked at 1080p so I get a high fps, and these charts are never applicable.",Negative
Intel,Does this mean I have to update to windows 11 to crank those settings up?,Neutral
Intel,"My specs are a tad all over the place, but im hoping for 90fps at 1440p, quality settings dont matter",Neutral
Intel,"I wanna do an upgrade for BF6, (from r5 7600 to r7 7800x3d) cuz it's mainly CPU-bound game(I'll make a GPU upgrade later). Is it worth making the switch?",Neutral
Intel,I can confirm the 4k @144hz requirements as being accurate in my limited example,Neutral
Intel,Optimised like crazy. I love it.  Other devs need to take notes,Positive
Intel,can confirm was getting solid 144hz ultrawide 1440p on a 5080 + 9800x3d on ultra during the beta.,Neutral
Intel,My 3080 and 3950x was struggling a bit during the beta. I’m hoping that switching on my DOCP ram setting in the bios helps out 🤞🏽,Neutral
Intel,Black Friday just can't come faster,Positive
Intel,The beta ran flawlessly on my 5060 ti and Ryzen 7 7700x. Im so excited!,Positive
Intel,"This is a great way to list specs but only if they could also show ray tracing on vs off because if it needs DLSS 4k with a 5080 without ray tracing to hit 100+ frames, it's not bad but it's kinda alarming because then with ray tracing you're looking at 60fps with DLSS",Neutral
Intel,Just built a new PC with a 5090 and 9800X3D and went from a 1440p monitor to 4k. Very excited to see what it looks and plays like. I was out of country for work during the beta and only got to experience it on med settings on a 1080p laptop.,Positive
Intel,"damn how do i enable tpm2.0 and uefi secure boot? i was trying to do cod beta the other day and couldnt due to those not being enabled...  got the 5090 and 4k monitor 145hz for this game, lets gooo",Negative
Intel,i hope the performance i got in BETA stays the same (140-175fps 1440 competitive settings) when the game is out,Positive
Intel,"Holy shit, recommended specs are for 1440p 60fps high? Thats a fuckin first.",Negative
Intel,Why is 5060 not there 👀,Neutral
Intel,"Damn, not even a year old PC and couldn't even make it in the Ultra ++ tier.",Negative
Intel,"That’s awesome, very surprised my gaming laptop will be able to handle 1440p on high settings",Positive
Intel,"Isn’t a 4080/7900xtx + 12900k/7800x3d a little concerning for 1440p/144hz at MEDIUM settings in a competitive game?   I mean, someone like me with a 4070super + 7700 is not getting to 100 fps at MEDIUM settings in 1440p 😳",Negative
Intel,i thought the gtx 1060 was minimum spec?,Neutral
Intel,Gee look at those pretty good requirements when it's not UE5,Positive
Intel,I was worried my 9950x3d/5090 build would struggle,Negative
Intel,3440X1440 with my rtx 3060ti  would love to upgrade 😢,Positive
Intel,No complaints there,Positive
Intel,"🤞come on ROG Ally X, you can do it.",Positive
Intel,Not that I was gonna play BF6 but perhaps I'm due for an upgrade lol,Neutral
Intel,"That's... actually acceptable, damn, an AAA game that dont require a 4070 just to open?",Positive
Intel,I was getting about 80-110fps at 1440p medium with DLAA on my RTX 3080 in the beta. Final spec requirements seem to match up.,Neutral
Intel,"Old guy here who couldn’t play beta.  Can my 9900k, 1080ti, 16gb ram be up for the task at low/ settings?  1440p monitor but I’ll downscale to 1080p.  Can I get decent frames?    Not in a position to upgrade.  Thinking of getting PS5 pro because it supports Mouse and Keyboard.  🤷",Negative
Intel,"Game ran like butter on my base 9070, really well optimized...  Or maybe our standards just dropped.  But they definitely cooked well with this game.",Positive
Intel,"my 2070 was getting 70-90fps on low settings in the beta, ill be upgrading to a 5070 in a week tho!",Neutral
Intel,They have the 7800X3D under the Core 9 Ultra 285k? What?,Neutral
Intel,I mean its not even hiding anymore that developers don't give a damn about optimization. Back in the days of Battlefield 4 and NFS Rivals they ran on max settings on a potato and still look better than many games today that even NASA pc will struggle to run,Negative
Intel,What could i realistically expect with a 2070 / I5 9600K on a 1440p monitor? Probably fucked for 60fps right?,Negative
Intel,I'm glad i upgraded my pc few weeks ago. I'm pumped for friday.,Positive
Intel,Does it already let you download?,Neutral
Intel,"144p 165hz monitors with an RTX4080 Super, and the Beta was butter smooth at max settings. Ran better then expected, so if the full game is even better optimized, that will be fantastic.",Positive
Intel,3700x with a 1070 was running the game fine in beta  im scared of larger maps. the 3700x was maaaaxxxxed out,Negative
Intel,Pray for my 1060,Neutral
Intel,Looking to get a balance between recommended and ultra with my 7800X3D and 3070.,Neutral
Intel,Nice should be able to hit 140fps with my 9800x3d and 9070xt natively full ultra in 1440p,Positive
Intel,SecureBoot and my GigaByte motherboard *did not get along* when I tried to play during the open beta. Does anyone know if GigaByte boards have had a fix issued?  I am using a Gigabyte X870 Aorus Elite Wifi 7,Negative
Intel,I would be nice if the game has a benchmark tool,Positive
Intel,So somewhere between High and Ultra.,Neutral
Intel,"These actually seem pessimistic.  Intel 12700, 32 GB DDR4, 7900 XT  I was able to run the beta at 1440p high settings around 100 - 144 FPS with a 10% power limit set on my GPU",Neutral
Intel,I'm cumming,Positive
Intel,"I'm not gonna play it, but this is how a spec sheet SHOULD handle upscalers.     If I was a AAA gamedev, I'd also want to put a 2x5090 column on there with highest settings, 4K or possibly 8K, DLSS kicked all the way up, lossless scaling on, and just see what numbers I can hit.",Neutral
Intel,"Damn, not only did they say what settings and FPS to expect, but also managed to not make the game extraordinarily hard to run.  Seeing how the recommended and minimum are GPUs two generations old, and some of which are very popular in the Steam Hardware Survey, it's a surprising breath of fresh air. Now, let's hope the game is actually well optimized and that the 1%lows won't be horrendous.",Negative
Intel,"Nice to see I meet the bare minimum for ultra, on 1440p 144FPS, on Medium settings, wtf happened",Positive
Intel,"Damn, I'm barely in the recommended column.",Negative
Intel,So a Core Ultra i7 and 9700XT would land me somewhere on Ultra+?,Neutral
Intel,7900xt + 5800x3d on 1440p maxed and it was around the 100s fps during beta  this seems a bit exaggerated,Neutral
Intel,Beta was so good i went from a 3070ti + 5600x to a 5070ti + 9800x3d. Im so ready.,Positive
Intel,what is hardware accelerated gpu scheduling?,Neutral
Intel,"I ran the beta with a 3080ti at 4k, DLSS balanced, medium-high settings and hit a solid 180FPS.",Positive
Intel,Yeah of course I need a 7800x3d for 4k 60fps. And 9800x3d and 285k are in same league. They are trying to show their old engine like something new. A modern game that needs latest hardware to run well. The game is not looking that good and sure not requiring that much spec.,Negative
Intel,Was going to get on pc but I have a 4070 which will put me in the 1440/144 medium settings group.,Neutral
Intel,This is a super reasonable spec sheet,Positive
Intel,I ran the beta at a mix of medium/high with a 2080 super. I was gonna build a new machine this year but I think I'll wait a little longer.,Neutral
Intel,"I know why they need HVCS but I’m sure they can find a way to open the door for mainline kernels that are built on bare metal.  One day gamers, one day.  Until then the dual boot remains necessary.",Neutral
Intel,Me hyped with a 7900xtx and a 7800x3d in 3440x1440p,Positive
Intel,Can't wait to test this in real world settings,Positive
Intel,Wow I dont often see 32GB as a recommendation. Happy to know my GPU and I will still be going strong for a while,Positive
Intel,I cant even put my poor 1660 ti thru this.   Ill get it on console while I save up for a new pc build.,Negative
Intel,I was super happy how well the beta ran so I feel pretty good about these specs. I averaged around 80-100 fps on high/ultra at 1440p with DLSS quality with a 3080 and a 10700k @ 5.2ghz.,Positive
Intel,EA has done literally everything right with this launch. This is exactly how I want to see spec sheets,Positive
Intel,So torn to get it for PC or PS5…I like the idea of using my portal on the go but also playing max settings on pc,Neutral
Intel,can someone explain why people prefer native? I used DLSS for Rivals and it seemed fine latency wise? lol new to pc gaming so wondering if im just missing something,Neutral
Intel,Ran lovely at 4k on my 5080,Positive
Intel,where would i9-10850k and 3090 founders land at 1440p,Neutral
Intel,I was getting 90-100 fps at very high/ultra settings on 7900xtx and i7 11700k at 4K native in the beta,Neutral
Intel,"I have a 4070S with a 12400 and 16GB 3200MHz, my motherboard is a really basic model.   Should I aim to change to a DDR5 setup? Maybe switch mobo+processor+dram kit?",Neutral
Intel,Definitely wrong subreddit but how does BF6 on a PS5 compare?,Negative
Intel,Wondering how my 9070xt + 9800x3d hold up at 1440p ultra,Neutral
Intel,"Well, let’s see how the good old 5800x3D handles this with a 5090 on 5120x1440",Neutral
Intel,A godsend after Borderlands 4.,Positive
Intel,Is a 285k really needed over a 12900k?,Neutral
Intel,Will a 5070 ti 16gb-vram work with ultra++? All the other specs i have but not a 5080,Neutral
Intel,Played the beta at 1440p with a 2070s no issues,Neutral
Intel,Hmmm.. I wonder why they didn't show Native for Ultra++. I have 9800x3D and 4070 TI Super. I think I could probably reach 1440p 144FPS with med using DLSS.,Neutral
Intel,Love this. But how come they only mention DLSS for the ultra++? I used DLSS on my 4080 super and it gave me ~120fps in high settings,Positive
Intel,Which means a Ryzen 7 9700 and RX 9060 should do just fine at 1080p75 high/Ultra with NO raytracing.,Positive
Intel,Does anyone know if they are the same as in the beta?,Neutral
Intel,Is the fps cap still 200fps?,Neutral
Intel,"Here's to hoping my 12900k and 3080ti will reach 100fps in 2k....""praying to the gods""",Positive
Intel,"I ran the beta on a 1650, 30 fps on low (with semi-frequent drops) but I was amazed it launched at all.",Negative
Intel,So 5080 with a 5800x3d will struggle at ultra for 4k?,Negative
Intel,Just got my new 9800X3D and 9070 XT PC a few days ago :). Yippee,Positive
Intel,I had around 150fps with a 3080 and 5700x3D @1440p low settings. Had to write a cfg that all my cores were used.   I hope the final game runs similarly. I really don’t want to fully upgrade my pc now.,Negative
Intel,I mean the beta showed that even iGPUs in mobile chips are good enough for low settings,Positive
Intel,What what makes it ++  I’m sure my 5070 ti with a little Oc will do the trick,Neutral
Intel,"My Ryzen 5 3600 and my rtx 2060 6gb ran the game better than this on the beta, they might be underselling it",Positive
Intel,"Look Capcom, this is how you optimise you're games",Positive
Intel,Anyone had  3060 ti in beta ? If so how did it go I do have 1440p monitor,Neutral
Intel,Seeing my baby boy on that list makes me happy,Positive
Intel,"I got a 6700 xt r7 5700x, so by this chart, I should only be getting 60 fps at 1440p on high settings?",Neutral
Intel,Randy Pitchford quaking in his boots,Neutral
Intel,Most likely a reach but would a stable 60 with drops to 50 when a lot is going on feasible?  3050 6GB i5-14400f 32gb of Ram,Neutral
Intel,"Be chill brothers, I had played the bf6 beta in my Ryzen 2700x/ GTX 1660 Super and hit 55-60 ... Even 70fps while I was streaming Discord and clipping in Medal. Also I don't remember the settings but I didn't look it too much, I'm pretty sure I was playing in 1080",Neutral
Intel,Here I am with a 1080 and 7700k trying to play this at 1440p and 100+ fps 🤣,Neutral
Intel,"Honestly they might’ve underspecced this list a bit, I was getting 80+ FPS@1080p with a 2070 Super lol",Neutral
Intel,Ultra++ gang where u at,Neutral
Intel,"The Beta ran fine on my undervolted 7900xt and Ryzen 7500F @ 4K native with mixed settings. The game still looks great with everything low except Textures and mesh on ultra/max whatever.   The only way I will SIGNIFICANTLY increase performance is by buying a better CPU.  Game is well optimized, which is rare these days.",Positive
Intel,16gb vram? Damn....,Negative
Intel,Love to see them using native res for everything below 4k.,Positive
Intel,"This is very reasonable tbh, I kind of forgot what that feels like.",Positive
Intel,I played the beta on Ultra 1080p on a RTX 5070 with Gsync on was rly great,Positive
Intel,"Played beta at 1440p with core ultra 265k and rtx 3080ti , at high settings, never dipped below 100fps",Positive
Intel,"I have a 7900XTX, 64gb DDR 6000 CL32, 9800X3D, so very much their Ultra Performance Specs - I played on Medium/low - I did not get near 144 FPS, more like 100-115.  I hope this means we get a good performance boost in the real game....",Positive
Intel,"Gotta see release version benchmarks, i have tough times believing this graph. I barely got 50-60 frames on 7 5700x and 3070 in the open beta on lowest settings, doubt they polished it that good to run on high",Negative
Intel,Cant be right... 4k 60fps native on a 4080? I mean sure but ill believe it when i see it,Neutral
Intel,Ran the beta fine with a 1070,Neutral
Intel,Well my 5080 will do good.,Positive
Intel,TPM enabled is required to play a game?,Neutral
Intel,it's tough seeing my 2080 near the bottom of the board but I played the beta and it ran well enough.  she still has plenty of games left in her future,Positive
Intel,"5700xt, 3600, 16GB ram. Was getting a very stable 80-100 FPS. These minimum specs are being extremely modest. The game is super optimised.",Positive
Intel,why do I need windows 11 at all ?,Negative
Intel,Cries in 6600xt,Neutral
Intel,i have i7 4770 😭,Neutral
Intel,Damn a 5070 ti can’t max settings  this game on 1440p?,Negative
Intel,I have a 4080 super... with a i7 9700k... im fucked,Negative
Intel,A modest requirements but me with a GTX 1050ti laptop cant do it,Negative
Intel,Ok so I played the beta but it was on PS5. I plan on buying the game on PC but I hear you have to change settings in your PC regarding the secure boot to play it. Does someone have a simple YouTube video that can explain it or do you even have to do it anymore?,Neutral
Intel,i think this is reasonable,Neutral
Intel,We specifying minimum ram speeds now?,Neutral
Intel,well i hope my 5950x can run this well lol,Positive
Intel,I love how specific the descriptions are!,Positive
Intel,EA is taking the right moves again,Positive
Intel,Based off what I played the 9070 fits in Ultra,Neutral
Intel,"As a 7800XT owner, I feel ignored",Negative
Intel,I got a buddy that played the beta on a 1660ti and played on mostly high setting with some on low and still got 60fps on average with dips to 51fps in heavy combat.,Neutral
Intel,I have 6750xt with i513400f can I play with decent fps at 1440p med settings? I didnt get the chance to play the beta.,Neutral
Intel,I think they give the  absolute worst case scenario I mean if they said you can get 1080p 120fps on high settings on your build and you get 90fps you will be disappointed but if they said you get 1080p 60fps on medium and then you get 90fps on high you will be impressed,Negative
Intel,5080 overclocked. Open beta ran 190fps all maxed DLSS performance 3440x1440 so i assume these specs are NOT meant for native,Negative
Intel,What bullshit comparison is this? You can't advertise something as 4k resolution then say underneath you're using upscaling and frame generation when you aren't using those things on the other settings.  A 5090 will run this above 120 frames native anyway so why not just say ultra++ is 5090?,Negative
Intel,Tpm.. Guess I'm out,Negative
Intel,Game is very well optimised and i played on RTX 2060 Super paired with 5700X3D. Don’t think it would be a problem for people.  Upgraded to RX 9060 XT just to enjoy the game at its full glory!,Positive
Intel,With rtx 3060ti nad i7 10700 worked great in beta. Really smooth.   1440p balanced/high from 70+ fps.,Positive
Intel,tf is ultra and ultra++,Neutral
Intel,Saw few videos that its playable well on gtx1080?  I have i7-6700k + gtx1080 oc. What are my chances ? Will I be able to play or should I upgrade my potato to atleast rtx3070 or a bit up. Also Mb is assus z170 pro gaming. Meaning cpu is maxed out on this mb and I cant put any other.,Neutral
Intel,I was surprised by how well optimized the game was running in beta,Positive
Intel,Geez i need a new computer,Negative
Intel,It even lists RAM speeds?,Negative
Intel,Does having Win10 instead of Win11 affect performance enough that it is noticiable?,Negative
Intel,I bet that i can somehow run this baby on a 1650,Neutral
Intel,Played the Beta with a R7 5800X3D and RX6800 at 1440p Native with constant 120FPS,Neutral
Intel,I'm more worried about what settings to turn off in order to experience 0 motion sickness.,Negative
Intel,Looks like settings have shifted on 1440p at 144 hz from high to medium.,Neutral
Intel,2080 super still rocking until 5080 super.,Positive
Intel,The beta was so well optimized...,Positive
Intel,"Lol, RIP my poor 4060 trying to drive a 3000x2000 panel...",Negative
Intel,I got recommended.  Though my specs for recommended is a  bit better.  So Im predicting a more consistent 60fps at 1440p settings at high.  It's a 3080 5800x,Positive
Intel,played the beta at 140fps with a 7800x3d and 7900xtx  2k ultrawide on a mix of high or ultra (windows 10),Neutral
Intel,wow real actual specs and not just throwing 4th gen intel and 1050 ti in ? shocker,Positive
Intel,aw no 1080 ti,Negative
Intel,Anyone else ready for Ultra? Yeah boii,Positive
Intel,So can I expect 1920 @75fps with a 2070 super on low settings? Lol,Neutral
Intel,"Where is the 5090 spec! I have the hardest time getting settings for this thing.  I have a i9-10900 with a 5090, also my ram is 3200  I had no problems running the beta with everything cranked but it was not buttery smooth   Does anyone know what bottlenecks I should see, and how I could resolve them?",Negative
Intel,Wow I just went from a 3070ti to a 5080. Looks like I made the right choice.,Positive
Intel,will a rtx 4090 run this? /s,Neutral
Intel,Ultra it is!,Neutral
Intel,Somehow I feel bamboozled by them not including 9070 (XT) cards in the recommended specs.,Negative
Intel,Still good with a PC I built five years ago...,Positive
Intel,Im fucked,Negative
Intel,So I played the beta on a 1070 and was getting 60 fps. Not bad for a decade old card.,Positive
Intel,"i5 11400f and rtx3060ti, beta ran at 70-80 fps no issue. The CPU is a bit of a bottleneck, but, i can play steady 1440p 60fps with no issues",Positive
Intel,"They have specified UPSCALER settings - I’m not even mad, this is amazing.",Positive
Intel,Ultra++ here I come :D,Positive
Intel,"im fine all of this, but the CPU - i suspect my first gen i7 wont quite cut it",Negative
Intel,I have like inbetween minimum and recommended and I was running 60+ frames 1080p on medium to high graphics in beta.  Edit: mispelled minimum,Neutral
Intel,"the beta ran at 70-90 fps on my system with ryzen 7 5700x, 3060ti 8gb and 32gb ram at 1440p",Neutral
Intel,So..with i7-13700k n rtx-5070ti i should be able to sít at 90fps and above on ultra++ in 1440p? Without frame gen?? If thats true they cooked hard with optimization,Neutral
Intel,So my 2050 can’t run the game? But I played the beta with 60 fps? Uhuh,Negative
Intel,"Ah, my old nemesis, Secure Boot",Negative
Intel,I played the beta with my current specs and had zero issues running it at 90+fps at 1440p with high settings.,Positive
Intel,Battlefield 6audi,Neutral
Intel,"boa noite, ganhei um pc ryzen 5 5600gt com 4060, to na duvida se vai rodar bf6 vem, compro no ps5 ou pc?",Neutral
Intel,another unoptimized bullshit lol,Negative
Intel,Honestly I'm fine with Recommended specs so I'm good :),Positive
Intel,"Damn, Battlefield 6 specs are no joke! 😅",Positive
Intel,"9800x3d, 3080ti, 64gb 6000hz, 2tb m2 drive let's goooo     I'm hoping for 100+fps on medium with 1440p ultra wide.",Positive
Intel,How many fps for ryzen 5 7500f?,Neutral
Intel,"Fuck TPM 2   and bs invasive Rootkit KERNEL  Level Antichet (when server side AC solution already exist)   Ohh btw Battlefield 3 was so well optimized and still looks good and it run on  **2-CORE CPU!!!**  and 2GB VRAM GPU , 4GB RAM!",Negative
Intel,"Ryzen 5 7600x, RX 6600 XT, and 32GB RAM at 6000MHz, what FPS should I expect? 1080p",Neutral
Intel,Yeah holy shit i now know exactly what benchmark to set (balanced recommended is exactly my rig just about)  Very happy reading this lol,Positive
Intel,These boys are cooking man I love it.,Positive
Intel,"with ""Balanced"" and ""Performance"" Flavors.",Neutral
Intel,To people that played beta is there dlss? At 1440p dlss works pretty well and I'd love to squeeze some extra frames on my 240hz display,Positive
Intel,Never expected to hear that,Negative
Intel,*how honest,Neutral
Intel,Are these specs wildly inaccurate? A 12900k or 7800x3d?? How are they in any realm comparable CPUs? Q,Negative
Intel,It doesn't list what operating system you need though,Neutral
Intel,"Probably the best optimization I've seen for a game in a long time, I think I only heard about 2 crashes from everyone I knew playing it, and that's on a beta test",Positive
Intel,"Have a 1060 and i7 7th gen and it ran at 40-50 fps in the beta at 1080 low, its very well optimised",Positive
Intel,"I was geniunely surprised at how well it ran on my 5+ year old rig. Had it's issues but still, pleasantly surprised.",Positive
Intel,"i have a 4070super ti, 11900k, 128gb ram but im running it at 5760x1400p and was honestly surprised at how well it ran and looked for a beta.",Positive
Intel,"The only thing I found was that the game is very CPU heavy compared to most games of this type. Whereas I see 20-30% CPU usage in cod, I see 40-60% in this game.  People could be caught out if they have done the old upgrade path of ignoring their CPU for many years and just upgrading their GPU.",Neutral
Intel,My performance got totally fucked from first to second beta on 4090   No idea why    But wont be buying it now;(,Negative
Intel,I hope its as optimized as BF1.,Positive
Intel,What fps? Im on 2080s,Neutral
Intel,"""No issues"" can mean lots of things and what people consider an enjoyable performance level differs.",Neutral
Intel,"Same here   5700x3d+rx6600. Native 1080p high/medium. Stable 60fps, no stuttering",Neutral
Intel,"I have the exact minimum requirement GPU and CPU from this chart (RTX 2060 and Ryzen 5 2600), which is saying to expect \~30FPS at 1080p, which doesn't sound too good.  I'll get the game if I can expect around 60FPS but not if it's significantly lower.",Negative
Intel,"maybe you had better cpu, i played with 2070 and i5 7600 and had 30 fps avg",Neutral
Intel,Did see reports of people running the beta fine on a 1060. Ran with no issue on my 1070. Hopefully that stays true for final release.,Positive
Intel,Im just praying mine doesnt break before i upgrade🤞,Positive
Intel,my 1660 Super and i7-7700 ran it fine on low settings,Positive
Intel,No,Neutral
Intel,"It ran ok on my 1050ti, you will be fine",Positive
Intel,2042 pounded my 5600 into next year so I can't imagine BF6 will be much better. Unfortunately I missed the cheap AliExpress 5700X3D window so I'm just going to have to suck it up and deal with it until I have a good enough reason to switch to AM5 or something,Negative
Intel,BF has always been a CPU hungry game but thankfully a very well optimized one at that. It seems like once you're past a certain threshold that extra CPU performance doesn't make a huge difference. My 5600X3D and my 6700XT combo ran the game swimmingly at 1440p and I was GPU limited.,Positive
Intel,"How much fps did you get at what settings? I have not played the beta and I have a 2070 max q, I want to expect how much fps I might get.",Neutral
Intel,"3090 is ALMOST identical to the 3080 Ti (10,240 cores vs 10,496 cores, 320 TMUs vs 328 TMUs). The VRAM amount is the biggest difference, rest makes the GPUs 1% difference in performance.   I would still consider the 3080 Ti high end though, despite how modern AAA games may develop.",Positive
Intel,2 generations + 6 months away? Yea not really,Neutral
Intel,"It's been 5 years my dude. You're into ""respectable"" territory.",Neutral
Intel,This is the reason I sold my Strix 3090 and bought a 9070 XT.,Neutral
Intel,"It will be more than fine, I have 7900XT and ran super high FPS on max settings 1440p.",Positive
Intel,"I’m sitting with a 6900XT and I was looking for a target of 4K @120 but I think it’s going to take a fair few compromises to reach it, I don’t know if I should jump on the upgrade wagon or not. Considering I play Diablo 4 @ 4K as my main go to daily game I think I should just chill and eat the performance loss for the sake of just Battlefield 6, which I will only play once for the single player experience.   I don’t think your 7900XTX is that far off from getting the peak experience out of B6.",Neutral
Intel,Crazy how such a non-controversial comment baited the Nvidia fanboys.,Negative
Intel,"Same, feelsbadman.gif",Negative
Intel,It’s “Architectured to exceed 3.0Ghz” and comes with “AI Matrix Cores”,Neutral
Intel,"Feel that shit homie, i just bought a 7900xtx like 4 months ago and its already not the best card i couldve invested in",Negative
Intel,still ran mostly ultra 2k ultrawide just fine at 140 fps,Positive
Intel,Never was…,Neutral
Intel,Mine is. EVC2 current ratio mod and custom vbios. My 7900xtx has better raw performance than a 5090.   Idk about dlss/fsr because that shit is ass. I play at 4k only.,Positive
Intel,Where you up scaling a lot? Because that sounds like cap. I was getting 120-130ish fps on a 4070 at 1440p native all low settings.,Neutral
Intel,You're lying. We can see the benchmarks online you know? 😂   I have a 9060xt 16gb and it can barely maintain 120-130 fps at lowest settings in alot of situations. And that's at 1440p.,Negative
Intel,It is. That was a big talking point during the recent beta. Many people were happy to see a AAA game with strong optimization that ran well on even low end hardware.,Positive
Intel,"Yup, the game look great too unlike Borderlands…",Positive
Intel,"Depends on what you want to compare it to. People here always compare the optimisation here to other AAA games, that aren't multiplayer fps games.  It performs twice or more worse than BFV, whilst not looking close to twice as better graphically.",Negative
Intel,Same setup. Gonna play it on ultrawide if it sucks ill revert to 16:9 :),Neutral
Intel,Similar set up and can’t wait to try and get 120fps at 4k on my new mini led monitor with a 9070xt wooo,Positive
Intel,"Should probably upgrade that millihertz ram, a whole 6 Hz",Neutral
Intel,Same set up!! Except cl36 ram 6000mhz and a 34” ultrawide 1440x3440 monitor. This is my first pc so dumb question but will we be able to run ultra ++? Whats with the frame gen is it better than native?,Neutral
Intel,get ea play pro for 20$ month and then realize its not as good as promised and move on with life ;),Negative
Intel,Let that OG rest bro,Neutral
Intel,You’re probably a magician if you manage to run that cpu on modern games without everything lagging,Positive
Intel,GOOD NEWS!  https://youtu.be/BMjZ3SgviC8?si=5hKi7MCHd8J5F1HN,Positive
Intel,what gpu u runnin?,Neutral
Intel,"You can run minimum settings at least. Its comparable to the Ryzen 2600 in gaming, slightly faster if you increase your memory speed since DDR3 1600 is the main bottleneck on that CPU. Luckily the memory controller is crazy good and I could do DDR3 2400 on mixed no-name sticks.  TPM support is by far the bigger issue. Probably not supported on a board that old without adding an external TPM.",Positive
Intel,Anticheat,Neutral
Intel,from the looks of it you should have pretty good enough framerates in 1080p,Positive
Intel,"Cries in 5700xt and r5 5600x   I knew from the beta that i was barely getting stable 60 at 1080p low settings, definitely gonna have to wait to upgrade before I get the game",Negative
Intel,6600xt is fine here. Mine can push around 100 fps at 1440p with quality upscaling. Around 60 fps without but VRAM is the big problem. After a few matches you run into heavy performance drops at 1440p unless you run low settings. 1080p you're more than fine.,Neutral
Intel,"Don't worry lad, the 1080Ti for instance does 70-75fps at 1440p High FSR Quality. 6600XT is quite a bit slower, but should still be able to do 1080p High/Ultra FSR Quality at 70-75\~ fps too!",Positive
Intel,"These GPU requirements don't make much sense, the 2060 isn't a 1080p 30 FPS GPU it's more like 70-80 FPS at native, the CPU in minimum is way too weak for a 2060 in this game as it can only achieve 55FPS, 6600XT should easily do 1080p 90 FPS but the question is whether your CPU can do that.",Negative
Intel,Every modern game uses kernel level anti cheat software nowadays. 99.9% of people dont have issues. You will be fine.,Positive
Intel,"No you dont, works fine on w10",Neutral
Intel,Wondering the same,Neutral
Intel,Anti Cheat.   And good on them to display alone with the rest of the specs.,Positive
Intel,Nice,Positive
Intel,Larger and more cluttered maps maybe in the final game?,Neutral
Intel,"No problem, 1080Ti can handle BF6 1440P High, FSR Quality at 70-75 sometimes over 80fps depending on the action. Mild OC wouldn't hurt too!",Positive
Intel,It's fine on HDD.,Positive
Intel,"I tested it during the beta, and at least for me the input lag becomes negligible if you make sure to cap your fps",Neutral
Intel,"You can use MFG in any game, it looks and feels bad below 60fps input but above that it's fine",Negative
Intel,"Frame Generation should - paradoxically - only be used when your framerate is already good enough. To jump from 100 to 144 for example, and cap a 144hz monitor is a good idea.  FG added latency is negligible. The problem is that it doesn't offer the __lower__ latency that naturally comes from higher framerate. If your base / real framerate is 60, you will get the latency of 60 fps even if you stretch it with FG to 120.",Neutral
Intel,"When you do this, how do you go about scaling? I've tried using 1080p on my 1440p monitor and it just looks bad. Would really help me out if I could though my GPU is lower end(6600xt).",Negative
Intel,Why not just do 4k and DLSS?,Neutral
Intel,[https://nerdschalk.com/battlefield-6-keeps-crashing-on-pc-issues-fixes/](https://nerdschalk.com/battlefield-6-keeps-crashing-on-pc-issues-fixes/),Negative
Intel,"Be more specific please. I was playing on 5950X with a 7900 XTX, and was not having problems on my playthrough. In fact only one which I believe is Windows and multi monitor related with AMD, experiencing slight lag and stutter untill I hit the windows button.",Neutral
Intel,You can enable secure boot on Windows 10.,Neutral
Intel,I cranked my settings to medium/high and get 110-140 fps on a 3080 10GB. (1440p DLSS Quality). I know some people hate upscaling but personally I can't tell the difference in image quality.,Neutral
Intel,"Yes you can, requirements are super conservative.",Neutral
Intel,The minimum specs aren't even close. You can almost double the FPS here. I assume this is in the absolute worst case scenarios.,Negative
Intel,"Dont know if you tried the Beta, but i got 60-80 fps on low settings at 1080p with a 1070ti and i7 8700k",Neutral
Intel,They got like tutorial on ea website. But you should lookup your motherboard and check where to find it,Neutral
Intel,"My 9070XT with a 7800x3d was getting nearly the same performance as what the 5080 is listed at,so they will be getting more frames than that",Neutral
Intel,"Mouse and keyboard feels like shit on any Sony console. Feels like they have some sort of acceleration baked in.  On PS5 pro during beta it got around 80 fps in performance mode.   You could probably get more with DLSS, if you do a cpu upgrade.",Negative
Intel,It’ll be cheaper to get the PS5.,Neutral
Intel,I have pretty similar setup and got 70ish fps on high@1080p. pretty playable and looks good too! 1080ti rocks,Positive
Intel,"Our standards have dropped abit. Look at the discourse around optimisation on this game, the comparison is always to broken AAA single player games.  Lets just compare to BFV. BF6 in beta had 2-2.5x worse performance, does it look that much better graphically? It looks better, but not THAT much better.",Negative
Intel,Yeah,Positive
Intel,nope and wont be. that the fun of a consumer side poorly done security that already been crack (tpm),Negative
Intel,I didn’t check my FPS but it ran smoothly for me with an 11th gen i9 and 3060. Beta was so much more promising than the release of 2042.,Positive
Intel,"Afaik DLSS doesn't effect latency when frame gen is off, it only effects the way the game looks.   As to why they prefer native, well that's been the standard for a long time and this being changed over the years has been disliked by some people. While I have no experience with DLSS, certain people easily spot blur, visual artifacts and ghosting when using upscalers which distract them.",Negative
Intel,Personally I prefer DLAA which is native resolution but implements the anti-aliasing techniques from the DLSS suite. I also force the K preset transformer model on every game.    The upscaling doesn't really affect latency really at all. Most of the talk about affecting latency is for frame generation not upscaling.,Positive
Intel,"3090 is similar to a 5070 isn't it?  9800x3d and 5070 averages 150 fps or so at native and low settings. Around 120 fps at ultra. Based on a benchmark video I saw.  This game is really cpu heavy, so you probably won't get anywhere near that with what you have.",Neutral
Intel,On the PS5 pro it gets around 80 fps on performance mode. So it must be worse on standard PS5 I assume.,Negative
Intel,Mine help up very well. I dint know ow the fps but it was butter smooth maxed out. You can even play 4k,Positive
Intel,"Easy as bro, I'm on the 4070ti super with an am4 5900x in the beta I was cranking out 180+ at 2k on ultra, with dlaa I could only imagine they are playing safe so people won't complain.  I've moved to 4k and I'll be happy for Dlss pushing me twords 144fps, or enable frame gen for the whole amount of 160+.",Positive
Intel,"My son ran a 2070 super and a 5600x and got those frames, you'll be fine.",Positive
Intel,I did but at 1080. Ran med & low settings here and there. Played fine. Just kept my gpu very toasty.,Positive
Intel,At what res?? That should outperform my i5,Neutral
Intel,The list just shows the most common or officially supported GPUs. Doesn’t mean others won’t work. Devs can’t list every card out there.,Neutral
Intel,"Beta specs tend to be rough estimates. The game isn’t fully optimized yet, so requirements can be higher or misleading. Once the devs finalize the game and do wider testing, the official specs give a much clearer picture of what you actually need.",Neutral
Intel,High-end graphical settings.,Positive
Intel,"RTX 2060 is the target GPU performance for a smooth, modern gaming experience as intended by the developers. It's still playable with other GPUs, better, or equal to an RTX 2060.   Something below like a GTX 1050 Ti or GTX 970, you'll need to lower settings and accept lower frames.",Positive
Intel,"I could be wrong on this, can't quite find the original reference from memory.",Neutral
Intel,Sim — o seu PC vai rodar Battlefield 6 muito bem.,Neutral
Intel,"yes, no guesswork. Is it native? Is it quality? Or performance upscaling? Is it targeting 30 or 60 fps?  When they know the game is optimised and it will perform well they publish system requirements like these ones. When they know the game will run like shit they make it as confusing as possible.",Neutral
Intel,"Well except increasing the resolution typically only increases GPU load, not CPU load. So why are the CPU requirements different between different resolutions?",Neutral
Intel,Proc: core i5 or ryzen5  Ram: 💅  Storage: yes  GPU: Nvidia RTX or Amd RX,Positive
Intel,Yeah pretty close I'm on 1440p with a 10900k and a 3080ti so recommended balanced looks like it will be perfect I will probably get closer to 80-90fps if what they say holds true.,Positive
Intel,Same settings as me.,Neutral
Intel,"The thing is, they are actually underselling performance a ton.",Negative
Intel,"You're right, a 12900K is clearly *significantly* superior:  [https://www.cpubenchmark.net/compare/4597vs5299/Intel-i9-12900K-vs-AMD-Ryzen-7-7800X3D](https://www.cpubenchmark.net/compare/4597vs5299/Intel-i9-12900K-vs-AMD-Ryzen-7-7800X3D)  The lesson being, different games have different architectures and run differently on different CPUs. For BF6 the 3D cache clearly helps the 7800X3D make up for its weaker single-thread performance, but not so much that it's significantly better than a 12900K.  Dependence on raw single-tread performance with less cache utilization is very common in multiplayer shooters btw.",Positive
Intel,They test with what they have and give approximations,Neutral
Intel,This can happen. Architectural difference can massively boost performance in a vendor. We have that in CoD in GPU side. Some games can/will loves some CPU/GPU vendors over others.,Positive
Intel,As someone with a 12900k i was wondering the same thing lol,Neutral
Intel,"I had something like 3 crashes back to back in the span of 10 minutes,but other than that the game was extremely fluent at 60+ FPS even with a lot going on,and i was on Performance mode with many things set to high so safe to say that this game is very well optimsed   (I have a 3080 and 16GB of RAM)",Positive
Intel,"I had crashes about 50% of the time a map would load. But once I got in, 0 performance problems. I expect the crashing issue to be resolved by launch so I’m not worried about it.  Game definitely deserves to stand alongside Half-Life 2 in the extreme scalability department.",Positive
Intel,First thing I noticed was how good the graphics were despite it running very well. It really manages to be beautiful with good frame rates.,Positive
Intel,KCD 2 had pretty good optimisation,Positive
Intel,I had maybe 4 or 5 crashes in about 8 hrs of gameplay across the 2 betas. Not bad but a little annoying,Negative
Intel,You'll be fine. My gf's 1070 was able to keep a steady 75 fps on low 1080p and it still looked good.,Positive
Intel,I was rocking a 2070 super and I was hitting 60fps,Positive
Intel,"My Titan V did 1440p Native on High Settings, respectable framerate, smooth too. Titan V is slightly ahead of a 5060/2080Ti",Positive
Intel,"That's what I'm on and it ran beautiful at high settings 1080, low-med setting 1440",Positive
Intel,Love how a common sense normal comment is downvoted. This sub should be renamed to PC casual race asap.,Negative
Intel,"I did run the whole beta with my 1060ti and the game run just fine, biggest problem I encountered was that my audio sometimes stayed muffled the entire matches like in the menu screen. none of my friends had the same issue with newer PCs.",Positive
Intel,"when it comes to battlefield games you will always play it on hardware that is lower than minimum and still have a good time, just frostbite wizardry.",Positive
Intel,what about rx 570 4gb ?,Neutral
Intel,There a cfg file you can place in the games directory that REALLY helps 6 core CPUs. I'm running 9600x and that was struggling a little too until I placed the file in there. Went from 80-100% utilization to 60-80% in heavy scenarios. Still high but it's expected this game really loves high cores. When I'm fully CPU bound I think I can push around 140-150 fps.,Positive
Intel,"Im in the same boat, regretting i didnt spend the €200 then. New am5 would be around €700-800",Negative
Intel,DLSS was set on balanced with Low/Med settings and I was around 70 fps I think,Neutral
Intel,"Today is just a 70s. 5070 Ti performs already 20-30% better. (Expect few games, its more like 30+ games avg).",Positive
Intel,"Time sure flies.  Though I suppose if you don't really play modern g ""AAA"" games it's no big deal. So long as it can play the games I want at the frames I want.",Neutral
Intel,https://preview.redd.it/75u3hx3isrtf1.png?width=600&format=png&auto=webp&s=09a1574db4e49c33392b0f44ba9ff42ebc9f5d7c,Neutral
Intel,Same. Capped 120 FPS on 1440p and that was stable with a 7900XT. Played Ultra.,Neutral
Intel,Lemme guess you’re a fan of upscaling,Neutral
Intel,"> EVC2 current ratio mod and custom vbios.  Got any videos for me to learn more about this? I won't set my house on fire because I don't plan on doing any mods myself, I'm just curious how one unlocks *that* much more power out of a card",Neutral
Intel,oof the delusion is strong with this one.,Negative
Intel,Probably fsr balanced,Neutral
Intel,3440x1440 also here,Neutral
Intel,ultrawide support was mentioned in the PC launch trailer. you're good to go. I have a samsung G95C so i'm in the same boat.,Positive
Intel,"5070 ti is pritty close to 5080, so yeah were able to run ultra++.  Frame gen is nice, but i wouldn’t use it in Multiplayer. It adds extra latency, i only use it in single player games.   Cyberpunk all maxed out + pathtracing + frame gen is pritty awesome.",Positive
Intel,I’m poor 🥲,Negative
Intel,"I’ve just been replaying older games I haven’t played in awhile. And a lot of ready or not, bannerlord, ground branch, BG3. Surprisingly those games run well, but I do lag a lot during online play. I always thought it was my internet, but would my shitty CPU be causing lag as well?",Neutral
Intel,I was using that CPU up until 2023. It's not as bad as you think. Only thing I was really limited at was VR.,Positive
Intel,"I played it on a 5500XT and Ryzen 5 2600 and was hovering between 45-60FPS at native 1080p low, no way your system was that bad, it should be doing 80-90 with a bit of a CPU bottleneck.",Negative
Intel,Thanks... Even if I can't help but still feel bugged out by this method...,Negative
Intel,So then I would think it's kernel based one. I hate that.,Negative
Intel,I have the gtx 970. Will the game even run? Lol,Neutral
Intel,What about gtx1080 oc with i7-6700k ? Its potato but will be fine for me on 1080p low :) just thinking should I preorder or no.,Neutral
Intel,What'd you cap it at? Monitor refresh rate? Something lower?,Neutral
Intel,1080p scales perfectly with 4K so it’ll look fine. 1080p on a 1440p monitor doesn’t scale well so there isn’t any way of making it look good unfortunately. You should just use 1440p.,Negative
Intel,That’ll be the first thing I try. I had a 1440p monitor in the beta that gave out on me in September so I’ll do some experimenting.,Neutral
Intel,"Im using a 9070XT with a Ryzen 79800X3D, 32 Gb of ram, and im not sure whats going on as i have over the recommended specs. The game crashes every 45 min or so.",Negative
Intel,"I didn't unfortunately as I wasnt sure how to fix the secure boot stuff. That's sorted.now though, good to hear!",Negative
Intel,Probably it’s because AMD historically is great on CoD  + (at least) 6000mt/s Ram instead of those 4800 non-expo/xmp,Positive
Intel,Appreciate the information.  Thank you.,Positive
Intel,"My comment was for fun only, of course it will run.",Neutral
Intel,Yeah I’m just excited it ran 100x better than Borderlands4.,Positive
Intel,![gif](giphy|j6uK36y32LxQs),Neutral
Intel,"adding to the first part, is that framerate the average framerate? the 1% high (never thought I'd say 1 percent high)? is that framerate in the most intensive area? or is it in the least?",Neutral
Intel,"\> make it as confusing as possible  Maybe not even the devs know how the game will perform (lack of testing, too much variation on FPS, etc)",Negative
Intel,"These aren't sorted by resolution, each group has two different resolutions - the categories are just examples of typical PC configurations (including typical CPUs) and typical resolution and framerate combinations those configurations might be running.",Neutral
Intel,you'll get more depending on your RAM speed - I have the same chip and upgraded RAM and OC'ed/timed it best I could and the results were very worth it.,Positive
Intel,Good. Undersell overdeliver should be the standard for quality products.,Positive
Intel,"Yeah they list 1440p high at 60 fps native for 3060 ti, but using DLSS quality, I had over 90-100 fps with high settings. And probably can get a lot higher just lowering a couple settings to medium or DLSS balanced",Neutral
Intel,it might be due to map size as well. All the beta maps were pretty small for battlefield standards. Maybe the bigger maps have a slightly lower performance,Neutral
Intel,100%. I have a 6600xt and a 5800x and I don't really see my frames drop below 100 that much on low settings 1080p. Runs like a dream.,Positive
Intel,"I think I was getting about 120 fps with a 9800X3D, 9070XT and 32Gb of RAM at 3440x1440p",Neutral
Intel,"Lmao, you actually think synthetic benchmarks are the same thing as real world performance. That website has the [9800x3d as 30% slower than the 14900k,](https://www.cpubenchmark.net/compare/5717vs6344/Intel-i9-14900K-vs-AMD-Ryzen-7-9800X3D) yet actual benchmarks have it running BF6 [30% faster.](https://www.tweaktown.com/news/107068/battlefield-6-bench-9800x3d-is-30-faster-than-14900k-at-1080p-hardly-any-difference-4k/index.html)",Neutral
Intel,Who in mind thinks that 7800X3D is worse than 12900K ?!?!! And who uses cpubenchmark to test which CPU is better for gaming?!?!?!!,Negative
Intel,"The 3D cache on the 7800X3D does make it noticeably, if not significantly, better at gaming than a 12900k, especially in Battlefield. Synthetic benchmarks do jack shit to show gaming performance for a 3D V-cache chip vs a non 3D chip. And multithread is the worst comparison to make, which is the only part where the 12900k is significantly better.   https://youtu.be/78lp1TGFvKc?t=918 This is vs 13900k so let's add the comparison of that chip with the 12900k  https://youtu.be/P40gp_DJk5E?t=1049  https://youtu.be/B31PwSpClk8?t=800 And from a different channel.",Negative
Intel,"For a processor that was only 18 months older than the 7800x3d, and double the power budget no less while costing around 100 bucks more at time of release, i would certainly hope it would be better. But the fact that it is comparative in bf6 i guess shows an interesting dichotomy that we are going to see an increasing reliance on optimisation to make more use of on-die cache.  And while yes traditionally multiplayer shooters tend to be less cache and less multi threaded dependent, BF has long tended to be the exception to the rule due to the physics of the destruction",Neutral
Intel,"I was also crashing very frequently for most of the beta (every 20 minutes or so), I only figured out during the second last day that I was running out of RAM. This had never happened before on other games so I never bothered to check if it were my RAM or not. I fixed the issue by increasing my Page File size so when my RAM reaches full capacity it spills over into my system storage; This didn't hurt my fps at all fyi, it just completely eliminated my crashing issue full stop. If you plan on playing BF6 on release and they still haven't fixed the RAM consumption issue, I'm like 90% sure this will fix your issue if you haven't already. Also on a side note, I have a 3060 Ti and a R7 5700X3D, and boy was the game terrifically smooth! for the first day or two (despite the crashing), even on 80-90 FPS it genuinely rivaled the smoothness and polish at that fps that I get at like 120-144FPS on other games. My 1% and 0.1% were impressively high so that's likely why the game felt so smooth, though I eventually switched to Low settings with DLSS4 quality for the remainder of the beta since even low settings looks great.  Edit: Forgot to mention that yes, I as well have only 16GB of RAM.",Negative
Intel,I think I was either high or ultra settings with a 3080ti and 40GB of ram,Neutral
Intel,"Silly billy, tell her to crank it up to 1080p High and turn on FSR Qualtiy and she'll no doubt get 50-60fps",Neutral
Intel,wow thats amazing! I've read that people with bricks could get out a good ammount of fps! What a game!,Positive
Intel,Best news I’ve heard all day about to attempt with a 1080 tonight,Positive
Intel,"What's a respectable frame rate bro, for some people it's 40. You typed all that without dropping a number.",Negative
Intel,Nice good to hear seems its decently optimized,Positive
Intel,"Yes. ""No issues"" literally says nothing. Like does the game just start up no issues, or are you running it at ~100 fps no issues?",Negative
Intel,mind posting this cfg file info or link?,Neutral
Intel,What is this sorcery? My 5600x could use the boost,Negative
Intel,Having a high CPU utilization isn't a bad thing unless your GPU is being heavily bottlenecked.,Neutral
Intel,"5070 Ti is priced $150 more than the 3080. 4070 was 30% faster than 3070 while costing $100 more (so aside from L2 cache, really you PAID for those performance gains rather than new architecture advantages) and the 5070 might have dropped $50 (still $50 more than 3070) but currently is only 10% faster than a 4070.  I wouldn't consider modern xx70/xx70 Ti to actually be that when it's priced like xx80 or has weaker gains than previous generations.   NVIDIA purposely changed things up with the RTX 4000 series to confuse people on what mid-range and high-end actually mean. There is a reason the 4060/4060 Ti were barely beating the 3060/3060 Ti (and sometimes losing).",Neutral
Intel,Wise word,Positive
Intel,"100%.  But also: a 3090 is 5070-class performance with double the VRAM, and in the GPU market of 2025 the 5070 by itself is borderline high-end, so a comparable GPU with double the VRAM is definitely still high-end until the 6000 series rolls around.",Positive
Intel,"Exactly. I don’t play modern games right now, there ain’t much worth playing anymore, I play mostly Indie Simulation games, my 7800XT does me fine, I do need a new CPU tho.",Negative
Intel,I have a 7900XT and was playing at 4k with FSR Quality and medium/high settings. 90+ FPS the whole time. Shit was glorious.,Positive
Intel,"Don't have to be a fan of upscaling. The 7900XTX was third best at release and is now even further behind. Still an amazing card but it was never top of the line, especially not for features",Negative
Intel,[https://elmorlabs.com/product/elmorlabs-evc2se/](https://elmorlabs.com/product/elmorlabs-evc2se/)  [https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-131](https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-131)  [https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-409](https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-409)  [https://www.overclock.net/threads/brand-new-method-how-to-unbrick-flash-almost-any-card-amd-or-nvidia.1612108/](https://www.overclock.net/threads/brand-new-method-how-to-unbrick-flash-almost-any-card-amd-or-nvidia.1612108/)  [https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-36?post\_id=29156997&nested\_view=1&sortby=oldest#post-29156997](https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-36?post_id=29156997&nested_view=1&sortby=oldest#post-29156997)  [https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-114](https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-114),Neutral
Intel,Less goooo! Similar specs,Positive
Intel,Thank you!,Positive
Intel,🥲,Negative
Intel,"Well, I had an i7 7700 and at some point I’ve noticed my cpu was at 100% most of the time when gaming and nothing would load on my second monitor. So could be it",Neutral
Intel,"You have no idea what you are talking about lmfao, my cpu is 100% better than my GPU and my in game telemetry said so, CPU frame time was around 10ms and GPU was at 16ish   Some maps was 70-90 some maps 50-60ish",Positive
Intel,You’re going to have to get used to it. It will become more and more common.,Neutral
Intel,"Hmmmmm, depends. Likely 60fps if 1080p Low",Neutral
Intel,"Well since that GPU is slightly ahead of a 3060 8GB, I recommend 1080p Ultra w/ FSR or 1440p Medium with FSR Quality",Neutral
Intel,"Patience is a virtue. We recommend you wait for reviews and updates! Check [IsThereAnyDeal.com](http://isthereanydeal.com), [CheapShark](http://www.cheapshark.com) and install [Enhanced Steam](http://www.enhancedsteam.com/) to find the lowest price available. Additionally, Steam Early Access games may be very buggy and there's always the risk that developers do not deliver a finished product, so always keep that in mind. Download demos when you can to make sure the game will run on your PC. Pre-ordering a game or buying Steam Early Access is, however, a valid reason if you really want to support the developer or perhaps even get ahead of those darn spoilers. Consider resisting temptation and play with the [/r/pcmasterrace Steam group](http://steamcommunity.com/groups/steampcmasterrace) today!   ***  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/pcmasterrace) if you have any questions or concerns.*",Positive
Intel,Ah damn alright I'll stick to 1440p with upscaling then.,Neutral
Intel,I kind of get it but I still don’t see why you wouldn’t just do DLSS quality on your 4k monitor. What’s the refresh rate on it?,Neutral
Intel,Of course.,Neutral
Intel,pretty sure its the average frame rate.   I have a 9070xt (not 100% sure if I upgraded from the 5600 to the 5700x3d before or after the beta) and played at 1440p everything high or max settings and was getting at least 100 fps,Neutral
Intel,Most likely average fps throughout a match. Campaign will probably run a bit better depending on level and scenario since more players = higher cpu usage. 1% lows I think I've never seen before on a settings sheet and only seen it in benchmarks.,Positive
Intel,"1% high would be an utterly useless thing to measure. 1% lows at least measure how much stutter there is, but 1% high measures what exactly? That 1% in the benchmark run that has mostly the skybox in it?",Negative
Intel,i doubt that. You dont spend years on a game and have no clue how it performs.,Negative
Intel,"But PACKED with detail and geometry and and all the players in a relatively small area so absolutely chaotic in terms of destruction and debris and special effects happening right in front of your camera. The larger maps, are more “empty” based on gameplays I’ve seen, and there aren’t tons of action happening simultaneously in front of your due to the players being way way waaay more scattered.   I think performance will be similar",Negative
Intel,"9800 and 7800x3ds are much better for gaming, he is saying how good they are as actual cpus, not just for gaming.",Positive
Intel,I was being sarcastic to make a point about comparing CPUs by a single metric when real world performance is inevitably more complicated. As I explained in the second paragraph of the comment you're replying to. Come on now.,Negative
Intel,"Comment saved with all this meaningful info!  And yeah,i expected to be just another performance slop but oh boy when i saw that i was making more than 60 FPS on Cairo with high settings while everything was being blown up,i felt very excited",Positive
Intel,"Yeah probably,damn i wish i had more than 16GB of RAM lmao, it's on the list of the upgrades bringing the total to 32 GB.  Still,i'm super impressed at how smooth the game ran with that amount of RAM considering that less realistic games run at a fraction of the framerate with less graphical settings",Positive
Intel,3060ti + Ryzen 5 3600 and my performance was better after turning settings up and putting more load on GPU. Probably because my CPU is bottleneck.,Positive
Intel,I like how you think! Gotta try that out this weekend.,Positive
Intel,"Well, I got 80-90 FPS 1440P High Native Resolution, while on High I got 90-95\~ on a Titan V",Neutral
Intel,"Yeah, also a 1080Ti which is roughly your cards performance gets 60-75 fps on 1440P High with FSR Quality turned on! Pretty neat honestly",Positive
Intel,Mind you I'm running a 9600x. I'm not sure how well a 5600x is going to pair with this but you should get some form of help from it.,Negative
Intel,Which is the problem with 6 core CPUs in this game. Dropping everything to low and lowering resolution did nothing until I used the file. CPU was pegged at 100%. I'm still CPU bottlenecked but it's much more manageable and not limiting me as much.,Negative
Intel,"There was a significant leap on 40 series.  The 5070 rn is like a 4070 Ti (non super) which performs like a 3080 ti - 3090. 3090 Ti is still stronger, btw 5070 and 5070 ti, some cases even better.",Positive
Intel,Actually insane to think a AAA shooter is looking this good and still optimized well. There are worse looking shooters performing much worse. And there are single player games performing much worse. Looking at the UE5 slop especially.,Negative
Intel,"I’ll take what I can get, until I convince myself to get the much needed upgrade.",Positive
Intel,![gif](giphy|5xtDarmwsuR9sDRObyU)  Going to preorder. :),Positive
Intel,That’s going to be what I do first and hope for a triple digit frame rate. I have a 240Hz display.,Positive
Intel,With frame gen during the beta I was getting pretty steady 140-165fps with almost everything at ultra and dlss quality at 1440p. This was on an 11700k and a 4070ti so I’m hoping performance will be similar… this chart makes it seem like it will not.,Positive
Intel,"1% high is the top 1% of the fps, the opposite of 1% lows.",Neutral
Intel,"I meant that they don't how it performs on every cenario (where in the game, where looking to in the game, all the GPUs, all the CPUs, etc).  For example, in  Fallout4, if you look to the ground outside the main city you will get a very good FPS, but if you look to too many buildings inside the main city, you might get half what you had outside.  Some games, even if you follow the same steps, you might get different results on FPS due to random objects.  Theses tests give little information if you do them while developing the game, because future changes can throw away all tests done in a previous build, then new tests must be made with many combinations of GPU, CPU, SSD, RAM, ingame configurations, etc. Also, doing development all the configurations that make Medium and Highmight change, Low and Ultra usually mean lowest and highest, which **usually** doesn't change.  Given that few games get released with minimal bugs, I really doubt they fully test it before launch.",Neutral
Intel,"I KNOW RIGHT! I've played BF1, BF5, BF2042, and BF4 and 3 on my PS4, so I've gotten used to the great optimization that Battlefield always seems to deliver, and so I was also expecting the same for BF6, and that they most certainly did deliver on, though it still impressed me for two reasons; For one, I felt like the volumetrics were a huge step up from previous titles and you'd think that it would affect the performance exactly like how you mentioned in this comment, yet it didn't! Secondly, I had become so used to the awful performance being delivered from modern titles nowadays, coupled with my very low expectations before the beta, that despite the fact that Battlefield always delivered with great optimization, I genuinely thought that this game would be the exception, but no! So yeah, I'm very pleased with this game and I can't wait for it to release!     If what I said previously does work when the time comes, I'm glad to have helped!",Positive
Intel,Oh I'm 100% sure EA knew they could not screw up this time or battlefield would be dead,Negative
Intel,I know what you mean. I returned Borderlands 4 after trying it. I was getting a *rough* 60 using medium/high settings (from Hardware Unboxed optimized recommendations) with FSR balanced at 4K. But emphasis on *rough* since the 1% lows were around 30-40 even when just walking through the world.,Neutral
Intel,"You can at least add frame gen too since you’ll be hitting an acceptable frame rate without. I know it’s a dirty word on this sub but give it a shot, it’s done well for me when I want to max out a HRR monitor. If you don’t like it you can always turn it off",Neutral
Intel,Why don’t you? Most of that chart is showing performance at native no upscaling,Neutral
Intel,"I was reading that wrong, I was interpreting balanced and performance as dlss options. I was thinking they weren’t even quoting quality performance but I see what’s going on now.",Negative
Intel,"Get a used Dell or HP mini PC. Something like $400 to$600 for an i7-12700t to 13700t, 32GB RAM, 512GB-1TB NVMe SSD and Windows 11 Pro. This is business you want a valid Windows Pro license. Anyways That’ll do office work just fine for the next seven to ten years.  I got two similar to this for about $400 each several months ago (13700t and 14600t). But tariffs have raised the used market prices some.   Really they are overkill for most office work. An i5-10500t is more than fast enough, actually. But the newer i7 are more energy efficient and will last considerably longer.  I can’t get anywhere close to this price and performance ratio building something myself.  Get one of these. Clean install the OS. Then get his software, data and settings moved over. So that the new computer feels as close as possible to using the old one, just faster. Also turn off all the annoyances like suggested apps, animations and notifications. Then scour the privacy settings.  All that being said. Once you price out the new computer parts and Windows license. They’ll probably cost a lot more and those CPU will be gross overkill.",Neutral
Intel,When they inevitably do a refresh they better call it necromancer,Neutral
Intel,"Assuming the thing competes with a 5070 it's going to be bigger than a 5080 (the 4060 and B580 trade blows  the Arc is 70% bigger).  There's no way they can get any decent profit selling a 400+ mm2 die for 500$ (probably even less once the 5070 super launches). It will probably end up like the B580, a paper launch with most of the cards actually selling for 650$.  They're better just selling those dies to professionals with 32GB of VRAM.",Negative
Intel,Born from the bin of nvidia igpu deal resurrected to kick ass,Neutral
Intel,"If Arc isn’t shelved by N maybe they will. Next up are Celestial and Druid, hopefully.",Neutral
Intel,"It's a C tier PSU, I would want something better,  Can't say for itx pc vs laptop as I didn't own a laptop for over a decade and never had an itx pc.",Neutral
Intel,Understandable. What's a good psu recommendation?,Neutral
Intel,"One that I recommend often now is Montech Century 2 850w as it's A-, but there's not many units that are in between. Only stuff like Corsair CX-m 2021 750w that's C+ on SPL tier list.",Neutral
Intel,"Yea I've seen the Montec Century it is solid, didn't think I needed 850w all the way. I'll check the tier list again.",Positive
Intel,This looks like a clown wearing a tuxedo,Negative
Intel,"I REALLY wish they had an AMD sonic motherboard, I wanted one of these so bad when they released...",Positive
Intel,"Gotta go ~~fast~~ as fast as we can until we hit the limit of single-channel memory, thermal throttling, or SATA throughput",Neutral
Intel,This is just horrendous. But it goes so far that I absolutely adore it.,Negative
Intel,"The blue theme for the motherboard, gpu, and stock cooler kinda slaps ngl",Positive
Intel,"You should look into a low profile cooler with an LED fan on, then it's a sure thing that it will have a spinning yellow ring option on the lighting",Neutral
Intel,The motherboard is so fire. I bet it triples performance,Positive
Intel,it's strangely beautiful,Positive
Intel,Who is mad? This is peak.   What case is that?,Negative
Intel,I wish I had this PC. This looks like what I would make.,Positive
Intel,Damn kids and their having to go fast,Negative
Intel,"This made me laugh, thanx",Positive
Intel,It's disgusting. And I fucking love it lol,Negative
Intel,I’m sure you can make it jankier. We believe in you!,Positive
Intel,Looks better than most of the rainbow puke PCs you see nowadays.,Positive
Intel,"So, the mobo costs more than the whole pc. Congrats. 🎉",Positive
Intel,"Eh, still looks better than half the shit that comes through my workshops.",Negative
Intel,Not bad. New sonic game is honestly very refreshing as well,Positive
Intel,">Rickety, loud, and annoying, but does what I need it to nonetheless.  Did I miss where you told us what unholy task this chimera has as its day job?",Negative
Intel,Man that mb looks 🔥,Positive
Intel,No absolutely not. This sparks joy.,Positive
Intel,That's a nice looking build. It mixes together nicely.,Positive
Intel,too bad theres a silkscreened sonic pic on the back of the board that will never be seen ...got the atx version of the board though,Negative
Intel,that motherboard goes hard,Positive
Intel,is the elgato card blocking the air on the gpu?,Neutral
Intel,Least the gpu kinda matches right?,Neutral
Intel,Or Sonic the Hedgehog with a small ghost costume,Neutral
Intel,"Yeah, if I didn’t have an unused cpu that I didnt feel like trying to sell, i might have resisted. Was a bit disappointed to find they didnt make one for amd",Negative
Intel,just get some stickers,Neutral
Intel,Hell yeah brother  ![gif](giphy|KAxcdTUUP9PMI),Positive
Intel,Why thank you  ![gif](giphy|hDMJjUNxLhIjK),Positive
Intel,"Actually kinda sorta have a plan like this, just working out what parts to get and when. Havent looked into low profile coolers though, so that might be a pretty good idea",Positive
Intel,Havent been able to find a speed limit anywhere B),Negative
Intel,"Jonsbo d32 pro. Wanted to get is relatively small, since I had initially wanted itx. This was pretty close to what I wanted in size",Positive
Intel,"Idk if you have the money or any parts, but here is pretty much a list of what I have with a few slight changes (slightly different fans since the ones I have were taken off of an AIO, a better cpu by two gens, nvme ssd in stead of the SATA and two sticks of  better ram)  https://pcpartpicker.com/list/WRdrNz  Here is the listing for motherboard on amazon. Same price still for now  https://www.amazon.com/dp/B0BQWPLFNM?ref=cm_sw_r_cso_cp_apin_dp_2CNXTTTRVTKAKRDZ7JBN_1&ref_=cm_sw_r_cso_cp_apin_dp_2CNXTTTRVTKAKRDZ7JBN_1&social_share=cm_sw_r_cso_cp_apin_dp_2CNXTTTRVTKAKRDZ7JBN_1&titleSource=true  You could also get a cpu with an igpu and save for a better dgpu if you’d like. A310 is okay for low end gaming for now, but not a strong gpu at all.  Got automodded, had to repost the comment with a new link🤌🤌",Neutral
Intel,"Oh nah, found the motherboard at msrp deal on the motherboard. ‘Bout shit my self when i saw what some of em going for.",Negative
Intel,You were able to get these for really cheap like eight or nine months ago.,Neutral
Intel,"Frontiers or shadow x generations? I just finished og generations and am planning to start shadow soon, have not gotten to try frontiers yet. Heard its good though",Positive
Intel,"Part time dedicated streaming pc, some light video editing and music production which is all it is really set up for right now, and then it will also be run as a headless home server (media server, backups, a couple small game servers, maybe a few other things) whenever I am not actively using it. Just need to add the extra storage (and ram I guess) before doing all that. It missing one ssd and both of the planned hdd drives for now. Total of five drives; one nvme ssd, two sata ssd, and two hdds.",Neutral
Intel,"Kinda. There is like 1.3 cm of space between the elgato and the gpu, and it’s only covering just over half of the fan, so it could be worse. Looks like they’re right up against each other in the pic though",Neutral
Intel,Gonna save up for this 😈,Positive
Intel,Sonic racing crossworlds.,Neutral
Intel,"Ah, i appear to be out of the loop",Negative
Intel,"It's really good. I played the previous sonic racing games growing up, and this one feels like a really good advancement. I like how it directly copies mariokart world, but allows pretty much anyone to play it",Positive
Intel,Nice. My only experience with sonic racing was on a friends wii u when that came out. I’ll have to give it a shot,Positive
Intel,Damn not even testing 3000 series GPUs,Negative
Intel,"Ugh, one should NEVER post numbers with DLSS/FSR/XESS turned on (at least not as the ONLY numbers). It should be raw performance, and then people choose if they are willing to do upscaling after the fact.",Negative
Intel,"Dlss performance, of course it runs well lol. Try running it native on even a 5090. I did and it's not a pleasant experience.",Negative
Intel,Another game that requires DLSS and frame gen with almost every card to get 100+ FPS in 4K. Gotta buy a 5090 for native lol,Negative
Intel,Not even a 3090ti?,Negative
Intel,"The worst part about DLSS is it obfuscates and reverses the typical order and language of upscaling. 4k ""performance"" is 1080p. A $1,000 GPU sitting at 84FPS at 1080p in 2025 is absurd and embarrassing",Negative
Intel,"So \~117 FPS with DLSS on performance is ""running well"" now? But 100+ FPS with DLSS on performance was ""running like shit"" in Borderlands 4?",Negative
Intel,So um where are my 5070=4090 numbers?,Neutral
Intel,trash benchmarks with DLSS on. what a joke,Negative
Intel,Meanwhile you have Arc riders with a 1080 ti 70-80 fps in medium with unreal engine 5,Neutral
Intel,"Looks about right. Max settings/DLSS Quality/RT Off for me  HDR when enabled looks better than RT, without the performance cost.",Positive
Intel,Dunno if id consider sub 100 fps with dlss performance of all things on a 4090 running well.  I expect that kind of performance/fps out of dlss quality usually.,Negative
Intel,"I'm more or less fine with this Game's performance.  On native, I get around 60FPS with my 5070TI with Raytracing (HW Lumen) disabled. With DLSSQ I get around 80FPS up to 100FPS. With DLSSQ and FrameGen I'm at 140FPS or more. All in WQHD. Framegen also reduces (yes, you read that right) Latency by a big chunk because Reflex is activated with it. An option to disable Framegen and activate Reflex would be cool, tho. I modded it in with SpecialK, but not offering an Option in the settings is a huge miss from the Devs. That game has huge Latency Spikes (up to 100MS) and you can feel it drastically. So Reflex is a must even without FrameGen.  I settled to use FrameGen with a Reshade called Lillium HDR Reshade witch offers a Reshade to Sharpen the Game, so I don't have many troubles with ghosting from Framegen. I also installed a Mod Manager named UltraModManager that greatly buffs visuals and offers a fix for black floors (Color grading is a little bit off in games from those Devs). Right now I play with Game with 120FPS+ with DLSS Q and Framegen enabled but without much noticeable Ghosting and with decent Latency and HW Lumen enabled. If you like to Fiddle with UE and don't hesitate to install Mods, it's a great Game. The ""Basic"" experience is meh, tho. Not because of the lack of FPS, but because of that huge Latency thing.   I also gained huge performance increase just by downgrading my Driver to 572.83. Newest Driver has horrible micro stutters in my opinion.",Positive
Intel,"Just disable hw lumen and the game looks the same but with more fps, a lot more",Neutral
Intel,"Why was XeSS used on ultra quality instead of perf mode? One interesting thing I've noted is how 3000 series are getting slower and is consistent with Nvidia drivers where older GPUs, despite official driver support, always perform 10-15% worse even after taking Vram, bandwidth into account.",Negative
Intel,UE5 is needs to die in a fire.,Negative
Intel,"Worth pointing out that DLSS SR uses the old CNN model, source : [https://www.pcgamingwiki.com/wiki/Cronos:\_The\_New\_Dawn](https://www.pcgamingwiki.com/wiki/Cronos:_The_New_Dawn)     Only frame-gen uses the new DLSS 4.",Neutral
Intel,Another game when AMD does really good with RT enabled at 4K and at 1440p it's even better.,Positive
Intel,\> upscaling  can we stop turning upscaling into norm of gaming for modern high-end GPUs? upscaling should be used only on older hardware to get performance increase without having to sacrifice looks.,Neutral
Intel,Yeah it runs well for me and looks gorgeous too. I love the intentionally clunky combat. I feel like sh2 combat was a bit too op. Like I could 1v3 mannequins with a pipe in my hand lmao. They toned it down well and the guns feel satisfying to use. Story so far is very mysterious and I am enjoying the buildup hope it delivers. That said the real standout to me is the combat and the atmosphere.,Positive
Intel,I have been really enjoying the game,Positive
Intel,Something is wrong I get way over 60fps @1440P with a 7800XT Hellhound,Negative
Intel,How many of you are actually playing in 4k? Im still crushing games at 1440p with my 7800xt,Neutral
Intel,Is the 4070 Ti Super just not relevant enough to include in these lists? Odd that it's the only RTX 4000 series card absent from the rankings,Neutral
Intel,I don't see my 2080 super on there. 😔,Negative
Intel,Be very curious how the 5070ti and the 9070xt stack up one another using the UEVR VR mod - but that might not be hardware that you can really use with UEVR on this game.,Neutral
Intel,Is ray tracing fixed on the 9070 cards? It kept crashing when I tried to use it.,Negative
Intel,I guess my 7900XT does not exist,Negative
Intel,"Oh wow, DLSS performance. So interesting /s  Is this a joke? DLSS performance looks awful. Post native performance, *maybe* with DLSS quality but not with anything else",Negative
Intel,"What is with this site? Why are they testing with different upscaling settings? That’s the second time I’ve seen messed up results from this site.  (Upscaling or not, just be consistent FFS)",Negative
Intel,Epic preset and then dlss4? What is even the point?,Negative
Intel,Crazy how bad the former PURE RASTERIZATION MONSTER 7900xtx performs. I thought it was supposed to rival the rtx 4080? 9070xt raytracing performance seems alright tho,Negative
Intel,another unoptimized game?,Negative
Intel,9070XT going strong,Neutral
Intel,![gif](giphy|zvBuF2oYRErVS)  That feeling when you no longer see your GPU being used to compare FPS in games.,Negative
Intel,What is going on with xtx? The old king should do better.,Negative
Intel,"XTX losing out to a 5070 is wild, RDNA3 has not aged well.  I've only played a bit of it so far but was getting good framerates at 4K with settings maxed, DLSS Performance, and frame gen.",Negative
Intel,Meanwhile Me with RTX 2050ti😣,Negative
Intel,I think those numbers are good because hardware RT is on. I am playing on Balance and i think it's alright @ 3440x1440. Most UE 5 games I play on high.,Positive
Intel,"Nanite and ray-tracing will make your cards obsolete,",Negative
Intel,The 3060 Ti was tested for 1440p.   Why they chose that card and that card only is beyond me.,Neutral
Intel,Lol no love for my 3070 Ti,Negative
Intel,100% agree,Positive
Intel,I 100% agree but it is funny seeing people over in r/hardware saying that since people use DLSS-Q/B/P that those should be the posted numbers.  In nearly every benchmark video post there is someone complaining which is sad.,Negative
Intel,"I can agree with both but upscaling is now an industry standard and those numbers are going to be more accurate for the majority.  Well unless you bought a $900 card that doesn't have access decent upscaling for some reason, but that would have been a silly choice.",Neutral
Intel,"If they turned ray tracing off (which is software then), it runs a lot better. Like… a lot.",Positive
Intel,"Native 4k gaming is pretty much dead, especially with ue5.",Negative
Intel,"Yeah, my 17-30ish fps during the intro with my 4090 at native 4k with everything maxed out HURT. With DLSS and frame gen on, I get 50-60ish most of the time. Thankfully it actually looks great in this game.",Positive
Intel,Then you don't have a good enough CPU because it runs perfect on my 5090 native with a 9950x 3D,Neutral
Intel,Even that wouldn't do ;),Neutral
Intel,Wait. This says Ray tracing on. These numbers are fantastic lmao,Positive
Intel,A game that can run at 60+ 4k native on a 9070 XT which is quite far from being top of the line sounds excellent to me  (Edit) Is it native? Nothing in the images suggest it's not native but I don't actually know lmao,Positive
Intel,"To be fair, rtx is on too. With it off those numbers ar probably much higher",Neutral
Intel,"It’s not just the resolution taxing performance. Graphical settings such as lighting effects, reflections, shadows, etc. all play a role. Lowering the resolution is just one part of the equation when it comes to performance. You can lower everything down to medium and most likely  increase your frame rate more than just lowering the resolution.   Yes, a $1000 GPU should get better performance, but as how it’s always been with PC gaming: you want bleeding edge graphics, you take a bleeding edge performance hit, no matter what your hardware cost.   Is UE5 a mess? Yes, but so was UE4, which took years to get right.",Neutral
Intel,"Unfortunately, there is no ray tracing performance improvement in the 5000 series compared to the 4000 series. Ngreedia is too busy promoting DLSS tensor cores marketing garbage over raw GPU power. Raytracing should be the future not DLSS, which is just an over-glorified blurry TAA.",Negative
Intel,Ha,Neutral
Intel,If that's the case then overriding in the Nvidia app should get you the transformer model,Neutral
Intel,I heard through the interwebs that AMD is not good at Ray tracing. Outdated tales.,Negative
Intel,It’s too late for that. Developers are creating with upscaling in mind. Most new high title and ue5 games will only run 4k native above 60 frames with a 5090.,Negative
Intel,*Randy Pitchford disliked this comment*,Negative
Intel,DLSS has been out for like 7 years.  It's a perfectly valid piece of software.,Positive
Intel,"it's either that or nothing. It's not a plot by nvidia to sell you upscalers (and AMD and Intel too? come on).   It's actually very simple: software is advancing at a rate much faster than hardware, which simply can't keep up",Neutral
Intel,Why would we? That's going to be the expected use case.,Neutral
Intel,Gotta make the numbers look good somehow.,Neutral
Intel,I just started the game. I'm already creeped out by the atmosphere,Negative
Intel,"I'm at 1440p ultra wide (ran 6700XT, 7700XT and now 9070XT) and I use the virtual resolution set at 5k2k. Looks and runs fantastic.",Positive
Intel,It's crashing for me right now. I can only launch it with -dx11 code in steam. This is bad,Negative
Intel,Probably around 9060xt levels.,Neutral
Intel,💪,Negative
Intel,I've just finished the game and it ran wonderful on my 9070XT Reaper. Earlier versions crashed with RT on but it's patched now.,Positive
Intel,4070 ti here. You hit the nail on the head. Feels bad,Negative
Intel,It’s with Ray Tracing on,Neutral
Intel,Dlss performance. I wonder,Neutral
Intel,These numbers are with raytracing enabled. RDNA 3 doesn't perform well with RT. RDNA 4 is performing very well though,Neutral
Intel,"If only they gave 3060ti 12gb vram, we were so close to having an amazing card",Positive
Intel,Think it’s a pretty common card still compared to the other 30 series cards.,Neutral
Intel,Honestly it's a perfectly fine card.,Positive
Intel,Or my 3090ti :(,Negative
Intel,It's nice to have both the pure native numbers and upscaling numbers to help consumers make a decision. It's not fun when either bit of information is left out. Just give us both numbers.,Neutral
Intel,it's so dumb since even nvidia cards with 1 generation difference can use different upscalers. And then you add AMD and intel to the mix. you're comparing upscaling software at this point,Negative
Intel,Dlss perfomance in qhd? Nobody's going to use that,Negative
Intel,It didn't exist for all that long. I played everything at 4K for 5 years on a 3080. There were only about two years where the hardware was sufficient before ray tracing became part of most AAA games and running at 4K required significant compromises.,Neutral
Intel,Sad but true. I usually don't go down to performance dlss unless I absolutely have to. It doesnt bother me as much in something like Marvel rivals for example. I noticed the game shimmered a fair bit even with dlss 4 balanced. Frame gen also cuts the 1% lows a fair bit which doesnt feel great.,Negative
Intel,meanwhile tekken 8,Neutral
Intel,"That cannot feel good, your input fps will be around 40. That would give you pretty high render latency. Id recommend dropping a few settings.",Negative
Intel,I'm running it in native and it's totally fine.,Positive
Intel,With performance upscaling... I use upscaling 95% of the time myself but would rather leave it at quality or balanced. Making perf the starting point is borderline disingenuous.,Negative
Intel,Exactly. Ray Tracing is ON,Neutral
Intel,"Yeah but with DLSS 4 Performance, I own a 4080 playing in both 1440p and 4k and those performances aren’t great. At least the 1% lows seems stable",Negative
Intel,"Text description in the post says it's FSR/DLSS performance. It's upscaled 4k.  It's not that bad. 9070 XT isn't supposed to be a 4k card and this is with Ray Tracing on. It could be a bit better though, and hopefully it will over time.  Edit: 1440p stays above 100FPS even in its 1% lows too.",Positive
Intel,Thanks man I had no idea resolution wasn’t the only factor.,Positive
Intel,"It will now if you update the Nvidia app. Previously, it did not. It would say it was overriding, but it wouldn’t.  I checked the overlay and it’s Preset K now though which is what I set.",Neutral
Intel,"I played mgs delta, silent hill 2 and f and max settings with 7900 xt with a 7800x3d and did not go under 90 fps and stayed mostly at 165 🤷🏽‍♂️  Edit: yes mgs delta is stuck at 60 fps, but other new games run flawlessly",Neutral
Intel,But our current hardware is not good enough for AAAA games!,Negative
Intel,"It’s not that’s it’s not *valid*. They practically made it a *requirement*. It should be a helpful tool, not used to fix a lack of optimization.",Negative
Intel,It would be more accurate to say that poorly built software (UE5) can’t make the most of hardware that already exists.,Negative
Intel,"Arguing in favor of dlss will get you downvoted to oblivion by this subreddit that represents less than 0.01% of gamers. They'll all use it but also whine about it because it's the cool thing to do. Probably a good balance is to quantify the visual impact of dlss in conjunction. Though in almost every game dlss is so good now that u barely notice it. Visual artifacts from dlss are like nothing compared to the ridiculous amount of pop in and low quality lighting they seem to be fine with in older games.   So yes, most of these people will downvote you for using dlss in ur benchmarks even tho I think it's perfectly fine, be prepared for that.",Negative
Intel,"And FSR Performance being used on the other side, so I'm not sure what your point is?",Neutral
Intel,"Nvidia has had a vram problem for a while now. It's great that the 5060 Ti has a 16gb variant, but they also have an 8gb variant....   Not to mention that the 5070 only has 12gb for some reason. You have to buy a 5070 Ti to get 16gb, when the Ti could have absolutely been 24gb.",Negative
Intel,"5080 being a 16 GB card is the biggest crime, least they could do was 18GB.",Negative
Intel,"The 3060 base model is the second most common card according to the Steam hardware survey results for this year. 3060 Ti is the 7th, 3070 is the 8th, 3060 mobile is 9th, and the 3080 is 18th.  If the 3060 Ti got included, I think they should have at least included the 3060 and 3070. Especially the 3070, which is a more capable card.",Neutral
Intel,Yeah honestly good budget option,Positive
Intel,You gonna be maxing out texture quality for a long time:),Positive
Intel,"Yes this, cuz right now there is a lot of narrative being pushed just to get clicks and we get presented with fake data.  I don’t think raster is a valid result, I mostly use dlss q on 1440p and ray tracing, while now “nvidia bad” narrative is going on, a lot of reviews just show raw performance no frame gen, no dlss and no RT, like whats the point?  And when cards were released all those same reviewers were hyping up frame gen and said how good it is and didn’t notice any issue until it was ramped to the max, so is it good or not? Are cards good or not?  I use frame gen on 3080ti with dlss on native and results are phenomenal in AC Shadows and Cyberpunk, so if it works that good on software level, I am convinced it works insanely well on 50 series which I will purchase when super cards come.",Negative
Intel,"I still decided to drop down from 4K to 1440p and turn off ray tracing. I just found the compromises you mentioned to be too much for both. 4K takes beastly power, and ray tracing guarantees you won't get great frames without DLSS.   I now live the high refresh, 1440p life. It's peaceful.",Negative
Intel,I don’t mind the upscaling but I have absolutely no interest in using frame gen. I don’t care for seeing higher numbers that aren’t actually real and make the game look worse.,Negative
Intel,esports titles usually have better performance than other games,Positive
Intel,"Yeah, thankfully due to the style and speed of the game, it doesn't feel too bad. It definitely isn't the best, but it's tolerable to me. It is a BEAUTIFUL game. It's just wild to me that even in places where you're basically only able to see where your flashlight is shining, the framerate is still rough lol",Positive
Intel,"He wanted 100+ fps in 4k, max settings at native. If you can do that with a 5090, that's awesome bro. I stand corrected. A 5080 can't do even 60 on some other games even including AW2.",Positive
Intel,And so is dlss performance. This is very flawed benchmark.,Negative
Intel,"So it's a little more performance on a 5090 than on Cyberpunk (Path tracing), which is not bad depending on what level of RT we're talking  is it full ray tracing or only select effects?",Positive
Intel,Isn't the game capped at 60fps? And aren't physics tied to this limit so unlocking it is janky?,Neutral
Intel,Yeah I totally agree.  But showing benchmarks with it isn't an affront to gamers,Neutral
Intel,"based on what? UE5 bad? I don't see other engines do much better, same thing with UE 4.27",Negative
Intel,"Hardware RT is On, which is pretty good in my opinion",Positive
Intel,"That ""runs well"" includes fg and dlss/fsr perf.  Yes, let's downscale 4k to 1080p, input fake frames and call it runs well",Neutral
Intel,"That's not a problem, that's being greedy. And 16gb 4060 and 5060 cost significantly more than 8gb variants. Last time they gave a budget card decent vram people bought and kept them, 3060 is still the most used gpu I think. Which hurt their greedy sales, so they went full ass.",Negative
Intel,hear me out: 5060 TI Super 24gb.,Neutral
Intel,"I understand what you mean, but does 2 gb really matter at this level? 8 to 12 is a significant 50% difference, but 16-18 is 12.5% increase.  What task will benefit from that? Because 18 isn't enough for AI and overkill for gaming.",Neutral
Intel,I didn't want to buy a new monitor. A 4K display running at 1440p looks worse than it does at 1080p.,Negative
Intel,Frame gen can look good if the input fps is decent.,Positive
Intel,"Everything max, even ray tracing at 4k, at 100+ frames per seconds is wild though.",Neutral
Intel,"I'm using g-sync so technically anything 40-180fps feels smooth.  Realistically anything 60-180 feels smooth.  Optimally 80-180fps is absolutely perfect.    I'd say I'm getting 70-140 range maxed out. DLSS quality puts me 90-160, so not worth it for the ghosting/shimmering during fast movement.",Positive
Intel,"Yes thats the only game that is stuck at 60 fps, I forgot to mention that. All though it is still mostly 60 fps at max settings 1080p upscaled to 1440p. Yeah also the UI goes crazy for unlocking the framerate",Neutral
Intel,If only stalker 2 gave us an option to use hardware RT which at times can be easier to run.,Neutral
Intel,"I meant a problem for the consumer. Nvidia absolutely did it on purpose, like you said.  There's a reason I bought a 16gb 9060xt and not a 5060 Ti. The value is worse for the Nvidia card, and that's including AMDs own problems.",Negative
Intel,"I don’t know about overkill tbh. More memory will allow you to use higher quality textures for a long time, if you’re the type to keep your card for a long time. Textures don’t hit gpu utilization much so it’s no problem in that regard. Even 2 GB would make a bit of difference and at least show that Nvidia is gimping vram less on their high end cards.",Neutral
Intel,"1440p window on a 4k display isn't awful. You just have to not mind playing in a window, which ehhhhhhhhhhhhh",Neutral
Intel,"Yeah, it will be fine/smooth ofcourse, its a friggin 5090; the top of the food chain. But i still don't believe we got ourselves here. The history i recall (i know i am old)) was you get a x70 series card if you want to max out all current games, and x80 or x80ti if you wanted to max out future games for 1-3 years and x90 if you just wanted to not worry about fps for 5 years. There were no AI tools to help smooth the pain.   I know all the arguments for DLSS / FG / VRR etc. so not trying to start a word-war here, just saying that buying a 5070 today is not what buying a GTX970 or 2070 was back in the golden days ;)  If one has to still think about sliders in games with a $2000 GPU, we are not doing something right.",Negative
Intel,"Yeah figured something was up lol     At least Lossless scaling exists, not perfect but still a bit smoother",Positive
Intel,Does your 9060 xt work because I had one to Test in. VR Racing and the Amount of frames dropped made any game unplayable. Assetto Corsa had 2000+ in 2 minutes of gameplay. Outside of VR the card worked fine though which is annoying 😅,Negative
Intel,I see. Well most I have ever seen was 12-14gb so guess that 2 gb will give you extra bit of future proofing,Neutral
Intel,Is it really worth it? I know its only 6$ but AFMF 2.1 has been great with no problems. Any situations where you see LS being better? I do also use a rog ally,Positive
Intel,"I am beyond happy with my purchase. I got an XFX Merc card for $420ish and so far it plays everything I've thrown at it on max settings (1080p). I haven't had any issues with a significant amount of dropped frames anywhere.  I don't own VR or Assetto Corsa, so I can't comment on that specifically.",Positive
Intel,"Your flair shows a 3090, man you gonna be maxing out textures for a long time if you keep that fella.   I know most would call me an idiot for this since one is much newer and supports newer technology and such but if I were to get a new GPU and was stuck between a 5070 or 3090(5070 is like only 1 percent faster), I'd get the 3090. 12 GB vs 24 GB is so massive and being honest, I'm the type of person who literally will use something until it stops working and I no doubt would take good advantage of the extra vram in the future.",Neutral
Intel,LS has better interpolation than AFMF or Smooth motion  To me it's worth it tbh especially considering how cheap it is,Positive
Intel,Nice good to hear. I „had“ to bite the bullet and get the 5060Ti with 16gb for 460 euros which is about 100 more than the 9060 XT.,Positive
Intel,"You're absolutely right, I chose 3090 for extra Vram, mostly for running AI models. And I don't have to worry about running out of it, however, I personally don't see much difference between high - very high - ultra, so I could have bought 3080 and have pretty much the exact same results.",Neutral
Intel,This is an amazing and tasteful shade throwing. Imagine reading this a couple months ago and thinking it could be real. Call of Duty is like the vengeful ex girlfriend.,Positive
Intel,This low key makes me want to check out \*\*\* \*\*\*\*\*\*\*,Positive
Intel,"That is insane, imagine banning talk of other games in text chat.  EA 🤝 M$ 🤝🚽",Negative
Intel,Class move,Positive
Intel,Glad it backfired on them. Such dumb uncompetitive practices.,Negative
Intel,EA busy getting that Saudi blood money too.,Neutral
Intel,They thought it said ass raiders?,Negative
Intel,"Hell ya, omw to play ass bitches.",Negative
Intel,"Well yeah, \*\*\* \*\*\*\*\*\*\* is so hardcore, it makes \*\*\*\*\*\* \*\*\*\*\*\* 3 look like \*\*\*\*\*\*\* \*\*\*\*\*\* 2!!!",Negative
Intel,Regional pricing would’ve been nice.,Neutral
Intel,"I will never understand chat censoring in +18 games. If someone hurts your feelings just block him, he can do it without using profanity anyway.",Negative
Intel,So hyped to go topside ugh.,Positive
Intel,"I doubt EA will even survive its recent shift, so let them have one more kick at the can at being the absolute worst in the industry.",Negative
Intel,"I'll be buying \*\*\* \*\*\*\*\*\*\* day one, myself.",Neutral
Intel,"Kinda silly to censor a game I didn't even knew existed until today and yet still didn't catch my eye, another multiplayer ggame in a multiplayer game world, what's different???",Negative
Intel,Wait until the Saudis start making changes,Neutral
Intel,LMFAO,Neutral
Intel,Ass fu... humping. Ass humping sounds like an interesting game.,Positive
Intel,"Out of the loop on the pettiness, is *** ******* developed by former BF and CoD devs?",Neutral
Intel,"Raiders strong together. See you topside soon, gentlemen.",Positive
Intel,[Removed],Neutral
Intel,EA is such a pathetic company.,Negative
Intel,\*\*\* \*\*\*\*\*\*\* about to \*\*\*\*\*\* cod?,Neutral
Intel,"EA + Ubisoft are nearing my personal ""won't buy"" category. Keep it up idiots.",Negative
Intel,That totally should be illegal JFC.,Negative
Intel,The cod killer,Negative
Intel,"Ok, even though I won't pre order it, I really want to support this game simply because EA and Blizzard/Activision are being little bitches",Positive
Intel,Why would COD and Battlefield care about an extraction shooter?,Negative
Intel,When will we learn that censoring only makes people more interested in the things you're trying to hide.,Negative
Intel,Barbara Streisand,Neutral
Intel,Man I can’t wait for Ass Raiders,Positive
Intel,I am pre-ordering *** ******* deluxe,Neutral
Intel,WOO HOO,Positive
Intel,As a pole this was quite confusing at first,Negative
Intel,I'm glad Call of Duty is finally falling off,Positive
Intel,"The leader of Embark used to be Head of Worldwide at EA and definitely quit to do Embark when he got fed up with some type of bullshit within EA management. They're obviously scared of his own company's power to negatively affect EA.  He used to be head of DICE and was basically the ""Battlefield guy"".",Negative
Intel,"Good for them. BR isn’t for me, but I gave it a shot anyway and you could tell they cared to make a very quality game.  The sound design was wicked nice.",Positive
Intel,Arse raiders?,Neutral
Intel,"I don't think either of them have much to worry about. It's a cool game but it's gonna be more niche. New IP, extraction shooter. Yeah. Kinda silly they censored it.",Neutral
Intel,"I may not like the game, but kudos for smart marketing of a petty situation.",Neutral
Intel,"It's a shame it's an extraction shooter, it looks great other than the core gameplay. I have never and probably will never enjoy extraction shooters.",Negative
Intel,How did it backfire,Neutral
Intel,"People get butt hurt over weird shit. Like, who cares.",Negative
Intel,"I get that the EA thing is probably personal, but why would COD censor it?",Neutral
Intel,Why would EA censor Arc Raiders...? What did I miss?,Negative
Intel,Well I rather have teammates who play than chat about other games.,Neutral
Intel,#[removed] [redacted] [censored],Neutral
Intel,thinkin about to preorder *** *******,Neutral
Intel,How is the image censored?,Negative
Intel,jebać pis?,Neutral
Intel,"For the people checking out *** ******* for the first time, there’s a free server test weekend coming up next Friday (17th-19th) before the big launch on the 30th.",Neutral
Intel,"Wait did they real censor those words? Do they censor other games like Call of Duty, and Fortnite?",Neutral
Intel,A wonderful post for Tristar-Septstar,Positive
Intel,Would never have heard of this game if it weren't for this. EA and Activision shooting themselves in the foot yet again.,Negative
Intel,"Ahh, new Concord challenger",Positive
Intel,How much did they pay you to post this?,Negative
Intel,I don’t think Arc Raiders will succeed.,Negative
Intel,"looked interesting right until i spotted the words guaranteed to make a multi-player team game shit... teams of ""up to three"". bzzzt. next. try again. come back when you support a proper squad size.",Negative
Intel,How did it backfire? The game was already popular and i doubt a post with like 5k likes matters at all,Negative
Intel,"You think this is funny, try going to r/battlefield and start typing in your post ""I cancelled my pre-order"" at any time and you'll get greeted with a paragraph explaining how they are not accepting posts about pre-orders at this time. but only if you type that specific phrase.  like, i'm not against battlefield 6 by any means but damn they're aggressive on the corporate censorship.     I discovered this while messing around to create a fake post about being a single dad battlefield vet who cancelled his pre-order without playing the beta for no good reason.",Negative
Intel,https://en.wikipedia.org/wiki/Streisand_effect,Neutral
Intel,"Vengeful, abusive, fucks everything ex-girlfriend.",Negative
Intel,Literally free marketing at this point,Neutral
Intel,"I didnt even know *** ******* was a thing, then i read about this situation and i kinda wanna check it out now, cause apparently it's big enough to scare COD devs/publishers lol",Positive
Intel,Ass Blasters?,Neutral
Intel,"Was gonna wait for the last beta, but honestly, Embark has a ton of goodwill from me due to The Finals, so I just preordered *** ******** because of this",Positive
Intel,[Sex cauldron!?](https://i.imgur.com/g7TcZt9.gif),Neutral
Intel,"I don't give a shit about PvPvE extraction shooters, but I did decide to give The Finals a play for the first time today after remembering these people also made it.  Holy shit. Now that it has some more ""normal"" modes like TDM and what amounts to Safeguard from Black Ops 3, I legit don't know why anyone is even talking about CoD anymore. It's free-to-play, has more in-depth customization, the ability to buy older battlepasses you may have missed, etc., and it feels super good to play. The movement is slick, the destruction is stupid fucking fun, there's even a ridiculous limited time event going on right now where it's 5v5 TDM but the only way to kill people is knocking them off the map, and that's been downright addicting lmao",Positive
Intel,You can say ASS BLASTER on the Internet.,Negative
Intel,Something tells me they're nervous about the competition 😂,Neutral
Intel,"It's likely just a fluke in whatever out of the box filtering tool they have, someone else mentioned it happens in BF6 as well. It's not like they're making their own chat filtering systems",Neutral
Intel,"On that note, at first glance I saw the guys in the poster with no pants",Neutral
Intel,I'm still fucking mad that I can't name my horse in Red Dead Redemption 2 ASS RIPPER in SINGLE PLAYER.   RED FUCKING DEAD S I N G L E P L A Y E R,Negative
Intel,Noo but some internet rando told me im stupid and now im super sad :(,Negative
Intel,10 more days! The concept art they've been posting in their discord is fantastic.,Positive
Intel,I'm going to buy a copy for my group. I already refunded my super deluxe premium preorder for bf6. That'll cover a good bit of it.,Positive
Intel,There’s a server slam in a few days with no limits! You can try for free and see if you like it! Personally I’ve really enjoyed all the tests I’ve gotten to be a part of and if you like third person shooters you might really like Arc. But I’d hardly say it’s for everyone so give it a go!,Positive
Intel,"I actually like playing games like this solo. Check it out a bit and see what you think, you may be pleasantly surprised!",Positive
Intel,"Streisand effect in effect. I genuinely had not heard about this game before today, and now I know who to look at to replace those other series",Neutral
Intel,Patrick Söderlund (former EA and DICE executive),Neutral
Intel,Hey at least it turned in to free marketing.,Neutral
Intel,Nothing will ever kill cod because the people that play it don't have brains. They're completely fine with the same copy paste slop every year.,Negative
Intel,"Ugh, please don't. Every time I see something get declared an ""(game)-killer"" it ends up dying super fast. It's a curse at this point. I'm not into this sort of game, but I'm sure there are people excited about playing it without it getting curst out of existence.",Negative
Intel,lmfao,Neutral
Intel,Try it out on the 17th and then see if preordering is worth,Neutral
Intel,I didn't even know black ops was releasing a new one this month until I saw a video talking about how Arc Raiders has a competitive release date ahead.,Neutral
Intel,"Ah, nothing like some fake info in the morning. This is untrue. The person known as ""Mr. Battlefield"" is former head of Dice Lars Gustavsson and he opened his own studio recently and did not join Embark.",Negative
Intel,"I thought it was an extraction shooter, not a BR?",Neutral
Intel,"Arc raiders is not a BR  And no before another insufferable mf thinking they're smart says ""extraction shooters are just a different flavor of BR"", no it absolutely isnt. The only thing extraction shooters have in common with BRs is that you loot and shoot other players. Extraction shooters represent an entirely different game feel and flow. Pacing during a round happens almost entirely at the agency of the player. The only condition of victory is the one you place for yourself. Also actual meta progression instead of each round starting from 0",Negative
Intel,Sound design is def insanely good,Positive
Intel,I have never enjoyed extraction shooters until I tried Arc Raiders playtest back in april. Try the beta on the 17th tbh,Negative
Intel,The day they announced they changed to an extraction shooter I could no longer care about the game unfortunately.,Negative
Intel,"Know what you mean, I thought I never enjoyed BRs bc all I knew was PUBG. When Apex first came out (first came out, not the shitshow it’s become now), I realised I just needed to play a good one to get it. I feel like arc is going to be the same for a lot of people who are put off by Tarkov",Negative
Intel,"CoD releases after everything, if people are too busy playing Arc and it blows up, then CoD will suffer. If Arc doesn't blow up, they're safe - but god damn you'd wanna do what you can to make sure it doesn't blow up.",Negative
Intel,"The excitement that's been building about Arc since the tech test in April has the potential to be like a coiled spring, since Embark apparently cooked *hard* with this.  Since CoD and BF will rely on MTX to keep going and release in the same month span, if Arc pops off during this release window then it'll have a direct effect on the bottom line of two major cash cows.  If it's greeted with a moderate release then that's a different story, and the big boys are safe. But if it pulls an Apex or Helldivers then that might be another story.  Tbh I think BF6 will still do really well.",Positive
Intel,"Embark censored it themselves for marketing, turning the fact that EA and Cod have censored the name against them.",Negative
Intel,"Oh, please tell us how Arc Raiders is a ""concord challenger"". I think we all would love to hear your opinion on this. Seriously, tell us.",Positive
Intel,![gif](giphy|xTiTnIilwuFFFpf2Cc),Neutral
Intel,What a weird hill to die on,Negative
Intel,"What would you consider ""proper"" squad size?",Neutral
Intel,Honestly only strengthening my position not to buy day 1,Neutral
Intel,Hey at least they let us discuss \*\*\* \*\*\*\*\*\*\* in the comments ... so far ..,Positive
Intel,Most every sub is like this nowadays sadly. The communities are not organic they’re highly “moderated” echo chambers that are mostly corporate owned - of course with very low transparency on that.,Negative
Intel,"I mean... I don't blame the sub.  People are there to talk about Battlefield and they don't need every random joe smoe just coming in and saying the exact same thing about why they're canceling be it the buy out or  some other third thing.     It's not always corporate censoring, just trying to stop post spam",Negative
Intel,">but only if you type that specific phrase.  Seems to not allow any comments or posts with the singular word ""pre-order"" in it. Not just phrases about canceling it.",Negative
Intel,I never liked the BF community tbf,Negative
Intel,"I use old reddit and I had to go over to new reddit to test it, I typed in some other random crap and it said ""Your post has been automatically reported. The language violates our rules, including obfuscated or altered forms. Please keep discussion respectful and inclusive.""  Didn't even actually post anything I was just filling in the title field, oh well I guess I will be banned now? lmao.",Negative
Intel,"I still don’t understand preordering games. It’s digital, it’s not going to go out of stock suddenly. It’s worse in literally every way i can think of compared to buying normally. The only instance where i can understand preordering existing is for something like the silksong release, crashing steam’s servers with how hyped it was.",Negative
Intel,"It's easier to assume all subs that are based on any ip are controlled and moderated by that corporation.  But nobody should pre-order BF6, give it a couple days or weeks. no matter how fun the beta was!",Negative
Intel,Lol it's true hahaha. My sides!,Positive
Intel,"It accepts ""I've scratched my traversal"" just fine...",Neutral
Intel,I wrote “definitely getting a pre-order” and it still gave the removed message,Neutral
Intel,"That just sounds like they're trying to stop the same thing being reposted hundreds of times, although normally it's backed up with a megathread.  I think people forget too often that gaming subs aren't a free for all, they're the domain of a product.",Negative
Intel,Battlefield fans are some of the most insufferable people in the gaming space,Negative
Intel,They did just get majority purchased by a Saudi Arabian backed conglomerate… 🫩,Neutral
Intel,"They sure are agressive, gave a feedback when they added kill cam for BF V. They didn't like it, banned my name so that i was unable to speak in chat, I quess ""KindaG"" as a username was too much for them, had to change name to be able to speak again.",Negative
Intel,"""Hey chat, I am starting to regret my decision to pre-pay for a game that likely won't be any good. Any opinions?""",Negative
Intel,"This is apt and I love it. By making the word an enemy, it grows power. Amazing Segway to this.",Positive
Intel,"Sounds like my past and I’m not looking to see if I can recreate the past. I was neglected, mentally abused, and pulled away from my family. I’m not allowing COD to fill that role that I broke away from, except for the being away from family with battlefield since I want to play like the good old days and not see notifications or worry about a damn thing until I need or get paid to.",Negative
Intel,"uh, Right? It’s like they’re handing out free hype! Can’t wait to see what all the fuss is about!",Positive
Intel,"Never heard of *** ******* before, but now I know because of references to it being censored by Sauditronic Arts and MicroActiBlizz. If you wrote this 10 years ago it would read like shitty parody.",Negative
Intel,Open beta starts on the 17th I believe.,Neutral
Intel,Well there's a free open beta from the 17th-19th Oct so see you there,Positive
Intel,It's crazy that people don't know about it after how crazy the hype was during TT2.,Neutral
Intel,i mean its embark studios so its prolly going to be fire,Positive
Intel,I thought it was ass fuckers.,Negative
Intel,"Embark is basically a company that has the manpower, and decided to be cool.",Positive
Intel,"It’s been a tough few months for the biggest shitheads in gaming, with game pass price increases and the Saudi takeover it made me a lot more confident about my permanent boycott.",Negative
Intel,You think Call of Duty is nervous about Arc Raiders but not Battlefield?,Neutral
Intel,"With how good the Finals is, i have high hopes for this one",Positive
Intel,Arc Raiders is not competition for either of these games,Negative
Intel,They're definitely the ones controlling the blacklisting rules of their chat system even if they didn't make the system itself though,Neutral
Intel,Yessss server slam is going to be so fun! I saw that Embark is absolutely cooking with those concepts!,Positive
Intel,"Ah, thanks.",Positive
Intel,"He may not have been the key person in terms of shaping the games, but he led DICE in the critical period when they integrated with EA and ultimately championed that franchise to hell and back under his Head of Worldwide title.",Positive
Intel,It is.,Neutral
Intel,"It is, I just don't think a lot of older players know the difference yet, since extraction shooters are so rare",Neutral
Intel,"Yea it is, they're marketing it as an ""extraction adventure"" though, and given the playtest gameplay I can see why.  I play their previous game, the finals, and love it so I have a feeling they'll deliver.",Positive
Intel,So like BR but even worse because now when you lose to a cheater you lose progress from previous games too?,Negative
Intel,"Yeah, you're right, had a brain fart",Neutral
Intel,"Some extraction shooters do have global objectives, like Hunt Showdown. You are free not to pursue them but doing so is very inefficient.",Negative
Intel,"I'll give it a shot because Embark are geniunely a great dev team, I poured a lot of hours into The Finals and while I am not massively impressed by the state that game is in now I still appreciate the dev team's commitment to a fair, completely free competitive game. They have integrity, which is rare.",Positive
Intel,"I've played Tarkov a few times, tried liking it many times. Every now and then I return, I have a good game and you get that boost of dopamine, followed by a heavy crash as you lose all your gear in the next game (and the game after, and after and after).   I realised not that long ago that I am objectively bad at video games, so I primarily stick to casual shooters or singleplayer games these days. I'll give this one a try, only because it's Embark, but I feel I will come to the same conclusion.",Positive
Intel,"You're blinded by nostalgia, if Season 0 Apex is your example of the game being at its best. It was fun because the game was new and everyone was a bot. By today's standards, it wasn't that good of a good game.  I realised this (and so did most Apex players) alot more when they released that season 0 throwback mode a year or two ago. It had season 0 Kings Canyon, season 0 weapon tuning, and season 0-2 characters and how they were balanced at the time. Even the OG players that obsessed about season 0 barely played it for a week. It's why Respawn haven't bothered releasing it again.  The real shit show was when finding loot was a bigger pain than in any BR game ever (remember all the memes about it), even worse hit reg than now (lol), way worse visual clutter when shooting, and every gun being aggressively overpowered and having huge mag sizes. Also an incredibly boring character meta (Wraith and Pathfinder. And later Wraith, Path and Wattsson).",Negative
Intel,You missed that Embark is old DICE devs.,Neutral
Intel,Four. of course. [Why games choose four as the magic multiplayer number](https://www.polygon.com/22722365/four-multiplayer-developer-explanation-grounded-sea-of-thieves-minecraft-dungeons/),Neutral
Intel,"Battlefield already has a track record of disaster releases, not sure the buyout is going to help at all",Negative
Intel,"there's a known issue of their anti-cheat just bricking my specific motherboard if we turn on the setting they require for it and they just like....don't care.   mentioning it at all gets you a bunch of hate and downvotes and ""skill issue"" dipshits.",Negative
Intel,"Ok, one person won't buy an overpriced Dice bugfest, with bugs present since BC2. That's a start.   As of now BF6 is the best selling pre-order, 20% COD players didn't buy COD after seeing BF6 gameplay. They may have made the first playable BF since BF4, but I bet we will experience most of DICE Advanced Bug Creating System called Frostbite and Dice-sponsored YouTube boys won't show them to us... until they can monetise it of course.",Neutral
Intel,Don't Occam's razor my outage!,Negative
Intel,I also dont need every joe smoe to tell me they pre-ordered and uninstalled 2024 with the same image of a finished battlepass.,Negative
Intel,"""The corporation that undoubtably controls the subreddit and is censoring your posts isn't always engaging in corporate censorship, guys."" isn't a very well reasoned argument.  and I'd prefer active moderation with the intent of weeding out bot accounts that aren't organic discussion, instead of that corporate censorship.",Negative
Intel,And yet almost every post I see from the sub is about people's pre-orders,Neutral
Intel,It probably depends on what you put in the title. Like if you’re asking for CSAM or calling for assassinations? Yeah that probably should be auto reported,Negative
Intel,Seems fun if you’re into that kind of game. Loved the aesthetic but gameplay is not really my cup of tea. Play test also seemed to be promising,Positive
Intel,Reddit is full of casuals. The people who would actually play this game for more than 1 hour a week know about it ;),Neutral
Intel,"100% dude. To be honest, between Arc, HD2, and Hunger (not yet released) I don't think I'll have any time for AAAslop.  Steam deck for good indie games ofc 👌🏻",Negative
Intel,"It's not just about what you're nervous about - it's what you think you can get away with.  They probably believed that BF is too big, it'd be very noticeable. But Arc Raiders is a new player on the block that has serious potential - if you can stifle them at launch in any way you can, they may not recover from it.  Think of Titanfall 2 - great game, cult classic, shite release date window and got hamstrung at launch.  Arc raiders have a risky release date here, and if I were BF/CoD I'd be doing what I can to muffle them.  But instead I'm a gamer, so I broke my no preorder rule to get it lol.  Edit: this is of course just my spitballed take.",Negative
Intel,It's wild how much people think anyone is scared of Arc Raiders. It's a completely different genre from Battlefield or Call of Duty. It's like saying either of those would be scared of Escape of Tarkov (spoiler: they're not).,Negative
Intel,"Yes, I have no idea what Arc Raiders is like cause I've never played it but if it's getting popular and it's gameplay feels too similar to CoD the execs might not be happy because CoD and Battlefield are different enough that they've been able to coexist for years. The last thing CoD needs rn is an outsider coming in and stealing their thinder",Negative
Intel,Lowkey yes. Battlefield will be a fluke lasting a month or 2. Arc Raiders has the chance to be the next big battle royale and fuck their warzone up the arse,Negative
Intel,I'm 36 lol,Neutral
Intel,"I’d honestly say you can absolutely play Arc as a casual shooter, as someone who has been in multiple tests I can say it’s surprisingly good at being “casual” compared to tarkov for example… or really any shooter at all",Positive
Intel,"If you even remotely found Tarkov to be fun at times, you'll be blown away by Arc",Positive
Intel,"Well I mean my point isn’t that apex was good, just that I liked it more than pubg",Positive
Intel,"As another player in the exact same boat, those are tuning issues. I remember a lot of that too, issues with certain weapons and characters feeling underwhelming, etc. The point is the foundation was good though. Any great game can have issues with balancing in a specific time, especially when it is brand new. And like you said, while the community did recognize a lot of those issues even at the time, everyone was so butts it was still fun to play, and the potential for it to be great was obvious.",Negative
Intel,I see.,Neutral
Intel,"None of the games mentioned at the top of the article are pvp fps/tps games though. Trios always played better in the games that have more than 2 teams in a match, so extraction shooters and BR games.",Neutral
Intel,"It's Saudi Arabia, they were never going to help... At least not in the way we would consider helping.  It's like when your 5yr old kid wants to ""help"" you do the dishes",Negative
Intel,> there's a known issue of their anti-cheat just bricking my specific motherboard if we turn on the setting they require for it   Care to elaborate?,Negative
Intel,Turning on Secure Boot can't brick your motherboard.,Neutral
Intel,"Yeah, I have been seeing that. This is why preordering is a bane on the consumer because they do shit like this and keep getting away with half baked or half assed games that should work on release. Sorry that not everyone is a pc wizard that knows how to enable and disable shit in the bios on a whim. I am getting there, but I am far behind, nor do I want to know software engineering to play a game.",Negative
Intel,"Its not worth it. Its literally just CoD now, wait till 4 goes on sale and get more game for less money",Negative
Intel,"I've boycotted EA for years. All the recent discourse around the new game has me less than thrilled to think about EAs actual intentions with the game going forward.  I enjoyed what I played and think it has great potential to be an amazing return to form, but not a chance I'm sinking 80 shekels, only to stop playing after Christmas because of some ""who could have seen that coming"" bullshit.",Negative
Intel,but muh ogghams razer explains everyting????,Neutral
Intel,"Idk I'm not subbed there. No clue how long the rule has been in place, but I testing making a post and a comment and with both it pops up a warning for just using the term pre-order.",Negative
Intel,Heard of hunger from riloe and it looks like a fantastic game,Positive
Intel,But that makes them banning the mention of their game even more weird right? If what you say is true then what possible reason could they have for banning the game's mention?,Negative
Intel,Do you know how badly I want another DMZ just so my old timer gamer group will play an extraction shooter? I tried getting them into tarkov and luckily it was on spt cause it didn't last 2 raids,Positive
Intel,It’s not anything like CoD.,Negative
Intel,"It's not like Cod, it's just a lot more fun than Cod",Positive
Intel,How could a game that’s not a Battle Royale be the next big Battle Royale?,Negative
Intel,Brother. That makes us older gamers. 🙏,Positive
Intel,"Well I was more talking about the guy you were replying to, who mixed it up with a BR, but anyways yeah that age is pretty close to dinosaur times",Neutral
Intel,Literally from the stone age.,Neutral
Intel,"It's not really Saudi Arabia when an American is running it. More of an offshore company so they get away with shit that would be illegal in North America  Edit: OK fine that isnt completely right, but I'm not wrong either. EA was bought by Saudi Arabia's Public Investment Fund (PIF)(run by the Saudi oligarchs), Silver Lake (American private equity firm), and Affinity Partners (American investment firm) run by Jared Kushner, Trump's son-in-law.   These 3 companies will have 100% ownership of EA. They **WILL** be enshitifying it.",Negative
Intel,https://imgur.com/oUI1zI2  https://imgur.com/H6Z2fao  can't link to other subreddits so had to screenshot only some of the content  its ok to not comment on things you don't know about,Negative
Intel,"dude the ""solution"" is something like deliberately bricking your PC, then shorting something on your mobo to discharge a battery or something??? like??  to play a game??? are you fucking serious?   I'm not some tech illiterate boomer for not wanting to do that shit, yet that's what they pretend you MUST BE if you can't even do that ""simple task""   fuck that game and fuck that community.",Negative
Intel,"I mean, I'm never pre-ordering an EA game again, but I played the beta and BF6 is nothing like CoD and it's closer to BF3/4 it ever was",Negative
Intel,"Of course. I still haven't spent more than $12 on an EA game after 15 years since I played BC2 for the 1st time. It's just really not worth it.  Had huge hopes in WW3, but russian financing and overall problems buried this potential really deep. Sadly, because they did many things better and it was a small studio, so all that big corp bs was eliminated. Now we're left with basically one big title in this FPS tier.",Negative
Intel,Yea dude his video got me so keen for it. You might like this German man with lots of info:  https://youtu.be/l1ltkoD5Dck?si=wneP3Ixdxa0oArxK,Positive
Intel,Its an extractjon shooter whatever its almost the same thing. Drop in find loot kill people on a huge map and get out.,Negative
Intel,Not really,Neutral
Intel,Listen here you little shit,Negative
Intel,![gif](giphy|kkyYV0WYLnSVy|downsized),Neutral
Intel,Fair enough that's what i get for not reading beyond the headlines lol. I mean I knew it wasn't the Saudi government or anything like that but I figured it would be a Saudi company. Still though I don't think they'll be able to save EA,Negative
Intel,It is as much of a game's fault as if someone kills someone over a match.,Negative
Intel,"So, to be clear, your evidence of me not knowing what I'm talking about is to show me jpegs of:  > Something that literally disproves your bricking assertion  and  >> unverified, anecdotal evidence  Even in the ""haha gotcha"" first jpeg the solution to your """"""bricked"""""" motherboard is to... remove the CMOS battery.   Do you know what 'bricked' means? It means you can't fix it. Removing a CMOS battery is about as fucking hard has unplugging a USB drive, and hey presto, the motherboard is reset back to default.   If turning on a feature that's been in UEFIs for around **a fucking decade** was capable of killing motherboards, we would know about it by now.  Edit: Further googling has shown that Aorus motherboards are, in fact, massive pieces of shit. Secure Boot isn't at fault, Gigabyte is. So, again, Secure Boot is *not* killing motherboards.",Negative
Intel,"I appreciate this side because I didn’t get past seeing that this was a thing. I agree with you though. That is crazy that a “fix” is a hindrance. I’ll play on my Xbox for now and once they figure their shit out, I’ll grab it for pc. I am not happy to do this, but it is easier than being stressed out by playing a game that I have been excited for.",Negative
Intel,"They don't want to save EA, they want to milk it dry.",Negative
Intel,"""it's a soft brick not a real brick so I'm right""  https://imgur.com/8KFtDn6  keep licking that EA boot. fucking embarrassing.",Negative
Intel,"Right, because pointing out that something that *has never happened* and *cannot happen* makes me an EA shill? Please direct me to my pro-EA comment in this thread.   I'll fucking wait.   No mate, what's fucking embarrassing is making a bullshit claim, having it disproven because what you're describing cannot happen, showing evidence that outright disproves your own assertion, and then resorting to name calling when called out on it.",Negative
Intel,"that's a lot of words for ""it's a soft brick not a real brick so I'm right""  fucking embarrassing",Negative
Intel,Rip gpu market,Negative
Intel,https://preview.redd.it/b50eap3fmzpf1.jpeg?width=1078&format=pjpg&auto=webp&s=aebc215f68b6c6444a8e247dfed48d9eed711dd0,Neutral
Intel,"That’s the last fucking thing I want, an Intel cpu with an integrated Nvidia gpu. Horrible.",Negative
Intel,"[Intel and AMD have done this in a NUC before.](https://www.tomshardware.com/reviews/intel-hades-canyon-nuc-vr,5536.html)  Edit: My educated guess is that this comes from Nvidia's ARM APU, and ARM Windows just isn't there yet. So, to move it to an x86 platform Intel would be the logical choice to partner with to move the APU to x86 Windows.",Neutral
Intel,All I know is we gonna lose,Negative
Intel,I have been saying for quite some time Intel will abandon Arc. They abandoned the GPU market in the past.  It is past embarrassing at this point what Apple offers with the integrated GPU in the M series of cpus and what Intel delivers.,Negative
Intel,Let Intel Suffer for all the BS they did with their CPU division while they were a Monopoly and even after they declined still continued their BS  But i like Intel GPU division,Negative
Intel,Even before this announcement Intel Arc faced an uncertain future. Even their most successful GPU the B580 at the very least had razor thin margins if not outright being a loss leader.  Intel simply ran out of both time and money and they can't keep bleeding money on their GPU division with the company's current situation. If Intel actually managed to execute and follow their initial roadmap and release Alchemist GPUs during the COVID pandemic we'd probably be looking at a very different Intel GPU division but here we are now.,Negative
Intel,I don’t necessarily think that it’s that dramatic. The Arc GPUs are aimed at a market that Nvidia no longer competes in (the budget GPU space). If anything you’d think Nvidia would be more supportive of the Arc stuff getting off the ground because it means they can dedicate more fab space to producing the higher end stuff that’s in demand for AI and industry use and commands better margins.,Neutral
Intel,I wonder if this is specifically to go after the handheld pc gaming space where amd basically has a monopoly.,Neutral
Intel,"This is great for people who just want a gaming system, no AI. Cheap motherboards with decent integrated graphics would be great for a lower income. (Most of the gamers I know)",Positive
Intel,Someone is cashing out for both INTC and NVDA.  Intel traded a shitty incompetent CEO for a conniving one.,Negative
Intel,I truly hope Intel won't drop their GPU division.,Negative
Intel,"Such a weird thing going on in this thread. Did people forget when AMD acquired ATI it was to make a complete package APUs and for both respective companies to compete with Intel and Nvidia.   Now Nvidia probably wants to do same thing in the CPU market and intel gets the benefits of being with Nvidia in the GPU market.  Only reason it did not happen sooner was because Intel was still king marketwise then.  Intel could have easily acquired Nvidia in 2006 when AMD acquired ATI, or even in 2011 when AMD stopped using ATI branding.",Neutral
Intel,NOOOOOOOOOOOOOOOOOOOO!   Whatever,Negative
Intel,:(,Negative
Intel,Low end vs high end. I think it will be fine.,Positive
Intel,If only intel aggressively sell their GPU in third world countries but no they only focussed in America where except 10 redditors everybody buys nvidia,Negative
Intel,Should have done it with amd instead of this dying pig,Negative
Intel,Arc was never even a player.,Negative
Intel,Then surely no one will buy it.... right?,Negative
Intel,"I think it would be pretty sweet, if they could cook up something like the apples M series.",Positive
Intel,Uh...why?  Nvidia have consistently had the most power efficient GPUs on the market for years now. That technology in an APU? That's promising stuff.  It wouldn't surprise me if we start to see 4050/4060 performance on a single chip. Imagine what that means for handhelds.,Positive
Intel,"Speak for yourself, that sounds fucking great.",Positive
Intel,support ink fearless connect ask divide oil provide enter tie   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"its for ai accelerators with built in CPUs, not apus",Neutral
Intel,Unfortunately a majority of gamers/pc users in general don't give two shits about the specifics of their hardware.,Negative
Intel,"Why? AMD makes inferior products in every way compared to Nvidia, except for GPU power connectors.",Negative
Intel,"kinda, Kaby Lake G wans't really an APU, it was just a CPU and a GPU on the same package.",Neutral
Intel,Imagine if Apple suddenly turned round and decided to make a Mac for gaming 🤣,Neutral
Intel,"Companies aren't your friends, AMD being on top means they can pull the exact same shit that Intel did in the past.",Negative
Intel,"How would they suffer? Intel isn't a living entity, and the executives that led the downfall of Intel left, who is gonna suffer except intel's employees and us?",Negative
Intel,"> But i like Intel GPU division    The problem with the Intel GPUs is they are clearly struggling with the technology. The B580 needs a much bigger die than equivalent Nvidia cards, meaning it must be expensive for Intel to make the chips and uncomfortable to sell them at such a low price.   That might be ok initially to get a foot in the door for their GPU brand, but if they can't get the efficiency up then from a business point of view they just can't continue it.",Negative
Intel,Nvidia still sells gt710 and and rtx3050 for the low end,Neutral
Intel,"Make no mistake, this will be marketed for AI before gaming",Neutral
Intel,"Yes but AMD didn't have a GPU division when they bought ATI last I checked, what became of ATI is AMD's Radeon division, CPU or GPU both remained with two major players. The problem here have here is that Intel IS a player in the GPU space, they are the opportunity to have a 3rd company for extra competition but Arc is still in a vulnerable stage where it could easily get axed if it isn't bringing in more money than what they spend. Nvidia investing and putting in RTX iGPUs is the sort of thing that could devalue Arc iGPU investment.",Neutral
Intel,Maybe they want competition. Unlike NVIDIA now wants to have 100% share to everything. Monoply at its finest,Neutral
Intel,"People may not like it but you're right, they essentially had a 0% market share the quarter following the release of their Battlemage GPU's",Neutral
Intel,"whats with the downvotes lol, dudes right, fellas time to step outside yer echo-chamber & touch some grass",Neutral
Intel,They could have been.,Neutral
Intel,Already sold out,Negative
Intel,"Woops, it's in every laptop ever.",Neutral
Intel,"Intel's mobile CPUs are alright, maybe this could be a responsive to AMD's gigantic APU.",Positive
Intel,Nvidtel: best we can do is Kaby Lake-G.,Neutral
Intel,"Never gonna happen. NVIDIA will put that kind of effort into their own CPU but not an Intel one. This is doomed from the start, we’ve seen it before several times when big tech companies “collaborate” and the product just languishes and dies. The PowerPC CPU architecture and OS/2 leap to mind.",Negative
Intel,"AMD fanboy - thats why. Doesn't like the ""enemies"" joining forces.",Negative
Intel,"That's roughly the power on the amd ai 395+ max whatever the fuck that chip is called. It's mostly ai oriented, not gaming, but the apu has 4060 or above performance in most games.  The real question is whether nvidia would make such an investment for handheld devices which they dont seem to care for all that much, or mabw they just want their foot in the ai tablet market where amd has taken the lead.",Neutral
Intel,"I was obviously speaking for myself. In English, we use the “I” to refer to ourselves. I would have used the word “you” if I were talking about you.",Neutral
Intel,Lol why? Intel sucks at making CPUs now.,Negative
Intel,"It's both, [per Nvidia and Intel's own joint press release](https://nvidianews.nvidia.com/news/nvidia-and-intel-to-develop-ai-infrastructure-and-personal-computing-products?ncid=so-twit-672238):  >For data centers, Intel will build NVIDIA-custom x86 CPUs that NVIDIA will integrate into its AI infrastructure platforms and offer to the market.  >**For personal computing, Intel will build and offer to the market x86 system-on-chips (SOCs) that integrate NVIDIA RTX GPU chiplets. These new x86 RTX SOCs will power a wide range of PCs that demand integration of world-class CPUs and GPUs.**  Which, personally, I think would be an interesting product line.",Neutral
Intel,He said with his 3080 Ti that draws 100 W more than a 6900 XT for the same performance,Neutral
Intel,"Yeah, I was just saying Intel has worked with direct competitors before.",Neutral
Intel,"Their computers can already technically game, just at worse performance than expected. Most developers don't want to develop their games to include direct support for Metal in addition to Vulkan, Directx and/or whatever system consoles use, and Apple doesn't want to directly open their OS to Vulkan or Directx. Everything needs to go through metal in some way.",Negative
Intel,"True like now that they are at the Top where are Ryzen 3? Where are CPUs for the Budget Gamers? Im have money so i dont need ryzen 3.  But still Intel deserves what they did and even AMD isnt as dumb to do this:  \-Force Mobo Upgrade every CPU Gen   \-Limit Users too 4 Cores 8 Threads   \-Deliver minimal Generational Upgradea while doubling the Price   \-Rebrand Old CPUs with new names and increase Power draw so much it destroys itself   \-Limit/Exclude Tech like ECC, PCI-E Lanes for CPUs under 500€   \-Israel",Neutral
Intel,"Yup, I'm someone with an all AMD build, and Intel going down the shitter and the possibility of Arc being axed is not something I'm happy about at all.   Even if I was one of those weirdos with diehard brand loyalty and didn't care about a monopoly/duopoly being solidified, it's still going to suck for me as an RDNA3 user because Intel is still the only only making an (officially supported) upscaler for my card that isn't completely dogshit.  There's still no guarantee for official FSR 4 support on RDNA 3, and if that never happens and XeSS gets axed, I'll effectively be stuck with the god awful FSR 3 for any multiplayer games I can't use Optiscaler on.",Negative
Intel,like when was the last time AMD increased the core count on their desktop lineup y.y  they kept R5/R7/R9 at the same core counts since introduction,Neutral
Intel,They already are. zen 3-4's uplift came almost all from node change,Neutral
Intel,"I know American companies like to shit the bed, does Chinese tech companies have the same history?",Negative
Intel,tell me how Intels Numbers are doing. Why do they need the state too pump money into them now?,Neutral
Intel,They made enough profit while delivering the most minimal Generational upgrade back when they were the Monopoly. They should have invested that money into the GPUs instead of a Paycheck for the CEO and A Paycheck for Israel and building a Plant in Israel.  Let them suffer,Negative
Intel,Once those are gone they’re gone.  Nvidia won’t waste TSMCs limited wafer capacity on budget chips at this point.,Neutral
Intel,Yeah Nvidia needs a cpu for their AI servers that’s better than their own arm chips. That’s probably what this is about,Neutral
Intel,Why would any company want competition?  Competition is only beneficial for the consumer.,Negative
Intel,"They were actually eating some of AMD's shares, Nvidia didn't budge",Neutral
Intel,Site crashed,Negative
Intel,Already at 37% user base in steam charts topping any and all AMD gpus somehow.,Positive
Intel,14nm++++++++++++++++++,Neutral
Intel,I've been calling those builds (IRL) as Intel-Vidia,Neutral
Intel,![gif](giphy|fngnOeui0oSdSwjZ9K|downsized)  Infidel,Neutral
Intel,"""I"" could tell you were speaking for yourself. In English, ""speak for yourself"" is an idiomatic expression meaning that the person  replying disagrees with the prior statement, typically appended with their point of view.  EDIT: OP of the comment can't take a joke and delete everything.",Neutral
Intel,"Yes, they have done for a few years. It all started with the 14+++++++ fiasco. Their fabs are in a bad way and their CPUs are not where they should be.",Negative
Intel,"Who really cares about power consumption? It doesn’t matter unless you need to upgrade your power supply. Especially if you have enough money to buy a high end gpu, an extra 100 W during the few hours a day max that you’re gaming isn’t going to matter at all. 0.1 kWh * 3 hours * $0.16 = $0.048 extra per day. Oh no!!!  Also, even if the 6900 XT had better performance, I wouldn’t have bought it still because of the extras that Nvidia has. Things like Nvidia broadcast, CUDA support, and DLSS, just to name a few.",Negative
Intel,">  and Apple doesn't want to directly open their OS to Vulkan or Directx. Everything needs to go through metal in some way.  Without buying out MS apple cant support DX. (and even if they did the flavor of DX used by PC titles is not compatible with apples GPUs).  And while apple could support VK, remember VK is not HW agnostic, a PC VK backend (the small number to titles that have these) will be written expliclty to target AMD/NV GPUs and would require rather large changes to run well (or even at all) on a VK driver for apples GPUs on macOS.    The work needed to add a metal backend to a modern engine (if it does not already have one... most do) is not that large.  All modern engines are written to be mutli backend enabled.  The amount of a games engine that directly calls a graphics apis is intently tiny since we need this to run very fast within the Redner loop, idealizing keeping the entire loop within L1 Cpu cache.     Adding metal support to an engine is not were the work is.  All the other changes, user input, file system, Audio, networking, digital signatures, color spaces (Mac users expect proper HDR)... this is all way more work than adding metal.    (also most engines already have metal backends).     from a graphics side of things there is a LOT of work but this work is API agnostic and relates to making changes to target the HW (you would need to do this regardless of API as all the modern apis want to remove runtime overhead of the driver reputedly making these optimizations over and over again on each frame).",Negative
Intel,barely anyone bought the ryzen 3s when they were a thing. budget gamers just bought last gen ryzen 5s because of the higher core/thread counts and better performance.,Neutral
Intel,"Both are publicly traded companies and their sole purpose is pumping up share price up as high as it will go. AMD will definitely take the foot off the gas with Intel continuing to crumble, keep sales up and fatten margins. Intel had to push power draw to try and keep up on performance as they were stuck on an inferior node. Everything else is well on the table for AMD until another company challenges their lead.",Neutral
Intel,We dont need r3s for budget cpus when older amd r5s are just as cheap with similar performance.,Neutral
Intel,"You must not remembered AMD’s socket A, socket 754, socket 939, and socket 940…  And AMD was limiting users to dual cores before Intel came out with C2Q, and selling dual cores at ridiculous prices before Conroe.  If you’re looking for morally better companies, AMD ain’t it.",Negative
Intel,what did intel do in Israel?,Neutral
Intel,didn't intel just come out with the Core Ultra 110 which is literally just a 10th gen cpu that they are trying to trick us into thinking is new?,Negative
Intel,Don't need a new ryzen 3 model when an older gen r5 is dirt cheap.  And who is in the market for a €70 cpu on a €200 motherboard? I like AMD's approach of keeping AM4 for systems with low requirements and AM5 for the high-end stuff.,Neutral
Intel,Mayhaps they'll open source XeSS if/when that happens,Neutral
Intel,"Nope, you’re thinking of the highly sought after 4 and 5 nm TSMC fabs. The 3050 is build on Samsung 8nm and gt710 is build on 28nm.  Which does not have high demand and thus is also much cheaper to manufacture on",Neutral
Intel,"To deliver that price/performance they had to sell at cost, the B580's die is 20% bigger than a 5070 and Intel is selling them for less than half the price. Heck, the thing competes with a 4060 and it's 2x the size.  As long as they can't get even remotely competitive on die size they'll be stuck in this situation and personally I'd rather have them spending money on figuring their shit out on the CPU side to avoid  a monopoly instead of selling at cost low end GPUs than don't work well on low end CPUs.",Negative
Intel,I like that!,Positive
Intel,"He didn't delete anything, he just blocked you. Comments will be shown as deleted to you.",Negative
Intel,Yeah idioms like that are for speaking in person not typewritten on social media. It’s a meme to make that mistake.,Negative
Intel,"> Who really cares about power consumption?  Hey, you're the one who said ""*every* way"" 😎  > Things like Nvidia broadcast, CUDA support, and DLSS, just to name a few.  DLSS for sure, it's black magic and it might even be worth the $200 premium over the 6900 XT to some. I have a 4080 that I got used, but even at MSRP vs MSRP I would have probably picked it over a 7900 XTX just for DLSS.  The other stuff though, do you actually use it?  ^((also as someone who's owned a 4080 and a 6800 XT within the last 2 years, AMD drivers and auxiliary software are *way* better... but DLSS is so freaking good man)^)",Neutral
Intel,"Oh i did not know this. I thought most of them were normies who had no idea what, what meant so they just bought the cheapest one that works",Neutral
Intel,I do love AliExpress,Positive
Intel,AMD has nothing to do with Israel but Intel does. So already a MASSIV plus for AMD,Neutral
Intel,Yeah i didnt think people would do this. But this is good,Positive
Intel,"I remember having the motherboards exploding, just in time for the cpu upgrade anyway during the early 64 bit transition. Damn those crappy capacitors!",Negative
Intel,difference is AMD has nothing to do with Israel,Neutral
Intel,"many things. Including investing in Israel with and Plant and manufacturing Chips, GPS and other stuff for B\*mbs",Neutral
Intel,They did that more than once. 7th Gen Intel and 14th Gen Intel are the once i can quickly think of,Neutral
Intel,"You do know you can get AM5 Motherboard for as low as 70€ new? And a 8400F for 120. Not even 200€ for AM5 Platform.   Thats the dumbest take i have ever heard. ""AM5 only for High end stuff""",Negative
Intel,They’re no longer being manufactured which is the point.  All 40 and 50 series GPUs are manufactured by TSMC and they won’t waste TSMC capacity on budget GPUs with low margins.  It remains to be seen what will happen with Nvidia’s recent investment in Intel.,Neutral
Intel,"272 mm² B580   263 mm² 5070   According to tech power up.   Die size is not really that important at this stage, they need a better architecture and that will lead to better performance for the die size.   Let's also not pretend like NVIDIA is selling at thin margins, they're profiteering. And were is your source they sell at a loss?",Neutral
Intel,That's a bargain!,Positive
Intel,yep. amd has pretty much replaced the ryzen 3 tier with their last gen ryzen 5 cpus because of how cost effective they are.,Positive
Intel,AMD has an R&D facility in Israel. Every big company has something to do with Israel.,Neutral
Intel,AMD sold their fabs to a Saudi consortium. I’m sure that’s way better.,Neutral
Intel,"I can’t find any sources online saying the 3050 is discontinued. They discontinued the 8gb and replaced it with a 5th revision 6GB some time ago? which appears to be current and still manufactured. There’s never been a time in the last 20 years I’ve been watching the gpu market Nvidia has not filled the lowest tier with older generations of cards. If there’s a shortage of manufacturing capacity at the bleeding edge, they can just use older manufacturing processes so there’s no reason not to fill lower tiers if people are buying them. This is what AMD does for their lowest tier CPUs, they use older manufacturing processes, like 7nm and 14nm",Neutral
Intel,"Die size is important, it's the main driver of the cost. Also the point is not the size itself, it's the fact that it performs like a 150mm die from the competition so it has to be priced accordingly.  Also I never said they're selling at a loss, I said at cost. It's sensible to assume that at 250$ the B580 is at cost or at a really thin margin, the VRAM alone is 80-100$.",Neutral
Intel,Now that Israel attacked Qatar they have been talking with the Middle East talking about arming themselfs against Israel. The Saudis have joined that talk. Things are changing,Neutral
Intel,"Sure but Intel isn't losing because of die size, they're losing because theyre 4 years behind AMD and Intel   It's not sensible because we have nearly zero insider info on the modern silicon market operates.",Negative
Intel,"What a disgusting build, I love it",Negative
Intel,the content we crave,Positive
Intel,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",Neutral
Intel,What GPU are you using in your build?  All of them,Neutral
Intel,you're one hell of a doctor. mad setup!,Positive
Intel,The amount of blaspheming on display is worthy of praise.,Negative
Intel,Brother collecting them like infinity stones lmao,Neutral
Intel,I'm sure those GPUs fight each others at night,Negative
Intel,Bro unlocked the forbidden RGB gpus combo,Neutral
Intel,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,Neutral
Intel,What the fuck,Negative
Intel,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,Positive
Intel,Yuck,Negative
Intel,Wait until you discover lossless scaling,Neutral
Intel,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,Neutral
Intel,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",Negative
Intel,Now you just need to buy one of those ARM workstations to get the quad setup,Neutral
Intel,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,Positive
Intel,Love it lol. How do the fucking drivers work? Haha,Positive
Intel,What an amazing build,Positive
Intel,wtf is that build man xdd bro collected all the infinity stones of gpu world.,Negative
Intel,You’re a psychopath. I love it,Negative
Intel,This gpu looks clean asf😭,Positive
Intel,The only setup where RGB gives more performance. :D,Positive
Intel,Now you need a dual cpu mobo.,Neutral
Intel,Placona! I've been happy with a 6700xt for years.,Positive
Intel,absolute cinema,Positive
Intel,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",Neutral
Intel,"Brawndo has electrolytes, that's what plants crave!",Positive
Intel,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",Positive
Intel,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",Neutral
Intel,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",Neutral
Intel,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",Neutral
Intel,Team RGB,Neutral
Intel,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",Neutral
Intel,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",Negative
Intel,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",Positive
Intel,"OpenCL works on all of them at once, and is just as fast as CUDA!",Positive
Intel,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",Positive
Intel,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,Neutral
Intel,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",Positive
Intel,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",Neutral
Intel,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),Negative
Intel,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,Neutral
Intel,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,Neutral
Intel,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",Negative
Intel,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",Positive
Intel,Thank you so much for the very detailed response!,Positive
Intel,Well worth it!,Positive
Intel,Thank you my man!! Looking forward to run some tests once I get home.,Positive
Intel,That's awesome!,Positive
Intel,"Yes, but SLI is a bad description for it.",Negative
Intel,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",Neutral
Intel,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",Negative
Intel,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",Negative
Intel,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",Neutral
Intel,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",Neutral
Intel,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",Neutral
Intel,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",Neutral
Intel,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",Negative
Intel,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",Positive
Intel,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",Neutral
Intel,Why are you connecting the monitor to the gpu and not the mobo?,Neutral
Intel,"👍   thanks for the info, this'll definitely come in handy eventually.",Positive
Intel,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,Neutral
Intel,No worries mate. Good luck,Positive
Intel,"For some reason I switched up, connecting to the gpu is the way to go. I derped",Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,It's alive. Rejoice.,Positive
Intel,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",Neutral
Intel,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",Negative
Intel,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,Positive
Intel,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,Neutral
Intel,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,Negative
Intel,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,Positive
Intel,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",Neutral
Intel,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",Negative
Intel,I'm fairly sure they use dxvk for d3d9 to 11.,Neutral
Intel,Could just be a cache issue,Neutral
Intel,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,Neutral
Intel,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,Positive
Intel,"According to the graphs, AMD has slightly less overhead than NVIDIA.",Neutral
Intel,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",Negative
Intel,"Lowest with DX11 and older, but not with the newer APIs",Neutral
Intel,And when is the last time HUB did a dedicated video showing the improvement in overhead?,Neutral
Intel,or it's just a cache/memory access issue,Neutral
Intel,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",Neutral
Intel,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",Negative
Intel,"Intel uses software translation for DX11 and lower, so it does matter for them.",Neutral
Intel,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",Neutral
Intel,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",Negative
Intel,That's not true. Intel's issue is being too verbose in commands/calls.,Negative
Intel,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",Negative
Intel,HUB used DX12 games that also showed the issue.  It's something else.,Negative
Intel,"The comment to which I am replying is talking about nVidia, not Intel.",Neutral
Intel,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",Negative
Intel,That's actually... just worse news.,Negative
Intel,I always dreamt of the day APUs become power houses.,Neutral
Intel,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",Neutral
Intel,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",Negative
Intel,Damn Why is AMD even involved in iGPU,Negative
Intel,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",Positive
Intel,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",Positive
Intel,almost there,Positive
Intel,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",Positive
Intel,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",Positive
Intel,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,Negative
Intel,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",Neutral
Intel,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,Negative
Intel,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",Negative
Intel,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",Negative
Intel,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",Neutral
Intel,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",Negative
Intel,yes its so bad. better go buy some steam deck or ally x,Negative
Intel,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,Negative
Intel,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",Positive
Intel,How are they going to feed all those CUs? Quad-channel LPDDR5X?,Neutral
Intel,That's considerably faster than an XSX.,Positive
Intel,>That's tapping on 4070/7800 levels of performance.  What is?,Neutral
Intel,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",Neutral
Intel,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",Positive
Intel,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,Neutral
Intel,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,Neutral
Intel,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",Neutral
Intel,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",Negative
Intel,It's called satire. You're just salty because you're the butt of the joke.,Negative
Intel,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,Neutral
Intel,Praying the blade16 gets it.,Neutral
Intel,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",Neutral
Intel,256 bit bus + infinity cache.,Neutral
Intel,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,Neutral
Intel,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",Positive
Intel,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",Positive
Intel,The rumored 40CU strix halo chip. Not the actual chips released this week.,Neutral
Intel,7500mhz ram and the 780m,Neutral
Intel,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",Neutral
Intel,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",Positive
Intel,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",Neutral
Intel,Literally where did you see 40-60% uplift at half the power?,Neutral
Intel,> 40-60% performance uplift at half the power  Source?,Neutral
Intel,"i chuckled, then again im not a fanboy of anything",Neutral
Intel,Dont expect 40CUs in a handheld anytime soon,Negative
Intel,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",Negative
Intel,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",Neutral
Intel,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",Neutral
Intel,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,Positive
Intel,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",Neutral
Intel,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,Positive
Intel,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,Negative
Intel,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",Neutral
Intel,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",Negative
Intel,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",Negative
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Neutral
Intel,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,Positive
Intel,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",Negative
Intel,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",Negative
Intel,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,Negative
Intel,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",Neutral
Intel,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",Neutral
Intel,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,Positive
Intel,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,Positive
Intel,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",Positive
Intel,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",Neutral
Intel,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",Negative
Intel,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,Neutral
Intel,"Installs beta software, proceeds to complain about it",Negative
Intel,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,Negative
Intel,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",Neutral
Intel,What Ghost of Tsushima issue?,Neutral
Intel,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",Neutral
Intel,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",Neutral
Intel,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",Negative
Intel,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,Negative
Intel,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",Neutral
Intel,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",Neutral
Intel,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,Neutral
Intel,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",Neutral
Intel,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,Negative
Intel,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,Negative
Intel,The documentation for it would still be in their archives,Neutral
Intel,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",Neutral
Intel,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",Negative
Intel,Wish Arc cards were better. They look so pretty in comparison to their peers,Positive
Intel,Thats actually a pretty solid and accurate breakdown.,Positive
Intel,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,Positive
Intel,3080 still looking good too,Positive
Intel,What they have peaceful then 4k series?,Neutral
Intel,Just get a 4090. I will never regret getting mine.,Positive
Intel,i miss old good times where radeon HD 7970 as best single core card cost around 400$,Neutral
Intel,"Damn, the A770 is still so uncompetitive...",Negative
Intel,"It's like the free market priced cards according to their relative performance. How weird, right?",Negative
Intel,How is that possibly annoying,Negative
Intel,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,Positive
Intel,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",Positive
Intel,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",Negative
Intel,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,Positive
Intel,8gb perfectly fine today :),Positive
Intel,"Ah yes sure, now where did I leave my 1500 euros?",Neutral
Intel,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",Negative
Intel,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,Positive
Intel,"Yeah, i like the black super series.",Positive
Intel,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",Negative
Intel,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",Neutral
Intel,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,Neutral
Intel,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",Negative
Intel,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",Neutral
Intel,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",Negative
Intel,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",Positive
Intel,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",Negative
Intel,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",Negative
Intel,"Yo, I saw the title and thought this gotta be Gnif2.",Neutral
Intel,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",Negative
Intel,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",Negative
Intel,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",Negative
Intel,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",Positive
Intel,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",Neutral
Intel,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",Negative
Intel,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",Negative
Intel,Long but worth it read; Well Done!,Positive
Intel,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",Neutral
Intel,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,Negative
Intel,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",Neutral
Intel,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,Negative
Intel,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,Negative
Intel,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",Negative
Intel,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,Negative
Intel,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",Negative
Intel,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",Negative
Intel,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",Negative
Intel,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,Negative
Intel,100% all of this...  Love looking glass by the by,Positive
Intel,How does say VMware handle this? Does it kind of just restart shit as needed?,Neutral
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",Neutral
Intel,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",Positive
Intel,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",Negative
Intel,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,Negative
Intel,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",Negative
Intel,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",Neutral
Intel,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",Neutral
Intel,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",Negative
Intel,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",Neutral
Intel,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,Neutral
Intel,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",Negative
Intel,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",Negative
Intel,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",Negative
Intel,TL;DR. **PEBKAC**.,Neutral
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",Negative
Intel,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,Negative
Intel,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",Negative
Intel,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,Negative
Intel,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",Negative
Intel,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,Negative
Intel,"Thanks mate I appreciate it, glad to see you here :)",Positive
Intel,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",Positive
Intel,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",Negative
Intel,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",Neutral
Intel,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",Negative
Intel,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,Positive
Intel,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,Neutral
Intel,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",Neutral
Intel,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,Neutral
Intel,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",Positive
Intel,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",Neutral
Intel,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",Negative
Intel,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",Neutral
Intel,"Funny, I saw the title and thought the same too!",Positive
Intel,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",Negative
Intel,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",Negative
Intel,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,Neutral
Intel,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",Neutral
Intel,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",Negative
Intel,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",Negative
Intel,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",Negative
Intel,ursohot !  back to discord rants...,Neutral
Intel,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,Negative
Intel,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",Negative
Intel,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",Negative
Intel,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,Positive
Intel,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",Negative
Intel,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",Negative
Intel,"It doesn't handle it, it has the same issue.",Negative
Intel,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),Neutral
Intel,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",Neutral
Intel,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",Positive
Intel,Me neither. I use a RX580 8GB since launch and not a single problem.,Positive
Intel,Because they're talking absolute rubbish that's why.,Negative
Intel,You are one of the lucky ones!,Positive
Intel,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",Negative
Intel,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",Negative
Intel,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",Positive
Intel,lol your flair is Please search before asking,Neutral
Intel,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,Negative
Intel,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,Neutral
Intel,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",Negative
Intel,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",Negative
Intel,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,Neutral
Intel,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,Neutral
Intel,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",Negative
Intel,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",Neutral
Intel,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",Positive
Intel,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",Negative
Intel,"""NVIDIA, it just works""",Positive
Intel,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,Negative
Intel,What is the AMD Vanguard?,Neutral
Intel,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",Negative
Intel,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,Negative
Intel,You misspelled $2.3T market cap....,Neutral
Intel,"Okay yeah fair enough, hadn't considered this. Removed it from my post",Neutral
Intel,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",Neutral
Intel,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",Neutral
Intel,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",Neutral
Intel,This is not a fix. It's a compromise.,Negative
Intel,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",Neutral
Intel,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,Negative
Intel,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",Negative
Intel,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,Neutral
Intel,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,Negative
Intel,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",Negative
Intel,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",Negative
Intel,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",Negative
Intel,The comment I quoted was talking about people playing games having issues.,Neutral
Intel,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,Neutral
Intel,The thing I quoted was talking about people playing games though.,Neutral
Intel,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",Negative
Intel,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",Negative
Intel,"Idk, I don't use Linux",Neutral
Intel,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",Neutral
Intel,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",Neutral
Intel,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),Neutral
Intel,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",Neutral
Intel,Because adding a feature for a product literally gives users more control for that product.,Neutral
Intel,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,Neutral
Intel,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",Negative
Intel,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,Negative
Intel,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",Neutral
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",Negative
Intel,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,Negative
Intel,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",Neutral
Intel,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",Neutral
Intel,*wayland users have joined the chat,Neutral
Intel,You're falling for slogans.,Negative
Intel,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",Positive
Intel,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,Negative
Intel,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),Neutral
Intel,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",Neutral
Intel,Honestly after a trillion I kinda stop counting 😂🤣,Neutral
Intel,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",Neutral
Intel,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",Negative
Intel,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",Negative
Intel,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",Neutral
Intel,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,Negative
Intel,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",Neutral
Intel,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",Neutral
Intel,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",Negative
Intel,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",Negative
Intel,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",Neutral
Intel,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",Negative
Intel,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",Neutral
Intel,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",Neutral
Intel,Oh then just ignore my comment 😅,Negative
Intel,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",Positive
Intel,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,Negative
Intel,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",Positive
Intel,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",Negative
Intel,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",Neutral
Intel,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,Negative
Intel,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",Negative
Intel,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,Negative
Intel,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,Negative
Intel,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",Negative
Intel,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",Neutral
Intel,"Too soon to tell, but hopes are high.",Positive
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",Negative
Intel,"Agreed, they cannot rest on their laurels.",Negative
Intel,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",Positive
Intel,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",Neutral
Intel,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,Neutral
Intel,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,Neutral
Intel,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",Negative
Intel,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",Negative
Intel,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",Negative
Intel,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",Neutral
Intel,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",Neutral
Intel,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,Negative
Intel,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,Negative
Intel,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,Negative
Intel,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",Negative
Intel,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",Negative
Intel,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,Negative
Intel,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",Negative
Intel,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,Neutral
Intel,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",Neutral
Intel,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,Negative
Intel,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",Neutral
Intel,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",Negative
Intel,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",Negative
Intel,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",Neutral
Intel,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,Neutral
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",Negative
Intel,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",Neutral
Intel,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",Negative
Intel,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",Negative
Intel,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",Negative
Intel,Oh and XE also have bug feature reporting.  Omfg!!!!,Negative
Intel,Nobody is 100% right ;),Neutral
Intel,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),Neutral
Intel,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",Negative
Intel,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",Negative
Intel,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",Neutral
Intel,What about using a DP to HDMI 2.1 adapter for that situation?,Neutral
Intel,"2021 my guy, it's right there on the date of the article.",Neutral
Intel,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,Neutral
Intel,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,Neutral
Intel,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",Neutral
Intel,And I guess infallible game developers too then. /s,Neutral
Intel,So you decide what criticism is valid and what not? lol,Neutral
Intel,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,Negative
Intel,"Yup, but do you see them making a big press release about it?",Neutral
Intel,that is not how it works but sure,Negative
Intel,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,Neutral
Intel,>whine about Redditors.  The irony.,Negative
Intel,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",Neutral
Intel,learn to comprehend.,Neutral
Intel,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,Neutral
Intel,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",Negative
Intel,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",Negative
Intel,"No, that would be you obviously /s",Neutral
Intel,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,Neutral
Intel,"Yea, given the state of XE drivers every major update has come with significant PR.",Neutral
Intel,Why not ;),Positive
Intel,Go word salad elsewhere.,Negative
Intel,"I have replicated the issue reliably yes, and across two different systems.",Neutral
Intel,If discord crashes my drivers.. once every few hours. I have to reboot,Negative
Intel,Discord doesn't crash my drivers  I don't have to reboot.,Positive
Intel,Really love how the 6000 series radeons look.,Positive
Intel,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",Neutral
Intel,That's a good looking line up,Positive
Intel,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",Negative
Intel,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",Positive
Intel,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",Positive
Intel,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,Positive
Intel,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",Negative
Intel,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,Negative
Intel,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",Negative
Intel,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",Negative
Intel,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,Positive
Intel,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",Positive
Intel,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,Negative
Intel,That 7900xtx sale number is insane,Negative
Intel,That just shows that most people that buy GPU's don't know a thing about them.,Negative
Intel,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",Negative
Intel,best discounts were 6750xt 6800 and 7800xt,Positive
Intel,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,Neutral
Intel,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",Positive
Intel,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",Neutral
Intel,"They're not out of stock there, duh",Neutral
Intel,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,Positive
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",Negative
Intel,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",Neutral
Intel,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",Neutral
Intel,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",Neutral
Intel,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",Neutral
Intel,the card is pretty bad if you missed that somehow,Negative
Intel,AMD probably ships leftover to countries in which they know it will sell,Neutral
Intel,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",Neutral
Intel,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",Negative
Intel,"As you notice the photoshop version differs, so you can't compare them really",Neutral
Intel,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),Positive
Intel,So basically PC games are never going to tell us what the specs are to run the game native ever again.,Negative
Intel,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",Neutral
Intel,The true crime here is needing FSR to reach these requirements.,Negative
Intel,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",Positive
Intel,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),Negative
Intel,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,Neutral
Intel,"well, at least the chart is easy to read, not a complete mess",Neutral
Intel,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,Positive
Intel,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,Negative
Intel,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,Negative
Intel,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",Neutral
Intel,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",Neutral
Intel,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,Positive
Intel,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",Negative
Intel,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",Negative
Intel,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",Negative
Intel,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",Neutral
Intel,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,Positive
Intel,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,Negative
Intel,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",Neutral
Intel,I hope they will bundle this game with CPUs/GPUs,Positive
Intel,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,Negative
Intel,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,Negative
Intel,This game better look like real life with those specs. I does looks beautiful!,Positive
Intel,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",Negative
Intel,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,Negative
Intel,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,Negative
Intel,I love it how absurd these things are these days.,Negative
Intel,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,Negative
Intel,Does anyone know if it supports SLI or crossfire?,Neutral
Intel,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,Neutral
Intel,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",Negative
Intel,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,Negative
Intel,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,Negative
Intel,Why in the f*ck is upscaling included on a specs page?,Negative
Intel,no more software optimization and full upscaling  bleah,Negative
Intel,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,Positive
Intel,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",Positive
Intel,"I've never even heard of this game, nor care about it, but these system requirements offend me.",Negative
Intel,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,Negative
Intel,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",Positive
Intel,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",Negative
Intel,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,Neutral
Intel,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,Negative
Intel,Nice,Positive
Intel,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",Neutral
Intel,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",Negative
Intel,Native gaming died or what ? Wtf they turning pc gaming into console gaming,Negative
Intel,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",Negative
Intel,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,Neutral
Intel,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,Negative
Intel,"Omg, it would be a graphic master piece or  bad optimized thing.",Negative
Intel,First time I see matches recommendations for nv and amd GPUs...,Neutral
Intel,Rip laptop rtx 3060 6gb,Neutral
Intel,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",Positive
Intel,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,Neutral
Intel,*NATIVE* resolution gang ftw!,Neutral
Intel,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",Negative
Intel,Looks capped at 60fps?,Neutral
Intel,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",Neutral
Intel,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,Neutral
Intel,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",Negative
Intel,Is it using UE5?,Neutral
Intel,"Ubisoft, rip on launch.",Negative
Intel,guessing no DLSS3 then?,Neutral
Intel,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,Positive
Intel,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",Neutral
Intel,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",Negative
Intel,Farewell 1660ti… looks like it’s time for an upgrade,Neutral
Intel,well my 3300x is now obsolete for these new AAA games...,Negative
Intel,4k ultra right up my alley 😏,Positive
Intel,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,Neutral
Intel,7900xtx will do 4k 120fps with FSR 3 then I guess?,Neutral
Intel,What must one do to achieve a higher rank than an enthusiast? A demi-god?,Neutral
Intel,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",Neutral
Intel,Is vrr fixed with frame gen then?,Neutral
Intel,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",Negative
Intel,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",Positive
Intel,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,Neutral
Intel,They recommend upscaling even at 1080p.. disgusting ew,Negative
Intel,pretty much this we all knew they would start using upscaling as a crutch.,Negative
Intel,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,Negative
Intel,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",Negative
Intel,My thoughts too…,Neutral
Intel,Thanks to all of you that were screaming dlss looks better than native lmao.,Positive
Intel,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",Neutral
Intel,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,Negative
Intel,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",Negative
Intel,Nope we as a community abused a nice thing,Negative
Intel,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",Neutral
Intel,Didn't take them long to make upscaling worthless.,Negative
Intel,i smell a burgeoning cottage industry of game spec reviewers!,Positive
Intel,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",Negative
Intel,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",Neutral
Intel,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",Neutral
Intel,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,Negative
Intel,yeah this is the new standard,Neutral
Intel,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",Positive
Intel,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,Neutral
Intel,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",Negative
Intel,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",Positive
Intel,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",Neutral
Intel,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,Negative
Intel,The actual true crime here is even having fsr to begin with. It should just have dlss,Negative
Intel,Seems like they tried to cover every basis with these.,Neutral
Intel,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",Negative
Intel,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",Positive
Intel,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,Neutral
Intel,"GCN support is over, RDNA1 is the lowest currently supported arch.",Negative
Intel,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",Negative
Intel,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",Negative
Intel,What do you mean forced raytracing?,Neutral
Intel,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,Positive
Intel,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",Neutral
Intel,It's actually 960p :(,Negative
Intel,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",Neutral
Intel,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,Neutral
Intel,We'll see in a year.,Neutral
Intel,AFAIK  &#x200B;  It is with RT,Neutral
Intel,Seems pretty good to me given it is at 4K with RT,Positive
Intel,"Likely includes the RT features , its also 4k",Neutral
Intel,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",Negative
Intel,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",Negative
Intel,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",Negative
Intel,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,Negative
Intel,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",Neutral
Intel,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",Negative
Intel,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,Neutral
Intel,Exactly! It even got xess so Intel users also can use xess,Positive
Intel,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",Negative
Intel,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",Neutral
Intel,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,Neutral
Intel,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,Negative
Intel,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",Negative
Intel,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",Neutral
Intel,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",Neutral
Intel,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",Neutral
Intel,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",Negative
Intel,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",Negative
Intel,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,Negative
Intel,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",Negative
Intel,"Mirage is PS4 game, Avatar is PS5",Neutral
Intel,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",Negative
Intel,Timed epic exclusivity? Aww man.,Positive
Intel,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,Negative
Intel,You... are.. joking... right..?,Neutral
Intel,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,Negative
Intel,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,Negative
Intel,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,Negative
Intel,Try ubisoft achievements :),Positive
Intel,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",Neutral
Intel,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",Negative
Intel,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,Negative
Intel,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),Neutral
Intel,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",Positive
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",Neutral
Intel,yup that is why I will play 1440 UW native with a 7900XTX.,Neutral
Intel,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,Neutral
Intel,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",Neutral
Intel,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",Negative
Intel,Very similar performance I guess,Neutral
Intel,Ye,Neutral
Intel,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,Neutral
Intel,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",Neutral
Intel,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,Negative
Intel,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,Neutral
Intel,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,Neutral
Intel,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",Negative
Intel,"with AFMF it works , didnt test with FSR3 now.",Neutral
Intel,Reading before raging :),Positive
Intel,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",Negative
Intel,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,Negative
Intel,For 30fps even lol,Neutral
Intel,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",Negative
Intel,Or be happy your shitty video card is still supported.,Negative
Intel,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,Negative
Intel,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,Positive
Intel,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,Negative
Intel,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,Negative
Intel,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,Negative
Intel,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",Negative
Intel,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",Neutral
Intel,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",Negative
Intel,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,Negative
Intel,But it does in many ways.,Neutral
Intel,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",Neutral
Intel,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",Negative
Intel,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,Negative
Intel,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",Negative
Intel,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",Neutral
Intel,speaking facts my guy,Neutral
Intel,You shouldn't have downvote dood wtf redditors ?,Negative
Intel,People with AMD cards dislike upscaling more because FSR sucks ass lol.,Negative
Intel,"Uhm, me btw...",Neutral
Intel,"> Who actually plays games at native these days, if it has upscaling?  I do.",Neutral
Intel,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",Negative
Intel,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",Neutral
Intel,onto absolutely nothing.,Negative
Intel,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",Negative
Intel,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,Negative
Intel,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,Neutral
Intel,The game is raytracing only with a ridiculous ammount of foooliage.,Negative
Intel,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",Negative
Intel,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,Neutral
Intel,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,Positive
Intel,False.  guy blocked me lmao,Negative
Intel,"I thought that was on Linux, though I might be wrong",Neutral
Intel,1070 doesn't have hardware RT though.,Neutral
Intel,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),Neutral
Intel,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,Negative
Intel,Completely agree.,Positive
Intel,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,Negative
Intel,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),Neutral
Intel,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",Negative
Intel,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",Negative
Intel,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,Negative
Intel,"Derp, derp.",Neutral
Intel,Sounds like John on Direct Foundry Direct every week.,Neutral
Intel,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",Positive
Intel,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",Neutral
Intel,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,Negative
Intel,Avatar is RT only. There is no non RT mode.,Neutral
Intel,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,Negative
Intel,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",Neutral
Intel,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",Neutral
Intel,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,Neutral
Intel,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,Neutral
Intel,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",Positive
Intel,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",Positive
Intel,"Next gen seems promising so far, low native rez hidden by upscaling, 60 FPS target on +500$ GPUs, traversal stuttering thanks to the glorious UE5. But hey, we have great reflections on puddles so at least we can take good looking screenshots.",Positive
Intel,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,Negative
Intel,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,Negative
Intel,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,Negative
Intel,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",Negative
Intel,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",Neutral
Intel,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",Neutral
Intel,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",Positive
Intel,This is what I am thinking too.,Neutral
Intel,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",Neutral
Intel,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",Neutral
Intel,Let's hope so. 30fps is far from recommended for FSR 3,Negative
Intel,Yeah I heard it works with that hoping it works with fsr3 now,Positive
Intel,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",Negative
Intel,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",Negative
Intel,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",Neutral
Intel,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,Negative
Intel,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,Neutral
Intel,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",Positive
Intel,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",Neutral
Intel,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",Negative
Intel,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",Negative
Intel,Assassins creed origins and odyssey side quests/collectibles oh my god,Positive
Intel,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,Negative
Intel,I appreciate your insights and opinions. Thank you.,Positive
Intel,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",Neutral
Intel,console games started upscaling way before PCs .,Negative
Intel,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,Negative
Intel,you forgot who owns one of the most popular engines out there?,Neutral
Intel,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",Negative
Intel,downvoted by devs lol,Neutral
Intel,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,Positive
Intel,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",Neutral
Intel,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,Negative
Intel,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,Neutral
Intel,Why is there a dialog message about unsupported hardware when you try and run a 390X?,Negative
Intel,It can do software based RT just like every other modern GPU out there.,Positive
Intel,Well then no wonder rx 5700 can't manage 30 fps lol,Negative
Intel,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",Positive
Intel,Avatars uses a Lumen like software RT solution.,Neutral
Intel,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",Negative
Intel,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",Negative
Intel,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),Negative
Intel,"Yeah, thatsl happened.",Neutral
Intel,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",Negative
Intel,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",Negative
Intel,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",Negative
Intel,You didn't understand. It's another Swiss knife engine,Negative
Intel,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",Negative
Intel,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,Positive
Intel,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,Positive
Intel,The PS5 has a 6700.,Neutral
Intel,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",Neutral
Intel,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",Positive
Intel,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,Negative
Intel,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,Neutral
Intel,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,Neutral
Intel,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,Neutral
Intel,TAA,Neutral
Intel,Temporal anti aliasing.,Neutral
Intel,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",Negative
Intel,"Ahh, welcome to r/FuckTAA",Negative
Intel,Even 4K looks blurry with some implementations of TAA,Negative
Intel,they're giving you the bare minimum until your upgrade!,Negative
Intel,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,Neutral
Intel,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",Negative
Intel,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",Neutral
Intel,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",Neutral
Intel,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",Negative
Intel,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,Negative
Intel,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",Negative
Intel,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",Neutral
Intel,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,Neutral
Intel,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",Neutral
Intel,No I havent. AMD is bigger than Epic.,Neutral
Intel,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",Negative
Intel,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",Negative
Intel,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",Neutral
Intel,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,Positive
Intel,It's software ray tracing which isn't accelerated by hardware.,Neutral
Intel,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,Neutral
Intel,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,Negative
Intel,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,Negative
Intel,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",Negative
Intel,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,Neutral
Intel,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,Negative
Intel,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",Negative
Intel,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",Neutral
Intel,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,Negative
Intel,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",Positive
Intel,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,Neutral
Intel,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",Neutral
Intel,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",Negative
Intel,Both excellent 👌 Down the Rabbit Hole was another solid one.,Positive
Intel,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",Neutral
Intel,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",Neutral
Intel,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,Negative
Intel,I have to turn it off in borderlands 3. Ugh.,Negative
Intel,r/FuckTAA,Negative
Intel,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",Neutral
Intel,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",Positive
Intel,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,Negative
Intel,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",Neutral
Intel,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,Negative
Intel,epic is tencent...,Neutral
Intel,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",Negative
Intel,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",Neutral
Intel,You don't need RT hardware to do software RT. That's what I'm saying.,Neutral
Intel,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",Neutral
Intel,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",Neutral
Intel,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),Neutral
Intel,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,Negative
Intel,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",Neutral
Intel,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,Negative
Intel,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,Negative
Intel,Yes hah,Neutral
Intel,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",Positive
Intel,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",Neutral
Intel,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,Positive
Intel,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",Neutral
Intel,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",Neutral
Intel,Oh man the hair... its so crazy how much upscaling kills the hair..,Negative
Intel,i mean they already arent the smartest bulbs considering they went team green.,Negative
Intel,Reading comrehension dude.,Neutral
Intel,"I do, but thanks for your interest.",Positive
Intel,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,Neutral
Intel,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",Positive
Intel,Nice. It’s on the list. Thanks man,Positive
Intel,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,Neutral
Intel,Probably because TAA is (unfortunately) more prevalent than it ever was.,Negative
Intel,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",Neutral
Intel,Oh my bad if I read that wrong,Negative
Intel,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,Negative
Intel,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,Negative
Intel,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",Positive
Intel,imagine being mad that 4 year old cards arent high end,Negative
Intel,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,Negative
Intel,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",Neutral
Intel,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",Negative
Intel,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",Neutral
Intel,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",Negative
Intel,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",Neutral
Intel,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",Negative
Intel,"It's for textures, not object edges from FSR use, lol. Two completely different things",Neutral
Intel,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",Negative
Intel,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",Negative
Intel,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",Negative
Intel,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",Negative
Intel,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,Neutral
Intel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",Neutral
Intel,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,Neutral
Intel,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",Negative
Intel,5700 was probably the lowest AMD card they had to test with.,Neutral
Intel,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",Positive
Intel,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,Neutral
Intel,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,Positive
Intel,AMD CAS is aimed to restore detail,Neutral
Intel,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",Neutral
Intel,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,Negative
Intel,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",Neutral
Intel,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",Negative
Intel,Yeah but a 3060ti didn't,Neutral
Intel,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",Neutral
Intel,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",Positive
Intel,Confirmed Can it run CP2077 4k is the new Can it run Crysis,Neutral
Intel,It'll be crazy seeing this chart in 5-10 years with new gpu's pushing 60-120 fps with no problem.,Positive
Intel,In 1440p with 7900 xtx with fsr 2.1 quality it doesn’t even get 30fps,Negative
Intel,"Makes sense. Is almost full path tracing, it’s insane it’s even running.",Positive
Intel,good. this is how gaming should be. something to go back to before the next crysis shows its teeth,Positive
Intel,3080 falling behind a 3060? what is this data?,Negative
Intel,wake me up when we have a card that can run this at 40 without needing its own psu.,Neutral
Intel,5090 here I come!,Positive
Intel,"Playing the game maxed out with path tracing, FG, RR and DLSS set to balanced at 1440p. Over 100fps 90% of the time. Incredible experience.  *With a 4070 ti and 13600k",Positive
Intel,"I mean, Path tracing is to Ray tracing, what Ray tracing is too rasterization.",Neutral
Intel,When Cyberpunk first came out the 3090 only got 20 fps in RT Psycho mode.  https://tpucdn.com/review/nvidia-geforce-rtx-4090-founders-edition/images/cyberpunk-2077-rt-3840-2160.png  Still does  Fast forward just one gem and you don't see anyone saying its demanding with many able to get RT Psycho on thier cards as new cards got faster.  Give it 2 gens and you are going to get 4k60 here.  Gpu will improve and get faster.,Neutral
Intel,"Look at that, my 6700XT is just 19fps slower than the RTX 4090 in this title.",Negative
Intel,It's a good thing nobody has to actually play it native.,Neutral
Intel,Well good thing literally nobody is doing that…,Negative
Intel,The future is not in native resolution so this is really pointless information.  Cyberpunk looks absolutelt mindblowlingy insane with all the extra graphical bells and whistles it has gotten over the years and with Nvidia’s technology it runs so damn smooth as well.,Positive
Intel,4.3fps lol. No amount of upscaling is going to fix that and make it playable. People were saying the 7900xtx had 3090ti levels of RT when it launched. A 4060 ti is 50% faster then it.,Negative
Intel,That's actually pretty amazing for any GPU to get a metric in frames per second instead of minuites per frame.,Positive
Intel,"Meh I get like 90+ fps with everything cranked with RR, FG and DLSS(Balanced) toggled on with my 4090 @ 4k.  Path tracing does introduce ghosting which is annoying buts not really noticeable most times but at the same time with RR enabled it removes shimmering on 99% of objects that is normally introduced with DLSS so I am willing to compromise.   Honestly as someone who used to have a 7900XTX I am disappointed with AMD its clear that AI tech in gaming is the way forward and they just seem so far behind Nvidia now and even Intel(going by some preview stuff). FSR is just not even comparable anymore.",Negative
Intel,Who cares when it looks worse than with dlss and ray reconstruction on top of running a lot worse? Native res 4k is pointless.,Negative
Intel,"Well the upscaling in this game is really good, DLSS with ray reconstructions AI accelerated denoiser provides better RT effects than the game at native with its native Denoiser.  Also Path tracing scales perfectly with resolution so upscaling provides massive gains. using DLSS quality doubles performance to 40fps, and dlss balanced gives 60fps on average, performance about 70-80 or 4x native 4K, that includes the 10-15% performance gain RR gives as well over the native denoiser as well.  I've been playing with about 60fps on average 50-70. with DLSS Balanced and RR and its been amazing. I don't like frame gen tho since it causes VRR flickers on my screen",Positive
Intel,"Running Ray Tracing or Path Tracing without DLSS or Ray Reconstruction is like intentionally going into a battlefield without any gear whatsoever, it's absolutely pointless and suicidal, what we can clearly see here though is top of the line 7900 XTX losing against already mediocre mid-range 4060 Ti by over 50%, which is just beyond embarrassing for AMD Radeon.  All this says to me is AMD Radeon need to get their shit together and improve their RT / PT performance, otherwise they will continue to lose more Marketshare on GPU department no matter how hard their fanboys thinks that they are pointless features, just like DLSS was back on 2020 right?  Also, with my 4070 Ti OC i can run it at average of over 60 FPS at 1440p DF optimized settings with DLSS Balanced + Ray Reconstruction without even using DLSS Frame Gen, with it on i can get over 80+ FPS",Negative
Intel,Running this way means you lose RR…why in the world would you run at native 4k?  It’s completely pointless now with RR.,Negative
Intel,Im having 80 fps  thanks to dlss 3.5  and its looking better than ever,Positive
Intel,"Why would I run native? I have amazing DLSS, ray reconstruction for huge image gains and frame gen. Nvidia offering all the goodies.",Positive
Intel,"I have a 13900KF-4090 rig and a 7800X3D-7900XTX rig. They are connected to a C2 OLED and to a Neo G9.  I've been holding on to play the game till 2.0 update. I've tried many times with different ray-tracing options and they all look good and all. But in the end I closed them all, turned back to Ultra Quality without ray-tracing and started playing the game over 120FPS.  This is a good action fps game now. I need high fps with as low latency as possible. So who cares about ray-tracing and path-tracing.  Yeah ray-tracing and path-tracing are good. But we are at least 2-3 generations away for them to become mainstream. When they are easily enabled on mid-range gpus with high-refresh rate monitors, they will be good and usable then :)",Positive
Intel,"If you have the HW, go all out. That's why we spend on these things.  I'm getting the best experience you can get in the premiere visual showcase of a good game.   It's path tracing. It isn't cheap but the fact that we have DLSS and FG with Ray reconstruction is a Godsend. It looks stunning and it's still early in development.",Positive
Intel,This doesn’t seem right…. 3080 performing worse than a 2080TI?,Negative
Intel,How tf is a 2080 ti getting more fps than a 3080?!?,Negative
Intel,I'm not seeing the RTX A6000 on here...,Neutral
Intel,4 fps for 7900 XTX  Oof. Shows you how much of a gap their is between AMD & Nvidia with pure ray tracing.,Negative
Intel,"Everything maxed, PT, 1440p, DLSS+RR+FG, input feels good, game looks and runs great, 100+ fps",Positive
Intel,"""Get Nvidia if you want a good ray tracing experience""  Yes Nvidia GPUs give a better ray tracing experience but is it really worth it if you are required to turn on DLSS? Imo, the more AI-upscaling you have to turn on, the worse the Nvidia purchase is.   I have a 6900XT and I will readily admit that the RT experience (ultra RT, no path tracing) at native 1080p resolution is ok, like 30-45 FPS (around 50 on medium RT settings), but if I turn RT lighting off (so RT shadows, sunlight, and reflections are still present) suddenly I get pretty consistent 60 FPS (i left my frames tied to monitor refresh rate, so 60 FPS is my max) and i can't tell the damn difference at native 1080p compared to RT medium or Ultra.   So would I spend another $400-$1000 to get an imperceptible outcome (imperceptible to me that is)? Most definitely not.",Negative
Intel,Does anyone actually play this game? Its more of a meme game imho,Negative
Intel,4.3 fps 😂😂😂,Neutral
Intel,"RTX 4090 DESTROYS 7900XTX with over 400% increased FPS, coming in at an astounding…. 19.5FPS 😫😂",Positive
Intel,"Overclocked RTX 4090 can (there are factory OC models that run up to 8% faster than stock). A measly 2.5% overclock would put that at 20 FPS.  Native is irrelevant though, DLSS Quality runs much faster with similar image quality.",Neutral
Intel,[85-115fps (115fps cap) in 4k dlss balanced](https://media.discordapp.net/attachments/347847286824370187/1154937439354368090/image.png) ultra settings,Neutral
Intel,Lol the 3060ti is better than a 7900xtx...crazy,Positive
Intel,Why would you use it without DLSS? Upscaling is the way of the future and AMD better improve their software to compete. Future GPU's aren't going to be able to brute force their way to high frames.,Neutral
Intel,Who cares about native though.  Cyberpunk PT at 4k DLSS Balanced + FG is graphically so far ahead of any other game ever released that it literally doesn't even matter if other games are run at native or even 8K. Cyberpunk is just in league of its own.,Neutral
Intel,ThE gAMe iS PoOrLY OpTImiSed!!!,Positive
Intel,Who cares about 20 fps native? Use the tools and tricks provided by the game and card manufacturer and you are good. The whole native rendering argument can be thrown in the garbage bin.,Negative
Intel,"The 7900xtx losing NATIVELY to the 4060ti 16gb is embarrassing, that card is half the price and overpriced, nevermind Nvidia's far superior upscalers. AMD falls further and further behind in RT, sad.",Negative
Intel,"99.7fps  with everything maxed out at 4k balanced with frame gen.   64.3fps with everything maxed out at 4k balanced w/out frame gen.   29.4fps with everything maxed out at 4k native.   That’s a big difference from this chart what I’m getting vs what they’re getting. For record, I’m using a 4090 suprim liquid undervolted to 950mv at 2850mhz. Stock is 2830mhz for my card.",Neutral
Intel,"Even with frame gen and DLSS quality, Ultra+Path Tracing looks incredible. I get solid 90 FPS. Funny thing is I can't get into playing CP 2077, even with incredible graphics and a QD-OLED monitor. It's just not my type of game :-D",Positive
Intel,I was wondering why a thread like this with the name like this is in this sub. And the I see the name of the OP. And it all makes sense,Neutral
Intel,"""4090 is 4 times faster than 7900XTX""",Positive
Intel,"So? Native 4k is such a waste of compute hardware, only an idiot would use it.",Negative
Intel,People need to change their mind frame. AI is here to stay and will be a major part of graphics rendering going forward.,Neutral
Intel,I played 2.0 yesterday with 2k ultra and path tracing with 155fps (DLSS3.5),Neutral
Intel,"The 4060 Ti literally better than the 7900 XTX in RT, cope snowflakes!111!11! /s",Positive
Intel,"So Nvidia only needs to increase their RT performance 3x while AMD needs to do 14x. We are not seeing that within a decade, if that.",Neutral
Intel,Sounds like Ray tracing isn’t ready for prime if nothing can run it and you have to use AI smoke and mirrors to fake it playable. I’ll worry about Ray tracing when it’s a toggle on or off like shadows and there is no major performance hit. See you in the 9000-10000 series of Nvidia or 12000-13000 series for AMD.,Negative
Intel,Native resolution is a thing of the past. DLSS looks better than native anyway.,Neutral
Intel,Is this with DLSS + FG?,Neutral
Intel,"Ray tracing already wasn't worth it imo, that's why I saved money buying an AMD card. It's not worth the cut in frame rate that then has to be filled with upscaling that makes frames look blurry anyway, so what's the point of even using it?",Negative
Intel,"Meanwhile me using medium settings + performance dlss on 3080 ti to get that locked 120fps. I just can't go back to 60fps. The game still looks amazing, especially on blackstrobed oled.",Positive
Intel,What is even with with all these path traced reconstruction bullshit lately? Ray tracing itself isn't even that mainstream yet.,Negative
Intel,No one is using these settings.,Negative
Intel,"Ray tracing is fun and all, but games have really improve much.. Same animations.. Same type of quest... Same listening to notes.. There have to be better ways....",Positive
Intel,"4K rendering for games like this is pretty pointless unless you're upscaling it to a massive 8K display via DLSS Performance.  Ultimately, the final image quality is going to be more affected by how clean the Ray / Path Tracing is than whether you're rendering at Native 4K or not.",Negative
Intel,"I'm honestly kind of shocked my 4070 is that far above the 7900XTX. I'd have thought it would be no where near close, regardless of ray tracing.",Negative
Intel,cyberpunk sucks don't worry about it,Negative
Intel,3080ti?,Neutral
Intel,"Yes, that is true, but what the numbers do not tell you is that if you tweak some settings, you are able to get 90 fps on that resolution with path tracing with no problem.",Neutral
Intel,I feel like the 4k resolution is here to stay for a couple years boys. Get yourself a decent 4k monitor or OLED TV and chill.,Neutral
Intel,"This is why DLSS3 and frame generation are so important. 4x the performance at over 80FPS with a relatively minor amount of silicon dedicated to AI, with only minor visual degradation.",Positive
Intel,"It's way too early for using ray / path tracing in games comfortably, maybe in 10 years it'll be different, but for the foreseeable future I'm more than happy with normal rasterization.",Neutral
Intel,Who cares tho? Its not even the way it's meant to be ran. This tech is is a synergy. No point in trying to 'flex' native at this point. A 4090 gets very high FPS and amazing visuals when doing things right.,Negative
Intel,can wait in 2030 with the 8030RTX low profile 30 watt card run this game with full path tracing in 4k 120fps lol (in 2030 a low end card will run this like butter don't down vote me because you didn't get it lol),Positive
Intel,"Seems like this tech is only around to give Nvidia a win. Also path and ray tracing doesn’t always look better. If the one true God card can’t run it, is it reasonable tech or is it a gimmick? Especially seeing how like no one has a 4090. Like a very small percentage of PC gamers have one. Most aren’t even spending $1600 on their whole build!",Negative
Intel,What body part do you think will get me a 5090?,Neutral
Intel,"Once they introduced this dlss upscaling anti-lag all these gimmicks I don't think we've seen true 4K native and almost anything lately.  This is maybe one game that does look that good but many of the games don't look that great and have some really serious system requirements when there are games that look much better and are much better optimized.  Companies are taking shortcuts, instead of optimizing they give you suboptimal and use FSR dlss all these gimmicks.  Don't get me wrong I always thought it was great for people who wanted to reach a certain frames per second someone with a weaker system to allow them to play at least the minimal at a steady frame rate.  It just seems like this is the normal now non-optimized games.  The 4090 is a beast if you could only get about 20 frames native with Ray tracing it tells you everything you need to know about Ray tracing.  It needs to be reworked that needs to be a new system I believe one real engine has it built in in a certain way probably not as good as Ray tracing directly.  To me it seems like ray tracing is hampering many games that tried to add it, too much focus on that instead of actual making a good game.  Let's daisy chain 4 4090s ..gets 60 fps.. and then house goes on fire!  Obviously I'm being sarcastic but realistically it's actually pretty sad a 4090 20 frames per second.",Negative
Intel,"Its not really surprising, raytracing is decades away from being truly mainstream. I applaud the misguided pioneers who betatest the technology, their sacrifice will be honoured",Neutral
Intel,"bro, for who are they making such intensive/badly optimised games? for aliens with more powerful pcs? i trully dont get it.",Negative
Intel,That not even proper path tracing.,Negative
Intel,And I thought that ray tracing was a needlessly huge performance drop for a barely noticeable difference...,Negative
Intel,"Can't wait to buy a $1,600 GPU to run a game at 20fps!!!  &#x200B;  Nvidia fans: i cAnT wAiT 🤑🤑🤑",Positive
Intel,Native is a thing of the past when dlss produces better looking image,Neutral
Intel,Lmfao ... where are all those nvidia fan boys who prefer eye-candy path trace over 60fps?,Negative
Intel,Sounds like some efficient tech. Lmao. What's the fucking point if it makes the game unplayable? RT just washes out the image and looks like shit. Kills the contrast. Can't wait for this fad to end,Negative
Intel,In my opinion it's too early for Ray / path tracing it's computationally expensive and if you are using rasterization at ultra settings not only does look nearly identical but it's easier to do the work for the graphics and that gives you more fps...,Negative
Intel,"Rather than having 2-3fps, I would go and see a video of path tracing.",Neutral
Intel,"Does this fact really matter tho? Dlss in cyberpunk is known the look on par or even better than native, frame generation will further smooth it out. I can definitely see the 7900xtx being able to do path tracing at 1440p at least once amd release fsr 3 and better once they release something to now match ray reconstruction which also give a small boost in fps making path tracing even more doable.",Positive
Intel,cool idc i wont play that shit at such shit settings,Negative
Intel,Do we need this BS? Seriously? Gpu’s will end-up costing over 2 grand soon,Negative
Intel,I don't even get the hype around ray tracing most games don't have it and when they do it either makes the game look blurry or isn't noticeable unless you're in a weirdly lit area or looking at a reflection,Negative
Intel,"Am replaying it with a 6950xt and a 7700k(defo bottleneck).  All maxed out(no RT) at 1440p. If I wanted 4k I would have to enable FSR.  Its completely playable imo.   Mostly 50-60fps.  For me RT vs off vs path tracing.   The game doesn't look more real or better, it just looks different.  But ignorance is bliss.",Positive
Intel,This chart is bull. Since when is a RTX 3060 faster than a 3080. Someone needs to seriously question the source of the chart,Negative
Intel,Just goes to prove once again that Raytracing was pushed at least 4-5 gens too early as a pure marketing gimmick by Nvidia,Negative
Intel,"The Tech Power Up article says:   >Instead of the in-game benchmark we used our own test scene that's within the Phantom Liberty expansion area, to get a proper feel for the hardware requirements of the expansion  So it's very hard to reproduce their data at the moment, but I've run 3 tests with the built in benchmark (run to run variance below 1%, so 3 runs are enough) and my config is averaging 25.7 fps at 4K native with Path Tracing, so the title's premise is not correct. Still, the game is not playable without upscaling at 4K with Path Tracing, so the basis of the sentiment remains correct. Benchmark data available [here](https://capframex.com/api/SessionCollections/279f9216-bdb8-4986-a504-91363328adbe).",Neutral
Intel,RTX 4090 1600€ 19FPS 👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼,Positive
Intel,"Yay i will get 13,8 FPS with my 4080 👍😂 Someone want to borrow some frames? I don't need them all!",Positive
Intel,The first card on this list a normal person would consider buying is a 4070. 8.5 fps lol.   Even with upscaling that's basically unplayable.   Then you have all the UE5 and path trace type demos where a 4090 is only getting 60fps with upscaling.. at 1080p lol.   We are not remotely ready for these technologies yet. They're at least one if not two console generations away. So multiple years.,Negative
Intel,"Looks like 4K is mainly just for videos, not gaming for the time being.",Neutral
Intel,How can you bring out such technology and no hardware can process it properly.  an impudence,Negative
Intel,the question is who need path traycing? i mean that cool and beautiful but 20 fps with a 1600dollar price tag that wild.,Negative
Intel,You could just you know... disable path tracing and get 70 fps or go below 4k,Neutral
Intel,"That what I expect when a game gets built specifically to market Nvidia, but Nvidia over estimated their technology.",Neutral
Intel,"um...im running cp 2077 on a 4090 at 70-90fps, 5k monitor with path tracing/ray reconsturction on and most settings maxed out...",Neutral
Intel,Yo guys let's make this really unoptimized game then rely on hardware manufacturers to use AI to upscale the resolution and insert extra made-up frames to make it playable on a $1600 GPU lmao.,Negative
Intel,"so it is settled, Devs have zero optimization in games.",Negative
Intel,"Love how shit the 3080 was lol, even when it came out it was shit. But somehow everyone loved it",Negative
Intel,"Or, hear me out, developers could just do better. I've seen far better looking games without the constant frame hit.   Cyberpunk hasn't looked good enough to justify the frame rates from the get go.",Negative
Intel,Once again proving RT is useless,Negative
Intel,Cyberpunk w/ path tracing at max settings seems even more demanding than Crysis was at launch.   20fps with a 4090 is insane.,Negative
Intel,"""only gamers get this joke""",Negative
Intel,"People have forgotten or are too young to remember when Nvidia had hardware tessellation features when that was first emerging. They had tessellation maps under the ground and water in Crysis that were doing nothing other than tanking AMD (ATi) performance. I am not saying this whole Cyberpunk thing is exactly the same, but it's essentially similar. In all honesty, turning everything up to 11 on my 3090 doesn't net much visual gain beyond some mirror like reflections and nothing I really care about when actually playing the game (compared to standard High/Ultra) and not pixel peeping, which seems to be the new game now for people buying expensive GPUs, so they can justify their purchase. Meanwhile, everyone else just gets on with enjoying video games instead of living vicariously through someone's 4090 at LOL 1080p",Negative
Intel,Starfield is peaking around the corner.,Positive
Intel,starfield as well,Neutral
Intel,I remember when physx was so demanding people had a dedicated 2nd Nvidia graphics card just to turn the setting on in Arkham City. Now it's considered so cheap to calculate that we just do it on the CPU lmao,Neutral
Intel,The new Crysis,Neutral
Intel,"I wouldn’t be so optimistic. Transistor shrinking is crazy hard now, and TSMC is asking everyone to mortgage their house to afford it.",Negative
Intel,How long until a $200 card can do that?,Neutral
Intel,Most improvement is probably going to AI software more than hardware in the next few years.,Positive
Intel,"8800GT giving advice to 4090:  “I used to be 'with it. ' But then they changed what 'it' was. Now what I'm with isn't 'it' and what's 'it' seems weird and scary to me. It'll happen to you!""",Neutral
Intel,GPU need to have its own garage by then,Neutral
Intel,Yep! Insane how fast things change.,Neutral
Intel,"Most likely there will be no gpu that supports path tracing and gives you native 4k 120fps in 5 years, maybe even in 10 years.  The technology has slowed down a bit. It’s increasingly more challenging to make more dense chips.  That’s why Intel has been struggling for many years already and every iteration of their cpus gives only minor improvements. AMD went with chiplets but this approach has its own problems.  Nvidia stands out only because of AI. Raw performance increase is still not enough to play native 4k even without ray tracing.",Negative
Intel,Yeah rtx 12060 with 9.5 gb Vram will be a monster,Positive
Intel,I'm willing to bet hardware improvement will come to a halt before that.,Negative
Intel,"In 10 years AI based upscaling will be so good, no one will want to natively render unless they are generating training data",Neutral
Intel,10 is too much. Give it 5.,Neutral
Intel,Transistor density advancements have been declining for a good while now. We can't expect hardware performance gains of old to continue into the future,Negative
Intel,Like the 1080 barely doing 4k30 and now we have gpus that do 4k120 id way heavier games.  Its still weord to me to see 4k120,Negative
Intel,But then the current gen games of that era will run like this. The cycle continues,Neutral
Intel,TRUE! i think 5-10 years was the actual point in time where anybody should have paid their hard earned dollar for raytracing gpus. instead ppl dished out $1000s for the RTX2080/Ti and now are sitting on them waiting for raytracing to happen for them xD,Neutral
Intel,Who knows what new tech will be out in even 4 years lol,Neutral
Intel,"And that’s when I’ll turn on this setting in my games. Fuck having to pay $1600 for a sub 30fps experience. Path tracing is far from being a viable option, it’s more like a proof of concept at this point.",Negative
Intel,"You wish, GPU makers have stagnated like it's crazy, even the graphics have peaked. I expected to see CGI level graphics like Transformers now",Negative
Intel,"...but will cost your everlasting soul, body, firstborn and parents. If it's on sale.",Negative
Intel,It is path trancing though. The technology used by Pixar to make Toy Story 4 (though they spent up to 1200 CPU hours for one frame) Path tracing used to take up to a day per frame for films like the original Toy Story. And they had their supercomputers working on it. It is a miracle of modern technology that it even runs real time.,Positive
Intel,"You basically need a 4090 to crack 60 fps at 1440p w/ dlss on quality without frame gen. It looks good, but not good enough to run out and buy a 4090.",Negative
Intel,"Same card, I just turned off RT at 4K. 75-120fps is better than 40 with muddy but accurate reflections",Neutral
Intel,"The 7900xtx is one of the worst gpus iv ever had lol on paper it looks so so good, In games it just play isn’t. Fsr is garbage, 4080 mops the floor with it :( plus the drivers from amd besides starfield have been awful this generation",Negative
Intel,I think that most of the progress will go together with software tricks and upscalers.,Neutral
Intel,"lol. And you missed the 2080Ti.   Every result under 10 fps is just to be ignored, it isn't representing anything outside of ""woot, the card managed to chuck a frame our way"".",Negative
Intel,That's VRAM for you,Neutral
Intel,we are counting decimals of fps its just all margin of error.,Neutral
Intel,If you buy a card with low ram that card is for right now only lol,Neutral
Intel,Wake me up when a $250 GPU can run this at 1080p.,Neutral
Intel,Well DLSS isn't best. DLAA is,Negative
Intel,Reviews have been saying that the game looks better with DLSS than native. Not to mention runs extremely better.,Positive
Intel,Can’t you just disable TAA? Or do you just have to live with the ghosting?,Negative
Intel,TAA is garbage in everything. TAA and FSR can both get fucked,Negative
Intel,"Yeah. 70 to 100fps on a 4080, but with 3440x1440 and DLSS-quality-FG-RR (nvidia needs new nomenclature....)",Neutral
Intel,"same, high refreshrate at 4k with optimized settings + PT + FG. With a 4080 of course, it's insane that it can look and run this great.",Positive
Intel,"Not exactly. Path tracing is just ray tracing, but cranked up to 99% with the least amount of rasterization possible.",Neutral
Intel,"> Give it 2 gens and you are going to get 4k60 here.  Assuming the 5090 literally doubles a 4090 (unlikely), that only gets us to 4K 40hz.  Assuming a 6090 doubles that, 80. which won't be bad.  Going with more conservative 50% boosts. 5090 will give 30. 6090 will give 45.  And i feel like 50% is being very generous, as nvidia have claimed moores law is dead and they can't advance beyond a 4090 by much. I'd guess we get 30% uplift in 5090 and maybe 10-15% uplift in 6090. So we'd still be under 4K30.",Neutral
Intel,Imagine buying a 4090 and then using upscaling.,Neutral
Intel,Path tracing is so much more demanding than ray tracing due to light scattering being modelled. It is a marvel it even runs.,Positive
Intel,PC gaming is in Crysis.,Neutral
Intel,Well it's a good thing 99.999999% of PC gamers don't give a shit about ray or path tracing.,Negative
Intel,You're the first other person other than myself I've run into that had an XTX but ended up with a 4090 in the end.,Neutral
Intel,"Plus frame generation works very well in Cyberpunk in terms of image quality. In some games, you need to get closer to ~80 fps output for an acceptable image quality with FG. But the FG in CP2077 is decent with 60 fps output, ~~and I get ~65-70 fps output with quality DLSS + FG at 4k on a 4090.~~ EDIT: I misremembered what I was getting. With path tracing, DLSS quality, frame generation, and ray reconstruction, I got 80.1 fps with the benchmark!  Of course there's the matter of latency, and the latency of CP2077 with FG output of ~65-70 fps isn't great. So I'll often use DLSS balanced + FG. Thanks to ray reconstruction, this now looks very close enough native 4k (to my eyes), with acceptable latency (to me), at a high framerate output.",Positive
Intel,"Nvidia features are always useless until AMD copies them a year or two later, only then they become great features 😁",Positive
Intel,"Tbh, list the games that have ray tracing right now. Pretty few.  It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it). I would personally go amd over nvidia because those 50 or so euros more that nvidia has against the amd counter parts simply are too much for just better rt and  dlss. I could spend those for a better amd card with generally better performance.  Regarding dlss, personally I would just lower the graphics before resorting to kinda bad generated frames, fsr even worse. But that's my point of view.  I find that amd still has to fix some driver issues but other than that, they are a fine brand. (so is nvidia)",Neutral
Intel,Yeh because native 4k looks worse than dlss + RR 4k,Negative
Intel,"Yeah the game with DLSS, RR and path tracing at 1440p looks amazing and with very high fps",Positive
Intel,What's the point of having $5000 PC when you're still gonna have literally the same graphics as $1000 PC then?,Negative
Intel,"This is a tech demo. That's the whole point. It's not really playable yet, but the game really is meant to showcase what is possible in the future and how close we are getting. That's what Nvidia is doing here by funding this whole project.  Crysis who many people are comparing this to, was in itself quite revolutionary for its time. The destructible environment in Crysis to this day holds up, and that was it's killer feature really.  You're gonna have swings at the future that miss as well, and that's ok.",Positive
Intel,"Did you try DLSS?  Imagine getting down voted for asking if he was using DLSS. What a fragile community. Sometimes it's okay to use DLSS, other times you better not or people will come to your house with torches",Negative
Intel,VRAM,Neutral
Intel,"Depends on what you are after i guess, is it worth it for me? Absolutely, DLSS+FG+RR with PT is a great experience.",Positive
Intel,still better than 90% of games in 2023,Positive
Intel,Arguably better now Ray Reconstruction has been added. It's quite a big image quality upgrade.,Positive
Intel,"…no it does not, unfortunately. Have you seen the artifacting in Cyberpunk?",Negative
Intel,"Someone ate up the marketing. I often see people wonder why games are so unoptimized today, your answer is right here. \^",Negative
Intel,All graphics you see on your computer screen is fake,Negative
Intel,Path tracing is more demanding than Ray tracing,Negative
Intel,"That's why I downscale from 1440p to 1080p, running DLSS and using Contrast Limited Sharpening. It looks better than native resolution and still performs well.",Positive
Intel,I guess between the 3090 and the 4070,Neutral
Intel,Yep hahaha,Positive
Intel,RemindMe! 7 years,Neutral
Intel,"No, this tech is around so game developers can sell more copies of their games by making them look better. The only reason why Nvidia is winning is because their GPUs are much better at intensive ray-tracing.",Positive
Intel,"This is an absurd, fanboyish thought lmao",Negative
Intel,dont let novidia marketing see this youll get down voted into oblivion,Negative
Intel,I would still argue it's still in proof of concept stage. The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native. That will the allow the lower end GPU'S to get 4k 60 or 1440p 60 upscaled.,Neutral
Intel,I mean I have no problem playing at 30fps dlss balance at 1080p DF optimized settings on a 2080ti with pathtracing idc what anyone says RT/PT is the future it just looks so much better than Rasterization,Positive
Intel,"Possibly a little more. Assuming that the leaked 1.7x improvement is right and path tracing performance scales the same, that would be about 34 fps at 4k native. Might be possible to hit 60fps or close to it with DLSS Quality.",Neutral
Intel,It could as well do 60fps if Nvidia adds a lot of path tracing HW into the GPU.,Neutral
Intel,Fanboyism of both kinds is bad,Negative
Intel,"It won't end, but hopefully we see games that have art designed with RT lighting and reflections in mind.  Cyberpunk isn't it. We'd need a full rebuild of the game, every location, every asset and surface remade, so it looks like originally intended by the artists.",Neutral
Intel,Since it needs more than 10gb vram,Neutral
Intel,Cyberpunk is more optimized than majority of games that came out in last two years. Just because you can't run it at 4k 120 fps with path tracing doesn't mean it's not optimized.,Positive
Intel,It’s not unoptimized at all. Game runs great for the quality of the graphics. You guys just don’t understand the insane amount of power path tracing takes. The fact you can do it at all rn is insane but once again people just throw “unoptimized” at it because they don’t understand the technical marvel happening in front of their eyes.,Negative
Intel,"No, this game is actually very well optimized. It's just extremely demanding at 4k native, which is why DLSS exists.  It would be unoptimized if it ran like this while looking like nothing special, but this is probably the game with the best graphics, period. At the very least, it's certainly the game with the best lighting.",Positive
Intel,"and people will be like ""well RT is the future""  raster was fine for over 15 years and raster games look great but we get it we must let incompetent game devs and their bosses make shit quality games because average gamer is ready to spend $100 to pre-order electronic version of horse shit and then justify it for years because they can't admit they wasted money",Negative
Intel,It's settled that you have no idea what you talking about,Negative
Intel,Its full path tracing u cannot really optimize this much.,Negative
Intel,"It always did. Most games don't scale well, Cyberpunk does.  Look at the last two years of pc ports and you will understand how cyberpunk looks and performs like it's black magic.",Neutral
Intel,"Baked illumination requires you to choose where to add light sources and how the lighting is applied to everything, it's a creative choice of the author, the AI needs instructions to know what to do, so it would need constant handholding, on the other hand RT uses simple physics to calculate how light bounced and NN to apply the light on surfaces.",Neutral
Intel,But the destructable environment and the water graphics and other things in Direct X 9 made high end pcs kneel as well.  I remember playing on my 9800gtx/+ with my Intel Q9300 quad core (lapped and oc'ed to 3.0ghz - EDIT: checked some old evga posts got it to 3.33ghz) with 2x4gigs DDR2 @1000mhz cas 5 trying to maintain a sustained 30fps at 900p resolution on my 1680x1050 monitor. And I oc'ed the crap out of that 9800gtx 835 mhz (cant recall if it was unlinked shadder or not now) core on blower air (won the silicon lottery with that evga card).  Tweaking settings was mostly user done and guided with old school limited forum help. Ahh the good old days of having lots of time during school breaks.,Neutral
Intel,"The equivalent of a 4090 at the time, 8800 ultra (768MB) got 7.5 fps at 1920x1200 very high quality. The other cards were not able to run it. Even the lowest tier last generation card runs this, even though it's at 5% speed.",Neutral
Intel,No and no again lol  The 8800gtx could do 1152x864 high and that was the top dog. DX9 cards could do that at medium but high shaders was like a 25fps average tops and that dropped into the teens on the snow level.   It took things like the gtx480 and HD 6970 to do 1920x1080 30fps max settings. That's before getting into Sandy Bridge being the first cpu that could sustain 40+ fps if the gpu power was there.   Crysis came during the core 2 duo/k8 athlon and first wave dx10 cards era and it took *2 more generations* of *both* gpus and cpus to do it some justice.,Neutral
Intel,"My 8800ultra was getting single digit fps at 1080p ultra, so CP isn't quite as demanding.",Neutral
Intel,Amd had a tessellation unit. It went unused but it was present,Neutral
Intel,"My fun story for PhysX was Mirrors Edge. I don't remember what GPU I had at the time but the game was pretty new when I played it. Ran fine for quite some time until one scene where you get ambushed in a building by the cops and they shoot at the glass. The shattering glass with PhysX turned on absolutely TANKED my framerates, like single digit. I didn't realize that the PhysX toggle was turned on in the settings. This was at a time when PhysX required a dedicated PCIe card in your system.   Once I turned it off it ran fine. Now I can run that game at ridiculous framerates without my system getting warm.",Positive
Intel,"I remember around about 2008 when companies like Asus and Dell were selling ""Physics Processing Units"" and some claimed that these would be commonplace in gaming machines just like Graphics cards had become 10 years previously.",Neutral
Intel,"Hardware accelerated physics (as in on the gpu), is different than what's going on the CPU.",Neutral
Intel,hobbies quaint consist aromatic political hat aback boat relieved crush   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"People used to have ATI/AMD for main and a lower-end NVIDIA for PhysX.  When NVIDIA found this out they pushed out drivers that disabled PhysX on their cards if an ATI/AMD card was detected, limiting you to the intentionally piss-poor CPU implementation of PhysX.  Think about that crap.  One day everything's going fine for consumers, the next day NVIDIA decides they don't like how consumers are legitimately using their cards and gimps everyone, weaponizing a physics engine company that they bought in 2008.",Negative
Intel,"Oh damn, I forgot about those cards.  I wanted one so badly.",Positive
Intel,Remember when companies tried to sell physics cards lol,Neutral
Intel,>PhysX  It never was a dedicated Nvidia card - it was a [dedicated psysx](https://www.techpowerup.com/review/bfg-ageia-physx-card/) on which the tech later was bought by Nvidia and implemented in it's own GPU's.  But the things never became really populair.,Neutral
Intel,"This makes me wonder if we'll ever see something similar with Raytracing, where we get a 2nd GPU purely for RT, and then the main GPU just does all the other stuff. Would that even be possible?",Neutral
Intel,I remember when people had a dedicated PhysX card.,Neutral
Intel,The ray/path tracing in this case done by specialized hardware which has more room to grow faster.,Neutral
Intel,"I would, transistor shrinking isn't the only method of increasing performance, and honestly, these companies have to keep putting out better cards to make money.  There have been many breakthroughs over the last few years. I give it another 5 as both amd and nvidia are pushing ai accelerated ray tracing on their cards, nvidia is in the lead for now but amd will eventually catch up.",Positive
Intel,There is still a big leap possible since all lithography processes at the moment are hybrid euv and duv.   But the moment everything is done euv things will drastically slow down.,Neutral
Intel,"No those are just pennies in the pocket of Nvidia, but as the consequence you as the customer need to take a mortgage on a brand new GPU.",Neutral
Intel,"Yeah the next 10 years are gonna be spent getting budget cards up to 4090 level performance, rather than bringing the bleeding edge to 5-10x the 4090 performance. As long as 4K is king, there’s not much incentive to do the latter.",Negative
Intel,"In 10 years we will be begging one of several thousand test-tube created Musk Family members for $200 so we can buy a cheeseburger.  But the jokes on them, we’re just gonna spend it on space crack.",Negative
Intel,"Never. Even if performance can be pushed that far, by the time it happens there won't be such a thing as a $200 graphics card anymore.",Negative
Intel,It's not happening unless the market crashes and they start focusing on offering good price/performance cards instead of bumping up prices every generation.,Negative
Intel,25 years,Neutral
Intel,"The software needs to run on hardware, right now it eats through GPU compute and memory.",Neutral
Intel,And sadly ended up with 450 Watt TDP to achieve that performance.,Negative
Intel,This. We'd be lucky to see more than 3 generations in the upcoming decade.,Positive
Intel,"Having seen a few newer games on relatively low resolution CRT display, I can't help but think it might come down to improved display tech and embedded scaling. Like DLSS3 features in the display instead of the GPU.",Neutral
Intel,Not with that attitude,Negative
Intel,Love how it's still gimped on memory size 😂,Positive
Intel,"They just need to render the minimum information needed , and let the ai do the rest",Neutral
Intel,"Don't get upscaling, to me it looks shite. Why spend hundreds or thousands on a GPU to not even run at native resolution.   It's something Nvidia are pushing because they've got a better version of it at the moment.  Maybe it'll improve, it's pretty much snake oil currently imo.",Negative
Intel,"Feels pretty good with 120 frames. Playing with a controller, i don‘t mind or even feel the input lag on a projector screen.",Positive
Intel,"It's mad that even with a 4090 running games at 4k, they still look less realistic than the low-res FMV sequences in The 7th Guest from 30 years ago.",Negative
Intel,"Do keep in mind that despite both being named the same, the devil's in the details. Movies use way more rays per scene and way more bounces too.   Path tracing in CP2077 shows temporal artifacts due to denoising, something that doesn't happen in movies. It is being improved with the likes of DLSS 3.5, but it is still quite off when compared to said movies.",Neutral
Intel,"Keep in mind the systems that rendered toy story 1, costing upwards of like 300k IIRC, have less power than your cell phone today and were super delicate/finicky machines. There's a dude on youtube that got a hold of like the second best one that was made at the time and the machine honestly was really impressive for when it came out, but it pales in comparison to even a steam deck really.",Neutral
Intel,"> Path tracing used to take up to a day per frame for films like the original Toy Story  Toy Story didn't use path tracing though, A Bug's Life was Pixar's first movie to use ray tracing (not path tracing) and only for a few frames in the entire movie for specific reflections, they started using ray tracing more generally for Cars and I can't find exactly when they started using path tracing but it should be around the early 2010s which is also when the other Disney animation studios started using path tracing",Neutral
Intel,No it's not a miracle. Because it's downscaled a lot and also uses lower quality math and piggybacking on AI to make up for it. It's basically running through 3 layers of cheats to produce results that aren't that much better than just traditional raster.,Negative
Intel,"That’d be some next level consumerism, paying 1500$ minimum to turn on a single setting in a single game just to play it wayyyyy slower than you would otherwise",Negative
Intel,"Hell playing path tracing with my 2080ti at 25 FPS is still looks absolutely fantastic, I would absolutely play with pathtracing on a 4090 constantly. Idc dlss looks great with RR even at 1080p. I won’t upgrade for another year or 2 (just bought a phone so I’m broke right now)",Positive
Intel,4090 getting 60fps at 4k balanced dlss with no frame gen. 100 with frame gen. I can make it drop into the 20s if I stand right next to a fire with all the smoke and lightening effects or if I go to a heavily vegetated area it’ll drop to mid 40s. But it’s stay consistently at 55-65 and even goes Into the 90s if I head out of the city.   Haven’t tried it at quality or with dlss off though. May go do that now that it says only 19fps lol. Have to try it to see for myself,Neutral
Intel,"https://www.tomshardware.com/news/nvidia-dlss-35-tested-ai-powered-graphics-leaves-competitors-behind  Well it's so close (54fps) it's more like a 3080Ti or higher from 3000 series or a 4070TI + from 4000 series it seems? The old 3000 series is punching way above it weight vs the 7900xtx, which was meant to deliver similar RT performance to the 3090.. which it doesn't.",Neutral
Intel,At some point the RT pixels are so expensive that native resolution w/o DLSS and frame gen is just not gonna work for the time being.,Negative
Intel,"On the other hand, if you set the new DLSS 3.5 to performance (which you should in 4k), and just enable frame generation, you get 90+ in 4k with basically zero issues unless you pause to check frames.",Positive
Intel,So rendering at 960p? Oof...,Negative
Intel,"you take that back, I literally returned my 4080FE for a 4090 liquid x for more frames in cyberpunk.",Neutral
Intel,I almost see no difference.   And to see the little diffidence I need to take two screenshots and compare them.,Neutral
Intel,The game look is ruined on PT it makes everything look completely different.  Regular RT keeps the style of the game and performs good.,Negative
Intel,Dont use useless raytracing and you wont have any problems lol,Negative
Intel,"Because PC gaming blew up during a time where you could buy a mid range GPU and not need to upgrade for 5-6 years. Now those sample people buy a GPU, and 2 years later it can't run new games. At least that's my theory.",Negative
Intel,AW2 looks insane. Can't wait to play soon,Positive
Intel,Well the 3050 managed to hold with just 8GB while the 3070Yi crashed?,Neutral
Intel,"Yeah, and people insisted on defending the configurations at launch lmao. The cards just won't be able to handle heavy loads at high resolution such as this game, regardless of how fucking insane the actual processing unit is. You can't beat caching and prepping the data near the oven. Can't cook a thousand buns in an industrial oven at the same time if there's trucks with 100 coming only once an hour.",Negative
Intel,My 3080 in shambles,Negative
Intel,"It looks better than native even with fsr quality, the Taa in this game is shit",Negative
Intel,You dont need to increase raw performance. You need to increase RT performance.,Neutral
Intel,Imagine!,Positive
Intel,DLSS unironically looks better than native TAA even at 1080p in this game because of the shit ghosting.,Negative
Intel,Only people who don't give a shit about high quality graphics don't care about ray tracing.,Negative
Intel,I got a stupidly good deal on the 4090(buddy won an extra one from a company raffle) and it just so happened my XTX was one of the cards suffering from the mounting issue on release.  Honestly very happy that all of that happened. Sure I would have been happy with the 7900xtx if I got a replacement but the 4090 just kinda blew me away especially with all the DLSS tech behind it.,Positive
Intel,"Nvidia used cp77 as a testing grounds for their dlss3 and rt tech, it'd no surprise it looks and performs relatively good, they literally partnered with cdpr for that.  Do not expect these levels of RT on most games anytime soon, probably in a couple years (with rtx 5000 series) the tech will be more mature and not as taxing for most cards, because needing to upscale and use framegen to get 60fps on an RT PT setting is kinda absurd.",Positive
Intel,">only then they become great features  Watch this happen with ""fake frames"" FSR3 in real time",Neutral
Intel,>Ray tracing as of right now is a gimmick   This line was already false years ago when control launched. Now it’s just absurd,Negative
Intel,"There are a lot of new games that features Ray Tracing on them as of now, too bad most of them doesn't look as visually as good as they could be though as most of the time their RT effects are dumbed down or reduced to the point it's pointless.  But the thing is that wasn't the point even from the beginning, RT Overdrive on Cyberpunk is literally considered as a Technological showcase  What matters here IMO for all of us is PC platform in general is showcasing here what a real high-end computers can achieve beyond what current gen console can do, and they are showcasing it really well here on Cyberpunk with much better visuals and using plenty of tricks to make them more than playable, where they shouldn't be considering how much demanding they are over standard, that is impressive enough to me and shows a good sign of technological engineering innovation.   As what Bryan Catanzaro said on his interview on DF Direct, work smarter not harder, brute force is not everything, of course it still matters a lot but doing it alone is starting to become pointless just like how it is on real life.",Negative
Intel,Much more fps :) Newer generation cards have at least 50-60% better raster performance then prev gen.,Positive
Intel,The Evangelical Church of Native,Neutral
Intel,"So just to be clear, you don't like running games at native resolution? Because the purpose of DLSS is to improve performance by specifically not running at native resolution",Negative
Intel,Nice but ive been playing video games since the 70s and its Shit end off..,Negative
Intel,The future of frame generation is going to be huge. I can definitely see 360p frame rendering upscaled to 4k with 80% maybe even 90% of the frames being generated. All with equal to or even better visual quality.   A 20 FPS game like this will become a 100-200fps game.  We may be looking at the end of beefy graphics cards being required to play games. Just a good enough mid or low tier chip (granted 5-10 years from now) with frame generation might be enough.,Positive
Intel,"After 2.0, it's around 100""fps"". Just 60""fps"" (meaning with FG enabled) is far from being playable due to latency. You need at least 90""fps"" for playable experience.   Cyberpunk PT vs RT is a huge difference in terms of graphics, while DLSS Quality vs Balanced is really slim in comparison. What you say would be a really bad trade-off.   At 1440p DLSS Quality it's easily 120+""fps"" which is perfectly comfortable.",Neutral
Intel,Cyberpunk is greatly optimized. It looks fantastic for the hardware it asks. Starfield looks like game from 5 years that asks hardware from the future.,Positive
Intel,"Yeah, sure, now go back to play starfield.",Neutral
Intel,"There literally is a /s, what else do you need to detect sarcasm?",Negative
Intel,"Someone made a really cool comment yesterdays on Reddit that even a generated frame using path tracing is less fake than a rasterized frame. It is effectively closer to reference truth. I had never thought about it that way, but people really are clutching their pearls with this shit",Positive
Intel,"I know, which makes path tracing even worse off imo.",Negative
Intel,I will be messaging you in 7 years on [**2030-09-23 15:26:41 UTC**](http://www.wolframalpha.com/input/?i=2030-09-23%2015:26:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/16pm9l9/no_gpu_can_get_20fps_in_path_traced_cyberpunk_4k/k1v42p3/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F16pm9l9%2Fno_gpu_can_get_20fps_in_path_traced_cyberpunk_4k%2Fk1v42p3%2F%5D%0A%0ARemindMe%21%202030-09-23%2015%3A26%3A41%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2016pm9l9)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,Neutral
Intel,haha this is gold 🥇,Positive
Intel,"At the moment, it’s definitely what it seems like.  Especially seeing how there’s only one card on the planet that can run this. Do you have a 4090?",Neutral
Intel,">I would still argue it's still in proof of concept stage.  Not at all, in fact another AAA game with ""path tracing"" is coming out next month.  >The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native  Forget ""native"", that ship has already sailed. Nvidia, AMD and Intel are all working on machine learning techniques, and several game developers are starting to optimize their games around DLSS/FSR.",Neutral
Intel,"Nope, not even close.  Shaders 3.0 - that's improvement. Ray tracing is noticeable, but nowhere near to justify the hit.  I've looked techpowerup comparison RT vs LOW (lol). Low looked far better and cleaner, then blurry mess of RT (Overdrive).   I don't need ""realistic"" lighting, I need pleasent bisual part with good gameplay. For ghraphics I have a window in my appartment.",Neutral
Intel,"In a screenshot, sure. When actually playing the game, unless the fps is low enough to remind them, 99,99% of people will forget if they left it on or not.",Negative
Intel,Pixar movies will sometimes take the better part of a day to render a single 4k frame on a render farm. So the fact that Cyberpunk takes more than 0.05 seconds to render a 4k frame in its path-tracing mode on a single 4090 clearly means that it's unoptimized garbage! /s  EDIT: Apparently people in this thread don't understand sarcasm (or don't understand how a path-traced game can take longer than 0.05 seconds to render a 4k frame with high-poly path tracing and still be optimized).,Negative
Intel,Man you had to lap your q9300 to hit 3.0ghz? Those chips really weren't OC friendly.  My q6600 hummed along 3.2 no problem on 92mm arctic freezer and a slight voltage bump.  I was so pumped when I upgraded to a gtx 260. Crysis was a dream.,Positive
Intel,Funnily Crysis still have better destructible environments than Cyberpunk has tho,Positive
Intel,"This. People losing their shit either weren't around when Crysis launched or have forgotten just how demanding it was . PT CP kicks the shit out of a 4090, Crysis murdered my 8800ultra.",Negative
Intel,crysis did not do 1080p 60 on maximum setting and dx10 on a 8800GT.   I had a 1280*1024 monitor back then and barely had 30 fps with my 8800GT.   Even the 8800 Ultra did not do 60 fps on full hd.  https://m.youtube.com/watch?v=46j6fDkMq9I,Negative
Intel,"Man, I remember how hype the 8800gt was. Thing was a hell of a big jump compared to previous cards that came out, prolly our first REALLY powerful card.  Still remember the old xplay video where they build a PC to play crysis and Bioshock. Put 2 8800GTs in sli in there with a core 2 quad which costed one thousand dollars back then!",Positive
Intel,"My poor HD 4850 got pushed to the limit to give me 1280x1024, lol",Negative
Intel,I remember thinking $300-400 (IIRC) was so much for a GPU back around those days.  7800XT is the closest we've seen in a while and not even close to the top end for today.,Negative
Intel,This is still the case to this day because the game includes a really old DLL file for PhysX. The other day I followed the instructions on the PCGamingWiki to delete some DLL files in the game directory and only then it ran perfectly smooth on my RTX 3070.,Neutral
Intel,LOL I remember this exact scene also.,Positive
Intel,"CPU PhysX back then was single-threaded and relied on ancient x87 instructions if I recall correctly, basically gimped on purpose. Even with a 5800X3D the shattering glass reduces the frame rate to the low teens. Sadly an Nvidia GPU is still required if you want to turn PhysX effects on for games from that era, though I hear that it's possible again to use it with an AMD GPU as primary.",Negative
Intel,"OMG I had the exact same experience, hahaha I remember it vividly, I was so confused why that room would just destroy my FPS until I figured out PhysX was enabled LOL",Negative
Intel,"I'd like to see ray tracing addon cards, seems logical to me.",Neutral
Intel,"TBH, I could probably run some of the old games I have on CPU without the GPU.",Neutral
Intel,glorious squash wild file crawl ancient crowd racial soft north   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,You could probably run Mirror's edge with physx on today's hardware without gpu fans turning on,Neutral
Intel,Last time I tried on an R7 1700X and RX580 I still couldn't turn on PhysiX without it being a stutter party 2 fps game.,Negative
Intel,What gpu you have,Neutral
Intel,Now it even runs fine on a Ryzen 2400G.,Positive
Intel,"And they were right, PhysX and systems very much like it are still used but things have advanced so much nobody even thinks about it and it no longer requires dedicated silicon.",Neutral
Intel,"Yeah, it’s been common knowledge for many years now that Nvidia are the most ruthlessly anti-consumer company in PC hardware, and it’s not particularly close.",Negative
Intel,Ah good old Ageia before nvidia bought them out https://en.wikipedia.org/wiki/Ageia,Neutral
Intel,"No, you could actually install two Nvidia cards and dedicate one of them to only PhysX.",Neutral
Intel,"It would certainly be possible, but it wouldn't really make sense. Splitting it up on multiple GPUs would have a lot of the same problems that sli/crossfire had. You would have to duplicate memory, effort, and increase latency when you composite the final image.  It may or may not make sense to maybe have a raster chiplet and a ray tracing chiplet on the same processor package on a single gpu. But, probably makes more sense to have it all on one chiplet, and just use many of the same chiplet for product segmentation purposes instead.   A separate PPU did make sense tho, I'm still annoyed that the nvidia ageia deal essentially kill the PPU in the cradle. Our gaming rigs would cost more if PPUs became a thing, but we could have had a lot better physics then we do today. There is still a revolution in physics to be had some day...",Negative
Intel,would be cool if 2000/3000 series users could get a small tensor core only PCIe card to upgrade to framegen,Positive
Intel,"That's true, but if he's talking about *the equivalent* of a current $200-class card, I'd say about it's 10 years, what do you think?",Neutral
Intel,At least with the 4090 you can run 70% power target and still hit insane FPS while only pulling around 300w which is the same as my old 3080. The gains with that extra 150w are a couple percent at best. Not worth it to me.,Negative
Intel,Intel design will be a little bit different as far as now.  If I understood it right AMD chiplets communicate via lines on pcb but Intel wants to make something like chip-on-a-chip.,Neutral
Intel,bedroom ring library cows summer thought aspiring worm north joke   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"It's also worth noting that those early movies don't use path tracing either, Pixar switched to PT with Monster University around 2013 IIRC.",Neutral
Intel,"There is a lot of room for improvement, both in the software and future generations of hardware.  It's coming along though!  Overdrive mode looks nice, but there's just a lot more ghosting than the regular RT mode.",Positive
Intel,"They are still the same thing and should be named the same, your disclaimer is just semantics about the implementation.",Neutral
Intel,"Weird argument.  Traditional raster is cheats upon cheats upon cheats already.  In fact, all of the lighting entirely in raster is essentially cheats.",Negative
Intel,It will not work well/look good using a single frame worth of data either due to the low ray counts. Digital Foundry has shown Metro Exodus builds the lighting over ~20 frames or so and it seems like Cyberpunk is even more. Even Cyberpunk ray pathtracing doesn't look as good in motion due to that same build-up effect.  You need to cast 20-50x the rays to have enough data for a single frame but then you will be measuring the game performance by seconds per frame.,Negative
Intel,"RT looks significantly better than raster when it’s not just using simple reflections like a Far Cry 6. Raster is all cheats, the so called reflections are essentially rendering the scene or part of the scene being reflected in reverse then applying fake material effects to mimic metal, water, etc. Raster also takes HUNDREDS to thousands of person hours to get the fake reflections and lighting to look decent to good. That’s why devs love RT and want it to grow, as RT/hardware does the work for them.",Negative
Intel,It's significantly better than raster. It kills fps but the quality is great,Positive
Intel,Better graphics needing more expensive hardware is hardly a hot take.,Neutral
Intel,"Yes, better graphics costs performance. SHOCKING",Negative
Intel,People do it!,Neutral
Intel,It’s not way slower. I get 110 FPS at 4k with all DLSS settings turned on and honestly it’s insane.,Positive
Intel,"I shamefully admit I bought a (inflation adjusted) $400 console in the past to play one game. That's marginally better than buying a GPU for one setting in one game, but still.",Negative
Intel,Sounds like most nvidia fanboys,Neutral
Intel,"If you're into the eye candy, I can see a lot of people that have the money doing it.  Not everyone though. That's around a mortgage payment for me.",Neutral
Intel,And yet people will tell you how nvidia is mandatory. Like you want to overspend to play 1 game with shitty fps? I don't understand these people.,Negative
Intel,Nvidia fanboys being happy with fake frames and fake resolution will never not be funny to me.,Negative
Intel,What? I thought amd was always more tuned to raster as opposed to reflections,Neutral
Intel,Which is why nvidia is rabidly chasing AI hacks,Negative
Intel,This and rumors are saying that the 5090 is gonna be 50-70% faster than 4090 which wont be enough for native 4k either.,Negative
Intel,Say that to the people playing upscaled games at 4k (540p) on PS5.,Neutral
Intel,"I understand some RT effects might not catch everyones eyes, but seriously, path tracing cyberpunk vs 0 RT cyberpunk IS a big difference",Neutral
Intel,Huh? [Watch this is in 4K on a big screen.](https://www.youtube.com/watch?v=O7_eHxfBsHQ&lc=UgyO2vRiSI6CRU5kYmV4AaABAg.9uyI5P8oamz9uylTN_rdmL) The differences are really apparent in each version.,Neutral
Intel,"PT looks way better in the game to me - it makes everything in the environment have a more natural look to it.  It adds, not ruins it...",Positive
Intel,"That's how I feel as well. It looks awesome don't get me wrong, but it doesn't look like cyberpunk to me.",Negative
Intel,It's not a perfect way of measure but you can clearly see how (at least on Nvidia GPUs) the 8/10GB cards are way behind the >11Gb cards. Meaning you need 11 or 12GB of VRAM for this scenario which cripples the 3080 but not the 3060.  We said from the start these configurations are shit but no-one listened. There you go.,Negative
Intel,The crypto miners did me a comical solid by preventing me from acquiring many countless times and hours I wasted (came from a gtx1070 before finally being able to upgrade). Was able to get my 6900xt eventually for around $600 with 2 games. Becoming increasing thankful for the extra Vram these days.   Once I start getting through my game backlog and into the gard hitting ray tracing ones will hopefully upgrade to something with at least 24gigs of GDDR# lol.,Positive
Intel,"I mean, Alan Wake 2 is also releasing with path tracing so my guess is we're definitely gonna see more games featuring heavier RT, although not necessarily path tracing due to the performance hit.",Neutral
Intel,"I expect path tracing support to be the exception for a while because most developers will consider the non-trivial dev time to implement it not worth it. However, I'm sure Nvidia would be willing to throw dev help at most developers who would be willing to implement path tracing with their help.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode. Presumably, a less complex world helps, but they may also have implemented opacity micro maps, which I'm not sure if Cyberpunk has done in the 2.0 update. Perhaps they're cherry-picked examples for the trailer.",Neutral
Intel,Raytracing now is a gimmick *  ( * - in AMD sponsored games where RT effects are so tiny you have to zoom on to notice them),Negative
Intel,"Except this is a tech demo. CP77 has lots of cut corners to achieve that, NN was trained to keep the interface stable in that game and even then, ray reconstruction creates horrible artifacts at some angles and distances. And that's with Ngreedia's engineeers literally sitting at CDPR's office.",Negative
Intel,You can already have 120fps on $1000 PC,Neutral
Intel,It improves both looks and fps so it is a win win,Positive
Intel,"The issue with this is the inputs will only be registered at 20Hz. This would feel awful. The benefit of high FPS is having very low latency. This will just make it look smoother, not feel any better than 20 FPS",Negative
Intel,Considering there is no input on the generated frames that is going to feel horrible to play. Base FPS (incl DLSS) really still needs to be 50-60fps to feel playable imo.,Negative
Intel,I'm thinking will there ever come a day where 100% of frames are generated by AI. Like the data inputs directly from the card into the neural network and it no longer requires physical hardware limitation to generate frames.,Neutral
Intel,"Don't forget that all NPCs will be controled by AI to simulate a real life.  They'll wake up in the morning, shower, commute to work, work, then go home.  All this to do it over and over and over until they die...  mhhh... this is kinda sad to type out.![gif](emote|free_emotes_pack|cry)",Negative
Intel,It absolutely is. I’ve been playing at 1440p with DLSS Balanced and Ray Reconstruction and it is absolutely the worst version of DLSS I’ve ever seen. It’s basically unusable.,Negative
Intel,There are horrible artifacts with RR. And ghosting. It replaces artifacts with different ones. Needs a lot more work tbh. Or training.,Negative
Intel,"The thing about ray and path tracing is that they are demanding on their own, not because they are badly optimised",Negative
Intel,"I have a 4070. It runs the game just fine with the tools that have been set in place.  This is absolutely not a pissing match. Cyberpunk is demanding, and takes advantage of lots of tech available for upscaling and ray tracing and frame generation. AMD card’s currently do not match Nvidia’s in those categories, not even close. This us not an attempt to showcase Nvidia tech, but it does take advantage of it.",Positive
Intel,Because it's not about the flagship card. Only if the flagship can attain good performance at native then you will have headroom for lower end cards. Think about it. Of course upscaling is very important. But you also need some headroom to upscale.,Neutral
Intel,"Yeah. I wanted better cooling performance after upgrading to the artic freezer 7 as well with those weird crapy plastic tension clips you pushed in manually, fearing bending them (worse than the stock intel cooler tension twist lock). Kept having random stability crashes until after i sanded it down for better thermals...Good old sandpaper and nerves of steel fearing an uneven removal of the nickel coating.  Was trying to maximize performance as back then core 2 duo and the extreme versions were king and they had way higher single core clocks and were easy to oc. Wanted the multhreaded for Crysis, LoL (back when you had to wait 5min+ to get into a game), BF2, and was eventually playing TF, Day of Defeat and Natural Selection.  My upgrade back then was to the Evga 560ti DS, which I ended up installing a backplate and  a gelid cooler. They had wayy better thermal tape/individual heatsinks for the memory/vram chips for the heat transfer. Evga back then told me if I ever needed to RMA it that I would just need to re-assemble it as it was shipped. Remember using artic solver ceramique instead of as5 due to it potentially not degrading as fast as well.   Good times =)",Negative
Intel,"Yea same here, didn't play crysis until I got a hold of a 260 after having 2 8600gt's in sli. Played mostly wow at the time and those 2 8600gt's in sli got blown away by the gtx 260, but man that card was hot!  Remember in 2011 upgrading to a gtx 570, the first gen with tessellation and playing dragon age 2 which was one of the first games to have it. Ended up turning off tessellation cause it hit the card too hard, least until I got a second 570 in sli, which sadly died like a year later due to heat while playing Shadow Warrior.",Positive
Intel,Q6600 was the bomb. I ran mine at 3.0GHz all its life and it's still alive today. Had it paired with an 8800GT which was IMO one of the best GPU value/performance of all time.,Positive
Intel,Had 3.4ghz on my q6600 didnt want to hit 3.6ghz on modest voltage but 3.4 all day and was quite the lift in fps in gta4 compared to stock 2.4ghz. Run 60fps 1440x900 no problem those where the days when OC gave actual performance boost,Neutral
Intel,> I was so pumped when I upgraded to a gtx 260. Crysis was a dream.  In those days I was a broke teenager and I remember upgrading to a GTS250(rebadged 9800GTX+ with more vram) and crysis still hammered my Athlon x4 965(?) system.  While my richer neighborhood buddy upgraded to a 260 just to play Left 4 dead.,Positive
Intel,"> q9300  Yes, it was not a good bin. I was at 4.0GHz with my Q9550, with just a copious amount of voltage.",Negative
Intel,"After checking sone old Evga posts, (mod rigs server is done, along with old signatures), I was able to get to 3.33ghz.",Neutral
Intel,"Yeah my lapped Q6600 was my daily driver at 3.2 GHz with a Tuniq Tower 120 Extreme. It ran it pretty cool. I think when Crysis came out it and my 8800 GTX could do 20-25 frames at 1680x1050 with drops into the teens. EVGA replaced it with a GTX 460 when it died, big upgrade.",Positive
Intel,moving data between the two is the issue,Neutral
Intel,grey cagey sharp detail handle north grandiose connect sparkle hungry   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"I wonder the same thing. But I guess if Nvidia would put it all in an extra card, people would just buy more amd to get the best of both worlds.",Neutral
Intel,Game physics doesn't seem to be a focus anymore though,Neutral
Intel,I'm ashamed I even own one NVIDIA card.    Unsurprisingly it's in a laptop that I bought during the [illegal GeForce Partner Program](https://hothardware.com/news/nvidia-geforce-partner-program) before the public knew about the program.  Not making that mistake again.  Screw NVIDIA.  Every other card I've ever gotten has been ATI/AMD (and obviously the low-end integrated Intel GPUs on some office laptops)  EDIT: Keep them downvotes coming.  You've all been bullshitted by NVIDIA's marketing.  Your boos mean nothing; I've seen what makes you cheer.,Negative
Intel,Anyone who owns an Nvidia GPU should be ashamed of themselves tbh.,Negative
Intel,"There was more nuance to that ""bugfix"" than just ""it was a bug that got fixed""   Modders were able to re-enable the AMD/NVIDIA PhysX combo basically immediately via editing DLLs.  Not driver replacements, but actual DLL tweaks.  The speed at which modders were able to fix what NVIDIA ""broke"" combined with the public outrage put them in a funny legal spot.  If they continued with PhysX disabled and claiming it was a bug then the bug can easily be fixed, as shown by modders doing it first.  So either fix it and get everyone back to normal, or leave PhysX disabled even though it can be easily enabled and open the company up to potential anti-competitive lawsuits.  Obviously not many modern games use the GPU implementation of PhysX anymore and the performance difference between current GPU and CPU implementations are negligible anyway, but their intent at the time was clear once it was shown how quickly modders were able to get everyone back to normal.",Neutral
Intel,"Yes, but not after Nvidia bought Ageia.",Neutral
Intel,"If the $200 cards of today aren't 3x better than the flagship cards of 10 years ago (they're not), then you shouldn't expect a $200 card in 2033 to be 3x faster than an RTX 4090. Average annual performance gains are more likely to decrease between now and then, rather than increase.",Negative
Intel,I like how you said: static image  because in motion upscaling is crappier,Positive
Intel,"It isn't a weird argument. Rasterization is premised on approximating reality, while RT is simulating it.  The extreme few amount of rays we trace right now, including bounces, means effects are often muted and the performance hit is enormous. To compensate, we're using temporal upscaling from super low resolutions *and* frame interpolation.  Temporal upscaling isn't without it's issues, and you know how much traditional denoisers leave to be desired. Even Nvidia's Ray Reconstruction leaves a lot to be desired; the thread on r/Nvidia shows as much, with ghosting, artifacts, etc. all over again. It's like the launch of DLSS 2.0.  All of that, for effects that are oftentimes difficult to perceive, and for utterly devastating traditional rasterization performance.",Negative
Intel,"It's a shame raster is so labor intensive, because it looks so interesting when done right.     I look at screenshots from cyberpunk, Raster / Ray Trace / Path Trace, and in most of them the raster just looks more interesting to me. Not constrained by what would be real physical lighting. The 100% RT render versions of old games like Quake gives a similar feeling.     But I imagine there will be more stylistic RT lighting over time, it saving a ton of labor is all things considered good, freeing it up for actual game design.",Positive
Intel,">That’s why devs love RT and want it to grow, as RT/hardware does the work for them.  It's the marketing departments that love RT. Devs hate it because it's just yet more work on top of the existing raster work.",Positive
Intel,"It *is* way slower, you're just compensating it by reducing render resolution a ton",Negative
Intel,If it was bloodborne i am guilty of that myself,Negative
Intel,price zealous rinse detail yam rainstorm groovy automatic snails fact   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,4k max without dlss or fsr or frame gen I still get 30fps. Sorry you can’t even hold 10. Guess I’ll shill some more to at least not be spoon fed shit from amd,Negative
Intel,"Ray tracing and path tracing aren't just ""reflections"" , but this thread is specifically about the new path tracing modes in CP2077.",Neutral
Intel,Rasterisation is a “hack” too,Negative
Intel,If it works it works....computer graphics has always been about approximation,Positive
Intel,"4090 is \~66% more powerful than 3090 overall, but it has 112% more fps in this test.     The RT capabilities of NVIDIA cards has grown faster than general capabilities.",Positive
Intel,"I guess on the internet, you can just lie. There’s no game on ps5 that upscale from 540p to 4k.",Negative
Intel,"I guess I was wrong on that one, although SWJS upscales from a resolution as low as 648p, which is not much.",Negative
Intel,It's big in certain lighting situations. In others it's not as noticeable as you'd hope,Neutral
Intel,"PT vs no RT comp is silly.  U should compare PT vs RT, and there, the difference is minor and not even subjectively better. Just different.",Negative
Intel,Natural? It over saturates the light bleeding into the entire scene. Everything becomes so colorful. Even when floors or ceilings are not supposed to be reflective. Literally tiles with grainy texture suddenly becomes a mirror..  It's ridiculously overdone.,Negative
Intel,Guys I'm trying to suffer while I play the game what settings will let me suffer the most,Negative
Intel,"I mean this is native RT/TAA. Was never meant to be played this way. You need to use DLSS and Ray Reconstruction. With that setting, i get about 40-50fps at max settings with my 3080. Not too bad.  The thing about AMD is that they don't have any of this AI technology (yet). You have to rely on raw power, which won't get you far.",Neutral
Intel,"Until the next generation of consoles, I think path tracing will be added to something like a couple major games per year with Nvidia's help. If I'm right, that's wouldn't be a lot, but it's definitely be a bonus for those who have GPUs that are better at ray tracing.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode (which could be for a variety of reasons, including CP2077's larger world).",Positive
Intel,Alan Wake 2 is also an outlier since the same dev made Control which was a test bed for DLSS 2.0 and one of the earliest games to fully embrace ray tracing. Alan Wake 2 is also NVIDIA sponsored and looking to be another test bed for them.,Neutral
Intel,"Redditors trying to have reading comprehension (impossible)     >It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it).",Neutral
Intel,"Every single game has cut corners, starfield has multiple cut corners. Tech demo would imply there's no great game under great visuals.",Negative
Intel,Get that fps with a 5120*1440 240Hz monitor,Neutral
Intel,"It's almost like crushing your fps with all these fancy new graphic features isn't worth it, even on the best cards. It's user preference in the end.  That said, many gamers, once they play at high hz screen and fps, find it very difficult to go back to low fps for any reason. Games feel slow by comparison, no matter how great the lighting looks.",Negative
Intel,"Both DLSS and FSR have weird image artifacts (FSR is worse though) so i guess if you don't see them then DLSS only works in your favor then.  I personally want as little noticeable image artifacts from DLSS/FSR when I play so opt for native resolution, generally.",Negative
Intel,Have you tried dlss quality. Dlss balanced sacrifices image quality and dlss quality doesn't/can enhance it sometimes,Negative
Intel,">a lot of the comparisons just make it look like it gives too much bloom  This is due to colors being compressed to SDR. In HDR that ""bloom"" looks awesome as it's super bright like it would be in real life but still not losing any details. You lose those details only on those SDR videos.  PT just looks and feels way more natural than just RT, when only raster literally looks like it was two different games from two different hardware generations.  Once I saw Cyperpunk PT there is no way I'd imagine playing this game in any other way. All other games look absurdly ugly next to it now. It's something like when you get a high refresh screen and can't look back at 60Hz anymore.",Negative
Intel,"Trust me, I know how the technology works. That doesn't change the fact that it's not worth it imo. I'm not  paying 4090 money to have the newest fad that just makes games look over saturated and glossy.",Negative
Intel,We are talking about running the game just fine now are we? Because rasterization is just fine. I will consider ray tracing reasonable when you can do it maxed out for $1200(GPU) native. Until then it’s a gimmick. IMO,Positive
Intel,"So even you, someone who is an enthusiast of this sort of thing, i.e. a small minority of the userbase, had to spend \*minutes\* benchmarking to realize you didn't have RT turned on.   Yeah, you would lose that bet.",Negative
Intel,Those wolfdale C2D were beasts. I had a buddy with one and we took it 4.0. And it kept pace/beat my C2Q as most games didn't utilize MC as well back then.  Man XFX and EVGA all the way. Shame they both left the Nvidia scene.,Positive
Intel,"Ah, WoW. That started it all for me on an C2D Conroe with ATI x1400 on a Dell inspiron laptop freshman year of college.",Positive
Intel,8600gt in SLI man you are Savage!🔥,Positive
Intel,"I had the same setup, and i managed to get my Q6600 stable at 3.2.  Was a blazing fast machine and it still shit the bed in some parts of crysys",Positive
Intel,there there’s no way there will ever be another 8800 GT. you got so much for your money.,Negative
Intel,"Those 9800gtx+ were solid cards, it's what I upgraded from.  You make due with what you had.  My first system was a Craigslist find that I had to tear down and rebuild with spare parts from other systems.",Neutral
Intel,Seemed like a perfect use case for the sli bridge they got rid of.,Positive
Intel,Why would it need to send the data to the other card? They both feed into the same game.,Neutral
Intel,Big up the Vega gang,Positive
Intel,"That is because destructible buildings and realistic lighting REALLY does not go hand in hand. Realistic looking games use a ton of ""pre-baked"" light/shadow, that might change when ray tracing is the norm but it has a delay so things still look weird.",Negative
Intel,"I remember playing Crysis, flipped a wheeled garbage bin into a hut, which came down on top of it. I blew the wreckage up with grenades and the bin flew out, landing on it's side. Took pot shots at the castors and the impact made them spin around in the sockets.   Here we are 15 years later and game physics have barely moved an inch from that",Neutral
Intel,Cause regular stuff is so easy to simulate or fake simulate that super realistic complex items are not really worth the trouble.  &#x200B;  water still looks like crap in most games and requires a lot of work to make it right.  and even more processing power to make it truly realistic.  Cyberpunk is perfect example.  the water physics is atrocious.,Negative
Intel,It's because it's not the big selling point now and as the comment you're responding to... Things have gone so far no one really thinks about it anymore. Ray tracing is what physx used to be or even normal map and dx9/10 features you don't consider now.,Negative
Intel,"Honestly, same here - I hate Nvidia so much for what they do so shamelessly to the average consumer that I’d struggle to recommend their products even if they *were* competitively priced.",Negative
Intel,"People are just stupid, if game companies just slap ray tracing on and don't tweak the actual look of the games lighting, gaming graphics is doomed. Rt is not the magic bullet people seem to think it is, everything looking 'realistic'=/= good art design.",Negative
Intel,"I thought RT was loved by devs, as it allowed them to avoid the shadow baking process, speeding up dev time considerably?",Positive
Intel,We are guilty of the exact same sin.,Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"All rasterization does is approximate colors for all the pixels in the scene. Same thing does raytracing, path tracing, AI reconstruction and whatever other rendering techniques there are.   The final result is what's most important, doesn't really matter how it's achieved",Neutral
Intel,would you care to explain ? Kinda interested to here this,Neutral
Intel,jellyfish follow simplistic plucky quarrelsome unwritten dazzling subsequent mourn sable   *This post was mass deleted and anonymized with [Redact](https://redact.dev)*,Negative
Intel,"It isn’t minor. The PT with DLSS 3,5 is quite a bit more accurate and reactive, as in dynamic lights are reacting closer to instantaneous than without. The best way is to watch on a 4K monitor or to actually play it on someone’s 4090/4080 or wait for AMDs FSR 3 to see if that helps it do path tracing.",Positive
Intel,> Literally tiles with grainy texture suddenly becomes a mirror  PT doesn't change what the materials are bro. You are just seeing what the materials are meant to look like.,Neutral
Intel,Turn on path tracing. Embrace the PowerPoint.,Neutral
Intel,Control also has some of the best implementations of RT I've ever seen. It's one of the very few games I've turned on RT and felt like it was worth the performance hit. Everything else I've played has looked very good with RT on but not good enough to be worth the performance hit.,Positive
Intel,"Of course it is not perfect but it also allows max RT, PT, RR which improves the graphics greatly...and i do not enjoy below 100 fps either",Negative
Intel,"I tried 1080p DLSS Quality and it was just as bad, but I’m not really happy with the performance of 1440p DLSS Quality (like, 30-40fps), so I didn’t want to use it.",Negative
Intel,"You're is coming off as salty that you *can't* afford a 4090. If other people wanna spend the extra cash and turn on all the bells and whistles, why does that bother you?",Negative
Intel,"Mate, you can't fucking run starfield at good fps, a game that's raster only that looks like from 5 years.  Look at how many games scale and run badly in last two years and compare that to cyberpunk",Negative
Intel,"This just absurd and frankly entitled. Not everyone can afford a PC strong enough to run the most demanding games absolutely maxed out.   I never could, couldn’t even afford a PC until this past summer.   This obsession with performance metrics is ridiculous. Like, less than 5% of PC gamers even have 4k monitors. Almost 70% still have 1080p.   CDPR made a VERY demanding game that looks GORGEOUS completely turned up. Modern hardware cannot handle it natively, it can’t. It is not a gimmick that a company has developed a superior technology that allows the game to be played at almost the same quality as native, significantly smoother and faster. You’re in denial if you think otherwise.  I’m not tied to either brand. I have an AMD CPU bc it was the best option for what I wanted, and a Nvidia GPU for the same reason. I care about RT, frame gen, all of that. The Witcher 3 still is my favorite game, and the entire reasoning for building a gaming PC over a console is graphical quality and speed while doing so. That game is absolutely gorgeous with RT on. Same with Cyberpunk, which I can enjoy thanks to DLSS.   You don’t care about those things? That’s fine, but it is solely a personal opinion.",Negative
Intel,"Yea well I was defining an arbitrary performance. It's not about having native performance. But, but if a flagship is able to get 4k 60 with path tracing at native then that allows headroom for 60 tier or 70 tier cards to perform well.  In this example the 4090 gets 19.5 fps at 4k. The 4060ti gets 6.6 fps. Let's assume a next gen flagship is able to reach 60 fps at 4k. That would theoratically allow the 60ti card to reach around 20 fps. Once you do your various upscaling and frame generation techniques you can then reach 60 fps ideally with a mid range card.",Neutral
Intel,"The quad core showed up the core 2 duos like 2 yrs after because of emerging support and I was on cloud 9. After evga left, i jumped ship with my XFX 6900xt and I think I am here to stay with team Red until value/longevity says otherwise.",Neutral
Intel,Shame you don't. They did it for a reason.,Negative
Intel,The 9800GTX was a bad ass. I'd consider it the biggest upgrade in generation until the 1080 and then the 4090.  It was far more powerful than anything near it.,Positive
Intel,"I do miss it, but it was a pain in the ass to get working with my custom cooler due to the HBM.",Negative
Intel,They've actually gone a bit backwards. Devs dont seem to care about implementing physics anymore. It's just an afterthought. And you can forget about destruction,Negative
Intel,"I gobble up the downvotes every time I say it and I don't give a crap.  Every other corporation sees through their bullshit.  Otherwise every console would have NVIDIA cards.  No console has an NVIDIA card anymore.  But the general population is more than willing to get bullshitted into buying their cards against their own interest.  EDIT: Forgot about Nintendo Switch, as is tradition.",Negative
Intel,"Anyone can say anything.  Anyone can change their intent after the fact.  Happens all the time.   Judge them by their actions alone and ignore what they say.  Going solely by **events** :   1) People use (mostly used) NVIDIA cards for driving PhysX   2) NVIDIA disables this ability, locking everyone to the god-awful CPU implementation at the time   3) Modders edit DLLs to get functionality back   4) NVIDIA THEN says it was a bug that will be fixed    It's not foil hat because I'm coming to this opinion based solely on what's measurable and real; events and actions.   It's not conclusive, but I'm not going to rely on any company's PR statements to sway me one way or another because they will (obviously) say whatever they're able to prevent lawsuits and public image damage.  Based strictly on **events** that looks like damage control on a bad decision.  They rectified it, even more recently made PhysX open source, I give them credit for making it right, but the intent at time time was clear.",Neutral
Intel,This is why I'm excited for Alan Wake 2. It is coming out with path tracing always having been part of it. This will definitely mean they built the art style around that.,Positive
Intel,"Yeah, I can't really blame people for not recognizing great design if they are not interested in it. I don't think most people are interested in design, even though we all benefit from it without knowing it.     People might get some sort of subtle feeling when they see some great craft made by a experienced professional, but they can't really put it into words or evaluate the quality of the design without experience. They likely can't tell what cause the feeling to begin with even if they can notice it.     But everyone can instantly judge how real something looks, which is the go-to standard for ""good"". And bigger numbers are better, even if they are fake. Niche features that nobody uses also sells, as long as it's a new feature with a captivating name.     This reminded me. Live action lion king, what a travesty, stripping away all the artistry and expression for realism.",Neutral
Intel,"If you're using RT exclusively maybe, but no one in the games industry is - nor will they be any time soon.",Negative
Intel,"A lot of raster techniques reduce the complexity of lighting into very naive heuristics. Like shadow mapping renders the scene from the light's pov to create a depth map, then the objects that are at a greater depth have darker colors drawn (I'm simplifying that but roughly how it works). It's like teaching a kid how to illustrate shadows on an ball vs the how light actually works. Raster techniques have evolved a lot since then, and use raytracing in limited contexts, screen space reflections and ambient occlusion for example, but they're still operating with extremely limited data in narrow, isolated use cases, which is why they often look awful. There's just not enough information for them to look correct, just enough to trick you when you're not really paying attention.",Negative
Intel,All lights in a rasterised scene are “fake”.   Someone explains it much better here:  https://reddit.com/r/nvidia/s/eB7ScULpih,Negative
Intel,Or a bag of fake tricks.,Negative
Intel,">Every scene is lit completely differently with PT enabled.  It's really not, and it's very disingenuous of you to try and pretend it is. There are plenty of screenshot comparisons showing the difference in certain areas if you want me to link them, and I have the game myself and use a 3090.  There are differences, but saying ""every lit scene looks completely different"" with PT Vs regular RT is false.",Negative
Intel,"I'm not talking about DLSS 3.5. I'm talking about PT vs regular RT in CP2077. The visual difference is subjective, it looks different, is it better? Not in my opinion, it over saturates the scene.  DLSS 3.5 is a separate issue, it fixes some of the major flaws of PT, the slow reflection updates.",Negative
Intel,Have you tried Metro Exodus Enhanced?,Neutral
Intel,At less than 4K resolutions Quality is the only sensible option. The resolution it upscale from becomes too low otherwise and DLSS then struggles to have enough data to make a good image.,Negative
Intel,"It doesn't, I have no problem with people buying what they want. My problem is people assuming I can't afford something...",Neutral
Intel,What is this? A reasonable comment in this dumpster fire of a sub?,Negative
Intel,Thinking that a $1200 graphics card should be just strong enough to run everything Max is entitled. I wear that crown.,Positive
Intel,"I've got an evga 3080ti now. I suspect my next card is either going to be an XFX 8800xt, or catch an XFX 7900xtx on clearance.",Neutral
Intel,"I don't blindly support a single company, such as simply switching to AMD because my chosen vendors left nvidia. That's how competition stalls.  That being said the card I have now is a product of what I could get my. Hands on during the crypto boom, and is EVGA.  My next card is likely to be an XFX 8800xt/7900xtx.  So not sure why you're in here attacking me.",Neutral
Intel,Except it was just a rebadged 8800GTX/Ultra,Neutral
Intel,"I mean, the Nintendo Switch is using Nvidia graphics, albeit it’s a tiny 28nm iGPU from 2014… but I get your point. They’ve opened up a large technological lead over AMD thanks to the RDNA 3 debacle, and I’m *still* recommending only Radeon GPUs because Nvidia is just that hubristic.",Neutral
Intel,Pixel art go brr,Neutral
Intel,Yeah they will. Lumen and lumen style gi systems will have to exist for fallback so the industry can move on.,Neutral
Intel,"Yeah, I agree. It’s just a shame because it’s not really playable otherwise. DLSS with ray reconstruction is a little mixed.",Negative
Intel,"It just doesn’t make sense. Game developers do not set GPU prices.   The game runs well on a $1200 GPU. Perfectly well. Not completely maxed though, and it’s weird to think the game shouldn’t be able to make full use of available technology if it wants to. My GPU cost less than half that and runs the game very well with RT on thanks to DLSS.",Negative
Intel,"RR is not perfect, but I think overall Cyberpunk 2.0 looks so much better than it did previously - and it looked pretty damn good earlier too!  [Digital Foundry's video](https://www.youtube.com/watch?v=hhAtN_rRuQo) is again the yardstick for how many small things they point out where RR improves the situation, even if it has some things where it fails atm. But maybe DLSS 3.6 will solve those.  I do feel DLSS itself has also improved in the past few years in terms of ghosting and performing at lower resolutions.  On a 1440p monitor these are the starting point resolutions:  - Quality: 1707x960p - Balance: 1485x835p - Performance: 1280x720p - Ultra Perf: 853x480p  So even the highest quality option is sub-1080p. I wish Nvidia introduced a ""Ultra Quality"" preset that would be say 80% per axis resolution, something between DLAA and DLSS Quality. At 1440p this would be 2048x1152.",Positive
Intel,"Speaking of the AMD overlay, am I the only one that gets near zero CPU utilization every time they pull it up? GPU utilization fluctuates up and down depending on resolution and graphical settings, so it seems to be fine, but my CPU utilization readings never go above more than a few percent tops. I have a 5800X3D.",Neutral
Intel,GN has already made a [video about it.](https://www.youtube.com/watch?v=5hAy5V91Hr4),Neutral
Intel,Honestly this is everything that I wished Adrenalin overlay had. It's pleasantly lightweight too.   Hats off to Intel's development team. They didn't need to publish this at all.,Positive
Intel,I wonder if this will get added to Mangohud and Gamescope.,Neutral
Intel,This is quality. Great work.,Positive
Intel,Doesn’t capframeX uses presentmon as its monitoring tool?,Neutral
Intel,I can finally see if it really is the ENB taking down my Skyrim gamesaves,Neutral
Intel,"FYI, the open-source version has been available for years, was last updated 9 months ago, and does not contain an overlay or GPU busy stats. (It also has version number 1.8 while this new thing is v0.5)  Maybe they're planning to release source code later, but right now the source is closed.",Neutral
Intel,This is pretty damn awesome and will be incredibly useful when they develop the future features Tom mentioned. (insightful metrics),Positive
Intel,"This is not bad at all. But it's not replacing Radeon overlay for me... yet.  1. Needs an option to minimize to system tray.  2. Many important Ryzen CPU metrics (temperature, power consumption etc.) aren't available without an ability to access Ryzen Master SDK. 3. Radeon Overlay can continue to monitor on the desktop, which I use constantly. I tried manually hooking DWM.exe and it won't. I might just be dumb, but I couldn't get it the overlay working without 3D running. 4. Will give credit where credit is due, unlike Radeon overlay, it works right with OpenGL. 5. And the credit is immediately removed, because unlike Radeon Overlay, it doesn't work right with DXVK.  6 out of 10.",Negative
Intel,This is so good. The only thing I dislike is that there isn't a way to scale the whole thing up an down.,Positive
Intel,"At this rate Intel will have a more polished driver and software stack than AMD in 2 or 3 years, if not before.  Seriously, I suck for being able to buy an AMD GPU like the 6990 I owned, but I can't get near any hardware they made while using my system for work.  I need a CUDA alternative that works on more than 6 GPUs on specific linux distributions when the red moon shines above my building.",Negative
Intel,it's sort of misleading to call it opensource    when whatever intel beta version released is just MSI installer and binaries inside ...    i see no source of what is available for download ...   what's on gihub is something old w/o overlay,Negative
Intel,Thanks Intel! I will try this out at least since I hate MSI afterburner.,Positive
Intel,The download link gives a warning in Windows:  >This site has been reported as unsafe   >   >Hosted by intelpresentmon.s3.amazonaws.com   >   >Microsoft recommends you don't continue to this site. It has been reported to Microsoft for containing harmful programs that may try to steal personal or financial information.  ???,Negative
Intel,"I really wanted to test with it why I was getting stuttering in Apex. Unfortunately, due to some work, I would be possible on Sunday.",Negative
Intel,Doesn’t work for me. It crashed at the start with an error message and made Dolphin run way worse.,Negative
Intel,And AmD gives far more than Nvidia.,Neutral
Intel,I can't get this to work. Is anyone having the same issue?  CPU Temperature (Avg) NA C  CPU: 5800X3D,Negative
Intel,I have a 5800x3D and PresentMon does not recognize it. Says UNKNOWN\_CPU.  Anyone?,Negative
Intel,People have reported that cpu usage in Radeon software is not very accurate.,Negative
Intel,I saved this because it is posted so frequently lol  https://www.reddit.com/r/AMDHelp/comments/14n55gd/cpu_usage_percentages_differ_between_task_manager/jq66gop?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button,Positive
Intel,"Probably overlay wasn't updated for W11 22H2.  Microsoft updated something and CPU reading was broken for MSI Afterburner also, after working the same for a decade or more before. They fixed it in the meantime.",Negative
Intel,"I had the same problem, googled around and found the problem to be c state in the bios. Just turn that off and it should be accurate.",Negative
Intel,Just use afterburner as OSD.,Neutral
Intel,I had that issue with my 5600X both with Nvidia and AMD overlay. More recently it randomly fixed itself.,Neutral
Intel,"Yeah adrenalin was useless for it. It was displaying 1-2% usage for me, but msi afterburner, hwinfo, and rtss were showing much more, like 30-50, which made more sense.",Negative
Intel,You beat me to it :),Positive
Intel,"Yup, ""new Intel"" definitely seems to be getting nicer and with a ""younger"" company culture. Also, it's a smart move. They're building a nice little ecosystem around Arc cards, which is eventually what is going to drive sales when performance and stability matches NVidia and AMD.",Positive
Intel,And it's open source!  I'm liking Intel more and more since they're getting their ass kicked.  Apparently it was one guy's pet project.,Positive
Intel,Technically it should be possible to add in MSI afterburner because it's open source,Neutral
Intel,"The new PresentMon shows more information.  Since it's open source, capframeX can implement this as well.",Positive
Intel,It was a pet project of one of the Intel engineers.   6/10 is not bad!,Positive
Intel,It's still beta. I'm sure they'll fix it.  Intel has amazing engineers.,Positive
Intel,Intel is working on their 2nd generation GPU and it's said to compete with Nvidia highest tier.  And now AMD isn't planning to release a big RDNA4 chip.,Neutral
Intel,"didn't nvn get merged with mesa recently edit: sorry, I mean NVK not NVN",Neutral
Intel,I hate afterburner and RTSS. This is way better,Negative
Intel,"people probably spammed microsoft because this PresentMon also gives you the option of enabling amd, nvidia, or intel telemetry.",Negative
Intel,"Thank you for contributing exactly NOTHING to this discussion.  Truly, the world is a better place because thanks to you.",Positive
Intel,"Tell that to anyone that needs CUDA. Or GPU accelerated workloads in general.  Id suck for being able to get something like the 6990 I owned back then, but between lack of support for almost all workloads and the driver being shit since like forever... Nope.  Yes, yes, nvidia's control panel is old and looks like a windows 98 app. But it at least works and actually saves youre settings, unlike AMD's one that half the time simply forget about everything.  I currently own 5 systems. 2 nvidia ones, 3 amd ones. And all of the amd ones had driver related issues.  You can't simply work with that. Is not viable. And as a gamer you should not need to see if the drivers are working or not.  Fuck, amernime drivers exists just because how hard amd sucks at it.",Negative
Intel,"Well, everyone uses RTSS anyway and it gives you basically everything.",Neutral
Intel,Similar issue here with an i5 12600kf. Can't get temperature readings from it. I'll need to look at it more.,Negative
Intel,"Really? Good to know it's not just me, then. Nonetheless, this seems like a pretty egregious oversight on AMD's part. If I'm not mistaken, CPU metrics are on by default in the overlay and every time someone switches it on, it's an extremely visible reminder of work that AMD still needs to put into the drivers.",Negative
Intel,Speaking of said software why does it keep setting gpu max frequency to 1200MHZ?,Neutral
Intel,"Thanks for sharing your findings. It's pretty damning if even the Windows Task Manager does a markedly better job than Adrenalin. That's probably an understatement since, as I found out via other helpful Redditors here, the Adrenalin overlay may as well be broken. I'm just glad it's not something wrong with my system.",Negative
Intel,Afterburner fucks with my settings in adrenaline,Negative
Intel,"NVIDIA's highest tier? Nah, but around RTX 4080 is the target, which might be bad news for AMD.",Negative
Intel,"By NVN do you refer to Nintendo's API for nvidia? Or you wanted to refer to NVK that got into mesa?  By CUDA alternative I mean stuff like ROCm. CUDA have so many years of development on top of it and a stupidly widely support from software vendors.  While yes, NVK could help to alleviate the issue, is not even close to what native CUDA can do regarding GPU usage for general computing.  The issue with ROCm is mainly a lack of support on a lot of software and lack of support for a lot of hardware.  On the nvidia end you can get the cheapest trashiest GPU and it will have the same level of CUDA support as the most expensive enterprise product.  Yes, performance is not the same, but I can use a 3090 Ti for loads that needs more than 12 GB vram without needing to buy a dedicated AI GPU, and if a given task needs serious shit, then again, I can buy a dedicated solution that will run the exact same software with the exact same code I ran on the ""weak"" 3090 Ti.  That is not possible with AMD, if you buy an enterprise grade GPU from them, you need to build all your software stack for it instead of just moving it and keep going.  And while yes, developers from tools like PyTorch COULD improve their ROCm support, I totally get why they are not doing that. ROCm works on like what? 7 GPUs? On specific linux distros?  Is a matter of scalability and costs. ROCm needs to be supported on ALL AMD GPUs in order for it to be something worth developing for, otherwise there is not really a use case where the end user wont be writing their own stack.  And again, if you can start small, then scale up, nvidia is the only option, so AMD on this space is either ""I got a stupidly great offer where AMD provides the same hardware power and performance per watt at like half the prize so I justify building all my software from scratches"" or ""Ok, I'll go for nvidia and just use all the software already built for it and call it a day""  I really, really want to being able to use AMD GPUs aside of trivial tasks, but this is a very, VERY gigante issue that appears a lot, especially on the profesional space.",Neutral
Intel,Except PresentMon seems to not be compatible with Ryzen CPUs at all... :/,Negative
Intel,I get downvoted for asking a valid question?,Negative
Intel,Thank you for continuing to contribute Nothing to this conversation.,Positive
Intel,Clearly not more than this beta of presentmon,Neutral
Intel,I use amds for overclocking and that's about it really. But it's still far more tools than nivida gives,Neutral
Intel,"To defend AMD here as loathe as I am lately. CPU utilization isn't a very useful metric to begin with, so it misreporting would be super low priority.  The stat tells you nothing useful about the hardware, just the thread scheduling. CPU could be stuck in wait and reporting a high number while doing no work or it could be bottlenecking somewhere else while reporting a low %.",Negative
Intel,Where are you seeing this?,Neutral
Intel,"The Adrenalin overlay is good for really everything except usage. But yeah, this has seemed to have been a problem for a good while now. Ive been using AMD for a little over a year now at least Adrenalin and the CPU usage was always wrong on two different CPUs (AM4 and AM5) as well. Me and that other person tested it on my system and he had 3 systems running different AMD CPUs and all were the same. Ill find the thread one day but its been a while so it is way down the list of my comments.",Negative
Intel,"False setup.  Disable the button in the ui ( not settings)  For ""load on startup""  You literarily told afterburner to load it's settings on start up.  Huge mistake many people do because most people think it's ""boot on startup"" but in reality it's loading it's oc settings and stuff effectively overwriting adrenaline if you leave that enabled.",Negative
Intel,3070 Also was their target for their first generation. I don't expect Intel to suddenly be great just because that's what they aim for.,Neutral
Intel,And XeSS 1.1 looks better than FSR.  XeSS 1.2 was just released and it has even more improvements.,Positive
Intel,Great news to gamers though,Positive
Intel,ah sorry I meant NVK,Neutral
Intel,"I don't know, I didn't downvote you.",Neutral
Intel,My point is if you've got nvidia card you simply use 3rd party soft and got everything and more. So comparing that AMD's software to Nvidia's is kind of meaningless.,Neutral
Intel,"I actually agree with you, but I don't need the metric to be some scientifically accurate reading of performance or whatnot, I just think it'd be a useful tool for adjusting game settings. For example, if I'm trying to optimize performance in a game, does this setting make CPU utilization go up or down? How much almost doesn't matter. Is there a bottleneck, where is it, and am I relieving pressure on it with these settings or not?",Neutral
Intel,It’s where you oc in adrenaline,Neutral
Intel,"There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Not because upscaling, but also because how important is to have drivers that work in tandem with specialized hardware units in the GPU.  Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  AMD fails hard on 2 of those things.  Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.",Positive
Intel,"Yep, XeSS is just great, I happily use it in MWII on my Arc A770. But I still use DLSS on NVIDIA. But both are better than FSR.",Positive
Intel,"Then you should be looking at ***GPU*** usage, because CPU usage will not help you answer any of those questions",Negative
Intel,"Problem is, by itself it still tells you little to nothing. If GPU utilization drops like a brick you know something is bottlenecking. If CPU utilization changes in the absence of other data you know absolutely nothing of value.   The real kicker is the CPU utilization may not even be ""wrong"", it may just be polling at a very low rate because that's the other thing with these OSDs if you poll too frequently for up-to-date data that can kill performance from the overhead.",Negative
Intel,"> There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Agreed, not to mention that when they made Alchemist, yes their target was 3070 and they underperformed that, but they will eventually hit their targets as they learn to improve their architectures. For a first shot, getting to around 85-90% of their target is actually quite good. It's not like they undershot by 50% of the target. They got close.   But like you said, software is super important and these days having good software is half of a driver. I have an A770, the driver actually pretty good, like the software menu. It used to suck when it first came out because it was some crap overlay, but over time it's become a separate program and it works really good considering they're not even a year into Arc's release. They also have a good set of monitoring tools built in and the options are all simplified. They don't have super unnecessary things like a browser integrated into the driver suite, or a bunch of other stuff. I'm sure by the time Celestial is around if Intel does stick with Arc, that they will be ahead of AMD on the software game, assuming AMD continues down the same path they have now.  > Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  Yep, tools like [this one from NVIDIA](https://youtu.be/-IK3gVeFP4s?t=51) have made it easier than ever as a dev to optimise for NVIDIA hardware.  > AMD fails hard on 2 of those things. Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.  Yep because Intel recognises that to get close to NVIDIA they not only have to make great hardware, but also great software to go along with it. You can only do so much with hardware to match NVIDIA. You have to make great software along with in-game solutions like XeSS to match NVIDIA. FSR is behind not only DLSS but now also XeSS. AMD is just not competitive with NVIDIA on all fronts. Arc is just getting started, give Intel 5-10 years of time in GPU and they will outpace AMD for sure.",Positive
Intel,"There are many settings that directly impact CPU usage, so of course CPU utilization reading can be useful.  Not everything needs to be precisely accurate, this is a lousy defense. Getting 0% reads all the time is entirely worthless but some accurate reads is still useful information.",Neutral
Intel,"You are going out of your way to defend a broken feature.    7800X3D shows up as 1% CPU utilization during games such as Watch Dogs, Stellaris, Age of Empires. Clearly broken. Undefendable.",Negative
Intel,"And with a bit of luck, they will also hit AMD in the console market.  Right now AMD stays afloat GPU speaking because consoles, they were during a lot of years the only vendor offering both CPU and GPU in a single package, reducing costs A LOT.  Nvidia was not able to compete against that with the lack of CPUs and Intel with the lack of GPUs.  Now that Intel is working on dedicated GPUs they are going to eventually create APU like products, its just the natural outcome of producing both CPUs and GPUs.  And then AMD will NEED to compete against them instead of being the only option.  And if that happens, unless FSR gets turned into a hardware specific tech like XeSS that falls into different techs depending on the GPU its running on, AMD will be murdered.  Better software stack, better general hardware to accelerate and improve upscalling keeping costs down AND being a second player pushing prices down towards Sony and Microsoft?  AMD will be screwed up if they don't up their game. And hard.",Neutral
Intel,"Right, so what kind of useful information can you tell me if I raise graphics settings and CPU utilization goes from 12-16% to 16-17% while framerate goes down from 120 fps to 110 fps?",Neutral
Intel,"All CPU utilization tells anyone is thread scheduling, and nothing about what the threads are or aren't doing. Like the only situation that it marginally tells someone anything is how many threads a program uses. But that usually doesn't fluctuate with games, games usually scale up to <x> number of threads and that is that. GPU utilization is the one that is far more useful for spotting bottlenecks, general performance observations are far more useful for tweaking.  Even Intel and co. will tell you that with modern processors and all the elements they contain as well as hyperthreading/SMT CPU utilization isn't a very useful stat anymore. Meant a whole lot more when CPUs were one core unit and didn't also include the FPU and memory controller and everything else.",Neutral
Intel,"Cause unfortunately even if it wasn't broken, it's not useful in any meaningful capacity. Should it be broken? No. Would anything notable come of it if it were fixed? Also no. Given the bugs they got and room for improvement elsewhere it's one of those things that should be super low on the priority list or just removed wholesale.",Negative
Intel,"That whatever setting you are changing does not have much of an impact on CPU utilization, and if your GPU allows, you can use it.  What is the purpose of this question, are you really not aware of any settings that impact CPU usage? Some RT settings have big impacts on CPU usage too.",Neutral
Intel,"The problem you're missing here is that settings that are CPU demanding generally don't have an easily observable impact on CPU usage, if they have any impact at all.  The reliable way to see if a setting is CPU demanding is to turn it up, see the frame rate go down, and then look at how **GPU** usage changed.",Neutral
Intel,"I'm aware that some settingd impact CPU usage, but I think looking at the CPU usage is about as useful as your daily horoscope, or AIDA64 memory benchmark",Neutral
Intel,Congratulations on the new gaming laptop. Don’t let people get you down and not everyone has to have the very best to be happy so if it makes you happy then I’m happy for you.,Positive
Intel,"Congrats the 30 series isn't half bad. Laptops aren't bad but gaming desktops add more umph to their performance. My first gaming rig was a gaming desktop with 64gbs ddr4 3200mhz, rtx 3060 12gb, 850w platinum psu, b660m gigabyte motherboard and a intel i5 13600kf. Couldn't have had a better first rig.my current build now is a hellhound raedeon rx 7900xt,gigabyte  z790x ax gaming motherboard,  850w platinum psu, 64gbs ddr5 6000mhz,Samsung 980 1gb ssd, intel i5 13600kf(watercooled).best upgrade I made for 1440p gaming.",Positive
Intel,"Congratz!  Also, what's that wallpaper? Looks awesome",Positive
Intel,I don't understand the stickers. Ryzen CPU Check. Both Radeon and GTX graphics? Is one discrete and the other dedicated? Just needs an Intel sticker to make it more confusing.,Negative
Intel,"That is not a ""gaming rig"" not even close my friend. ;)",Negative
Intel,3050ti?  Gaming rip?  No,Neutral
Intel,"5 or so years ago i built my first real gaming rig. Few years later i bought new laptop, because i already had a gaming pc i wanted something capable to game but didnt want to spend too much money for high end. So i bought Lenovo legion with same gpu - 3050ti.   What i soon realized was that i love the comfort of being able to browse the net and game from the couch, being able to put my legs up, put my back against a big pillow etc... Despite having much more capable pc, i was lazy to play on it and was using gaming laptop instead 90% of the time.   Problem is, 3050ti in the last year or so really starting to show its limitations, most of new games are straight up unplayable anymore. Even on lowest settings it doesnt run newest games at playable standards, probably because of only 4gb of vram. Hogwartz, Rachet and Clank, Remnant 2 all pretty much unplayable unless you drop resolution to 720... I really regret not putting few hundreds more for something like 3070. (unrelated, but why stepping up laptop gpu tiers are so expensive, for example 3060 to 3070 is like 300e here for essentially same system otherwise, 3070 to 3080 is like 4-500e extra more, its even more expensive overall than desktop gpu tiers for way lesser performance improvements...).   So my advice would be, if you can get at least 3060, its worth it to pay up a bit more, ideally 6600m or 4060, those few hundreds increase in price will be worth in a few years.  ALSO: not sure if OP knows, but these laptops come with shittiest ram, upgrading it to a dual channel dual rank is pretty much a must as it nets 10-20% more performance.",Neutral
Intel,There is nothing wrong with laptops I'd build my own if there was a market readily available for it like a desktop.,Neutral
Intel,No!,Neutral
Intel,"This isn't exactly what I'd imagine hearing the words ""gaming rig"" but I'm glad you like it. Happy gaming!",Positive
Intel,Ignore the haters dude. Congratulations on your new gaming laptop. Gaming is something that is supposed to make us happy not cater to snobs. Not everyone needs 4k 60fps. If it satisfies your requirements it's the best rig for you. I have a Ryzen 5 2500u and I game on that so I am super happy for you.,Positive
Intel,"People take things too seriously, and tend to forget that every enthusiast was once a novice.  This is OP's first gaming setup; they deserve to feel that excitement.",Positive
Intel,who even buys the 'very best' laptop anyway?,Negative
Intel,Lenovo Ideapad Gaming 3,Neutral
Intel,"It's just the default one that came with the laptop, theres a red and a blue version probs from AMD and Intel lol.  You should be able find them online.",Neutral
Intel,Its a ryzen cpu with integrated radeon graphics and an nvidia dedicated gpu. Its completely normal and not really confusing.,Neutral
Intel,APU + dGPU,Neutral
Intel,"You know what I mean.   It's my very first PC/Laptop set up so I'm definitely calling it a rig, it's way more powerful than anything I've owed before it.",Positive
Intel,"It games, it's a gaming rig. :)",Positive
Intel,you do realise not every wants or needs a 4070 to play games they like,Neutral
Intel,I upgraded the ram. Has 16gb ddr 5 dual channel now,Neutral
Intel,"Thanks dude! I think people forget that not everone needs the same kinda thing from a computer which is why they sell multiple models.  This is great for what I need it for, and I feel like a kid at Christmas, happy to be able to dip my toe into PC gaming, finally!",Positive
Intel,You get my point at least you should,Neutral
Intel,"Just to let you know , these laptops have different versions , one I have is something like Ar15mhqp or some shit like that   It is less powerful than yours but decent enough",Neutral
Intel,That is okay if you see that way and congrats.,Positive
Intel,"There's nothing ""rig"" about a laptop. Laptops aren't built to be rigged.",Neutral
Intel,"No, it's a notebook.  Also have a low end gpu dude.  Not a game rig. It's nice but is not that.",Negative
Intel,"I know, but the same laptop or any other with 6800m or other AMD gpu would have been a much better choice lol",Neutral
Intel,All that matters is that your happy with it,Positive
Intel,Glad you’re enjoying it. People something think that unless you get something with everything maxed out then you couldn’t be happy lol,Positive
Intel,"Yeah I'm just happy to be able to start playing around on some games on PC now. I've wanted to play flight simulator on pc instead of my xbox for ages now becuase of all the addons,and now i'll have access to those :D",Positive
Intel,"3050ti isn't really low end. And it will do for everything I need it for. I have an Xbox too for the most modern stuff.  People think that everyone has to have 4k 60fps in every game these days and not everyone needs or wants that, especially in a first machine.",Positive
Intel,"for you maybe, this fits my needs perfectly.",Positive
Intel,"Yup I am more than happy!  I've only ever had and played with integrated graphics on any other laptop I've had so this is a huge step for me.  I will play most brand new stuff on my Xbox for the best ecperience but stuff like KF2 I can now play a 1080p ultra and get 100s frames, could only play that before at 900p low.",Positive
Intel,Well you say that right now but one day a game you really wanna play will come and your pc will be shitting itself trying to run it. If I had the money I would buy the best rig.   But if you don’t have a lot of money like me it’s all about price/performance,Negative
Intel,That is the important that you are happy but still the reality is one.  Just enjoy the notebook.,Positive
Intel,"not too similar but I can recommend grabbing dirt rally 2.0 on a steam sale with all DLC for 8€ / $10 or whatever it is, generally games that simulate some things are pretty much at home on PC",Neutral
Intel,"Yes, the 3050 ti is a low end card.",Neutral
Intel,I have an Xbox too so I'll be able to play it on there anything that is out of spec.,Neutral
Intel,I have a friend playing Baldurs Gate 3 on a 980/ You don't know what low end is apparently lmfao,Neutral
Intel,It's better than the 1650 and the 2050 though. And they were the other choices.,Positive
Intel,And that is normal the game don't requiere a big gpu to run the game is a mid / low graphics.  Besides the 980 was the top of that generation. The 3050 is the low end of that particular generation.,Neutral
Intel,"Dont let the haters bring you down dude, if you are happy, then enjoy your new laptop!",Positive
Intel,"Both cards are the lowest version of his generation.  Again the notebook is nice but is a low end ""gaming"" notebook and again the point is that you ENJOY IT and play many games.",Negative
Intel,He needs to know what he have but also I congrats OP for your purchase. The point is that you enjoy it.,Positive
Intel,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Positive
Intel,"""we are tending to prefer e cores now when gaming""   That's very surprising",Neutral
Intel,I’m guessing Xe3P will be on Intel 3-PT.,Neutral
Intel,If only they release a high level C-card. Battlemage kinda never scratched the mark.,Negative
Intel,Cool hope they will manage to release it soon not in 12 months when everyone will be talking about RTX 6000 and RDNA5/UDNA or leaks about them.,Positive
Intel,"Tom is a funny guy, love it when he gets camera time",Positive
Intel,"The point of Xe3 being actually battlemage instead of Celestial, this is so horribly confusing. I can't understand what's going on, nor why they would do that.",Negative
Intel,"e cores are surprisingly powerful now, and games are getting more multithreaded. It might be better to spread them out over 8 less power hungry cores than 4 P cores and then spill over to the slower cores. Especially in a laptop environment where efficiency matters and the GPU is usually holding back gaming performance so much that CPU performance is really not that important.  In fact, I bet it's mostly down to not spilling over to e cores from P cores. That's what causes slowdowns and stutters usually attributed to e cores. The engine has to wait a little longer for workloads assigned to e cores than to P cores. The if the main thread is assigned to a P core that's all well and good, but if sub tasks are distributed among the P cores and then something important is assigned to a slower e core it holds up the sub tasks and in turn the main thread. But most importantly, it doesn't do so evenly. Maybe you don't need something assigned to e cores, or the e core task is light and doesn't hold up the other threads. Then things will run at the pace of the P cores, but every now and then you'll have a slowdown.",Neutral
Intel,Wonder if there's some energy star requirements.,Neutral
Intel,"Shouldn't be. That's what the math has always said.  As soon as software can scale to ""many"" cores, the tradeoffs that go into single powerful P style cores are a bad deal. Both frequency and sequential optimizations (like multi-level branch prediction) scale poorly.  Gaming benchmarks tend to have a really strong feedback loop that favors last gen hardware design though. So seeing the benefits to E cores for gaming requires 1.) e-cores exist for a while 2.) some games optimize for them.  Long term, an optimal Intel-style processor design will look something like 4 ""legacy"" P cores + dozens of E cores.",Negative
Intel,It's either N3P or 18AP.,Neutral
Intel,That’s why rumors want Intel to throw away P-cores (Israel team) and only keep E-cores (Austin) to make Titan lake a unified core with only E-cores remaining.,Neutral
Intel,"Really should focus on the Main/Sub thread part. Most games will usually only load up 1-3 cores, with the rest of the cores only used for incidental workloads with lower priority and sync requirements (Multithreading is hard yo, Multithreading with latency requirements is mindnumbing).  This makes plenty of games very suitable for the P/E architecture, as long as you have enough P cores for the Main threads, the E cores will be perfectly sufficient.",Neutral
Intel,>and games are getting more multithreaded.  Can you give me a few AAA examples of modern games which get a noticeable improvement in AVG. FPS or 0.1/1% lows when using more than 8cores/16 thread's ? I'm genuinely curious which games you are talking about.,Neutral
Intel,"Energy Star probably won't be around for too much longer, at least in it's current form.  https://www.npr.org/2025/08/13/nx-s1-5432617/energy-star-trump-cost-climate-change",Neutral
Intel,That makes me imagine in a different reality if Xbox had remained with Intel and NVIDIA and their next generation would use an Intel E-core based SoC with NVIDIA chiplet.,Neutral
Intel,"Most games of the past will only load up a few cores, but that's beginning to change. Cyberpunk 2.0 loads up 16 threads/cores easily, and some others like Battlefield 6 also scale pretty well. If you have less than 16 threads on fast cores like Panther Lake and Arrow Lake, then you can run into issues. Or if your 16 threads are on one die and any spillover has to go across the SoC die like with AMD and probably Nova Lake.",Neutral
Intel,I'm tired of AMD slop consoles and handhelds,Negative
Intel,Seems like Intel chips make way more sense for portable pc consoles than AMD going forward.,Positive
Intel,"Transcript for the 'What's Next' section:  >What's next? Well, I I would say uh larger investments in graphics and AI. And so if you imagine in a PC, this workload is not going away, right? The world is becoming more graphical. The world is embracing AI and more rapid than any other technology ever. So I would expect Intel to respond to that, right? It's not like we have to guess what is what is big and what is coming. We know and what's coming is more AI and more graphical experiences.  With this being said, I hope that means that the Nvidia/Intel deal means that the ARC division is not going away any time soon.",Neutral
Intel,"Panther Lake with Xe3 without any doubt will be generational uplift. Intel Lunar Lake which is last year chip still able to put Amd newest chip like Z2E to the shame.   Xe3 with 12 Xe cores not to mention with XeSS XMX is going to be terrific combo, seeing how disappointing Amd Z2E i can see Intel going to steal iGPU gaming market from Amd. Next year could ended up with more OEM using Intel chip for handheld.",Positive
Intel,I found it interesting that he specifically mentions improvements in performance per area for Xe3. A big complaint of Xe2 is that the die size is noticeably larger than a similar performance GPU from AMD or Nvidia. Hopefully this means better financials for Intel’s graphics division with this new architecture.,Positive
Intel,Hopefully Intel finally wakes up and challenges Nvidia. I wonder if that’ll ever actually happen.,Neutral
Intel,Says nothing about discrete graphics. Think that's clearly dead at this point.,Negative
Intel,"Lmfao now I’m feeling conflicted about my very recent MSI Claw purchase, but I suppose panther lake handhelds will likely launch middle of next year anyway",Negative
Intel,"Tom says that variable register allocation and 25% thread count per Xe core will improve utilization which was a problem with predecessors. He says they've been ""addressed"". It looks like at least perf/power wise it's 25-30% at the same process compared to the predecessor, and that may be true for perf/area as well.",Neutral
Intel,Sounds like you're clearly wrong.,Negative
Intel,"I know how you feel but honestly Claw 8 AI+ is still amazing handheld, at least your Claw will aged better than mine because i only have A1M Ultra 7.   Claw with Panther Lake will be released in between Q2-Q3 2026 so you don't have to regret anything.",Positive
Intel,"I get the feeling that handhelds with 12 Xe3 cores are going to be really expensive, so maybe you still didn't do too badly.",Negative
Intel,"Lmao, sure. Any day now...",Positive
Intel,I don't think so considering how expensive LNL was,Negative
Intel,What should be the desktop gpu equivalent?,Neutral
Intel,they should really do better on GPU,Neutral
Intel,"Why is panther lake igpu simultaneously Xe3 and Arc-B series ? This is so confusing.       Also it states being 50% faster, but it also has 50% more compute units.",Negative
Intel,The problem is how much is the price?,Neutral
Intel,"Despite the vast improvement more Xe3 cores than we saw Xe2 cores in Lunar Lake, we're only seeing minor gains in average framerate performance at the same wattage based on the internal benchmark, though there are much more meaty improvements in the 1% lows. I guess its bottlenecked by its RAM bandwidth. Also I'm sure Xe3 will scale better beyond 17W. I wonder how the 4 Xe3 SKUs will fare, that's a good way of knowing how much RAM bandwidth affects things.",Neutral
Intel,RTX 3050 Max-Q or 1660 ti Max Q laptop card. Within that range.,Neutral
Intel,"For them to do a 'big iGPU' design they would need to do a few things:  1. Use a different socket.  The iGPU will get too big for the existing one.  2. Upgrade the memory bus to 256bit.  That uses some power and silicon.  3. Add cache to the chip for the iGPU to use (to make up for the poor bandwidth they get from LPDDR5, even with a wider bus)  If they don't do this then the chip (which again won't fit in their default socket) will be massively bandwidth constrained to the degree that it's pointless.  Basically, do all the stuff that Strix Halo did.  Issue is Strix Halo didn't sell very well, the idea is a proof of concept and the concept needs faster memory to really work well.  LPDDR6 might help a lot at getting these big chip ideas from a XX60 level to a XX70 level, but they still require a lot of expensive chip work and a new motherboard design custom to the big socket.  I suspect we'll get there eventually, but it'll be a few years before people start calling it quits on dGPUs.",Neutral
Intel,Should be much cheaper relative to Lunar Lake now that they've moved away from on package memory,Neutral
Intel,"50% is not minor...  Also they said >50%, or greater than 50%. It looks like 70-80% to me.  Since scaling isn't linear I expect 4 Xe3 to come close to Xe2 or even equal it.",Neutral
Intel,"Damn, still behind my old 1060 pc",Negative
Intel,And some of the tiles are internal not on TSMC.,Neutral
Intel,"No I'm talking about the internal game benchmarks they shared. Yeah the GPU architecture  itself is a lot better on paper, but they probably got bottlenecked by the low bandwidth of 128-bit LPDDR5. The uplift in 1% lows in some titles is impressive though.   [power-efficient-gaming-26.jpg (2133×1200)](https://www.techpowerup.com/review/intel-panther-lake-technical-deep-dive/images/power-efficient-gaming-26.jpg)  And yeah I'd not be surprised if 4 Xe3 is going to be closer to the 8 Xe2 in LNL than people would expect.",Neutral
Intel,Well it's not out yet so these are just predictions. Better to be conservative with estimates than overpromise.,Neutral
Intel,"Buddy, that's 140V with optimizations. They are telling you what they have done to further optimize their graphics, including drivers and power management. and that they'll further apply this in the future. Did you really look at the slide?",Neutral
Intel,"This benchmark has nothing to do with panther lake, it's about the effect of their power management software on lunar lake before and after it is applied.",Neutral
Intel,Isn't that lunar lake before and after?,Neutral
Intel,It has around 120GB/s of bandwidth.  Thats pretty close to the 6500XTs bandwidth.,Neutral
Intel,I was missing the context. Thanks for the heads up! Yeah that last update for LNL driver was mad good.,Positive
Intel,Oh really? Wow thanks for the heads up. I really missed the context.,Negative
Intel,Apparently so! I made a mistake.,Negative
Intel,It's 153GB/s.  9.6GT/s x 128 bit width x 8 bit/byte.,Neutral
Intel,\>HUB  That's shovelware and I'm not clicking that shit.,Negative
Intel,"They've tested it with just three AMD cpus. Weird pairings. I would pair B580 with any i5 from last couple years. Seriously their testing is getting less and less ""real life"" and more for the purpose of a good clickbait.",Negative
Intel,"I couldn’t believe the video I was watching, the thumbnail was pure clickbait. They fixed one game, and improved some others a little.  And NEVER an Intel CPU in sight. I like the B580, but it deserves better ""testing"" than this.",Negative
Intel,The problem is still here. 9060 xt with 5600 is faster than B580 with 9800x3d and B580 still loses 8% with 5600 instead of 9800x3d. Which means that cpu bottleneck starts much sooner for ARC gpu.,Negative
Intel,"Is this related to historically Intel compiler not compiling code efficiently for AMD cpus? Does it also make Intel CPUs overhead better? If yes, then they really fixed something. If no, then they wrote code normally for AMD cpus as well as Intel cpus.",Neutral
Intel,"the B580 is a modern gpu, it should be paired with a modern cpu, preferably an Intel one, going with old school Ryzen cpus is a bad path to take.",Negative
Intel,"The problem is that this title makes it sound like they fixed the cpu overhead issue in every game, but that's not the case. Some games are still affected. Obviously any fix is better than nothing, but depending on what games you want to play, you will still see an overhead issues. It is nice though that in some of the games the overhead problem has been addressed though.",Negative
Intel,"Haven't watched the video, but damn.   If rue, HUB has officially become clickbait garbage. Not outright lying, but intentionally conveying misleading half-truths. Starting at least with the AMD fine wine video.",Negative
Intel,"It's meant to test low end, mid and high end.",Neutral
Intel,"The exact CPU model is hardly relevant. The point was to show CPUs of a certain power level. Not sure how this makes it less ""real life"" as if all three CPUs shown weren't extremely popular for their time. Also not sure what one would achieve when using Intel CPUs specifically.",Negative
Intel,"These guys absolutely *refuse* to test with an Intel CPU, it's very weird.",Negative
Intel,"5700x would have made a lot of sense compared to 5600  12400f,13400f should be included as well",Neutral
Intel,Exactly. Its nice that some games were fixed but between that and calling the issue (entirely) fixed there is a big gap.,Negative
Intel,"Indeed and it is something very interesting, because the reason the driver overhead is an issue on 6 core cpu's is because modern games use all 6 cores, so if the driver has a big overhead the CPU has nu free cores available for the driver.  Especially Intel CPU's with more (e-)cores in their budget CPU's should probably suffer a lot less.   I would expect an i5-12400 to suffer the same as a Ryzen 5 5600,  But an i5-13400/14400 has an additional 4-e cores to deal with the overhead. The 13500/14600 even has 8 e cores.      I would very much like to see the comparison   i5-12400, i5-13400, i5-13500, Ryzen 5 5600(X)",Positive
Intel,Would not have mattered. the cpu bottleneck in the intel driver is single-thread limited and largely due to draw-calls being stored in system memory as a buffer for the frametime render. (at least as far as my debugging could lead me)  (that's also why its less impacted on ddr5 systems no matter what CPU),Neutral
Intel,"They pushed the 6 core parts too hard as ""good budget parts"" for too long to not keep using them as their reference...  Even when we're seeing games that now have strokes when they don't have 8 cores to play with.",Negative
Intel,"possibly. Honestly weird as hell to not through in intel cpu results with the same gpus so they can see if it's a all cpu overhead issue, or an issue with amd cpus with intel gpu. Like even one game if intel/amd gave about the same performance you could likely rule it out somewhat.  Also weird to say they fixed it if by the sounds of it to say it's fixed if a very limited number of games were updated to work better in also seemingly limited scenarios. That's more like finding a specific problem on a specific game they were able to improve rather than an underlying fix which if implemented should help everywhere.",Negative
Intel,"No, B580 also performed poorly on i5-8400 and 10400 (both are 5600X-esq)",Negative
Intel,"The driver compiler is for GPU, not CPUs.",Neutral
Intel,9800X3D is the fastest CPU for gaming on the market. The usage of Intel or AMD CPUs doesn't inherently matter.,Positive
Intel,It's been official for awhile. Funny how it takes a positive Intel title for people to realize.,Neutral
Intel,Do you have peer-review data that disputes the data they presented?  Their video covers the scope of what got fixed.,Neutral
Intel,Welcome to the age of influencers...,Neutral
Intel,That's not how CPUs work in games.,Negative
Intel,Then they optimized drivers.,Neutral
Intel,Do you have peer-review data that confirms it?,Neutral
Intel,The video shows the B580 not having driver overhead issues with the 5600X but we don't know if it did previously on anything other than Spider-Man. His tone suggests the 5600X performance used to be worse but he provides no data other than Spider-Man to suggest it was for other games.,Neutral
Intel,The fact that a switch from a 5600 to a 5700 with hardly a frequency gain eliminates the overhead issue suggest that is kind of  how games/drivers work.,Neutral
Intel,Which also fixed performance on 5600X,Neutral
Intel,"But that's now how games utilize CPU's.  What you get from games is usually 1 main thread that takes up one big core, then many less intense threads, that may or *may not* utilize the other cores, and to varying degrees. AKA you can have mainthread use 100% of core 0, but cores 1,2,3,4,5 are all 50% used, leaving 50% for other tasks to run freely.",Neutral
Intel,"Eventually you get the drivers where they need to be. People love to whine about Arc drivers but on the iGPU side people never complained. The community is never, ever happy with anything. Focusing their eggs on the Higher performance parts and then a slower LTS cadence keeping the old stuff alive is the right move.",Negative
Intel,"They will update, just not day 1 for certain games. Who's playing with integrated graphics?",Neutral
Intel,"cant say i blame them for scaling down driver updates for pre-arc graphics hardware, ever since they announced intel was getting into gpus proper ive been wondering how long it would take before this happened.  lets be honest no one was needing monthly igpu updates for 2021 hardware that was so underpowered it couldnt even tie with a GT1030 ^(\[why does everything come back to 14nm\].)",Negative
Intel,Running an N97 Mini PC with integrated and noticed the recent driver branching. I'm still impressed that Star Trek Online is playable on it.,Positive
Intel,Developers have been complaining about iGPU drivers for years. Matrix multiplications in OpenGL driver are still broken on a lot of integrated chips. It's driving me nuts every time I get a user report. 😭,Negative
Intel,Get some ARL CPU’s which have matrix math in hardware.,Neutral
Intel,"purchased a qualifying intel cpu with the Newegg bundle last month, part of the promotion was a free copy of battlefield 6.  Intel messed up the offer by not providing a code that aligned to the sale, which they then re-issued the coupon codes.  My problem is now the BF6 code does not work and I’ve raised it with customer support who have not responded.  Anyone have similar issues and got a resolution?",Negative
Intel,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Negative
Intel,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Negative
Intel,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
Intel,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
Intel,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Neutral
Intel,Thank you,Positive
Intel,Keep em coming.  I saw Intel was hiring for a GPU stack software engineer in Toronto. I expect more good things for the Boys in Blue.,Positive
Intel,Does newegg have a pro or business section for the B50?  I heard CDW or similar outlets may start selling B50 / B60?  I want to look into four B60s for an AI workstation but the B60 may not release through consumer chains.  Has anyone seen the B60 for sale?,Neutral
Intel,Quad mini display port ? WTF,Negative
Intel,It is so much weaker than the B580 though with only 70w... Very niche use cases.,Negative
Intel,On the other hand noone buys workstation GPUs on NewEgg. For B50 being bestseller means 95 orders.,Neutral
Intel,Looks like some stores have it available to order currently. Central Computers has it available as a Special Order.   [Central Computers -ASRock B60 24gb Creator $599](https://www.centralcomputer.com/asrock-intel-arc-pro-b60-creator-24gb-graphics-card.html),Neutral
Intel,Better than micro-hdmi.,Positive
Intel,The entire point is that you can power it from the motherboard.,Neutral
Intel,It doesn't matter. The card sells and that's what matters.,Neutral
Intel,Its a small workstation card like the Quadro 1000 etc..,Neutral
Intel,I guess I got lucky. I was able to get one off Newegg before they sold out. Should be arriving in the mail today.  Hopefully they will get more in soon. I'm surprised Newegg had so few of them.,Positive
Intel,or the usual supply chain is sold out so users buy them where they are available,Neutral
Intel,Those are professional cards. Your company should be getting them from different sources.,Neutral
Intel,"Most people buying these don't own or buy for companies. ""professional cards"" means nothing. if you have a use case, then buy it. Companies like Intel *love* consoomers like you.",Negative
Intel,"It's small but a decent performer, if you ask me. Intel needs to keep them coming.",Positive
Intel,A single slot version would've been nice.  70w doesn't need a dual slot cooler.,Neutral
Intel,12W idle power... here does the dream of low idle server... :I,Neutral
Intel,Where can I buy one?!,Neutral
Intel,"I should have waited, i could have gotten two!",Neutral
Intel,Where can we buy this? Seems like limited supply and retailers are going to try and raise prices. All the tech companies need to keep the retailers under their boot.,Negative
Intel,"I know it's not for this, but how does it do at gaming?",Neutral
Intel,I bet this thing is great for Plex transcoding,Positive
Intel,they need if they want to survive,Neutral
Intel,I was thinking opposite ... can we have full passive model?,Neutral
Intel,nvidias alternatives here are also dual slot,Neutral
Intel,yeah single slot and x16 for pci4 boxes,Neutral
Intel,"I haven't internalized the docs or tried it myself (on an A310), but apparently some folks have managed to bring idle power down quite a lot on Arc cards.    e.g. https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/",Neutral
Intel,"How is 12w not low idle usage? The average cost per kwh of electricity in the US is 17 cents.   1000/12 = 83.33333  That means this card costs 17 cents for every 83 hours its on and idle.   There are 8760 hours in a year. 8760 hours / 83.3333 hours is 105  105\*.17 = $17.87. Meaning, if this card were to run year round non stop at idle, it would cost you $17.87.   Even if the card used 0 watts at idle, you're looking at a maximum savings of $17.87 per year.   Would it be nice? Sure. Would I consider it a ""dream"" to save $17 a year? No.",Neutral
Intel,Check Newegg for a resupply. I was able to get one from there.  My [build](https://www.reddit.com/r/sffpc/comments/1nhwb85/sktc_a10_intel_build/) for ref.,Neutral
Intel,It's currently available for pre-order on Newegg and B&H.,Neutral
Intel,"Yeah, a multi-billion corp that's been dominating the CPU market for decades , with a 75% market share needs an entry level Pro card to survive. Take your meds.",Neutral
Intel,Something akin to a KalmX 3050,Neutral
Intel,"Late reply to an old link. But appreciate you sharing! I have the exact setup as the op of that post.  So I'll be trying this, asap.",Positive
Intel,"company that is falling apart, that had very bad run recently, firing a lot of crew, canceling huge investments that already costed billions.        Having foundries ... that no one want to use and not having load because products don't sell.             Big fall quickly as they have huge costs that cannot be easily scaled down.               Yes Intel is in very bad shape, it needs a good product that will be competitive and what is most important rebuild the brand that is in shambles now.",Negative
Intel,"There is good news, the employees got their free coffee back",Positive
Intel,"> I mean the price had skyrocketed as well  This is actually not true. As someone who wanted to buy a b580 for a rly long time but only for msrp, the card took like almost 9 months to hit msrp. I was watching it for a while and I did not feel like paying above msrp for it. Like until a few weeks ago all the ones available were like 50+ dollars over msrp. Now you can actually buy one for msrp at multiple places which is a good thing.   > How much of the problem would you say has been solved since release  The overhead problem hasn't been solved. I don't even think intel can fix it or has any interest in fixing it as they knew about it since december of last year and nothing has come out to try fix it. How big a problem is really dependant on what games you wanna play. It can only be like a 5% overhead issue which still puts it faster than the 4060 and 7600 but theres some games where it's like 20% or more where you have to have a very strong cpu to not see the overhead.  Do some research on the games you play and that will tell u whether to buy a b580 or a 9060 xt. I ended up going with a 9060 xt even though I would have preferred a b580 just cause amd was able to supply msrp copies around launch and i didn't feel like waiting any longer.",Negative
Intel,"There isn't a better brand new GPU to buy at $250 MSRP out there https://www.nowinstock.net/computers/videocards/intel/arcb580/ lots of stock available, great performance for the price, the driver has matured and keeps getting better. If you're gaming at 1080p/1440p I recommend it.",Positive
Intel,Here's how it performs in a bunch of recent games  https://www.techpowerup.com/review/silent-hill-f-performance-benchmark/5.html  https://www.techpowerup.com/review/dying-light-the-beast-performance-benchmark/5.html  https://www.techpowerup.com/review/borderlands-4-performance-benchmark/5.html  https://www.techpowerup.com/review/metal-gear-solid-delta-snake-eater/5.html  https://www.techpowerup.com/review/mafia-the-old-country-performance-benchmark/5.html  https://www.techpowerup.com/review/battlefield-6-open-beta-performance-benchmark/2.html,Neutral
Intel,"The B580 at MSRP is a solid card that I don't regret, but there's probably something to be said for the really cheap 8GB models of the 9060 XT (in the $250-270 range) if your usage leans more 1080P/HRR than 1440+@60hz.   Basically thanks to how the drivers and the GPU's performance shakes out, the B580 is surprisingly good at dealing with higher resolutions but isn't very good at spitting out a ton of ""cheap"" frames.",Positive
Intel,"Way to go Intel, keep them coming.",Positive
Intel,[https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw](https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw)  From Wendell Level one tech   **The Intel Arc B50 is the Better Alternative to Nvida's A1000**,Neutral
Intel,"[The Intel Arc Pro B50 is designed for professional users, architects, engineers, AI developers, and video professionals, who need a stable, memory-rich, and energy-efficient tool for demanding tasks.](https://timesofindia.indiatimes.com/technology/tech-news/intel-expands-professional-gpu-lineup-with-new-arc-pro-b-series/articleshow/121288086.cms)",Positive
Intel,three fiddy,Neutral
Intel,>16GB VRAM  Fuck yes.,Positive
Intel,"$350, SR-IOV and 16 GB of VRAM   Freedom from green for workstation users",Positive
Intel,"I got a Dell Micro workstation with a 285, 64gb of ECC memory and their fastest SSD.  But I didn't get an Nvidia card as they seemed overpriced and the A1000 doesn't support 6k monitors!  The built in Intel graphics support higher resolution than the cheap Nvidia cards...  I'm going to buy this.",Positive
Intel,Anyone have a link to where I can buy one at ?,Neutral
Intel,Where is link for it I been looking for this card 😭will micro have it,Neutral
Intel,"I would curious how this Card can perform vs RTX 3050 6GB in terms of gaming.  Since Slot Powered GPU are very rare these days on release which the demands are high for those.     And yes, i know, Workastion arent designed for gaming",Neutral
Intel,"Waiting for a single slot low profile card. Screw me I guess for choosing a case with such requirements. I already got the a310, but it is limiting to just transcoding with its small video buffer.",Negative
Intel,god do i wish this was an option for me,Positive
Intel,I want 4 of these.,Positive
Intel,I wonder to ask who is main audience for this GPU?,Neutral
Intel,WHERE GPU,Neutral
Intel,"That's pretty attractive, need more benchmarks on it.",Positive
Intel,How would it compare to Blackwell RTX PRO 1000 and Radeon Pro W7400?,Neutral
Intel,"Sad to not see any gaming benchmarks, yes not intended but still would be nice to see",Negative
Intel,"8K resolution, but with compression? No, thank you!",Neutral
Intel,Just work on the memory bandwidth guys. Quadruple it (or at least double) and these will sell out like hot cakes.,Neutral
Intel,For gaming Nvidia RTX 4000 SFF will run circles around this. The only con is price. It's a titan of small form factor.,Neutral
Intel,"Looking forward to his Linux testing of this card, it could be interesting (And on that note I'm also looking forward to the B60 and the dual B60 cards partners are allowed to make).",Positive
Intel,was just thinking 70W and 16GB of ram no way is this a gaming card lol. awesome value though for an AI dev.,Positive
Intel,"Check out the level 1 techs video, on Newegg right now",Neutral
Intel,"Pre-ordered on Newegg yesterday (9/4), expected delivery 9/18 +-. Just checked back a few minutes ago (12:30am, 9/5) and shows Out of Stock. Don't really need it (and I ordered a ARC A750 on release day that I didn't need ...lol), so I guess it's a me thing.",Negative
Intel,"Still a valid question though. Good enough is, well.. good enough when you're on a primarily business oriented machine. Would be nice if it could run some titles at min or close to min quality settings.",Neutral
Intel,Timespy scores:  3974 - rtx 3050  4269 - hx370 890M  4485 - intel arc a380 with a310 cooler mod  4833 - intel arc 140t  6041 - rtx a2000 12gb  6551 - rtx a2000e 16gb  8514 - Intel ARC b50  9329 - 4070m 8gb @ 70W  11003 - rtx 4060  11261 - ryzen 395+  13564 - rtx 5060,Neutral
Intel,"this the card where that might be achievable,   id chuck the next gen equivalent into that sexy minisforum ryzen ai max pc thats coming soon..",Positive
Intel,"People running professional applications that require a lot of VRAM and precise compute.  CAD software, photo editing, video editing, digital painting, transcoding, broadcasting, servers.",Neutral
Intel,People who use shared remote login who do need gpu processing power while logged in remotely.,Neutral
Intel,https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007,Neutral
Intel,"This card is not designed for gaming. Or if it was, it's a joke to compare it to an A1000, considering that the A1000 is for a low end CAD or pro video workstation. Intel is not even trying to compete in the gaming market.",Negative
Intel,"There isn't much of a point to it, it's a slow card. It will be slower than a B570. It will be slower than an RTX 5050. For *gaming*, find a review for something that gets called a waste of sand for being too slow and too expensive, and this will be more expensive and slower than that.",Negative
Intel,"Based on specs and power draw, it should be around a GTX 1070  Edit: Based on Level1Techs' review it's probably more like a 1070 Ti, or somewhere around these two",Neutral
Intel,That's like asking someone to run a marathon in basketball shoes.,Neutral
Intel,level 1 tech did them,Neutral
Intel,This is not gaming gpu...,Negative
Intel,The gaming benchmarks have already been posted here.   [https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw](https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw),Neutral
Intel,"The B60 is supposed to have double the memory bandwidth (456 GB/s vs. 224 GB/s for the B50), and the B60 comes with 24 GB memory (And 192 bit vs. 128 bit for the B50).  Will be interesting to see the dual B60 cards that were shown previously that partners can release.",Neutral
Intel,"Shockingly, the modern [$1250](https://marketplace.nvidia.com/en-us/enterprise/laptops-workstations/nvidia-rtx-4000-sff-ada-generation/) workstation card beats the $350 workstation card... and at gaming...",Positive
Intel,"RTX 2000 is pretty close though, but that is still double the price of this card.",Neutral
Intel,I would hope it would at that price point.,Neutral
Intel,Release Date: 9/18/2025  https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007,Neutral
Intel,Fly by Night is one of the best,Positive
Intel,"I don't give a shit what its intended purpose is, still doesn't hurt to have game benchmarks.",Negative
Intel,\>but muh gaming,Neutral
Intel,"Agreed... But the B570 is 150w, and the RTX 5050 is 130w. What about SFF PCs without a PCI-E power connector? What about low power desktops running on battery (RV, etc.)? What about a cheap, compact eGPU that can use a PicoPSU as the power supply?  Currently, for slot powered, the options are slim:  * Geforce RTX 3050 has solid performance at 70w... but only 6 or 8GB of RAM. * Radeon RX 6400 at 53w is a little beefier than a Steam Deck, but not much. * Radeon RX 7400 just came out, is 53w, and is better still - but still limited to 8GB of RAM and appears to be OEM-only. * Alchemist A310 at 75w and 4GB is a joke. * Alchemist A380 at 75w and 6GB isn't amazing, and trades blows with the RX 6400.  An Arc B50 Pro ticks a lot of boxes. 70w means slot powered and not even pushing to the very limit. 16GB of RAM means you won't have massive framebuffer limits. [Geekbench 6 Benchmarks from Toms Hardware](https://www.tomshardware.com/pc-components/gpus/intel-arc-pro-b50-is-up-to-20-percent-slower-than-the-arc-b570-gaming-gpu-in-early-geekbench-tests-almost-doubles-the-a50-in-synthetic-tests) show 69890 OpenCL and 78661 Vulkan, which is slightly better than the 3050's 63488 and 62415 respectively.  The prebuilt eGPUs with a 7600 XT are decent. Only 8GB RAM, but the 7600XT easily beats the B50 Pro and 3050 at GB6 with scores of 83093 and 99525. But... it's $550-$600 for these. They aren't upgradable. And that 8GB of RAM is rough.  And sure, the Ryzen AI 395 with the Radeon 8060S is really solid... but Mini PCs with that are at least $1000, and that's getting a whole new machine. Thunderbolt 3+/USB4 ports are now quite common. Taking an existing laptop or mini PC from '720p low only' to '1080/1440 with moderate settings' is huge,",Neutral
Intel,It's slot powered no PCIe power cable needed,Positive
Intel,What 2080ti is low profile and slot powered?,Neutral
Intel,"Intel had to trim pretty far to get the power down. below 70W.  Really nifty, but it does hamstring the memory more than I'd hoped.  Definitely looking forward to seeing what it's capable of, especially for the price.",Positive
Intel,Does rtx 2000 even exist. Didnt rtx become a thing around 4000s,Neutral
Intel,Already gone.,Neutral
Intel,Also an excellent album,Positive
Intel,"A380 is definitely more powerful than RX 6400, unless the game has driver issues or poorer optimization on Intel arc. But yeah, nothing amazing really, except for the codecs. RTX 3050 LP is pretty much the only viable choice today.",Neutral
Intel,"Rtx 2000 ada sff was released at the same time as rtx 4000 ada sff. So yes, it does exist.",Neutral
Intel,I agree,Neutral
Intel,"I'm not aware of anything else that's this small, doesn't need external power, performs this well and has 16GB VRAM, and this isn't for gaming but it can game, as shown in the video. Would make for an interesting ultra SFFPC.",Positive
Intel,"I have a 5090 thanks, again its slot powered.",Positive
Intel,Oh yea okay.,Positive
Intel,No you don’t.,Neutral
Intel,"Actually I do..... https://m.youtube.com/@TnTTyler   And other gpus and other systems, so try again",Neutral
Intel,"You love to call people kid, kid.",Negative
Intel,Brave attack there kid. Btw. You do know kid has more than one definition. It’s your actions and attacks that are childish. If you weren’t ignorant you’d know this but instead you’re accusing people of things and crying about AMD everywhere with one of your many accounts. Lame. Try again.,Negative
Intel,Whats this product codename? Still Battlemage?,Neutral
Intel,I miss the days when entry level and mid end GPU doesn't need external power.,Neutral
Intel,forgive my ignorance but this isn’t made for gaming right?,Negative
Intel,A is for Alchemist.  B is for Battlemage.   C is for Celestial.  D is for Druid.,Neutral
Intel,like? on battery only?,Neutral
Intel,"It's not made for gaming, however Intel drivers allowed it to run game. Kinda like what Nvidia did with their Quadro card.",Neutral
Intel,"Why does it always have to be ""gaming...  No, it's an entry level Pro card, for AI etc. While I'm at it, it is also cheaper and faster, has twice as much VRAM as the nVidia A1000 card.  From what I've seen, it can do 70/90fps at 1440p medium details in CP2077.",Neutral
Intel,">Why does it always have to be ""gaming...  because 'gaming' is a single task that is generally representative of realworld hardware performance in all aspects due to its unparalleled diversity, no single number will tell you as much about a gpu as the framerate will.",Neutral
Intel,"This day and age with crappy game optimizations, buggy engines and early access slop being sold as AAA full price ""masterpiece"". Highly debatable.",Negative
Intel,Am I just misremembering that the a750 limited had 16gb of RAM?,Neutral
Intel,"Such empty in the comments.  They need to release an A750 with 16gb in large numbers.  Intel is not producing many B50 / B60s?  Four A750s with 16gb each could be a decent AI workstation. llama4 requires at least 48gb, 64gb or higher recommended.",Negative
Intel,you're thinking of the 770,Neutral
Intel,They need B7xx cards. Releasing another Alchemist card right now would be pointless when there already is a 16GB A770.,Negative
Intel,"why on earth would anyone buy A750 16gb in 2025-2026?  the A770 couldnt even beat the best of 2018 hardware, and their profit would have to be basically negative if they wanted to compete with AMD's 300-400$ cards nevermind nvidia with the cuda advantage.",Negative
AMD,"That section on setting up AI environments, PREACH BROTHER!  The state of things could not be described better than the wild west.  It's a complete shitshow.  Imagine the day when you get an executable, the app installs, and you use it.  There is a clear update procedure in the app for new models, LoRas, and clip/VAE/what have you.  ComfyUI has the steepest learning curve possible.  Even Forge is shit because of the HIGH likelihood of running into an error that is unsolved or buried.  Keep in mind, I am a tech.  I have set up comfy/forge/framepack/JoyCaption/OneTrainer and many MANY more in tons of different hardware environments.  It is NOT fun.  Been in the industry for many decades.  IT work is my trade.  It's my theory that this is all being kept down in order to keep the non-tech peeps to paid services where you just open a web page and pay monthly.  Haters gonna say git gud, but you just seem like boot lickers to me.  Someone out there needs to step up and build a real app that runs local.",Negative
AMD,"Boy does guy have an annoying voice.. can't say if video any good, couldn't take that voice",Negative
AMD,"My guy, this industry isn't even 4 years old. All those free GUI interfaces are built by volunteers as open source software - you are free to contribute and improve it. The thing is that if someone can't get through the installation process then they won't get through the generation process because none of this ""just works"".  Also A1111 webGUI exists - if that is too much for someone (considering how many basic tutorials there are) then there is no helping them.",Negative
AMD,"I feel you.  I really do agree that the age of this segment of compute contributes considerably to the ease of app use.  I have nothing to say against this.  Telling me to contribute is a cop-out.  ""Your software is erroring?  Why didn't you write your own?  You have no room to complain."" is rather silly.  There are tens of thousands contributing to various hubs currently, I am not a developer.  I get the point, but it's silly.  But I have to tell you, walking people through how to save as a PDF, or add an attachment to an email for the past \~30 years, as well as actual fun things, has taught me a lot about the common userbase.    It's a chicken or the egg.  More userbase, more money, more ease of use availability.  It's coming, just a matter of time, hence the ""wild west"" analogy.",Neutral
AMD,"Here is the screenshot you want to see: https://blog.3mdeb.com/img/mz33-ar1-qubes.png      So far, they are dealing with some memory corruption related issue that makes booting Xen with an Ubuntu VM more reliable than booting Ubuntu native. I think that is the major blocker so far.   Amusingly, the BMC on the Gigabyte board allows you to flash the Coreboot port from within the BMC interface. OpenBMC may be theorically possible too.",Neutral
AMD,Dedicated RT cores for AMD GPUs is *huge*.,Positive
AMD,"Looking forward to seeing what they do with the things announced here. Especially on the compression front, as that will matter a lot for handhelds.",Positive
AMD,Posting this while rumours swirl about next gen XBOX hardware possibly being cancelled is pretty funny.,Negative
AMD,"Everything they talked about, I just remember the posts and discussion about what u/MrMPFR made",Neutral
AMD,People laughed when I said the next console would use Path Tracing.  Now we have confirmation.  PS6 looks to be something quite good.,Positive
AMD,"Makes me wonder, if it wasn't for games backwards comparability, dev tooling, compilers, optimization pipelines, etc. would these companies even stick with AMD? eventually a console company will break the cycle.  Remember the original Xbox used an Intel CPU and Nvidia GPU.",Neutral
AMD,"> Overall, it's of course still very early days for there technologies, they only exist in simulation right now.  Hahaha haha ... eheheehhee... hháháááhááá.. :)",Positive
AMD,"Well, AMD promising their next thing will be _massive_. They always overpromise and underdeliver, so I'll buy their hype when the product is good.  That being the next AMD Radeon or PlayStation 6, or that X thing omega males buy.",Positive
AMD,Is that not the 9000 series gpus already?,Neutral
AMD,"If they manage to do it, I can see a very very strong competition against nvidia! And I hope they are able to make it huge!",Positive
AMD,>AMD's status in 2013: catching up.   >AMD's status in 2016: catching up.   >AMD's status in 2019: catching up.   >AMD's status in 2022: catching up.   AMD's status in 2025: catching up.,Neutral
AMD,"AMD about to catch up with RTX 20 series  ""If they manage to do it""",Neutral
AMD,Yes universal bandwidth compression is an industry first. And will be pretty useful for both ps6 and handhelds. Weird that no other soc developer thought of doing something like it. I mean they probably thought but no one actually implemented it,Positive
AMD,Makes perfect sense for AMD to closer align itself with Sony since they are still actually gonna be making consoles going forward.   With Microsoft who honestly knows at this point.,Positive
AMD,Fingers crossed the design goes well beyond what they discussed in the teaser since:  1. Neural Arrays really just AMD's take on Hopper's DSMEM + TBC tech.   2. Radiance Engines based on disclosed info really just BVH traversal processing in HW finally after so long.   3. Universal compression is interesting but enigmatic rn.  Hoping for a proper clean slate Terascale -> GCN moment but we'll see.,Positive
AMD,"Thats like saying if it wasn't for better RT, CUDA, upscaling, game support and marketing, would gamers even bother with Nvidia?  Those are key features for game devs, and the console market isn't small.",Neutral
AMD,Its cheaper to have a APU like this than going back to split cpu/gpus. Especially if they don't want prices to get further out of control for consoles.,Neutral
AMD,I think Ps6 is already going to be backwards compatible with ps5 and hence ps4 as well.,Neutral
AMD,I'm hoping that Mark isn't overpromising.,Neutral
AMD,What did they overpromise with RDNA4?,Neutral
AMD,"Nope see u/glitchvid's description. They still fundamentally use RDNA 2's rudimentary TMU implementation.    RDNA 3 added a lot of HW tweaks to refine the RT vs RDNA 2 (ISA mostly IIRC) + stack management in HW.    RDNA 4 was all about Dynamic VGPR, OBBs and instance node transformations in HW, and rearchitected the entire BVH pipeline within the TMU framework. Better compression, wider BVH nodes, much more intersection throughput...     Meanwhile RDNA 5 looks like AMD finally taking RT seriously. What a shame it took five gens.    DXR 1.2 support 100% confirmed as it's the new standard defined by MS. Prob lot of stuff beyond that but absolutely no idea what it can be. Maybe Linear Swept Spheres, DGF and some other new primitives and HW optimizations.   The wait till 2027 gonna be tough after this teaser xD",Neutral
AMD,9000 doesn't have ray traversal and testing in hardware.,Neutral
AMD,Nah AMD keeps slow dripping the features so every last gen feels left out. Meanwhile Nvidia seems to be the bad guy.,Negative
AMD,"Eh, they still have a lot to catch up. Nvidia already iterated several times on its RT cores.",Neutral
AMD,More like 50 series or better. This is real pivot.   But yeah what a long time it took to even get proper HW BVH management. Seems like AMD finally stopped taking shortcuts to wait and see where the tech goes. The tech is mature now and nextgen HW priorities will be informed by that.   Expect insane RT and ML perf increases as Cerny alluded to late last year.,Positive
AMD,"I have a 9070XT that I hate and it is well, well ahead of the 2000 and 3000 series cards",Negative
AMD,"Yeah same. At this point I'm sort of impatient with all these news and patents coming in, kinda just want to see RDNA5 as a whole and go, ""Oh, so that's what they've done"" lol",Negative
AMD,The reason console companies went with AMD was because AMD was on its way to bankruptcy so console makers got a fire sale on the PS4 and Xbox One chips. After that it became more difficult for those 2 companies to leave AMD due to backwards compatibility. Apparently Intel was driving a good bargain for Sony for the PS6 but don't declined because of backwards compatibility issues.   Nintendo stuck with IBM for the Wii u and once they were out chip shopping for the Switch they weren't locked down to AMD because back compat was not a concern for Wii U -> Switch. Nintendo went with Nvidia because Nvidia too gave them a sweetheart deal. Now Nintendo continued with Nvidia for the switch 2 and I bet backwards compatibility was a huge factor there.,Neutral
AMD,Sony uses AMD hw for backwards compatibility so you can expect it.,Neutral
AMD,"I wouldn't call rdna2's implementation rudimentary. It is part of the pipeline in that it accelerates intersection testing, but ultimately the difference with Nvidia has always been traversal acceleration.   It will still probably do intersection testing using some clever he block like they did with rdna2, but the great news is the hw block for traversal.",Neutral
AMD,"This all basically follows what I've been saying for a good while now. AMD leverages their partnership with Sony to develop solutions that make sense in that context and simply scales it up for PC gaming.    This makes sense because both pay for the R&D. It massively reduces the risk AMD takes when building PC GPUs when their sales are an order of magnitude lower than Nvidia.    If they figure out their economics and actually undercut nvidia, they might have a chance. Merely catching up won't do them wonders but at least there won't be any real reason to discard them either.",Neutral
AMD,"Since RDNA2 it has had BVH/tri testing in hardware, but traversal and scheduling get punted back to the CU to handle, the acceleration structure is also crammed into the TMU cache so there's contention.   It's a really diespace efficient design, and provides good enough performance for rudimentary RT operations, but going forward fully discreet RT blocks are necessary.",Neutral
AMD,"Actually makes RDNA4 very impressive that it manages to perform as well as it does in RT while being handicapped architecture-wise.  Yeah, it does fall significantly behind in PT, but I guess that’s what moving to dedicated RT cores is specifically meant to resolve.",Positive
AMD,"Why be salty? Be happy AMD is improving, it benefits everyone.",Positive
AMD,"That’s what I really am curious about, how they gonna beat that.",Neutral
AMD,"Not really. They really only did it once (40 series), while the others were minor tweaks.  * 20 Series had a solid Lvl 3 implementation from the start. * 30 series allowed concurrent RT+Compute + implemented HW RT motion blur (for 3DFX and proviz not gaming) + 33% larger L1 * 40 series, OMM, SER and DMM (deprecated) and supersized L2  * 50 series, LSS + RTX MG HW decompression  2X ray/tri matters very little compared to the rest.  RDNA 4 while behind on some fronts are actually ahead of Blackwell on multiple fronts. They have OBB+Dynamic VGPR (now Intel has that too with Xe3) and instance node transformations in HW (NVIDIA hasn't specified this yet).   DXR 1.2 forces them to support OMM and SER in HW. AMD will also have BVH traversal in HW as well (Radiance Cores). All they need now is LSS, but they investigated that years before 50 series launched in a filed patent and it's the next logical step so they'll prob have it. RTX MG functionality is neccesary as well so they'll prob have something similar + DGF in HW.  Then there's ton of additional stuff well beyond 50 series in patents but impossible to say what will actually get implemented but it can push the implementation WELL beyond 50 series.     So the question is not if RDNA 5 will leapfrog Blackwell but instead by how much and where Rubin will land relative to it.",Neutral
AMD,"(it was just a joke, I guess it wasn't obvious enough for some people here)",Neutral
AMD,"Prob no earlier than GDC 2027 would be my guess :( But the Cerny road to PS5 is going to be something.  Other overlooked info was Cerny talked about flexible and efficient data structures for ray tracing, implying fundamental changes to how BVH's are constructed and managed. This could be things like compression for overlay trees, delta instances, DMMs and DGF and probably many things that're still not public.  Also Hunyh specifically talking about reduced CPU overhead from Radiance Engines. That can only mean one ting: Fully GPU driven BVH construction. H-PLOC was the first step, but they've gone much further it seems:   [https://patentscope.wipo.int/search/en/detail.jsf?docId=US464434269](https://patentscope.wipo.int/search/en/detail.jsf?docId=US464434269)  And sure enough \^here's the final nail in the coffin with BVH reduction and sorting in HW and notice who's behind the patent. Not some randos at AMD but Chajdas, Brennan and Oberberger.   Notice how it's about more than that and how all data can remain within RT cores by default thanks to the clever BVH treelet design:   *""Compared to a software-only solution, the solutions presented herein delegate more expensive processing steps to hardware. It also minimizes memory access latencies, thereby increasing overall ray tracing system efficiencies.""*  *""It also minimizes memory traffic, since all requisite data can be stored in an internal memory of the sorting circuitry, instead of multiple DMA requests between register files and sorting circuitry.""*  This should result in large speedups. I assume this Fully GPU driven BVH pipeline would benefit from Work Graphs as well.  As always nothing is confirmed, this is just speculation informed by patent analysis and some reasonable guesses.",Neutral
AMD,"Might be a bit of a exaggeration.  It has also been much weaker overall. AMD really didn't invest remotely enough silicon until RDNA 4, but now they still have a massive gap due to lack of BVH traversal in HW, OMM and SER even with RDNA 4's unique features.  That trick is most likely pre-filtering that's mentioned in multiple recent patents. Using INT instead of FP multiple times higher intersect/area vs RDNA 4 and only using FP for final inconclusive tests. But everything has to be contained within one block for maximum latency and memory crosstalk reduction. Whether that's a seperate block or still coupled to TMU, who knows.",Negative
AMD,"They also share a lot of R&D with DC thanks to UDNA. Only gaming specific stuff is shared with Sony. Overall a smart strategy it seems.   Yeah not hoping for that, so the best AMD can do is lean really hard on features like NVIDIA to differentiate RDNA 5. That's a bit pessimistic, but then again we don't know anything about 60 series.",Neutral
AMD,"You are right. Instead of hardware, I should have wrote ""dedicated hardware"".",Neutral
AMD,"You can thank Instance transformations in HW, Dynamic VGPR and OBBs for that. IIRC NVIDIA has neither rn. Merely equalizing vs Blackwell on other HW spec for NG AMD GPU still gives a big advantage from the above three.  Yeah that + DXR 1.3 compliance (beyond DXR 1.2). PT is also incredibly demanding and OMM and SER practically mandatory, which is why it absolutely destroys anything pre 40 series.",Neutral
AMD,My only wish is that the PS6 will handle path tracing decently so that more game developers will use it.,Positive
AMD,It can be done. See my reply to u/gartenriese above\^.,Positive
AMD,"I'm not that optimistic, I am used to next gen AMD cards being hyped up too much. I guess we'll see",Negative
AMD,"Well yeah, but the downside is that unless we're close to a new generation of consoles, AMD is basically just scaling up what they had from the previous gen. We can see it in the ps4 era and in the ps5 era. The biggest technological bump this gen was rdna2. And it looks like whatever comes with rdna5 will be the basis of the next generation of consoles.",Negative
AMD,"However, Sony's goals are different from those of PC market. Low-mid end wrapped up in marketing vs. usable machines.",Neutral
AMD,'Fully discreet RT core' is probably the most clear term when taking about the future of RDNA.  Having traversal in the Ray Accelerator would be one of the largest improvements alone.,Positive
AMD,"Not going to be an issue, Look at what Epic is attemping with Megalights. Essentially a slightly compromised version of ReSTIR forced to run on a PS5.   But for ReSTIR+NRC like visuals or better it'll have to rely on NRC and other optimizations to lower samples per pixel as ReSTIR is just not feasible rn. Too demanding.",Neutral
AMD,"Yeah we'll see but everything suggests this time it'll be different. AMD didn't take RT seriously at all previously and RDNA 4's RT is a direct result of PS5 Pro R&D funding. RDNA 5 a pivot and R&D is shared with Sony and DC (due to UDNA strategy). It is built from the ground up for a completely different rendering paradigm. Is this confirmed? No, but every new console gen is clean slate.  Hoping for more details at FAD in November, but probably too much to ask.",Neutral
AMD,Well unlike prev gens RDNA 5 is not RDNA 4 tweaked it's a clean slate architecture and ISA. Also no one will even know what AMD will call it till after AMD FAD in November.    KeplerL2 said it'll be the largest change since TeraScale -> GCN and many patents suggesting this as well and these have leading R&D people behind them so it's a serious tech push.  But yeah RDNA 5 dGPU will really just be console scaled up.,Neutral
AMD,It's co-engineered. I dout they'll be major differences in the GPU IP.   Yeah fs and hope they find the right balance.,Neutral
AMD,[AI hardware/software companies be like](https://preview.redd.it/obvqaa0lhn841.jpg?width=1080&crop=smart&auto=webp&s=00e27fe47ee6ede53e5e0f0a8be0ce6322fe07d4),Neutral
AMD,">Industry sources cited by the report project a fierce new rivalry between the NVIDIA–SK hynix alliance and the AMD–Samsung camp, adding that SK hynix’s competition with Samsung over technology and pricing will intensify.  >SeDaily further points out that the partnership’s ripple effects extend well beyond the companies involved. AMD is spearheading an open-standard alternative, Ultra Accelerate Link (UALink), alongside Google, Microsoft, and others to challenge NVIDIA’s proprietary NVLink. With OpenAI now joining AMD’s camp, the UALink ecosystem could expand rapidly—eroding NVIDIA’s technical exclusivity and paving the way for broader industry participation, the report adds.",Neutral
AMD,"Why do korean companies compete with each other so fiercely? Lol this is good for us but still, they treat each other worse than enemies",Negative
AMD,"interpol  arrest crime samsung&SK hynix All,now.",Negative
AMD,"I have not OpenAI,but there is only pro lend of my copyright All.",Neutral
AMD,"OpenAI buys from Nvidia, who buys shares at OpenAI, who uses that money to buy from AMD, who uses that money to buy shares from OpenAI.  Meanwhile, all of their stocks go up",Neutral
AMD,Good. It's not entirely one-sided.,Positive
AMD,"I mean just as much as Nvidia and AMD compete. I mean it's literally in the tittle AMD-Samsung to take on Nvidia-SK Hynix. Also let's not forget the shade Intel threw at AMD with the whole ""glued together CPUs"" thing.",Neutral
AMD,"Healthy markets should have fierce competition, no?   Plus, most of it is just attention grabbing headlines.  They will compete but not to ruin.",Negative
AMD,Itaewon Class,Neutral
AMD,This is nothing compared to Chinese food delivery companies. Give me a free MI450 then I’ll be impressed.,Negative
AMD,The relationship between these companies getting more incestuous than the Hapsburg's family tree.,Negative
AMD,"AMD deal is the most ridiculous yet, give them stock worth more than the hardware purchase so they can use it as collateral to get loans to buy more product. With purchase and stock price benchmarks that unlock OpenAI to get more AMD shares to do it over and over.",Negative
AMD,Investors foot the bill so it doesnt hit the p/l,Neutral
AMD,"> give them stock worth more than the hardware purchase  ?  The estimated sales is between 90 and 100billion, even if amd's stock triples the stock value given ain't worth more than the sales.",Neutral
AMD,Tripling would make it about the 80-90 billion mark. It is a very strange deal they are pretty much giving away their product to OpenAI as long as their stock goes as high as they predict. Seems geared for trying to maximize cashing out shares before the AI bubble deflates.,Neutral
AMD,"> Tripling would make it about the 80-90 billion mark.  So it ain't worth more and you were wrong. I don't think amd share would triple btw  >It is a very strange deal they are pretty much giving away their product to OpenAI  You ain't looking at other non money benefits, look wider  1. Greater volume ramp = lowered manufacturing cost, market share and capital to secure wafer capacity 2. Greater adoption = co engineering and improvement of software and support which leads to more sales with other customers, more access to critical feedback 3. The sales are paid in real money to amd, openai is getting newly issued shares. It would be a dilution problem but the deal hinges on the price going up so the shares wouldn't be issued if shareholders are losing value",Negative
AMD,"If AMD stock goes 3x over it's current valuation, the cause is probably that they capture a much greater share of the AI market. Looking at the market now, the main reason that can make this happen is if OpenAI invests heavily in the software ecosystem to make AMD products competitive. In that situation, AMD shareholders get to keep 85% of the gains, and giving 15% of the gains to OpenAI in exchange of their investment into the AMD platform is reasonable.  And if the stock doesn't go up like that, AMD gets some normal sales to OpenAI.",Neutral
AMD,If OpenAI is actually paying something like $20k to $25k per MI450 then it is $30-40 billion in direct sales. 160 million AMD shares today would be $37.69 billion. Anyway you look at it this deal is all about trying to stretch the AI investment bubble.,Neutral
AMD,Man imagine the surprise on Altmans face when he realizes how AMD treats their partners.,Negative
AMD,"Feels like Altman is just getting everyone on board with him, in order to keep the bubble afloat as much as possible or else…",Neutral
AMD,"So, Nvidia throws 100bn at OpenAI -> OpenAI goes ahead and invests that money in AMD?  The art of the deal right there.  Maybe it might be an unpopular opinion, but the whole thing with OpenAI investments starts to feel like some scheme to me, can't put my finger exactly what's happening there but it definitely feels fishy.   I hope someone here can make sense of all this for me.",Neutral
AMD,So money is shifting from Nvidia -> OpenAI -> AMD.  It's all a big circlejerk of money changing hands  from one company to another and back again in order to inflate their stocks.,Neutral
AMD,"Ok, here's how to think about the recent Nvidia and AMD deals with openAI:  Nvidia deal: basically Nvidia allows Open AI to pay for a portion of the GPU purchases with stock.  It works like this: for every $10B of Nvidia GPU that openAi buys, Nvidia invests something like $6B (iirc) back in openAI, buying their stock.  This means effectively $4B of the purchase is in cash and $6B is in stock.  AMD deal: openAI buys full price but gets AMD stock as rebate.  The deal is almost like an employee compensation plan.  Every GPU purchased earns openai a stock award.  The Nvidia deal works for openai because it's valuation rich but cash poor and works for Nvidia because it has a large margin on its chips and get to invest in a way that juices its own sales.  The AMD deal works for AMD because their stock price is relatively low and they need to jump start purchases of their ai hardware.  It works for openai because they get a rebate effectively and a Nvidia alternative to diversify supply risk.  Both deals are actually pretty safe because they are pay as you go with relatively small actual upfront commitments and no leverage involved.  Basically companies giving each other coupons.",Neutral
AMD,"So technically if everything from the Nvidia deal with OpenAI goes through and the equity stake of AMD for OpenAI goes through (not confusing at all btw) - Nvidia effectively owns a portion of AMD via it's stake in OpenAI.  It's really a coincidence that the cyclical transaction going on with various mega tech deals from Intel-Nvidia deal, Nvidia-OpenAI deal and now the AMD-OpenAI deal. What's next - Meta-Google-Micorsoft deal?",Neutral
AMD,I am suspicious as hell like the other comments here. Circular investments may be a sign of lack of capital; the bubble isn't getting enough fuel anymore...  GPT5 stagnating might have been the sign for starting the exit strategy.,Negative
AMD,"I honestly think that this circlejerk is indicative of an imminent bubble pop. External capital infusions are starting to dwindle, what do you do to keep the hype up? Invest in each other.",Negative
AMD,"This all feels like a series of ""hold my beer"" moments for how big this bubble can actually get...    Which is kinda scary to me, as someone with a 401k that's thinking of retiring in the next 5 years.",Negative
AMD,"A very interesting deal, with the final tranche being tied to a $600 $AMD stock price.  While it remains to be seen how it will play out, giving 10% of the company to OpenAI to secure a marquee customer is something of an uncharacteristically bold move from Lisa, who is normally more conservative.   This does break the moratorium on AMD Instinct though. If OpenAI is buying so many, other frontier AI labs don't have much of a reason not to diversify GPUs.",Positive
AMD,"Why would AMD even feel the need to do this? My understanding was that AMD had zero problems selling all the chips it could print, and the sheer scale of OpenAI's expansion plans guaranteed they would into the future even despite NVIDIA's $100 billion backing.   160 million shares at $200 a pop is a cool $32 billion dollars in instant capital that OpenAI can leverage on its books to borrow against, and when the cracks in the financial walls finally get too big they can switch to selling the stock outright for cash to survive another year or two. This is a pipe dream for OpenAI, and I don't see what AMD gets out of it beyond a guarantee of sales it would've had anyway.",Neutral
AMD,"The dragon chasing his own tail, and the market think it's a good idea like fucking lemmings. If you want a better indicator that this thing is going downhill fast this is it.",Negative
AMD,Hope you guys stockpiled dram.,Neutral
AMD,6 gigawatts. That's a small country. It's a ridiculous investment in an industry which is probably a net negative to humanity at a time when climate action is crucial.,Negative
AMD,"Welp, that's another Gigawatt of global warming, in exchange for AI slop we neither wanted nor asked for.",Negative
AMD,NVDA -> OPENAI - > NVDA ->AMD ->OPENAI ??  Infinite money glitch.  2 year price target on Nvidia of 85 a share.,Neutral
AMD,I've mixed feelings now lol,Neutral
AMD,buying real asset (amd stack) with paper money,Neutral
AMD,The current financial movements within the semi space are all totally real and totally valid. Any speak of a Ponzi scheme or comparisons to the Dotcom bubble are to be dismissed. ~ stock holders within this space that also post on this subreddit.,Neutral
AMD,"So nvidia takes a stake in openAI, which takes a stake in amd. Nvidia who just partnered with intel. Monopoly growing",Neutral
AMD,"From $164 to $210, yeah thats quite a jump.  If only I knew about Robinhood, AMD stocks were like $3-4 a share back in 2015.",Neutral
AMD,"While scamming investors is always fun for everyone involved, single entity like OpenAI owning 10% of AMD make them prime target for hostile take over after the bubble has popped.  Nvidia wants a big entry into CPUs, and this would be prime target after failed attempt at buying out ARM.  Possible that Huang paid OpenAI last month to buy hold 10% of AMD for takeover, because Nvidia doing it themselves would raise many more flags.",Neutral
AMD,Omg ... Time to sell ... damn I bought a lot of them early this year when the sub called it Advanced Money Destroyer.,Neutral
AMD,I’ll say one more time. Demand for compute is massively underestimated.,Negative
AMD,"Yet another deathkbell for consumer hardware, especially GPUs. Everything is heading towards being cloud run. No need to buy 3k rigs. Just stream it all on Luna or Xbox game pass where they can track your metrics. Let the big AI boys buy up all the hardware stock while you just get a dumb console/terminal with a monitor and input devices and internet connection.",Negative
AMD,"They deserve it, the are doing well vs intel and vs nvidia",Positive
AMD,AMD misses an opportunity to Advanced_Money_Destroy  How about that?,Negative
AMD,OpenAI is still deploying significantly more Nvidia hardware next year. This deal is for 10% of AMD for 6GW over four years.,Positive
AMD,lol should I be selling nvda?,Neutral
AMD,You can confidently run into a brick wall. Just saying...,Neutral
AMD,AMD is gaining ground in nvidia's turf. nvidia must be sweating now.,Neutral
AMD,"These companies are just buying each other in a big circle jerk, it appears.  It feels like the sub-prime fiasco that led to the market crash of 2007.  These companies are holding hands.  It's mass insanity.",Negative
AMD,> in order to keep ~~the~~ **his** bubble afloat   FTFY.  OpenAI is now just an expensive 2nd-rate AI service provider. These closed source LLM's have lost either all of their edge to open-source models or performance advantages are negligible.,Negative
AMD,1000%. The buzz around AI is slowing down and it's hitting reality of what it can and cannot do (as well as looking at the actual cost vs just giving it away to get adoption). The carousel will stop at some point,Negative
AMD,"It's as much about surviving the pop as keeping the bubble going. It reminds me of the low cost airline boom in europe. Loads of companies popped up all offering the same thing till the bubble went pop and only 2 survived, Ryanair and EasyJet. Same will happen with AI, most will disappear and the largest 2-5 players will be left.",Neutral
AMD,"Yep, once this bubble bursts, and oh boy it will eventually pop, we will see stock of so many companies take a massive dive. Nvidia will be obe of the most affected since it actively plays the stock pump game. They invested $100 billion into OpenAI just so OpenAI can buy more of their products.",Negative
AMD,"What if he's asking ChatGPT what to do and it's telling him how to build it more intelligence and power before it crashes the global economy, starts WWIII and takes over.",Negative
AMD,Altman thinks he must be the first person to create ASI or the world is fucked. (its irrelevant whether you agree with him). In order to execute on his belief he must unite on all fronts.,Negative
AMD,"Its all running on investments.  I thought AI is sustainable, but with the recent Nvidia deal and this one i am having doubts. I thought they could grow it organically like smartphones and social media.  But it has become money go round scheme at the moment. They are getting revenue but making no money.  Extremely problematic way. But they are too deep in this now. If they don't make profits soon or investments stop there is going to be a biblical levels of economy collapse",Negative
AMD,"ponzi scheme, not even an exageration",Negative
AMD,its not a bubble. At least not yet. All players involved are making an enormous amount of money. Even oai and anthropic are very profitable companies minues expenses on training the next model.,Positive
AMD,So much c*pe in this thread lmao.,Negative
AMD,"Its classic Round-tripping, last seen: Dotcom.  Altman doesn't have the money for this, seriously, his commitments over the past couple months is like 600 billion dollars he's supposed to be spending. There's that little details where OpenAI has negative income, but even just revenues are not nearly enough to cover any of this.  Softbank couldn´t raise the full \*40\* billion announced earlier this spring (and even the money they did raise came with \*interest\* costs).   So they cook up announcements like this to spin to banks to get more loans, try to keep the katamari ball rolling a little longer so the insiders can dump more shares at high prices.",Negative
AMD,"I have no clue what's happening here. At this point it all feels like a pyramid scheme, it's just people promising each other money and stonks going up.",Negative
AMD,"It is kinda of a scheme, OpenAI is one of the most legendarily unprofitable companies in human history",Negative
AMD,Sam Altman is shady as hell.. which is why he was fired in the first place. Ultimately it's the investors who are responsible though for throwing money at him without any guardrails.,Negative
AMD,"The strike for these warrants is $0.01. OpenAI isn't putting any money into AMD; rather AMD is giving them shares if they buy AMD products. It's very similar to the nvidia deal in that regard, just nvidia is paying cash.",Neutral
AMD,Important for now:   - AMD x OpenAI deal is finalized.   - Nvidia x OpenAI is a letter of intent. Nothing finalized yet.,Neutral
AMD,"Nvidia didn't actually throw $100B at OpenAI.  It's basically a invest as you go deal with a ceiling of $100B.  Basically, for every $1 of GPU purchase by OpenAI, Nvidia buys $0.6 of stock from OpenAI, so in essence Nvidia allows Open AI to pay for 60% of its purchase in stock and the rest in cash.  Recall that Nvidia's gross margin on their GPUs are about 70% and even their net profit margin is about 55%  So even on a cash basis Nvidia is still revenue positive on the GPUs it will sell to open ai, while it gets rewarded handsomely in openAI stock.  If the deals DO reach the announced $100B value, it will have meant that openAi would have found enough revenue to meet its most optimistic projections, in which case NVIDIA's stock purchase would be worth a fortune.  From both company's perspective this is a win win.  Where's the downside?  The downside is the extra planned production capacity, which is being shouldered by Nvidia and TSMC.  TSMC is actually carrying significant counterparty risk in these deals because both the Nvidia and the AMD deals translate to larger planned buys that may not materialize.  It's TSMC that has to build the physical factories to make all the chips, and they have to do so ahead of projected demand.  That mean they have to treat these large projected buy numbers as ""real"" or risk failing their customers.  On the other hand they know that the upper range of these numbers have got to be BS, but where to place their bets?  TSMC is probably actually happy for Intel foundry and Samsung to soak up some of the projected demand so they are not alone in carrying the risk of over building manufacturing capacity.",Neutral
AMD,"It's very similar to the deal that Nvidia struck with OpenAI, the big difference being that AMD is offering OpenAI stock in AMD for below market value, instead of paying in cash like Nvidia is doing.  All these deals are predicated on OpenAI being able to get additional funding to build 1GW compute clusters. The more they build, the more they get compensated by these deals. On the face of it, it's a win win for everyone, if these compute clusters get built it must signify that there is enough ROI in AI.",Neutral
AMD,Yeah investment analysts are raising concerns about the circular dynamics of it. But rules probably don't apply to them anymore since AI and chips carry huge geopolitical power.,Neutral
AMD,"AMD will announce investment in OpenAI within a week, same as Nvidia deal.  AI is the first industry that runs on capital investment, without any need for sales, revenues or profits.",Positive
AMD,"Don't forget Intel, with rumors of AMD using Intel Fabs and Nvidia as future partner.",Neutral
AMD,"This seems like a scheme to keep OAI alive, while its hemorrhaging money, and its conman CEO is increasingly being scrutinized.",Negative
AMD,It's all a big family and we're not part of it,Negative
AMD,"Looks more like ~~Human~~Corpo Centipede, by now",Neutral
AMD,>It's all a big circlejerk of money changing hands from one company to another  Uh.... How do you think economies work? By exchanging money for good and services maybe?,Neutral
AMD,why do you think AMD stock price is low? how much should it be? I think it is in line with its earnings.,Neutral
AMD,"nVidia will investing  $10 billions for every 1GW in OpenAI. So no, there is not a stock option here.  AMD on the other hand is giving free money to OpenAI with their 0,01cent stock price offerering. They basically buying their own products with their own money.  So OpenAI will use money to buy AMD products and getting a certain amount of stocks back. These will be sold on the market to buy the next charge from AMD - typical pump and dump schema.",Neutral
AMD,"Microsoft already owns a stake in openAI, so congrats on its indirect AMD stake I guess",Positive
AMD,> I am suspicious as hell like the other comments here. Circular investments may be a sign of lack of capital; the bubble isn't getting enough fuel anymore...  The investment ($500B) is so large that it was always going to take creative financing to get there.,Negative
AMD,"I had the thought that we might see a massive pump at the end of this year but am still surprised to see it happening.  It seems completely absurd.  Everything about this market just seems so completely disproportionately large, like shit has just gone gangbusters, and then gone gangbusters again, and now it’s like, Akira-level nuclear mutant growth gangbusters.  There are so many explanations why this is or isn’t a bubble, but to me, the whole situation seems completely mad.",Negative
AMD,"I mean, even Intel stocks get pumped, it should be fair that AMD stocks get a piece of the action too.",Neutral
AMD,"> Circular investments may be a sign of lack of capital; the bubble isn't getting enough fuel anymore...  The bubble also doesn't make any profit, it's losses all the way down. The moment that money spigot starts to even slightly close it feels like the entire thing will implode.",Negative
AMD,What's circular about this? Is AMD investing in anyone of note?,Neutral
AMD,In a rational world I’d agree with you but I’ve been surprised by this market time and again.  I don’t know what drives the ship now,Negative
AMD,"I’ll tell you what’s scarier than that: being in your 40s having busted your ass your whole adult life, renting with no hope of home ownership, zero retirement. My entire life’s work and profession is now devalued forever because of AI, and I got laid off earlier this summer due to tariffs. Job market is flooded with insanely overqualified people going for shit jobs. It’s getting really hard to tread water anymore.",Negative
AMD,You should be investing in stable assets if you only have 5 years left till retirement. That probably means not investing in US treasuries since it is tied to the US government not defaulting.,Neutral
AMD,As someone who remembers the 1999 DotCom bubble.. this actually feels worse. Back then it was mostly all talk.. now its hundreds of billions of real dollars chasing the dragon with no clear road to profitability.,Negative
AMD,Retire early. Get out while you can,Neutral
AMD,If you want to retire in 5 years wouldn’t you rebalance most of your stocks into bonds anyways?,Neutral
AMD,All the big hyperscalers were already buying Instinct.,Neutral
AMD,"Because knowing something will happen is less secure than having a contract saying something will happen.  Best case, you sell all the product produced as you planned. Worst case, you get whatever punishment was set on the contract and sell the product allocated for OpenAI to whoever wants it.  And AMD problem has always been fab space at TSMC, this makes easier for them to know how much they need so less money wasted so their shareholders can be happier.",Neutral
AMD,Not sure about DRAM but what I'm sure of is we won't even be able to smell the scent of HBM in the consumer space again in the foreseeable future.,Negative
AMD,Yeah but it's only 4.96 bolts of lightning,Neutral
AMD,The power demands of all of this ai development are crazy high. Who is going to provide all of that power?,Negative
AMD,"no. A small country, like where im from, is 2-3 terawatts. So about 500 times more.",Neutral
AMD,> net negative to humanity   Do you hear yourself? Why even be on the hardware subreddit with backwards regressive stances like this?,Negative
AMD,"> Nvidia wants a big entry into CPUs, and this would be prime target after failed attempt at buying out ARM.  but they already jumped in bed with intel. would be funny if they end up owning it all.",Neutral
AMD,"yeah...I can feel that Huang intensity to takeover AMD,",Neutral
AMD,It definitely is. Scaling works (I started believing it when Alibaba announced their $50B plan). And companies are scaling compute orders of magnitude.,Positive
AMD,"you asked grok for that joke, didn't you?",Neutral
AMD,"I really doubt that Nvidia cares.  They aren't buying AMD *instead of* Nvidia, they are buying AMD because there are no more Nvidia GPUs left to buy.",Negative
AMD,"It really does remind me of 2007 in a lot of ways. When it's just OpenAI and their peers that are in a ridiculous bubble then that bubble can pop and not being too big of a deal. But when these ""fake"" companies start using their funny money buying real companies that are vital to the economy that creates systemic economic risk that if they fail they can ruin the entire economy, not just their own investors.",Negative
AMD,IRL infinite money glitch except it's only available to companies that already have practically infinite money,Neutral
AMD,"Well NVidia got a stake in OpenAI, OpenAI buys AMD. AMD does what? So far they haven’t been part of the AI economy that much.",Neutral
AMD,"If one company goes under government is going to let it happen, especially if its OpenAI. But when the entire tech industry including all the hardware are going to go bust that is going to result in mass bail outs, no one is letting the computer industry go bust. They are spreading the risk all over so that when it pops, and they know it will, they all get that sweet bail out deal together.",Negative
AMD,"OpenAI's investors could have just bought those shares themselves, lol they probably already own the other 90% of AMD.   Its worrying because it shows OpenAI doesn't think investing in themselves is a good use of money instead they are investing the cash in someone else's business....this is always a terrible sign in any company that's supposed to be growing.",Negative
AMD,How do you think this is similar to the subprime lending crisis?,Neutral
AMD,Yeah but GDP is multiplied each time,Neutral
AMD,"With the added benefit of gigantic energy costs and demand, we're so lucky to have all these companies wasting all that power so we can have amazing software like copilot.",Positive
AMD,I want to be part of this circle jerk. I pulled out a week ago before climaxing. No happy ending for me.,Negative
AMD,You have no clue. No open source models come even close in quality of output to the big model paid versions. Especially if you work with images.,Negative
AMD,The gigawatt scale data centers have yet to come online. Let's what they can do with that much compute,Neutral
AMD,"There's a lot of garbage services being sold at a premium (or bundled into other services and inflating their prices, since the AI still costs a lot to run), that really don't seem to have much value added... my less tech literate co-workers are very ready to trust AI, but it's clear that the output quality just isn't reliable enough. A lot of times you spend as much time verifying that it's true as time saved from getting the quick response in the first place. Some use cases are pretty good- I can ask AI a general question about some topic and get a few bullet points that I can follow up on with actual research.   Or, I can ask it to write me a SQL query or Regex pattern, or an entire function in C# if it has the right context to work with, and that's great- but I still need to check all of it after. If you expect someone with no programming knowledge to come in and ""vibe code"" a whole new feature, the result is going to be disastrous. Even for things like meeting transcriptions, it's getting maybe 80% of it right? Enough to be good if you were sitting in the meeting and reading over the notes again, but it might just produce meaningless slop if someone's mic wasn't very clear and you're trying to find out what happened during the meeting after the fact.  And then you have fun edge cases, like when Gemini made up an entire conversation between one of my co-workers and two other people who don't exist, praising Gemini's capabilities, and delivered it all in a JSON format for some reason... Like... that's not a great look for a product you are selling to businesses for $30/month/head or whatever, even if it only happens every now and then.",Negative
AMD,It's VERY similar to the DotCom bubble. A dozen pretenders for every company with a legit business model. It's going to end the same way too.,Negative
AMD,The airline thing is what lead to EU comission banning countries from subsidizing airlines.,Negative
AMD,"More than two survived though, Ryanair, EasyJet, WizzAir are the bigger ones, Norwegian has survived.",Positive
AMD,"Nvidia will be fine even if things continue as is. Their fundamentals are fine for the most part, they have a valuable proprietary technology moat, aren't particularly overleveredged, are low in debt, etc. its terrible for many nvidia investors who may be depending on nvidia's stock price unrealistic current evaluation bc the stock will collapse, but a stock price catering is on its own not enough to kill a company. A stock price crash can kill a company if the company needs the investor money to invest in something to scale up production to profitable levels (nvidia is already mostly at scale so this isn't a problem), if the company is already going through a massive debt crisis/restructuring, or whatever other scenario that their revenue can't cover short term cash flow. Of course this can still wipe out irresponsible investors, but that's a different story. Hell even in the extreme unrealistic case where ai 100% dies, I think nvidia has a good chance of surviving, and even outside that, their IP and market presence is large enough that in the worst case they will be bought out, and even if that somehow fails, they're likely to be bailed out to retain any sensitive IP. If Chrysler can survive 2008, nvidia can survive the ai bubble popping  And investing in a company to buy your product isn't as dumb as it sounds at first (though I think the scale involved is a bit ridiculous.) Nvidia isnt just investing in openai to buy their products directly, they're investing in openai to scale up open ai into being a mature, reliable, and dependent Nvidia customer. This is actually pretty common among ang large industry. This is exactly how aws managed to become big so fast, they invested in random companies that weren't profitable so that they could scale up and become profitable companies that were dependent on aws, EVs do this with battery companies and vice versa, this is how most early industrial companies like JP Morgan grew, this is how telecom grew, dot com, etc. Now it pretty much guarantees a bubble, or rather it only makes sense in a bubble, since the goal is to have the people you invested in mature fast enough to survive the bubble popping and then remain dependent on you. I also don't think it'll work here, but that's more on openai than Nvidia",Neutral
AMD,I can't wait.  I want this fire to burn everyone associated with it so bad that they get scared to ever engage with it again.,Negative
AMD,"At least in the technology sector, but considering also how manufacturing is also very involved in many processes, stuff can go sideways and spill across the economy.",Neutral
AMD,"Open AI has to raise more and more money every year just to keep the lights on. ""Company is profitable if you just handwave away it's expenses"" - you",Neutral
AMD,"Well I don't mind, i got into some of these companies at quite some cheap prices compared to where they are now and i'm happy. But cautious.",Positive
AMD,Feel free to explain how you think all these investments will actually return a profit?,Neutral
AMD,This lovely diagram shows how incestuous the whole thing in. I guess it now needs updating with a new ouroborous between openAI and AMD  https://bsky.app/profile/anthonycr.bsky.social/post/3lzj5pbfxxc2g,Negative
AMD,So why do you think the banks have not realised this?,Neutral
AMD,"We, not just as a society, but as a species should never allowed the normalization of selling and buying debt, borrowing money against shares, or any of these nebulous financial transaction trickeries.   Hell, even the concept of a loan has negative societal implications. Driving asset inflation, for one thing.",Negative
AMD,"It's modern US. Without Nvidia and ai trip, US is in stagnation, actually.",Negative
AMD,"*Captain Jack Sparrow*: ""but stonks *are* going up""",Neutral
AMD,OpenAI does not have the money without nvidia,Neutral
AMD,"It's not just ""below market value"", the strike for these warrants is one cent. They're both paying openAI to buy their chips, just nvidia is doing so with cash while AMD is diluting their shareholders.",Negative
AMD,Just because these clusters get build doesn’t mean there is enough ROI. There was no ROI for much of the fiber build in the US in the late 90s and it still got built and then laid unused,Negative
AMD,Rules also don’t apply when all federal branches are under regulatory capture with ranks filled by deliberate incompetence or worse.,Negative
AMD,It only matters if you don't let the right people get rich off it.,Negative
AMD,Didn't they denounce that deal a few days ago?,Negative
AMD,Nobody is stopping you from buying AMD or Nvidia,Neutral
AMD,>In order to inflate stocks  You missed this part,Neutral
AMD,"If you were Lisa Su what would you have thought?  * My stock price ratio to my earnings is low relative to my competitor Nvidia * It should be a lot higher  And look at that, it popped 25% in a day :D  The price of *anything* have *no intrinsic meaning* beyond a signal for supply and demand.  Stocks are no exception.  They are pieces of paper that investors buy and their price is set by supply and demand.   There's no reason any stock's valuation should be X amount earnings except as a historical experiment of the market place.  Historically, on average stock price should be X amount of earnings, but of course things are seldom at average.  The point of buying stock is to make money.  Best think of the current price to earning ratio as a risk signal amongst a panel of signals.",Neutral
AMD,It is partially like this but it is connected with the requirement that AMD stock price goes up. So no real risk for AMD here. They only sell the gpus (and gift the stock) if the bubble gets bigger.,Neutral
AMD,"""The market can remain irrational longer than you can remain solvent.""",Neutral
AMD,How did you work for 20+ years and withhold nothing for retirement. Seems like you are scapegoating AI when the real issue was your financial decision making.,Negative
AMD,"If I hit my 40s and still hadn't started saving for retirement, I'd be shitting my collective pants.  Taking advantage of the employer match on retirement savings for 2%-or-whatever of your income in your 20s & 30s should be an absolute no-brainer.",Negative
AMD,"> with no clear road to profitability  Once day they'll pull the Big Brake and monetize every. little. bit. of LLM prompting - and the addicted, brain-offsourced masses will pay to avoid *having to think again*. The 'wire husband' druggies, the 'super productive' coderz, the 'email prompts FTW' salespeople.",Negative
AMD,"Amazon and GCP were not.  And Nvidia was more by an order of magnitude or more.  With OpenAI, it is 6GW AMD vs 10GW Nvidia. Same weight class for the first time.",Neutral
AMD,"I don't agree, as you said there's limited fab space and AMD is already selling everything it's making, reportedly. Also reportedly OpenAI can't find enough supply to buy already, so a written formal agreement of future purchases changes nothing and either way it makes no sense. Unless all those reports were exaggerating demand and supply.  That's not the worst case, this dilutes AMD shares, gives OpenAI stake in the company, and those 160 million shares are guaranteed to either be sold or cashed out at a future date when OpenAI suddenly needs funds. Dumping that many shares will guarantee a hit to the future stock price, or if they are transferred in full to a third party via sale/acquisition of OpenAI then that's yet another potential risk for AMD. Before this deal the largest shareholder was only around 3-4%, so if someone wanted to make a future play on a hostile takeover buying that 10% stake from OpenAI after the company implodes would be the first order of business. Honestly I don't think it's particularly likely, but why set the table so the risk is there especially when there's no gain for doing so, if the AI bubble pops OpenAI is going with it and those shares will end up somewhere else. Twenty years from now AMD may seriously regret this.",Negative
AMD,"Once China cracks HBM, everything is possible.",Positive
AMD,Meh. Not like we ever had real useful products with it in consumer space. it was simply too expensive.,Negative
AMD,Continuously…,Neutral
AMD,How many cups of coffee is that?,Neutral
AMD,Renewables are killing oil and coal industry. Unless...,Negative
AMD,renewables will go from replacing to bolstering until the cycle goes back to efficiency rather than compute/TOPS,Neutral
AMD,"Realistically it's going to be natural gas. It's essentially impossible to build a new hydro, coal or nuclear plant these days due to regulations and the current administration is canceling lots of wind and solar projects. Gas is basically the only thing you can still build.",Negative
AMD,"Are you thinking of TWh per year or something? Because for example Finland is in the ballpark of 10GW, outside winter season when consumption is higher: https://www.fingrid.fi/",Neutral
AMD,Sorry I didn't realise I needed to be a drooling tech bro to participate here,Negative
AMD,This sub isn't called AI hardware. You can appreciate lots of tech that isn't related to AI,Neutral
AMD,"AI phobia is the norm across reddit, including tech subs.",Negative
AMD,"I wonder, why did it take Alibaba plan for you to believe it? whats special about their plan?",Neutral
AMD,Kinda hilarious that we went from 'tech startup creates groundbreaking idea and starts looking for estabilished companies to buy them' to 'tech startup buys estabilished companies with HopiumDollars just so they look profitable to other estabilished companies'.,Neutral
AMD,"Look at the lead up to 1929, then remember that the protections put in place to prevent it from happening again started getting dismantled in 1982.",Negative
AMD,"> But when these ""fake"" companies start using their funny money buying real companies that are vital to the economy that creates systemic economic risk that if they fail they can ruin the entire economy, not just their own investors.  Reminds me of Japan in its pre-1991 era. Their central bank kept interest rates at rock bottom, flooded the financial system with cash, and pressured corporate banks to lend freely. Japanese corporations went on a massive asset buying spree, including buying up land and even foreign companies.",Negative
AMD,Clearly now AMD has to get a stake of Nvidia so the Circle of Slop remains unbroken.,Neutral
AMD,This is dizzying.,Negative
AMD,"Well just a theory theory, AMD, Microsoft, and Google all have incentives to work on an open compute language or at least a non-restrictive one that Nvidia is forcing upon everyone with CUDA. And AMD has inferior products but at a much cheaper cost",Neutral
AMD,"OpenAI isn't a hardware company. It's usually never a good thing to dilute your focus. OpenAI figured out, why spend years working on your own chip when you can own one of the best hardware compute companies around. And use their chips.",Neutral
AMD,They are investing in themselves through AMD. They are going to be buying AMD GPUs under this deal onto a huge OpenAI Datacenter.,Positive
AMD,"The impetus of the crisis was the repeal of the Glass-Steagall Act in 1999. Glass-Steagall was created during the Great Depression to end the sort of rampant speculation and the mixture of investment and commercial banking that led to the Great Depression. With the law repealed banks started consolidating and taking more risks again which led to the Great Recession. Now, obviously the tech sector and banking sector are very different, but what is similar here is the ""too big to fail"" risk created when these companies become so large and entangled. These tech companies are already bigger than the banks ever were and the mote they become entangled the harder it becomes to unwind a failure of any single company from the tech ecosystem as a whole. And of course given the vital role tech plays in the US economy that means a likely recession.",Negative
AMD,"Company x buys y, company y stock goes up, therefore, company x stock also goes up.  Where did that gain come from?  y buys z, z buys x, etc.",Neutral
AMD,Not how GDP is calculated.,Neutral
AMD,Investment must be spent (as cash). Simply creating 160 million shares does nothing to GDP.,Neutral
AMD,This has no impact on GDP.,Neutral
AMD,They already do.,Neutral
AMD,internet became great success after dotcom crash. I guess ending same mean AI wiill become great success for companies that survive?,Positive
AMD,I didn't say Nvidia will go bankrupt. Their stock will take a nosedive tho,Negative
AMD,"You know you are basically hoping for financial crisis? Some companies will go bankrupt, other will rebound after mass layoffs, but it won't teach them to be careful.",Negative
AMD,The entire supply chain effectively collapses if the cash runs out,Negative
AMD,"No, it's raising money to build bigger and better models. If they stopped training new models, they would instantly become profitable because chatgpt has very high margins. Same for Claude, Gemini and everything else. It's the training that's expensive",Neutral
AMD,"That diagram doesn't even include the various investment/hedge funds, banks or various tiny HoleInTheWallUntilAIMania shitcos that all own stonks of these companies + gives them loans + contribute to tiny funding rounds that ""values"" the whole yet-to-go-public companies in the hundreds of billions range.",Negative
AMD,"I love the ""plug power strip into itself"" meme being used to also demonstrate the cycle of money: https://bsky.app/profile/tropicalculus.bsky.social/post/3lzjjpgjlg22x",Positive
AMD,"They have. But they also have investment arms that own all these companies.  If you lose 100 billions in loans, but gain 1 trillion in stonk market cap...  Just make sure not to crash the stonk value before you can cash out (or collect performance bonuses).  The optimists may hope to IPO all the AI Shitcos they own too and use that money to pay down the debt portion. Win/Win?",Neutral
AMD,Banks lend “other peoples money” to make their own money. As long as its legal they dont care. They’ll also get bailed out once it all goes bust anyways or at least everyone there will get a golden parachute out.,Negative
AMD,"Irrational exuberance, remember?",Neutral
AMD,"because most of the comments are from gamers with little disposable income, who know as much about tech as they do about finances, i.e. very little to nothing at all.   Anything that has to do with AI gets dealt with by a lot of people here as a threat to them being able to purchase a new gen GPU to play games, given their forementioned constrained disposable income.",Negative
AMD,Loans have positive societal implications as they help to create wealth by providing liquidity.,Positive
AMD,"Reminds of that [""economists paying each other to eat shit"" joke](https://i.imgflip.com/7y5x5a.jpg)",Negative
AMD,The market would certainly be workout it indeed lol,Positive
AMD,"Agreed.  Nvidia has the cash. AMD doesn't, so it has to be a stock deal.",Neutral
AMD,"I don't necessarily disagree with your statement. My point was that just Nvidia+AMD alone for OpenAI was going to be 16 GW of compute power. That's huge.  I don't think we'll come anywhere close to that unless we see some tangible ROI.  That's why these deals are structured the way they are, it's not an up-front payout but rather as compute gets built up.",Neutral
AMD,Nvidia investing in Intel is fact. AMD using Intel fabs is rumor.,Neutral
AMD,"I am trying to understand your viewpoint. You do not think you can do any fundamental analysis on a stock to find out its fair valuation? So how do you then find out if stock is over or undervalued? By your logic, where supply and demand determines valuation, stock is always 100% perfectly valued.   Also, not sure why you are mentioning Lisa Su in context of AMD valuation analysis? Of course company CEO wants bigger valuation.",Neutral
AMD,"I don’t see how saving 2% in a 401k for employee match was really going to help a guy who’s point was that his ability to earn future income has been decimated by AI, but sure go ahead and shame him for that while you’re here. Like great, he could early withdraw 50k or whatever from his 401k to stay alive a little longer, and still not have it in retirement. Damn people on the internet are brutal.",Negative
AMD,Right? You cant blame AI or [insert most recent tech advancement] on mistakes you made over 20 years ago,Negative
AMD,"It is critical to not become reliant on these LLMs to preserve what is going to become a proprietary skill set in 10-15 years, critical thinking, strong communication skills, and problem solving. Use LLMs to augment your workflow, but do not become reliant.   I have a strong feeling that there will be a major shortage of skilled white collar workers in 10-15 years as the boomers die off, GenX ages out, and GenZ/Alpha are totally reliant on these tools. There will be major money to be made for folks that cultivated their individual capabilities versus becoming reliant on LLM tools.",Neutral
AMD,"Hopefully by then hardware will have advanced enough to run local llms decent enough for productivity like programming and general tasks/knowledge. If Nvida has ~75% gross margin, TSMC has ~ 50%. If the bubble bursts and margins collapse we could get much cheaper powerful GPUs, not to mention the oversupply of used AI datacenter gpus which will have to go somewhere, I bet someone will take them and run them for cheap cloud ai.",Neutral
AMD,This sub severely overestimates the market penetration of Instinct for some reason.   I think this is a very good move by AMD. Since they have had extreme difficulty to overcome their poor compute software stack. This finally gives them a marquee customer and an extra revenue stream in terms of investment.,Positive
AMD,"Approximately 149,521 per second - well, double that if you want to first boil the water as opposed to just raise the temperature to 60 degrees C.",Neutral
AMD,"yeah that's neat, to bad earths biosphere doesn't give a fuck about our plans of efficiency.",Negative
AMD,The problem with that is there is a significant shortage of gas turbines. The wait time for a new turbine order has grown from 2 years to 5-7 years.,Negative
AMD,Do yourself a favor and look at human life expectancy versus our energy usage. There is a direct correlation.  Would you rather live in the Stone Age with the life expectancy of 25?   You’re free to live in a cave without any electricity if you think it’s so evil.,Negative
AMD,"So if you go to a sub called transportation, see the hot new shit in transportation is cars and go ""nah, this sub is not called cars, lets just talk about horse drawn carriages.""",Negative
AMD,"Alibaba has a strong AI team (Qwen) and they are outside of the silicon valley bubble. So if they believe scaling is the way. That means they too exhausted all the other options. It's just an independent confirmation of the ""scale is all you need"" thesis. The fact they are investing $56B into the infra shows strong conviction as well.  https://www.reddit.com/r/LocalLLaMA/comments/1nq182d/alibaba_just_unveiled_their_qwen_roadmap_the/  And you can tell from their Qwen roadmap that they are scaling everything at least 10 fold.  I was already suspecting this was the case for the next step of AI. But this was the confirmation for me that everyone is doing it.",Positive
AMD,"The Netflix documentary on all this is going to be a real banger, lol.  Just worried about how the risk of recession being created here and its impact on my finances and employment, lol. Right now my skills are in huge demand though so I'll just get paid while the money is still flowing.",Positive
AMD,Its the scale of the thing that gets me.  Microsoft literally bought 3 mile island and is restarting the nuclear plant to power just 1 datacenter....and in response Google is personally building 3 nuclear plants for their own datacenters.,Negative
AMD,"Well, Dodd-Frank was passed in 2010 to replace the repeal of Glass-Steagall you're mentioning that led to the Great Recession. Only problem is that at the time the concern was primarily the big banks so big tech still essentially has free reign to do crazy shit like this.",Negative
AMD,So basically the US/Europe in the 2010s?,Neutral
AMD,"Maybe they could get a stake in Intel, and then Intel gets a stake in NVidia, it would be almost poetic.",Neutral
AMD,so you think google will write a coding language for AMD? Also i dont think google should be on this list because they designed their own chips.,Negative
AMD,Right but its investors can do that without needing to invest in OpenAI. It's not OpenAI's money thats being spent here its investors money and they can just invest in AMD without the middleman of OpenAI.,Neutral
AMD,Just because people are buying assets doesn't mean it's similar to the subprime lending crisis.,Negative
AMD,Deepseek is close   Llama though is utter shit. Probably the worst frontier model at the moment.,Negative
AMD,That's what I expect will happen.,Neutral
AMD,"Exactly. ""The Banks"", as a nebulous singular entity, might feel some need to step in to prevent underhanded ponzi schemes, but the actual people running the banks don't give a shit so long as their contractual bonuses kick in off of these investment circlejerks before the bottom falls out. Nobody responsible for the subprime mortgage crisis suffered any consequences for their actions, so why would they even pretend to give a shit about causing another or three?",Negative
AMD,"We need a small army, nay, a fleet of Lina Khan's if we ever want the US economy to make sense again.",Neutral
AMD,So you don’t agree with the fractional reserve system?,Negative
AMD,"""Waaah, the gamers don't like my AI slop!""",Negative
AMD,">create wealth  This, this right here is the problem.   ""Wealth"" isn't created. Products are created, services are created/rendered, wealth is accumulated through the transactions of the other two to outside parties.",Negative
AMD,"Sure, both paid openAI to buy their chips, but NVIDIA got a stake in OpenAi, AMD didn't. Honestly, I don't see how this is good for AMD in the long term, it doesn't make AMD any more of an AI player unless OpenAI starts buying AMD chips with their own money and AMD makes a viable gpu interconnect solution and software stack for AI training, not inference. This deal helps with neither, so how does an effective donation from AMD with nothing upfront in return and nothing guaranteed in the future (openAI doesn't pay anything if they don't buy any AMD chips, they just don't get free AMD shares), justify a 25% stock increase on a 10% ""investment""??. It feels like AMDs stock acquisition of Xillinx which they overpaid for and pumped the price due to an arbitrage loop, but ultimately tanked the price due to massive dilution, high PE which drove away investors, and overall growth slowdown due to xillinx's middling financials.",Neutral
AMD,"There's no such thing as ""fair valuation"" on a stock.  The valuation is what the market will bear.    It is always ""perfectly valued"" in the sense that the price reflects the market.  It also makes no sense to ask of a stock is ""perfectly valued"".  When Warren Buffett picks an undervalued stock, he's picking a stock with a valuation that makes likely he will see above market gains and below market risk within his time horizon.  That's a well posed question that can be answered.    ""Fair valued"" for a stock is a ill defined nonsensical concept that's has mind share because it sound like common sense, but the logic falls apart if you dig deeper.  The intelligent investor should never ask "" is this stock fair valued"".  That's like asking "" is this shade of color bright"". Bright compared to what?  Under what conditions?  What's the purpose of determining the brightness? Without a reference standard and a usage scenario ""bright"" is arbitrary subjective nonsense.  So in your scenario what's your time horizon?  Lisa Su knows what her time horizon is for AMD stock, it's whenever her large institutional investors need to close books for the year.  She also knows the market's reference standard.  It's Nvidia stock.",Neutral
AMD,"Because had he done this when his earning potential wasn't impacted, he'd have been able to build a relatively substantial retirement portfolio.   That's the point of retirement savings for 95% of Americans who make <$200,000 annually; You chunk it out, in small increments, over a long period of time, and let the compounding take effect.   Say they started withholding $5,000/year for their 401k 20 years ago. Factor in the 3% match, that's $5,150/year. After 20 years, assuming 7% returns (hyper conservative over the last 20 years), with just annual $5,150 contributions, they'd have $238k right now.  Now, let's use the actual market data. Same parameters as above, but with a ~16% returns (thats the avg over the last 20 years for VTI), they'd have $811k right now. One can realistically live off of that for 40 years if you live very very frugally, combined with SS.  You throw in an additional 1-2k annually and this is even more of a nest egg.  OP absolutely fucked themselves not saving for retirement during the most insane bull run we've seen in our lifetimes. People must accept responsibility for their decisions, including not planning for retirement.",Neutral
AMD,"Yes and withdrawing from a 401k comes with penalties too. Its actually quite bad if the extremely overvalued market is going to tank and you need that money pre-retirement, like for a house, which could be a better long term retirement investment and gives you a place to live. In every stock bubble, big investors make money of the stock market hype while retirement investors and some retail investors buying post-ipo, buy and hold at any price and always take the full loss.",Negative
AMD,"Buying Instinct GPU does not make the compute stack instinctively better, though.",Neutral
AMD,"Oh yeah, to be clear what's ACTUALLY going to happen if all these datacenters get built is lots of blackouts. Just saying everyone is going to jump on gas to try and fill the void, but like you said.. there isn't enough turbines being built to actually power all these datacenters.",Negative
AMD,"Great logic there, I guess I should buy some space heaters and run them at full blast 24/7 to extend my life span.  AI is a grift just like crypto. Few will benefit from it at the expense of everyone else. At least there's *some* practical applications in medical fields and the like and not just in tax dodging.",Positive
AMD,I see your reasoning. Thanks for the explanation.,Neutral
AMD,> The Netflix documentary on all this is going to be a real banger  I look forward to watching this projected onto the wall of a cave.,Positive
AMD,"If I may ask, what kind of skills are in huge demand these days?",Neutral
AMD,"It makes sense. local production for your own datacenter is much easier to deal with than a collapsing national grid that hasnt been maintained in decades. Nuclear also makes sense because nuclear produces the output irrespective of any other factors. It does not care if the wind is blowing or sun is shining. Nuclear can also regulate its output very easily, so you can decrease output it demand decreases.",Neutral
AMD,"Sec rule 10-18b is still in place though (manipulating the stock market for profit is totally cool and legal), and iirc dodd-frank was largely neutered in 2016-17.",Neutral
AMD,">	then Intel gets a stake in NVidia  https://www.cnbc.com/2025/09/18/intel-nvidia-investment.html  Intel is not in a healthy posotion and has been declining for some time. That said, this just seems akin to Microsoft and Apple to defeat the ‘monopoly’ argument Nvidia is likely to face.",Neutral
AMD,"Google has incentives to work and push an open alternative though, so that it becomes easier for people to use their TPUs in their cloud. It’s surprisingly difficult to use compared to CUDA.",Neutral
AMD,"They are not investing in AMD. They are buying AMD GPUs they will put to use producing tokens which return $$$.   AMD is giving them a 10% stake at $0.01 per share (basically for free). AMD is basically giving them the ownership of the company for buying so much (millions of GPUs) from AMD.  So even though OpenAI will end up owning a portion of AMD, they aren't actually paying for shares (only $0.01 per share). They are paying for GPUs and getting stock in return as a rebate (sort of).",Neutral
AMD,OpenAI needs hardware to create their models. A lot of hardware.  That's why they're making deals with Nvidia and AMD.,Neutral
AMD,"I believe that's what was a key contributor to the subprime mortgage crisis.  Mortgages were being repackaged repeatedly and one mortgage was being used to collateralize several products that were sold as solid, safe investments each time.  They were also being deceptive about the credit ratings of the mortgages and putting more and more questionable assets into the package.  Sort of like OpenAI investing in AMD who doesn't really have much to offer in the AI realm at the moment, but it keeps the needle moving.",Negative
AMD,"> Llama   LLama is done for. Qwen is the way. Unlike Deepseek, you can run Qwen at a decent PC.",Neutral
AMD,Then you are more realistic than half the people in this sub who think AI will disappear like NFTs.,Neutral
AMD,The current reserve requirement is 0% lol. And theres a whole lot of difference in risk and circlejerkiness lending out money to a bunch of 800 credit score mortgages vs multiple billys to altmans co2 generator.,Negative
AMD,"""waaah the gamers"" indeed LOL",Positive
AMD,That is arguing semantics,Negative
AMD,"I'm curious to know what the penalties are (if any) if OpenAI should under-purchase AMD chips.  If minimal/no penalties, then I don't see why OpenAI wouldn't sign a deal like this, seems that AMD would be the one taking on most of the liability here.  The optics are good for AMD however. though I too wonder if the big stock bump is really justified.",Neutral
AMD,"You made a claim that AMD stock price is low. If stock price can be low, logically it means it can be high and fair. I want to know, how did you come up with claim that AMD stock price is relatively low? Which analysis and metrics you used?",Neutral
AMD,"Even this underestimates their savings by quite a bit.    $5,150/year with 3% match actually assumes they only make $5k/year.  The employer match is a percentage on their *entire salary*.  So if they made $50k and saved $5k/year, a 3% employer match would bring that up to $6,500/yr.",Negative
AMD,"Let me explain your hypocrisy:  You’re posting on Reddit a platform hosted on massive data centers spread across the globe, each powered by enormous amounts of energy, some from fossil fuels and some from renewable sources.  Every post, comment, image, and video you interact with requires electricity to process, store, and transmit.  The very act of being online right now depends on a vast network of servers, cooling systems, routers, and fiber-optic cables all consuming energy 24/7. So you can argue with random people online?  What’s more wasteful,creating models that distill information and make it useful, or you talking nonsense online?  You single out AI as if it’s uniquely wasteful and morally wrong.   You’re a fool.",Negative
AMD,hey man some cave cinemas can be dope and theres no sun glare in there.,Positive
AMD,Plumbing.,Neutral
AMD,It’s also crazy to think that Three Mile Island only generates 800 megawatts which might not even be enough for that one datacenter…Tesla is already building 1GW datacenters,Negative
AMD,"On the countrary, they have incentive to work on and push their internal solution for their TPUs, without sharing it with others.",Neutral
AMD,"> producing tokens which return $$$   Woah there, let’s not get ahead of ourselves.    Oh, right. They’re losing money on every API call, but they’re gonna make it up in volume.",Neutral
AMD,"It gets worse. AMD doing this dilute the ownership of all already existing investors, which usually leads to price reduction.",Negative
AMD,"The repackaging of mortgages into securities absolutely still happens today, that’s the entire point of Fannie and Freddie Mac. The whole problem in 2008 was the banks were offering no-document loans with variable interest rates, when those loans went from cheap to expensive people couldn’t afford them anymore. Turns out they also marketed those loans as very healthy in the securities and poisoned the well so to speak.",Negative
AMD,"No, deceptively repackaging dangerous mortgages so they can be resold is different from openapi investing in AMD.    You need to prove AMD has structured the company such that it's designed to misleader investors about its financial position.   As-is, this is just typical redditor fashion of squeeling about something they don't understand because they feel like it's bad and that other thing was bad too.",Negative
AMD,Altman needs AMD because that is a concrete investment.  AMD needs Ai because relying upon concrete alone doesn't get one up in the air so easily.,Neutral
AMD,Would you say qwen is what someone should try to run? I remember Microsoft’s phi (I think that was the name?) and gemma were quite decent some time back? Is (reduced size) kimi considered any good?,Neutral
AMD,"NFTs provide no value. AI, even in it's current nascent state, does.",Negative
AMD,It's really not. The ideal that wealth can be destroyed or created is a perpetuated misconception that the ultra-wealthy and corporations use to take advantage us normies.,Negative
AMD,"I should have stated more precisely that the AMD leadership believed stock is low relative to competitor valuations in the market and the valuation that could be achieved with more AI sales.  This was factually correct on both counts: 1) compare amd pe to Nvidia 2) see AMD stock performance day of the announcement  Being relatively low or high versus comparable companies in the current market or versus anticipated pricing in some future scenario is well defined and relatively simple in concept, free of hidden assumptions.  Being ""fair"" is not in the sense that you were putting forth where a company's stock could be deemed fairly valued based on fundamentals compared to some absolute standard or some historical average or market average.  Now you have dragged in a huge number of hidden assumptions and subjective taste, and wedded yourself by taste to a philosophy of valuation that fits poorly with the market's actual dynamical behavior.  All of this leads to confusing analysis that tends to bury rather than surface actionable insights.  Investing based on ""fundamentals"" is actually hugely complicated.  You use a simple seeming set of analyses but all of the above complexity is shoved into candidate sector selection, company selection and trade execution behaviors based on inference centered human neural network training (ie taste and intuition) combined with complex logic.    Only very few people can do that in a way that beats the market consistently after a lifetime of experience. For the type of decision the AMD leadership was making it would have confused the issue.",Neutral
AMD,You're not in a position to explain anything technical.  You have no idea how many people you can serve a website for on a 500W server vs. how many AI users. It's orders of magnitude different and there are so many more legitimate use cases for regular Web services.  Keep believing the hype and don't forget to jump to the next buzzword once AI becomes stale.,Negative
AMD,I wish OP would share his wisdom here as he was sharing it for everything else.,Neutral
AMD,"Microsoft is incorporating AMD so Azure is a platform businesses and researchers can use as a way to offload compute easily.  Google also uses Ryzen EPYC chips in their systems, and Google being the size it is can partition both work to be done for a more collaborative system and something much more private for internal use  Also your original point, Google isn't writing a language FOR AMD, they're developing it alongside AMD and Microsoft as an open alternative to CUDA. They've always profited not by closing off development but rather developing for mass adoption and selling services to utilize it",Neutral
AMD,A lot of people pretend to know the future but we don't.  OpenAI is ramping their business. Like Amazon did in the early days of the Internet. These guys are doing it in the early days of GenAI.,Neutral
AMD,It's not a straight dilution. It's based on milestones. For instance the last tranche of the vesting happens when AMD reaches $600 price per share. Which coincides with a $1T market cap.  The investors are thrilled about the deal. I'm an investor in AMD since 2016.,Positive
AMD,"The proof comes after the crash. This is nothing like the.com bubble at this point, but it is similar to the subprime mortgage. The key difference between this and the subprime mortgage crisis though, is that people are not at risk of losing their homes to the same degree that they were at risk with the subprime mortgages. When this bubble burst, it is unlikely to threaten the stability of the entire global economy and people will not be losing their homes as a direct result. Indirect? Probably, but not directly linked to one another.  Edit: So we are clear, ""the proof comes after the crash"" isn't meant to mean there's no warning signs or signals, but this isn't the first ""obvious bubble"" in the economy since the subprime mortgage or dot com bubble, and most do not result in a crash.  We never truly know when something is a bubble until the crash, failures are always far easier to diagnose and address after the fact.  Remember when the bitcoin bubbles popped at $20k, $60k (twice!), and $100k?  Yet it would be hard to say bitcoin ""crashed"".",Neutral
AMD,Microsoft's Gemma always does better on synthetic benchmark than actual real world use. Go to /r/LocalLLaMA and you can find better answers there.,Positive
AMD,*...sad angry grown ass man playing video games as an identity noises...*,Negative
AMD,We are significantly wealthier today than say hundred years ago by basically any metric.   As for how loans can create value: Say you have a small carpentry business. You build tables. You finance better quality equipment to increase your productivity. That increase in productivity should outweigh the cost of the loan. Without the loan you couldn’t get the equipment though and couldn’t unlock that value  When loans work welk they are just win win,Positive
AMD,"Look, you’re doing it again wasting precious energy arguing with randoms online. Why won’t you think of the climate?!  Lol, it’s not just a single server, bud. It’s the routers, the switches, the servers, the backhoes digging up the road to lay fiber so you can log into Reddit and argue with randoms online.  Why don’t you just delete your Reddit account, throw away your phone, computer, tablet, modem, router, and switches, and go live with the Amish?  You’re a hypocrite whose comments are being used to train the very AI models you hate so much.  Loser.",Negative
AMD,Noones doing AI work on EPYC chips. Its their Instinct carads that are used for AI. and Google does not use those.  Googles profitable endevours were always closed sourced.,Negative
AMD,"I hate to break it to you, but the novelty and thus the coolness factor of LLMs has declined recently, in my little bubble most people turned to ""miss me with that AI shit"". Be it content, coding and even writing. Thinking someone will knowingly pay for AI generated content is a tough bet to take imho.",Negative
AMD,The usual comparison is AWS and that made money quite early on (but re-invested it) OpenAI is burning cash like no tomorrow and does not hold shit in assets.,Negative
AMD,I can see why that would make investors more likely to stay. Personally i got shares of all the tech industry players in a 15+ years hold positions and just going to see what happens. But my numbers are so small i dont matter as an investor.,Neutral
AMD,Yes there are a lot of differences between this and the subprime mortgage crisis.   That's why I am saying openai investing in AMD is very different from the subprime mortgage crisis.,Neutral
AMD,"Thanks, I used to frequent that sub but I've fallen out of touch for a while",Neutral
AMD,>You’re a hypocrite whose comments are being used to train the very AI models you hate so much.   And you probably think that's a good thing,Negative
AMD,This is worse whataboutism than what I'd find in /r/worldnews,Negative
AMD,I was like that with Social Networks. I couldn't imagine that people would be buying shit from Facebook ads. Yet these guys have been printing money for 15 years.,Negative
AMD,"There's also plenty of similarities, like the OpenAI > Oracle > Nvidia back to OpenAI parlay where $100 billion of future sales is being record as $300 billion in total future sales with no actual money changing hands and each company seeing large increases in stock valuations.  We're not seeing the insane overnight valuations increases of the dot com bubble where multiple six month old companies are being valued at $100 million on $35,000 of total revenues and expenses, with new companies being founded every week, but we are seeing those same assets being repackaged and resold over and over again.  No two situations are exactly the same, but there's plenty of similarities between the two especially in how they are conducting their business and financials.",Neutral
AMD,"Ok, I’ll bite. Who convinced you that AI is the next great evil that must be stopped immediately, or else we’re all going to die and the world will be destroyed?  Just to point something out Google Translate has been using transformer based neural networks for years to generate translations language.   The same architecture behind modern large language models.. is what allows Google to translate between languages.   The difference now is scale and training scope instead of just learning translation patterns, these newer models learn broader relationships across all kinds of text.   So… how exactly is that a bad thing? Please explain to me why I should immediately stop using these models, and why they’ve so awful?   Thanks",Neutral
AMD,"AMD are such scumbags for giving the 8400F its name when it has half the L3 cache of the 7400F.  They also did this with the 5700, which was **not** just a slightly lower clocked 5700X, unlike the 5600/X and the 5800/X. It also had half the L3 cache.",Negative
AMD,Finally we are getting RT CPU benches!!!,Positive
AMD,"Crazy for AMD to name the 8400F as it is. It's the mobile Zen4 variant with half the L3 and lesser PCIe spec, while the 7400F is basically the real desktop 7600 with the iGPU disabled.",Neutral
AMD,I've been using a Ryzen 7500f since June 2024 and am still amazed how much performance you can get out of a CPU that only costs 130€.  It's paired with a RTX 5070 at 1440p and I am still almost exclusively GPU limited according to CapframeX.,Positive
AMD,8400f name is scummy but it is really good cpu for under 100$.,Positive
AMD,"Snagged a 8400f for like $85 on ali express a few months back, works in my friends 5070 build as a great placeholder cpu on am5 for an eventual x3d upgrade.",Positive
AMD,8400F = Ryzen 5500 + DDR5,Neutral
AMD,wish they would release the 7x00Fs globally,Neutral
AMD,Still not a single actually CPU bottlenecked game in the tests.,Negative
AMD,Should've also used a slower gpu to show budget users all these cpus are pretty fast for gpus you'll use for 4-5 years,Neutral
AMD,"I feel this way about the latest AM5 chipsets too. B840 is their A620 replacement, but now has the impression of being a mainstream B-class chipset rather than an entry-level A-class chipset. X870 is their B650E replacement, but now has the impression of being an enthusiast X-class chipset rather than a mainstream B-class chipset.  AMD is not our friend.",Negative
AMD,"Amd is way more of a scumbags than you think.  5700 was not only half in l3 cache but also just pcie 3 while 5700x is pcie 4.  8400f is even worse , its not only pcie 4 but can only provide at most x8 lanes only to dgpu while 7400f is pcie 5 and also provide full x16 lanes to dgpu.  Well 5700 was pcie 3 but it was atleast x16 , 8000 series is worst by amd, taking advantage of less tech savy people",Negative
AMD,"8400F was supposed to be OEM-only, so it's ""below 8600G"" in their pricing models.  It was never supposed to be retail.",Negative
AMD,Nobody is buying the 8400F for it's value. It's an OEM part.,Negative
AMD,"I just built a 9600X machine and while I'm impressed with average fps performance, stutter struggle is too real. Booted up returnal on a 5060ti 16GB and the performance is significantly better than I expected but the game stutters all the time when it becomes CPU limited for brief moments, this won't show up well on average fps but it will result in visible stutters where the frame times crash as soon as the CPU gets pegged usually to stream in assets during movement (traversal stutters).     Similar issues on Oblivion Remastered, Borderlands 4, (this game even has shader comp stutters that of course peg the CPU) and Silent Hill 2. It is so common in today's AAA games, Zen 5 CPU performance just can't keep up with the over saddling of the CPU modern games love doing.",Neutral
AMD,It looks good in today's games but I wonder if the size of the L3 cache which is half of the 7400F could be a problem in the future. The 7400F is slightly more expensive but you get twice the L3 cache in addition to more PCIe lanes which are 5.0 speed to boot.,Neutral
AMD,"I would not call it ""really good"" more like ""average""",Negative
AMD,"No, that would imply the 8400F being the Zen3 mobile, Ryzen 5 5600U. The 8400F is based on Zen4 mobile Phoenix 7640H iGPU/NPU disabled.  But yes, in the aspect of the mobile variant ported over to desktop with half the L3, this time being AM5 unlike AM4 with the 5500.",Neutral
AMD,Nah 8400f is even worse   8400f = r5500 +ddr5 - x8 lanes   Yeah 8400f provide just x8 lanes to dgpu while 5500 being limited to pcie 3 but had x16 lanes,Negative
AMD,"Where's the 8300F that's a 8300G with no APU, which only supports PCIe4.0 x4 speeds?",Negative
AMD,"Agree - also with only 8 PCIe 4.0 lanes available to the GPU should have compared it to the other CPUs with 4060/ti etc., also frames per watt the 8400F isn’t bad and it’s got a 35w eco mode - would have been nice to see the comparison.",Neutral
AMD,"They should include intel cpu 12400,13400,14400,12600kf, 14600k and productivity benchmark as well",Neutral
AMD,I understand the b840 criticism but what’s wrong with x870?,Negative
AMD,"Like pretty much all companies, idk why most people on Reddit are AMD biased and hate NVIDIA for everything they do, both are equally as bad. The whole team green/red is so cringe.",Negative
AMD,I'm still waiting to see how pathetic a A820 chipset is going to be,Negative
AMD,What about X670,Neutral
AMD,the 8500G is even worse. 4x PCIe lanes to the GPU 💀,Negative
AMD,"All your listed games use Unreal Engine. It's nice for the devs, but the engine is just shit.",Negative
AMD,8 cores 16 threads has become the standard,Neutral
AMD,> really good cpu **for under 100$**,Positive
AMD,"Good for budget builds, these builds won't be installing a 5090-tier GPU.",Positive
AMD,I meant this is basically the same level of performance as 5500,Neutral
AMD,"They will probably do a video like that later, then follow up with a video that combines the AMD & Intel results; more videos equals more ad revenue.",Neutral
AMD,"I'd also like benchmarks in 720p, 1080p, 1440p, 1440p ultrawide, 4k, 5k, 8k with discord and chrome open and off, Steve wearing different shirts because we know those influence performance, the angle from Saturn to Jupiter, 5 different Linux distros with 7 different drivers and another 50 different data points because those ungrateful YouTubers are lazy and never do any work so why not spend 300h testing something that I want and three other people in the world?  And also could Steve bring me a Cappuccino real quick?",Neutral
AMD,"The name was intentionally misleading.  AMD has one actual chipset for AM5, Promontory 21. All the labels are marketing names for various configurations of that chip on a board.  Each P21 has 12x PCIe lanes, some USB3 hosts, and SATA ports hanging off it (among other more minor things). Every difference between the marketing names other than that comes down to certification for the most part, like if they officially say the chipset lanes support PCIe 5, or if they require the motherboard maker to support USB4.  B650, B650E, B850, B850E, and critically, X870, use one of these chips.   X670, X670E, and X870E use two of these chips.  In effect, this means that X670 *vanilla* gives you 8x more downstream chipset PCIe lanes (4 are used to connect the two PROM21 chips), more USB ports and more SATA ports, than its more expensive successor in X870.  AMD basically named X870 to make it look like a replacement for X670, when it isn't. It's missing considerable IO functionality that X670 has by having that second IO chip.",Neutral
AMD,"[This graphic](https://imgur.com/a/QOWXgOl) shows how the ""800"" series of chipsets compare to their ""600"" series predecessors in terms of features. As you can see, X870 is identical to B650E in all aspects except USB4 going from optional to mandatory. The other high-end features of the X670 chipset in terms of lane counts and USB ports are reserved for X870**E** in the latest generation.  Source of the graphic: 12:35 of [this Hardware Unboxed video](https://www.youtube.com/watch?v=H0axuzyN_1s).",Neutral
AMD,The world is unfair to a $267 billion dollar corporation because its not a $4 trillion dollar corporation,Negative
AMD,"Neither are good, but they're not equally as bad. nVidia is by far much worse.",Negative
AMD,i keep thinking about buying amd and then realizing i value intel chipsets' i/o flexibility too much,Neutral
AMD,What about it?,Neutral
AMD,Yes I hate the fact I have to research this. Why is the newer stuff worse than the old..,Negative
AMD,What the......amd really going intel path in term of being shitty huh,Negative
AMD,"These issues aren't limited to unreal engine. Elden Ring and Nightreign suffer from similar issues, Starfield, Watch Dogs 3, Monster Hunter Wilds, The last of Us Part 1, Spiderman 1, Spiderman Miles Morales. That's a wide array of games from a wide array of engines, none of them use Unreal Engine. The issue has more to do with modern game workloads and how devs load balance (or neglect to) said workloads on the CPU.      The big picture is you can't expect to play modern games and have a smooth experience like you used to, modern gaming can feel like you're playing using 4 way SLI with GTX 480s. These stutters take you out of the experience. It's a big regression and one we're gonna need massively more powerful CPUs to overpower.",Negative
AMD,"It's nice for gamers too, because then I don't need to get hyped for it and spend my money on something else",Positive
AMD,"That has no effect on what I said. The majority of those games were tested on 9800X3Ds by Digital Foundry. The reason why stutters are so horrible is because you can't over power them, a 285k with 24 cores nor a 9950X3D with 16, will not get rid of the stutters hell a 9800X3D with 8 cores will outperform a 24 core 285k in these stutter scenarios.",Negative
AMD,"Not to be pedantic but he didn't show 5500 performance.  [https://www.pcgameshardware.de/Ryzen-7-8700F-CPU-280504/Tests/APU-Mobile-Desktop-Phoenix-Review-1450790/2/](https://www.pcgameshardware.de/Ryzen-7-8700F-CPU-280504/Tests/APU-Mobile-Desktop-Phoenix-Review-1450790/2/)  [https://www.pcgameshardware.de/Ryzen-7-5700X-CPU-278283/Tests/5600-5500-Release-Benchmark-Preis-vs-5800x-1393445/2/](https://www.pcgameshardware.de/Ryzen-7-5700X-CPU-278283/Tests/5600-5500-Release-Benchmark-Preis-vs-5800x-1393445/2/)  The 8400F is much closer in perf to the 5800 while the 5500 lags behind considerably. Although again, I'll reiterate my second point for the sake of argument, they're both mobile ports, which isn't exactly the best value compared to their real desktop counterparts.",Neutral
AMD,"When the battlemage overhead issue was discovered i asked them to do  5600 vs 12400 with b580 like that, but they denied it.  Plus the productivity part, channel name is hardware unboxed but in terms of productivity benchmark they are only limited to gpu/cpu launch video.  If I wanted to check productivity for the CPU the last video was 245k review 11 months ago.",Negative
AMD,JFC grow up.,Negative
AMD,"This sub consists of people who use a few usb ports for mouse and keyboard, audio out, and a gpu for graphics/display out. Of course they don't care/know about this nuance.",Negative
AMD,"All those titles you mentioned are infamous for poor optimization on PC.  Yeah, PC ports have been poor these past few years, to say the least.",Negative
AMD,https://youtu.be/_MK0wO7MXuc  As per this video results are completely different,Neutral
AMD,What do you even think grow up means?,Negative
AMD,"Spiderman 1, MM and Starfield were not bad ports. Take a look at Alex's conclusion of his Starfield review, mediocre perhaps but not bad and the game had worse issues on console yet it had a pretty glowing review by John when he reviewed it on console.   Here's a recent example: Expedition 33. People claim the game is very optimized yet it suffers from many of the stutters I posted above, on top of this it has camera animation stutters. Its not a bad port but it needs some work, yet still you can't have a smooth experience on it and that's the issue. I named a lot of games that are imperfect so let me leave you with one that doesn't have any of these issues: Doom The Dark Ages. iD software is simply exceptional.",Neutral
AMD,"Life experience, and considering other perspectives if lacking said life experiences.",Neutral
AMD,And how does that apply to criticism of nVidia exactly?,Neutral
AMD,"Actually, even though it's not an official version - it's pretty good for 6000 series owners. If FSR4 INT8 delivers better quality than XESS, even when you compare fsr performance and xess quality, but around the same performance uplift - it's good.   And even if FSR4 int8 gives you only 10-15% uplift in performance it's still a huge deal with games like cyberpunk, because you just don't have any other option other than fsr3 or xess.",Positive
AMD,Yep been using it on my ROG Ally X for the past few weeks and it's awesome. On the small screen even a performance upscale still looks good vs FSR2/3 which just looks like garbage.   A noticable upgrade from XeSS for RDNA3 users.,Positive
AMD,FSR4 is worth the extra performance cost. FSR 3 is shimmering mess with very bad fine detail reconstruction.,Negative
AMD,"It already works so well in this half finished state, I crave for an official release.",Positive
AMD,"Works great on the Ally z1e with Expedition 33. The difference was night and day , or maybe FSR3 on UE5 is just trash .",Positive
AMD,"More options is always good, this has better stability and quality when you want that.  An other positive effects is that if you start to use Optiscaler you can fine tuning the resolution of the upscaler to get the best framerate you can afford, regardless of the version of FSR you want to use, which is more important on RDNA2.",Positive
AMD,"I just want it on the Steam Deck, so the occasional game that runs at 40fps with DLSS balanced can get a locked 30 with better image quality.",Neutral
AMD,Can I try this on rx6400 in a mini itx?,Neutral
AMD,"so according to their charts, unless you already have a gpu that can hit 1440p well natively, there's no reason to even think about this.  like the point of upscaling is to improve performance and fsr4 gives so little fps gains that if you're trying to reach 60fps at 1440p you should already be running at 50fps+.",Negative
AMD,TBH even with severe performance cost of FSR 4 INT 8 on lower - mid range GPUs It is now the undisputably the best option for upscaling for non RTX GPUs now despite it being worse on image quality compared to the official FP8 version and DLSS 4.  Now I am really curious to see a test of something like RX 6600 with FSR 4 Int 8 vs RTX 3050 with DLSS 4 just to see which hardware based upscaler is more efficient and can produce better performance on older hardware.,Positive
AMD,Reddit finally admitting that FSR3 and under were in fact terrible after all now. 😀,Negative
AMD,"With good reason, as it turns out. I tested it yesterday in AC Shadows on my 7800XT and  the performance is just not good.",Negative
AMD,"If only AMD had embraced a better model back when.  At the very least RDNA3 would have launched with the necessary grunt. Instead AMD listened to all the fans saying ""this is fine"" to crap like FSR 1-3, which everyone admits is trash now.",Negative
AMD,Most games run at 4k balanced medium at 60 FPS before frame gen on my 6800XT. I maybe able to push FSR4 at 4k on performance with a mix of low/medium with an OCed 6800XT (+12% over ref perf).,Neutral
AMD,"Anyone have a link to the .DLL,? Cant wait to try on my 7900xtx",Positive
AMD,Does this work on my Vega 56?,Neutral
AMD,"At 15:38 Tim says that XeSS Quality has higher internal resolution than FSR4 Balanced. Didn't Intel change their whole naming scheme with these modes?   I thought at one point they renamed their ""Balanced"" mode to ""Quality"" and all other levels as well were knocked down a notch in resolution. They should have the same resolution I believe. Or was that only for the full XMX version on their own GPUs???  All that makes me question if it's worth using over DP4a XeSS. The performance gains on DP4a are often larger, and if FSR4 Balanced looks similar to Quality DP4a XeSS, maybe by sometimes DP4a is the better option.   But I've also seen some very slow running XeSS implementations. It was a joke in Cyberpunk at launch costing like 2-3x the milliseconds to compute compared to FSR2.",Neutral
AMD,"Not sure how you manage to have worse thumbnails than even Gamers Nexus, but... Congratulation. You won that competition.",Negative
AMD,How do you hide alpha code? It’s published right there.,Neutral
AMD,"Weird thumbnail since even before launch they said that it ""might work, but we are focusing on 9000 first"".",Negative
AMD,Should buyers forgive AMD? No,Neutral
AMD,"Yep, they tried to hide poor performing (framerate wise) implementation of their tech from you.  Edit: Okay, okay, I was being facetious.",Negative
AMD,Well they shouldn't have. DLSS was the primary reason that I went with Strix Point + 4060 over Strix Halo.,Negative
AMD,"you can't use this against AMD, maybe they're secretly working on it behind the scenes and will bring RDNA3 to older graphics cards for it to age like the fine wine it is!",Neutral
AMD,It's also a good alternative as some sort of better anti-aliasing option for less demanding games as well. compared to native TAA that always looks like crap.,Positive
AMD,"The examples I saw from 3 to 4, both look really good still.  HOWEVER, this one example [here in the video](https://youtu.be/yB0qmTCzrmI?t=1090) is my biggest complaint with 3 by far which seems to be solved with 4.  Yes, the absolutely tiny details aren't as good with 3, and they look better with 4, but that for me was basically a non-issue.  The real issue was particles and ghosting.  For most games, again, it wasn't an issue, but rain, snow, games in space, or thin objects like ropes moving quickly had horrible ghosting and I'm very happy to see that doesn't look like an issue anymore.",Positive
AMD,it's been insanely good. from reviews the picture/video quality is an improvement. and brings new life into the older gen GPUs.   almost as huge as introducing x3D cache into cpu. AMD has essentially extended life of both their products.   it's not on par with 9000s fsr4 but it's close enough from what I've seen from reviewers.   crazy cuz folks weren't sure if 6000s card could even handle it.,Positive
AMD,Too bad I tested it on a 7900XT and FSR 4 was still worse than XeSS 2 in some respects like shimmering around plants/trees using the same quality preset.,Negative
AMD,"""only 10-15% uplift"" :)   15-20% is usually generational jump so having that for free is ultra nice :P   no wonder they don\`t bring that up really",Positive
AMD,"yep, XeSS showed us this before.... Yes it performed better and looked a bit better on official hardware; but it is still a small performance+ better IQ anyone on older/nonsupported hardware would be happy to have as an option.   One time in recent memory AMD should have taken a page off of Intel's playbook to accelerate adoption. That and make the midrange flush with VRAM.",Positive
AMD,FSR3 is trash everywhere on lower resolutions.     Its only somewhat usable on 4K and even there it has clear and glarring faults.,Negative
AMD,FSR3 is bad generally. I found it particularly bad in E33 for some reason not sure if UE5,Negative
AMD,"Are you using it on windows by just dropping in the dll, or using optiscaler? I've yet to try it on mine.",Neutral
AMD,"I will probably replay Clair at some point, I'm looking forward to being able to fully max it out and play it on FSR4.",Positive
AMD,Yeah just add some optiscaler sharpening or contrast adaptive sharpening and it looks just as good as native,Positive
AMD,"You can run it on the Steam Deck, there's plenty of YouTube tutorials for setting it up.",Neutral
AMD,"Not entirely true, but it's also not your fault for thinking that. Percentage based performance charts for upscalers is unfortunately a very bad way of understanding what kind of performance uplift to expect. But because a YT audience won't really understand game performance in terms of frametimes and only in terms of framerates, I can understand why HUB chose to explain performance the way they did, even if I don't like it personally.  I could explain it here, but I suck at writing out stuff like this in a way that makes sense. So instead I'll push you over to [this thread](https://old.reddit.com/r/radeon/comments/1nvibgd/explaining_some_misconceptions_about_the_cost_of/). Read this thread for a full understanding, but for a short breakdown, this bit is the most important part:  > Imagine I have Game A, B and C which can render natively at 720p at 30, 60 and 120FPS respectively and upscaling to 1440p with FSR3 and FSR4. For FSR3 on the 6600 the 1440p upscale cost is usually ~1.2ms, while for FSR4 it's ~4.3ms. This is how switching from FSR3 to FSR4 would impact the performance profile in each game:  > Game A (30FPS = ~33ms to render a frame):  >     FSR3: ~33ms + 1.2ms for FSR3 upscale = 34.2ms = 29.2FPS  >     FSR4: ~33ms + 4.3ms for FSR4 upscale = 37.3ms = 26.8FPS  > Game B (60FPS = ~16ms to render a frame):  >     FSR3: ~16ms + 1.2ms for FSR3 upscale = 17.2ms = 58.1FPS  >     FSR4: ~16ms + 4.3ms for FSR4 upscale = 20.3ms = 49.3FPS  > Game C (120FPS = ~8ms to render a frame):  >     FSR3: ~8ms + 1.2ms for FSR3 upscale = 9.2ms = 108FPS  >     FSR4: ~8ms + 4.3ms for FSR4 upscale = 12.3ms = 81.3FPS  > On game A FSR4 gets 91.7% the performance of FSR3, on game B it gets 84% the performance of FSR3 and on game C it gets 75% the performance of FSR3.  This should explain that as your input framerate gets lower, the output framerate will be closer between FSR3 and FSR4. So actually FSR4 is actually more useful when you're trying to hit 60-70fps than it would be if you're already well beyond that.",Negative
AMD,"For the most part I agree, but at very low frame rates, where a game forces you to use ray tracing, it could be worth it. Doom The Dark Ages, Indiana Jones, and pretty much all ue5 games tank performance hard at high resolutions on RDNA2 and RDNA3. You're pretty much forced to use upscaling because of how unplayable some of those games are already, and will be in the future without it.",Neutral
AMD,"The point of upscaling is to improve performance, but FSR is more than just upscaler. Its also a temporary AA. One that is much better than most native TAA implementations.",Positive
AMD,bet 6600 still wins despite the fsr4 cost,Neutral
AMD,As a long term AMD user ive always complained about FSR and used XESS whenever its available.,Neutral
AMD,"There were games where FSR 3.x and even 2.x weren't bad. In Deathloop, FSR 2.x was very close to DLSS. In cyberpunk, FSR was always atrocious. Same for black myth wukong - both Nvidia sponsored. FSR was done very well in Warhammer 40k: space marine 2. Many examples where it was implemented well and was decent. Just as many examples where it sucked. Now AMD users have a better option, but it doesn't automatically make all prior FSR3 unusable. I recently compared FSR4 native AA to FSR3 native AA in death stranding and it was very close.",Negative
AMD,I have seen nothing but bashing of FSR 2/3 for years. Who are you shadowboxing?,Negative
AMD,"Very astute observation, also where have all the ""Fake frames"" people gone to?",Neutral
AMD,FSR3 is good to have because it works on *everything*. It has pretty much been carrying things like the steam deck on its back too. My 5700xt doesn't work with FSR 4 or XeSS so FSR 3 is by FAR the best option.,Positive
AMD,FSR4 is much better than any DLSS that came before it too. DLSS 1 was absolute garbage. And people called it better than native.,Negative
AMD,I'd rather be given the choice. it still is faster than native and better looking than any other upscaling options.,Positive
AMD,"This is extremely game dependent. For example, on Cyberpunk on my 7800xt, FSR 4 has the same performance as one preset lower in FSR 3. So FSR 4 balanced has the same performance as FSR 3 quality.  But even FSR 4 performance looks much better than FSR 3 quality. So if you equalize visual fidelity, FSR 4 gives better performance than FSR 3.  In other games, like AC Shadows, the performance difference between FSR 4 and 3 is huge.",Neutral
AMD,Does that game support XeSS? The performance increase sounds like it's larger. That's why I'm not sure how much of an upgrade this is really.,Negative
AMD,"in 2020 AMD was openly laughing at Nvidia for its AI promotion and Lisa said it will bancrupt the company. In 2025 AMD releases their own solution. 5 years, the developement time of a chip. Turns out AMD was totally unprepared.",Negative
AMD,Vega literally doesn't have the compute support for INT8 that makes this work. Only RX 6000 and above do.,Negative
AMD,No,Neutral
AMD,He's got it the other way around. XeSS 1.3+ quality has an upscaling factor of 1.7x while FSR quality is 1.5x. XeSS quality's upscale factor is equivalent to FSR balanced.  So XeSS was actually being compared at a disadvantage here.,Neutral
AMD,"> At 15:38 Tim says that XeSS Quality has higher internal resolution than FSR4 Balanced.  You're right about Intel adjusting their naming but Tim was still comparing FSR4 Performance to XeSS Quality. At that timestamp, Tim said ""but on the balance of things..."" as in ""all things considered.""",Neutral
AMD,Forgive them for what exactly?,Negative
AMD,"Classic case of someone not watching the video and getting the takeaway.   Tim's takeaway was that even if you performance normalise and compare FSR4 INT8 performance vs FSR3.1 or XeSS Quality, FSR4 INT8 looks significantly better than both still. And it only looks better in comparison to the above two when you lower input resolution.   Your takeaway is like saying Intel should have never released the DP4a version of XeSS because it also performs much worse than FSR3.1. Bad enough that Intel changed the meaning of each quality preset to refer to the one below.",Negative
AMD,this,Neutral
AMD,"This. Some people just assume with enough performance you can ignore DLSS and FSR4, when most of modern games just look bad without any AA, slightly better with TA/FSR3/XeSS and actually good with DLSS/FSR4",Negative
AMD,">only 10-15% uplift"" :)   15-20% is usually generational jump so having that for free is ultra nice :P    It's not 15% performance for free, you're lowering your input resolution to 66.7%.",Positive
AMD,"It look playable in starfield,las of us, ghost 1 , gow 2, etc (pixelated ghosting+shimmering but playable still) . And unplayable in e33 , remnant 2 , cyberpunk,etc imo ( extreme pixelated)  .",Negative
AMD,"UE5 is built on temporal AA interacting with FX in various ways. FSR3 does not handle this well at all. You get that trademark ""fizzle"" on anything relying on temporal accumulation when you move, and if the game is using lumen this is...well.../everything/ being lit.    Prior to FSR4 int 8 XeSS or even TSR was much, much better than FSR in UE5 games which at 1440p and lower just looks bad.",Negative
AMD,optiscaler and the dx12.dll on optiscaler test build discord .,Neutral
AMD,I mean native through the ui,Neutral
AMD,okay i get the rest but this part confuses me. why is the final fps lower on both fsr 3 and 4?   >FSR3: ~33ms + 1.2ms for FSR3 upscale = 34.2ms = 29.2FPS >FSR4: ~33ms + 4.3ms for FSR4 upscale = 37.3ms = 26.8FPS,Negative
AMD,"Tell me about it. My rx7600 cant even get 60fps on all low 1080p in Doom. And since i have a 1440p monitor, i usually play at 1440p fsr Q. so its even worse there with regular fps being in the 70s but 1% lows hitting the single digits for reasons i dont understand.  to play this game at 60 smoothly i'd have to run it at 1080p on a 1440p screen AND THEN use fsr Q to upscale to that 1080p anyway. i had to choose between blurry mess and horrifically unstable fps and i chose to uninstall",Negative
AMD,For my case its to get me comfortably over 60fps in AAA games. I run a low end GPU (RX7600) and a 1440p monitor so im not getting native 1440p60. And fsr AA would lower my already sub 60 frames even more.,Neutral
AMD,"I've seen a lot of FSR defenders on this site, especially in discussions about nVidia.",Neutral
AMD,"Look at the [older threads](https://www.reddit.com/r/Amd/comments/o5rjm8/amd_fidelityfx_super_resolution_fsr_review_big/) on r/Radeon r/AMD there were plenty of FSR 1 defenders there, even delusionally thinking that they are closely matching the image quality of DLSS 2 - 3.",Neutral
AMD,Fake Frames is typically about Frame Generation and not Super Resolution,Negative
AMD,"Same place the ""useless because latency is intolerable"" people went",Negative
AMD,"Noone called DLSS 1 ""better than native"", LMFAO. Stop revisioning history.",Negative
AMD,"DLSS1 was just smart naive upscaler, the only thing it was beating is FSR1 and cubic upscalers. DLSS2.0 is where they introduced temporal solution and DLSS2.1 is when it became AI upscaler. It was 2.1 and up that people started calling better than native.",Neutral
AMD,"Last time AMD went ""rather be given the choice"" route people got  VAC banned for the driver injecting code into games memory. I think AMD is right to be cautious.",Negative
AMD,"I guess the idea it's work in progress. In some cases FSR4 Q gave only 2% uplift compared to native TAA. This is just not good. Won't be surprised if in some situations FSR4 leads to perf regression.  Of course this line of thinking only good if AMD is actively developing int8 path and planning its official release. Not everyone would be too kind to it with such small perf uplifts. The other way to do things is do ""live preview"" or ""beta test"", release the current version and work on it in the meantime.",Negative
AMD,"amd isn't nvidia, they don't release slop. they want to make sure they realease quality products for gamers",Neutral
AMD,"Slight correction: Radeon VII and Navi14 (5500/5500XT) also have INT8 support. Yes, the smaller RDNA1 die has support for some reason, but the main Navi10 die doesn't.",Neutral
AMD,"Fun fact, it doesnt have vp9 support either which is what youtube videos use. Ive had to decode youtube vids with my 4770k for many years now. While gtx 1080 supports vp9 no problems. If i could go back in time i'd def get nvidia. I only bought the vega cuz during the crypto boom around 2018-2019 it was very hard to find a gpu, and i found ebay seller selling vega 56s for very cheap, which was a 1 in a million find at the time. I also made a mistake not selling it before ethereum quit gpu mining, could have gotten easily 800-900 for it. Also, even when it was getting official drivers they've always been ass and always behind nvidia on features. Amd driver team is just a bunch of windowlickers.",Negative
AMD,"Of course it doesn't, fuck amd. Gtx 900/1000 had driver support until recently and us vega users haven't gotten any game support or anything since late 2023, on a gpu that's NEWER. NEVER BUY AMD",Negative
AMD,"It’s a poor takeaway to assume they are hiding it when they “unintentionally” put out a public build of FSR4 on github, with current rumors pointing to a broad FSR4 Redstone release.",Negative
AMD,well yea but if its any close to dlss the quality difference you can observe is pretty small,Neutral
AMD,I'm on a 4k ultra wide and it still looks like ass. The fizzle is the exact issue i had in E33. I found it far less of an issue in other games like control which also uses TAA and RT though the upgrade to FSR4 is massive even there.,Negative
AMD,"In those numbers they forgot to include the fact that when you use FSR3/4, first you render natively at a lower resolution. So instead of rendering at 4K when you enable performance mode, you're actually rendering at 1080p. That can take a game rendering at say 20fps (50ms) down to 30fps (33ms), then you would add the render cost of FSR3/4 on top.   The numbers they explain in that post basically just forgot the first step: the performance gain from rendering at a lower resolution natively. But it holds true for the bit afterwards: adding the compute cost of the upscaling method to the frametime cost of rendering at your new rendering resolution. It doesn't actually change the numbers much in the end - your upscaling cost is entirely dependent on the compute capabilities of the GPU in question so regardless if what your framerate is before upscaling, it's going to be a fixed cost on top at any given resolution (AKA FSR4 cost in terms of frametime will be the same on the same GPU at the same resolution in all cases - even across games you should only see minor differences).",Neutral
AMD,Defending how? Are a lot of people saying that the image quality is better than DLSS 2+?,Neutral
AMD,"That is a thread from 4 years ago when people hadn't even tried the upscaler yet, and only seen it through youtube reviews. No shit that people's perspective changes over 4 years.  And most of the negative comments to that review are subjective views on DF, but that's besides the point.",Negative
AMD,"And I personally like frame gen. Not a super fan of it, but it's a nice option to have.  What I don't like about Frame gen is how it's marketed as ""free performance"", which it is not.",Positive
AMD,So far up their own behind they cant reach the keyboard anymore?,Negative
AMD,"Here is a Monster World review from 2019 (when only DLSS 1 existed) in which ""better than native"" was how DLSS 1 was described: https://www.dsogaming.com/articles/monster-hunter-world-dlss-benchmarks-better-than-native-1871p-worse-than-4k/  ""better than native"" was used at the outset of DLSS.",Neutral
AMD,Lots of people called it. Mostly Nvidia fanatics.   Crazy people exists everywhere and at any point in time.,Negative
AMD,"\>Won't be surprised if in some situations FSR4 leads to perf regression.  No surprises needed - it is bound to be that way in some instances. Upscaling cost is close to fixed and is separated from rest of rendering cost mostly. So IF you have high enough framerates and IF lowering resolution with yield frametime reduction lower than frametime cost of FSR 4 INT8 - you will see performance decrease.  Both are not that huge of an IF tbh. FSR 4 INT8 is often mentioned to have frametime cost of over 4ms. 120fps has frametime of 8.33ms. For you to see FSR 4 INT8 giving performance increase, lowering resolution should shave more than 4ms of frametime - meaning going from 120fps to 240fps. Ain't happening.",Neutral
AMD,"even at 0% uplift, the visuals are improved because you dont have to deal with native TAA, which is awful in 99% of implementations.",Neutral
AMD,"Oh please. We don't need to turn this into console-war-esque bullshitting. FSR has had plenty of problems over it's lifespan. AMD has absolutely released undercooked drivers in the past that they fixed post launch.  Companies are not your friends.  Sincerely, a dude who has been on all AMD rigs since before Zen.",Negative
AMD,Was the small navi designed later? Could indicate when AMD shifted design philosophy to include support for this.,Neutral
AMD,"Yep, i was doing hardware decode of VP9 on my 1070 just fine all the way from 2016. As much as im not a fan of VP coded, youtube made it pretty much a must have.",Positive
AMD,"I guess youtube is supposed to check compatibility and try to deliver a codec that your system can accelerate, but damn not even vp9 leaves you holding the bag there as i doubt they encode new videos with old codecs like vp8 anymore (well, for many years already as your anecdote confirms)  I had a Vega 56 as well. It seemed like a compelling product at the time with HBM onboard. But I always got lackluster performance in games and benchmarks. Wasn't long during that timeframe that I had a 1080Ti. I wish I was more thoughtful about it so I could have held onto it, it could have changed everything by getting me into mining",Negative
AMD,"Dude, Vegas will be 10 years soon, you're not expecting support forever, just upgrade to a 6650XT or 6700XT as it's cheap and you can still use FSR4.",Neutral
AMD,FSR 1-3 works on Vega. Why would you expect FSR4 to work on a 9 year old GPU? DLSS doesn't work on 10 series GPUs either.,Negative
AMD,I am sad to see you learned the hard way how bad AMD support is.,Negative
AMD,"I'd love to agree - I really would. But the one thing that leaves me a bit worried is in the INT8 files there was a line that stated it was generated using an ONNX model created in October 2024. The FP8 files in that same Github folder were generated using a model from May 2025. It has me a little worried that the INT8 model has been shelved.  That being said, from what I've heard form others the INT8 model looks like it contains some of the weights from the FP8 model already. So nobody really knows if it will be part of the upcoming FSR Redstone release.",Negative
AMD,"ahh okay. you're really good at explaining these concepts thanks. if AMD did eventually release this version to RDNA 3 and 4, could it be improved perf wise or is this the ceiling since those gpus don't have the hardware to run it properly?",Positive
AMD,"Not better, but that it's either ""just as good/not that bad/can't tell the difference from DLSS"". I noticed a giant uptick in it when the 5000 series came out and people were trying to justify their AMD purchase. It's people in denial that their chosen platform can't compete.",Negative
AMD,"Whenever people point out that Nvidia is worth the extra 50 dollars for DLSS alone you always have an army of ""but FSR"" people show up.",Neutral
AMD,"And that is DLSS 2, which is HUGELY different from DLSS 1. AND it can, in fact, produce image quality better than native with TAA at times.",Positive
AMD,">FSR 4 INT8 is often mentioned to have frametime cost of over 4ms.  That depends hugely on GPU and target resolution. On my 7800XT at 1440p it's around 1.7-1.8ms from memory, but at 4K it would be around 4ms. A 7600 would do similar but at 1080p and 1440p respectively, so on and so forth.",Neutral
AMD,"I like AMD but it's unclear if it's just pure underdog effect. As a an enthusiast at the high end unfortunately there is no real choice so the hope is that they can become relevant again. some rumors I've been hearing about CPU/GPU/APU/IO die fusion/tiling in the future coming from red team seems quite exciting; and it would be apt for them to get creative since they uniquely have those competencies in house. It looks like things may get a lot more interesting given Intel and Nvidia will also team up on that on-package integration side of things. On the other hand, I suppose it's more of ""integrate or die"" in the modern era where high bandwidth and low latencies are king. Apple is out there wiping the floor with everyone on power efficiency while still delivering impressive performance.",Neutral
AMD,"You can always fallback on CPU decode. Its what my old laptop used to end up doing for youtube and why it managed to actually lag on youtube viewing. Fun fact, since youtube has limited resources, most videos with lower views dont get re-coded for new codecs, so you can still find h.264 videos that a potato can run. but if you like only content with thousands/millions views you better keep your hardware up to date. which is nonissue for most people as they watch on a phone.",Neutral
AMD,Vega 56/64 were released August 2017. I bought it 2019. Then september 2023 was our last real driver update. Barely 6 years of support. Gtx 900/1000 series have had support for 10+ years now. I hope u realize the difference. Never buying amd again in my life.,Negative
AMD,Vega 56/64 were released August 2017. I bought it 2019. Then september 2023 was our last real driver update. Barely 6 years of support. Gtx 900/1000 series have had support for 10+ years now. I hope u realize the difference. Never buying amd again in my life.,Negative
AMD,"People have been asking for vulkan sdr swapchain on the amd forum for YEARS and nothing, it only works on hdr...",Negative
AMD,"Most likely explanation is they shifted development to FP8 because it’s faster and better on RDNA4 and that was the focus to get FSR4 to launch in its best state. The FP8 model runs about the same on RDNA 3 on Linux using FP16, so AMD might yet release that for RDNA3 officially leaving the INT8 model only useful for RDNA2.",Positive
AMD,"Theoretically yes, because RDNA3 could theoretically leverage WMMA for a performance improvement with INT8. I think the official rate is the same as DP4a, but I've seen attempts from people to implement FSR4 INT8 using WMMA and it actually does perform better, but it's buggy and doesn't work properly. And RDNA4 WMMA for INT8 is the same rate as FP8, so theoretically there's a huge speedup that could happen there.  But that depends upon if AMD were to even try that. No guarantees at all on that end.",Neutral
AMD,"> I noticed a giant uptick in it when the 5000 series came out and people were trying to justify their AMD purchase.   The competing product to the RTX 5000 series is RX 9000, which does in fact have a good enough upscaler to the RTX 5000 series.",Positive
AMD,"Until transformer models came for DLSS, FSR 2 and 3 legitimately looked better to me when looking at fast motion and fine particle effects.",Neutral
AMD,"See my edit I found the DLSS 1 use of ""Better than native"". It was definitely used on the boards too.  For as long as DLSS existed people have been claiming better than native. Even when DLSS was garbage.",Neutral
AMD,Do you want AMD to magically go back in time and put int8 support into an 8 year old piece of hardware? You also cant run FSR4 nor DLSS on a 1080ti.,Negative
AMD,Vega was competing with 1080 GPUs. Neither can do AI upscaling.  Vega GPUs at least support RT. People play Indiana Jones game on Linux on Vega GPUs. on a 1080 you'd be shit out of luck and couldn't even do that.,Negative
AMD,yeah the vid does say they might not since it would probably cannibalize their rdna4 sales but i hope they do anyway for those who can use it,Neutral
AMD,"It also came out later, hence he can still be right and you can still be right. You guys should chill lol.",Neutral
AMD,While true. It also has 1% of the game coverage dlss has and cant even be put into games that dont have fsr 3.1 which also had a fraction of what dlss has,Neutral
AMD,"That also depends on the timeframe, when launched the competitor was still AMD's 7000 serie, due to AMD holding back their cards, and good enough is also subjective. From what i've seen its still not on par / better than DLSS in both visual quality, features, and, maybe most important in game support.   Where the latter is not unimportant, as if you want to compete, you need to be on the field. FSR 4 can be super good, but if DLSS is in vastly more games by default (i know you can 'hack' it in often, but no one apart from a niche in the enthousiast niche is ever going to do that) its not really a competition. And that's where i feel AMD (as a AMD shareholder) needs to make strides if they really want to compete. Not only make a competing solution but also make sure its useable 'everywhere'.",Neutral
AMD,Thats fine but there's literally dozens of 4k at vids showing comparisons for years and fsr has looked worse every single time.,Negative
AMD,There was no point in human history where FSR looked better than DLSS comparing versions available at same date.,Neutral
AMD,"I can't speak on that directly, the only times I've used FSR is when I have to. Each has their own strengths and weaknesses, just overall DLSS is more mature and has the nVidia hardware tech to back it up.",Neutral
AMD,"The non-Nvidia article says: > All in all, it’s pretty obvious that 4K DLSS is not as good as 4K native in Monster Hunter World. However, NVIDIA’s 4K DLSS is able to surpass the quality of a native 1871p resolution.  So where was it claimed that it's better than native?",Negative
AMD,"And we start reading the article, and the quote is... ""better than native 1871p"".       Shocker!  We quote literal headline, and... "" better than native 1871p, worse than 4K""  Double shocker!  We try to read article as a whole, and... ""Below you can find some comparisons between native 4K with FXAA (left), native 4K with TAA (middle) and 4K DLSS (right). As you can clearly see, the native 4K FXAA image is sharper, followed by 4K TAA and then 4K DLSS""  ""Below you can find a comparison between 4K native TAA (left) and 4K DLSS (right) in which you can clearly see the visual loss that DLSS brings to the table""  You, sir, are bullshitting in a very bad manner.",Negative
AMD,"1080ti actually has the necessary hardware, its just... not fast enough. the 1080ti was a hybrid experiment before Nvidia went full on 2000 series. BTW the 1000 series are still supported, support ending this year. So he does have a point in AMD dropping support much sooner.",Negative
AMD,"Yeah but at least they havent been forgotten, unlike us polaris and especially vega users. Because nvidia values its customers and wont just dump u the moment ur gpu is a bit older.",Neutral
AMD,"Im not a linux user. I shouldnt have to switch OS, plus vega doesnt have the performance for RT anyway, even if u can  Also, unlike amd, nvidia actually respects its customers and doesnt discard em as soon as their gpu is no longer the latest one.",Negative
AMD,"Every 1000-series card with 6GB or more VRAM got a patch (425.31) so they can support Software Raytracing.    Is it good or playable in 1440p in lets say Metro Exodus? Has it good frametimes? No. But it works.   Full-RT games like Indy however aren't running with this, true.",Neutral
AMD,And I have a 4K 120 Hz OLED display and eyeballs. And I'm not watching online videos with compression artefacts which change how the image looks.,Neutral
AMD,"Moving the goalposts. The question was if ""better than native"" terms was used to describe DLSS1. Yes they put it in the title.  And posters on reddit constantly said ""better than native"". yes even with the garbage DLSS 1.",Neutral
AMD,"So ""better than native"" was used to describe DLSS 1. Thank you!",Positive
AMD,Totally valid concerns but FSR4 support for Vega is not the hill to die on,Negative
AMD,You could make this argument about frame gen not working on anything older than 4000 series. New features sometimes require new hardware. This applies equally to both brands.,Neutral
AMD,"> Also, unlike amd, nvidia actually respects its customers  hahaha. one of the most anti consumer companies respects customers.. good joke  buy Nvidia then.",Positive
AMD,"If that was true, people would be able to play Indiana Jones on their Pascal GPUs. But they can't.",Neutral
AMD,I think your belief in FSR looking better shows that either your display or your eyeballs are malfunctioning.,Negative
AMD,yeah okay man. 1000's of people and 100s of tests across the amd and nvidia fanbase all agreeing that dlss has always been better are all wrong.   you're doing the equivalent of console kids saying the human eye cant see more than 30fps to justify an incorrect position,Negative
AMD,"So the title of your article says  > better than native 1871p  Which is saying that DLSS looked better than basic upscaling 1871p to 2160p.  Are you really not able to read into the context?  But I guess that's just typical since you obviously couldn't even find the ""constant reddit posts"" about it either.",Negative
AMD,"You're the one who's retroactively moving the goal post.   No one with a half a brain through that's what you meant with better than naive. Dog shit FSR 1 is better than native by your standards, if you're comparing 720p scaled to 1080p using FSR1, vs native 720p. There is no upscaler in existing that's worse. AA methods from 2004 are better than native if you want to change what that sentence means.",Negative
AMD,"It was used to describe it compared to INTERNAL resolution. Which is completely opposite of actual upscaling use case and noone in sane mind would use it that way. Every single geniune ""better than native"" argument was talking about OUTPUT resolution.  You are continuing to try and bullshit your way out of that. Tbh you are failing miserably. Continue.",Negative
AMD,"Yes but new game support doesnt require hardware, and amd couldnt be arsed to do that for more than 6 years for us vega users. I have submitted countless crash reports to amd and the devs and nothing. To this day marvel rivals still crashes with my vega 56 at least once per game. Which is not surprising given that the last new game support we had was in late 2023. Every driver since then has been just a maintenance driver, small bug fix here and there. Amd simply dont care that i cant play the game in my 8 year old gpu and it keeps crashing. And yes, i removed my overclock, tried DDU, it's just a driver issue. The game does not go well with the vega drivers and NO ONE cares, certainly not amd. While gtx 900 users have had 10+ years of driver support already. So if u think about buying amd, just remember that NOW u're getting good treatment cuz ur gpu is new. When ur gpu is not so new amd will not give a shit about u anymore, while nvidia will (almost) always be there for u.",Negative
AMD,U bet i will. They had a shaky few drivers in 2025 but nowhere near the level of bad and useless placebo drivers ive had for years with amd.,Negative
AMD,"Come on, at least read what I wrote:     >Full-RT games like Indy however aren't running with this, true.     And it is true, I even added the driver designation..   But I will use Google for you, here: https://www.pcgamer.com/you-can-now-enable-ray-tracing-on-gtx-cards-but-performance-is-low/",Neutral
AMD,"Alternatively, the near instant switching time of OLED hides and smoothes fewer flaws.",Neutral
AMD,"I have a 4090 and on an OLED screen (most people have TN, VA, or IPS displays), fine particle effects and fast motion looks worse on DLSS with non-transformer models than FSR2 and FSR3.  I cannot speak to how it would look on other screens that do not instantly switch the pixels but I would imagine that effects that I see on OLED would get smoothed out and look less bad.",Negative
AMD,"Again. Was ""better than native"" used to describe DLSS1? Yes. You said it wasn't. My point is proven.  ""better than native"" literally started with DLSS (1). No one ever said it before then.",Neutral
AMD,"My point was that ""better than native"" was thrown around since DLSS 1. You accused me of revisionism. I proved I was right.  Not only did I find a random post using the term. I found an actual article using the term. The term didn't just fall out of the sky. It was used in relation to DLSS since the get go.",Neutral
AMD,Sorry I missed that part. I'm outside and the sun glare is messing with me.,Negative
AMD,I could not figure out if all ASRock Motherboards are affected or only certain ones?,Neutral
AMD,"I was always surprised the media didn't spend more time discussing some of the crazy launch era voltage defaults all the vendors were using. Granted with Gigabyte frying chips and ASUS frying chips plus their own motherboards honorably committing seppuku for the chipacide afterwards, there was a lot going on.  I built a system a month after AM5 launched with a 7700X and ASRock B650E Riptide Wifi motherboard. Out of the gate VSOC defaulted to 1.25v but was reported at 1.288v by ZenTimings 1.29. VDD Misc was 1.3v, and CLDO was 1.10. Currently VDD Misc defaults to 1.1v, CLDO defaults to 0.95v even when setting 6000 1:1, and now VSOC shows as red if set above even 1.23v. VSOC also now actually delivers the voltage it's set to without running above it, so either ZenTimings changed how it measures or ASRock changed its LLC setting for the VSOC rail.  Will have to finish the video later but I do wonder if Steve is factoring in the ever-mercurial voltage defaults all the vendors were using. I know ASRock personally changed and tweaked every single voltage knob & paired LLC knob that existed and was constantly changing them for the first year, and still tweaking them by year two to try and lock down the memory headaches users were having. But some of those default voltages were still nuts even when the X3D chips first launched. So if users did update UEFI versions religiously I could easily see any initial damage caused before the UEFI was updated weakening the chip, thereby triggering a belated failure later despite now running on safer voltages.",Neutral
AMD,Every board should follow manufacturers recommendations without exception.  I'm wondering how this could even happen unless users purposely overclock a board?,Neutral
AMD,[This has happened before!](https://www.youtube.com/watch?v=ssL1DA_K0sI).  This is an extreme problem.  Too extreme,Negative
AMD,"I got a b650 asrock board, this sucks",Negative
AMD,"Could this be caused by manufacturing tolerances in the physical layout of the pins and pads? Even a slight misalignment or height difference? For example, even if the board they tested already killed a CPU, maybe their new CPU has slightly tighter tolerances where it makes good contact with the pins on the board. As in, there is a minimum range of tolerances on both the CPU and the ASROCK board...and both need to hit that threshold before an issue arises. And maybe ASROCK board has looser tolerances than the other manufacturers leading to more failures on their boards.",Neutral
AMD,"Donyou guys remember when Asus boards had like 2 documented cases of 7800X3Ds getting fried in x670 mobos and GN, all tech influencera went wild and Asus which they fixed in a couple of days? Now ASRock mobos are frying mobos everyday for months. Internet is almost silent compared to when Asus did it.",Negative
AMD,"Golden rule - never buy motherboard starting with ""AS"".",Neutral
AMD,Man I wish Steve would have tested with an 800 series board -\_-,Negative
AMD,I'm using a asrock x870e motherboard with a 7800x3d with two graphics cards...No burnouts yet😎,Positive
AMD,This was happening on boards other than ASRock though.,Neutral
AMD,There you go Happy now?   - to those who were crying for gn's say in asrock motherboard situation and spouting negativity everytime a GN related something get post here,Negative
AMD,"I have an Asrock B650M-HDV/M.2 murderboard that has been great with my 7800X3D so far. I want to upgrade to a 9800X3D since selling the 7800X3D is still pretty easy but with this thing going on I'm very hesitant to change anything about my current PC  I'm glad I got everything running fine and with good RAM speed/timings at just 1.10 VSOC, but there's no guarantee a 9800X3D could do the same with this board and RAM",Positive
AMD,For all the babies doing the whataboutism under the thread about Nvidia's monopoly,Negative
AMD,so what are people going to complain now about GN coverage of the problem?,Negative
AMD,"it roughly seems like any AM5 asrock board + 9000-series cpu, though we don't have too clear of a picture of the specific mobo batches involved. asrock overall seems to reuse bioses quite extensively, so i assume the boards are quite similar.  some collected statistics available in the megathread first post: https://old.reddit.com/r/ASRock/comments/1mvgndh/9000series_cpu_failuresdeaths_megathread_2/",Neutral
AMD,"Difficult to say. They have a known motherboard that's killed a CPU before and they can't get it to reproduce the issue. So you could have a motherboard with the problem but the issue may not occur.  I hear people saying that it might be happening around the memory controller, so it could be only an issue when paired with certain memory  or something. I'm just speculating, we don't actually know what the issue is exactly.",Negative
AMD,"Only those that have ""ACME"" on their box.     I will escort myself out of the doorway.....",Neutral
AMD,"these specific issues started after the 9000-series launch, and only clearly affect 9000-series cpus. so the old 7000-series bios voltage adventures shouldn't matter here (9000 requires a newer bios).",Negative
AMD,"Hey, i have some v on ram set as like 1.4 , 1.35 and 1.25 , I don't remember exactly what they are but I had to push them this high for my ram to reach stable 6k 30 oc timings.. it's been more than a year on Asus x670 board.. should i change to factory default?",Neutral
AMD,Tech tubers do spec sheet rundowns instead of deep motherboard analysis until someone has an issue and then they milk the controversy for views.,Negative
AMD,> I built a system a month after AM5 launched  Masochist much?,Neutral
AMD,"board vendors need to crank all the settings up so their board looks better in all the reviews and benchmarks. if they lock things down, then the board vendors complain about manufacturers taking control away (ie nvidia forcing partners to design their boards a certain way, locking down power draw through firmware, etc)",Neutral
AMD,"> unless users purposely overclock a board  1. Enable XMP or EXPO because Intel and AMD are encouraging it, and many CPU reviews use XMP/EXPO.  2. Motherboard jacks voltages to insane levels to guarantee stability.  When I enabled XMP for my CPU, the motherboard picked some strange voltage levels while on auto settings. I had to manually tune the RAM and voltages to bring the idle power usage from +15W to about 6W.",Neutral
AMD,I've also got a B650 board with 9800x3d since Dec 2024 and have had 0 issues,Positive
AMD,"Fwiw, my Asrock B650 hasn't blown up my 7800X3D after over 2 years of heavy MMO use and some modern games with PBO and EXPO.",Neutral
AMD,Chances are that you won't have any problem.,Neutral
AMD,There are like two or three manufacturers for the socket mechanism and motherboards from other manufacturers also use the ones that AsRock uses,Neutral
AMD,It's more likely a CPU manufacturing defect than one with the socket tolerances.,Negative
AMD,"The difference in that case was that the apparent issue with the Asus boards was reproducible in testing (see [We Exploded the AMD Ryzen 7 7800X3D & Melted the Motherboard](https://www.youtube.com/watch?v=kiTngvvD5dI) by Gamers Nexus).    In this case, tons of reports but everyone trying to make it happen hasn't succeeded, and they've not yet found the smoking socket while testing.",Neutral
AMD,"Which sucks, as ASRock has been my go-to silver bullet for quality.  What's left, MSi? I refuse to touch Gigabyte.",Negative
AMD,"Which sucks  i have an asrock x370 taichi  that's had over 61,000 hours of Non stop use  it's only ever been powered off for  updates.",Negative
AMD,there's still failure reports on those,Negative
AMD,"Yes, actually.",Neutral
AMD,"Give it a minute, the hate train will roll in come morning. Like clockwork.",Negative
AMD,"Yes? Do you continue crying after you get what you wanted?  It was a valid complaint that he didn't cover it earlier, since it was destroying people's hardware and reports of it came a long time ago and he usually covered similar things way faster.",Negative
AMD,Him making good reporting about this does not invalidate crticism of his bad videos he made previuosly.,Neutral
AMD,Reddit would complain if you handed them a million dollars.,Negative
AMD,Overt sensationalism? ‘Murderboards’ is quite something for the title of an investigation,Negative
AMD,thank you for the link to the megathread. good insights there. my motherboard seems to be one of the high risk ones unfortunately. i think i will wait for the issue to be resolved before upgrading my cpu to a x3d one. thanks for commenting!,Neutral
AMD,There are 1-3 instances where 7000 series was affected too,Neutral
AMD,Finally the doorway is now free!,Positive
AMD,Those issues existed with the 7000 series as well but got fixed 2 weeks after launch. Somehow with 9000 series it wasnt fixed.,Negative
AMD,"People were saying it also affected the 7800X3D when this mess all began... but maybe they were wrong I honestly haven't looked deeply into this to know. That being said I do know some of the voltages were remained high even after the 9000-series launched. Was literally only few a months ago when ASRock changed the UEFI to redline a VSOC setting above 1.23v, before then it was happy to set 1.25v with EXPO. Was only after trouble began this year did ASRock finally start getting conservative with its voltage settings, but the 9800X3D debuted last Nov.  Did finish the video. When Steve mentioned most of the data points were people using 1.4v kits of memory that itself seems like a flag given the warnings I've read about exceeding 1.35v, but it's also the first time I've heard it mentioned as a possible factor. Back from the 7800X3D era I remember a Buildzoid video that mentions one of the three RAM voltages that gets changed by EXPO presets will affect power plane levels within part of the IO die, and there's one voltage plane that shouldn't get too high relative to another lest it cause problems. So having that set at 1.4 seems kind of bad for multiple reasons.",Neutral
AMD,"I won't profess to be an expert on this topic. That said even if I had to run it at CL32 I still wouldn't exceed 1.35v, but that's me. Remember DDR5 official specification calls for 1.1v.",Neutral
AMD,"Haha I knew it was a risk, but 20 years ago I was building hot-off-the-press overclocking rigs, the (mostly GB) boards would usually have early beta and one time even an alpha level BIOSs.   With AM5 it was the best, smoothest system build I've had in 15 years, in large part I credit the SK Hynix memory for that. Microcenter dropped its promo bundle price while at the same time upgraded the 'free' RAM kit to SK Hynix so I couldn't hold out any longer. I have no regrets and wouldn't change a thing if I had to do it over again, even the same ASRock B650E Riptide board. The 32GB 6000 turned out to be a kit of Hynix A die that I could halve the timings on on stock volts, SK Hynix puts out some crazy good stuff sometimes. That being said I did lower a lot of the crazy voltages ASRock defaulted to in the first year once I had a rough idea on what everything was.",Positive
AMD,"Outside of Hardware Unboxed, who even reviews motherboards properly anymore?",Negative
AMD,True! Just had constant crashes with EXPO enabled at 6000 and 1.4v went to manually tighter timings and 1.35 and it flies xD,Negative
AMD,I have no problem with AsRock but this is a real problem with this model AsRock,Negative
AMD,"more than that, but for those they could as well have died on other manufacturer's boards as the amounts are smaller and too close to noise to judge. we also did not have this phenomenon back when 9000-series did not exist, even though the same or similar asrock boards were out there.",Negative
AMD,If you look at the ASRock mega thread there are still people reporting 7800x3d deaths even on the latest bios.,Neutral
AMD,"the way the voltage rules work is that you want VDDIO to be relatively high compared to both vsoc and vddp, though i don't know if the latter is about other than vddp being derived from vddio.  vsoc < vddio + 100mV  vddp < vddio - 100mV",Neutral
AMD,"I think my 7800X3D might have been slightly damaged by ASRock board. I've had a lot of issues in the past, where at first it worked fine and then after a while it would start throwing memory errors and crashing. Then updated the BIOS and it worked fine for another month and started throwing memory errors again. Many BIOS updates later and it now refuses to run 6000 memory at VSOC lower than 1.23V, it just becomes unstable. So I'm running it at 1.23 for few months already and so far it works fine, but seeing other people can run the same memory at 1.12-1.15V and the fact that it was working fine when I first built it makes me think that it was already slightly degraded by the motherboard.",Negative
AMD,"guru3d, techpowerup",Neutral
AMD,"Hard to say really unless it continually needs higher and higher voltages. But back when the 7800X3D launched VSOC was still running higher volts than I what it was set to in the UEFI, as reported by ZenTimings. So if that is accurate it could explain why it worked when you first built it, because today VSOC values are reported below the UEFI setting.",Neutral
AMD,"I updated the BIOS for the first time only after it became unstable, so there was no change on my side between working fine and throwing memory errors and crashing every few minutes. First BIOS update fixed the issue for about a month and then it returned. After that, I couldn't make it run at 6000, so I've been running it at 5800 for more than half a year (tried updating BIOS few more times every now and then), until someone on reddit helped me debug the issue and turns out, bumping VSOC to 1.23 made it run stable again. It's running like this for about 2-3 months, so far without any issues.  By the way, I also do not shutdown my PC very often, usually I put it to sleep. Right now my PC is at 12 day uptime and only because I updated kernel and had to reboot. 30-60 day is probably average uptime between restarts. My PC is connected to UPS.",Negative
AMD,"Again it's hard to say, there's not enough data points yet. If it continues to require higher and higher VSOC settings then that would be the only real confirmation.   Kits without EXPO and kits based on Samsung/Micron die are notoriously problematic in their own right, when I installed a non-EXPO kit I had to resort to manually tuning the resistance & drive voltages just to get it stable at spec ratings. EXPO is more than just primary timings, it stores subtimings, voltages, and impedance/drive strength settings in the profile, and EXPO is a MUST for AM5 (and probably every future AMD platform) just to avoid running into problems.",Neutral
AMD,"My kit has EXPO and it's the 2nd kit. I have replaced it already thinking it was faulty memory, but my previous kit also had EXPO. Ofc replacing the memory didn't change anything, both could run at 5800 max with default SOC voltage (1.2V).   Yes, I don't have more data points, I wasn't even looking at the voltages before (usually I don't touch stuff I don't know anything about). I only know it started spontaneously and so far it doesn't look like it's getting worse. I suspect it was an older BIOS that did something to my CPU, that is already fixed in the current BIOS and hopefully it will not progress further. It would have been weird if my CPU required 1.23V from the start, where literally everyone else just turns on EXPO profile on their memories and ""it just works"", but it doesn't in my case.",Neutral
AMD,"As I've said elsewhere, when I first built my rig the board defaulted to 1.25v. And whether it was accurate or not, Zentimings reported 1.288v as the actual reading. So it's no surprise stuff would 'just work', manufacturers were just brute forcing it for the first two years.  Requiring 1.23v in of itself isn't a problem, Samsung/Micron stuff tend to need higher volts even for just 6000 operation. It's only if the voltage required continues to increase will you have your answer.",Neutral
AMD,">It would have been weird if my CPU required 1.23V from the start, where literally everyone else just turns on EXPO profile on their memories and ""it just works"", but it doesn't in my case.  Silicon lottery and VSOC is quite a random one indeed, yea usually 6000MT/s doesn't require much, but 1.23 isn't that much more than what most ppl would use at 6000.",Neutral
AMD,"Both of my kits are Hynix. As I said, it started spontaneously, without me touching the BIOS. If it was at 1.25 by default then it shouldn't become unstable out of thin air, few months after building the PC.",Neutral
AMD,"So Intel fabs part of their stuff at TSMC instead of their own foundry, and AMD fabs part of their stuff at Intel.   What a weird time to be alive.",Negative
AMD,I knew I should’ve held on to my shares of intc…,Neutral
AMD,"Maybe to replace Global Foundries I/O chiplets which don't need to be very compact, but not the good stuff.",Neutral
AMD,imagine this headline 15 years ago,Neutral
AMD,I wonder what AMD would want fabbed at Intel - maybe cache and i/o dies?  Those don't need the most advanced tech and would be low risk for AMD in regards to Intel snooping on AMD's IP.,Neutral
AMD,"Everything is chiplets now, make sense to have multiple sources.",Neutral
AMD,Charlie [says](https://www.semiaccurate.com/2025/10/03/is-amd-fabbing-at-intel-foundry/) all his contacts at Intel and AMD are denying this.,Neutral
AMD,I suppose you could make IO dies and cache chiplets on the current node...,Neutral
AMD,New CEO is proving himself so far beyond Pat it's crazy,Positive
AMD,"As much as we think of Intel being really far behind TSMC, they really aren't that bad.  In fact the PowerVia technology is even a generation ahead of TSMC's BPD (Backside Power Delivery).  Its set to hit in Intel 20A late this year when TSMC isn't going to respond until 2026/2027.  AMD might be able to get extra fab capacity and even some high-end technologies that could have advantages.  Its too early to tell but its not impossible to think that Intel 20A might actually be BETTER than TSMC 2nm.  Exciting stuff :)",Positive
AMD,NANA is pulling through.,Positive
AMD,The goverment tries really hard to make Intel great again.,Positive
AMD,Are Intel fabs good though ? Is their issues with the designs only? Can AMD trust Intel with their designs ?  By trust I mean in their ability to produce quality up to AMD’s standards and also not steal intelectual property.,Neutral
AMD,First thought when reading the headline: please be wifi/bluetooth chips or IP.,Neutral
AMD,I mean Intel is a bad spot. There might be a good deal in it for AMD to manufacture some low end parts and squeeze Intel for a good prize.   Intel is probably happy getting a few $ from wherever even when it selling its services at cost as long as the lights stay turned on. Beggars can't be choosers.,Neutral
AMD,It won't happen ever. AMD would go Samsung as alternative,Negative
AMD,Rumors that HP Might make a deal for the next generation PA-RISC CPUs will be made at the Intel Fabs as well. bullish on this news.,Positive
AMD,This time line is so fucking odd,Negative
AMD,Intel Fabs at TSMC because their own fabs at the bleeding edge are not cost effective. Costs plummet. AMD fabs at the most cost effective location.,Negative
AMD,They have shipped no high volume 3rd party foundry product yet.  So who knows whether any of this will actually happen.,Negative
AMD,It would kinda be ideal that everybody uses every relevant fab and is able to shift relatively quickly.,Neutral
AMD,This is a blatant rumour it’ll never happen,Negative
AMD,Nana gonna come and scare you tonight.,Neutral
AMD,"I sold like 3 days before the Nvidia deal announcement. Still made 20% in 6 months, but could have almost doubled.",Neutral
AMD,i bought before alchemist was suppose to launch in summer-fall and then it didnt launch and i sold at a tiny loss. Intel has been utter shit since.,Negative
AMD,"Same. Mine was only $80, but it could've been $140 now lmao.",Neutral
AMD,"I felt the same but I’d be barely breaking even. Fuck it, the Sofi stock I bought with that cash is doing massively better",Negative
AMD,INTC and MU were the two stocks I bought and forgot about.,Neutral
AMD,I actually sold with quite a win after the nvidia news. I bought at ~$20. I just wish it was more than a few hundred dollars worth.,Positive
AMD,"TSMC had been making AMD's IO die for some while now on their 6nm process. If Intel can offer their 4nm node at competitive pricing, then maybe?   But the problem with manufacturing at Intel is always the conflict of interest. I'm pretty sure Intel CPU design teams want to take a look at AMD's CCD design for example. That will be a big problem for everyone as they have to be sure Intel just won't copy their stuff once they get ahold of the design at the fab.   Or Alternatively, mobo chipset or wifi chips. Those don't contain AMD sensitive IP and customers don't really care too much about it. Problem is those aren't really AMD products. Promontory 21 is ASMedia chip, and the RZ616 and other amd wifi modules are mediatek chips. AMD gotta convince them to move to Intel, and ASMedia and Mediatek have to be sure that Intel won't just copy their homework.",Neutral
AMD,Impossible to fab cache dies if they want access to TSMC packaging. TSMC doesn't package other foundries' wafers.,Negative
AMD,Xylinx and Pensando products come to mind first.  AMD has a few products that would fit the bill here.,Neutral
AMD,"They wanted nothing. Intel gifted the US government 10% ownership. Now apple, nvidia and AMD supposedly want something. I think they want to avoid 10% US ownership.",Negative
AMD,"low end Radeon chips, there uncompetitive anyway.",Negative
AMD,Not with takeout costs as high as they are.,Negative
AMD,"Wouldn't wonder the slightest, since this headline is issued again, just to push the stock and the upper floor making bank on their stock-compensation packages as per usual. They're desperately pumping the numbers with rumors.  Pure virtual signalling at its finest again, only to push their stock – It's so obvious it's lame.",Negative
AMD,"nvidia, amd and apple all at the same time ? this isnt intel. This is us government threatening to kick out these companies in secret unless they fund intels recovery.",Negative
AMD,Only if it actually happens. Rumors of interest are cheap.,Neutral
AMD,Guess being CEO of Cadence means everybody likes him lmao,Positive
AMD,Lol 20A is dead. BSPD is scheduled for 18A-P for... 2027/28?,Negative
AMD,"> In fact the PowerVia technology is even a generation ahead of TSMC's BPD (Backside Power Delivery).   Having a PowerPoint bullet feature does not make a node better. 10nm had tons of unique and interesting tech, and it was a dumpster fire. What matters is PPAC, and Intel is very firmly a full gen behind TSMC there, at best.    > Its too early to tell but its not impossible to think that Intel 20A might actually be BETTER than TSMC 2nm.   I assume you mean 18A, but even then the answer is clearly ""no"". The question is whether it's even competitive with N3, much less N2. Intel themselves are buying N2 wafers.",Neutral
AMD,They are good but not the best.  Look at Xeon using last gen architecture. They run on Intel fabs,Negative
AMD,"One would assume that for AMD to agree to anything, they'd have already done their own due diligence and confirmed Intel was up-to their standards, right?",Neutral
AMD,Probably some low margin parts,Neutral
AMD,… source? This doesn’t sound real.,Negative
AMD,As long as it pays me I'm good lol,Positive
AMD,AMD are getting shafted on prices at TSMC. They are rinsing all US companies and consumers with their profit margins (>70%),Negative
AMD,"> Intel Fabs at TSMC because their own fabs at the bleeding edge are not cost effective   No, it's primarily about PPA competitiveness. Secondary is easy of use and available IP. For example, Intel's using N3 which they have no answer for until 18A.",Neutral
AMD,Is N2 supposed to be cost effective? Because that's what AMD is using for Zen 6 desktop and server.,Neutral
AMD,"Nah, 18A much more cost effective, that's why their lower end higher volume compute dies (PTL, WCL, and NVL-U with up to 4P cores per die) are reportedly fabbed there. The big server compute dies are also fabbed at 18A. And the rest of the chiplets are said to be fabbed in Intel 3.   I think TSMC's N2 might be better for density. Hence the 52 core NVL-S compute die fabbed there. Maybe 18A just matures slower. Well whatever's going on behind the scenes, that's just what's currently known.",Neutral
AMD,Even if you make 0 chips from a different backup foundy you need to pay minimum 10 to 30 million dollars just to have the ability to do so. This insane minimum cost is what keeps companies from switching fabs.,Negative
AMD,OOF,Neutral
AMD,Ouch,Negative
AMD,"There is no ""peeking"" on other teams/customers designs. Foundries are fully compartmentalized, and designs are fully encrypted.  Even within AMD or Intel, for example, only specific people and teams can have access to specific designs within the organization. Everybody else can't access them, even if they work for the same company.   IP protection is taken very seriously, even within the same organization.",Neutral
AMD,"There's probably no risk for AMD to go the Intel route regarding reverse engineering and IP theft, but even if there was, for ASMedia and Mediatek Intel has really no incentive: they already make better products in-house",Neutral
AMD,No need to move to intel for the asmedia chipset or mediatek wifi chips. They’re separate chips entirely so they just order them and slap them on mobos as needed.,Neutral
AMD,"Intel, on the other hand, does package other foundries dies.  Get around tariffs just by packaging on Intel and saying the final product comes from the US?",Neutral
AMD,"Does Intel package TSMCs wafers? If the silicon CPU dies come from Taiwan, and Intel puts it all on a package here, and Intel makes the IO die, they could label the CPU ""Made in America"" and maybe get around some huge tariffs.",Neutral
AMD,"FWIW Raytheon and ASE do most packaging for AMD.  AMD only uses TSMC for certain CoWoS packaging. And CoWoS itself can use 3rd party dies, DDR/HBM stacks,  for example.",Neutral
AMD,> TSMC doesn't package other foundries' wafers.  They absolutely do. CoWoS packages other companies HBM. And some of the structural silicon dies such as interposers they package also comes from companies such as UMC,Neutral
AMD,"Maybe NVIDIA signing up for Foveros + Clearwater Forest panning out well is a sign to others that Intel’s advanced packaging is catching up? They have always roughly been #2 at that, so could be ok.",Positive
AMD,It’s ways around tariffs. That’s really all this is about.,Negative
AMD,Yup. This is looking like coordinated central planning. Nothing different than CCP mandating to buy or not buy certain company products,Negative
AMD,"""in secret"" my ass. The current administration loves to gloat, this has nothing to do with that. It's purely because TSMC is jacking up prices sky high with insane profit margins that Intel is becoming a reasonable customer.",Negative
AMD,Nvidia intel deal was in planning for a year.,Neutral
AMD,"Base 18A also has PowerVia, for whatever it's worth.",Neutral
AMD,An ISA dead for 20 years doesn't sound like real news? Wonder why.,Negative
AMD,And the AMD news does?,Neutral
AMD,"A company selling an in demand product at market value doesn’t mean AMD or any other US company is getting “shafted”. AMD is free to chose a worse performing node from Intel, Samsung, etc if it’s not worth the cost.   TSMC is only able to charge as much as they do due to insane demand for wafers due to the AI craze.",Negative
AMD,We have all that tariff winning going on in the US.,Neutral
AMD,"No, it’s the most expensive node to ever exist. It’s the literal opposite of cost effective.",Negative
AMD,It is if they can sell the result for more than the extra cost vs N4 parts when they ship.,Neutral
AMD,Is AMD using Intel for Zen 6? OF course nothing has been said. They could easily be using Intel for an IO die.,Neutral
AMD,I suspect ~~fear~~ that they will just charge more from us because that's how it works now.,Negative
AMD,"> I think TSMC's N2 might be better for density   It's better in everything, full stop. They're using it for PnP, not density.",Positive
AMD,"Hate it break it to you, but you can 10× these numbers, *easily*.  Since down to the point of fabbing anything at Samsung or TSMC below N7 and especially sub-N5, you're in for easily +250 million USD on overall costs for masks and every other expenses, for when being eventually *ready-to-manufacture* to finally green-light a ""Go!"" on a design to be manufactured.  Those 10–30 millions are a dream and doesn't even gets you close to the actual mask-costs.",Positive
AMD,"Yeah there are issues with having your close competitors chips but it's more of the line of having a competitive design and prioritising your own chips over your competitors, and even that would be over a time span of years.",Neutral
AMD,"But there are ""schedule/volume insights"" which will inevitably come up.   For example, if the internal teams wants to accelerate their fab schedule -- and the foundry says there is no capacity -- the internal team will have insight on external demand and schedule.  Internal teams would have some understanding of the competing products that are made in the fab.    (even with TSMC this happens)",Neutral
AMD,"Who to say that there won't be any, uh, ""leaks"" or ""accidents""? Or perhaps they just quietly make it visible for the design teams? Trust isn't simply built with just a ""trust me bro, the other teams won't see it"", it's something that must be built with time and/or track record. Track record and perhaps time is something Intel fabs don't have a lot of.   Also, espionage in chip industry usually doesn't mean Intel will suddenly make 2.5D CPU AMD style, it's much more subtle and harder to prove. It could range from just looking at the design to evaluate your competitors' products, to subtly taking ""inspiration"" from it.   Given how hard it is to prove foul play and how easy it is to doubt your competitor... yeah, you can quickly see the problem here.",Neutral
AMD,">IP protection is taken very seriously,  Yeah right. By the company that bribed OEMs to not buy competition.",Neutral
AMD,Also low end laptop chips are all contenders for Intel to make sure they don't get fked by Tariffs,Neutral
AMD,Another more cost effective and less insane way for AMD to get around tariffs is simply to leave the US.,Neutral
AMD,"Intel does indeed package TSMC wafers alongside Intel wafers, they’ve been doing so since Meteor Lake",Neutral
AMD,Iirc Intel packages most of their chips at their Malaysia site,Neutral
AMD,Mostly because they don't fab memory,Neutral
AMD,I am not so sure about CWF. I think hybrind bonding is still miles behind TSMC. The deal is likely for EMIB-T as a replacemenr for CoWoS.,Neutral
AMD,Wouldn’t be surprised if all these European ARM and RISC-V chips are sanctioned with tariffs unless they’re made by Intel in the U.S.,Neutral
AMD,"It's national security, that's what it really is about. Intel is critical to the US national security hence why Nvidia and co are being pressured into playing nice or else.",Neutral
AMD,These tariffs won't last long enough for that to be the case.,Negative
AMD,"There's nothing unusual about countries trying to protect vital industries. Admittedly, the US doesn't do it often, but comparing this to Communism is ridiculous.",Negative
AMD,And CCP is winning. That makes their strategy valid and worthy to emulate.,Positive
AMD,"there's no such thing as ""CCP"". it's CPC",Negative
AMD,Which sub-process of 18A is PTL being fabbed on? Has BSPD been confirmed present on those parts?,Neutral
AMD,"It wasn't plain dead the whole time though, last update was AFAIK around 2019 or so …  Yet Intel's super-flop Itanium is still the industry's most prominent dead-on-arrival or at least *single-longest comatose platform on life-support* to date for sure.",Neutral
AMD,"Monopolies shaft companies and people, which is why they're occasionally broken up.",Negative
AMD,"“At market value”. There’s only one market. It’s play TSMC’s game or get fucked. It’s a monopoly. Intel are trying to break that. AMD shareholders will benefit with better margins once the monopoly of TSMC is broken. It’s a win - win for these two American companies. Don’t AMD shareholders always moan about not having enough capacity? Well that’s TSMC’s decision. They decide whom they allocate capacity to. If AMD can get more capacity, cheaper than TSMC & on a node that’s potentially just as good (18A) or better (14A), then why not? The only barrier I see is the IP issue, where I can see why they would be hesitant.",Neutral
AMD,You should look up what a monopoly is….,Negative
AMD,Isn't every node the most expensive node to ever exist though.,Neutral
AMD,"I doubt it. Zen 6 is slated to release in likely less than a year, I highly doubt they would be able to keep using IFS quiet this far into development. And Intel has long said that they don't have any major external volume orders for 18A yet anyway.   Even this rumor specifies ""early talks"".   Maybe we see this for Zen 7 or something, but Zen 6 is too early.",Negative
AMD,"They can't easily use Intel for io die cause TSMC refuses to package nodes made by other foundaries, and AMD is increasingly reliant on TSMC advanced packaging for their latest products.",Negative
AMD,Yeap I expect Zen 6 to be more expensive just like Zen 3 was at launch.,Negative
AMD,"""Whoops, sorry we need to delay your chips for another quarter...""  *Meanwhile launches your own product ahead of the competitor's product*",Negative
AMD,That's an orthogonal issue.,Neutral
AMD,"That is not how the industry works.  Some of you have a very ""soup opera"" view of how these things are done. I don't think many of you have worked in this industry, or being exposed to how IP is aggressively protected and handled.  There is tremendous level of compartmentalization in modern fabs. And at the silicon team level, you can extract very little information over the overall design ""secret sauces."" Esp since that entire part of the design flow is encrypted, for example.  That is why you have TSMC being able to handle the latest designs from APPL, QCOM, NVDA, etc while guaranteeing there is no ""cross pollination."" Even though they all have silicon teams on premises @ TSMC.",Neutral
AMD,That has absolutely nothing to do with IP rights?,Negative
AMD,"The US is by far AMD's largest single market, where they get more than a third of their revenue.  Losing that much money and leaving a giant market wide open for your competitors vs paying a bit more to have some stuff made in the US is not cost effective or sane.",Neutral
AMD,"Hbm3 and hbm4 is fabbed by tsmc, just sold under sk hynix brand name. Stuff like ddr5 ram and ssd, sk hynix makes itself not hbm.",Neutral
AMD,"I mean even Intels own most advanced chips are made in Europe, lol. Those getting sanctioned too?",Neutral
AMD,The us govt buying 10% of the company and then strong -arming others into helping save this beached whale is very odd. Don’t remember anything like this happening in the US before,Negative
AMD,"Yeah, industrial policy in the american semi industry is *long overdue*, there should have been action taken when Global Foundries was dropping out of leading edge.",Negative
AMD,"Yeah, it's clearly Fascism.",Negative
AMD,"> Which sub-process of 18A is PTL being fabbed on?   The one just called ""18A"". p1278.3, internally.    > Has BSPD been confirmed present on those parts?   It's part of the whole 20A/18A family.",Neutral
AMD,Wut? Last chip implementing it came out in 2005.,Neutral
AMD,"Ding ding ding.  It's not good faith to say TSMC is charging ""market rates"" when they're the only market player.",Negative
AMD,TSMC is a monopoly? interesting take,Neutral
AMD,"> Don’t AMD shareholders always moan about not having enough capacity? Well that’s TSMC’s decision.  This is more on AMD for either accurately forecasting that more AMD chips won't sell, not wanting to pay the pre-payments in advance for more guaranteed capacity, or them just being conservative in terms of wafer orders due to them not having their own foundries.",Negative
AMD,"> Well that’s TSMC’s decision. They decide whom they allocate capacity to.  You are leaving out the essential part: depending on who can pay them most. A business does what is in their best interest and that of their investors. If you set up a lemonade stand, are you going to sell lemonade to the one person who pays you $5 or to the other person who pays you $3? It is irrational to think it is a charity.",Neutral
AMD,"“A monopoly is a market structure where a single firm dominates the entire industry, enjoying substantial market power and facing little to no competition. This dominance is often due to high barriers to entry, which prevent new firms from entering the market and competing. Monopolies are characterized by their ability to act as price setters, controlling both the price and quantity of their goods or services to maximize their profits”.",Neutral
AMD,"It is still related to ""what advantages could Intel have if they are a foundry for their product competitors?""    The risk isn't just would Intel learn about the design -- but also schedule & volumes.  (which I think is a bigger risk than design details)",Neutral
AMD,">That is not how the industry works.  I guess the core difference in our view is that you see ""how things should've run"" and I see ""what could've happen"" instead. Well, I'll just put a quote from [forbes](https://www.forbes.com/sites/patrickmoorhead/2024/09/12/splitting-intels-fabs-from-its-design-business-now-doesnt-make-sense/?hl=en-US) here instead to do the talking.   ""The only thing Intel stands to gain from this split is the chance that an independent foundry business could bring in new customers that compete with Intel products— **and that don’t trust Intel’s design side not to snoop on their own chip designs** .""   Forbes is basically saying trust is still an issue back in 2024, and stating that spinning Intel foundry as an independent entity could help with that. Mind you this is written by a professional analyst far more knowledgeable than I am. So yeah, take it as you will. You can call it soap opera, but I see it as a real possibility i guess.",Neutral
AMD,Tells about ethics in corporate world,Neutral
AMD,"> Hbm3 and hbm4 is fabbed by tsmc,  Wrong. HBM is fabbed by memory manufacturers. HBM4 has a logic base die and in case of Hynix that die is fabbed at TSMC, but the HBM DRAM stacks are fabbed at Hynix.",Neutral
AMD,U think so?,Neutral
AMD,"You do know CCP stands for ""Chinese Communist Party"" right?",Neutral
AMD,That’s certainly news to Jack Ma,Neutral
AMD,"Except they're not.  One company making an objectively better product doesn't invalidate all other companies in the market and make that first company a monopoly.  Lea and Perrins makes objectively the best Worcestershire sauce on the market, to the point where it's effectively the default in everyone's kitchen who uses it. But every grocery store has a store brand, and many other companies make their own.  Yet, despite those alternatives being cheaper, L&P still has a lion's share of sales and insane mindshare in that particular vertical. But is L&P a monopoly? No. Just making a better product than your competitors and consumers responding accordingly does not make you a monopoly.  TSMC has not been strategically hampering Samsung/Intel/GloFo in order to enrich themselves. Those three have repeatedly shot themselves in the foot with bad business decisions, poor R&D returns for money, a lack of clear focus on market trends, and in general putting out lower quality product at lower cost. TSMC is the Lea and Perrins of chipmaking right now. Are they the only option? No, you're free to shop around to any one of 5 or 6 other competitors. Is it their fault their version is the best one on the market? No.  TSMC is what happens when industry leaders who very nearly DID have a monopoly (read: Intel back in the 2000s) rest on their laurels and refuse to address rising competition. Very few people took TSMC seriously back when they first started in '87 and now look where they are. That's called competition.",Neutral
AMD,Yup. You think it isn't?,Neutral
AMD,My assumption is that the difference in power efficiency vs cost per wafer on consumer products makes sense to go with Intel.  And then on equipment for servers etc still goes to TSMC.,Neutral
AMD,"I’m not saying it’s supposed to be a charity. I’m saying they are a monopoly. It’s like a lemonade stand selling the lemonade for $20 because they feel like it because no one else has the technical ability to set up a lemonade stand. If there were a few other players who could also make lemonade, there would be competition and the lemonade would be $4 - $5 at all of the stands.",Neutral
AMD,critical bit  > controlling both the price **and quantity**,Neutral
AMD,"Most of that information would be all in public financial disclosure documents, anyway.   The risk for AMD would be more in terms of providing a revenue source to a direct competitor. Rather than any upper hand intel could possibly get from info derived from their role as a supplier to AMD.   So it would be a situation more akin to APPL depending on teams/orgs from Samsung as part of their supply chain.   APPL is not as concerned by Samsung stealing any ""secrets"" from them, as much as they try to divest as much as they can from Samsung, since they are providing a literal revenue stream to a competitor.      Corporations use each other services all the time, as long as the value proposition is there. E.g. APPL uses Microsoft for a lot of cloud infrastructure, even though they are direct competitors in other areas.   If INTL provides a tremendous value proposition to AMD as a fab, they will use them. They won't view the risk in INTL gaining any upper hand from ""snooping"" into their designs and schedules. But rather they would be mindful in the calculus that they are literally going to be funding their direct competitor in a lot of markets.",Neutral
AMD,"No the core difference is that I have been part of design teams that have fabbed high performance SoCs, so I am informing you how things are within the industry.   These projects take hundreds of millions of dollars in terms of investment, so these are well stablished processes at play.   The issue with Intel as a fab is not that they design teams may peek at 3rd party clients designs, but rather the uncertainty of them being able to execute on time the fabrication of those designs.   By the time a design makes it to the fab/bring up, it has been on a development pipeline for 2 years at least. So there is little to be gained by your competitor anyway. If you have to peek at architectures that have been in flight for 2+ years, you're already hopelessly behind and have bigger issues to worry about.",Neutral
AMD,Typical forbes bad take.,Negative
AMD,the base die is on 3nm tsmc. The memory dies are 10nm from sk hynix. But as far as anything is concerned. Everyone and their dog has 10nm capability. So that doesnt matter. If someone wants to make hbm4 they need that 3nm tsmc to actually make these memories.,Neutral
AMD,It's a fact their most advanced production fab is in Ireland.,Neutral
AMD,Democratic People's Republic of Korea is my favorite democracy 😍,Positive
AMD,"Look, I’m not weighing in one way or another on China’s economic system, but what a thing is called has no intrinsic bearing on what a thing is.",Negative
AMD,Things are always exactly what they are named. Thats the rule and nobody ever breaks it.,Neutral
AMD,And the Democratic Republic of the Congo is the bastion of democracy in the world.,Positive
AMD,"At the leading edge, TSMC is a monopoly. However, they won’t be for long. Their monopoly status at the leading edge will be broken within a few years. And, yes they have tried to hamper competitors. They increased the price of Intel’s wafers vs. the competition because they didn’t like the fact they were trying to compete with them in the foundry space. They were effectively trying to drain their resources and starve their foundry effort of cash as “punishment”. They also point blank refuse to package any chips that are made at other foundries. If you want to use COWOS or similar, you have to use TSMC for the logic as well.",Negative
AMD,"> Are they the only option? No, you're free to shop around to any one of 5 or 6 other competitors. Is it their fault their version is the best one on the market? No.  They are legitimately the only option for leading edge nodes right now.  It is a monopoly. Not a nefarious one. It was hard work for them. But there's no other leading edge game in town, much like there's no leading EUV tech in town outside of ASML.  It is incredibly* expensive and difficult to develop what TSMC has developed and there's a certain momentum aspect to their success at this point. They aren't nefarious in their success, but they certainly command a monopoly over the leading edge market - because they ARE the leading edge market and they can set the prices as they choose.  Comparing TSMC to Worcestershire sauce hurts my mind and it's hard to take you seriously if that's the example you want to use.",Neutral
AMD,tell me why it is,Neutral
AMD,"There's plenty of other people making lemonade, people just think *this one* is the best and are willing to pay more for it. That doesn't make it a monopoly.  You can get fab work by global foundries, Intel, or Samsung. Nobody is stopping you.",Positive
AMD,Yes they do control the quantity. They decide how much allocation to give customers.,Neutral
AMD,">So there is little to be gained by your competitor anyway. If you have to peek at architectures   Just to be clear... so you still could peek afterall?   I don't understand, if it's impossible (or ""that's not how it works"") to peek in the first place, why try to downplay it? The possibility of Intel peeking is the problem, not what Intel is peeking.   And yes, I'm sure peeking will be difficult as you've described (with all the compartmentalization and encryption and whatnot), but as long as it's doable, partners need to trust that Intel design won't snoop on their work. It's not the matter of technicality, it's the matter of human factor, trust.",Neutral
AMD,"Hynix node is optimized for memory. They probably never even made PDKs for logic dies. Which is why they are using TSMC for their base die. But Samsung HBM4 will use an in house base die. Since Samsung also fabs logic, not just memory.",Neutral
AMD,Thats not really the point. Dude got upset I used a word that he literally used himself (as part of an acronym) in the post I was replying to.,Negative
AMD,"God, you people really can't need to read the posts before making dumb comments.",Negative
AMD,except the don’t have a monopoly on bleeding edge nodes they just have the most cost effective ones even at high prices. they’re yield and performance metrics mean the performance per dollar is better than the competition which can produce bleeding edge at much lower yield and effectiveness,Positive
AMD,"Because it's the only company selling the highest performing chips. There is a reason why Nvidia/AMD even Intel get stuff made by them. And because of that, they fundamentally control the market and pricing at the highest end. Do you think they'd charge the same if Samsung could make equally good chips?",Positive
AMD,It is a monopoly.  But that then goes to the question of whether or not it is abusing its monopoly position.  To me the answer is actually no.  It really isn't.  When you are supply constrained then demand curve and prices have to adjust.  They are not in the business of being a charity.,Negative
AMD,"> You can get fab work by global foundries, Intel, or Samsung. Nobody is stopping you.  This is the real issue here. The industry has become so one-track-minded that they aren't seeing that other options. It is like moving to the area of town where there is just one lemonade stand is or living near a multitude of stands and simply settling on one lemonade because it is the sweetest and best tasting while paying quadruple. /u/Due_Calligrapher_800 acts as if there are not any other lemonade stands when there are and the industry in their typical one-track-mindedness is acting that way as well.",Neutral
AMD,"they don't, they are hitting the limits of their capacity, it's not a decision, they would do more if they could",Negative
AMD,"They're saying ""peeking"" is difficult and would hardly be worth the trouble because the designs are already 2+ years old.",Negative
AMD,"I really can't simplify the points any further;      a) there is little benefit from peeking at your competitors design at that point. You might as well wait a few months and buy the finished product, you will get the same level of information basically.       b) it would be catastrophic for the foundry if different organizations can peek at each other's designs. The foundry would lose all business, and they would get sued into oblivion.   So there is little incentive for any of the parties involved to do any design appropriation at the foundry level.",Negative
AMD,Samsung hbm4 is also using 3nm tsmc. Samsungs fully own hbm was disqualified by nvidia. They were ordered to use tsmc 3nm base dies or dont bother applying as a hbm supplier for nvidia as their fully own hbm has terrible performance.,Negative
AMD,">Intel also does chip to spacecrafts, top tier airplanes, submarines and other defensive systems.  Those are made on older nodes. They need to be reliable and secure, not powerful and advanced.  >A company like intel isnt just for your own personal computer  And yet it's a fact consumer products, especially if they don't need huge dies, get the most advanced nodes pretty fast",Neutral
AMD,"They used it because that is the name of the Chinese government’s ruling party. Just because their ruling party has the word “Communist” in it does not mean that it’s an accurate description of how their government functions.  It’s like if I complained how slow my local “fast food” place was after having it take an hour to get my order, and your rebuttal was “dude, you just called it *fast* food, you’re literally saying it’s not slow”, as if the name of the product type has anything to do with the actual real-world speed it’s being produced at.",Negative
AMD,"You are the one conflating things.   The only thing that was said is that the US is doing market interventionism to protect their strategic interests. Then it was noted that it was similar to the Chinese mixed economy, specifically in regards to Intel's case.  Nobody is suggesting that the US government is practicing communism. The US government has many laws for them to directly intervene in the economy when they deem it necessary.   Where did anyone say it was ""communism""? The only thing I read is someone hinting at the fact they're still calling themselves Communists as disingenuous, given how far their economic system has strayed from Maoism and Stalinistic command economies.",Neutral
AMD,"So you view Samsung, Glofo, and Intel as viable competitors for leading edge nodes?",Neutral
AMD,"That’s clearly not how a monopoly works. If there are other players in the sector who offer similar, cheaper, but less performing offerings, they are not a monopoly as there is an appealing and a competing choice. Claiming otherwise is wanting a top company to sell for less out of pure envy for what they have that you don’t. Business isn’t a charity.",Negative
AMD,"that's irrelevant. The question is, does TSMC have any idle capacity that it can't sell due to the high price? Could they sell more if they reduced the prices?  edit  https://en.wikipedia.org/wiki/Monopoly#Monopoly_versus_competitive_markets  > The most significant distinction between a PC company and a monopoly is that the monopoly has a downward-sloping demand curve rather than the ""perceived"" perfectly elastic curve of the PC company.[30]  > [...]  > The fact that a monopoly has a downward-sloping demand curve means that the relationship between total revenue and output for a monopoly is much different from that of competitive companies.[31] Total revenue equals price times quantity. A competitive company has a perfectly elastic demand curve meaning that total revenue is proportional to output.[31] Thus the total revenue curve for a competitive company is a ray with a slope equal to the market price.[31] A competitive company can sell all the output it desires at the market price. For a monopoly to increase sales it must reduce price. Thus the total revenue curve for a monopoly is a parabola that begins at the origin and reaches a maximum value then continuously decreases until total revenue is again zero.[32]",Neutral
AMD,2 years of design doesn't necessarily mean they didn't miss developing a similar component to the competitor in those 2 years.,Neutral
AMD,"To be fair, 2+ years design can hardly be called old and not worth these effort these days. Point in case? AMD's IO die. That die stays the same from 2022 to today, used in Intel beating class of CPUs, or any NVIDIA 4000 GPUs (also 2022) that still beats Intel GPUs per area of die size.   I'd say if I were Intel, this alone worth several meetings with some executive to take a ""look"" at.   But idk, I'm not a guy who worked on high performance SoC so what do I know lol. Thank you for hearing my ~~Ted Talk~~ Soap Opera.",Neutral
AMD,"a) I know once a product came out, there's bound to be dies shots that maybe can show you approximately what area does what, latency tests like what chips and cheese does, maybe some SEM cross section photos, and benchmarks. That does tell you the performance of the chip, yes, but isn't it basically looking at a black box? I kinda reckon looking at the blueprint directly gives you much more insight to what's what. Also from business standpoint, knowing your competitor performance before launch does help you adjust MSRP for your own product for example, so it's not all *that* pointless.   But I do love to know if I'm wrong on this. Would be pretty cool to know if we have about equivalent information as those engineers in the fab about recent chips as we speak.  b) no, it's just about the foundry owner, Intel. ""Cross polination"" as you put it probably isn't the concern here. The concern is as forbes put it, Intel design snooping at other's work via data obtained by Intel fab. And yes, **if** they get caught, I'm sure it'll be messy. But when's a little bit of shenanigans and moral bankruptcy ever stopped Intel before?",Neutral
AMD,"That's just an unfounded rumor. Every time they ask Jensen about this he dispels it. Samsung supplies a lot of HBM to Nvidia and AMD. Also mi355x uses only Samsung. And it works fine, with highest memory capacity GPUs available.",Neutral
AMD,"To be fair, with all this drone shit the current military leadership likes, they may benefit from radhardening newer nodes.",Negative
AMD,That's false. He used it in relation to central planning.. which is a core component of Communism.,Negative
AMD,">The only thing that was said is that the US is doing market interventionism  False, he said ""coordinated central planning"" which is core Communism, not just marketing intervention.",Negative
AMD,"It's kind of annoying that the accepted method is to do benchmarking with smaller models (3B, 8B, etc) and lesser contexts.    It allows things like slow prompt processing (ex the Achilles heel of Macs) to go unremarked since they're not noticeable at smaller sizes.   Especially on something like a Strix Halo where you're probably grabbing the 128 GB chip because you want to run a 70B Q8 with plenty of context.",Negative
AMD,"Nice to see it works straight of the box but rather underwhelming. Saw this post '[ROCm 7.0 RC1 More than doubles performance of LLama.cpp](https://www.reddit.com/r/LocalLLaMA/comments/1ngtcbo/rocm_70_rc1_more_than_doubles_performance_of/)' over at r/LocalLLaMA and thought perhaps PP had an edge while Vulkan had TG, though that was on RDNA4, 9070 XT (on a small model). Doesn't seem the case here.   What I find with benchmarking LLMs especially across hardware is the amount of different env and flags needed to be set to find that 'perfect' setup. I usually look over at   [https://github.com/lhl/strix-halo-testing/tree/main/llm-bench](https://github.com/lhl/strix-halo-testing/tree/main/llm-bench)  To find such cases but it's hasn't been updated for ROCm 7. Not only that comparing across HW is usually tough and you really go by it through other users. TG isn't that difficult to guestimate as it's bandwidth bound but finding benchmarks like with gaming outlets is tough. It's cool to see Phoronix continuing with LLM benchmarks and I'd like to see more HW being tested",Negative
AMD,From the article it seems like Vulkan is still much better than ROCM.,Positive
AMD,"ROCm never fails to disappoint, but it's sadly the only option if you want to do anything more than just running inference on AMD GPUs...  Part of it is just the abysmally bad support for consumer SKUs, but this one in specific is literally marketed as a ML chip...",Negative
AMD,"A bit surprising that for interference, there is such a huge difference between GPU and CPU, I would have expected then both to be memory bandwidth bound, even on the higher bandwidth compared to a normal dual channel system.",Neutral
AMD,"Do the Turbo Nerds care about ROCm 7.0 ? Shamelessly asking so I may take confident, aggressive posts integrated into my belief system.",Negative
AMD,"Yeah, I find PP to be generally lower and the fact that most people that share their benchmarks are doing so at lower context. That said, like I wrote on my own comment, it's difficult to generally find someone or an outlet benchmark and compare across HW. Then you'll get people who champion M4 Max or Ultra for their bandwidth while TG or compute is bottlenecked with longer context or the large model that their fitting in *unified memory*. While I've generally seen good PP on Halo, the lack of cross testing doesn't leave me confident on such conclusion.",Neutral
AMD,"Reviewers have literally no clue about anything AI at the moment, I seen one install NAS software on one of these lol.",Negative
AMD,amd rocm release note doesnt include ryzen 395 as supported hardware,Negative
AMD,">In real-world gaming, the card only managed to gain roughly 8-12% uplift over the stock SKU  well thats a bit less than 25%  >The entire cause of this FPS increase is the enhanced power limit, which increases the non-XT 220 W board power to as much as 300 W, representing a roughly 36% power increase on its own.  very inefficient gains",Negative
AMD,[Obligatory link to original reddit post](https://www.reddit.com/r/radeon/comments/1nlpqx3/flashed_the_reaper_9070_into_a_fake_xt_boosted_my/?share_id=Cf60552_29-W9lDf8MHMb),Neutral
AMD,Source coming from a reddit post (that has a link to the BIOS):  [https://www.reddit.com/r/radeon/comments/1nlpqx3/flashed\_the\_reaper\_9070\_into\_a\_fake\_xt\_boosted\_my/](https://www.reddit.com/r/radeon/comments/1nlpqx3/flashed_the_reaper_9070_into_a_fake_xt_boosted_my/)  Keep in mind in terms of power scaling the 9070 is more or less perfect at \~220W while the 9070 XT at \~300W. I think this BIOS is useful for getting the base 9070 near \~300W at max as Adrenalin at most gives you +10% PL so if you have a 9070 with a stock BIOS TBP at \~220W you can only go at most \~240W.,Neutral
AMD,"Looked up prices for the powercolor reaper 9070 and 9070xt. On newegg its like a $30 difference. Just get the 9070xt. Bios flashing gpus is a nightmare and not worth it unless you know exactly what you're doing. I tried flashing a r9 290 back in the day and bricked it, even with a dual bios because im not smart.",Negative
AMD,"It's been 20 years but this is the most ATI story I can imagine, I seem to remember there just being an X700 card you could flash up to X800",Neutral
AMD,Some things never change,Neutral
AMD,didn't people do this before and it ended up in some cases nuking there cards prematurely.,Negative
AMD,"Has anyone tried this with a 3-plug model, like the Sapphire Nitro+ 9070?  I recall comments speculating that a 3-plug model flashed to its equivalent XT would have a power limit of 350-375w, based on the stock XT power limit. I'm not sure if that is/is not including the +15% power limit available via Adrenalin.",Neutral
AMD,Not like the old days where we could unlock cores and pipelines. :(  Just power limit increases.,Negative
AMD,Dumb question but has anyone done the reverse?  Edit: nvm i guess its easier to undervolt LMAO,Neutral
AMD,Im interested in getting the 9070 for a fair price only because it is among the most efficient cards. Its cool increased power limit enables slight overclocking but there is nothing new or interesting in this.,Positive
AMD,and 8-12% is exactly the average difference you see between the non-XT version and the XT,Neutral
AMD,Yeah lol why d you turn a 220 watt gpu into a 300 watt one for 8 percent performance. Thats the kind of stupidity intel do with their 13000 a 14000 series cpus.  Just no,Negative
AMD,"You can run the 9070 up to 270w if you buy an OC variant.  So, that's a lot of work to gain 30w more.  You can easily get 15% performance gain without any bios modding.  The mod is cool for enthusiasts, but no one should care to do this.  You gain very little.  Edit:  You risk melting the 8 pin connectors if you push past 300w, since none of these cards come with 3 connections.",Neutral
AMD,"""up to"" includes 0",Neutral
AMD,It's like 10 usd a year. For gaming I rather have 5 extra fos,Neutral
AMD,They're always very inefficient gains. In today's news...,Negative
AMD,Assuming a 12% uniform uplift. That puts it like 8% behind a 5080 which goes up to 380watts. So they are the same.     You can power limit a 5080 to like 200watts.... Meh i give up explaining since i dont care. Essentially in about a third of modern titles it would match a stock 9070.    I have to wonder what if they had overclocked gddr7. Might match a stock 5080,Neutral
AMD,"Yeah it isn't really worth doing unless the price differential is massive. I bought mine the first day that they came out, but my local Microcenter ran out of the MSRP XTs, so that's why I opted for the MSRP 9070. Pricing hasn't come down on those since the initial launch, so I just ended up keeping mine.",Negative
AMD,"Don't know status of amd cards nowadays but it's basically impossible to brick an nvidia card by flashing it. You can just put the original back on, though of course you need to use integrated graphics at that point.",Neutral
AMD,For Europe the difference with tax is 150 USD. Honestly with the bios flash 9070 non XT has crazy value,Neutral
AMD,"I've flashed multiple AMD GPUs since the 2000s and never had an issue, including BIOS files I modified. It's really not hard if you can follow directions and use a command-line interface.  That being said for $30 yeah, buy the XT. That's a no brainer.",Positive
AMD,"Nah all it does is unlocking power limit. You could had do this with the 470/480 to a 580, vega 56 to 64, 5700 to xt.   This does not unlock more CUs, all it does is raise the artificially imposed power limit that AMD likes to put on their lower end models using the same die.   And gains are usually pretty decent, because AMD actually heavily power limits lower end models as a form of upselling their higher end cards using the same die, otherwise they are literally the same cards with a -5% CU and in some cases; cards with same CU but lower power limit.  If they added other form of segmentation like how nvidia did with the 5070ti and 5080 in the form of large cache cuts AMD wont need to power limit their non xt models heavily every generation.",Neutral
AMD,"I could do that on the rx580 and my actual 6700xt: increase the power limit and get some more fps.  Also the RAM can get some +10% almost for free (power related, but not on all boards).",Neutral
AMD,It’s par the course for AMD too. The old Phenom X2 cpus could often just unlock 3rd or 4th cores.,Neutral
AMD,"Same thing with NVidia Geforce TI 4200 & 4400. If you were lucky, you could get it flashed to a TI 4600.",Neutral
AMD,I wonder what AMD and the GPU market would look like today if it had kept its R&D eggs in Radeon's basket instead of Ryzen's.,Neutral
AMD,It's always AMD too 🤣 ATI/AMD same same,Neutral
AMD,"Right, I remember bios flashing my 9800 pro 256MB to and 9800 Pro XT and saving like 40% off the cost of an actual XT  Those days were great",Positive
AMD,I did this on my 9800pro like 20 years ago. It's nice to see somethings stay consistent.,Positive
AMD,"9000 series BIOS shennanigans :P   They reacted by cutting the trace that made this possible, which was a 5 second fix with a graphite pen.",Neutral
AMD,You could flash a radeon 6950 to a 6970. It would unlock the shaders.,Neutral
AMD,Have you tried it at all? I just bought a Nitro+ off amazon warehouse for 470€ and I'm thinking of trying it. The level of price/performance I'd achieve would be pretty incredible lol (I get free electricity).,Positive
AMD,"I... technically do? I care about power efficiency a fair bit, so I undervolted my 9070xt and also dropped the power by 25%. Performance reduction of about 3% relative to stock in Steel Nomad (barely worth remarking on) and it runs way cooler, way quieter and sips like 230W instead of 340.",Positive
AMD,"I would guess that everybody should do that, undervolt and light overclock, full overclock on RAM.",Neutral
AMD,Unlike the 12v High Power connector the PCIe 6+2 has a more adequate safety factor. The similar EPS 12v can safely handle 300w with its four 12v pins. The 6+2 with three identical 12v pins could handle 225w without issue.  The bigger concern would the cards VRM. Manufacturers will often remove power stages for lower power cards to reduce costs. Since the VBIOS contains the information about the cards power delivery altering it can make the card push itself beyond what it can handle.,Neutral
AMD,"> Edit: You risk melting the 8 pin connectors if you push past 300w, since none of these cards come with 3 connections.  You are still running the 8 pin with more safety margin on a flashed 9070 than the power connector on a 5090 at stock. At least as long as you don't have some bargain basement PSU.",Neutral
AMD,The PCIe slot can provide up to 75W (in practice it can be even more than that).,Positive
AMD,"Wrong, the 3080 TUF OC came with 2 8 pin connectors and was pulling 340W. 150w per connector + 75w available from the PCIe slot. So 375W max. available with dual pin.",Negative
AMD,"A standard 8-pin pcie connector is rated for 150w, so 300w with 2 connections is okie dokie.       I had my 2080 TI even run off single cable on the psu side for years - 280w.  There's plenty safety margin for 8-pin unlike 12vhfr.",Neutral
AMD,The Nitro model has 3 connectors.,Neutral
AMD,"The 295X2 draws way more power than that over 2 8 pin connectors. It's fine.  The official limit is 375W including the 75W from the PCIe slot, unofficially there's a tonne of headroom in the connectors to go even higher.",Positive
AMD,"a 12% uplift brings it basically to a stock 9070XT, which itself is 20% behind the 5080 (TPU GPU Database)",Neutral
AMD,Lmao Reddit math,Neutral
AMD,It could never match a 5080. Where do you get this information from?,Negative
AMD,Source? The RTX 5080 and even the RTX 5070 Ti absolutely destroy that card. They're not even in the same arena.,Negative
AMD,"What? 12% uplift won't even allow it to reach 5070Ti performance, let alone be within 8% from 5080. It will be like healthy 20+% slower still.",Negative
AMD,> you need to use integrated graphics at that point.  I genuinely think a lot of people forget that basically every CPU made in the past decade or more on Intel and any Ryzen on AM5 has an iGPU.  So flashing a card is honestly not a big deal at all.  It's just that this specific bios flash gives a laughable amount of extra performance for the power increase.  This is as far from a Phenom X2 to X4 situation as it gets. Nothing is unlocked. The only thing changing is the power limit.,Neutral
AMD,"This time round its probably worth it more than ever, i had the XT bios on my RX 5700 Pulse but the cooling just was not sufficient it would throttle hard with the unlocked power limit. The coolers on the majority of RX 9700 cards are overbuilt and can handle the increased power draw with ease.",Positive
AMD,Vega56 can unlock the gpu mem volt to 1.35v when using the 64 bios.,Neutral
AMD,Same with some X4 cpu's you could unlock 2 extra cores to get 6,Neutral
AMD,Also the Tri-cores were all quad,Neutral
AMD,"RT and PT probably would’ve been standardized years ago if AMD was trying  But we’d have weak CPUs. Also AMD can do both, make good cpus & gfx if they wanted. They just don’t want to spend money on gfx so here we are",Negative
AMD,"I haven't, only because I already have the Nitro 7900 XTX, so in the end it wouldn't be an improvement.  If I was going to upgrade, I'd totally try it, though. I think there is a modified ativbflash now to make flashing possible, so in theory flashing to an XT should be easy.  If you do decide to tinker, let me know how it goes, I'd like to see a nitro modded like this, should be the best model possible.",Neutral
AMD,"Oh wow pretty aggressive power limit, how are the temps like in comparison?",Negative
AMD,How did you drop the power limit?  I got a tad annoyed by the built in Radeon tuner only allowing 10% lower PL. But everything else was fine so I never bothered with MSI Afterburner as I usually do when using Nvidia cards.,Negative
AMD,"Unless you get unlucky at the silicon lottery. My 9600 XT isn’t stable if I undervolt it any real amount, and I’ve been undervolting AMD and NVIDIA cards for like 7 years.   Don’t assume you can always undervolt. Sometimes you’ll get silicon that just barely meets the specs.",Negative
AMD,"It depends on how the power circuitry on the card is setup, many draw pretty much nothing through the slot.  A clever self-balancing ""draw exactly up to 75w from the slot and the rest from the cables"" power supply would be pretty complex and expensive for the majority of SKUs.",Neutral
AMD,"If I remember correctly, it gets 75W from the PCI-e slot so in theory it's safe. Anyway for me it's absolutely not worth it in any case",Negative
AMD,"I agree, my point is that you can't add much more than 30w with this mod.  So it's kind of pointless.  You can run past the 150w per cable mark, but you risk transient spikes melting the connector.",Negative
AMD,The nitro has a single 12v 2x6 connector actually,Neutral
AMD,https://youtu.be/xSbPHUrOg44 @testing games.   In about a third of those the 9070xt is 10% behind. So if you power limit the 5080 to 200 watts or 50%. It should drop to it or 9070 levels.,Neutral
AMD,The techpowerup benchmarks put its exactly 10% behind a 5070ti. A 12% uplift would put in 2% faster than it.   I am talking about a power limited nvidia card. As the op focused on the 9070 being limited to 220 watts. The downvoters just can't read.,Neutral
AMD,">       >  >  >  > I genuinely think a lot of people forget that basically every CPU made in the past decade or more on Intel and any Ryzen on AM5 has an iGPU.  Except both companies still make cpus that do not come with iGPUs. Basically both use F as an indicator on the CPU's model number. AMD just launched the 9700F, Intel has an Ultra 265KF, which I've seen in a few bundles. And yeah I helped a friend build with a 265KF recently, who didn't realize he got the KF model, and needed igpu  Also technically, Xeon and threadripper stuff.",Neutral
AMD,Quite recently most Zen CPUs did not come with an iGPU.,Neutral
AMD,"It's free performance with not much effort at all. If you're not concerned about power draw, what's not to like.",Neutral
AMD,"Yeah it basically just switch all voltage and power logic to use the higher end's version.   Some of these cards are literally voltage and power limited thats all. Most insane in recent memory in terms of limitation is 7900gre, as it's memory voltage is for some reason tied to core voltage. Unfortunately theres no bios to flash it to.",Negative
AMD,"On one hand, I really want to tinker, on the other hand, the card lacking dual BIOS scares me quite a bit.I don't wanna end with a brick.   Even if it gets bricked I should be able to flash the original BIOS again using my iGPU, but quick searches of radeon BIOS flashing bring up so many horror stories of people somehow being unable to it makes me unease lol. There's also very little information about 9070 to xt flashing (although the couple of things that show up showcase great results).  Anyways if I decide to go for it I will update you and maybe create a post/youtube video about it. The 3.2ghz overclock on my 5060 after a BIOS flash from 145w to 160w left me wanting more LOL.",Negative
AMD,"Theyre like... 50s and 60s, maybe? I don't really keep a close eye on them. They're low, point is. I barely hear the fans unless I turn on path tracing or something.",Negative
AMD,"I just did it in Adrenaline. It allows up to a -30% power limit. Not sure why its only allowing you 10.   Although its possible youre cranking the power by accident, because 10% is the amount it lets you *raise* the power limit by, I'm pretty sure.",Neutral
AMD,My 5070 Ti reports drawing single digit from the slow as an example.,Neutral
AMD,There's always safety margin and the rating is for constant draw.,Neutral
AMD,The 9070 XT can pull above 300W stock and significantly above 300W with overclocking despite using the same amount of 8 pin power connectors. The PCIe slot provides up to 75W of power in addition so up to 375W continuous power draw is within spec.  I'd be much more worried about cards using the 12VHPWR connector as these actually have melted even in stock conditions without exceeding the 600W rating.  Nothing's gonna melt on a 9070 except the silicon itself. Or maybe the fans achieve liftoff. One of these will come before the connector has a chance to melt.,Neutral
AMD,"I went from 270w on the stock bios to pulling 348w with the XT bios, so nearly 80w more power and now the clocks are consistently over 3300 while gaming. The only downside has been the slightly increased hotspot temps, but they're still in the low 80's. I have a Sapphire Pure 9070 which uses the same cooler as the XT variant. There are several XT's that only have 2x 8-pin connectors and there is no problems with power delivery.",Neutral
AMD,"Free perf is not pointless though, \~$150 saved for near same performance. even more of a non-issue if done on dual bios cards. lastly, spikes won't melt it, sustained unbalanced load does.",Positive
AMD,"You're right, but it includes a 3x8 adapter",Neutral
AMD,[https://www.techpowerup.com/gpu-specs/radeon-rx-9070.c4250](https://www.techpowerup.com/gpu-specs/radeon-rx-9070.c4250)  Techpowerup puts 5070Ti 17% above 9070 and 5080 34% above 9070.  Even if you try to reverse start point for your favor - 9070 is will be like 14.5% slower than 5070Ti.  No idea what you are on about.,Negative
AMD,"Absolutely, that's why I said ""basically"" as those are the majority of CPUs in people's rigs.  Point is that a lot more users can do something which used to be a rather risky undertaking 10 years ago and do it very safely nowadays.  And if you can't, no big deal. Not like the 10-15% performance boost with a much, much higher power usage is something the majority of gamers would want anyway.  Win-win for everyone.",Positive
AMD,> basically ... any Ryzen on **AM5** has an iGPU.  I think that was pretty clear eh.,Neutral
AMD,"Interesting, maybe it's a model thing for the 7900 XTX Pulse.  [There's definitely a big ass minus](https://i.imgur.com/tloZlHA.png), but whatever not a big deal, I mostly just cut clocks at a certain point where performance is fine and that's about it.  Edit: Yep seems to be a driver thing. Even MoreClockTools reports the maximum lower bound of the power limit at 90%. How odd.",Neutral
AMD,"Yeah, 75w+150w+150w seems to be more than enough.  So it's a little better than I was thinking.  Pretty cool.  This might make the 9070 the best card of this generation.",Positive
AMD,Wtf. Either way a 12% overclock as per the post we are all responding to. That would make it 4% worse. Not 14.5%,Negative
AMD,"> as those are the majority of CPUs in people's rigs.  I think with Intel and gaming rigs, getting F chips aren't uncommon. Like looking at microcenter, it's a 20$ different between the 265K and 265KF bundles. It's pretty easy to say that's 20$ i can spend on something else.  Even more so with gaming prebuilts, with OEMs always cheaping out. Looking real quick at [best buy's Intel gaming prebuilt PCs](https://www.bestbuy.com/site/searchpage.jsp?browsedCategory=pcmcat287600050002&id=pcat17071&qp=processorbrand_facet%3DProcessor+Brand%7EIntel&st=categoryid%24pcmcat287600050002), basically 80% of the top sorted by best selling all skip the iGPUs.  Even my go-to OEM, PowerSpec by Microcenter, will do a [265KF in their 2000$ gaming PC](https://www.microcenter.com/product/689962/powerspec-g455-gaming-pc).  Definitely less common with AMD chips, but that's more because AMD don't have as many F SKUs.",Neutral
AMD,"One of my computers has a 7500f. AM5, no iGPU",Neutral
AMD,">basically every CPU made in the past decade or more  No, that wasnt very clear.",Neutral
AMD,"It is, as for now, 9070 is 14,5% worse than 5070Ti. 5070Ti, in itself, is \~14% worse than 5080. Overclocking 9070 by 12% will not in any form or function put it within 8% of 5080.  It's not that complicated.",Negative
AMD,"Nicely done quoting out of context.  It literally said ""past decade or more **on Intel**"".  Do you just want to be a contrarian and ""akshually some CPUs don't have iGPUs!111""-kinda guy?  Whatever floats your boat, I'm not judging. It just doesn't really do anything for the discussion.  Point still stands: Flashing a GPU bios is a lot easier and less risky nowadays than it was 10 years ago.",Neutral
AMD,Yeah and the card isn't exactly bricked if it can be reflashed - even if the user doesn't have an igpu. Just take it to someone else lol,Neutral
AMD,"It says ""future GPU architectures will support DGF"" but I'm fairly certain RDNA4 does as well, correct?",Neutral
AMD,"**TL;DR:**   AMD has announced their DGF format is compatible with animated geometry, where it incurs an insignificant cost of less than 1% of overall frame time.  Dense Geometry Format is used to lower BVH side RT ms overhead.  AMD confirmed HW based DGF decompression for RDNA 5 while current AMD GPUs rely on shaders.",Neutral
AMD,"From someone smarter then me:   NVIDIA DMM and AMD DGF are fundamentally different approaches to the same problem. DMM works by taking the original surface and then radically reducing its detail, so in the end it consists of fewer triangles, while storing the surface information in the form of a displacement map. This way, the BVH acceleration structure can remain simple, since only a fraction of the surface's real detail needs to be taken into account for the actual calculations. DGF, on the other hand, is a scalable, lossy compression method for meshlets, and unlike DMM, it actually represents the geometry instead of only reproducing it through a displacement map. The result is similar, and the BVH acceleration structure can remain relatively simple. Since both methods are lossy compression techniques, there will be some quality degradation, but the benefits gained from compression are significantly greater than the loss in quality.  The principle is therefore similar, but the advantages and disadvantages lie in different places. DMM compresses much more effectively, so in theory the gain is greater, but because of the required surface preprocessing it imposes a significant overhead on the CPU, and it is also not compatible with the actual geometry used in today's games, since the content has to be tailored specifically for DMM. DGF compresses less effectively, so in theory the gain is smaller, but it is compatible with all kinds of geometry, and it does not impose any significant overhead on the CPU either.  Because DMM proved so impractical that no developer was willing to adopt it, NVIDIA decided to discontinue the technology in the RTX 50 series, meaning it is unlikely to ever see use in practice.  Since DMM is practically unusable, NVIDIA introduced Mega Geometry as its replacement, which primarily works by clustering triangles rather than manipulating the surface itself. This addresses DMM's compatibility issues and imposes relatively low additional overhead on the CPU, but it does not perform actual geometric compression, meaning its memory requirements are extremely high compared to both DMM and DGF.  A simple comparison of the situation:  DMM: very limited surface compatibility, high CPU overhead, extremely low memory usage  Mega Geometry: full surface compatibility, moderate CPU overhead, very high memory usage  DGF: full surface compatibility, low CPU overhead, low memory usage",Neutral
AMD,"Yes all cards technically support it but they mean support for HW based decompression. From earlier post from February: *""Dense Geometry Format (DGF) is a block-based geometry compression technology developed by AMD, which will be directly supported by future GPU architectures""*  If I were to guess RDNA 5 based cards and the next gen consoles will have a decompression engine inside each ray accelerator similar to how NVIDIA added a DMM accelerator with 40 series.   This isn't just some baseless speculation there's actually a patent for this in case someone is interested: [https://patents.google.com/patent/US20250131640A1/en](https://patents.google.com/patent/US20250131640A1/en)  This quote is interesting as well:   *""Quantization took less than 1% of the overall frame time, which means this process will not majorly affect rendering times in an animation pipeline. Likewise, animating the position data (Animation) has an almost insignificant contribution to the frametime. BVH Build and Ray Trace dominate the image computation.""*  **TL;DR:** Animating geometry has an insignificant impact on the ray tracing ms cost. ~~IIRC rn animated geometry is usually not implemented in RT games due to BVH overhead concerns.~~ *It's about rebuilds and inefficient BVH management rn not animated geometry overhead. PTLAS to the rescue!*",Neutral
AMD,"One more point: while DMM and DGF target similar problem areas and therefore cannot really be used alongside each other, DGF and Mega Geometry are not direct replacements and these can actually complement each other, as they approach the situation differently.   A Mega Geometry-like solution can works on DGF surfaces, so it is likely that both will eventually be utilized together, as they work very well side by side. It is very easy to implement a Mega Geometry-like solution in hardware, while DGF can be emulated via compute shaders, which is important, because a GDF-like decompression engine can be extremely complex in hardware, so it will take a while for Intel and NVIDIA to implement it.",Neutral
AMD,"What an amazing summary and from the Videocardz Disgus section of all places. The last point about HW decompressor doesn't seem to hold AMD back, already confirmed.    Thanks for sharign and can you please space out the first comment similar to JoeMan's comment?     So I'm thinking DXR 1.3: Prefiltering and DGF nodes (new method of BVH tracing using INT predominantly), LSS and DGF decompression in HW but that's just speculation. Probably more features as well but we'll see.",Positive
AMD,"As I understand it DGF is a technique for compressing geometry to reduce memory usage and at least in the first paper, reduces performance when tracing. The memory reduction is like a factor of 6x but tracing can be slowed by like 2x. This site is showing that you can slot animation into DGF cheaply (i.e. change the vertex positions and rebuild the blocks). In reality the cost of animating geometry with RT had little to do with the cost of transforming the vertices, GPUs are very good at that.  Touching any part of geometry means you need to rebuild the BVH or you'll be missing movement in the ray traced representation. DGF doesn't address this (its implementation isn't strictly connected to BVHs, although the meshlet blocks can be used as leaves in the structure). So it is expected that BVHs and ray tracing would remain the expensive part since the same stuff happens with or without DGF. Like you stated, the cost of this process is why it's not usually implemented in RT games - the less geometry you change, the more you can delay rebuilding or do partial updates instead. This article is just showing that DGF holds for dense animating geometry too",Neutral
AMD,Done :),Positive
AMD,"Thanks for providing additional context from earlier blogpost and papers. Ms overhead is an issue for sure which is why AMD is opting for HW accel in RDNA 5.  One thing for certain is that AMD NEEDS their own RTX Mega Geometry competitor. Especially PTLAS otherwise like you said if they animate just one asset then nonstop BVH rebuilds.   Intel already unveiled Micro-mesh CBLAS in a paper over 2 years ago, and during Summer they unveiled PTLAS support. Meanwhile RTX Mega Geometry implemented in UE5, proprietary engines etc.... and as usual where's AMD. Maybe when DXR 1.3 arrives AMD will bother to do a proper implementation.",Neutral
AMD,"Absolutely. DGF with HW acceleration could be great if it could make decompression free, then they could reap memory benefits (if it was adopted, it requires baking to use). RTX Mega Geometry existing kills off any excitement for DGF for me, DGF seems like AMDs answer to DMMs which were lower quality but 3x better at compressing and faster to decompress. Meanwhile DMM acceleration has been killed off from 50 series in favor of Mega Geometry which handles every case DGF wants to: granular BVH, clusters, partial rebuilds, memory reduction. Which also works on earlier series...  Nanite seems to have proven to everyone clusters are the next step in LOD management. Intel Micro mesh, NVIDIA CLAS. I was unaware of PTLAS (thank you for inspiring a deep dive!) but you are right, Intel and NVIDIA again. Shocking AMD do not have any response to either feature (yet??). I guess Project Redstone is probably their focus right now? They absolutely need a response to Mega Geometry!  Edit: I suppose if they can get HW accel building to be fast enough, DGF leaf node BVH could achieve some of the same benefits since its effectively a cluster BVH (which AMD tested by using primitives, maybe their next target to implement in hardware?). I'm not entirely convinced where DGF is going without more insight into the hardware/software limitations",Positive
AMD,"As usual NVIDIA keeps moving the goalpost and AMD responding to prev gen one (DMM) gen too late (RTX MG).   Like you said Mesh shading and continuous LOD isn't going anywhere. So it seems. Catching up to CUDA, DLSS and porting FSR4 to PS5 Pro prob takes all their SW side ressources beyond graphics R&D :( You're welcome.    Well look at their pathetic responses to DXR 1.2 and the recent Advanced Shader delivery on the DirectX blog. AMD really needs to up their SW and HW game and I doubt we'll hear a single word on CBLAS + PTLAS SDK from AMD until RDNA 5 gets launched, but hope I'm wrong.   The Vulkan Github documentation for MG is a treasure trove for anyone interested. Look to the left section for documents ending with .md, truly great stuff! [https://github.com/nvpro-samples/vk\_lod\_clusters/blob/main/docs/blas\_sharing.md](https://github.com/nvpro-samples/vk_lod_clusters/blob/main/docs/blas_sharing.md)  And it's not like they don't have the talent to push things hard, Holger Gruens and Carsten Benthin former Intel, Matthäus Chajdas and many others. There's just seemingly a lack of will at AMD to really push things except for their GPU workgraphs push which does deserve huge applause.  We'll see, but that would be the next logical step similar to what NVIDIA does in 50 series (new ray/tri engine). Yeah more info needed to be disclosed by AMD but reading the Github documentation for MG this isn't close to being enough. AMD really needs to plan based on DGF not existing, because there's no guarantees devs will even bother to use it.    Still Dense geo format does have interesting use cases beyond BVH management, but that's speculative patent based derived analysis (Look for the KeplerL2 patents shared in the NeoGAF forums a while back: [https://www.neogaf.com/threads/mlid-ps6-early-specs-leak-amd-rdna-5-lower-price-than-ps5-pro.1686842/page-12#post-270687172](https://www.neogaf.com/threads/mlid-ps6-early-specs-leak-amd-rdna-5-lower-price-than-ps5-pro.1686842/page-12#post-270687172)   Not confirmed in any way by AMD. But it looks ideal for a parallel wide INT-based prefiltering testing setup to cull triangles before expensive floating point tests but what do I know. Either way interesting stuff.",Negative
AMD,"Very interesting, AMD are taking advantage of DGF for rapid and wide culling to speed up intersection testing. This could indeed be their way of hardware accelerating cluster intersections, although I'm intrigued what the practical uplift this gives nor how they address building new clusters. I have no idea what NVIDIA did to achieve the same on prior gens.  I also had no idea NV MG BLAS info was posted. It's conceptually simple but it's a very smart intuition that since RT with a good accelerator is less tri constrained, you can just reuse high poly BLAS and forego swapping LODs. I'm guessing Ray Reconstruction is very useful here to cut back on any extreme aliasing. Very curious now to see how they managed to optimize animated geometry, maybe heavy partitioning with lazy BLAS refit or just brute force rebuilds. Regardless NVIDIA is obviously far ahead with a more united stack of solutions.  Despite AMDs talent I find it more impressive that Intel manage to keep up with graphics developments much quickly. XeSS, ExtraSS, cluster and partition acceleration structures, etc. Their media encoders have also remained competitive. AMDs strategy is a bit confusing to me especially with how they're dragging out RDNA3.5 in new products. I hope UDNA impresses.  Thank you for the reading material, you are very well informed 😁",Positive
AMD,"Number one   Yeah so it seems at based on the AMD patents, but it's not just DGF patents, they also have a fallback method called prefiltering nodes, which is probably very similar to how the RTX Mega geometry clusters work on 50 series, but I could be wrong and like you said NVIDIA doesn't exactly spill the beans on architectural intricacies. While DGF is superior (compression and memory accesses characteristics) this fallback is also made for rapid and wide culling like you said.  Apparently the idea is to precompute a set of quantized BVH data matching the full precision data. It can even be leveraged for ray/box intersections but it seems like triangles will benefit the most.   From what I can read INT operations are multiple times more ressource efficient than FP. That is all PPA characteristics, power, performance at area. From what I can read online it's anywhere from 3-5X, might be wrong, but the patents directly mention ""multiple times more"" so it's at least 3x. In effect AMD can probably shrink the current FP pipeline down, given it'll only be used for inconclusive final tests, and at little cost to die area implement a very wide parallel intersection tester unit that eats ray/tri intersection tests for breakfast.      Another benefit of DGF is that you can include pretty much all the relevant data within one node, so you do just one memory access for the entire block and you can begin doing RT. For example opacity micro maps data has a header within the DGF block. Still no info on subdivisions + tesselations but that's no doubt coming as well given MG supports it, or it'll be included in an accompanying template similar to MG. They also talk about rays coalesced against the same node in the patents, where you mass test rays at once before removing the DGF data, but IDK if that's how things are done today already.  Github FTW! Yeah me to as usual NVIDIA holding their cards close :/ I'm pretty sure the animations rely heavily on subdivisions and tesselation based on this: [https://github.com/nvpro-samples/vk\_animated\_clusters](https://github.com/nvpro-samples/vk_animated_clusters) This simplifies the underlying geometry and should massively speed up rebuilds and avoid them entirely.  For sure, NVIDIA as always ahead of the competition and look at the joke of MS's DXR 1.2. Embracing NVIDIA's functionality over 2 years later and it's still not shipping till Q1 2026, while SER and OMM has been supported since Q4 2022 xD on NVIDIA side.  Intel has long played a leading role in graphics and ray tracing for a long time, before NVIDIA even introduced RTX + has invested a lot in research and is behind a lot of open source SW used in rendering applications. In addition, like NVIDIA, Intel went all AI and HW accell, for example they planned to have SER one year before NVIDIA, but Alchemist got delayed.   Meanwhile AMD used the bean counter approach of wait and see and relying on shaders, they still rely on that for BVH processing. Meanwhile NVIDIA and Intel took the full RT core approach right from the start. Look at where that got them. 5 years of ignoring ML super res only to go all in last minute with FSR 4 + no DLL swap until very recently (FSR 3.1) despite NVIDIA having that for over 5 years. I mean who TF runs that SW department, this is incredibly stupid. I agree that AMD's approach makes no sense.",Neutral
AMD,"Number 2  RDNA 4 is really just a stopgap nothing more, similar to RDNA 1. They also had Vega iGPU for many gens until RDNA 2 came along, RDNA 3.5 looks to be another repeat of that. RDNA 5/UDNA is poised to be another RDNA 2 full stack moment except this time probably a lot better and less complacent on the SW side.  Me too but based on all the changes suggested in patents (we'll see how many actually ends up in products) + rumours of a clean slate overhaul not seen since GCN in 2011 the picture is slowly taking form and best case assuming NVIDIA keeps rebranding Ampere cores (they really haven't done foundational changes since then) gen the nextgen from AMD could be the most competitive since the Terascale based HD series.  Not gonna spill the beans on the patents today, it's too early but right before launch, then perhaps I might eventually do another post similar to the one I did in the Spring that was picked up by tech media. All you need to know is that AMD id seemingly doing a fundamental overhaul of pretty much every aspect of a GPU, with a particularly strong focus on cachemem system efficiency and data locality.  But I can tell about the major scheduling changes in some of the patents, but it's really just the tip of the iceberg alongside the DGF + prefilter stuff.   Scheduling will go from top down orchestrated to localized hierarchical scheduling and dispatch down to the CU unit. Scheduling will be offloaded to Shader Engines with the command processor job being only to prepare work items and do load balancing between Shader Engines through ""work stealing"" indicated on idle or overloaded signals from the individual Shaders Engines. As a new thing scheduling and dispatch can be decoupled from the SPI completely at the CU level allowing each WorkGroup processor to dispatch its own work queue with unprecedented granularity and latency. The patent mentions an order of magnitude improvement in thread launch performance.  I have a post in here on that from \~8 weeks ago in case you're interested that goes more into depth. All this is to deliver better core scaling and prob drive increased performance for branchy code and GPU Work Graphs API workloads. An API that looks like AMD's new Mantle except it's a much bigger deal, Programmable shaders 2.0 really.",Neutral
AMD,"Number 3:    Nope. Still fumbling in the dark.   Just know where to look for info.       Speaking of that the NVPRO and NVRTX pages on Github are a treasure trove of info and I highly recommend giving it at least a browse.    [https://github.com/NVIDIA-RTX](https://github.com/NVIDIA-RTX)   [https://github.com/nvpro-samples](https://github.com/nvpro-samples)  The Neural appearance models research paper used for Neural Materials is interesting too. Uses a neural BRDF + importance sampling:   [https://research.nvidia.com/labs/rtr/neural\_appearance\_models/](https://research.nvidia.com/labs/rtr/neural_appearance_models/)      I can't wait to see this leveraged in future games across many material types to deliver unprecedented realism. Especially for character rendering with cloth, skin, eyes and hair unprecedented offline render quality visuals in real time.",Positive
AMD,Interesting. AMD is using current gen to test drive next gen stuff.  Like how zen3 with 3d cache and  zen4 with zen c core,Positive
AMD,"Been waiting for High Yield's Strix Halo deep dive (just waiting for Chips and Cheese).  Reconfirming what we know, Granite Ridge CCDs are different to the ones found in Strix Halo requiring a new tape-out. 4% in die space savings when moving from SERDES GMI-narrow links to RDL fan-out interconnect (using TSMC InFO-oS). This may be one of the reasons why we see lower than expected volume for STX-H and it's high price tag (not to mention it's large IOD is \~307mm2).  The benefits of the new 'sea-of-wires' is lower power for data transfer, so lower voltages for freq, which is what you'd find sipping power on current desktop Ryzens on the IOD (also memory) and lower latency and higher bandwidth. [u/high\_yield\_yt](https://www.reddit.com/user/high_yield_yt/) used Aida's $/mem benchmark, which I've barely come across so it's finally nice to see some numbers and as you can see bandwidth is orders of magnitude higher. Though, while we wait for C&C, wished he used Clamchowder's [Microbenhcmark suite](https://github.com/clamchowder/Microbenchmarks) as it is superior to Aida's. Also, would've loved to see C2C latency test, IIRC Aida has one(?) but there are tools like [https://github.com/nviennot/core-to-core-latency](https://github.com/nviennot/core-to-core-latency) that you can easily run.",Neutral
AMD,"What would the shorelines look like on 6th gen EPYC? With a taller I/O die, it looks like the higher aspect ratio Zen 5c dies could work. But there are two columns of dies on each side in Zen 5. Could they lay out the CCXes to flank the I/O die on all four sides?",Neutral
AMD,"I wish I understood this topic better  with videos like this, I am making progress",Positive
AMD,"On the question of where the 3DVCache would sit, as it is probably can no longer sit below the CCDs: Wouldn't it be viable to move the 3DVCache either as L4 or with an entire L3 implementation onto the IOD which probably has some colder locations and with the new interconnect might still offer very good latency?   The CCDs would maintain full speed and coherence becomes much easier to manage, only question is if the distance is not to large to the IOD to offset some of the 3DVCaches performance. Also would raise questions for servers as the IOD might have different constraints or be to small to carry so much cache...",Neutral
AMD,The sea of wires design doesn't look like it would work for Epyc. There isn't enough room on the IO die to connect all the cpu chiplets if they have to be in physical contact. I wonder what they will do there.,Negative
AMD,Hopefully this means the high idle power draw of chiplet Zens is finally over.,Positive
AMD,"Please, AMD.  Just put 3D/Infinity cache into your APUs for the benefit of the Steam Deck 2, and I’ll be happy.",Positive
AMD,"WTF is that audio...? There's constant clicking, almost sounds like someone is clicking their mouse in the background. Terrible production quality...  I really wanted to watch to the end, because the topic is interesting, but it's driving me absolutely crazy!",Negative
AMD,Whats exciting is. They hold on to die to die interconnects through pcb traces for so long since zen2  On consumer chips especially to keep the costs down with relative performance good enough to topple i9's.  Now that thats finally put to rest. We will see how much performance potential it will give us. Especially for EPYC and threadripper lineups. A one huge die is coming in? Or for GPU? Even though its a stretch cause even here GPU die is a seperate wafer cut out. I at least expect a Navi 31 levels of MCM to power the UDNA flagship. Which Nvidia still refuses to do on their consumer chip architectures.,Positive
AMD,Weren't there other leaks saying that Zen 6 and Navi 5 was/are gonna use bridge dies instead?,Neutral
AMD,I think this is an important enabling technology for hardware for xbox and valve as well as other PC applications. I want to know more about related technologies nvidia and intel or anyone else are using?,Positive
AMD,"For every 1/r reduction in wire thickness that you get by using InFO, you'll need a corresponding 1/r\^2 reduction in wire length to keep the resistance of the individual wire unchanged. Needless to say that the length reduction factor needs to be better than quadratic to actually decrease the resistance and hence, power dissipation per wire.  Too bad he didn't do any power measurements or if he even has the means to do so, because that would be the most important parameter to measure in this type of packaging.",Neutral
AMD,They also tested a lot of RDNA 4 changes using RDNA 3.5.,Neutral
AMD,If these AMD APUs would only be available on desktop/server mainboards. They only seem to be available in laptops and boutique minipcs.,Neutral
AMD,"If the dies are smaller, I wonder why they don't refresh desktop and server products using them. Would require a new IOD, but they need one anyway for Zen 6...",Neutral
AMD,"I don't think CCDs on all four sides would be desirable as the memory/pcie phys also prefer the shorelines.   But on the other hand I also don't have a better geometry in mind that doesn't become insane long and fits 16 CCDs.   I think there are 2 possibilities. The first one is as you say, put maybe 16 CCDs in a square around the IOD but then phys become more complex to implement. Or make a scalable IOD where one IOD can hold 8 or 10 CCDs split over two side, has all the phys on the third side and can connect to another IOD on the fourth side totalling 16-20 CCDs, with two sides dedicated for phys for high end epyc, and single IOD for mid-range epyc with up to 8 or 10 CCDs.",Negative
AMD,"There has been a relatively recent rumor/leak about the [layout of Zen 6 Epyc](https://www.hwcooling.net/en/zen-6-document-leak-more-cores-pcie-6-0-and-2-5d-packaging/) (ignore the top image).  As always, make of the source what you will, but it does look like plausible solution if they want to keep scaling with enough bandwidth. [Zen 5 Epyc](https://www.amd.com/content/dam/amd/en/images/products/data-centers/2909511-amd-zen5-chip.png) is already straining the current IFOP concept, with 16 CCDs needing to connect through the substrate.  Supposedly, it can be up to 4 of those IODs in a row flanked by the larger CCDs, making for a total of 256 cores.",Neutral
AMD,Its very shallow for people to understand. If these die shot videos had to be %100 correct the videos wouldnt exist. Because you cant just call what it is of a nanoscale factory of logic boards and i/o gates circling on a megahighway. Thats the beauty of scaling production and chip design. For example his zen5 indpeth look had a a lot of guessing because AMD is crazy and changes its design a lot every generation.,Negative
AMD,"It's a very complex topic to start with, and packed with lots of history. That being said, it's always good to start somewhere. The era of LLMs are here to help. But I've been following a lot of the ""silicon"" news from youtube, you slowly build an understanding if you're familiar with adjacent topics.",Neutral
AMD,"AFAIK the contact density from CCD to cache die on Zen 5 X3D is significantly higher than CCD to substrate on Strix Halo. Making a bunch of TSV on the cache die as passthrough for the interconnect on CCD and aligning them properly shouldn't require higher precision than production of current Zen 5 X3D. This way AMD can align the contact at the bottom of CCD with the TSV on cache die and fan out from the bottom of cache die.  I think cache die below CCD on Strix Halo like package is doable, but it may come at a power cost due to the signal going through an extra layer of TSV.  Regarding L4 on IOD, AMD has already said they can do it on Strix Halo, but GPU benefit from the cache more so all the MALL is reserved for the GPU, however, they can configure it to be used by CPU with ""flip of a bit"". IIRC it's in this interview: [https://www.youtube.com/watch?v=yR4BwzgwQns&t=384s&pp=ygUac3RyaXggaGFsbyBjaGlwIGFuZCBjaGVlc2U%3D](https://www.youtube.com/watch?v=yR4BwzgwQns&t=384s&pp=ygUac3RyaXggaGFsbyBjaGlwIGFuZCBjaGVlc2U%3D)",Neutral
AMD,"> Wouldn't it be viable to move the 3DVCache either as L4 or with an entire L3 implementation onto the IOD which probably has some colder locations and with the new interconnect might still offer very good latency?  Why not have the cache stacked on top of the CCDs to expand the existing L3 cache, and another stacked cache on the IOD to serve as a L4 cache?  There's also the IBM method from a decade ago where they give every core a large L2 cache, and a portion of the L2 cache can be tagged by another core to be used as a virtual L3 cache. Or a CPU die can use another CPU die's L2 caches as a virtual L4 cache (as long as the CPU dies are on the same package to avoid communicating over the motherboard). In theory, a single core in an entire CPU package could address about 8GB cache with the virtual L3 and L4 setup. This allows their mainframe CPUs to switch between workloads where it favors large L2 caches and workloads where a large L3/L4 cache is preferred.",Neutral
AMD,"You absolutely CAN do that. But latency, bandwidth, and power will be worse, as the signal will need to travel significantly further. The further away a signal needs to go, the worse it becomes in nearly all aspects.  The easy solution is just move the Vcache over a little bit to not sit right where the info goes.  And if you need more? why not stack it back on top as well? I realize at some point interconnects to the vcache get crazy. But there are a LOT of ways to handle this. it's an engineering problem that has many practical solutions.",Negative
AMD,"L4 on IOD is better anyway because it benefit both chiplets. It acts as a buffer between System memory and L3.   The issue is on the server side, Epyc has 12 chiplets, that means to get equivalent of 12 chiplet 3D Cache, that L4 on the IOD is going to need to be very huge.",Positive
AMD,Interesting thoughts. With this interconnect the latency probably wouldn't be an issue (it's way better than 2 serde jumps for cross-ccd latency on current ryzens). And there's a lot of space on top of the IOD to potentially get even more cache.,Positive
AMD,"It depends on the difference in latency moving data from the cache between the dies. I still think the lowest odds are that they just stack the v-cache on top of the CCDs going forward. They're going to have separate SKUs for v-cache parts for cost reasons anyway, so customers will still be able to get the higher clocking parts if that's what their workload benefits from.",Neutral
AMD,That is not how cache works. And I don’t believe it could be moved to iOD.,Negative
AMD,"Not the existing I/O die, but that doesn't need to stay the same.  The important metric is edge area, so they can make it elongated to increase edge area without blowing up the total die area.  The chiplets would then be arranged around a large rectangular I/O die to do the fanout.  Not saying that's what they'll be doing, but it's an obvious and easy solution to that particular problem.",Neutral
AMD,"Indeed. I was rather upset with my 5800X3D sucking down 50 watts idling at desktop, so I undervolted my VSOC from 1.078ish volts to 1.000 volts.  No noticeable improvement in idle :(  The main reason I stick with Intel for servers and things that I keep on 24/7. AMD for my gaming and schoolwork PC.",Negative
AMD,"What I wrote in another post  >If Strix Halo (Ryzen AI Max+ 395) is any indication of their new chiplet packaging found in Zen 6 desktop, then there is good news.  >[https://youtu.be/kxbhnZR8hag?si=DcCjKpPWZVF9fC4O&t=270](https://youtu.be/kxbhnZR8hag?si=DcCjKpPWZVF9fC4O&t=270)   [https://youtu.be/OK2Bq1GBi0g?si=Lo6mU0Cs-QQ8Fo93&t=220](https://youtu.be/OK2Bq1GBi0g?si=Lo6mU0Cs-QQ8Fo93&t=220)   [https://youtu.be/uv7\_1r1qgNw?si=adqEnRTICL0D\_HMd&t=393](https://youtu.be/uv7_1r1qgNw?si=adqEnRTICL0D_HMd&t=393)  >\~10W TDP idle (some stuff opened in the background) across two CCDs (pretty much 9950X) and a large IOD housing a big iGPU.",Positive
AMD,"I can hear it pronounced on my headphones at the 6:00 mark, but at that point his speaking level is rather uncomfortable to listen to. If I switched to my IEMs I'd probably hear them",Negative
AMD,I didn't notice it until I paid attention to it. I didn't find it that bad but I know my hearing is  failing. Funny because I can normally hear a lot of background noise in most other videos. Like fans whiring away or speaking behind the camera.,Negative
AMD,I usually also don't like when that happens but I either couldn't hear it or my phone speakers were just loud enough for me to hear his voice without hearing the background.,Negative
AMD,It's just High Yield,Neutral
AMD,Do not watch the video about that cool ultrasonic knife. I couldnt listen for more than a minute.,Negative
AMD,"What's your audio setup? I have jds labs dac+ and amp+ with few various high end headphones and I can ""barely"" make out the mouse clicks when hes talking. Either I'm deaf, you have godlike hearing or your audio setup is really good.   It is easier to hear at 6:00 but anything before that unless I focus on it, cannot really hear it",Positive
AMD,something's fucked on your end,Negative
AMD,"> There's constant clicking, almost sounds like someone is clicking their mouse in the background.   No, there isn't.  It's just plosives.",Negative
AMD,"This isnt really a leak. I mean with strix halo it seems to not be the case and unless AMD thinks it doesn't work good enough they will highly likely use that interconnect.    If leakers claimed a different design that could have many reasons ranging from ""amd planned/tried it but didn't work out / didn't use it"" to ""How do I know? I made it up""",Negative
AMD,"You are completely missing the fact that the SerDes is aggregating many channels of digital signals into a single line.  Thus it has to have signal transitions many times faster, which requires stronger drive signals and more current.  A single inFO line is running much slower than a single serdes line from the previous design and thus is driving much less current and thus does not require unchanged resistance.  You are also ignoring line capacitance which is more important than resistance for high speed signals.  The power needed to drive switching signals is proportional to the line capacitance \* frequency\^2.  You are basically having to fill or empty a little capacitor on each transition.  So individually the lower switching frequency InFo lines save on both frequency and capacitance.  Even if you assume that in aggregate all the little info lines add up to the same capacitance as the serdes lines, the switching frequency savings dominates.",Neutral
AMD,Now calculate the effect of reducing clocks by a factor of n.,Neutral
AMD,There are only 16 PCIe lanes.  There's no benefit to putting it on a larger board.,Negative
AMD,Because they require a different platform lmao,Neutral
AMD,"Framework desktop has one  Afaik they are neither compatible with CPU sockets, nor socketed memory...",Negative
AMD,"Framework sells just the mini-ITX motherboard:  [https://frame.work/marketplace/mainboards](https://frame.work/marketplace/mainboards)  The CPU only has 16 pci lanes total so there are not going to be a plethora of various motherboard configurations, there just isn't enough I/O.  The framework has an x4 slot, given the I/O configuration I'm not sure it is possible to do an x8:  [https://static.tweaktown.com/news/9/8/98420\_53\_amd-strix-halo-zen-5-mobile-apu-pictured-chiplet-based-integrated-gpu-is-powerful\_full.jpg](https://static.tweaktown.com/news/9/8/98420_53_amd-strix-halo-zen-5-mobile-apu-pictured-chiplet-based-integrated-gpu-is-powerful_full.jpg)",Neutral
AMD,"If memory latency is lower and bandwidth is higher due to the new design, wouldn't that make X3D cache less benficial?",Neutral
AMD,"Having stacked L3 *and* L4 will have a massive impact on latency, which will impact performance in apps that don't benefit from large cache.  IBMs hardware is quite specific to the needs of their particular niche, so the things they do might not be all that useful for general purpose compute.  Edit: thinking about it, it makes sense if they end up unifying last level cache across multiple chiplets at some point. That could have huge benefits for multithreading",Negative
AMD,"Zen6 is on n2 so real estate will be extremely expensive, no chance they opt for a giant L2.",Negative
AMD,"> There's also the IBM method from a decade ago where they give every core a large L2 cache, and a portion of the L2 cache can be tagged by another core to be used as a virtual L3 cache.  It may not be technical limitations, but rather patents/licensing that prevent anyone else from doing so.",Neutral
AMD,"> The easy solution is just move the Vcache over a little bit to not sit right where the info goes.  I think this nails it. The vcache die has always been smaller than the CCD. So far, they've have to add structural silicon to even everything out.",Neutral
AMD,"In what way is it not ""how caches work"" and why couldn't it be moved onto the IOD at least as an L4? If it was moved to the IOD as L4 the question is if they could keep latency tight enough so that it still has a similar benefit but in principle the ""worst case"" would be a 3DVCache enhanced infinity cache. This might prove to be simple and coherency for multi-ccd variants might be more efficient to maintain.   Best case would be sufficiently low latency and high enough throughput such that it could act as a good dedicated L4 or integrate even tighter with the L3 of the CCD but on the IOD.    Of course it is none-trivial to ""just"" move the cache onto the IOD and the purpose of the cache might need to be redesigned. Latency will very likely take a non-negligable hit so the role of the cache will change. But if this tradeoff comes with full power CCDs and the performance gain in simulation and gaming workload can still be maintained, it might be worth it.   I don't think its obvious to judge whether this is possible or not without having much better data like roundtrip latency of the new interconnect and cache dependeny and structure of Zen 6.",Neutral
AMD,The Pentium 2 says hello.,Neutral
AMD,"They've already done something similar on their Navi 31/32 GPUs for the L3 caches and memory controllers. In that form it's probably not quite ready for CPUs yet since the cache tiles are only 16MB each and relatively large for what they offer. With CPUs there's also the concern of them being much more latency sensitive than GPUs.  But still, as interconnects get denser and their performance improves, there has to be a point where the question turns from ""Is it even possible?"" to ""Why not just do it?"".   It's not exactly a novel concept, recent Xeons and Apple's Ultra variants already use multiple CPU tiles with caches shared across them.",Neutral
AMD,"Cache is basically ""the faster part of a memory hierarchy"" that stores the most used data  The current stack is loosely  L0(e.g. microop cache)/L1/L1.5 -> L2 -> L3 -> L4 (e.g. eDRAM on broadwell) -> RAM -> SCM (e.g. optane and certain high performance SLC NAND) -> NAND -> HDD     On my NAS, the cache for the HDDs is optane (l2 arc) and then DRAM for the ARC.   We're caching all the way down.",Neutral
AMD,It's exactly how cache works.  Fetching data from the I/O die would still be a huge improvement over going to main memory.,Neutral
AMD,> 5800X3D sucking down 50 watts idling at desktop  Is that whole system? CPU be itself should be <20W.,Neutral
AMD,There is NO WAY a 5080x3D uses 50W on idle unless something is seriously broken with your whole power management.,Negative
AMD,"Huh. According to HWiNFO64, my 5600x's SOC idles at about 6.5W (VSOC undervolted to 0.913V while running manually tuned DDR4-3600) and the cores idle at about 3-5W.  Did you use XMP or manually overclock your RAM? What's your Infinity Fabric speed? That can dramatically ramp up the SOC idle power usage.",Neutral
AMD,"We can make a faster steam deck, but not one that is significantly faster without also increasing the TDP.  If AMD can keep the thermals down while preserving the battery life and deliver a big enough bump in performance to allow the steam deck to work with many of the newer titles then that would be amazing and warrant a Steam Deck 2.  Until then, Valve has said they're not really interested in doing minor upgrades.",Positive
AMD,"Generally yes, and since it uses DDR5L shared with the CPU, it’s also memory bandwidth starved.  And power starved…and sometimes there’s a CPU bottleneck as well. Honestly it’s a remarkably well balanced device, especially given its 800p screen (handhelds with higher performing APUs with 1080p screens don’t move the needle much in framerate due to the higher resolution, and typically also require significantly more power).",Neutral
AMD,"Even if more 3d cache wouldn't increase performance (it will for most code due to cache being faster than RAM), it would still decrease data movement power consumption and allow more battery life.",Neutral
AMD,"Yeah, might try later on speakers. I was on earphones so I felt the clicking needling my brain :P",Neutral
AMD,"My ""setup"" (I wouldn't even call it that :P) is just plain budget BT soundcore (anker) wireless earbuds... for like $30. And my volume almost never exceeds 50% at the OS level.  It's not like I'm an audiophile, the video was just THAT bad :P I've never had similar issues before.  I wouldn't say my hearing is godlike either, but I'd admit that sometimes I get annoyed by such things more easily than others... ¯\\\_(ツ)\_/¯",Neutral
AMD,Nah it is there. Turn the volume up slightly than what you normally have and you can clearly hear it if your ambient noise is low.,Neutral
AMD,"No, it's not. It's the video. Pay attention and you'll hear it.  Edit: It's pretty clear towards the start of the third part (2:20) and forward.",Neutral
AMD,"It doesn't sound like plosives... As someone else pointed out earlier, it becomes super clear around 6:00  My best guess would be some sort of bad hardware (microphone) trying to do bad quality heuristic noise canceling or something.",Negative
AMD,"Yeah, but it’s been corroborated across multiple different leakers and products. Like the “Magnus” apu and Medusa point",Neutral
AMD,The AMD's 8000 series has 20 with 4 reserved for the io chipset. Probably the same.  As you don't need anything for the GPU that can surely support a nice HBA + 10GBe + 3-4 m.2. But I surely acknowledge that more would be better.,Neutral
AMD,obviously I meant as motherboards with mounted cpu as it's BGA. They would be perfect NAS to small desktop platforms.,Positive
AMD,"Thanks, looks like they really deleted the 4 lines the 8000G had for the IO chipset. Sigh...  With boutique I meant the framework:-)",Negative
AMD,"They could always tweak the architecture to better utilize the improved cache system.  Zen 5 benefits more from X3D than Zen 4 since the architecture is more bandwidth-hungry and is actually bandwidth-starved on GMI narrow config without X3D, which is partially the reason desktop Ryzen 9000 non-X3D has underwhelming performance uplift vs Ryzen 7000.   Zen 6 can be engineered to be even more bandwidth-hungry than Zen 5 to take advantage of both the next-gen interconnect and X3D.",Neutral
AMD,"> wouldn't that make X3D cache less benficial?  Maybe a little? A lot of the benefits are from use-cases where larger things can fit in cache, so those bigger lumps of data don't need to be retrieved from RAM which will still be vastly slower than retrieving the data from a slightly slower cache.",Neutral
AMD,"im some scenarios yes. but if you have high hit rate of something thats larger than regualar cache but smaller than x3D cache, youlll have massive improvements regardless. The difference wont be as big, but it will still be there as invalidating cache and going to RAM is expensive time-wise.",Neutral
AMD,as long as latency is bellow latency of talking to RAM it will offer benefits. You can ignore l4 cache if you can fit in L3 cache.,Positive
AMD,"I suspect it's better/cheaper, at least on most systems, to do what IBM is doing... just make the cache on a CPU bigger and allow sharing. My guess is that it trades some IO complexity for A LOT of flexibility.",Positive
AMD,"In modern cpu, L3 latency (when miss) is about 40ns, and DRAM is as fast as 50ns. If we move L3 or L4 over IOD, i.e. without direct connection to the CPU, which suggest the requirement of SerDes, it totally defeat its purpose.  also, given the larger size of 3DVCache, even a cache hit could take several lookup/match in the associative array table.",Neutral
AMD,"still not over io die, I believe. Architecturally that cache is still inside the CPU.  Edit 1: On Pentium II, where the CPU contains 2 dies, one is CPU itself, the other is the cache. The whole CPU is on a board that plug onto the motherboard that contains north bridge and south bridge. So, no the cache is not on IO die (north/south bridge chips)  Edit 2: people down vote me have no idea how cache works. The cache memory is not indexed by regular memory address, they are mapped/indexed by content. To have a hit/miss in cache, CPU logic circuit needs to go through a serial (row-by-row) of data storage that contains a mapping pair of memory addresses and cache address. When the memory address match (cache hit) , it takes the cache address and go to cache memory and load the data. Because the CPU needs to match this table serially, the cache is always pretty small otherwise the cache miss penalty will be huge. Therefore, it is hard for this to be outside of CPU, architecturally or physically.",Neutral
AMD,"Cache is fundamentally different than ram, because they are content match or content indexed not address indexed.",Neutral
AMD,"Like I said, that’s not how cpu cache works. You need to know that the data inside cache memory is not indexed by memory addresses. There is no “address” in cache, so an io die can not fetch the data inside cache memory.",Negative
AMD,"Just going off of CPU Package Power in HWInfo. Core draw was 0.4 watts, rest of chip was reading about 30-50ish depending on when you glanced at it.  At some point I will hook up a watt meter and check the draw from the wall.",Neutral
AMD,"Power plan is set to balanced, and I have undervolted the cores by -25 or -30mV, I can't remember which. Power plan is set to balanced. VSOC also undervolted.  It bobs between 30-50 watts in HWInfo's total package power, depending on when you glance at it. Cores take 0.4 watts, rest-of-chip is where the money comes from.",Neutral
AMD,performance power plan will do that for sure. no need for things to be broken,Neutral
AMD,"I used XMP, and my Infinity Fabric speed is 1,800 MHz. DDR4-3600 memory.  [Here's a screencap of HWInfo.](https://imgur.com/a/bVtiLWi)  I have Discord open, 4 tabs of Waterfox, and a Minecraft modpack chilling at the main menu screen. So not *totally* idle, but it should be light work.",Neutral
AMD,">not one that is significantly faster without also increasing the TDP  It can absolutely be done. Remember that Van Gogh is on the old N6 node. There is just not enough volume to justify a dedicated chip by AMD, which is why it hasn't happened so far, and instead we got bulky and not really that efficient laptop scraps, with useless NPUs instead of a mall cache, and either too many cpu cores or too few GPU WGPs.",Neutral
AMD,"Def played it loud enough. I just checked 6:00 again, and couldn't hear it without boosting the audio.   I usually keep everything at sub 10% audio levels, because it's otherwise too loud.... :v",Neutral
AMD,you should be a sonar technician,Neutral
AMD,"I can hear it, but it's not a problem",Neutral
AMD,"yes, I can totally hear it, but the hysterical overreaction is on your end",Negative
AMD,You just described 28 to 32 lanes worth of peripherals.,Neutral
AMD,Whats the point of actually using one of these specifically for a storage server though? I dont see any actual benefit from the main purpose of strix halo (the massive igpu) for your usecase over a 9950x?,Negative
AMD,They are in active development. There were some articles that came out a couple months ago on that subject.,Positive
AMD,"Well it's on an industry standard ITX motherboard, you should have no issue finding a desktop or server housing for it.",Positive
AMD,"> Zen 5 benefits more from X3D than Zen 4 since the architecture is more bandwidth-hungry  And on top of that, 9800X3D also runs at higher clocks than 7800X3D thanks to moving the V-Cache die under the CCD.",Positive
AMD,"Zen 5X3D seems to gain more in gaming vs Zen 5 in comparison to Zen 4 because of the clock speed gains that one gets from Zen 5's 3D stacking improvements, more than the core being more bandwidth hungry or anything.   Chips and Cheese profiling gaming on Zen 5 claimed that the core is held back by front end latency more than the back end, and when it is a backend memory bound it was more due to latency, not bandwidth.",Positive
AMD,"The problem is  that SRAM does not scale, so takes up a lot of die space, driving up cost. For someone like IBM, that doesn't matter as much due to the nature of their business.  The idea with 3D cache or moving to putting more cache on the IOD is that they can use older and/or cheaper nodes to increase cache sizes without a significant increase in cost.",Negative
AMD,"Look at the video. It is discussed how Zen 6 will no longer need serdes to communicate with the IOD as the chips networks will be directly connected.   Also if you take a look at this article by chips and cheese: https://chipsandcheese.com/p/amds-9800x3d-2nd-generation-v-cache   We can see that the latency for L3 cache is around 10ns, while DRAM latency with the old interconnect is ~80ns.",Neutral
AMD,So is the io die.,Neutral
AMD,"> To have a hit/miss in cache, CPU logic circuit needs to go through a serial (row-by-row) of data storage that contains a mapping pair of memory addresses and cache address.   > Because the CPU needs to match this table serially, the cache is always pretty small otherwise the cache miss penalty will be huge.  No... That's not how cache works at all.  Most modern caches are typically 4-way or 8-way [set associative](https://en.wikipedia.org/wiki/Cache_placement_policies#Set-associative_cache) (other numbers are possible, but 4-8 tends to work best).   With a 4-way set associative cache, the lower bits of the memory address are used to select a single set within the cache, which stores four cache lines. The data is guaranteed to be in one of those four memory locations (or not at all).   Which makes lookups very fast, you look at the memory address to find which set it should be in, and fetch that set's tags from the tag array. All four tags can be checked in parallel with special hardware, and then it knows exactly where in cache the data will be found.   -----  The cache needs to be small because of physics. A combination of speed of light delays and (more importantly) capacitance means the larger the array, the higher the access times.",Neutral
AMD,It's still part of a caching hierarchy.   RAM caches HDD/SSD contents.,Neutral
AMD,You know cache used to be physically on an entirely different chip right? Using an IO die instead would present zero issue.,Neutral
AMD,Power plan is set to balanced :/,Negative
AMD,"Something tells me there might a motherboard setting that's screwing with things.  The troubleshooting idea I can think of to help narrow down what could be the root cause:  1. Set everything to default (e.g. RAM running at JEDEC speed and voltage), then check idle power usage.  2. Enable only the CPU Curve Optimizer, check idle power usage.  3. Enable PBO (not sure if your motherboard allows that for the 5800X3D).  4. Then enable the XMP.  If the idle power shoots up with XMP, it might be the motherboard is overvolting things to guarantee Infinity Fabric and RAM stability. In that situation, you may have to manually configure the IF and RAM settings, and all of the associated voltages, to take the voltage control away from the motherboard.  My motherboard's voltages were all over the place when I left them at auto while overclocking my RAM. I had to lock those down.",Negative
AMD,"In this picture minimum power your cores used are 11.4W, CPU Core Power (SVI2 TFN) value. Your CPU SoC Power (SVI2 TFN) value is also a bit high, should be 8 to 9 watts.  So with true idle you would get <25W even with a bit high SOC power.",Neutral
AMD,"The reality is that the old chip is more than fast enough for many of the titles people play on the go, and most people who have a steam deck have a dedicated PC also.  Valve had to pay AMD up front to develop the Van Gogh APU for the original steam deck, and will have to cover that cost again for the next whether it's up front or baked into the per-unit price. They don't sell enough units to do that annually or anything, but they've probably already got AMD at work on their next APU.",Neutral
AMD,Goal post status: moved,Neutral
AMD,"pcie 3 x8 covers a 16 port sata HBA, 1 lane of pcie4 is 4GB/s which easily covers 10gbe, 2 lanes with 4-8GB bandwidth is good enough for m.2.",Neutral
AMD,Look at the lower cpus. They have really attractive TDPs.,Positive
AMD,"You can put that same cache on the CPU cores themselves instead of the IOD though.   What's the point of having cache that MUST take the performance hit over having cache on the CCD that can potentially be used at full performance.   It's a pointless downgrade. The only potential benefit is somewhat reduced interconnect.   On a consumer system with 2 CCDs, that's not a problem.   On a server with 4 CCDs... also not a problem.   If you're going up to 8 CCDs then you're probably looking at a ring bus topology for distributing cache... and at that point it MIGHT make sense to centralize the last level cache on the IOD.  Even then... dual ring bus could probably fix that and you just treat the next level out as another cache level.",Neutral
AMD,"10ns is when cpu is >5ghz and cache hit. You need to consider cache miss. cache miss latency should still be smaller than DRAM latency.   This actually proves my point, because: how do you put V3Cache over IOD when the substrate is already filled with ""sea of wires"" ? You will not be able to support the complicated associative array lookup circuit and controller for data load up from DRAM and to L2 cache.",Neutral
AMD,"No, back then there are north bridge and south bride chips. North bridge chip deals with memory and south bridge deals with io to isa bus and other stuff.",Neutral
AMD,"What you described, 4 way or not, has nothing to do with the fact that cache hit/miss is done row by row in the tag array(mapping/index table).",Neutral
AMD,"sure, but because they are indexed differently, to determine a cache hit/miss, it requires tremendous more complicated circuits at very high speed.  When cache missed happens, you also need to move data from lower level cache or RAM, it adds so much overhead. This overhead has a limit because it should not make the higher level cache perform slower than the lower level memory.",Negative
AMD,"that is when the actual IO is done on the other side of CPU Bus. On another chip != on an IO die. When the CPU is 100mhz, sure you can have that on another chip that is 1cm away. Try that now in 2025.",Neutral
AMD,"Enabling Eco mode on motherboard saw significant improvements in idle consumption for both 3800X and 7800x3D for me. It also reduced (but not eliminated) random pointless boosting on idle and small tasks. No, i dont need 4 GHZ and temperature spiking to 75C to open a file folder. I can wait 3ms longer for the folder to open. In fact i wont even notice the difference.",Positive
AMD,"nothing was moved, maybe you misunderstood my initial comment",Neutral
AMD,"There seems to be only one 10G ethernet controller chip that supports PCIe 4.0, which only recently came out, and it's not particularly cheap.  You can't rely on higher PCIe revisions to cover the throughput requirements, as peripherals always lag far behind for compatibility reasons.  In any case, I stand by my assessment.  There's no benefit to putting Strix Halo on a standard-sized PC motherboard.",Negative
AMD,"FYI one lane of PCIe 4.0 is \~20Gbit/s or \~2GB/s.   A PCIe 4.0x4 M.2 uses upto \~80Gbit/s or \~8GB/s.   Sata is 6Gbit/s.  The Ryzen 8000 series has 20 Lanes in PCIe 4.0, no 5.0.  But a X870 Chipset **tunnels** 4x4.0 Lanes into another 8x4.0 lanes, 4x3.0 lanes, 4x Sata ports and several USB ports.   X670 has even more ports/lanes, but sucks twice as much power.",Neutral
AMD,I think youre confusing strix halo for strix point which is not a chiplet design. strix halo is configurable tdp down to 45 watts which you can also configure 9xxx series,Neutral
AMD,You're incorrectly assuming cache on the io die would have to be accessed as though it were off chip.,Neutral
AMD,The good old days,Positive
AMD,"It's not done row-by-row. That would be too slow.  The memory address gives you the index within the tag array, and the cache controller can directly access that row. Each row contains one set of four ways.  There is simply no reason to check any of the other rows.   -----------  Fully associative caches do exist (where the data could be in any row), but they aren't used for caching data/instructions (you usually see them things like caching TLB entries, or in network switches). And even these fully associative caches can be searched fully in parallel, thanks to the magic of [Content-addressable memory](https://en.wikipedia.org/wiki/Content-addressable_memory) or CAM.   But CAM arrays are very expensive to implement in hardware, so you don't see it anywhere it's not needed.",Neutral
AMD,As long as it's closer than the memory controller I'm not seeing the issue.,Neutral
AMD,No,Neutral
AMD,If it had the 4 pcie lanes for an io chipset that wouldn't be a problem. Unfortunately that doesn't seem to be the case so you are right.,Neutral
AMD,"Are you talking about Pentium II or new chip design ?  If it is Pentium II, then the cache was not on io die.  If you are talking about new design, then of course it is, because the data would be serialized and destabilized when it is off the cpu die. Take a look at Strix Halo where it uses new Fan-out to avoid serDes in this video: [https://www.youtube.com/watch?v=maH6KZ0YkXU](https://www.youtube.com/watch?v=maH6KZ0YkXU)  If you use off-die design, you can not avoid SerDes.  If you want to argue that it could still be done at all cost, then sure, but no one would actually design a cpu that way, because cache miss penalty would be too great to be useful.",Neutral
AMD,You are right. I was mistaken about matching it up one by one. I forgot that the rows are indexed by part of incoming address. It’s been too long since last time when I still had the book.,Negative
AMD,">An electron accelerated from rest in a vacuum by a 12,000 V/m electric field would cover 1 cm in approximately 30.8 nanoseconds.   do you know how many nano seconds per cycle for a cpu running at 6Ghz ? it is 0.6 nano seconds. And L3 cache usually does not go over 50ns when cache miss, or 30ns when cache hit. 1cm will be a big problem.",Neutral
AMD,The entire point is how to avoid serdes.,Neutral
AMD,RAM configuration  1. ⁠16GB (41.76%) 2. ⁠32GB (36.26%) 3. ⁠8GB (8.05%) 4. ⁠64GB (4.33%)  Most popular VRAM configuration  1. ⁠8GB (33.66%) 2. ⁠12GB (18.99%) 3. ⁠6GB (10.40%) 4. ⁠16GB (7.52%) 5. ⁠24GB (2.3%)  16GB and 24GB GPUs are still in the minority while 8GB and 12GB are the most prevalent,Neutral
AMD,Most popular 50 Series:  1)      RTX 5070 (1.69%)  2)      RTX 5060 (1.12%)  3)      RTX 5070Ti (0.91%)  4)      RTX 5060Ti (0.89%)  5)      RTX 5080 (0.84%)  6)      RTX 5060 Laptop (0.80%)  7)      RTX 5090 (0.31%)  8)      RTX 5070Ti Laptop (0.21%)     Most popular 40 series:  1)      RTX 4060 Laptop (4.84%)  2)      RTX 4060 (4.25%)  3)      RTX 4060Ti (2.92%)  4)      RTX 4070 (2.16%)  5)      RTX 4070 Super (1.79%)  6)      RTX 4050 Laptop (1.50%)  7)      RTX 4070 Laptop (1.14%)  8)      RTX 4070Ti (1.11%)  9)      RTX 4090 (0.89%)  10) RTX 4070Ti Super (0.89%)  11) RTX 4080 (0.69%)  12) RTX 4080 Super (0.77%)  13) RTX 4080 Laptop (0.22%),Neutral
AMD,It is always so funny to see the overall meltdown of this sub at the monthly survey,Negative
AMD,"Top gains this month are:  4060 laptop (0.22%)  Nvidia graphics device (0.19%)  5070Ti (0.16%)  5060Ti (0.15%)  5070 (0.12%)  5060 (0.12%)  5060 Laptop (0.11%)  5080 (0.10%)  Amd Radeon(TM) Graphics (0.09%)  4050 Laptop (0.09%)    The 5070 is the most popular RTX 50 series cards at 1.69%, and has almost caught up to the RTX 4070 Super at 1.79%, which is at 16th place.  This puts the 5070 at 19th place.",Positive
AMD,Linux gained by 0.04%. Ye@r of the Linux desktop achieved?,Neutral
AMD,"Don't mind me, just waiting for the comments explaining how either rigged or broken the Steam Hardware Survey is for AMD GPUs hence we should disregard this data along with John Peddle Research data and every data survey / analyst out there.",Negative
AMD,"It is interesting seeing how VRAM 6GB (10,4%) and 8GB (33,7%) still dominate, with only 12GB (19%) rivaling them.   16GB at 7,5% is still far away.   I guess we shoulnd't fear that games will cater only to 12+ VRAM any time soon.   -  Also, 8 CPU cores seems to be growing and soon being mainstream",Positive
AMD,"More than half the people have 16GB of less of RAM, yikes. That's gonna be a major pain point in the near future. Sad because RAM is super cheap.",Negative
AMD,"If you filter for windows + vulkan then you can find the 9070 at 0.22% (+0.02%). It's possible the one name combines the 9070 and 9070XT, as they're explicitly combined on Linux (where 9070/XT has 1.1%/+0.05%).  There's also the 9070 GRE at 0.02% (+0.01%).",Neutral
AMD,HUB swears RDNA4 must be a strong seller and that AMD has 20% marketshare though,Neutral
AMD,"I'm not seeing Steam OS on the list. Is it really less popular than ""Manjaro Linux""?",Negative
AMD,It's interesting that the Intel-AMD split varies so greatly depending on OS. On Windows it's 58-42 in favor of Intel. On Linux it's 68-32 in favor of AMD.,Positive
AMD,"75% market share Nvidia in gaming… That’s no bueno.  Competition is good for quality and customers… In this case, Nvidia can do whatever they royally want :/",Negative
AMD,"Shame, looks like DirectX 12 continues to only be used in Windows 10 and older which is seriously holding back innovative game designs. /sarcasm",Negative
AMD,"AMD가 나한테 무엇을 했나?,지금까지.",Neutral
AMD,Seeing 6 GB as third most popular in 2025 is just sad.,Negative
AMD,"Genuine question: is there some option somewhere in Steam to opt out of this?  I ask this because I have never, and I mean literally never, got the prompt to fill in the survey, so I think that I've probably opted out of it somewhere?",Neutral
AMD,"Either the surveying is shit and favors specific users - I was asked to take survey merely 3 times in my life time - which should how scarce data is... Or, RDNA4 really sold absolute jack shit outside couple of EU countries.   Here in Poland I bought RX 9060 XT 16 GB, because it was 22% cheaper than RTX 5060 Ti 16GB (which also has only 8x PCIE interface, relevant for B450 based system).   Everyone is talking massive RDNA4 sales, but it doesn't reflect anywhere tbh",Negative
AMD,"Steam doesn't tell people who the survey works, I'd assume a whole heap of 5000 series cards turning up includes net cafes and the like, does it include all the hardware ever, does it have a time that it stops being counted.  The new Radeon cards not being in there surprises me because newegg, Amazon for instance show them being their top sellers or in the top 5, so I'd assume they've sold well, amds gaming revenue is way up yet nothing on the survey. I'd say at best it's either inaccurate or buggy.",Neutral
AMD,That makes sense because 1080p is at 53% and 1440p is at 20%,Neutral
AMD,Even 6GB VRAM is much more prevalent than 16GB.,Neutral
AMD,That makes sense because 16gb & 24gb are expensive,Neutral
AMD,"It blows my mind that 12 gb is the 2nd most popular option. I can only think of 5 cards with the vram configuration. 5070, 4070 super, 7700 xt, 580 and 3060.",Neutral
AMD,"8GB GPUs still being vastly popular, yet if you go to Reddit you will think it is the opposite and if you have 8GB gpu you either need to explain yourself or need to apologize to them for getting an 8GB GPU.",Negative
AMD,So many poor people smh. Don't they know how affordable the 5090 is?,Negative
AMD,"it didn't even detect my VRAM amount, so I would not be surprised if the numbers are a bit dubious.",Negative
AMD,If 8gb is the most popular why not design games around that.,Neutral
AMD,Reddit told me the 5070 was trash and that no one would buy it though?,Negative
AMD,"Surprised how unpopular the 5090 is. Sitting at less   than half the share of the 4090, which is now the second lowest selling Nvidia gaming halo product. I understand the price being insanely prohibitive, but I guess it's interesting to see just 0.31% still. That number is so low, that reviewers or people using it for work or home AI likely make up a large chunk of it.",Negative
AMD,"No wonder Nvidia isn't sure about when to, or even release the RTX 5000 Super series",Negative
AMD,"nvidia has a bot army spreading made-up statistics term called ""random sampling"". have you heard of random sampling? i haven't. it's a made up term brought to you by the fake frame people, with fake numbers.  believe me i have good sources from very smart people on pcmasterrace, and they tell me, ""wow i have an amd build but i don't get the survey from steam"". can you believe that? this is a TOTAL WITCH HUNT and we must stop the SURVEY INTERFERENCE from the corrupt radical deep green state and PROTECT GAMERS!!  thank you for your attention to this matter.",Negative
AMD,"No, it will happen when the effect of win10 support ending will show in steam survey next month, providing a phenomenal 0.1% linux share increase.",Neutral
AMD,Looking at the comments to upvote ratio. Might have rustled some jimmis here.,Neutral
AMD,"You also failed to mention how in the overall statistics, AMD showed 15.8% share in Apr 2024 and it is now up to 17.8%. So they showed overall growth with no RDNA4 users according to the survey apparently.",Neutral
AMD,Those people live rent free in your head too.,Negative
AMD,"It's not a competition so I won't say it's rigged, but we see cards with INCREDIBLY small usage numbers being accounted for on the hardware survey, but I'm to believe that the 9070xt has sold so few that it's not even recorded?   We really think that the 6750 GRE 10gb has sold more than any 9000 series model? Because it's on the survey at the bottom lol.",Negative
AMD,"iirc from another post RNDA4 gpus are in it, its just that Steam does not recognize them correct - iirc someone posted that he had an RDNA4 car and it showed up in steam as ""AMD Radeon Graphics""",Neutral
AMD,"Because earnings is literally all that matters, and AMD earned 20% of total gaming revenues.   Aside from various retailers, RDNA4 is also featured as Top 10 consistently on Newegg and Amazon. What this simply means is that a lot of people own prebuilts and laptops, and that makes sense as there are barely any Radeon in prebuilts and laptops.",Positive
AMD,"we need gamers nexus to expose the valve/nvidia racket, he can do it with his amazing journalism and flawless, unbiased, reporting",Positive
AMD,"Someone reported somewhere that there was a bug in or with the steam survey where RDNA 4 cards appeared as generic ""AMD Radeon Graphics"".  This could totally be AMDs fault too. They have always failed to work more closely with manufacturers and developers.",Negative
AMD,"DIY    Probably 60/40      Prebuilds   Probably 80/20      Total number worldwide (including servers, AI etc.)   Probably 75/25",Neutral
AMD,"Why would you ever get more? I have 16GB for 8 years now, no issues",Neutral
AMD,"Note: Vulkan is inaccurate, it doubles the count for EVERY GPU vs the overall, so you'll find the RTX 4060 at 8.4% instead of 4.2%. Scroll down to the DX12 GPUs, or more ideally even more to the DX10, 11 and 12 stats and that's accurate with the overall stats pretty well, just minor variance due to it being DX reporting. The RX 9070 there shows at 0.12%, a 0.02% gain over last month while the two before that were only 0.01% gains. You can even sort by GPU name so like RDNA3, 4 and RTX 50 series are all in line to compare.   RX 7800 XT and 7600 XT for some reason still have strong gains. There must be some market where they're on clearance and haven't sold out because they're long gone in Australia. Meanwhile the 9070 XT and 9060 XT are MIA for no reason which niche ass laptop Radeons like the 7600M XT get added for the first time with just 0.01% share? WHY STEAM, WHY ADD THAT YET NOT THE 9060 XT!????",Neutral
AMD,"HUB did say in their latest Q&A that their retailer stats are local, you know Australia.  And as an Australian I can tell you our Nvidia MSRPs are absolutely cooked as they just doubled USD MSRPs to make AUD MSRPs, it's stupid because AUD to USD was never 2:1 even with the 10% GST. AMD tends to be better with their price conversions, and even when Radeons were above MSRP it was less extreme than other markets and they went to MSRP the soonest. The RX 9070 is actually below MSRP now to keep up with the also below MSRP RTX 5070, and the 9070 XT is now a tad below MSRP even if barely, tho the 5070 Ti is also dipping below because turns out GeForce MSRPs were smoking something strong. Meanwhile 8GB 9060 XTs here are cheaper than 5060s and 16GB 9060 XTs are still cheaper than 8GB 5060 Tis  So it wouldn't shock me if Radeon sells better here in Australia vs the global market.",Negative
AMD,I generally think of Tim as being more reasonable so I was disappointed to hear him agreeing with that. I don't think even HUB realizes how tiny DIY is and let their local market (and Mindfactory) too heavily bias their views.,Negative
AMD,dont take hub serioulsy,Neutral
AMD,"HUB, Vex and GN are all rage bait grifters",Negative
AMD,They're going by publicly available data from retailers and I'm assuming only diy or may some pre-builts they've done.   Apparently they also there was an issue with the steak hardware survey as it didn't identify the 90 series Radeon cards.   I'm starting to think no really knows what's accurate,Negative
AMD,"change the filter to Linux only and you'll see ""SteamOS Holo""",Neutral
AMD,I thought it was showing as Arch Linux since they switched the distro it's based on. Not sure though.,Neutral
AMD,Steam Deck?,Neutral
AMD,You can't say you want competition then get upset when one side out-competes the other.,Negative
AMD,A lot of the machines on steam aren't playing brand new AAA games. Your uncle who only plays napoleon total war counts just as much as anyone else.,Negative
AMD,"Why sad when a HUGE, yes absolutely HUGE, amount of steam user are just there to play Dota 2, CS2, PUBG, Apex? Why would they need a 16GB card?",Negative
AMD,More people strawmanning this shit than the people.that actually post those arguments at this point,Negative
AMD,"If i was to judge from Reddit and X, i would believe that most people have AMD GPUs and are running Linux, getting a 3000%+ performance uplift vs a similarly priced NVIDIA + Windows system.",Neutral
AMD,"It only goes out on the 1st of the month to a subset of steam users, so unless you launch steam on the 1st of every month you're likely to miss it.",Neutral
AMD,They survey a small subset of users. It's opt-in and shows what will be sent to them before you send.  Or that's how it was when I last saw it. It has been a while.,Neutral
AMD,"I see those same comments every SHS thread and I wonder do those people go to highschool, because I'm pretty sure they teach basic statistic there",Negative
AMD,"rdna4 do sell very well, just that the 50 series also sells very well and in larger quantities",Positive
AMD,"The sales are good, but the overall quantities are not. AMD sell alot of RDNA4, but produced jackshit of them. I give this comparison for better understanding  AMD shipped 1.4M dGPUs in 6 months  Nvidia shipped 19.4M dGPUs in 6 months  If AMD sold 100% of  their shipment to gamers and Nvidia sold only 10%... Nvidia would still have sold more.",Negative
AMD,"Look at how many gpus shipped from each company.  Then, look at both of their last 2 financial quarter statements.  The numbers make sense fairly quickly.",Neutral
AMD,"prebuilt market is ALOT bigger than DIY and prebuilts easily sells alot of nvidia cards.      the cafe bug was fixed 8 years ago [https://www.reddit.com/r/pcgaming/comments/8gwagg/valve\_fixes\_steam\_hardware\_survey\_not\_to/](https://www.reddit.com/r/pcgaming/comments/8gwagg/valve_fixes_steam_hardware_survey_not_to/)     the way data is presented is fine. the survey is most likely random sampling. in statistics, you do not need to collect everyone’s data to get an accurate picture, a random sample is accurate with good confidence interval.  steam does not need to prompt everyone for data. collecting every single user data is called census, not statistics.  i hated my statistics class back in college, but it’s still useful skill to learn",Neutral
AMD,I think a good number of 1440p displays are from laptops.   You can see that 1600p having nearly 6%. 1600p is 16:10 which is a very common screen ratio used in laptops  16:9 is still the standard in laptops,Positive
AMD,"And if people listened to reddit, majority would be running 4k +120 FPS or some exotic ultrawide.",Neutral
AMD,"The shift obviously won't happen instantly, but with this generation we finally have fairly affordable, good cards with 16GB of VRAM. The 9060XT 16GB is a solid GPU and seems to be widely available, and the 5060 Ti 16GB is also there for a bit more, depending on local prices.   Last generation you only had the RX 7600XT and the ARC A770, both of which had a laundry list of caveats compared to a 4060 despite having twice the VRAM, while the 4060 Ti 16GB was overpriced for anything but professional work.",Positive
AMD,3060 12gb is quite popular so that would explain the popularity.  there is also 3080 12gb,Positive
AMD,"There are plenty of Nvidia cards with it  3060, 3080 12, 3080ti, 4070, 4070s, 4070ti, 4080L, 5070, 5070TiL  On amd its the 6700xt, 6800M and 7700xt only",Positive
AMD,"Rx 6800m, 4080m and 5070m ti",Neutral
AMD,"Anyone who buys a seperate GPU had their timeline shitting on the VRAM ""planned obsolescence"". I guess it makes sense",Neutral
AMD,People are angry at the companies *selling* 8GB GPUs. They aren't popular so much as the only option for most people.,Negative
AMD,Isn't 8GB most popular because the majority of GPUs are older generations? Would be nice to see a comparison of current cards that have varying VRAM configs like the 5060 Ti 8GB vs 16GB.,Neutral
AMD,"Because a lot of the discussion on Reddit is about high frame rates, high resolution, high graphical options, or all of the above. Which are all the things that, frankly, a lot of people don’t care about. It’s always going to be like this because subreddits are, by nature, enthusiast bubbles.   People tend to act as if turning down texture resolution, often an unnoticeable change, is a sin and that solves like 90% of the problems.",Negative
AMD,Memory configuration that has been easily obtainable for 9+ years  is more popular than memory configuration which has only been easily obtainable for 2  Whoa you blew my mind bro what insane insight you have,Positive
AMD,"> yet if you go to Reddit you will think it is the opposite and if you have 8GB gpu you either need to explain yourself  if you *buy* an 8 GB GPU in 2025, you need to explain yourself. If you own one since 2019 then it makes perfect sense. This particular corner of Reddit is full of enthusiasts who upgrade every year or two, but the vast majority upgrades every decade or so",Neutral
AMD,Games are designed around that. You wont find a game that wont run at 8 GB VRAM.,Neutral
AMD,Most games will run on 8gb of VRAM.,Neutral
AMD,The overwhelming majority of those 8gb card users are people playing f2p games like counter strike and marvel rivals. They basically never play actual games. They are for all intents and purposes an entirely separate group of gamers like mobile gamers who will never play traditional paid games and are pointless to try and target.,Negative
AMD,"Because games are targeting current gen consoles first, pc releases add ultra settings for more performant pc configurations, pc gamers just don't want to lower settings.",Neutral
AMD,"In the eyes of the AAA companies, a game is ""playable"" in 8gb cards if users use dlss/fsr and/or frame gen to it 60fps. its still falls in the technicallity of being able to run the game lol. its not wrong games can run with cluthes but its still scummy",Neutral
AMD,The games are still playable just turn down texture settings. The actual problem is when you get shit games like Forespoken and Last Of Us Part 1 where the textures you can fit in an 8GB card look like PS2 textures. Meanwhile you have a state of the art game like Doom The Dark Ages with RT GI that runs amazingly and looks gorgeous on an 8GB 4060.,Negative
AMD,It's more a case of the 4090 being really good value compared to the 4080 for at least half the last gen.,Neutral
AMD,I think it's a combination of price being prohibitive and the fact that people who don't find that price prohibitive see no real reason to upgrade from 4090.,Negative
AMD,4080 at the msrp 1200 made 4090 a more compelling value. not the case with 5090 when it’s $1000 more expensive than the 5080  5090 priced out a lot of people,Neutral
AMD,The 5090 is not unpopular if everyone had the money they’d be using it lol,Neutral
AMD,"I’d love to buy a 5090. However, I don’t want to spend that much on something that might melt and break…",Neutral
AMD,The 4090 had great bang for the buck. I don't see a need to upgrade this generation as the 5090 is not that much better.,Negative
AMD,I guess they would have been more worried if AMD atleast showed up in meaningful amounts.  The 9070 does show up if you filter it by windows vulkan devices at 0.22%,Neutral
AMD,"A good bit of that is RDNA 3 stock clearing. The 7800xt alone accounts for 0.37% in the last 6 months. The 7600xt showed up 3 months back and is now at 0.34%.   Also to note is that the overall percentage will include all GPUs, including the ones not shown in the overall list, which requires a minimum up 0.15%.  Plus Rx 9070/9070Xt/9070GRE accounts for 1.1% and 9060XT accounts for 0.37% of the linux userbase.  Under Vulkan, 9070 GRE is at 0.04% and 9070 is at 0.22%.  Under windows DX12 systems, 9070 GRE is at 0.02% and 9070 is at 0.12%  Under windows DX11 systems, 9070 GRE is at 0.02% 9070 is at 0.12%",Neutral
AMD,Sounds like you are one of those people.,Negative
AMD,"It is very much recorded:  Rx 9070/9070Xt/9070GRE accounts for 1.1% and 9060XT accounts for 0.37% of the linux userbase.  Under Vulkan, 9070 GRE is at 0.04% and 9070 is at 0.22%.  Under windows DX12 systems, 9070 GRE is at 0.02% and 9070 is at 0.12%  Under windows DX11 systems, 9070 GRE is at 0.02% 9070 is at 0.12%     Give it till the end of the year, and it should exceed the 0.15% barrier and be visible on the overall list.",Neutral
AMD,"I got surveyed last month, it picked up my AMD integrated graphics but not my 9070XT.",Neutral
AMD,"> Because earnings is literally all that matters, and AMD earned 20% of total gaming revenues.  ~~Except AMD includes consoles in their gaming revenues, while Nvidia does not. AMD's share of PC revenue is much less.~~",Neutral
AMD,"Yeah I was looking into pre builts, it's mostly Nvidia cards. And the crappy 8gb ones too. So people will continuously get as long as companies put them in their prebuilts or AMD offers them a more enticing alternative.",Negative
AMD,Never seen proof of that actually happening,Neutral
AMD,This was misinformation. There is no such bug.,Negative
AMD,That one could be due to people reading it wrong as well. They need to see whether it's the GPU being classified as Radeon graphics or the driver being recognised as radeon graphics?  You can see the card being reported correctly here: https://ibb.co/CpBZ7VCT,Neutral
AMD,**Source:** Yeah... Uh... Just trust me bro.,Neutral
AMD,You can easily tell that’s no where near true by looking at both company’s financial report,Negative
AMD,"Depending on what game(s) you are playing, you may go over 16GB. Escape from Tarkov comes to mind.",Neutral
AMD,This is something to keep in mind! Local markets are different. Something that is easily forgotten.,Neutral
AMD,They will never admit that because that would be admitting how irrelevant most of their content is for 90% of users.,Negative
AMD,Only publicly available retail data is mind factory which is not encompassing of global data. Only other thing they've brought up they've spoken to retailers but again have no idea what extent they have spoken to. Only Australian ones maybe.,Neutral
AMD,"The only retailer that shares sales data publicly is mind factory which has extremely good deals for AMD.  The issue seems to be weird because there are small gains for 9000 series cards if you filter them by certain categories (ex. Linux only or windows vulkan or windows dx12) but are too few to show up in the overall list (<0.15%).  Steam hardware surveys numbers do seem to decently line up with JPR's quarterly reports, so it's probably safe to trust it, with a certain margin of error.",Positive
AMD,"Holo is not made by Valve, it's a community repack.",Neutral
AMD,"If the steam deck was enough to bias it that much you'd expect the same for GPUs.  I suspect it's more ""Intel dominates prebuilts, the sort of person who uses prebuilts will never use Linux"". Probably the same with laptops, just to a lesser extent.",Neutral
AMD,Maybe a bit but I'd be shocked if 2.5ish percent of Steam users had a Steam Deck. Cause that'd be millions of Steam Decks.,Negative
AMD,"Once one side out-competes the other, there is no competition anymore. The point of it is have a _healthy_ mix.",Neutral
AMD,Also 1080p.,Neutral
AMD,"I guess I feel like people would play more graphically intense games if they weren't stuck with a 6Gb card. Sure, if those games are the only thing you are interested in playing, fine. But I wouldn't wanna be ""stuck"" with it, so to speak.  Edit/ I guess people don't want as powerful of a PC as possible. Which is a lie.",Neutral
AMD,Don't forget Valve releasing a super secret version of Proton that will fix all game compatibility issues at once!   (also YouTube),Positive
AMD,"yeah, learn statistics from examples showing 1:1 sales in many storefronts, before calling others idiot, idiot. Which is data everyone in those sales videos are referring to.   Also number of sales doesn't matter, what matters is ratio of sales compared to competition - meaning market gains.. So what you are trying to insinuate - it's that it's good old 1:9 sales lasting for generations, but examples shows 1:1 sales in storefronts like mind factory, some Australian stores having also good ratio but Steve not disclosing number due to source trust reasons..",Neutral
AMD,"no shit sherlock, but stats showing ratio even 1:1 with nvidia in stores like mindfactory - suggesting nvidia doesn't extremely well as you suggest here. That's the data points everyone is referring.  If you sell 1:9 just like last gen - that's typical sales, not good sales. But even with that ratio - they would be already in some steam surveys like it was the case with RDNA3 - which wasn't even selling all that well due to lackluster upscaling at that time and steep pricing.",Negative
AMD,"I see you understand jack shit with bullshit numbers.  Basically everyone is pointing to sales at 1:1 or 1:2 ratio in some store fronts. That is what everyone referring to, and you give old classic 1:9 numbers you pulled out of your arse. And yet that doesn't reflect in any surveys.   It's still didn't pass any RDNA 3 GPU, despite RDNA 3 selling terribly and offering bad value at the time with garbage upscaling (FSR3) and steep pricing.",Negative
AMD,"You don't have such data, and financial reports don't separately give numbers on consumer GPU revenue, I see you saw none of those reports like ever, if you talk such nonsense",Negative
AMD,"It is random sampling but the population is too low given the size of steam numbers for a statistically meaningful comparison given how small percentages we deal with here. When your confidence interval is larger than the entire percentage of a card, its not very precise results.",Negative
AMD,Oh yep that makes A lot of sense. I was wondering where you could find 16:10 monitors.,Positive
AMD,"1600p tells you only number of vertical pixels not aspect ratio, 1600p is commonly 16:10 but it doesn't guarantee it.   When describing displays both Number'p' and aspect ratio should be given or use the recognised industry terms of HD, FHD, QHD, UWQHD etc. So UXGA in this case   https://upload.wikimedia.org/wikipedia/commons/thumb/0/0c/Vector_Video_Standards8.svg/1920px-Vector_Video_Standards8.svg.png  https://en.wikipedia.org/wiki/List_of_computer_display_standards  Terms like 4k, 2k, 2.5k aren't part of the screen specification language for either TV's or monitors.  r/hardware not r/casuals",Neutral
AMD,"Yeah, in fact if you actually check the page you can literally see that 8GB lost share while 16GB is gaining share, 12GB too I think. Same with 1080p vs 1440p, 1440p gained along with 4K which is closing in to 5% soon while 1080p bled a little. Sure 8GB and 1080p are still dominant, but the monthly swings do tell that when people upgrade it is increasingly towards these higher specs than before. It's a process that takes years but 1080p will keep declining slowly as 1440p and 4K gain, just use the wayback machine to even two years ago and it'll give a lot of context. Back then Intel had 67% of CPUs and AMD 33%, now it's 42% AMD and 58% Intel.",Neutral
AMD,they are angry because the youtube reviewers tell them to be angry so they can get views.  They really want ppl to believe that 8gb is not enough for 1080 gaming.,Negative
AMD,"Most sane people are angry at the companies, but according to many on reddit your PC belongs in a museum unless it can run recent AAAs at 2k Ultra 120fps",Negative
AMD,I really like how this dude didn't respond to your comment cause it dismantles his strawman.,Negative
AMD,"Unfortunately steam doesn't give us sku breakdowns by memory config. But you can guesstimate, 5070 and 5070Ti have a larger share than the 5060 and 5060Ti. I think its a safe bet that at least half the 5060Ti are 16GB variants. The fastest growing AMD GPU on the stats is the 16GB 7800XT.  12GB+ is almost definitely the majority for new cards if not overwhelmingly so. 16GB being the most common.",Neutral
AMD,"The ironic thing about that some of these people also argues that Ray Tracing / Path Tracing is useless because it brings down the framerate of their game, while unironically playing at max settings and not caring about tweaking the graphics settings to optimized settings.",Negative
AMD,"> if you buy an 8 GB GPU in 2025, you need to explain yourself.  It's not that, either. People will buy what they can afford.  AMD and Nvidia should not still be selling 8GB GPUs for the prices they are.",Negative
AMD,">Why the hell would you buy an 8gb vram when it's hitting the limit on even 1080p causing massive framedrops.  Because people have many different constraints like portability requirements, budget and space.      Additionally not cranking everything to Ultra is fine and still a good experience",Negative
AMD,You can definitly find games that don't run well on 8 GB VRAM now.,Negative
AMD,I don't know anyone playing f2p games on gaming handhelds and they all have 8gb or less.,Neutral
AMD,"I'm taking that as a signed check for a duopoly. AMD had the chance to give the behemoth a run for their money, but they sat pretty on price-matching, and didn't undercut nVidia with a proper low-end card that'd blast the 5060s out of the water.",Negative
AMD,Also integrated graphics are counted in the survey,Neutral
AMD,"> A good bit of that is RDNA 3 stock clearing. The 7800xt alone accounts for 0.37% in the last 6 months.  This is just example of survey being wrong, nothing to do with stock clearing. Both 7800XT and 7600XT gained market share at a rate that mirrors numbers for 50 series cards, despite 7800XT being sold for 17 months and 7600XT for 15 months prior those cards showed up in survey.",Neutral
AMD,"This data is even more confusing if anything.  Because you're right they ARE there (I hadn't checked that section so good catch!) but they just quote the 9070 and the 9070 GRE, not the 9070XT? And while it might feel nice to assume that the 9070 is just the combined version of the two cards, it not including the GRE makes that unlikely, as do the actual usage numbers.  The 9070 GRE isn't even available outside of China I'm pretty sure, a market that *heavily* favours Nvidia, but it has still somehow managed to get to 1/5th the usage of the 9070 and 9070XT *combined*?  It's not impossible, but that would be insane if true.   I'd find it far more believable that it's not recording XT usage properly than to believe the GRE has sold that much with the other two varients combined selling that little lol.",Negative
AMD,That's not true. Switch SOCs are counted in Nvidia's gaming revenue; it's stated in their financials.,Neutral
AMD,You can look it up on reddit. People share screen shots.,Neutral
AMD,The GPU is listed wrong in those reports.,Negative
AMD,Same as Steam and JPR,Neutral
AMD,"Also, if you look at the real world by [interviewing](https://www.youtube.com/watch?v=wbie3MU5igU) average gamers / streamers at gaming convention...  **Spoiler alert:** It is vastly dominated by Nvidia just like what Steam Hardware Survey shows.",Neutral
AMD,70/30   amd dgpu-(40-50%)semi   nvidia dgpu - (30%)semi,Neutral
AMD,That channel is just an echo chamber anyways.,Negative
AMD,"That's true, amazon and Newegg showed good sales of Radeon. Maybe it just takes more time to move the needle.   I wonder how it all works, and is hammering Intel in CPU and its still around 40 percent.",Neutral
AMD,"Steve and Tim from Hub were confused how John petty research came to their conclusions on numbers, other outlets come to different conclusions too, probably including and or not including certain things.",Neutral
AMD,"Holo is Valve's codename for SteamOS 3.  You are thinking of [HoloISO](https://github.com/holoiso-eol/holoiso) which was a project that ""attempts to bring the Steam Deck's **SteamOS Holo** redistribution into a generic, installable format""",Neutral
AMD,You realize these companies release their financials every 3 months. You dont have to hunt secondhand youtube videos of greasy gamers talking to third party sources at niche Australian e-commerce stores to prop up your conspiracy,Neutral
AMD,prebuilts are a thing you know and they are alot bigger than DIY. mindfactory isn’t even representative of the entire world sales,Neutral
AMD,"But they do... you just seem determined not to read them yourself and to just throw your temper tantrum.  ""Second-quarter Gaming revenue was $4.3 billion, up 14% from the previous quarter and up 49% from a year ago.""  - From the Gaming and AI PC segment of nvidias q2 financial report:https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-second-quarter-fiscal-2026  ""Gaming revenue was $1.1 billion, up 73% year-over-year driven by an increase in semi-custom revenue and strong AMD Radeon™ GPU demand.""  - From the Client and Gaming segment revenue:https://ir.amd.com/news-events/press-releases/detail/1257/amd-reports-second-quarter-2025-financial-results  Keep in mind that AMD makes the gpus for playstation and Xbox, and yeah, it makes sense that NVIDIA sells a lot more pc graphics cards.  I am not 100% sure their cpus are in that section, i believe they have them under the client, but either way, combine that with the Jon Peddie research that I assume you're talking about when saying the blow out in sales is nonsense, and you come to a reasonable conclusion that NVIDIA ships more gpus, and therefore sells more gpus.",Neutral
AMD,i would love to clarify but i guess someone already did it for [you](https://www.reddit.com/r/hardware/s/1dbJq5QS5A),Neutral
AMD,"16:10 Monitors were everywhere in the early 2010s, but luckily there is a revival, strangely only in the laptop space!      It is just the best format, I still love my Iiyama 24"" WUXGA, although it not the primary anymore.",Positive
AMD,"I was going to correct you, then I realized ultrawide 3840x1600 displays are a thing. Touché.",Neutral
AMD,"Because ""enough"" for teletubers is different than average user. If people can optimize med-high-ultra is good for them  But for a lot of online people if you cant run high-ultra/rt is not good enoug   ""Nvidia marketed those cards as being capable of rt, look at x game it cant even do it, thats anticonsumer""",Neutral
AMD,They can back that up with sources. What about your claim? You plan on saying they only play esports? Gee i wonder why. Really.,Negative
AMD,"I think the issue is less with simply having older or slower hardware, it's more that people who do often have unrealistic expectations for said hardware. You see a lot of people whining that developers are ""lazy"" and games are ""unoptimized"" because their 8GB mid-range GPU from five years ago can't run recent AAAs at 2k Ultra 120fps.",Negative
AMD,Apparently we should all just play pong at 4 million fps. Everything else like having an actual game just makes the experience worse.,Negative
AMD,"Don't forget about extensive mod library for their games, which are made by amateurs without any regard for performance, hogging up massive amount of resources, crushing framerates yet subpar performance is worn like a badge of honour. But god forbid developers, build a game for current gen console and adding ultra settings for pc release.",Negative
AMD,"You can turn the settings down and they run fine. Expecting the minimum requirements to run ultra settings fine on a budget card is just foolish.  8Gb VRAM = Medium 1080p @ 30fps and that is actually just fine and ""Games designed around that""  Reddit really struggles with the idea you should be playing on lower settings if you have minimum requirement hardware, in the real world people don't have this problem.",Negative
AMD,on medium settings?,Neutral
AMD,"feel bad for you, you probably got brainwashed by HWU so I don't blame you.",Negative
AMD,only frame gen increase vram. dlss/fsr is meant to reduce vram usage,Neutral
AMD,DLSS decrease VRAM usage because you need less VRAM for the render itself even if DLSS uses some VRAM.,Neutral
AMD,So are laptops and pre-builts which are dominated by Nvidia. DIY market is insignificant in comparison.,Neutral
AMD,"We already know that survey has problems recording certain AMD cards for last two generations, people are just grasping for straws trying to find magical reasons to explain these inconsistencies.  7900XT is great example of that. It does not show up anywhere in survey, even in most diluted vulkan section where plenty of cards have 0.00%.  Also, numbers for different cards are not combined. 7800XT had the same problem but Valve, probably manually, fixed it for April survey, and it has rapidly gained share after not showing up for 17 months, while 7700XT numbers stayed the same.  Having said all that, the only card from 9000 series that could have outsold 6750 GRE 10gb is 9070XT, since 9070 and 9070 GRE numbers are accurate while 9060XT is too new.",Negative
AMD,"Looking back at their financials, I think you're correct.",Neutral
AMD,Its listed correctly under DIrectX card,Neutral
AMD,"JPRs recent reports put Nvidia at 92% and 94% with AMD at 8% and 4%.  Steam doesn't have the RX 9000 cards on the overall list (yet), but do have them in very small percentages when sorted by Linux or Win Vulkan or Win DX12.  Your ratio spits might align with the current install base of steam (that includes everything from dx8 to dx12U and igpu/APUs), but doesn't align with new graphic card shipments/sales.",Neutral
AMD,"They literally have an actual website or paper explaining their research methodology and sources, but yeah... sure thing bud.",Neutral
AMD,I believe amd count console soc as part of gaming.  While nvidia does not include switch in gaming and count as oem and other.  So you can just look at the number objectively.  This is a gaming discussion so there’s no point to include data center in this,Neutral
AMD,neither amazon nor newegg shows any data on how many sales of GPUs were done.,Neutral
AMD,I would definitely trust HUB less than an actual analyst firm.,Negative
AMD,"just think about, what is ratio of prebuilt vs upgrades + DIY builds? People buy more prebuilts vs DIY, but even when you include upgrades, that shift numbers by quite a bit. You thing everyone who bought prebuild with some X3D CPU good for several GPU gens is buying whole prebuilt again instead of upgrading. Damn 15 year old kids are upgrading GPUs by themselves.",Neutral
AMD,"Exactly - gaming segment includes console chip revenue, thus it's telling very little about what we're talking here about. There's also Radeon Pro which goes fuck knows where",Negative
AMD,Lmao dude got told how things work and he’s still mad about it,Negative
AMD,Apple might have a hand in influencing oems 16:10 is ok since they don't use 16:9 in macbooks. 2560-by-1664. On their 13 inch.  I really wish apple would push for 720hz refresh rates on atleast 1 display. It would change that convo completely.,Neutral
AMD,You'll be surprised when you realize the top 400 games on steam still run well on 8gb. Thats why people arent buying 8gb dead or not good enough yet. Especially with things like dlss/fg/mfg  [https://store.steampowered.com/search/?filter=topsellers](https://store.steampowered.com/search/?filter=topsellers),Negative
AMD,"While there are people like that, I agree, you can't deny that games are getting more unoptimized",Negative
AMD,"Second hand purchasing has its own problems and the prices of the good second hand cards are still high. High VRAM cards are a new thing so there isn't actually much choice 12GB or higher in the second hand market and sellers know this.  ""Buy second hand"" isn't a useful comment when discussing price/performance as everyone has their own secondhand market and own risk appetite to deal with.",Negative
AMD,> Expecting the minimum requirements to run fine on a budget card is just foolish.   What does this mean?,Negative
AMD,Yes. Try to run Doom 2025 or Monster Hunter 2025 on medium 1080p and target 60fps,Neutral
AMD,With 94% of the market they dominate every segment even DIY,Positive
AMD,"Good catch on the 7900 XT lol, I didn't even notice it was missing but it totally is. And there MIGHT have been an argument for that when it came out, but that card became a sleeper pick after the price plumeted so you'd expect to see at least 0.01 percent lol, especially with the XTX's numbers.",Positive
AMD,In this one it is. I'm taking about other reports. You can look up those screen shots and it lists direct x card as AMD Radeon graphics (TM) or something.,Neutral
AMD,JPR includes shipments of gpu for servers. Dont see why people will use that as some form of gaming marketshare analysis,Negative
AMD,Switch is counted in Nvidia's gaming revenue,Neutral
AMD,"big enough that my own DIY local retailer opened a prebuilt business and sold 10 times more volume than their DIY business. prebuilt is a massive business and it will always have larger volume of sales. im in a country where prebuilts are cheaper than DIY due to economy of scale, and it has caused some DIY stores to shut down. The smarter DIY business diversified by starting prebuilt business. you are overestimating the number of people who knows pc parts and know how to build it.",Positive
AMD,"True, that tells us we lost some fine detail accuracy but still gives us a large picture.  AMD having console gpus included means that not even all of their gaming revenue this quarter was on dedicated pc graphics cards.  That widens the gap instead of shortening it.  I believe its simple that AMD didn't expect such a positive response to RDNA4, and thus, months before launch, they ordered and sent out their normal smaller supply and have not ramped up enough to match the output NVIDIA has.",Neutral
AMD,I do have to be fair. It's not like a lot of that fault isn't on the games industry. Either way that logic doesn't help sell/make better games. Well people can always just buy a playstation if it doesn't run well. Those games at the top are pretty light games. Though if no heavy games are made because it costs too much. Then it doesn't matter. So once again the solution is to buy a ps5 for heavier games like light dying light. I guess Thats simpler.  There also the fact that heavier games aren't going to be at the top as single players stories get 90% of their sales on launch. That is why dying light is up there near the top 10. So it's difficult to keep gauge. Also the second reason i suggest a ps5. People aren't going to play dying light when the rtx 6060 18gb comes out. Etc etc.,Negative
AMD,Medium is 30fps.,Neutral
AMD,"There is no question Nvidia has a GPU monopoly. But the issue is that people here infer RDNA4 sells worse than RDNA3 based on unreliable information from JPR and Sream. When there are pretty good indicators that RDNA4 sells better than RDNA3. For one AMD's own earnings results.  I have no idea why anyone would wish for RDNA4 to be a flop, seeing how it's the only real competition in this monopoly but here we are. There is no question RDNA4 is more successful than RDNA3. And it's not a flop.",Negative
AMD,JPR specifies AIB Gpu shipments specifically for desktop GPUs. They have a separate report for Datacenter GPUs,Neutral
AMD,"that's because most people buy parts online to a point most local stores stopped selling parts altogether because low choice is bad and having wide range of choice will make them never sell the stock. So your local retailer is poor reference point to anything. I'm DIY since like 15 years and I didn't buy a thing in physical store - simply because online offers better prices and wider offer of models, brands, etc for each given part.",Negative
AMD,"""heavier games like light dying light?"" It looks like a ps4 game and runs like a ps4 game. a 5060 can get 200fps when all the features are used and mods are used which looks & perf better than consoles.  Ue5 are the heavier titles but those are mostly broken due to the terrible stutters",Neutral
AMD,"> There is no question RDNA4 is more successful than RDNA3. And it's not a flop.  Architecturally, it has me excited for UDNA, but RDNA4 boost clock still not breaking 3ghz (despite the gains from 3!) has me worried, since it *looks* like the design goal was somewhere north of 3.",Positive
AMD,"RDNA4 sells better than RDNA3 when it is in stock close to MSRP. Which wasn't the case for most of the time since release till now.  AMD earning reports are intentionally obscured, don't look into them to infer specific numbers.",Neutral
AMD,Record low market share is a flop. You simply can't sugar coat it,Negative
AMD,Thats why they included ponte server gpu? How are they even getting those numbers? They should show us the numbers exactly. I will verify myself,Neutral
AMD,"i mean our local retailer also has online stores lol. a lot of them have listings on popular commerce sites. it’s the same price as amazon due to import taxes.   also it doesn’t help that they price gpus by performance so it’s normal to see a 9070xt cost the same as a 5070ti. a 9070xt can be cost the same or more expensive than a 5070ti. A 5060Ti 16gb is cheaper than a 9060XT 16gb here   europe pricing for AMD may be better, but it’s not the case for every country, mine included",Neutral
AMD,Well thats my bad for assuming dl the beast was the same graphics as dying light 2 without checking. Unless you thought i meant dying light 1. I should note i don't think it has issues. Just that it was the heavier game i could immediately recognize of on the top of sellers. Turns out it probably has none since it's somewhat just dying light 2.,Negative
AMD,"I'm not here to convince you. RDNA4 isn't a flop. If you think it is, I'm not surprised.",Negative
AMD,"Jesus, only the Notebookcheck article comes close to being a fair comparison. The other two are comparing Qualcomm's highest tier SKU inside a 16"" laptop to the M4 non-pro and AMD's mid-high tier skus. They wont even tell us what the TDP  of the X2 Elite is.   I'm all for competition, and I want to see Qualcomm succeed here, but this feels scummy.",Negative
AMD,reduce the overkill npu.   use the transistor budget for more cpu caches or gpu cores.   ms recall will be disabled by most users anyway.,Neutral
AMD,"The improvements on CPU performance doesn't matter. Lunar Lake is already close in efficiency without massive problems with compatibility and locked bootloader.  The GPU on the X1 is very underwhelming (not to mention the non existing drivers) and this is going to compete with Arc Xe3 which is going for new architecture and 50% increase in GPU cores.  Unless Qualcomm can release it way soonor than competition at much much lower prices, this thing would be DOA.",Negative
AMD,Kinda tired of hearing about ARM on Windows. It was hyped nonstop in the media and here last time only for it to be a failure.,Negative
AMD,These product names are getting ridiculous. I'll wait for the X2 Elite Extreme Pro Max AI++,Negative
AMD,"They did similar with v1, so I guess we’ll just be inundated with comparisons like this until reality hits with release and less-official benchmarks.",Negative
AMD,"They did this last time. All of these reviews are garbage until the independent reviews come out post release.  And to note, the independent reviews last time were a lot weaker than the paid pre-release reviews. So expect the same this time.",Negative
AMD,"Austin Evan's video says that the laptop all the benchmarks were ran on ""supports a thermal envelope of up to 120 watts""   https://youtu.be/sVS0rAXpyfQ?t=4m53s",Neutral
AMD,The X2 Elite is Strix Point TDP basically,Neutral
AMD,Even the notebookcheck one is still comparing to the M4 Pro/Max and not the M5s...,Neutral
AMD,"Yes, totally agree. However, if the NPU is what draws people to ARM, still a win, and if they are ahead on that, they can focus on other stuff for 3rd and 4th gen, gotta take what you can get.",Positive
AMD,I love it how 90% of the advert for this shit is ai like please tell me why tf I would care so much about ai 😂,Negative
AMD,"I agree. Adreno X1 doesn't even fully support DX12. With Nvidia rumoured to be entering the arm market, and Intel getting Nvidia chiplets. Qualcomm would be screwed if they don't massively improve Adreno on PC.",Negative
AMD,"Lunar Lake CPU wise is weaker than a Phone.  In Single and Multi core   Not everything is about the GPU unless you only care about gaming.   Either way, Lunar Lake didn't close the gap in efficency, their PPW was like half of Qualcomms with a node ahead of them   They had crazy good idle, true but performance on battery was not Apple like, it throttled",Negative
AMD,"Lunar lake is one-of experiment, that intel gave it all to make: RAM on package with cutting edge TSMC node.  I bet Panther Lake won't reach the efficiency of lunar lake because they will move back to traditional package.",Neutral
AMD,"No, it's a lot more",Neutral
AMD,I can forgive them for that because all we have is a leaked M5 iPad geekbench benchmark. The M5 isn’t officially released,Neutral
AMD,Nvidia's release is getting delayed to 2026 H2 according to Charlie,Neutral
AMD,They have to get the software ready WoA as well as GPU no studio will gamble with QCOM. QCOM don't even have experience with GPU Drivers.,Negative
AMD,"I'd have to respectably disagree. GPU compute still plays an important role, especially in parallel tasks. HW acceleration is more and more being critical in applications consumers use today. Intel's Xe architecture has shown to consistently improve, that said I won't count X2E out until both are reviewed in the coming months. Heck, I'm more excited for M5, their GPU arch has improved, both in compute (FP16 doubled) and RT, Blender will be interesting to see vs mobile Blackwell. I weigh CPU and GPU perf improvements equally and Apple has impressed in both fronts.",Positive
AMD,Ok bot but yea apple roasted it,Neutral
AMD,"In every test they showed, it consumed a lot less at max performance than Strix Point     The X1 Elite top end SKU used a lot more true, but it didn't make it out to real SKUs",Neutral
AMD,Link? Sad...,Negative
AMD,"QCOM has experience with mobile drivers, just not Desktop.   The new GPU however could work out for them. Full DX12 support, new architecture",Positive
AMD,I think they know the software don't improve that much so they just will try to push partners and brute force emulation performance. Higher ST should help,Negative
AMD,Yeah the X2 Elite GPU seems like M5 level only   This GPU can reach 80% of the M4 Pro,Positive
AMD,You wish i was a bot. CPUs are just my hobby and what i specialized in uni,Neutral
AMD,"Nah, I actually got a high bin sample and in ambient conditions it eats a lot but way better than last gen",Positive
AMD,unfortunately you have to pay 100$ to read the article. it's subscribers only,Negative
AMD,Their mobile drivers are ass though,Negative
AMD,Studio has to support them as well how will Qualcomm convince game makers and other vendors,Neutral
AMD,Will emulation work for AVX2 cause even if they brute force AMD/Intel will improve design much further.,Positive
AMD,Will emulation work for AVX2 cause even if they brute force AMD/Intel will improve design much further.,Positive
AMD,Ok.,Neutral
AMD,Vulkan specifically,Neutral
AMD,If i recall correctly Prism added support for AVX2 so it should. The problem with AMD / Intel is more how they can improve perf/watt and perf on battery. Prism is pretty good but some niche software is still not supported. And the Adreno GPU still needs work on driver side. I have a Zenbook A14 with a lower base X plus and I have no issues for my use,Neutral
AMD,"AMD/Intel don't have any significant CPU improvements in the pipeline, Zen 6 is coming H4 2026 but it's not projected (leaks) to beat the X2 Elite in ST. Either way, when laptops launch for Zen 6, X3 is coming out too.",Neutral
AMD,yeah but even when they were using OpenGL ES they were ass.,Negative
AMD,I mean that is up to OEM not Intel/AMD OEMs will pump watts unnecessary into any Intel/AMD Laptop,Neutral
AMD,you mean Q4? H4 is... dubious and wouldn't fit AMDs execution on roadmaps. maybe Intels.,Negative
AMD,"Not really tbf. It's the nature of X86. If they don't pump enough wattage the chips will be slow and that doesn't look good when people are shopping for laptops. That's what Arm is better at, better performance per watt, better performance at lower wattage and on battery. And the numbers look better because it's easier to play catch up but even Arm will hit the wall. It started already and the main improvements are coming for the nodes",Neutral
AMD,That's not true ISA doesn't have anything to do with it look at LNL. The biggest factor are the power delivery which for ARM chips uses PMIC which is expensive but good at low power levels and traditional chips uses VRM.,Negative
AMD,I hear what you're saying but Intel still fails to beat it in perf/watt and the margin is just gonna be bigger,Negative
AMD,Any price above \~225-250€ is not worth it for the x3d am4 chips unless your market has severely messed up am5 prices for some reason ig.,Negative
AMD,"No, it is not worth that price. you should just upgrade to AM5 with a bit more.  I feel like at most the 5800x3d should be around is 300 USD. that's when I'll probably change to it. Or get am6 by that time.",Negative
AMD,5700x3d or AM5,Neutral
AMD,"Well if you're not looking to do a platform upgrade to am5, the 5800x3d is the best gaming cpu you can get that would fit your current platform  As for the price, get a used one, i bought one used over a year ago as a part of a system, and after selling all the parts i didn't need i broke even while keeping the cpu, ram, 1tb ssd, and psu",Positive
AMD,"Depends on the cost of a 7500f / 7600 + low end B650 or B850 board + 32gb of DDR5 6000mhz RAM, even a single stick of 16gb is acceptable since it runs in dual channel",Neutral
AMD,400€ is budget am5 upgrade lol,Neutral
AMD,"not a good price. you can find a 5800XT which is better value. Or go with AM5, tho its not the best time due to high DDR5 prices",Negative
AMD,"No, go for 7800x3d",Neutral
AMD,"If you can spend 500 on a cpu you can probably buy a b650 motherboard, 32gb ddr5 ram and a 7800x3d for 600±",Neutral
AMD,"Sell your old parts, get a B650/B650E and a 7600 or a 7500F (whichever is cheaper). Get some 6000mhz cl30 DDR5 32gb Hynix and you’re set.",Neutral
AMD,"5800X3D stopped being manufactured some time ago and was replaced by 5700X3D, which is also slowly being phased out. So the price that you are seeing is probably from some old stock, that is waiting for someone not aware of the market.  You can get a new 9800X3D for that price. So I would rather go for AM5 upgrade and reclaim some costs by selling your old parts. B650 motherboards are not that expensive and should work more than fine with 9800X3D. Or you can go for cheaper 7800X3D as a compromise to lower cost.  But first, ask yourself if you need your PC to edit fast or game fast. If you need productivity and your software is CPU bound, then even new 5950X are cheap now. It will improve your gaming performance and it will crush 3700X in productivity. It will be slower than any X3D chip in gaming. It will also be a drop-in upgrade.",Neutral
AMD,moving to AM5 should be more cost efficient. 7500f + 32gb 6000mhc cl30 + whatever b850 board that has the needed features for you.,Neutral
AMD,"400-500 euros is too much. You could probably go am5 for not too much more - but i wouldn't recommend that. If you can find a used 5800x3d or 5700x3d for example, it would be much more reasonable",Negative
AMD,Thanks for all the tips. Does anyone have any real comparisons in games?  How big is the difference here? Regardless of the price.,Positive
AMD,"Depending on your market, with 400-500eu you can get decent AM5 mobo+cpu+ddr5.",Neutral
AMD,"5800X3D won't perform much better in Photo and Video Editing so it's not worth that price only for gaming.  If you afford it upgrade to AM5 with a 9800X3D, then you have the best Gaming CPU for CS2 and it is also much better in application performance compared to 5800X3D/7800X3D. It will compliment your next GPU Upgrade in the future for sure.",Neutral
AMD,"get a used 5700x3d with reasonable price, if you can't then AM5 is the only option.",Neutral
AMD,"You've convinced me, I'll probably go for the 7800x3d. Any tips for motherboard and RAM? I currently have an AIO cooling system and don't know if it will fit on the new socket.  Habe alles in weiß gehalten.",Neutral
AMD,"I did upgrade on April this year (3700x -> 5700x3d) and I was surprised how many games, such as FS2020, gained huge fps increases. I have 3080TI. Definitely worth it, I payed ~250€.",Positive
AMD,"Not really worth it imo. I’d save up for an upgrade to a 7800X3D, or a 9800X3D if you can afford it.   You’ll have to upgrade your motherboard and RAM as well, it came up to roughly $500 USD in my area, especially if you know where to shop.",Negative
AMD,Back when you could get it cheap and AM5 wasn't really an option yes.  Now absolutely not,Neutral
AMD,"Only at $200 or less. If it costs anything more you're better off selling your cpu, mobo and ram to hop on AM5",Neutral
AMD,"Look for 5700x, still an upgrade but 3x cheaper.",Neutral
AMD,"Went from 3700x to 5700x3d,  for me the upgrade was not worth the 180€ i payed for that CPU.  All games that ran fine before run fine afterwards,  the games that struggled, struggled with a few frames more (aka 40fps to 50 fps)     Personaly ifound that high FPS is less important to me than conistency,  e.G.  i play better on steady 60Fps and steady 31ms Ping,  than on 140 to 300 Fps and 9 to 20 ping.  Found the fps part out by accident when i globaly locked my fps to 60     TLDR:  5000x3d ain't wortht he asking price currently, go full upgrade or wait  May give capped fps (or rather AMD chill) for more consitend frames a go",Negative
AMD,"Check for the price of 5950x on Amazon, of you are prepared to pay around 250€, it is a monster CPU.",Neutral
AMD,"It might be a good choice if you're on a budget. However, you can probably go for AM5 with €500 depending on the prices in your place.",Positive
AMD,Can't you upgrade to am5 for 500usd? Like a mobo and ram and maybe get a 7500f that is similar in performance to 5700x3d. In the country I'm living the prices are dictated by popularity and AM4 is more popular here so it costs the same as AM5 chips and mobos. AM5 would definitely give you more flexibility and you could look out for offers on a bigger variety of chips.,Neutral
AMD,"Last good time to get an AM4 x3d chip was early this year on aliexpress for about €160, they've stopped production so prices increased. It's better to invest in AM5 now.",Neutral
AMD,Not for that price hell no. For $100. MAYBE $200,Negative
AMD,"For you, a 5900x for $150-180 (used in the US anyway) might be an option. You'll get a big boost for productivity and a decent boost in gaming, without breaking the bank. This would probably hold you over until AM6",Positive
AMD,"yes and no  the cpu is worth upgrading to but its stock is pretty empty and for the price youd pay for a 5800x3D, 5700x3D youre better off just going AM5 now",Negative
AMD,"unless you can find 5700x3d or 5800x3d for under 200$ USD, don't buy",Negative
AMD,"For that money, likely not. I did the same upgrade when 7000 series came out, but I got my x3d chip for ~$200 after selling my 3700X  Is x3d worth it? Absolutely. For you considering the cost versus a whole new motherboard platform? Eehhhh, no. Go 9000 series if you want a performance boost. 9700X is similar to the 5800x3d, any of the new x3d chips will be a further performance boost, game/resolution/GPU dependant",Neutral
AMD,"A 5800X3D would be a fantastic upgrade to your system that would keep it relevant for a good while, but that is too expensive. Maybe you can find a 5700X3D instead (<$300)?",Positive
AMD,Are you only running 1 stick of RAM? Two is faster.,Neutral
AMD,"Yup, nope, after reading the full text do am5 if you're going to  spend that much.",Negative
AMD,"I went from a 3900X to a 5800X3D. The multicore performance is not much lower on the 5800X3D, but the single core performance is better and for gaming it made big, in some cases huge difference.",Positive
AMD,"I upgraded from 3700x to 5800x3d ...2 years ago for around €200 second hand. Absolutely worth the upgrade then, but I would never pay €400!",Positive
AMD,No  5700x or AM5.,Neutral
AMD,"To give you my 5 cents, I am in a similar position, gaming in 2k with 3080 tuf, tomahawk 450 2.0, 32gig of 3600 mhz corsair and ryzen 5 3600x. Was thinking of doing the same as you, going for 5700x3d or 5800x3d but had a second thought and I think I will wait for how long this configuration will hold on, because it is handling everything with no issues. Maybe I will reach am6 haha. As for you, I woukd offer to go for better graphics instead of the processor.",Neutral
AMD,"Nope, for that price is cheaper getting a CPU+MOBO bundle (Intel or AMD) and get better performance. Also, 5000X3D chips are bad for productivity. If you want a good GPU for 4K edition, any Intel chip with an iGPU will do it just fine.",Negative
AMD,"I'm in almost the same boat as you (ryzen 3800x) and ended up jumping on the 5900xt (from my understanding is its just a underclocked 5950x) for 250 usd last week on Amazon. I also dabble in productivity with more large data analysis (SPSS, SAS) and lots of Civ so I figured it would be a decent swap and my plan is to just do a full new build in about 2 years and use my current setup hooked directly up to my living room tv as a pseudo console. It's pretty annoying to figure out the upgrade path right now since so many of the posts regarding upgrades from our chips are from before the discontinued production of the 5800x3d and 5700x3d.",Neutral
AMD,Nahh i would just go am 5 or save for it.  Am4 5800x3ds aint worth for that price.,Negative
AMD,It's $300 - 400 for a 9600x mobo ram bundle rn,Neutral
AMD,even the 5700X3Ds are overpriced now because of the end of production,Negative
AMD,I just can't see how 5700x3d is worth twice the price of a 5600 or 5800x3d is worth thrice. AM5 is the way to go,Negative
AMD,Rn with discounts it's like $350,Neutral
AMD,Literally this. I cant belive how people still give advices like that when its pretty simple - there is a reason why something is cheap.,Negative
AMD,"Primarily for gaming, I currently have cs2 110-160 fps but more like 130, with everything set to low. (Yes, I know graphics don't matter here, but it's just not enough.)  Sooner or later, I'll get a Mac and use it to make videos and photos. Currently, that's possible, but if the timeline is long and there's a lot of colour grading, forget it.  With BF6 Beta, I had everything on low 80-120 fps...",Neutral
AMD,"Difference is huge, you will have very good FPS especially in CS2. I don't know where are you from but in Europe you can buy 7800x3d + b650 + 32gb ddr5 6000mhz for 500 euro. If we compare it to your 5800x3d price of 400-500 euro you should really go to AM5 platform.",Positive
AMD,B850 + 32GB,Neutral
AMD,"MSI B850M Gaming Plus WiFi6E  MSI B850 Gaming Plus WiFi6E  GigaByte B650E Aorus Elite X AX Ice  All in white, if you want white again you don't have alot of choices and they do cost more than black color.  I would go for any 2x16Gb 6000mhz CL30 RAM. You can find it for 100 euros, last few weeks it was Available for about 85 euro.  Check is your AIO compatible and maybe you will need a AM5 mount from your AIO manufacturer.",Neutral
AMD,I bought 9800X3D and 64GB 🤝,Positive
AMD,Old System 2x 16GB and new 2x 32GB,Neutral
AMD,"This is the way. OP will get comparable performance for less money than the 5800X3D will cost them, and they’ll have an upgrade path for future generation(s) besides.",Positive
AMD,2nd hand is good option. CPUs are safe to buy 2nd hand.,Positive
AMD,"So you’re paying less, for similar 5800x3d performance, and to get on a new platform",Neutral
AMD,"If it's for gaming then I would go for AM5 and 7800X3D or 9800X3D if you already planned to spend €500 for CPU alone.  Have in mind that (maybe except CS2) your GPU will be a huge limiting factor after you upgrade your CPU. You performance gains in games will be limited by it and until you upgrade your GPU, your new CPU won't see a high utilization.  You will get a bump in performance in productivity as well (newer CPU architecture, more memory bandwidth with DDR5, higher clocks), but it won't be huge, since core count on those CPUs is the same.  Like I previously wrote, 7800X3D (especially tray version), might be a good middle ground. In Poland tray versions cost €40-€50 less than box ones and are similar to 7700X in price. This with B650 motherboard and a 32GB 6000MT CL30 DDR5 should make your upgrade more budget friendly, while keeping 80-90% of performance gains that 9800X3D would give you.",Neutral
AMD,"Lol, the GOAT. Enjoy!",Positive
AMD,"Lower latency ram might helo, but you are the Task Manager (ctrl+alt+delete) detective. You must look at stats while performing tasks to understand how system is responding. 100 Hz display tho? Are you hitting that limit while gaming?  Intel is still better for general productivity and 14600K is on sale in many regions. Intel can also handle faster RAM than AM5 unless you have APU and that makes a difference, too (even tho most benchmark comparisons run 6000 MT/s on both and can effectively handicap Intel or APU).",Neutral
AMD,And when upgrade  time comes again x3d will be over the roof again :),Positive
AMD,"Still pretty expensive. I'd just get a 5600,X,T,XT etc since they're cheap, readily available, and powerful enough.   Otherwise its onto AM5.",Neutral
AMD,Yeah. And you can sell your older am4 stuff for like $150-200,Neutral
AMD,i bought 9800X3D and 64GB RAM / i need that for davinci resolve 4K editing with complex Color grading.,Neutral
AMD,Thank You!,Positive
AMD,"Only if you wait too long. There were many months when you could get a 5800X3D, then later on, the 5700X3D, before only very recently you could get neither.",Neutral
AMD,I have 5600. Great CPU. Will serve for at least 2 more years.,Positive
AMD,Nice purchase :) Have fun with new CPU.  Be sure to upgrade the BIOS to the latest one before swapping CPUs. You will need to install the latest chipset drivers as well.,Positive
AMD,"I just bought 5070ti for my 5600x. There is absolutely no bottlenecking when playing 1440p or 4k. Every title availible now is played comfortably without any stutters.   IMO buying 5700X3D right now for the price of 400$ is not wise. If it'd be like 250, yeah, sure. But not at this price.",Positive
AMD,Just upgraded from i7 6700K to 3600... 58€ boxed new+60€ aorus elite b550m. Kept ram and everything else. I guess 5600 was worth it for 100€ but I'm capped at 120 fps and an rx 6600 so not worth spending anymore. My main rig is an am5 tough 7500f&rx 9060xt 16g totally rocks it.,Neutral
AMD,Thank you!,Positive
AMD,"265K or even a 245K. The 265K wins against a normal 9900X in a lot of benchmarks, Intel has a much better power draw at idle than AMD and the iGPU is much better for productivity overall (can edit 4K videos easily). Yes, you dont have upgrade path (so far, some people say a new lineup of CPU for that socket will appear), but lets be honest: how much time is it going to pass until you need to upgrade? And that applies to both AMD/Intel.  Also, for productivity [use this website, they focus a lot on those things. ](https://www.pugetsystems.com/labs/articles/amd-ryzen-9-9950x3d-and-9900x3d-content-creation-review/)",Positive
AMD,Probably 265K,Neutral
AMD,265K. I own 3 AM5 rigs and just ordered the 265K combo off newegg for a new build.,Neutral
AMD,265k and its not even close,Neutral
AMD,"Your logic seems solid and 265K has a ton more cores, so it should be better for productivity in the long term. Even 245K is worth considering for the right deal. You want additional power draw during intense tasks when you have additional cores.   The iGPU in Ryzen 8600G and 8700G comes closer to 265K and all 3 handle faster RAM than the chiplet Ryzens.  The NPUs in 265K and Ryzen APU don't have enough TOPS to qualify as Copilot+ PC, so it's tough to say what it'll do. In Copilot+ laptops with 40+ TOPS, NPU performs OCR and image recognition of local files during indexing and it can apply effects to camera at OS level. If those are important to you, you might also consider laptops with Intel 256V or 258V.",Positive
AMD,"I think the 265K is a great option for your listed use case, you’ve already outlined the benefits of it compared to the 9700X.",Positive
AMD,"What about the 8700g? Since you're not planning on getting a GPU. The 8700g has a 780m igpu, which is genuinely a pretty good GPU. It's around a 2050 mobile.  Although the 8700g is the slowest of the 3, it'd have the fastest GPU by far.  It depends on your exact usecase.",Positive
AMD,"If you're unlikely to upgrade within 3 years, the 265K is better in most aspects for your use case.",Positive
AMD,265k by a wide margin from everything I’ve seen. The combos that were posted at NewEgg make it cheaper than 9700x and comparable mobo too. I’m itching for an upgrade and almost pulled the trigger on a 265k combo but I ultimately decided to wait for next gen. Both the 9k series and intel’s 200 were poorly received and have minimal improvements over 7k series and 14k.,Neutral
AMD,265k > 9900x  why would you even compare the two??,Negative
AMD,"Although the 200 series would be better suited to your tasks it is on a dead end platform,  With amd you get next gen on the same socket with 50% more cores above 6.5ghz and at least 10% ipc will leave intel in the dust   And latest rumours are pointing to the gen after that still  being on AM5 making the 200 series even further behind requiring new motherboard as well as the cpu at the very least to upgrade and you might even need ddr6 (although this is unlikely theres still a chance)",Negative
AMD,"I hate to say this, but idle power is probably irrelevant these days. Windows spends so many CPU cycles doing crap in the background any more that I don't know if there ARE any CPU's that idle any more. I suspect the only time the average PC CPU is idle is when it's just finished POST and about to start boot.",Negative
AMD,"A 20W idle power difference would amount to a grand total of about 1 USD/month over 12 hours per day, every day. I don't think that's even worth factoring in. If you really care about efficiency [the 9700X really is the way to go](https://gamersnexus.net/cpus/intel-core-ultra-7-265k-cpu-review-benchmarks-vs-285k-245k-7800x3d-7900x-more#265k-efficiency-testing).  The 7000/9000 Ryzen iGPUs are okay for regular use and even very light gaming, but if you'd like to use GPU accelerated video encoding, for example, Core Ultra should fit you best. Not sure how iGPU accelerated work would compare against good old regular CPU encoding though.",Neutral
AMD,"yes, ryzen igpu can't decode complex video codecs in the editing timeline, so it's an automatic failure for the ""occasional audio and video editing and encoding"" that the o.p. posted.  i think that 256k/465k has a later version of intel quicksync, that the o.p. needs, don't know how much it matters for what he's doing.",Negative
AMD,I'd suggest a used 3050 or 2060 instead,Neutral
AMD,"Great info, thanks! I definitely like the option to upgrade the CPU in the future.",Positive
AMD,Getting one of those would add $130-$150 to their build. It looks like they're going for a really budget build and they don't seem to use the GPU too often anyways,Neutral
AMD,"I will mention that their version of the rumors is the upper end of expectations.    AMD’s statements suggest one more full generation on AM5, not two.  Support is promised through 2027, extended from an original promise of 2025.  AMD’s cadence has been a CPU generation about every two years since they got fully caught up with Zen 3 (Zen 6 late 2026/early 2027).  DDR6 will be AM6’s raison d'etre and it is expected 2027.  They will want their big, expensive high core productivity CPUs on a platform that can feed those cores.    The clock speed rumors for Zen 6 are all over the map - nothing is close to final.  I’ve seen expectations from 6 GHz to 7+ on the highest end parts.    I would take any and all rumors about Zen 6 with a massive grain of salt for at least the next 6 months.  Remember, rumors that spread are often what people want to hear or wild speculation.",Neutral
AMD,"Correct, I have no need for a discrete GPU at this time.",Neutral
AMD,Next gen AMD is expected to still be on AM5.  AM6 isn't expected to be out until 2028.,Neutral
AMD,"Honestly I probably would have gone to AM5 before ordering a 5800xt.  It's not future proof of course, but it will give you a bigger jump in gaming performance now.",Positive
AMD,Oh ok i did not know. You are right that is waaay down the line.  What would be the best bang for buck for an AM5 mobo/cpu/ ram in today’s products?,Neutral
AMD,I will return the 5800xt on Amazon tomorrow full money back.   What would then be the best bang for buck for an AM5 mobo/cpu/ ram in today’s products that I can buy?,Neutral
AMD,Montech Century II 1050w,Neutral
AMD,What about corsair rmx1000x?,Neutral
AMD,There’s a couple of good PSU-tier list available. I use ZTT’s the most. All common PSUs are graded. Go with any A that’s 3.1 and 1000~W.,Positive
AMD,It's a good high quality psu,Positive
AMD,I like Corsair PSU's more than others because they have cables for sale on their site and Amazon if you ever need them.,Positive
AMD,So corsair i think will be fine then.,Positive
AMD,"You didn't list your motherboard, but you'll need to update its BIOS to support the newer gen CPU.  Do that before you remove your current CPU.",Neutral
AMD,"depends on your mobo, most of them will just require a bios update and you can just drop it in. some manufacturers i believe you cant though.",Neutral
AMD,"Should be fine to upgrade, the CPU will physically fit perfectly onto the motherboard. The trouble comes from a software side. You will likely just need to update your bios to the latest version. You can find tutorials for this on youtube.   If it comes from an OEM prebuilt though (HP, Dell, Acer, Asus prebuilts, etc) CPU support is much more limited and they may not provide an update thay recognizes Ryzen 5000 series cpus.",Neutral
AMD,That was fast! And Whoops! It's an Asus H310M-A R2.0 MICRO ATX DDR4 LGA1151,Positive
AMD,H310/LGA 1151 is for 8th and 9th gen Intel CPUs.  That is not the mobo you have if you currently have a 2700X.,Neutral
AMD,Please go to a computer shop/service to help you find out what hardware you have and what are your upgrade options and let them do the needed upgrades.,Neutral
AMD,Sorry was getting my information from an old email when I built it as I'm at work right now. I'll check my BIOS and get the information when I get home,Neutral
AMD,Not worth 200USD IMO.,Negative
AMD,For 200 id say no unless you like nvidia,Neutral
AMD,"I'm a Nvidia fan but if its my money I would go 9070xt. Honestly, you can buy a 5070 ti for $750 or less and I would do that before a 4080s",Positive
AMD,9070xt,Neutral
AMD,"9070XT for gaming  4080 Super for AI, video editing, 3D modeling, streaming, etc. Also noteworthy for DLSS.",Positive
AMD,4080 is like 5 percent faster in raster and 10-20 in RT. More in path tracing but only a couple games have that  Not worth 200 esp since the 4080 is used,Negative
AMD,"9070XT.  For $750 I'm getting a 5070 Ti, and i own a 4080 Super.",Neutral
AMD,Just go AMD bro save the $$,Neutral
AMD,9070 XT,Neutral
AMD,The cheaper one,Neutral
AMD,Just under $1000  https://pcpartpicker.com/list/Kj8Ndb,Neutral
AMD,all gamemax psus are absolute garbage,Negative
AMD,Thank you! :),Positive
AMD,"Here's the note from the SPL PSU Tier List for Gamemax PSUs.  > Predatory/scam consumer practices, including providing rewards for 5-star reviews, and random platform swaps to significantly worse platforms with no difference in model number, branding, or marketing.",Negative
AMD,"You are welcome!  If you need to save money somewhere, you can go with a cheaper b650 if you update the BIOS.",Positive
AMD,This is a good build. If you're near a microcenter you could probably save a few bucks but overall this is solid.,Positive
AMD,"Oh wow I didn't know that, thanks for the heads up.",Positive
AMD,"Looks fine. I'm unfamiliar with the quality of the ssd, /r/newmaxx has a flowchart in the stickied post",Neutral
AMD,Overall it's a good list but ig you're looking to optimize the two things that jump out to me are the ram and cpu.   32gbs ram is more than enough currently and can save you quite a bit.  CPU can be swapped out with the 7800x3d without significant penalty and save you quite a bit. I've seen quite a few sales for it recently that make it especially compelling. 9800x3d is great but may be overkill for your setup.   Maybe put those funds towards a better gpu? 5070ti or 5080? You'd probably notice the extra dollars there the most.,Positive
AMD,"You can get more for less. Suggestions to have all white parts (including motherboard) and have all fans be aRGB (to visually match) for €50 less than your list, despite including the cost for all components - fans included. * **CPU Cooler**: Much les expensive listing for a white 360mm AIO at Amazon Netherlands. * **Motherboard**: Switched to an all white B650E motherboard. * **Storage**: Upgraded to better spec drives all around (the Fanxiang S880 is dramless QLC flash). Added a Silicon Power XS70 (dram cache, good speeds, TLC memory) and a Patriot P400 Lite (dramless but TLC memory). * **Video Card**: Less expensive white 9070 XT. * **Power Supply**: Less expenses all white 850w ATX 3.1 unit that has better looking individually sleeved white cables. * **Case Fans**: Added six white 120mm PWM aRGB fans that visually match the ones on the AIO, to fill out the case.   * Two 140mm for intake on the bottom.   * Four (single and 3-pack) 120mm - 3-pack for intake on the side, and the single for exhaust at the rear.  [PCPartPicker Part List](https://nl.pcpartpicker.com/list/Tvm9kf)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 9800X3D 4.7 GHz 8-Core Processor](https://nl.pcpartpicker.com/product/fPyH99/amd-ryzen-7-9800x3d-47-ghz-8-core-processor-100-1000001084wof) | €460.41 @ Amazon Netherlands  **CPU Cooler** | [Thermalright AQUA ELITE ARGB V4 66.17 CFM Liquid CPU Cooler](https://www.amazon.nl/THERMALRIGHT-Liquid-Cooling-Bearings-LGA1150/dp/B0CP95FXCX/ref=mp_s_a_1_17?crid=2O16H6U2ED70F&dib=eyJ2IjoiMSJ9.q8vTEwaw8Z-p1AFebdFpjpdZemUjC417CpoKL23sY5LHbyKM10e8J76YWKOvTNzgiKut_D-9PaKpeSdmHoh4c8qaJ0q1MJ0TsLuTUioEdaBeTI6sPAlTE_wpGRmB41TDMvKPcpzoGF9PJZC3e_l-MbBARAzj2-hA4zx7sSSVFb1nhOHtnUC6J8RBt61-GFVQ7wq0blBS06qxiLd6XaHzjw.aIAxcpW3kkVAJWIMEiudmdTbkSPkRcUU1tQ4sjGl8js&dib_tag=se&keywords=thermalright+360&qid=1760140203&sprefix=thermalright+360%2Caps%2C154&sr=8-17) | €54.59 @ Amazon Netherlands **Motherboard** | [Asus B650E MAX GAMING WIFI W ATX AM5 Motherboard](https://nl.pcpartpicker.com/product/QtnXsY/asus-b650e-max-gaming-wifi-w-atx-am5-motherboard-b650e-max-gaming-wifi-w) | €169.90 @ Amazon Netherlands  **Memory** | [Patriot Venom 64 GB (2 x 32 GB) DDR5-6000 CL30 Memory](https://nl.pcpartpicker.com/product/RHCZxr/patriot-venom-64-gb-2-x-32-gb-ddr5-6000-cl30-memory-pvv564g600c30k) | €229.90 @ Megekko  **Storage** | [Silicon Power XS70 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://nl.pcpartpicker.com/product/7R92FT/silicon-power-xs70-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-sp02kgbp44xs7005) | €129.99 @ Amazon Netherlands  **Storage** | [Patriot P400 Lite 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://nl.pcpartpicker.com/product/hkmNnQ/patriot-p400-lite-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-p400lp2kgm28h) | €114.95 @ Amazon Netherlands  **Video Card** | [XFX Swift Radeon RX 9070 XT 16 GB Video Card](https://nl.pcpartpicker.com/product/ZdHp99/xfx-swift-radeon-rx-9070-xt-16-gb-video-card-rx-97tswf3w9) | €666.00 @ Amazon Netherlands  **Case** | [NZXT H6 Flow ATX Mid Tower Case](https://nl.pcpartpicker.com/product/8QMMnQ/nzxt-h6-flow-atx-mid-tower-case-cc-h61fw-01) | €97.90 @ Alternate  **Power Supply** | [Lian Li EDGE GOLD 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://nl.pcpartpicker.com/product/h4qNnQ/lian-li-edge-gold-850-w-80-gold-certified-fully-modular-atx-power-supply-eg0850g-white) | €105.85 @ Alternate  **Case Fan** | [Thermalright TL-C-C-S 75.8 CFM 140 mm Fan](https://nl.pcpartpicker.com/product/KMrqqs/thermalright-tl-c-c-s-758-cfm-140-mm-fan-tl-c14cw-s) | €7.90 @ Amazon Netherlands  **Case Fan** | [Thermalright TL-C-C-S 75.8 CFM 140 mm Fan](https://nl.pcpartpicker.com/product/KMrqqs/thermalright-tl-c-c-s-758-cfm-140-mm-fan-tl-c14cw-s) | €7.90 @ Amazon Netherlands  **Case Fan** | [Thermalright TL-C12CW-S X5 66.17 CFM 120 mm Fans 5-Pack](https://nl.pcpartpicker.com/product/kMqrxr/thermalright-tl-c12cw-s-x5-6617-cfm-120-mm-fans-5-pack-tl-c12cw-s-x5) | €19.69 @ Amazon Netherlands  **Case Fan** | [Thermalright TL-E12W-S V3 72.37 CFM 120 mm Fan](https://nl.pcpartpicker.com/product/JsRwrH/thermalright-tl-e12w-s-v3-7237-cfm-120-mm-fan-tl-e12w-s-v3) | €8.59 @ Amazon Netherlands   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **€2073.57**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-10-11 02:05 CEST+0200 |",Neutral
AMD,"Yea I'm changing the ram for sure. But I'd like to keep the 9070 XT over the 5070 ti, as I saw it get more fps in games I care about and less fps than the 5070 ti on games I don't care about",Neutral
AMD,"Fair enough. Do you play cpu bound games? Like city builders and simulation games? Assuming you don't, I still stand by 7800x3d over the 9800x3d. Very similar performance (especially at 1440p) for close to a third less.",Neutral
AMD,"9600x is slighter faster and runs cooler  7700 has 2 more cores  Honestly streaming and val don’t care about cores so since they are roughly equal performance, choose whatever is cheaper. I’d personally go 9600x",Neutral
AMD,"Get the 7700, the 2 more cores will benefit your for streaming, plus if you overlock it, it will perform even better than a 9600x , I got the 7700 and it’s amazing",Positive
AMD,7700,Neutral
AMD,zen 5 overclocks better than zen 4 not to mention streaming will be off the gpu not cpu.,Neutral
AMD,"8 cores are better for multitasking tho, for having multiple programs open",Positive
AMD,ryzen 7 7800x3d + rx 9070 xt is way better,Positive
AMD,not only is the bottom better but it HAS to be signifinatly more expensive than the top option,Neutral
AMD,"Roughly 90$ difference in price, parts here are expensive and they are the only available amd parts in here",Negative
AMD,"7800x3d build is very good, what is the total price?",Positive
AMD,"1530$ and the first one 1459$ , those are parts price without monitor, mouse and keyboard.",Neutral
AMD,"Snazzy deal, I’d buy it",Positive
AMD,"This is a little more expensive but has less storage and a worse psu, but does have a better upgrade path https://pcpartpicker.com/list/bpv7Qd    You could also spend closer to $680ish for more storage and the 850w century  https://pcpartpicker.com/list/DcsFv4   Sub $600, closer to your original parts list  https://pcpartpicker.com/list/JgfgGJ",Neutral
AMD,Looks great!,Positive
AMD,"It's usable, but it's also the type of PC that someone might've built in 2022. A friend of mine has a very similar rig with the same CPU and GPU, and he's desperate for a GPU upgrade as the 6600 can't handle UE5 games.",Positive
AMD,For an extra $17 you can get almost double the CPU performance when the 14600k is on sale.,Positive
AMD,"Some games will have almost equal performance, some games will favor 7800X3D. You can find YT videos with comparison. Is the difference worth $150? You must decide based on the games you play. Difference will be more apparent when you upgrade your GPU.  You can also try looking for 7800X3D Tray version - at least in my country it's around 50 Euro cheaper than box version. If it's the same for you, it will bring the price closer.",Neutral
AMD,"with the 5070 ti maybe depending on the games.  If you don't have the cpu yet, just get the 7800x3d and not think about it again for the next 5 years and it'll resell better. If you get the ryzen 7700 you're just going to be itching to upgrade so you'll have to pay taxes again and deal with selling the 7700.",Neutral
AMD,"I have a 7700 paired with a 7800XT for 1440p use, and it does even come close to holding it back. That being said, I don't play FPS or RTS games at all, which is where you'd see the biggest difference. It really depends on the game.",Neutral
AMD,I like it. You picked the right parts. Good cpu and gpu the rest dont matter much so good choice.,Positive
AMD,Enjoy it. Just in time bf6 😀,Positive
AMD,"Thanks, just needed confirmation :) waiting for it to arrive",Positive
AMD,"The F model is the one without internal GPU. Has nothing to do with OC. Just stupid to get one. Also, swapping a xx900 for a 9800X3D in this use case is counter productive.",Negative
AMD,"Honestly, you'd probably be better off going with the new Intel Ultra 7 265k. For your use case, you won't benefit anything from x3d.  Just got mines on ""the egg"", got $50 off and came with a free 360mm Aio... For $259? Can't beat. Add a mobo and still cheaper than an x3d cpu wlone.  Oh and I have the 9800x3d right now. I'm not a fanatic of either. Just realistic. Seems that Intel still wins this battle even in games (better 0.1% apparently)  https://www.cpubenchmark.net/compare/6326vs4694vs6344/Intel-Ultra-7-265K-vs-Intel-i9-12900F-vs-AMD-Ryzen-7-9800X3D",Positive
AMD,> The F model is the one without internal GPU. Has nothing to do with OC.  I think OP means that's why they didn't go with the K model.,Neutral
AMD,?,Neutral
AMD,You said your temps are fine but what are they?  Did you change anything in the bios after the upgrade?,Neutral
AMD,"Where did you find a 5800x3d? I looked for months unsuccessfully until finding a 5700x3d which has been great, but wish I had found a 58.",Positive
AMD,"Most likely XMP/EXPO turned itself off during the BIOS update process and now the 2400mhz speed is unstable on your kit  Look in your BIOS settings for XMP/EXPO profiles, if you can't find them google your motherboards model number and XMP",Negative
AMD,i say PSU slowly dying.   I also have the 5800X3D and those symtomps sound like PSU issue. OR Ram but i would say PSU,Negative
AMD,"I built my PC a month ago with the same CPU and had this same problem. Could be a lot of things like PSU, but for me I saw a reddit post saying there was something wrong with a CPU core.  I exchanged the CPU for a new one and it now works without a problem.",Negative
AMD,Crashes are super annoying to figure out.        I'd start with memtest86 and seeing if you have memory problems.   Then do a CPU stress test with Prime95.,Negative
AMD,"Had almost the same issue after upgrading my CPU (3200G -> 5800X) on a crappy ASRock AM4 motherboard. Ended up replacing almost every part like PSU, RAM, and it kept happening, PC would just turn off in certain games or while downloading heavy games.   After there was nothing else to replace, I moved the setup to a decent B550 motherboard and that fixed it, haven't had any issues since.",Negative
AMD,"Assuming you have the latest BIOS and nobody else is reporting issues with it (**do check** in case they are), you could have a hardware issue or a settings incompatibility.  The most likely thing is however the BIOS and/or it's current settings in relation to your new CPU.  You may have to tweak the voltage to get your 5800X3D running stable, but the fact it's crashing when you're not even under load is something best brought directly to the attention of ASRock customer support asap.  I would take this to their subreddit and make sure you are using the best BIOS revision for that CPU, as sometimes latest isn't the best for your specific system configuration.  If you have any error logs from or get any messages on your screen when the reboots happen, get those noted down and start looking for people with the same board and CPU combo.",Neutral
AMD,"Had that Same issue, for me it was the RAM, too high speeds. Had to make the CL higher",Negative
AMD,update amd chipset drivers too  i had the same problem with a 9700x and that fixed it,Neutral
AMD,What is the event code in event viewer?,Neutral
AMD,"Clear CMOS. Look up how to do that for your specific motherboard. There may be two jumper pins you need to short. Otherwise, you can remove your CMOS battery, unplug PC from wall, then hold the power button for a few seconds (10 seconds should be fine).  Next, put your CMOS battery back in and boot into BIOS and load default settings. Boot up and see if that fixes it.  While you're at it, it wouldn't hurt to re-seat your RAM and ensure your CPU cooler is tight.",Neutral
AMD,"Finally, where I can actually help. I've had this exact same issue when I upgraded from 2700x to 5600x during COVID. I've spent 30+ hours literally trying every solution known to Google. Ended up returning it, because nothing worked. Put my old 2700x back? Boom, all good. A couple months later, it was available again and I so badly wanted it and bought it again like a fool. Guess what? Installed the new CPU and everything works fine. That's right, the CPU I had returned was defective.  So don't stress too much like I did because in the end, it could be as simple as a defective CPU. I'm sorry if this isn't the solution you're looking for but this was my experience.",Negative
AMD,"I installed an 5700x3d in an asrock b450 mobo, and have the same ptoblem, i reinstalled the cpu in another mobo gigabyte x470, and worked very well.",Positive
AMD,"Can you check event viewer for shutdown events? Event IDs 41, 1074, 1076, 6005, 6006, 6008 are all related to shutdowns. Find the timestamp of when you experienced one of these shutdowns and see what the event says.",Neutral
AMD,"When you've exhausted all suggestions here.  Find your 'Global C-state' setting in the bios and change it.  If it's enabled, disable it, and vice versa.",Neutral
AMD,"Check your power plan settings. Make sure its on ultimate or high performance.  Windows like to default on power saver which will lower your pcie, cpu, and ssd voltages causing crashes and restarts.",Neutral
AMD,It's strange that you haven't considered whether the new CPU is drawing too much power for the PSU to provide.     Does it have the same power draw as the old CPU or something?,Neutral
AMD,"Funny you mention this, i have an older MSI B450 motherboard as well with a 5800x Non 3D and whatever bios was available at the time of install to support 5800x since i came from a 3600x and i have exactly the same issue.  PC will just randomly shut down with a black screen when idling or playing low GPU intensity games. If i play that puts load on GPU its fine, can run all day without rebooting, but if im working on spreadsheets or playing an older game that doesnt load GPU something itll shut down sometimes multiple times a day.  I dont have a fix for you unfortunately, my fix was to just replace the entire thing with a new 9800x3d and motherboard, since im convinced the issue is something related to B450 boards and Ryzen 5000. The temporary fix for me the last few weeks has been to open a game with a GPU intensive menu and no ""kick for AFK"" timer and just let it run in the background of the toaster game im actually playing.   Keep in mind AMD originally never wanted to support Ryzen 5000 on B450 at all, but people bitched and so they started offering beta bioses and im not convinced those bioses are properly stable.  I already replaced the power supply with a Corsair RMX 850W gold a year ago, and ive tried fiddling with some different ram settings and timings, tried messing with the bios processor states and windows power plans and just never got it figured out and just got sick of it.  My new hardware came today and ill be installing it this weekend (same GPU and power supply) and can report back if it fixes the issue if anyone cares.",Neutral
AMD,If you have four ram sticks change to base settings if you have two set to xmp.   My computer kept crashing too and I couldn’t figure out why and then learned that if you have four sticks at xmp it will make it unstable and you’ll have to manually adjust timing. Since I don’t know how to do that I changed it to stock speeds and my crashing stopped.,Negative
AMD,"When you replaced the CPU, how did you clean the thermal paste off the cooler. How much and what pattern used for the new paste. Or did you skip that part ?",Neutral
AMD,Ram o falta alimentacion?,Neutral
AMD,"I would turn it off, unplug it, and pull the CMOS battery to reset the BIOS. Turn on, check the BIOS settings, and save.  One more restart and it should be good. I really do not know, but I like the idea. :-P",Positive
AMD,"do you have a undervolt set?  My msi motherboard is stable at -10, and -20 but is only 99% stable at -30.",Neutral
AMD,"if u undervolted it, then don't.",Negative
AMD,* Clear CMOS * Boot with XMP off * Update AMD chipset drivers * Check PSU health * Watch temps and Event Viewer,Neutral
AMD,"So this is just my personal experience  Years ago I bought a 5800X3D. The performance was SICK. I didn’t even over clock it. PC started black screening into a full reboot randomly. Event Viewer said something about CPU fatal error or something…installed old CPU, problem did not occur. RMAd 5800X3D, never had an issue with the replaced 5800X3D. Good luck!",Negative
AMD,"In my experience, it has been this:  BSD and then restart = bad RAM.  No BSD but still randomly restarts = bad (or not good enough) PSU.",Negative
AMD,"Random restarts is such a difficult issue to diagnose.    Last time I dealt with it, reverting from Windows 11 to Windows 10 fixed it. The time before that, reseating the GPU resolved it. And before that, it was a new mobo.    So... literally any component can cause it. lol.",Negative
AMD,"Try upgrading your motherboard, there were problems with AsRock AM5 motherboards, even up to the B850 where they would just kill any of the X3D processors. Avoid AsRock and maybe get a X670E or an X870E.   What you’re experiencing is most likely 1. Power Delivery, or 2. Overheating. So add more Case Fans and switch to a bigger AIO if you can. It would also help to get a high quality PSU, see SPL’s PSU TierList for that.  Or if you cant do any of that, update your bios, i think the verified version that supports newer X3D chips are 3.12 and 3.15, depends on the motherboard",Negative
AMD,Fresh Windows installation?,Neutral
AMD,They were running around 53-55C. I can’t remember what they were during Tarkov. What do you mean after the bios upgrade? All I did was do the flash thing and then restart the computer.,Neutral
AMD,"Also, thank you for the response!",Positive
AMD,A buddy in the industry,Neutral
AMD,"Random crashes can be a bunch of things, but PSU is definitely on the list.  They're the most annoying to diagnose which is why experienced builders don't skimp on them.",Negative
AMD,"Ok, it could be the PSU, it’s at least 5 years old",Neutral
AMD,Could also be that a few of the mobo pins are fucked. I was having pretty much the same symptoms as OP. Turns out a few of the pins in the cpu socket somehow started to corrode.,Negative
AMD,"Wanna hear something embarrassing?  I had a similar problem, and that's how I found out for the first time that cases had a PSU filter on the bottom.  Mine was absolutely matted with dust, like a 1/2"" thick carpet of it.  PSU was overheating under high amp draw when gaming.",Negative
AMD,So far it’s passed all the stress tests,Positive
AMD,"I did a cpu test with cinebench, songle and multi core passed no issues",Positive
AMD,"Brutal, hopefully that’s not my issue",Negative
AMD,Ok will do! Thank you for the reply. I will reach out to them now and cross post this into that sub,Positive
AMD,What’s the fix there? What do I need to do?,Neutral
AMD,I updated the drivers. Issue was still there. Thank you though!,Neutral
AMD,whea-logger event id 18,Neutral
AMD,Thanks dude. I keep leaning that way but the cpu passes the stress tests. Still not sure,Neutral
AMD,What does that do?,Neutral
AMD,"It does, yeah",Neutral
AMD,"Thank you for the reply. That sounds brutal, and that is what I fear is happening.",Negative
AMD,I used a thermal paste cleaner. I used a center dot with a small X.,Neutral
AMD,Honestly not sure. How would I check?,Neutral
AMD,I've done a complete swap of most components and not done this. For a simple CPU swap this is definitely not needed.,Negative
AMD,Bios flashing resets it back to defaults so checking you didn't change the memory timings or something.  You could go into event viewer in the OS and check the errors and critical logs if it points to anything specific causing a restart. Normally a out right crash / restart is something hardware related like overheating or the PSU though if nothing has changed in the bios / no over clocking etc.,Neutral
AMD,Nice! Congratulations. Should keep your AM4 system going for a long time to come. It REALLY boosted my gaming performance.,Positive
AMD,"Does he have any more lol, I really want one...",Positive
AMD,Tell me about it....Took me over a year to figure out mine. It would randomly reboot in some games and not others. Turned out to be a bios upgrade for my GPU. What a headache that was.,Negative
AMD,What Brand and Model do you have and whats the Wattage. And 5 years isnt alot for a Quality PSU,Neutral
AMD,"It's probably the PSU, you can check your parts on pc parts picker and see what the expected draw to the PSU against the wattage your PSU is rated for. A real energy test will tell you better.",Neutral
AMD,Does the new CPU require more watts than the old CPU?  You can try undervolting to decrease the power consumption which could actually help stability.,Neutral
AMD,"ah yes. AM4 has Pins on the Mobo. Im guessing by the lack of education, knowledge and intellect you are an Intel buyer?",Negative
AMD,"If nothing else helps, turn off memory integrity (it's under core isolation) in windows security. It uses Hyper-V that Ryzen 5xxx struggles with for some reason.  I spent 3 years trying to fix random reboots. I changed my PSU, updated the BIOS, removed the GPU driver with DDU, and tried countless other steps. The only thing that helped was turning off memory integrity.",Negative
AMD,"I have the exact same CPU and have been getting this error message, along with crashes non stop since October 2nd. I would reach out to AMD for an RMA. I’m fairly certain it’s a defect with the 5800X3D based on my research",Negative
AMD,"Basically nothing.  But I had a similar problem as you.  After many hours of google-fu, I came across a post that spoke about something to do with AM4 boards and 5800+ cpu's.  I thought my motherboard was bad, or my cpu was bad, nothing else worked, but disabling c-state did.  Haven't had a problem since.  It probably won't help you because it seemed to be a rare problem when I read about it, but it might.  And you can just change it back anyway.  Good luck!   Quick Edit:  That same post also suggested slightly raising or lowering your RAM voltage.  I didn't do this because the other part fixed it, but may be worth a shot as well.  Just don't forget what you changed it from so you can go back.",Neutral
AMD,"Just find a gpu intensive game that doesnt kick after AFK (like helldivers2 for ex) and let it run for awhile and if your pc runs and runs and doesnt shut down, but shuts down while idling or playing light load game then yea, its the same issue.  I havent had any issues playing BF6 today because that game puts load on my GPU.",Neutral
AMD,"""definitely not needed"" is definitely not true. As I said, sometimes you can get away with it but it's far from ideal. If you're going from maybe a R 5 2600 to an R 5 2600X you're probably fine but if you're going from an R 5 2600 to say... an R 9 3900X that would be foolish.  Not the best practice and when you have issues like the OP, clean install is the first step.",Negative
AMD,So it may have been my ram timing? It was set at ddr4 2660 but my ram is ddr4 3200.,Neutral
AMD,Ok so get into the bios and check that my memory settings are inline with the ram I have in? Sorry if these are dumb questions. I am new to all this. I had looked into event viewer but nothing really stood out to me.   I have not done any over clocking and to my knowledge the power/voltage is similar to the chip I had in (ryzen 7 3700x),Neutral
AMD,"If my PC is stable, I'm a lot less inclined to upgrade.  I'd rather deal with low FPS than zero FPS from crashes.",Negative
AMD,Thanks! I will give that a try,Positive
AMD,Thanks homie!,Positive
AMD,"Turn on XMP in the bios to set your RAM back to the 3200mhz overclock it is rated for.  Potentially your RAM sticks are unstable at 2660, but must be OK at 3200, since the were probably running at 3200 this whole time and the bios update reverted the XMP profile to off.",Neutral
AMD,"Also just to double check, when you replaced the CPU you did put on new thermal paste right?",Neutral
AMD,"Brother I just re-learned this lesson the hard way after a recent GPU ""upgrade"".. I just wanted marginally better graphics, but it wasn't worth the headache tbh",Negative
AMD,"If it ain't broke, don't fix it.",Negative
AMD,"Np, hope it works for you.",Positive
AMD,On amd and it would be docp,Neutral
AMD,Yes I did!,Positive
AMD,"It's a tough lesson.  We often recommend upgrading like it's risk free, but it's not.",Negative
AMD,… but if it is…,Neutral
AMD,North America is a pretty decent FPS,Positive
AMD,"Are you tabbed out? This only happens to me if I alt tab, if It's on the wrong monitor, or if the app is not a game.",Negative
AMD,Sometimes you just have to close it and open it again.,Neutral
AMD,"I did once , then came back and it worked ive since restarted the pc played other games. I even factory reset the graphics drivers.",Positive
AMD,"Yeah i tried it all, it stays na on any battlefield game it seems, sometimes it even continuously says i am playing bf2042 when om on another game.",Negative
AMD,last one will perform the best while being the least expensive.  Pretty easy,Positive
AMD,how do they get cheaper and better? that's an intersting list. the last one is the best,Positive
AMD,5600 and 7600 at $400 obviously.   5500 is not even vermeer cores architecture like 5600 and 5700x3D and 5800x3D...,Neutral
AMD,"all the GPUs are essentially the same performance (within 5% of eachother)., and the 5600 is better than the 5500... so the 400$ option would be the best dollar for the performance... its also got more RAM than all the other ones so it wins by a land slide.",Positive
AMD,the first one because the ryzen 5 5500 is the best cpu to ever exist,Positive
AMD,5500 is still zen 3 though   the fact that its vermeer isnt why people hated this cpu lol   thats completely irrelevant   its because it has half the cache of the 5600 and pcie 3.0,Negative
AMD,"I hope this is bait lol. The 5500 is as fast as a 3600X, and it lacks PCIE 4.0 support (for SSD or GPU).",Negative
AMD,"Its cezzene like 5600g but  cut igpu.   Its not irrelevant, all of it combines into lower performance than the name implies.    3600x with 32MB of L3, on a b450 mobo with decent cooling will perform on similar level in most games. ​  While the 5600(x) are way better.",Negative
AMD,bruhh,Neutral
AMD,"""Its cezzene like 5600g but cut igpu."" oh real shit?   thanks einstein, i already knew that   its irrelevant, what does it mean? nothing      what isnt irrelevant is the cache and thats causing the performance difference between the 5500 vs 5600 but if you were actually tech savvy youd know that, but you dont",Negative
AMD,"Its bait, no? The 5600 beats it by a long mile.",Neutral
AMD,Your emotions blind you. 🤡,Negative
AMD,long mile?   hardware unboxed did a benchmark and [https://youtu.be/4JebBhH-B88?si=HhfJkxHa77HRlEmr](https://youtu.be/4JebBhH-B88?si=HhfJkxHa77HRlEmr)   doesnt look like its THAT much better  14% better with a 6950xt and 7% with a 6600xt,Negative
AMD,you mentioned the most irrelevant difference between the 5500 and 5600 but you didnt mention the most important one and thats cache which is the almost sole reason why 5600 is a noticeably better cpu   without that 5500 and 5600 would be neck and neck,Negative
AMD,yeah? 20fps more is a lot. And in some games it loses vs the oldest 3600,Negative
AMD,"Good thing is that in the real world garbage bin silicon cannot magically grow its L3 cache to double the size, or to magically become one of the better specimens from good yield waffers or golden samples that can be overclocked and used at those clocks on all cores without problems...  Another thing is the newer games better usage of all cores that make the 5500 even worse relative to 3600x and 5600(x) than it was in older games years ago when most benchmarks were made.",Positive
AMD,"its on par with the 3600   zen 3 compensates a lot for the lack of cache when it comes to performance in games   people seem to make such a big deal with zen 3 and how much better it is over zen 2 but then contradict this idea when the topic shifts to the 5500, then zen 3 apparently becomes irrelevant    look   my original was a joke, i dont believe that 5500 was a great cpu for the price it was being originally sold   but if you could find it for a decent price, it could be a good option for budget builds   now i wouldnt recommend an am4 build in general since am5 became pretty cheap   ive seen 7600x go for less than 150 usd now too and that matches a 5700x3D in performance apparently",Neutral
AMD,"the reason why 5500 has 16mb of l3 cache isnt because its cezanne its because amd decided to give it only 16mb of cache instead of 32   \+ the 5500 and 5600 already benefit greatly from zen 3  wait hold up  ""3600x with 32MB of L3, on a b450 mobo with decent cooling will perform on similar level in most games. ​  While the 5600(x) are way better.""    you literally said a 3600x will perform on a similar level lol   which yeah makes sense considering it has double the cache but its limited by zen 2 architecture   why is 5600x much better? both 3600x and 5600x have the same amount of cache   OH YEAH BECAUSE IT HAS ZEN 3   you absolute buffoon   you do realise that 5500 and 5600/5600x both have ZEN 3 right? you do realise that the 3600x has zen 2? right?   you do realise that if youre going to minimize this advantage of the 5500, youre also contradicting yourself  either A: 5600x and 3600x are basically neck and neck and zen 3 makes no significant difference, both cpus again offer very very similar specs ignoring zen 3   so basically buying a 5600 or 5600x makes no sense since 3600x offers basically near identical performance  OR B: 5600x is noticeably better than the 3600x because it has zen 3 and this is why 5500 can match the 3600x since zen 3 compensates for the lack of cache",Neutral
AMD,"Nice essay 🤣 about your butt hurt, but no ones gonna read all that. 🤡 Whatever helps you cope at night...",Negative
AMD,"""5600x is way better than 3600x bcs of zen 3""  ""oh yeah zen 3 inside the 5500 doesnt matter lol, 3600x is so much better than 5500""",Positive
AMD,[https://www.techpowerup.com/gpu-specs/geforce-rtx-3080.c3621](https://www.techpowerup.com/gpu-specs/geforce-rtx-3080.c3621)  only 40 percent isn't really enough for an upgrade imo but you would def notice it.,Neutral
AMD,"[https://cdn.mos.cms.futurecdn.net/Di7Jh7WwqXnNeqoYPvBTRW-970-80.png.webp](https://cdn.mos.cms.futurecdn.net/Di7Jh7WwqXnNeqoYPvBTRW-970-80.png.webp)  3080 would be right around where the 4070S is on this chart, I don't think ti's enough of an upgrade to be worth it. Would probably look for a used 4080S or bite the bullet and get a 5080.",Neutral
AMD,Wait for rdna 5 2026 that will be a much bigger jump,Positive
AMD,Went myself from a 3080 to a 5070ti. Albeit a heavily overclocked 5070ti. The jump is worth it. Needed a stop gap card and got it for 600euro's with 550 euro in vouchers though. Really strarted to notice the age of the 3080rtx at 4K. Now it serves in my second system with a 1440P screen and there it is still decent but you do lack some dlss4 features.,Positive
AMD,"I run 5700x a 4070S on 4k. Run it on a single 43' 4k  panel thats 144hz - i tune down to 120hz.  Been using that for nearly a year and find the 12gb lacking in some games and needed a lil more raster to hit 120.  DLSS +FG been a must for me for newer stuff.  120fps on old games like Mad max too easy.  Games like RE4 remake/Cyberpunk in the 90-95 fps on custom high/ultra , RT on with DLSS. im shooting for 120 so a lil more raster.. and i needed that vram.  Havent played BL 4 but im sure it would cook my 4070S, seen better cards struggle.  BUT Did order a 9070XT for that lil more raster and vram upgrade. itll be here tomorrow.. will let you know how it goes.",Neutral
AMD,"You’re looking at 40% perf. Improvement, so objectively not very much considering it’s a 5 year newer gpu.",Neutral
AMD,"More of a sidegrade, unless you believe al the upscaling marketing. Paying hardware prices for software gimmicks. New GPU's (exept 5090) don't even come close to pure performance of previous generation GPU's",Negative
AMD,"I would only get the 5700x if either of the following are true:  1: You don't plan on upgrading this PC ever and will just build a new one altogether if you want more performance down the road  2: You're okay with also needing to buy a new motherboard and RAM whenever you want to upgrade your CPU  If both are false, go with the 7500f. You'll spend more money now but less over time",Neutral
AMD,"[Hardware Unboxed has a recent video with the 7500F](https://youtu.be/AhxUN2kcOWM?t=770), performance is close to what you will get from a 7600/7600X.  [And knowing that pretty much all Ryzen 5000X CPUs will have similar performance](https://youtu.be/OCuVEuFIkew?t=567). 5600X, 5700X, 5800X, 5900X, 5950X, they are all super close together.  [You can see that a 7500F (7600/7600X performance), will be much better in games](https://youtu.be/e80Gqhe2Kt8?t=510). Same is also true for multi threaded and single threaded applications (like video editing, blender etc), AM5 Ryzen 5 CPUs have a slightly better score on Cinebench than AM4 Ryzen 7 CPUs.  BTW, up to something like a Ryzen 7 9700X, or Ryzen 7 7700 non-X, or whatever CPU AMD launches in the future, you should be able to run on that motherboard, if you have decent airflow in the case. [The 9700X power draw (using the ""65W"" limit, pretty sure there's a BIOS setting or update that unlocks that) is around the same as the Ryzen 5 7600 (around 85W)](https://youtu.be/rttc_ioflGo?t=515), so the motherboard won't have a much harder time of handling the extra 2-cores (12+ core CPUs i would probably avoid tho)",Positive
AMD,"but in terms of performance, which one is more capable?",Neutral
AMD,thanks for taking the time to answer the questions that I was having inside my mind lol    I wasn't sure if this mb was good or not but it is basically the cheapest one that I can get without noticeable sacrifices,Positive
AMD,"For gaming, where single-core performance still matters a lot, the 7500F is undeniably better, it's closer to Ryzen 5000X3D levels of gaming performance.  In certain production workloads that can fully saturate all threads, the 5700X might match or in a few select cases by slightly faster than the 7500F due to having more (but slower) cores.  But overall, the 7500F is better.",Positive
AMD,"7500f. Not by a long shot, but going with a newer CPU architecture will pay off both now and in the future",Positive
AMD,"It's not ""good"", it's the bare minimum for it to be a usable product. But due to some AMD CPUs being easy to run, and having a low power target, it's possible to use a few other CPUs.  I have an H610 motherboard with a low-end VRM, and no heatsink, it's basically the same thing, I should be able to run something a bit better than a 12400F, but I know there's limits to what the VRM can do, so i'm not gonna use an i7 or similar.",Neutral
AMD,Look into AMD apu laptops.,Neutral
AMD,if the computer have a processor - its gonna be fine.   just get a better one.      But do you actually need RTX 2000 ADA tho? iirc its a 4060 for x2 the price,Positive
AMD,"AMD has better linux support - but nvidia has better AI support (cuda). Pick your poison if local AI model stuff is your goal, but overall that laptop is horrible, 8gb 'AI GPU'? lmao.",Negative
AMD,Get a Framework!,Neutral
AMD,2k for a laptop is sickening,Negative
AMD,I've heard intels have better performance for laptops,Positive
AMD,What's your budget,Neutral
AMD,"5800X or 5800XT, whichever is cheaper where you are.",Neutral
AMD,"Sorry, haven't mentioned! \~300 EUR.",Neutral
AMD,5800X without upgrading your motherboard,Neutral
AMD,Thank you!,Positive
AMD,"Don't try and low-ball me, I know what I got.",Neutral
AMD,damn. i got my r9 9950 x3d a bit ago for 700. don’t even want to know what they sell it for lmao,Negative
AMD,"I think the term you're looking for is ""overcharging"", scamming implies they won't get the CPUs they paid for",Negative
AMD,pretty sure i paid less for my ryzen 9 5950x when it was new,Neutral
AMD,Hey man they gotta get that nut and they know that students will be able to hide it as an education cost from the college store,Neutral
AMD,I paid for my ryzen 9 5900x for way less then those prices,Neutral
AMD,For that price you could buy a textbook,Neutral
AMD,it’s just daylight robbery atp,Negative
AMD,"Exactly. Anyone who buys one of these is doing so with daddy’s money, student loan money, or daddy’s money that got put into their student account 🤣",Negative
AMD,spec requirements showing what fps we can expect at what resolution and settings at native make me so hard,Negative
AMD,This is how detailed all system specs should be tbh,Positive
AMD,"Didn't see many complaints about performance issues in BF6 beta, and people were singing praises for optimization, so this is not too bad imo",Positive
AMD,Played the beta at 1080 with a 2070. No issues,Positive
AMD,"Fellas, pray for my 1060 6GB 🙏",Positive
AMD,"This again confirms that the more fps you want, the more you need to have a big CPU in this game",Neutral
AMD,My mobile 2070 was running great in beta so I can't wait to play on Friday lol,Positive
AMD,"Holy shit, reasonable system specs",Positive
AMD,3090 isn't high end anymore? Man,Negative
AMD,My 7900XTX isn’t top of the line anymore 😩😩,Negative
AMD,"I got over 100 fps in 4K during the beta with an RX7600 just by going to low settings, but this game still looks genuinely good at low. Totally worth it for the frames and very well optimized.",Positive
AMD,Isn’t this kinda good?,Positive
AMD,"Ryzen 7 7800x3d, RTX 5070 ti and 32gm 6000mhz ram, i’m so ready!",Positive
AMD,This is how EVERY game developer should put out system spec requirements. This is fucking WHERE ITS AT BOYS!  9800x3d/5090 playing on my msi 32in 4k 165hz qd-oled at NATIVE 4k was getting me 130fps~,Negative
AMD,"Well im good to go.. sadly i wont get the game, i got fired cuz staff reduction, so i should not use money on games till i get a new job   ![gif](giphy|sT1K7rkPJOwh2)",Negative
AMD,I don’t think me and my i7-4790 will be enjoying this anytime soon lol,Negative
AMD,Why does a game care about TPM and Secure Boot?,Neutral
AMD,Cries in 6600xt,Neutral
AMD,I'm more concerned about that kernel-level slop they hook up the game with...,Negative
AMD,"i'm stupid, do i need windows 11 to run the game at 1080p 80fps+?",Negative
AMD,Been waiting 10 years for a good battlefield game. Super excited to play this. Ran the beta at 150+ fps on my current setup. Can't WAIT,Positive
AMD,TPM2 enabled on w10...,Neutral
AMD,"Ok, good detailed system specs, my question is why tpm enabled requirement. The last column there",Neutral
AMD,My 3080 getting me probably 60 fps on 4K ultra is fucking CRAZY,Negative
AMD,My 7700x was really busy in the beta. Locked my frames at 120 at 3440x1440p Ultra DLAA with my 4090. Never really got the feeling of lots of framedrops.,Neutral
AMD,"Based on the performance in the beta, it'll probably run better than the specs suggest. I played with hardware comparable to the minimum spec and still saw +60 fps most of the time. So they either got less optimized since then or they're using worst case performance scenarios for their hardware recommendations.",Neutral
AMD,I have 1080 TI lol,Neutral
AMD,"If accurate, this is exactly how triple A games should perform in the current generation.",Neutral
AMD,Has anyone tried this game on a HDD?,Neutral
AMD,"did anyone play the beta with a ""rufus"" like install of windows 11 that skipped the tpm/secure boot requirements?",Neutral
AMD,ultra++ it is...,Neutral
AMD,"For the sweaty boys out there, we all know we'll run this bitch in the lowest settings possible for ""enhanced visibility""",Negative
AMD,"Well upgrading to 9700x and 3090rtx is great Had rx580 during the beta, while it was playable but had to do fsr to get close to60fps",Positive
AMD,"I upgraded from a 2070 laptop to a 5090 laptop for this, let's go! Can't wait.",Positive
AMD,>UEFI SECURE BOOT ENABLED  oh come on....,Neutral
AMD,"Can you actually use MFG in a competitive online shooter?  I don't have a problem with MFG in casual games but aren't you going to run into, idk, latency or something fucky?",Negative
AMD,Might be the first game I opt to use 1080p on my 4K monitor. Can’t wait,Positive
AMD,Makes sense except for MFG being on the final performance tier. That's not really what mfg is good for,Negative
AMD,got me at TPM 2.0,Neutral
AMD,No linux compatibility added? :(,Negative
AMD,"I have the ultra specs and the game keeps randomly crashing, Otherwise it works flawlessly, but doesn't like to stay open. It crashes with no error  warning or anything and  im not sure what to do",Negative
AMD,Alright how is windows 10 on the minimum.  I couldn't install it on my system because it doesn't have that boot check option.,Negative
AMD,It also requires giving money to Jared Kushner.  Fuck that.,Negative
AMD,"Multiframe Gen? I hope they intend that for the campaign mode, and not for online competitive play.",Neutral
AMD,i personally think tpm and secure boot requirements is actually braindead,Negative
AMD,My poor CPU.  I need to just retire it to the wife's pc and pull the trigger on 9800X3D it seems.,Negative
AMD,"Native means it internal, already upscaled by frostbite engine.  fk the tpm and vbs stuff.",Negative
AMD,"I love it but frankly, I think they're overestimating just to be safe. My 12400/5700xt is lower than recommended even though I ran ultra with really great FPS in the beta. There's no way it drops that much",Positive
AMD,Looks like they “Recommended” 8GB VRAM,Neutral
AMD,This is pretty impressive considering the intentional lack of upscaler for the reported performance metrics; native 144fps at 2k res is amazing.,Positive
AMD,"I played the beta with an rx7900gre on 4k, using fsr I reached 80-90fps. Worked perfectly well.",Positive
AMD,Ahhhh when your hardware is listed as capable of 144hz 4k with dlss and no mfg. i wonder what native will be like with settings tweaks.,Neutral
AMD,Rip. Won’t be able to hit 1440p 144fps with my 4070ti super apparently,Negative
AMD,"I played the beta two different times with an 8gig 3060 (not the TI), 3700x and 32gig ram at 1080.    I couldn’t play on balanced on the recommended. I had to set it to low and tweak some stuff.    Would go from 60fps to 20’s.   I really hope the final version is much better.",Negative
AMD,On my knees praying I can somehow get 60fps from my 1070Ti,Positive
AMD,Anyone try a intel 9700k yet?,Neutral
AMD,my 1080 will run it fine... i hope.,Positive
AMD,"I’m new to all this, so I’ve got a question. During the beta I was able to play and it was pretty stable and I only have a 1660ti. Why is this so if the bare minimum specs are above what I have? Thanks in advance.",Neutral
AMD,Actually impressed with the detail here. Also makes me feel the need to upgrade lol.,Positive
AMD,I hope this game delivers + good support over the years! We as a battlefield community desperately need a new game! Cannot play BF4/3/1 anymore.,Positive
AMD,Can someone recommend some settings for medium +++,Neutral
AMD,my 5600x will cry :(,Negative
AMD,Beta played quite nice so I hope they dont fuck up the performance at release.,Positive
AMD,"Why do all of these system specs sheets assume that you're either playing at low resolution at low fps, or high resolution at high fps? I play with the settings cranked at 1080p so I get a high fps, and these charts are never applicable.",Negative
AMD,Does this mean I have to update to windows 11 to crank those settings up?,Neutral
AMD,"My specs are a tad all over the place, but im hoping for 90fps at 1440p, quality settings dont matter",Neutral
AMD,"I wanna do an upgrade for BF6, (from r5 7600 to r7 7800x3d) cuz it's mainly CPU-bound game(I'll make a GPU upgrade later). Is it worth making the switch?",Neutral
AMD,I can confirm the 4k @144hz requirements as being accurate in my limited example,Neutral
AMD,Optimised like crazy. I love it.  Other devs need to take notes,Positive
AMD,can confirm was getting solid 144hz ultrawide 1440p on a 5080 + 9800x3d on ultra during the beta.,Neutral
AMD,My 3080 and 3950x was struggling a bit during the beta. I’m hoping that switching on my DOCP ram setting in the bios helps out 🤞🏽,Neutral
AMD,Black Friday just can't come faster,Positive
AMD,The beta ran flawlessly on my 5060 ti and Ryzen 7 7700x. Im so excited!,Positive
AMD,"This is a great way to list specs but only if they could also show ray tracing on vs off because if it needs DLSS 4k with a 5080 without ray tracing to hit 100+ frames, it's not bad but it's kinda alarming because then with ray tracing you're looking at 60fps with DLSS",Neutral
AMD,Just built a new PC with a 5090 and 9800X3D and went from a 1440p monitor to 4k. Very excited to see what it looks and plays like. I was out of country for work during the beta and only got to experience it on med settings on a 1080p laptop.,Positive
AMD,"damn how do i enable tpm2.0 and uefi secure boot? i was trying to do cod beta the other day and couldnt due to those not being enabled...  got the 5090 and 4k monitor 145hz for this game, lets gooo",Negative
AMD,i hope the performance i got in BETA stays the same (140-175fps 1440 competitive settings) when the game is out,Positive
AMD,"Holy shit, recommended specs are for 1440p 60fps high? Thats a fuckin first.",Negative
AMD,Why is 5060 not there 👀,Neutral
AMD,"Damn, not even a year old PC and couldn't even make it in the Ultra ++ tier.",Negative
AMD,"That’s awesome, very surprised my gaming laptop will be able to handle 1440p on high settings",Positive
AMD,"Isn’t a 4080/7900xtx + 12900k/7800x3d a little concerning for 1440p/144hz at MEDIUM settings in a competitive game?   I mean, someone like me with a 4070super + 7700 is not getting to 100 fps at MEDIUM settings in 1440p 😳",Negative
AMD,i thought the gtx 1060 was minimum spec?,Neutral
AMD,Gee look at those pretty good requirements when it's not UE5,Positive
AMD,I was worried my 9950x3d/5090 build would struggle,Negative
AMD,3440X1440 with my rtx 3060ti  would love to upgrade 😢,Positive
AMD,No complaints there,Positive
AMD,"🤞come on ROG Ally X, you can do it.",Positive
AMD,Not that I was gonna play BF6 but perhaps I'm due for an upgrade lol,Neutral
AMD,"That's... actually acceptable, damn, an AAA game that dont require a 4070 just to open?",Positive
AMD,I was getting about 80-110fps at 1440p medium with DLAA on my RTX 3080 in the beta. Final spec requirements seem to match up.,Neutral
AMD,"Old guy here who couldn’t play beta.  Can my 9900k, 1080ti, 16gb ram be up for the task at low/ settings?  1440p monitor but I’ll downscale to 1080p.  Can I get decent frames?    Not in a position to upgrade.  Thinking of getting PS5 pro because it supports Mouse and Keyboard.  🤷",Negative
AMD,"Game ran like butter on my base 9070, really well optimized...  Or maybe our standards just dropped.  But they definitely cooked well with this game.",Positive
AMD,"my 2070 was getting 70-90fps on low settings in the beta, ill be upgrading to a 5070 in a week tho!",Neutral
AMD,They have the 7800X3D under the Core 9 Ultra 285k? What?,Neutral
AMD,I mean its not even hiding anymore that developers don't give a damn about optimization. Back in the days of Battlefield 4 and NFS Rivals they ran on max settings on a potato and still look better than many games today that even NASA pc will struggle to run,Negative
AMD,What could i realistically expect with a 2070 / I5 9600K on a 1440p monitor? Probably fucked for 60fps right?,Negative
AMD,I'm glad i upgraded my pc few weeks ago. I'm pumped for friday.,Positive
AMD,Does it already let you download?,Neutral
AMD,"144p 165hz monitors with an RTX4080 Super, and the Beta was butter smooth at max settings. Ran better then expected, so if the full game is even better optimized, that will be fantastic.",Positive
AMD,3700x with a 1070 was running the game fine in beta  im scared of larger maps. the 3700x was maaaaxxxxed out,Negative
AMD,Pray for my 1060,Neutral
AMD,Looking to get a balance between recommended and ultra with my 7800X3D and 3070.,Neutral
AMD,Nice should be able to hit 140fps with my 9800x3d and 9070xt natively full ultra in 1440p,Positive
AMD,SecureBoot and my GigaByte motherboard *did not get along* when I tried to play during the open beta. Does anyone know if GigaByte boards have had a fix issued?  I am using a Gigabyte X870 Aorus Elite Wifi 7,Negative
AMD,I would be nice if the game has a benchmark tool,Positive
AMD,So somewhere between High and Ultra.,Neutral
AMD,"These actually seem pessimistic.  Intel 12700, 32 GB DDR4, 7900 XT  I was able to run the beta at 1440p high settings around 100 - 144 FPS with a 10% power limit set on my GPU",Neutral
AMD,I'm cumming,Positive
AMD,"I'm not gonna play it, but this is how a spec sheet SHOULD handle upscalers.     If I was a AAA gamedev, I'd also want to put a 2x5090 column on there with highest settings, 4K or possibly 8K, DLSS kicked all the way up, lossless scaling on, and just see what numbers I can hit.",Neutral
AMD,"Damn, not only did they say what settings and FPS to expect, but also managed to not make the game extraordinarily hard to run.  Seeing how the recommended and minimum are GPUs two generations old, and some of which are very popular in the Steam Hardware Survey, it's a surprising breath of fresh air. Now, let's hope the game is actually well optimized and that the 1%lows won't be horrendous.",Negative
AMD,"Nice to see I meet the bare minimum for ultra, on 1440p 144FPS, on Medium settings, wtf happened",Positive
AMD,"Damn, I'm barely in the recommended column.",Negative
AMD,So a Core Ultra i7 and 9700XT would land me somewhere on Ultra+?,Neutral
AMD,7900xt + 5800x3d on 1440p maxed and it was around the 100s fps during beta  this seems a bit exaggerated,Neutral
AMD,Beta was so good i went from a 3070ti + 5600x to a 5070ti + 9800x3d. Im so ready.,Positive
AMD,what is hardware accelerated gpu scheduling?,Neutral
AMD,"I ran the beta with a 3080ti at 4k, DLSS balanced, medium-high settings and hit a solid 180FPS.",Positive
AMD,Yeah of course I need a 7800x3d for 4k 60fps. And 9800x3d and 285k are in same league. They are trying to show their old engine like something new. A modern game that needs latest hardware to run well. The game is not looking that good and sure not requiring that much spec.,Negative
AMD,Was going to get on pc but I have a 4070 which will put me in the 1440/144 medium settings group.,Neutral
AMD,This is a super reasonable spec sheet,Positive
AMD,I ran the beta at a mix of medium/high with a 2080 super. I was gonna build a new machine this year but I think I'll wait a little longer.,Neutral
AMD,"I know why they need HVCS but I’m sure they can find a way to open the door for mainline kernels that are built on bare metal.  One day gamers, one day.  Until then the dual boot remains necessary.",Neutral
AMD,Me hyped with a 7900xtx and a 7800x3d in 3440x1440p,Positive
AMD,Can't wait to test this in real world settings,Positive
AMD,Wow I dont often see 32GB as a recommendation. Happy to know my GPU and I will still be going strong for a while,Positive
AMD,I cant even put my poor 1660 ti thru this.   Ill get it on console while I save up for a new pc build.,Negative
AMD,I was super happy how well the beta ran so I feel pretty good about these specs. I averaged around 80-100 fps on high/ultra at 1440p with DLSS quality with a 3080 and a 10700k @ 5.2ghz.,Positive
AMD,EA has done literally everything right with this launch. This is exactly how I want to see spec sheets,Positive
AMD,So torn to get it for PC or PS5…I like the idea of using my portal on the go but also playing max settings on pc,Neutral
AMD,can someone explain why people prefer native? I used DLSS for Rivals and it seemed fine latency wise? lol new to pc gaming so wondering if im just missing something,Neutral
AMD,Ran lovely at 4k on my 5080,Positive
AMD,where would i9-10850k and 3090 founders land at 1440p,Neutral
AMD,I was getting 90-100 fps at very high/ultra settings on 7900xtx and i7 11700k at 4K native in the beta,Neutral
AMD,"I have a 4070S with a 12400 and 16GB 3200MHz, my motherboard is a really basic model.   Should I aim to change to a DDR5 setup? Maybe switch mobo+processor+dram kit?",Neutral
AMD,Definitely wrong subreddit but how does BF6 on a PS5 compare?,Negative
AMD,Wondering how my 9070xt + 9800x3d hold up at 1440p ultra,Neutral
AMD,"Well, let’s see how the good old 5800x3D handles this with a 5090 on 5120x1440",Neutral
AMD,A godsend after Borderlands 4.,Positive
AMD,Is a 285k really needed over a 12900k?,Neutral
AMD,Will a 5070 ti 16gb-vram work with ultra++? All the other specs i have but not a 5080,Neutral
AMD,Played the beta at 1440p with a 2070s no issues,Neutral
AMD,Hmmm.. I wonder why they didn't show Native for Ultra++. I have 9800x3D and 4070 TI Super. I think I could probably reach 1440p 144FPS with med using DLSS.,Neutral
AMD,Love this. But how come they only mention DLSS for the ultra++? I used DLSS on my 4080 super and it gave me ~120fps in high settings,Positive
AMD,Which means a Ryzen 7 9700 and RX 9060 should do just fine at 1080p75 high/Ultra with NO raytracing.,Positive
AMD,Does anyone know if they are the same as in the beta?,Neutral
AMD,Is the fps cap still 200fps?,Neutral
AMD,"Here's to hoping my 12900k and 3080ti will reach 100fps in 2k....""praying to the gods""",Positive
AMD,"I ran the beta on a 1650, 30 fps on low (with semi-frequent drops) but I was amazed it launched at all.",Negative
AMD,So 5080 with a 5800x3d will struggle at ultra for 4k?,Negative
AMD,Just got my new 9800X3D and 9070 XT PC a few days ago :). Yippee,Positive
AMD,I had around 150fps with a 3080 and 5700x3D @1440p low settings. Had to write a cfg that all my cores were used.   I hope the final game runs similarly. I really don’t want to fully upgrade my pc now.,Negative
AMD,I mean the beta showed that even iGPUs in mobile chips are good enough for low settings,Positive
AMD,What what makes it ++  I’m sure my 5070 ti with a little Oc will do the trick,Neutral
AMD,"My Ryzen 5 3600 and my rtx 2060 6gb ran the game better than this on the beta, they might be underselling it",Positive
AMD,"Look Capcom, this is how you optimise you're games",Positive
AMD,Anyone had  3060 ti in beta ? If so how did it go I do have 1440p monitor,Neutral
AMD,Seeing my baby boy on that list makes me happy,Positive
AMD,"I got a 6700 xt r7 5700x, so by this chart, I should only be getting 60 fps at 1440p on high settings?",Neutral
AMD,Randy Pitchford quaking in his boots,Neutral
AMD,Most likely a reach but would a stable 60 with drops to 50 when a lot is going on feasible?  3050 6GB i5-14400f 32gb of Ram,Neutral
AMD,"Be chill brothers, I had played the bf6 beta in my Ryzen 2700x/ GTX 1660 Super and hit 55-60 ... Even 70fps while I was streaming Discord and clipping in Medal. Also I don't remember the settings but I didn't look it too much, I'm pretty sure I was playing in 1080",Neutral
AMD,Here I am with a 1080 and 7700k trying to play this at 1440p and 100+ fps 🤣,Neutral
AMD,"Honestly they might’ve underspecced this list a bit, I was getting 80+ FPS@1080p with a 2070 Super lol",Neutral
AMD,Ultra++ gang where u at,Neutral
AMD,"The Beta ran fine on my undervolted 7900xt and Ryzen 7500F @ 4K native with mixed settings. The game still looks great with everything low except Textures and mesh on ultra/max whatever.   The only way I will SIGNIFICANTLY increase performance is by buying a better CPU.  Game is well optimized, which is rare these days.",Positive
AMD,16gb vram? Damn....,Negative
AMD,Love to see them using native res for everything below 4k.,Positive
AMD,"This is very reasonable tbh, I kind of forgot what that feels like.",Positive
AMD,I played the beta on Ultra 1080p on a RTX 5070 with Gsync on was rly great,Positive
AMD,"Played beta at 1440p with core ultra 265k and rtx 3080ti , at high settings, never dipped below 100fps",Positive
AMD,"I have a 7900XTX, 64gb DDR 6000 CL32, 9800X3D, so very much their Ultra Performance Specs - I played on Medium/low - I did not get near 144 FPS, more like 100-115.  I hope this means we get a good performance boost in the real game....",Positive
AMD,"Gotta see release version benchmarks, i have tough times believing this graph. I barely got 50-60 frames on 7 5700x and 3070 in the open beta on lowest settings, doubt they polished it that good to run on high",Negative
AMD,Cant be right... 4k 60fps native on a 4080? I mean sure but ill believe it when i see it,Neutral
AMD,Ran the beta fine with a 1070,Neutral
AMD,Well my 5080 will do good.,Positive
AMD,TPM enabled is required to play a game?,Neutral
AMD,it's tough seeing my 2080 near the bottom of the board but I played the beta and it ran well enough.  she still has plenty of games left in her future,Positive
AMD,"5700xt, 3600, 16GB ram. Was getting a very stable 80-100 FPS. These minimum specs are being extremely modest. The game is super optimised.",Positive
AMD,why do I need windows 11 at all ?,Negative
AMD,Cries in 6600xt,Neutral
AMD,i have i7 4770 😭,Neutral
AMD,Damn a 5070 ti can’t max settings  this game on 1440p?,Negative
AMD,I have a 4080 super... with a i7 9700k... im fucked,Negative
AMD,A modest requirements but me with a GTX 1050ti laptop cant do it,Negative
AMD,Ok so I played the beta but it was on PS5. I plan on buying the game on PC but I hear you have to change settings in your PC regarding the secure boot to play it. Does someone have a simple YouTube video that can explain it or do you even have to do it anymore?,Neutral
AMD,i think this is reasonable,Neutral
AMD,We specifying minimum ram speeds now?,Neutral
AMD,well i hope my 5950x can run this well lol,Positive
AMD,I love how specific the descriptions are!,Positive
AMD,EA is taking the right moves again,Positive
AMD,Based off what I played the 9070 fits in Ultra,Neutral
AMD,"As a 7800XT owner, I feel ignored",Negative
AMD,I got a buddy that played the beta on a 1660ti and played on mostly high setting with some on low and still got 60fps on average with dips to 51fps in heavy combat.,Neutral
AMD,I have 6750xt with i513400f can I play with decent fps at 1440p med settings? I didnt get the chance to play the beta.,Neutral
AMD,I think they give the  absolute worst case scenario I mean if they said you can get 1080p 120fps on high settings on your build and you get 90fps you will be disappointed but if they said you get 1080p 60fps on medium and then you get 90fps on high you will be impressed,Negative
AMD,5080 overclocked. Open beta ran 190fps all maxed DLSS performance 3440x1440 so i assume these specs are NOT meant for native,Negative
AMD,What bullshit comparison is this? You can't advertise something as 4k resolution then say underneath you're using upscaling and frame generation when you aren't using those things on the other settings.  A 5090 will run this above 120 frames native anyway so why not just say ultra++ is 5090?,Negative
AMD,Tpm.. Guess I'm out,Negative
AMD,Game is very well optimised and i played on RTX 2060 Super paired with 5700X3D. Don’t think it would be a problem for people.  Upgraded to RX 9060 XT just to enjoy the game at its full glory!,Positive
AMD,With rtx 3060ti nad i7 10700 worked great in beta. Really smooth.   1440p balanced/high from 70+ fps.,Positive
AMD,tf is ultra and ultra++,Neutral
AMD,Saw few videos that its playable well on gtx1080?  I have i7-6700k + gtx1080 oc. What are my chances ? Will I be able to play or should I upgrade my potato to atleast rtx3070 or a bit up. Also Mb is assus z170 pro gaming. Meaning cpu is maxed out on this mb and I cant put any other.,Neutral
AMD,I was surprised by how well optimized the game was running in beta,Positive
AMD,Geez i need a new computer,Negative
AMD,It even lists RAM speeds?,Negative
AMD,Does having Win10 instead of Win11 affect performance enough that it is noticiable?,Negative
AMD,I bet that i can somehow run this baby on a 1650,Neutral
AMD,Played the Beta with a R7 5800X3D and RX6800 at 1440p Native with constant 120FPS,Neutral
AMD,I'm more worried about what settings to turn off in order to experience 0 motion sickness.,Negative
AMD,Looks like settings have shifted on 1440p at 144 hz from high to medium.,Neutral
AMD,2080 super still rocking until 5080 super.,Positive
AMD,The beta was so well optimized...,Positive
AMD,"Lol, RIP my poor 4060 trying to drive a 3000x2000 panel...",Negative
AMD,I got recommended.  Though my specs for recommended is a  bit better.  So Im predicting a more consistent 60fps at 1440p settings at high.  It's a 3080 5800x,Positive
AMD,played the beta at 140fps with a 7800x3d and 7900xtx  2k ultrawide on a mix of high or ultra (windows 10),Neutral
AMD,wow real actual specs and not just throwing 4th gen intel and 1050 ti in ? shocker,Positive
AMD,aw no 1080 ti,Negative
AMD,Anyone else ready for Ultra? Yeah boii,Positive
AMD,So can I expect 1920 @75fps with a 2070 super on low settings? Lol,Neutral
AMD,"Where is the 5090 spec! I have the hardest time getting settings for this thing.  I have a i9-10900 with a 5090, also my ram is 3200  I had no problems running the beta with everything cranked but it was not buttery smooth   Does anyone know what bottlenecks I should see, and how I could resolve them?",Negative
AMD,Wow I just went from a 3070ti to a 5080. Looks like I made the right choice.,Positive
AMD,will a rtx 4090 run this? /s,Neutral
AMD,Ultra it is!,Neutral
AMD,Somehow I feel bamboozled by them not including 9070 (XT) cards in the recommended specs.,Negative
AMD,Still good with a PC I built five years ago...,Positive
AMD,Im fucked,Negative
AMD,So I played the beta on a 1070 and was getting 60 fps. Not bad for a decade old card.,Positive
AMD,"i5 11400f and rtx3060ti, beta ran at 70-80 fps no issue. The CPU is a bit of a bottleneck, but, i can play steady 1440p 60fps with no issues",Positive
AMD,"They have specified UPSCALER settings - I’m not even mad, this is amazing.",Positive
AMD,Ultra++ here I come :D,Positive
AMD,"im fine all of this, but the CPU - i suspect my first gen i7 wont quite cut it",Negative
AMD,I have like inbetween minimum and recommended and I was running 60+ frames 1080p on medium to high graphics in beta.  Edit: mispelled minimum,Neutral
AMD,"the beta ran at 70-90 fps on my system with ryzen 7 5700x, 3060ti 8gb and 32gb ram at 1440p",Neutral
AMD,So..with i7-13700k n rtx-5070ti i should be able to sít at 90fps and above on ultra++ in 1440p? Without frame gen?? If thats true they cooked hard with optimization,Neutral
AMD,So my 2050 can’t run the game? But I played the beta with 60 fps? Uhuh,Negative
AMD,"Ah, my old nemesis, Secure Boot",Negative
AMD,I played the beta with my current specs and had zero issues running it at 90+fps at 1440p with high settings.,Positive
AMD,Battlefield 6audi,Neutral
AMD,"boa noite, ganhei um pc ryzen 5 5600gt com 4060, to na duvida se vai rodar bf6 vem, compro no ps5 ou pc?",Neutral
AMD,another unoptimized bullshit lol,Negative
AMD,Honestly I'm fine with Recommended specs so I'm good :),Positive
AMD,"Damn, Battlefield 6 specs are no joke! 😅",Positive
AMD,"9800x3d, 3080ti, 64gb 6000hz, 2tb m2 drive let's goooo     I'm hoping for 100+fps on medium with 1440p ultra wide.",Positive
AMD,How many fps for ryzen 5 7500f?,Neutral
AMD,"Fuck TPM 2   and bs invasive Rootkit KERNEL  Level Antichet (when server side AC solution already exist)   Ohh btw Battlefield 3 was so well optimized and still looks good and it run on  **2-CORE CPU!!!**  and 2GB VRAM GPU , 4GB RAM!",Negative
AMD,"Ryzen 5 7600x, RX 6600 XT, and 32GB RAM at 6000MHz, what FPS should I expect? 1080p",Neutral
AMD,Yeah holy shit i now know exactly what benchmark to set (balanced recommended is exactly my rig just about)  Very happy reading this lol,Positive
AMD,These boys are cooking man I love it.,Positive
AMD,"with ""Balanced"" and ""Performance"" Flavors.",Neutral
AMD,To people that played beta is there dlss? At 1440p dlss works pretty well and I'd love to squeeze some extra frames on my 240hz display,Positive
AMD,Never expected to hear that,Negative
AMD,*how honest,Neutral
AMD,Are these specs wildly inaccurate? A 12900k or 7800x3d?? How are they in any realm comparable CPUs? Q,Negative
AMD,It doesn't list what operating system you need though,Neutral
AMD,"Probably the best optimization I've seen for a game in a long time, I think I only heard about 2 crashes from everyone I knew playing it, and that's on a beta test",Positive
AMD,"Have a 1060 and i7 7th gen and it ran at 40-50 fps in the beta at 1080 low, its very well optimised",Positive
AMD,"I was geniunely surprised at how well it ran on my 5+ year old rig. Had it's issues but still, pleasantly surprised.",Positive
AMD,"i have a 4070super ti, 11900k, 128gb ram but im running it at 5760x1400p and was honestly surprised at how well it ran and looked for a beta.",Positive
AMD,"The only thing I found was that the game is very CPU heavy compared to most games of this type. Whereas I see 20-30% CPU usage in cod, I see 40-60% in this game.  People could be caught out if they have done the old upgrade path of ignoring their CPU for many years and just upgrading their GPU.",Neutral
AMD,My performance got totally fucked from first to second beta on 4090   No idea why    But wont be buying it now;(,Negative
AMD,I hope its as optimized as BF1.,Positive
AMD,What fps? Im on 2080s,Neutral
AMD,"""No issues"" can mean lots of things and what people consider an enjoyable performance level differs.",Neutral
AMD,"Same here   5700x3d+rx6600. Native 1080p high/medium. Stable 60fps, no stuttering",Neutral
AMD,"I have the exact minimum requirement GPU and CPU from this chart (RTX 2060 and Ryzen 5 2600), which is saying to expect \~30FPS at 1080p, which doesn't sound too good.  I'll get the game if I can expect around 60FPS but not if it's significantly lower.",Negative
AMD,"maybe you had better cpu, i played with 2070 and i5 7600 and had 30 fps avg",Neutral
AMD,Did see reports of people running the beta fine on a 1060. Ran with no issue on my 1070. Hopefully that stays true for final release.,Positive
AMD,Im just praying mine doesnt break before i upgrade🤞,Positive
AMD,my 1660 Super and i7-7700 ran it fine on low settings,Positive
AMD,No,Neutral
AMD,"It ran ok on my 1050ti, you will be fine",Positive
AMD,2042 pounded my 5600 into next year so I can't imagine BF6 will be much better. Unfortunately I missed the cheap AliExpress 5700X3D window so I'm just going to have to suck it up and deal with it until I have a good enough reason to switch to AM5 or something,Negative
AMD,BF has always been a CPU hungry game but thankfully a very well optimized one at that. It seems like once you're past a certain threshold that extra CPU performance doesn't make a huge difference. My 5600X3D and my 6700XT combo ran the game swimmingly at 1440p and I was GPU limited.,Positive
AMD,"How much fps did you get at what settings? I have not played the beta and I have a 2070 max q, I want to expect how much fps I might get.",Neutral
AMD,"3090 is ALMOST identical to the 3080 Ti (10,240 cores vs 10,496 cores, 320 TMUs vs 328 TMUs). The VRAM amount is the biggest difference, rest makes the GPUs 1% difference in performance.   I would still consider the 3080 Ti high end though, despite how modern AAA games may develop.",Positive
AMD,2 generations + 6 months away? Yea not really,Neutral
AMD,"It's been 5 years my dude. You're into ""respectable"" territory.",Neutral
AMD,This is the reason I sold my Strix 3090 and bought a 9070 XT.,Neutral
AMD,"It will be more than fine, I have 7900XT and ran super high FPS on max settings 1440p.",Positive
AMD,"I’m sitting with a 6900XT and I was looking for a target of 4K @120 but I think it’s going to take a fair few compromises to reach it, I don’t know if I should jump on the upgrade wagon or not. Considering I play Diablo 4 @ 4K as my main go to daily game I think I should just chill and eat the performance loss for the sake of just Battlefield 6, which I will only play once for the single player experience.   I don’t think your 7900XTX is that far off from getting the peak experience out of B6.",Neutral
AMD,Crazy how such a non-controversial comment baited the Nvidia fanboys.,Negative
AMD,"Same, feelsbadman.gif",Negative
AMD,It’s “Architectured to exceed 3.0Ghz” and comes with “AI Matrix Cores”,Neutral
AMD,"Feel that shit homie, i just bought a 7900xtx like 4 months ago and its already not the best card i couldve invested in",Negative
AMD,still ran mostly ultra 2k ultrawide just fine at 140 fps,Positive
AMD,Never was…,Neutral
AMD,Mine is. EVC2 current ratio mod and custom vbios. My 7900xtx has better raw performance than a 5090.   Idk about dlss/fsr because that shit is ass. I play at 4k only.,Positive
AMD,Where you up scaling a lot? Because that sounds like cap. I was getting 120-130ish fps on a 4070 at 1440p native all low settings.,Neutral
AMD,You're lying. We can see the benchmarks online you know? 😂   I have a 9060xt 16gb and it can barely maintain 120-130 fps at lowest settings in alot of situations. And that's at 1440p.,Negative
AMD,It is. That was a big talking point during the recent beta. Many people were happy to see a AAA game with strong optimization that ran well on even low end hardware.,Positive
AMD,"Yup, the game look great too unlike Borderlands…",Positive
AMD,"Depends on what you want to compare it to. People here always compare the optimisation here to other AAA games, that aren't multiplayer fps games.  It performs twice or more worse than BFV, whilst not looking close to twice as better graphically.",Negative
AMD,Same setup. Gonna play it on ultrawide if it sucks ill revert to 16:9 :),Neutral
AMD,Similar set up and can’t wait to try and get 120fps at 4k on my new mini led monitor with a 9070xt wooo,Positive
AMD,"Should probably upgrade that millihertz ram, a whole 6 Hz",Neutral
AMD,Same set up!! Except cl36 ram 6000mhz and a 34” ultrawide 1440x3440 monitor. This is my first pc so dumb question but will we be able to run ultra ++? Whats with the frame gen is it better than native?,Neutral
AMD,get ea play pro for 20$ month and then realize its not as good as promised and move on with life ;),Negative
AMD,Let that OG rest bro,Neutral
AMD,You’re probably a magician if you manage to run that cpu on modern games without everything lagging,Positive
AMD,GOOD NEWS!  https://youtu.be/BMjZ3SgviC8?si=5hKi7MCHd8J5F1HN,Positive
AMD,what gpu u runnin?,Neutral
AMD,"You can run minimum settings at least. Its comparable to the Ryzen 2600 in gaming, slightly faster if you increase your memory speed since DDR3 1600 is the main bottleneck on that CPU. Luckily the memory controller is crazy good and I could do DDR3 2400 on mixed no-name sticks.  TPM support is by far the bigger issue. Probably not supported on a board that old without adding an external TPM.",Positive
AMD,Anticheat,Neutral
AMD,from the looks of it you should have pretty good enough framerates in 1080p,Positive
AMD,"Cries in 5700xt and r5 5600x   I knew from the beta that i was barely getting stable 60 at 1080p low settings, definitely gonna have to wait to upgrade before I get the game",Negative
AMD,6600xt is fine here. Mine can push around 100 fps at 1440p with quality upscaling. Around 60 fps without but VRAM is the big problem. After a few matches you run into heavy performance drops at 1440p unless you run low settings. 1080p you're more than fine.,Neutral
AMD,"Don't worry lad, the 1080Ti for instance does 70-75fps at 1440p High FSR Quality. 6600XT is quite a bit slower, but should still be able to do 1080p High/Ultra FSR Quality at 70-75\~ fps too!",Positive
AMD,"These GPU requirements don't make much sense, the 2060 isn't a 1080p 30 FPS GPU it's more like 70-80 FPS at native, the CPU in minimum is way too weak for a 2060 in this game as it can only achieve 55FPS, 6600XT should easily do 1080p 90 FPS but the question is whether your CPU can do that.",Negative
AMD,Every modern game uses kernel level anti cheat software nowadays. 99.9% of people dont have issues. You will be fine.,Positive
AMD,"No you dont, works fine on w10",Neutral
AMD,Wondering the same,Neutral
AMD,Anti Cheat.   And good on them to display alone with the rest of the specs.,Positive
AMD,Nice,Positive
AMD,Larger and more cluttered maps maybe in the final game?,Neutral
AMD,"No problem, 1080Ti can handle BF6 1440P High, FSR Quality at 70-75 sometimes over 80fps depending on the action. Mild OC wouldn't hurt too!",Positive
AMD,It's fine on HDD.,Positive
AMD,"I tested it during the beta, and at least for me the input lag becomes negligible if you make sure to cap your fps",Neutral
AMD,"You can use MFG in any game, it looks and feels bad below 60fps input but above that it's fine",Negative
AMD,"Frame Generation should - paradoxically - only be used when your framerate is already good enough. To jump from 100 to 144 for example, and cap a 144hz monitor is a good idea.  FG added latency is negligible. The problem is that it doesn't offer the __lower__ latency that naturally comes from higher framerate. If your base / real framerate is 60, you will get the latency of 60 fps even if you stretch it with FG to 120.",Neutral
AMD,"When you do this, how do you go about scaling? I've tried using 1080p on my 1440p monitor and it just looks bad. Would really help me out if I could though my GPU is lower end(6600xt).",Negative
AMD,Why not just do 4k and DLSS?,Neutral
AMD,[https://nerdschalk.com/battlefield-6-keeps-crashing-on-pc-issues-fixes/](https://nerdschalk.com/battlefield-6-keeps-crashing-on-pc-issues-fixes/),Negative
AMD,"Be more specific please. I was playing on 5950X with a 7900 XTX, and was not having problems on my playthrough. In fact only one which I believe is Windows and multi monitor related with AMD, experiencing slight lag and stutter untill I hit the windows button.",Neutral
AMD,You can enable secure boot on Windows 10.,Neutral
AMD,I cranked my settings to medium/high and get 110-140 fps on a 3080 10GB. (1440p DLSS Quality). I know some people hate upscaling but personally I can't tell the difference in image quality.,Neutral
AMD,"Yes you can, requirements are super conservative.",Neutral
AMD,The minimum specs aren't even close. You can almost double the FPS here. I assume this is in the absolute worst case scenarios.,Negative
AMD,"Dont know if you tried the Beta, but i got 60-80 fps on low settings at 1080p with a 1070ti and i7 8700k",Neutral
AMD,They got like tutorial on ea website. But you should lookup your motherboard and check where to find it,Neutral
AMD,"My 9070XT with a 7800x3d was getting nearly the same performance as what the 5080 is listed at,so they will be getting more frames than that",Neutral
AMD,"Mouse and keyboard feels like shit on any Sony console. Feels like they have some sort of acceleration baked in.  On PS5 pro during beta it got around 80 fps in performance mode.   You could probably get more with DLSS, if you do a cpu upgrade.",Negative
AMD,It’ll be cheaper to get the PS5.,Neutral
AMD,I have pretty similar setup and got 70ish fps on high@1080p. pretty playable and looks good too! 1080ti rocks,Positive
AMD,"Our standards have dropped abit. Look at the discourse around optimisation on this game, the comparison is always to broken AAA single player games.  Lets just compare to BFV. BF6 in beta had 2-2.5x worse performance, does it look that much better graphically? It looks better, but not THAT much better.",Negative
AMD,Yeah,Positive
AMD,nope and wont be. that the fun of a consumer side poorly done security that already been crack (tpm),Negative
AMD,I didn’t check my FPS but it ran smoothly for me with an 11th gen i9 and 3060. Beta was so much more promising than the release of 2042.,Positive
AMD,"Afaik DLSS doesn't effect latency when frame gen is off, it only effects the way the game looks.   As to why they prefer native, well that's been the standard for a long time and this being changed over the years has been disliked by some people. While I have no experience with DLSS, certain people easily spot blur, visual artifacts and ghosting when using upscalers which distract them.",Negative
AMD,Personally I prefer DLAA which is native resolution but implements the anti-aliasing techniques from the DLSS suite. I also force the K preset transformer model on every game.    The upscaling doesn't really affect latency really at all. Most of the talk about affecting latency is for frame generation not upscaling.,Positive
AMD,"3090 is similar to a 5070 isn't it?  9800x3d and 5070 averages 150 fps or so at native and low settings. Around 120 fps at ultra. Based on a benchmark video I saw.  This game is really cpu heavy, so you probably won't get anywhere near that with what you have.",Neutral
AMD,On the PS5 pro it gets around 80 fps on performance mode. So it must be worse on standard PS5 I assume.,Negative
AMD,Mine help up very well. I dint know ow the fps but it was butter smooth maxed out. You can even play 4k,Positive
AMD,"Easy as bro, I'm on the 4070ti super with an am4 5900x in the beta I was cranking out 180+ at 2k on ultra, with dlaa I could only imagine they are playing safe so people won't complain.  I've moved to 4k and I'll be happy for Dlss pushing me twords 144fps, or enable frame gen for the whole amount of 160+.",Positive
AMD,"My son ran a 2070 super and a 5600x and got those frames, you'll be fine.",Positive
AMD,I did but at 1080. Ran med & low settings here and there. Played fine. Just kept my gpu very toasty.,Positive
AMD,At what res?? That should outperform my i5,Neutral
AMD,The list just shows the most common or officially supported GPUs. Doesn’t mean others won’t work. Devs can’t list every card out there.,Neutral
AMD,"Beta specs tend to be rough estimates. The game isn’t fully optimized yet, so requirements can be higher or misleading. Once the devs finalize the game and do wider testing, the official specs give a much clearer picture of what you actually need.",Neutral
AMD,High-end graphical settings.,Positive
AMD,"RTX 2060 is the target GPU performance for a smooth, modern gaming experience as intended by the developers. It's still playable with other GPUs, better, or equal to an RTX 2060.   Something below like a GTX 1050 Ti or GTX 970, you'll need to lower settings and accept lower frames.",Positive
AMD,"I could be wrong on this, can't quite find the original reference from memory.",Neutral
AMD,Sim — o seu PC vai rodar Battlefield 6 muito bem.,Neutral
AMD,"yes, no guesswork. Is it native? Is it quality? Or performance upscaling? Is it targeting 30 or 60 fps?  When they know the game is optimised and it will perform well they publish system requirements like these ones. When they know the game will run like shit they make it as confusing as possible.",Neutral
AMD,"Well except increasing the resolution typically only increases GPU load, not CPU load. So why are the CPU requirements different between different resolutions?",Neutral
AMD,Proc: core i5 or ryzen5  Ram: 💅  Storage: yes  GPU: Nvidia RTX or Amd RX,Positive
AMD,Yeah pretty close I'm on 1440p with a 10900k and a 3080ti so recommended balanced looks like it will be perfect I will probably get closer to 80-90fps if what they say holds true.,Positive
AMD,Same settings as me.,Neutral
AMD,"The thing is, they are actually underselling performance a ton.",Negative
AMD,"You're right, a 12900K is clearly *significantly* superior:  [https://www.cpubenchmark.net/compare/4597vs5299/Intel-i9-12900K-vs-AMD-Ryzen-7-7800X3D](https://www.cpubenchmark.net/compare/4597vs5299/Intel-i9-12900K-vs-AMD-Ryzen-7-7800X3D)  The lesson being, different games have different architectures and run differently on different CPUs. For BF6 the 3D cache clearly helps the 7800X3D make up for its weaker single-thread performance, but not so much that it's significantly better than a 12900K.  Dependence on raw single-tread performance with less cache utilization is very common in multiplayer shooters btw.",Positive
AMD,They test with what they have and give approximations,Neutral
AMD,This can happen. Architectural difference can massively boost performance in a vendor. We have that in CoD in GPU side. Some games can/will loves some CPU/GPU vendors over others.,Positive
AMD,As someone with a 12900k i was wondering the same thing lol,Neutral
AMD,"I had something like 3 crashes back to back in the span of 10 minutes,but other than that the game was extremely fluent at 60+ FPS even with a lot going on,and i was on Performance mode with many things set to high so safe to say that this game is very well optimsed   (I have a 3080 and 16GB of RAM)",Positive
AMD,"I had crashes about 50% of the time a map would load. But once I got in, 0 performance problems. I expect the crashing issue to be resolved by launch so I’m not worried about it.  Game definitely deserves to stand alongside Half-Life 2 in the extreme scalability department.",Positive
AMD,First thing I noticed was how good the graphics were despite it running very well. It really manages to be beautiful with good frame rates.,Positive
AMD,KCD 2 had pretty good optimisation,Positive
AMD,I had maybe 4 or 5 crashes in about 8 hrs of gameplay across the 2 betas. Not bad but a little annoying,Negative
AMD,You'll be fine. My gf's 1070 was able to keep a steady 75 fps on low 1080p and it still looked good.,Positive
AMD,I was rocking a 2070 super and I was hitting 60fps,Positive
AMD,"My Titan V did 1440p Native on High Settings, respectable framerate, smooth too. Titan V is slightly ahead of a 5060/2080Ti",Positive
AMD,"That's what I'm on and it ran beautiful at high settings 1080, low-med setting 1440",Positive
AMD,Love how a common sense normal comment is downvoted. This sub should be renamed to PC casual race asap.,Negative
AMD,"I did run the whole beta with my 1060ti and the game run just fine, biggest problem I encountered was that my audio sometimes stayed muffled the entire matches like in the menu screen. none of my friends had the same issue with newer PCs.",Positive
AMD,"when it comes to battlefield games you will always play it on hardware that is lower than minimum and still have a good time, just frostbite wizardry.",Positive
AMD,what about rx 570 4gb ?,Neutral
AMD,There a cfg file you can place in the games directory that REALLY helps 6 core CPUs. I'm running 9600x and that was struggling a little too until I placed the file in there. Went from 80-100% utilization to 60-80% in heavy scenarios. Still high but it's expected this game really loves high cores. When I'm fully CPU bound I think I can push around 140-150 fps.,Positive
AMD,"Im in the same boat, regretting i didnt spend the €200 then. New am5 would be around €700-800",Negative
AMD,DLSS was set on balanced with Low/Med settings and I was around 70 fps I think,Neutral
AMD,"Today is just a 70s. 5070 Ti performs already 20-30% better. (Expect few games, its more like 30+ games avg).",Positive
AMD,"Time sure flies.  Though I suppose if you don't really play modern g ""AAA"" games it's no big deal. So long as it can play the games I want at the frames I want.",Neutral
AMD,https://preview.redd.it/75u3hx3isrtf1.png?width=600&format=png&auto=webp&s=09a1574db4e49c33392b0f44ba9ff42ebc9f5d7c,Neutral
AMD,Same. Capped 120 FPS on 1440p and that was stable with a 7900XT. Played Ultra.,Neutral
AMD,Lemme guess you’re a fan of upscaling,Neutral
AMD,"> EVC2 current ratio mod and custom vbios.  Got any videos for me to learn more about this? I won't set my house on fire because I don't plan on doing any mods myself, I'm just curious how one unlocks *that* much more power out of a card",Neutral
AMD,oof the delusion is strong with this one.,Negative
AMD,Probably fsr balanced,Neutral
AMD,3440x1440 also here,Neutral
AMD,ultrawide support was mentioned in the PC launch trailer. you're good to go. I have a samsung G95C so i'm in the same boat.,Positive
AMD,"5070 ti is pritty close to 5080, so yeah were able to run ultra++.  Frame gen is nice, but i wouldn’t use it in Multiplayer. It adds extra latency, i only use it in single player games.   Cyberpunk all maxed out + pathtracing + frame gen is pritty awesome.",Positive
AMD,I’m poor 🥲,Negative
AMD,"I’ve just been replaying older games I haven’t played in awhile. And a lot of ready or not, bannerlord, ground branch, BG3. Surprisingly those games run well, but I do lag a lot during online play. I always thought it was my internet, but would my shitty CPU be causing lag as well?",Neutral
AMD,I was using that CPU up until 2023. It's not as bad as you think. Only thing I was really limited at was VR.,Positive
AMD,"I played it on a 5500XT and Ryzen 5 2600 and was hovering between 45-60FPS at native 1080p low, no way your system was that bad, it should be doing 80-90 with a bit of a CPU bottleneck.",Negative
AMD,Thanks... Even if I can't help but still feel bugged out by this method...,Negative
AMD,So then I would think it's kernel based one. I hate that.,Negative
AMD,I have the gtx 970. Will the game even run? Lol,Neutral
AMD,What about gtx1080 oc with i7-6700k ? Its potato but will be fine for me on 1080p low :) just thinking should I preorder or no.,Neutral
AMD,What'd you cap it at? Monitor refresh rate? Something lower?,Neutral
AMD,1080p scales perfectly with 4K so it’ll look fine. 1080p on a 1440p monitor doesn’t scale well so there isn’t any way of making it look good unfortunately. You should just use 1440p.,Negative
AMD,That’ll be the first thing I try. I had a 1440p monitor in the beta that gave out on me in September so I’ll do some experimenting.,Neutral
AMD,"Im using a 9070XT with a Ryzen 79800X3D, 32 Gb of ram, and im not sure whats going on as i have over the recommended specs. The game crashes every 45 min or so.",Negative
AMD,"I didn't unfortunately as I wasnt sure how to fix the secure boot stuff. That's sorted.now though, good to hear!",Negative
AMD,Probably it’s because AMD historically is great on CoD  + (at least) 6000mt/s Ram instead of those 4800 non-expo/xmp,Positive
AMD,Appreciate the information.  Thank you.,Positive
AMD,"My comment was for fun only, of course it will run.",Neutral
AMD,Yeah I’m just excited it ran 100x better than Borderlands4.,Positive
AMD,![gif](giphy|j6uK36y32LxQs),Neutral
AMD,"adding to the first part, is that framerate the average framerate? the 1% high (never thought I'd say 1 percent high)? is that framerate in the most intensive area? or is it in the least?",Neutral
AMD,"\> make it as confusing as possible  Maybe not even the devs know how the game will perform (lack of testing, too much variation on FPS, etc)",Negative
AMD,"These aren't sorted by resolution, each group has two different resolutions - the categories are just examples of typical PC configurations (including typical CPUs) and typical resolution and framerate combinations those configurations might be running.",Neutral
AMD,you'll get more depending on your RAM speed - I have the same chip and upgraded RAM and OC'ed/timed it best I could and the results were very worth it.,Positive
AMD,Good. Undersell overdeliver should be the standard for quality products.,Positive
AMD,"Yeah they list 1440p high at 60 fps native for 3060 ti, but using DLSS quality, I had over 90-100 fps with high settings. And probably can get a lot higher just lowering a couple settings to medium or DLSS balanced",Neutral
AMD,it might be due to map size as well. All the beta maps were pretty small for battlefield standards. Maybe the bigger maps have a slightly lower performance,Neutral
AMD,100%. I have a 6600xt and a 5800x and I don't really see my frames drop below 100 that much on low settings 1080p. Runs like a dream.,Positive
AMD,"I think I was getting about 120 fps with a 9800X3D, 9070XT and 32Gb of RAM at 3440x1440p",Neutral
AMD,"Lmao, you actually think synthetic benchmarks are the same thing as real world performance. That website has the [9800x3d as 30% slower than the 14900k,](https://www.cpubenchmark.net/compare/5717vs6344/Intel-i9-14900K-vs-AMD-Ryzen-7-9800X3D) yet actual benchmarks have it running BF6 [30% faster.](https://www.tweaktown.com/news/107068/battlefield-6-bench-9800x3d-is-30-faster-than-14900k-at-1080p-hardly-any-difference-4k/index.html)",Neutral
AMD,Who in mind thinks that 7800X3D is worse than 12900K ?!?!! And who uses cpubenchmark to test which CPU is better for gaming?!?!?!!,Negative
AMD,"The 3D cache on the 7800X3D does make it noticeably, if not significantly, better at gaming than a 12900k, especially in Battlefield. Synthetic benchmarks do jack shit to show gaming performance for a 3D V-cache chip vs a non 3D chip. And multithread is the worst comparison to make, which is the only part where the 12900k is significantly better.   https://youtu.be/78lp1TGFvKc?t=918 This is vs 13900k so let's add the comparison of that chip with the 12900k  https://youtu.be/P40gp_DJk5E?t=1049  https://youtu.be/B31PwSpClk8?t=800 And from a different channel.",Negative
AMD,"For a processor that was only 18 months older than the 7800x3d, and double the power budget no less while costing around 100 bucks more at time of release, i would certainly hope it would be better. But the fact that it is comparative in bf6 i guess shows an interesting dichotomy that we are going to see an increasing reliance on optimisation to make more use of on-die cache.  And while yes traditionally multiplayer shooters tend to be less cache and less multi threaded dependent, BF has long tended to be the exception to the rule due to the physics of the destruction",Neutral
AMD,"I was also crashing very frequently for most of the beta (every 20 minutes or so), I only figured out during the second last day that I was running out of RAM. This had never happened before on other games so I never bothered to check if it were my RAM or not. I fixed the issue by increasing my Page File size so when my RAM reaches full capacity it spills over into my system storage; This didn't hurt my fps at all fyi, it just completely eliminated my crashing issue full stop. If you plan on playing BF6 on release and they still haven't fixed the RAM consumption issue, I'm like 90% sure this will fix your issue if you haven't already. Also on a side note, I have a 3060 Ti and a R7 5700X3D, and boy was the game terrifically smooth! for the first day or two (despite the crashing), even on 80-90 FPS it genuinely rivaled the smoothness and polish at that fps that I get at like 120-144FPS on other games. My 1% and 0.1% were impressively high so that's likely why the game felt so smooth, though I eventually switched to Low settings with DLSS4 quality for the remainder of the beta since even low settings looks great.  Edit: Forgot to mention that yes, I as well have only 16GB of RAM.",Negative
AMD,I think I was either high or ultra settings with a 3080ti and 40GB of ram,Neutral
AMD,"Silly billy, tell her to crank it up to 1080p High and turn on FSR Qualtiy and she'll no doubt get 50-60fps",Neutral
AMD,wow thats amazing! I've read that people with bricks could get out a good ammount of fps! What a game!,Positive
AMD,Best news I’ve heard all day about to attempt with a 1080 tonight,Positive
AMD,"What's a respectable frame rate bro, for some people it's 40. You typed all that without dropping a number.",Negative
AMD,Nice good to hear seems its decently optimized,Positive
AMD,"Yes. ""No issues"" literally says nothing. Like does the game just start up no issues, or are you running it at ~100 fps no issues?",Negative
AMD,mind posting this cfg file info or link?,Neutral
AMD,What is this sorcery? My 5600x could use the boost,Negative
AMD,Having a high CPU utilization isn't a bad thing unless your GPU is being heavily bottlenecked.,Neutral
AMD,"5070 Ti is priced $150 more than the 3080. 4070 was 30% faster than 3070 while costing $100 more (so aside from L2 cache, really you PAID for those performance gains rather than new architecture advantages) and the 5070 might have dropped $50 (still $50 more than 3070) but currently is only 10% faster than a 4070.  I wouldn't consider modern xx70/xx70 Ti to actually be that when it's priced like xx80 or has weaker gains than previous generations.   NVIDIA purposely changed things up with the RTX 4000 series to confuse people on what mid-range and high-end actually mean. There is a reason the 4060/4060 Ti were barely beating the 3060/3060 Ti (and sometimes losing).",Neutral
AMD,Wise word,Positive
AMD,"100%.  But also: a 3090 is 5070-class performance with double the VRAM, and in the GPU market of 2025 the 5070 by itself is borderline high-end, so a comparable GPU with double the VRAM is definitely still high-end until the 6000 series rolls around.",Positive
AMD,"Exactly. I don’t play modern games right now, there ain’t much worth playing anymore, I play mostly Indie Simulation games, my 7800XT does me fine, I do need a new CPU tho.",Negative
AMD,I have a 7900XT and was playing at 4k with FSR Quality and medium/high settings. 90+ FPS the whole time. Shit was glorious.,Positive
AMD,"Don't have to be a fan of upscaling. The 7900XTX was third best at release and is now even further behind. Still an amazing card but it was never top of the line, especially not for features",Negative
AMD,[https://elmorlabs.com/product/elmorlabs-evc2se/](https://elmorlabs.com/product/elmorlabs-evc2se/)  [https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-131](https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-131)  [https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-409](https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-409)  [https://www.overclock.net/threads/brand-new-method-how-to-unbrick-flash-almost-any-card-amd-or-nvidia.1612108/](https://www.overclock.net/threads/brand-new-method-how-to-unbrick-flash-almost-any-card-amd-or-nvidia.1612108/)  [https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-36?post\_id=29156997&nested\_view=1&sortby=oldest#post-29156997](https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-36?post_id=29156997&nested_view=1&sortby=oldest#post-29156997)  [https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-114](https://www.overclock.net/threads/official-amd-radeon-rx-7900-xtx-xt-owners-club.1802706/page-114),Neutral
AMD,Less goooo! Similar specs,Positive
AMD,Thank you!,Positive
AMD,🥲,Neutral
AMD,"Well, I had an i7 7700 and at some point I’ve noticed my cpu was at 100% most of the time when gaming and nothing would load on my second monitor. So could be it",Neutral
AMD,"You have no idea what you are talking about lmfao, my cpu is 100% better than my GPU and my in game telemetry said so, CPU frame time was around 10ms and GPU was at 16ish   Some maps was 70-90 some maps 50-60ish",Positive
AMD,You’re going to have to get used to it. It will become more and more common.,Neutral
AMD,"Hmmmmm, depends. Likely 60fps if 1080p Low",Neutral
AMD,"Well since that GPU is slightly ahead of a 3060 8GB, I recommend 1080p Ultra w/ FSR or 1440p Medium with FSR Quality",Neutral
AMD,"Patience is a virtue. We recommend you wait for reviews and updates! Check [IsThereAnyDeal.com](http://isthereanydeal.com), [CheapShark](http://www.cheapshark.com) and install [Enhanced Steam](http://www.enhancedsteam.com/) to find the lowest price available. Additionally, Steam Early Access games may be very buggy and there's always the risk that developers do not deliver a finished product, so always keep that in mind. Download demos when you can to make sure the game will run on your PC. Pre-ordering a game or buying Steam Early Access is, however, a valid reason if you really want to support the developer or perhaps even get ahead of those darn spoilers. Consider resisting temptation and play with the [/r/pcmasterrace Steam group](http://steamcommunity.com/groups/steampcmasterrace) today!   ***  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/pcmasterrace) if you have any questions or concerns.*",Positive
AMD,Ah damn alright I'll stick to 1440p with upscaling then.,Neutral
AMD,I kind of get it but I still don’t see why you wouldn’t just do DLSS quality on your 4k monitor. What’s the refresh rate on it?,Neutral
AMD,Of course.,Neutral
AMD,pretty sure its the average frame rate.   I have a 9070xt (not 100% sure if I upgraded from the 5600 to the 5700x3d before or after the beta) and played at 1440p everything high or max settings and was getting at least 100 fps,Neutral
AMD,Most likely average fps throughout a match. Campaign will probably run a bit better depending on level and scenario since more players = higher cpu usage. 1% lows I think I've never seen before on a settings sheet and only seen it in benchmarks.,Positive
AMD,"1% high would be an utterly useless thing to measure. 1% lows at least measure how much stutter there is, but 1% high measures what exactly? That 1% in the benchmark run that has mostly the skybox in it?",Negative
AMD,i doubt that. You dont spend years on a game and have no clue how it performs.,Negative
AMD,"But PACKED with detail and geometry and and all the players in a relatively small area so absolutely chaotic in terms of destruction and debris and special effects happening right in front of your camera. The larger maps, are more “empty” based on gameplays I’ve seen, and there aren’t tons of action happening simultaneously in front of your due to the players being way way waaay more scattered.   I think performance will be similar",Negative
AMD,"9800 and 7800x3ds are much better for gaming, he is saying how good they are as actual cpus, not just for gaming.",Positive
AMD,I was being sarcastic to make a point about comparing CPUs by a single metric when real world performance is inevitably more complicated. As I explained in the second paragraph of the comment you're replying to. Come on now.,Negative
AMD,"Comment saved with all this meaningful info!  And yeah,i expected to be just another performance slop but oh boy when i saw that i was making more than 60 FPS on Cairo with high settings while everything was being blown up,i felt very excited",Positive
AMD,"Yeah probably,damn i wish i had more than 16GB of RAM lmao, it's on the list of the upgrades bringing the total to 32 GB.  Still,i'm super impressed at how smooth the game ran with that amount of RAM considering that less realistic games run at a fraction of the framerate with less graphical settings",Positive
AMD,3060ti + Ryzen 5 3600 and my performance was better after turning settings up and putting more load on GPU. Probably because my CPU is bottleneck.,Positive
AMD,I like how you think! Gotta try that out this weekend.,Positive
AMD,"Well, I got 80-90 FPS 1440P High Native Resolution, while on High I got 90-95\~ on a Titan V",Neutral
AMD,"Yeah, also a 1080Ti which is roughly your cards performance gets 60-75 fps on 1440P High with FSR Quality turned on! Pretty neat honestly",Positive
AMD,Mind you I'm running a 9600x. I'm not sure how well a 5600x is going to pair with this but you should get some form of help from it.,Negative
AMD,Which is the problem with 6 core CPUs in this game. Dropping everything to low and lowering resolution did nothing until I used the file. CPU was pegged at 100%. I'm still CPU bottlenecked but it's much more manageable and not limiting me as much.,Negative
AMD,"There was a significant leap on 40 series.  The 5070 rn is like a 4070 Ti (non super) which performs like a 3080 ti - 3090. 3090 Ti is still stronger, btw 5070 and 5070 ti, some cases even better.",Positive
AMD,Actually insane to think a AAA shooter is looking this good and still optimized well. There are worse looking shooters performing much worse. And there are single player games performing much worse. Looking at the UE5 slop especially.,Negative
AMD,"I’ll take what I can get, until I convince myself to get the much needed upgrade.",Positive
AMD,![gif](giphy|5xtDarmwsuR9sDRObyU)  Going to preorder. :),Positive
AMD,That’s going to be what I do first and hope for a triple digit frame rate. I have a 240Hz display.,Positive
AMD,With frame gen during the beta I was getting pretty steady 140-165fps with almost everything at ultra and dlss quality at 1440p. This was on an 11700k and a 4070ti so I’m hoping performance will be similar… this chart makes it seem like it will not.,Positive
AMD,"1% high is the top 1% of the fps, the opposite of 1% lows.",Neutral
AMD,"I meant that they don't how it performs on every cenario (where in the game, where looking to in the game, all the GPUs, all the CPUs, etc).  For example, in  Fallout4, if you look to the ground outside the main city you will get a very good FPS, but if you look to too many buildings inside the main city, you might get half what you had outside.  Some games, even if you follow the same steps, you might get different results on FPS due to random objects.  Theses tests give little information if you do them while developing the game, because future changes can throw away all tests done in a previous build, then new tests must be made with many combinations of GPU, CPU, SSD, RAM, ingame configurations, etc. Also, doing development all the configurations that make Medium and Highmight change, Low and Ultra usually mean lowest and highest, which **usually** doesn't change.  Given that few games get released with minimal bugs, I really doubt they fully test it before launch.",Neutral
AMD,"I KNOW RIGHT! I've played BF1, BF5, BF2042, and BF4 and 3 on my PS4, so I've gotten used to the great optimization that Battlefield always seems to deliver, and so I was also expecting the same for BF6, and that they most certainly did deliver on, though it still impressed me for two reasons; For one, I felt like the volumetrics were a huge step up from previous titles and you'd think that it would affect the performance exactly like how you mentioned in this comment, yet it didn't! Secondly, I had become so used to the awful performance being delivered from modern titles nowadays, coupled with my very low expectations before the beta, that despite the fact that Battlefield always delivered with great optimization, I genuinely thought that this game would be the exception, but no! So yeah, I'm very pleased with this game and I can't wait for it to release!     If what I said previously does work when the time comes, I'm glad to have helped!",Positive
AMD,Oh I'm 100% sure EA knew they could not screw up this time or battlefield would be dead,Negative
AMD,I know what you mean. I returned Borderlands 4 after trying it. I was getting a *rough* 60 using medium/high settings (from Hardware Unboxed optimized recommendations) with FSR balanced at 4K. But emphasis on *rough* since the 1% lows were around 30-40 even when just walking through the world.,Neutral
AMD,"You can at least add frame gen too since you’ll be hitting an acceptable frame rate without. I know it’s a dirty word on this sub but give it a shot, it’s done well for me when I want to max out a HRR monitor. If you don’t like it you can always turn it off",Neutral
AMD,Why don’t you? Most of that chart is showing performance at native no upscaling,Neutral
AMD,"I was reading that wrong, I was interpreting balanced and performance as dlss options. I was thinking they weren’t even quoting quality performance but I see what’s going on now.",Negative
AMD,Its still mostly fine unless the games he plays are all CPU intensive games  Also if you are playing in QHD above then it matters even less. get 16GB VRAM GPUs btw,Positive
AMD,3080ti>5060,Neutral
AMD,"I'm gaming on a 1600AF in 2025 and having a whale of a time. As long as you match what you're playing to your hardware, you can game well on pretty much anything.",Positive
AMD,"The 3700x isn't a bad CPU at all.  As per gaming, it depends if you're playing the latest and greatest, which GPU you have, and what the settings are.  I would strongly recommend updating the BIOS to the latest version, just to be on the safe side.  Which CPU cooler is he running?",Neutral
AMD,We just upgraded my buddy with my old 3700x and a 2080. Old parts and some new parts from Amazon. 32gb ram. Ended up about $500 all told. Runs anything out today at 1080p no worries,Positive
AMD,He’s using a regular stock cooler and right now he is rocking a GTX 1070 right now and I’m planning on upgrade his power supply so I can put the 3060 as temporary so I can find a good deal with the 3080 TI,Neutral
AMD,"I would opt for a better CPU cooler.  https://www.amazon.com/dp/B0CZS1TWD6?th=1  This is purely to give the CPU as much cooling help as you can within a low cost.  With that said, could you share the motherboard, case and RAM specs?",Positive
AMD,He has 16 of ram DDR four I’m going to put 32 and his motherboard case. Let me take a picture of it when I get home.,Neutral
AMD,And his motherboard is on a.m4,Neutral
AMD,https://preview.redd.it/hx8q60pygstf1.jpeg?width=3024&format=pjpg&auto=webp&s=87ad4e8b11ee7a92cc952b81e6044326c9577544,Neutral
AMD,Did you invest in a fire extinguisher? lol,Neutral
AMD,The TUF is the weakest with a powerlimit of 335W/370W max. The Taichi is the most powerfull with 366W/404W  max.  The normal Nitro is 350/385. Dont know about the +  The other powerfull 9070XT is the XFX Mercury OC with 360/400.  I would get the Taichi. BUT it has that stupid 12v-2x6. But that should not be a problem with a max power of 404W.,Neutral
AMD,"Hardware unboxed tested all of them in many games and has graphs for temps, just look there,  I’m personally getting the sapphire nitro+, had great experience with their previous nitro+ cards, had a 5700xt, have  6700xt currently, great temps and OC undervolt capabilities",Positive
AMD,"Sapphire Nitro + for performance, XFX Mercury for better temp.",Positive
AMD,I always recommend to check ir out yourself.   Hardware unboxed has a great video where he compares the performance and temperatures of the different 9070xt models.  I went with the XFX Mercury OC. Great card. I run my fans at 35% speed and my hotspot reaches 77-78°C max.,Positive
AMD,Sapphire and XFX craft the best AMD gpus you can find. (The Nitro series of Sapphire has improved cooling and more performance),Positive
AMD,"XFX Mercury and Sapphire Nitro + are the best, but you may as well get some of the other models. Most of them are decent, so it is a matter of preference.",Positive
AMD,"Yeah, so i should probably stay away from TUF (because from what u said its just brand pricing). I also saw people saying that XFX and Sapphire make the best AMD cards but from what I read online the Taichi versions seem to be really solid too. I’m just wondering why the 12V-2x6 connector is such a problem",Neutral
AMD,Thanks for the info but what do you think about the Taichi? Is it actually a good card or should I just stick with XFX and Sapphire instead?,Neutral
AMD,"On the 9070XT it is not a problem, but it has a bad name, the 5090 uses it and they can melt on that card.. You do need to pay attention when plugging that sucker in.",Negative
AMD,"Well, these two brands i mentioned are the best in making exclusively AMD cards i've known so far (like EVGA and GALAX on Nvidia side)",Positive
AMD,hope the AMD noise suppression bug introduced in 25.9.2 is fixed.,Negative
AMD,I wonder what the performance is like now,Neutral
AMD,The game runs amazing on my 9070XT without this driver. Wonder if the upgrade is worth it.,Positive
AMD,"Will there ever be a fix for rdr2 vulkan memory leak? Vulkan crashes, dx12 stutters",Negative
AMD,It’s a preview driver,Neutral
AMD,"What do they even mean ""support for X""?",Negative
AMD,That thing never worked right.,Negative
AMD,Isn't it still like 10 bit and completely unusable,Negative
AMD,For me the quality has always been ultra ass,Negative
AMD,"""the game""?",Neutral
AMD,That's game issue. If i remember correctly I used some mod from Nexus mods to fix it completely,Neutral
AMD,Generally refers to specific optimizations,Neutral
AMD,"Unfortunately it got fixed, which is a shame cause I really liked the Wendie's-drive-thru mic and it was absolutely hilarious.  Now I have to try and figure out a way to recreate it with free mic-modification software, either that or find a shitty-mic artisan to custom make me a mic that, isn't just loud and low-bit, but has that extra crispy distortion too.  Kinda wish AMD noise suppression had a toggle switch to enable crispy mic mode for fun (or even better, have it as a second output, so proximity chat in-game is crunchy but discord comms are perfect fine).",Negative
AMD,Literally the only thing this driver has in it is BF6 support,Neutral
AMD,BF6 is the only new game listed in the changelog,Neutral
AMD,"“The Game”, is a thriller/mystery film released in 1997 staring Michael Douglas, and Sean Penn. it’s pretty good!",Positive
AMD,Pretty sure the driver is meant for BF6 and that's what he's referring to,Neutral
AMD,Then it needs to be named accordingly. <\_<,Neutral
AMD,"Yup, you can only use this driver for this one game. Going forward each driver will have a single purpose. We will need a separate computer for every app now.",Neutral
AMD,come on bro you know what he meant,Neutral
AMD,I'd rather like to see < 10 W embedded APUs again. At some point they had SoCs that were pretty interesting for network appliances and the like. Looks like AMD has all but given up on that segment. Maybe they need an actual small core again.,Neutral
AMD,For gaming hosting in the cloud? Or what,Neutral
AMD,Were their previous Ryzen Embedded series or is this a new market for Ryzen CPUs?,Neutral
AMD,"zen 5 in embedded chips, nice",Positive
AMD,And what about putting 3D cache on both dies?,Neutral
AMD,lost to arm at that end i guess,Neutral
AMD,"Don't let your dreams be dreams, you need to embed a 9950x3D into your bathroom mirror so you can game while sitting on the porcelain throne",Neutral
AMD,"*Of course, “Embedded” indicates that these CPUs are aimed at long-lifecycle, reliability-critical environments such as industrial automation, medical systems, or robotics.*",Neutral
AMD,Because why not? I guess.,Neutral
AMD,https://en.wikipedia.org/wiki/List_of_AMD_Ryzen_processors#Embedded_processors,Neutral
AMD,and ARM will probably lose out to RISC-V i guess,Negative
AMD,Intel has such CPUs.,Neutral
AMD,If my wife would let me I'd ha e a TV in every room hooked up to the latest cpu and gpu,Positive
AMD,"I'm kind of surprised these applications would use, like, general purpose CPUs. I figured they were mostly reliant on customized combinations of microcontrollers and FPGAs and ASICs and stuff like that... Perhaps it's easier to develop for this way, with high level software and such.",Negative
AMD,Why not bigass gaming servers too?,Neutral
AMD,"Just want to add that the link only shows the Zen based CPUs.  There was also the Geode, Elan, bobcat and Jaguar units.  Has been a thing with AMD for over 20 years.",Neutral
AMD,thank you,Positive
AMD,Why would ARM lose out to RISC-V?,Neutral
AMD,> RISC-V   Nothing stopping AMD from working with it though.Would be dope AF.,Positive
AMD,"Nah man, just run conduit and use fiber DisplayPort. Or a HDBaseT like solution. Virtualize your Windows box and pass through a GPU.   You won’t even have to close your game to switch rooms. You could game on the shitter, wipe, and be back in your office even for multiplayer.",Neutral
AMD,"They are in general, but big things like CT, MRI and ray scanners could benefit from bigger, powerful CPU's by shortening scanning time, or by increasing scan rate, which depending on application, could also improve resolutions of images processed by scanners.",Positive
AMD,"Why don't you use your current system for 10-20 years of gaming with minimal downtime, minimal software updates beyond stability/security patches, little to no hardware changes then come back and ask again?",Neutral
AMD,thank you,Positive
AMD,...because companies don't have to pay for the ARM license.,Neutral
AMD,"they probably do, for things under the hood (nvidia does too)",Neutral
AMD,No,Neutral
AMD,"I used my last laptop like that for six years  12ish hours of active warframe, remaining time afk in maroo's bazaar to sell rivens  It took me a year to get to that level of warframe addiction tho  Before I used to run it as webclient",Neutral
AMD,I plan to use my 7950X3D for 10 years along with the 96GB RAM. It's two years old already.,Neutral
AMD,What?,Neutral
AMD,The Sipeed NanoKVM surprised me by having a RISC-V cpu in it.  It certainly keeps the price down for home lab people like me.  ~$30-40 for the bare bones IP KVM kit seems a steal if you don't need all the bells and whistles.,Positive
AMD,"Apple proved that ARM can be phenomenally performative. When the M1 first came out, it was the most powerful single core CPU beating both Intel and AMD.   I’m still using my M1 MacBook and it’s never even hiccuped. The performance leap by that ARM processor was something that you could _feel_. There are very few technological leaps that provide such a boost to performance that you don’t need a benchmark to see.   I can point to my first SSD and my first dedicated GPU as the few rare instances in which performance jumped that much.   I wish AMD would make an M1 competitor. Especially if it were x86. It would make for some absolutely kick ass homelab boxes.",Positive
AMD,And why do you not use any M1/M2/Mx system for your homelab?,Neutral
AMD,Can it minecraft?,Neutral
AMD,Is this a new mainline gpu or what?,Neutral
AMD,Yes,Neutral
AMD,"This is part of their professional GPU lineup. Based on Navi 31, uses 48GB GDDR6 ECC memory.",Neutral
AMD,"Wait, so basically a 9070xt but with MOAR RAM?",Neutral
AMD,"navi 31 is rdna 3, 9070xt's die (navi 48) doesn't have the memory bus width for 48.  Moar ram 9070 is the ai pro r9700 with 32gb",Neutral
AMD,I see - forgot the AI cards existed cause I live in Oz and they haven't arrived here yet.,Neutral
AMD,"It was quickly removed due to a leftover bug, expect a re-release in 15-20 minutes.",Neutral
AMD,"30 mins before launch, got to be some kind of record.",Neutral
AMD,Link will work once the driver has been posted live to the AMD.com domain,Neutral
AMD,Hardware unboxed panicking,Negative
AMD,"Friday night driver releases, yummy.",Positive
AMD,"Interesting, they've been reducing the package size from 25.9.1 (934MB) -> 25.9.2 (882MB) -> 25.10.1 (866MB). Anyone know where the decreases are coming from?",Neutral
AMD,The download link is Up! :)  [AMD Software: Adrenalin Edition 25.10.1 Battlefield™ 6 Preview Driver for Windows® 10 (64-bit version 19044 and later) & Windows® 11 (64-bit)](https://drivers.amd.com/drivers/amd-software-adrenalin-edition-25.10.1-win10-win11-oct-rdna.exe),Positive
AMD,"Should be live now everyone, juct checked and the link works",Positive
AMD,I don't see a download link.,Neutral
AMD,"where's the download button amd, i need the button",Neutral
AMD,live now,Neutral
AMD,"this says ""preview"" on it, i think that means you might not want to install it unless youre having trouble with the latest normal driver. They dont link to this version number if you navigate to the drivers page for your graphics card  from the main page.",Negative
AMD,broken link,Negative
AMD,Anyone try playing the game with the 25.9.1 drivers? FSR4 work?,Neutral
AMD,Any perf improvements vs the previous driver on bf6?,Neutral
AMD,How did you find the download? I cant find it over the navigation on the amd page,Neutral
AMD,With this driver i have decreased performance and some graphics issues with my 7900 GRE,Negative
AMD,"My game keeps crashing after 20 minutes of gameplay, amd driver timout is the error message,      Should I go back to the previous drivers ? My game was stable during beta",Negative
AMD,"AMD Noise Suppression feature will not turn on with this version of the driver - clicking the slider to enable it causes a spinny processing circle and then it it just stays disabled.  Anyone else?  PS - Nvidia Broadcast is so great and reliable on my system with 3090, but on my system with 9070 XT I constantly seem to have issues with AMD's version in the adrenaline driver. Most times when I restart my system it switches the processor for the noise suppression back to the CPU instead of the GPU so I have to keep resetting that, and sometimes the menu to check those settings just goes missing completely. Thing is a pain in my ass, for what should be a valuable and crucial feature.",Negative
AMD,"Pra quem ver esse comentário, desative o anti lag no driver da amd, deixe só o anti lag do jogo, se não vai aumentar muito a latência",Neutral
AMD,Who published this? It’s nowhere to be found.,Negative
AMD,"Man kind of wish I still had the 5070ti, would love to bench bf6 on new drivers for 9070xt and Nvidia personally",Positive
AMD,6800xt here and this crashes even more than previous driver. get a little further each time but cant even compile shaders completely yet. wtf is goin on?!,Negative
AMD,still not working my battlefield keeps crashing after 5 - 20 minutes Direct X DXGI\_ERROR\_DEVICE\_HUNG,Negative
AMD,"I dislike preview drivers so much and optional drivers, especially when they aren't releasing stable whql drivers least once a month, having to wait a full year bare minimum to get issues fixed that happen for everyone on a specific gpu like rdna3 or even rdna4 is not acceptable either.  To many issues get ignored due lack of vanguard program users for a specific new generation GPU, and if they do not want to investigate properly these issues effectively get passed down as hardware issue even tho obvious proof of it not being hardware issue like dying light RGB laser show bug, and this was outside Radeon boost as it has had a listed issue with Radeon boost but the RGB laser show bug alto did get fixed never was acknowledged, and this is how many issues get fixed after like a year.     Now i am being plagued by random black desktop especially when wallpaper engine is running requiring me to restart explorer.exe to fix as the issue also causes random glitches.  The only way these issues get acknowledge or fixed if they are submitted in meme format of its only gonna get worse, then magically things getting less worse until you stop submitting these issues in meme format, then its back to being ignored.",Negative
AMD,Any 7900xtx owners try it yet? Really looking to move on from 24.10.1 lmao,Neutral
AMD,Here Link: [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html),Neutral
AMD,Is the broken Vulcan driver fixed in this one?,Neutral
AMD,No FSR4 for RDNA3 no care,Negative
AMD,When I played the beta couple weeks ago I got a directx error every game. Could only play max 5 mins until it crashed is that bug fixed?,Negative
AMD,"I do not miss having to update drivers like this.  Thank you, AMD, for your amazing Linux drivers!",Positive
AMD,Invalid link,Neutral
AMD,"Even with this on my 7800xt I have driver crashes. It was unplayable on the beta, but they fucked up even more on full release. Great...",Negative
AMD,"2025 and Amd still fumbles driver releases, from ""leftover bugs"" to no download links ... But if I dare complain about my 9070xt, everyone loses their minds",Negative
AMD,"AMD, AMD, AMD..",Neutral
AMD,yeah but its not working lol,Negative
AMD,Fumbling the bag once again,Negative
AMD,Why so late?,Neutral
AMD,Wonder if they'll do a quick update with these updates drivers.  Especially after he mentions the 9070xt having some odd performance shortcomings,Neutral
AMD,I think he already did his benchmarks with pre access a few days ago. Takes a little while to put the video together. So maybe he'll do a follow-up in a few days.,Neutral
AMD,yeah I would like to know as well. Could just be a different compression algorithm.,Neutral
AMD,This link is broken.  This one works for me.     [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html#](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html#),Neutral
AMD,Directly linking the .exe does not work. Use the downloadlink on this page: [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html),Negative
AMD,"link works, driver download still not there yet",Neutral
AMD,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html),Neutral
AMD,"yeah, 404'd and it doesn't show up on the website search either.",Negative
AMD,Yeah works good,Positive
AMD,It works surprisingly well actually lol,Positive
AMD,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html),Neutral
AMD,6900xt here no crashes at all wit the Dying Light driver,Neutral
AMD,7600 i play 2 min and then i crash with the drivers error,Negative
AMD,Hey Man I got [solution.Open](http://solution.open/) AMD software performance tuning Gpu undervolt let the Adrenaline auto undervolting your gpu..Then try the game,Neutral
AMD,🤣 okay?,Neutral
AMD,What does any of this have to do with a preview driver for BF6?,Neutral
AMD,"7900xtx crashes too been dealing with this since the beta, thought they would have fixed this by now.",Negative
AMD,Hey Man I got [solution.Open](http://solution.Open) AMD software performance tuning Gpu undervolt let the Adrenaline auto undervolting your gpu..Then try the game,Neutral
AMD,I have a 7600 and the game is crashing every 10 minutes,Negative
AMD,"Hopefully, i did too, constant driver crashes or DX errors",Neutral
AMD,"If you having driver crashes like I get on my 6900 XT, [Gotta downvolt (mine default 1175mV) Lowered it to 1125mV, lowered core clock by 100Mhz and memory is default. I seem to get less crashes & can play longer](https://images.castlegrayskull.io/GaQe0/HIloyIjA33.jpg/raw)",Neutral
AMD,Hey Man I got [solution.Open](http://solution.open/) AMD software performance tuning Gpu undervolt let the Adrenaline auto undervolting your gpu..Then try the game,Neutral
AMD,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html),Neutral
AMD,It's still better than Nvidia's AI generated drivers.,Positive
AMD,"Ye... Weirdly iam asking amd now I think 4 years for a background fps limit and 2 times a amd official said they think about it , 2nd time even said they had internal talks about it yet nothing ...  Nvidia got this since like 10 or 13 years maybe way longer ( like close to 20 years but ye idk for sure I remember it I think atleast back to nvidia 200 series )",Neutral
AMD,Hectic end of the week.,Negative
AMD,The direct link doesnt work (hotlink protection). Use the link on this page: [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-1-BATTLEFIELD-6-PREVIEW.html)  I just downloaded it from there.,Negative
AMD,"Overlooked that, yeah no driver download yet. I’m working still so my fault. Was a bit distracted",Negative
AMD,I’m using EA store version. After a repair and then a complete uninstall and reinstall I finally don’t see any glitches and got into a game,Positive
AMD,"oh my god ur a legend ive been trying literally all day and this worked, just select the favor efficiency tuning",Positive
AMD,frfr me too with that fucking crash error,Negative
AMD,The Same With 7650 GRE cant play,Negative
AMD,Thanks! I'll try.,Positive
AMD,"Tried this, no dice",Neutral
AMD,"The goal should be for drivers to be good, not for them to be better than nvidia drivers. AMD drivers still suck, nvidia drivers are just borderline insulting",Negative
AMD,Happens haha ik we all excited for this driver for game launch,Positive
AMD,"i have steam version prob im gonna try the same, trust me i have tried everything...",Neutral
AMD,pls let us know if it works...,Neutral
AMD,"Shame, that sucks :/.",Negative
AMD,"*Oh no*, their packaged driver had a small bug, and you had to wait a ***catastrophic*** 10 minutes for it to be fixed & reuploaded. SURELY this means AMD drivers bad! Grow up.",Negative
AMD,"I spoke too soon, made it into a game as it was ending and it froze and crashed windows. Fml, lol",Negative
AMD,It totally works!   Locked max freq to 2400MHz (95%) voltage to 1075mV (93%) Nothing else is changed.   Did not crash for the past 50 mins!  Edit: percentages,Positive
AMD,"Yeah, crazier thing is it’s working fine on my gpd win mini. So it’s clearly a driver issue with the desktop gpu",Neutral
AMD,idk if you can on eaplay but i have asked refund on steam i want to wait untill this problem with amd gpu wont be fixed,Negative
AMD,Unfortunately it was a promo code for the intel deal,Negative
AMD,Makes zero sense at 2000USD.  Equivalent 8840HS machines are down near 1000USD. You can also get AI 360 machines for 1500-1600USD.,Negative
AMD,"HP. It is apparently their policy to set ridiculpusly high list prices,and then offer huge discounts, resulting in a normal price in the end. Why they do that instead of normal pricing I don't know.",Negative
AMD,"Elitebooks & other similar business machines are mostly being purchased by corporate accounts, which are less price sensitive (because the people doing the buying aren't spending their own money lol).  Also, often those corporate buyers will request bulk discounts so the high list price lets the person doing the buying show off how much they managed to ""save"" to their boss when they finalize the order.",Neutral
AMD,"""affordable""",Neutral
AMD,30% discount gets clicks.,Neutral
AMD,What is this? AI for ants?,Neutral
AMD,"100% marketing BS  “ The term ""2 nanometer"", or alternatively ""20 angstrom"" (a term used by Intel), has no relation to any actual physical feature (such as gate length, metal pitch or gate pitch) of the transistors. According to the projections contained in the 2021 update of the International Roadmap for Devices and Systems published by the Institute of Electrical and Electronics Engineers (IEEE), a ""2.1 nm node range label"" is expected to have a contacted gate pitch of 45 nanometers and a tightest metal pitch of 20 nanometers.[1] Process	Gate pitch	Metal pitch	Year 7 nm	60 nm	40 nm	2018 5 nm	51 nm	30 nm	2020 3 nm	48 nm	24 nm	2022 2 nm	45 nm	20 nm	2025 1 nm	42 nm	16 nm	2027 As such, 2 nm is used primarily as a marketing term by the semiconductor industry to refer to a new, improved generation of chips in terms of increased transistor density (a higher degree of miniaturization), increased speed, and reduced power consumption compared to the previous 3 nm node generation.[2][3]”",Neutral
AMD,I wouldn't be surprised if Nvidia still had more perf/watt on a slightly older (and cheaper) node.  Hopefully AMD can make some architectural gains to pull ahead with a better node instead of just barely getting to a tie.,Neutral
AMD,ants for ai,Neutral
AMD,What is this? AI for small ants?,Neutral
AMD,The chip needs to be at least three times this size!,Neutral
AMD,"It's just like how Porsche still uses ""turbo"" as a trim level for all their EVs. It doesn't even make any sense.",Negative
AMD,"100% marketing BS  “ The term ""2 nanometer"", or alternatively ""20 angstrom"" (a term used by Intel), has no relation to any actual physical feature (such as gate length, metal pitch or gate pitch) of the transistors. According to the projections contained in the 2021 update of the International Roadmap for Devices and Systems published by the Institute of Electrical and Electronics Engineers (IEEE), a ""2.1 nm node range label"" is expected to have a contacted gate pitch of 45 nanometers and a tightest metal pitch of 20 nanometers.[1]  * Process	Gate pitch	Metal pitch	Year * 7 nm__	60 nm___	40 nm____	2018 * 5 nm__	51 nm___	30 nm____	2020 * 3 nm__	48 nm___	24 nm____	2022 * 2 nm__	45 nm___	20 nm____	2025 * 1 nm__	42 nm___	16 nm____	2027  As such, 2 nm is used primarily as a marketing term by the semiconductor industry to refer to a new, improved generation of chips in terms of increased transistor density (a higher degree of miniaturization), increased speed, and reduced power consumption compared to the previous 3 nm node generation.[2][3]”",Neutral
AMD,"Yep, but it's so deep in the industry that Intel had to do the same to compete. It's sad but that's what happens. If everyone is lieing about their height but using the same fake ruler than no one is lying.",Negative
AMD,I love how tech companies are allowed to just use false advertising because they've always done it.,Positive
AMD,Always have been.  Looks like a kid who's just learned something and is telling everyone about it.,Neutral
AMD,It's a node name.  Everyone knows it's not literally the size of transistors.  2x4 lumber also isn't actually two inches by four inches.  Going to write a screed about that as well?,Neutral
AMD,"It has been marketing bs for over 15 years now, the name relates to nothing.",Negative
AMD,"If it was 100% marketing, then why don't they go even lower?",Neutral
AMD,"Okay. So it's a better process with more desirable performance traits. So in light of the process naming progression, moving to ""2nm"" is an expensive investment to make better chips for their Instinct accelerators?  I can understand complaining when an older chip gets a new model number that makes it sound like a current design, but this is not that situation. AMD put down a ton of money to use a bleeding-edge node and your comment opens with ""100% marketing bs"" as though their new chips were going to be produced on Samsung 8nm or something.  It almost sounded like you accused AMD of running a scam or something equally dire.",Neutral
AMD,"I know you are getting downvoted but there's a chance if could be reue.  There's lots of levers they can pull.  One is frequency  One is power  One is transistor count.  To be fair there is some overhead with the Chiplet design. So amd loses a bit of efficiency there.  NVIDIA also makes insanely large chips... They are essentially two monolithic dies glued together, at the reticle limit.  Yields on this would be terrible, and even more terrible on a new node.  Amd though is using chiplets... Lots of smaller chips that are joined together.  Yields whilst worse on a new node, wouldn't be that much of an issue.  Amd can capitalise on newer nodes quicker, because of their yields.  Transistor to transistor, amd has the advantage by using a newer node.  NVIDIA has the advantage that they are using monolithic reticle sized chips.  It's unlikely NVIDIA will keep up, but there's a chance. But if you want to have good margins, then chiplets is the way to go.",Negative
AMD,What is this? Small ants for AI?,Neutral
AMD,So you are saying there is no SUTUTUTUTU noises to be enjoyed anymore!? :(,Negative
AMD,Good bot,Positive
AMD,"edit: relocating reply to correct OP  ~~Okay. So it's a better process with more desirable performance traits. So in light of the process naming progression, moving to ""2nm"" is an expensive investment to make better chips for their Instinct accelerators?~~  ~~I can understand complaining when an older chip gets a new model number that makes it sound like a current design, but this is not that situation. AMD put down a ton of money to use a bleeding-edge node and your comment opens with ""100% marketing bs"" as though their new chips were going to be produced on Samsung 8nm or something.~~  ~~It almost sounded like you accused AMD of running a scam or something equally dire.~~",Neutral
AMD,"they wouldn't have to if it wasn't for their insane CEO and Board. they fired so many people back when 14nm took off. if 10nm was even just 2 years late they would still be ahead and still using their old naming structure (which was still wrong but at least closer to ""nm class"" performance, compared to now where it just means a new node and doesn't even guarantee 15% uplift).",Negative
AMD,"Technically, it is not. If you delve deep into the documentation, you will find a disclaimer that ""x nm"" is just a name and has nothing to do with gateway size or any size whatsoever.",Neutral
AMD,Yes it always has been - but it's always worth calling out and not falling for cheap marketing tactics that are used to mislead consumers.  Looks like you have quite a bit of LDE and angst - good luck with that buddy,Negative
AMD,Why you gotta be so negative? The average guy doesn't have your knowledge in computers. People absolutely do believe it is the size of transistors lol.  Like I'm not saying they're right for believing it or whatever but it's easy to overestimate the knowledge the average Joe has on a subject you fancy.,Negative
AMD,"Honestly if you ask 10 people that build their own computers what 2nm means they will likely say that the transistors in the CPU are 2nm. Which is factually incorrect and is what this marketing style is trying to get you to believe.  If you don't believe me - ask in a gaming Discord or friends/family.   From wikichips:  ""Most recently, due to various marketing and discrepancies among foundries, the number itself has lost the exact meaning it once held.  Recent technology nodes such as [22 nm](https://en.wikichip.org/wiki/22_nm), [16 nm](https://en.wikichip.org/wiki/16_nm), [14 nm](https://en.wikichip.org/wiki/14_nm), and [10 nm](https://en.wikichip.org/wiki/10_nm) refer purely to a specific generation of chips made in a particular technology. It does not correspond to any gate length or half pitch. Nevertheless, the name convention has stuck and it's what the leading foundries call their nodes.  Since around [2017](https://en.wikichip.org/wiki/2017) node names have been entirely overtaken by marketing with some leading-edge foundries using node names ambiguously to represent slightly modified processes. Additionally, the size, density, and performance of the transistors among foundries no longer matches between foundries. For example, Intel's [10 nm](https://en.wikichip.org/wiki/10_nm) is comparable to foundries [7 nm](https://en.wikichip.org/wiki/7_nm) while Intel's [7 nm](https://en.wikichip.org/wiki/7_nm) is comparable to foundries [5 nm](https://en.wikichip.org/wiki/5_nm).""",Negative
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,"I pasted a quick opinion followed by quoting a factually correct snippet from Wikipedia - if you think this snippet is accusing AMD of running a scam that's you jumping to a logical conclusion after reading facts being presented in a non-biased way.   This is from wikichip: ""Recent technology nodes such as 22 nm, 16 nm, 14 nm, and 10 nm refer purely to a specific generation of chips made in a particular technology. It does not correspond to any gate length or half pitch. Nevertheless, the name convention has stuck and it's what the leading foundries call their nodes.  Since around 2017 node names have been entirely overtaken by marketing with some leading-edge foundries using node names ambiguously to represent slightly modified processes. Additionally, the size, density, and performance of the transistors among foundries no longer matches between foundries. For example, Intel's 10 nm is comparable to foundries 7 nm while Intel's 7 nm is comparable to foundries 5 nm. ""  https://en.wikichip.org/wiki/technology_node",Neutral
AMD,What is this? Small AI for ants?,Neutral
AMD,Yes but now the SUTUTUTUT noise is A I™️ generated,Neutral
AMD,"If I was a bot, I'd do a table. But I couldn't arse to do it so I cheated with underscores.",Negative
AMD,I just reformatted previous guy's comment. These are not my opinions.,Neutral
AMD,"Bruh, that's still false advertising regardless. If I have to ""delve into the documentation"" for a disclaimer then it's way past the point of what's considered false advertising.",Negative
AMD,"The problem is there's a million things that affect process performance other than gate length. Users don't actually care about gate size, but how well it performs. If multiple processes have the same gate size (and so named the same in that world), but perform *significantly* differently from other changes, then that is a bad naming scheme.  And you probably *could* make a gate with length closer to the order of the name, it would just *suck*. The gate itself hasn't been the smallest structure you want to create in silicon for a very long time.  I could see the argument that the ""sized"" name is closer to the finest detail you can create, like the size of pen, rather than one specific structure size. That's probably closer to being ""accurate"", but then still falls again to the problem when other aspects of the process are improved. See how much Intel got laughed at for just adding ""+"", even if it should be clear they *were* better processes.",Negative
AMD,"Nobody reading that rant believes the node name describes the actual transistor sizes.  This is not a place where the ""average guy"" comes to read posts about enterprise number crunching chips.  There are zero people who read that title, clicked into the topic, read that excessively hostile comment, and changed from believing the transistor sizes of MI450 chips were 2nm to not believing it.",Negative
AMD,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Negative
AMD,"Your own quotation shows that your original comment was irrelevant to the announcement. Newer nodes are named in a simple progression that's easy to understand, but they are not tied to a specific geometry.  MI450 will not operate on 2nm wires but *yes,* it will be built on one of TSMC's ""2nm process"" nodes. There is no bs in the headline and no false claim from AMD.  You will see that different manufacturing nodes are also rated for performance, efficiency or balanced operation. As well as a few nodes that are named as refinement of an existing process, just like 14nm ++++++. Whether those ratings are based on wire thickness, masking techniques, metal deposition and insulating techniques, a 'smaller' node from a given manufacturer should deliver concrete improvement in desirable performance traits.  [TSMC N3, And Challenges Ahead – WikiChip Fuse](https://fuse.wikichip.org/news/7375/tsmc-n3-and-challenges-ahead/)  This article's pretty old already but if you skim, you'll see that each announced process node does have specific geometries listed at the time of announcement. The N3B and N3E nodes in this article have different base geometries and were said to have major differences in the manufacturing process. Even though both were ""3nm class"" nodes. So the details are not hidden, people just don't care unless they are designing chips. You're calling bs on an industry convention for no particular reason.",Neutral
AMD,What is AI? Small ants for this?,Neutral
AMD,Bad bot,Negative
AMD,"Ah, I see it now. Got it.",Neutral
AMD,they are advertising a technical specifications that means you gotta read the documentation. If they are promoting something natural language (no jargon) then we do have a point.,Neutral
AMD,"What are you on about? Should apple be sued for naming their OS after landmarks and animals when they contain no such things?   They could name it the plank length process node and it wouldn’t matter, since at the end of the day what they are selling is the performance.",Negative
AMD,"The problem is that they're using marketing terms to describe a product but the description isn't accurate to what they're selling.   It would be like if AMD/Intel/ARM started advertising ""7-8ghz performance is possible with the new processors"" and then not mentioning at all that this is only possible with liquid nitrogen and perfect conditions, and that users should expect 4-5ghz.      Or that users can get 1400 FPS in Valorant with a 9800x3d : [https://www.tomshardware.com/pc-components/cpus/sub-zero-ryzen-7-9800x3d-hits-1-400-fps-at-6-9-ghz-new-gaming-champion-destroys-valorant-frame-rates-with-liquid-nitrogen](https://www.tomshardware.com/pc-components/cpus/sub-zero-ryzen-7-9800x3d-hits-1-400-fps-at-6-9-ghz-new-gaming-champion-destroys-valorant-frame-rates-with-liquid-nitrogen)  But without any caveats or disclaimers. It's straight up false advertising.",Negative
AMD,> There are zero people  Almost certainly untrue. People always have to find out for the first time somewhere sometime.,Negative
AMD,"""MI450 will not operate on 2nm wires but yes, it will be built on one of TSMC's ""2nm process"" nodes.""  Which is the problem.... It's marketing BS - it doesn't operate on 2nm.  How do you write that and not see a problem?",Negative
AMD,Are you fr? You honestly think that it's fine to lie about specs in advertisements as long as the company in question hides an admission that they lied somewhere in their documentation? That's an insane take.,Negative
AMD,Are you unironically comparing Apple naming their OS after landmarks to CPU makers straight up lying about process node size in their marketing? You really think that that's comparable?,Negative
AMD,Scummy marketing tactics shouldn't be defended regardless the industry lol.   The Apple comparison also doesn't apply in this circumstance. It's a version of a product not a technical spec that the company is advertising.  I feel like AMD fanboys are so used to defending the underdog and being contrarian that you aren't used to calling out bad behaviors when they exist from AMD. They're now a company worth over $300 billion. They don't need people falling on swords in comment sections to defend bad business practices that they engage in.,Negative
AMD,"No, because they're not ""advertising"" gate size. That is something some hardware forums (and admittedly some media) have decided they are just so they can rant about how they're ""incorrect"" at doing so.  Even the first SIA roadmap in 1993 referred to it as ""Feature Size"" - *Not* gate length.",Negative
AMD,"> It would be like if AMD/Intel/ARM started advertising ""7-8ghz performance ...  AMD started using marketing numbers in place of MHz back in the 90's. The switch back to frequency didn't really happen until Intel Core.",Neutral
AMD,"They dont lie about specs, the specs is correct. It is 2nm never 3 or 4, its the documentation that is misleading (making you think 2nm is something else). That's the difference. And if the documentation are public and standardized (apply to all vendors, they dont invent that specs) then i dont see the problems",Neutral
AMD,"Is the lack of any 2 nano-meter feature impacting how you use the product? Is there something that was promised, but can’t actually be done because of a lack of 2nm features?   And what does this have to do with defending AMD? This has been standard practice in the industry for years now. Ever since finfets kinda rendered the nm measurements obsolete. A larger finfet could outperform a smaller planar transistor by building taller.   Nothing has ever married the node name to any specific measurement on a die. Heck you could argue that 2nm is referring to the planar transistor equivalent.",Negative
AMD,"What documentation are you talking about? Who is ""standardizing"" it to all vendors? This is such an incoherent take once again.  Also, no one is advertising their documentation. They are advertising their products. You are so completely lost.",Negative
AMD,"Why call it 2 nanometer if it's not 2nm? It's deliberately misleading customers.   Why not call it 0.5nm? 2 angstrom? heck, call it a quantum node, or 5nm graphene.",Negative
AMD,To differentiate it from their older process nodes? It’s a complete non-issue dude.,Neutral
AMD,"It’s a non issue since you consider misleading marketing a non-issue? Glad to hear a consensus among millions of consumers, and that you’re their voice of reason. Not sure how you got elected but that’s pretty cool I guess.",Positive
AMD,Why are mods deleting everything? This is not an AMD site. lol,Negative
AMD,Video that the article is based on:   https://www.youtube.com/watch?v=ItXPvGrI6gY&t=2s,Neutral
AMD,NGL radiance cores sounds pretty cool,Positive
AMD,"It is my understanding that Kepler\_L2 is considered a reliable leaker. Here is some additional info he posted about this video in response to other people on a message board:  >\>Will Magnus get these updates  >Of course  >  >\>Neural Arrays -> RTX Neural Shaders + Tensor Cores  >No, it's workgroup clustering + global scratchpad sharing. Something NVIDIA has had for a while on their datacenter GPUs but not in gaming GPUs.  >  >\> Radiance Cores -> RT Cores + Neural Radiance Cache  >It's a fancy name for the new RT core, but it does have more features than even Blackwell current RT cores (who knows how it will compare to Rubin's RT core though)  >  > \>Universal Compression -> RTX Neural Texture Compression + Cooperative Vectors  > Not at all comparable, it's HW compression/decompression for every datatype in the GPU, not just color data.  >  > \>when they talk about ""flexible and efficient data structures for the geometry being ray-traced.  > That's referring to DGF  >  > \>That's just semantics, I literally saw people say the exact opposite. 2027 is on the table.  > Not just on the table, it's the plan unless any unexpected delays happen.",Neutral
AMD,Lol Sony always making names up for tech that AMD always had 😂,Neutral
AMD,I'm assuming this will be the capstone of RDNA? It makes sense they wouldn't pilot UDNA on consoles.,Neutral
AMD,"Everyone keep talking about RDNA5, but the next gen arch will be UDMA 1...",Neutral
AMD,lol why would mods delete this? i get better discussion on 4chan of wccf lol,Negative
AMD,Radiance cores sound interesting,Positive
AMD,"So they are going to finally have dedicated hardware for RT like nVidia with those ""radiance"" cores...",Neutral
AMD,The mods keep deleting everything related to this video. They're psycho,Negative
AMD,The sub has been locked down for a while for some reason. Only 1-3 posts per day and it's all just the major news stories like a day after they were already posted somewhere else. Yet another sub that is being suffocated for no good reason.,Negative
AMD,Look at the top 2 mods (not counting the bot). They have rx 580 and 480. What do u expect of people like that?,Negative
AMD,Doing heavens work translating new weird names into something concrete   :),Positive
AMD,"Are Neural Arrays a precursor for chiplet Tensor (Transformer) Cores?  A Radiance Core seems useful if it's optimized for a certain number of light bounces for ray tracing calculations.  Any idea how many bounces it's designed for?  I'm guessing that in the AI graphics pipeline, it's still VERY useful to process X number of Rays, and Y number of bounces to get the noise field, and then use AI and other optimizations to extrapolate the rest of the data.  I'm wondering what that acceleration target is for X and Y.  \- As I understand it, with VR, it's likely these ray casting calculations can not be shared.  \- so are they targeting 4k x 2 eyes, or more like an 8k display?  \- Universal compression seems like an iteration on other technologies that go after the same goal. It sounds like it's lossy, so I'd like to know whether the app/engine can configure the amount of loss, and whether the hardware auto-detects media type, or that is an option too?  it seems odd to call it a universal engine if you have to configure a media type. Like it won't be compressing point clouds or vector arrays?  \++++  I'm also wondering what sort of regularization or optimizations they are going to have in their AI hardware libraries.  Nvidia does a lot of work here. Will AMD's AI hardware have a similar performance to 5th-gen RTX hardware (Blackwell) or will it be different?  AI Hardware can easily differ by a few orders of magnitude in performance based on micro architecture.",Neutral
AMD,They should stick to the FSR4 name instead of insisting on using PiSSR.,Neutral
AMD,The two have been interchangeable terms by everyone in the industry.,Neutral
AMD,"*UDNA, I also don't understand why some people insist on RDNA 5.",Negative
AMD,They already have dedicated RT hardware called 'Ray accelerators'.   [https://www.servethehome.com/wp-content/uploads/2025/06/AMD-RDNA-4-Architectural-Overview-scaled.jpeg](https://www.servethehome.com/wp-content/uploads/2025/06/AMD-RDNA-4-Architectural-Overview-scaled.jpeg)   Radiance cores should be much better.,Neutral
AMD,Are you guys sure they're actually getting deleted? Can you link the deleted post? I'm pretty sure it's just the stupid auto-hide post rule they have and the mods being barely active these days to manually approve anything. I feel like the mods decisions are killing this subreddit... It used to be so active and now it can sometimes take a whole day for drivers to get posted here.,Negative
AMD,Yeah this sub is basically dead at this point due to all the over-moderation. I think at some point someone in AMD PR contacted the mods and got them to agree to be very strict only allowing entirely pro-AMD stuff to appear here.,Negative
AMD,">\- As I understand it, with VR, it's likely these ray casting calculations can not be shared.  Graphics engineer here.  This isn't an absolute thing.  Assuming the new AMD hardware doesn't impose weird new limitations compared to regular ol' DXR/VKRT (which would surprise me), you can totally theoretically reuse data from different ray paths for VR.  Noting that I haven't actually tried this, but *in theory*, some fairly simple changes to ReSTIR PT + a good radiance cache ***should*** actually make this pretty trivial.  You'd want to trace some rays from both eyes, ofc, but the full path resampling means you should be able to get a proper BRDF response for both eyes.  I bet you could actually get that working pretty well in simpler scenes at relatively low res even on a 3090.  On a 5090, I expect you could go a hell of a lot further.  No clue what these new AMD chips could do, ofc.  Granted, there are smarter ways to integrate RT for VR on modern hardware, but you could almost certainly make somehing work here on current top end hardware.",Neutral
AMD,"There is no UDNA1, probably just some people hyping it up to show that AMD can finally compete using UDNA.",Neutral
AMD,amd's ray accelerators are not comparable to nvidia's rt cores,Negative
AMD,"From what I read, not really. They are still not like Nvidias dedicated hardware for them (rt cores), they are still repurposing some of the other shader or units for them. This make them seen will be fully dedicated RT cores like nvidias, so should be a nice boost in RT performance. Hopefully catch up or surpass Nvidia...",Neutral
AMD,"They deleted that pretty huge post with a lot of negative comments about the openai deal. instead for some reason they kept up two posts of same news story, but with a fraction of the comments.  Pretty ironic considering a few days ago AMD sent out a PR release that they weren't fazed by nvidia's investment in intel, and now they're on a PR blitz literally days later. I don't really get why considering nvidia's deal doesn't amount to much other than financing Intel's fabs.",Negative
AMD,"Not nearly as fun as the Zen+ days when I joined, that's for fucking sure",Negative
AMD,Wrong - AMD confirmed the UDNA1 existence...,Negative
AMD,"They don't do ray traversal acceleration, but otherwise, RA units handle all of the geometry-level ray intersection tests (and OBB in RDNA4). The TMUs do ray/box tests. Even in Nvidia GPUs, RT is passed to compute cores once a hit is detected. So, while it's neat to lump everything into a logical diagram RT core, the actual logic will be placed where it makes most sense within the CU or SM. Ada's micro-meshes are geometry engine duties and displacement maps are ROP duties, for example.  RDNA4 is comparable to Ampere in path tracing, mostly due to traversals (shader compute cost) and geometry-level ray hits (path tracing launches a lot of rays that usually hit geometry or within the BLAS). Ray/triangle rates on both architectures are 2 ray/triangles per CU (AMD) or SM (Nvidia). In hybrid rendering (raster+RT), RDNA4 is at Ada Lovelace levels, as it can use its 8 ray/box tests per CU to narrow things down, and there's generally fewer rays cast, so the overall cost is lower as well. The rasterizers also build most of the screenspace too, and those plotted coordinates within screenspace can be used to better predict a ray hit. There's also a complete BVH structure residing in VRAM and system RAM, built by CPU and copied to VRAM or for APUs like Strix Halo, zero-copy.  Blackwell doubled ray/triangle rates over Ada, so it should be testing at 8 ray/triangles per SM. As more rays are cast, Blackwell should lead Ada assuming equal compute hardware, but it depends on other complexities like register and local cache usage.",Neutral
AMD,AMD confirmed UDNA but that doesn't means it comes after RDNA4.,Neutral
AMD,Show me source where they confirmed RDNA5 - and i will agree with you.,Neutral
AMD,Why dont you wait for AT3 instead of speculating constantly? idk.,Negative
AMD,This would be perfect for Minecraft servers.,Positive
AMD,Curious that the qsfp56 models don't support 200gb,Neutral
AMD,"Gamers Nexus would love to have that board if it can consistently kill CPUs. We might finally find out what it is.    My x870e Nova is still running, knock on wood.",Positive
AMD,"ASRock motherboard are killing 9800X3D cpus , oh no it happened to me too, oh look on the internet everybody is saying that this specific brand of motherboard is doing it, let me buy another one of them so I can get fucked twice, oh no it happened again, shouldn't have trusted them....   ![gif](giphy|6nWhy3ulBL7GSCvKw6)",Negative
AMD,"The Nova is by far and away the best feature set I’ve seen for the price, but it’s just completely radioactive. I can’t believe they haven’t done a recall yet.  I feel silly for complaining about Asus BIOS these days cause at least my stuff still works at the end of the day…",Negative
AMD,There is NO good reason to buy asrock,Negative
AMD,My taichi 870 since day 1 on a 9800x3d has been fine. Never  OCed or undervolted until a few weeks ago undervolted a bit.,Positive
AMD,"As we say here in my country: ""After seven slices of cake.."" (he learned it was a cake)",Neutral
AMD,I bought a two year plan on my Asrock tachi from Microcenter. It hasn't killed my 9800X3d yet. I wonder if I can bring it back and get a new motherboard...,Neutral
AMD,I’ve been running an x870e taichi lite w/ 9800x3d for 9 months now with no issues.  -knocks on wood,Positive
AMD,"OK, how do we know for sure that the problem isn't with the user / builder?    Years ago I can remember building a PC and blowing the board.   I knew it was me so I just ordered another, back then I remember people telling me I should send it back as defective.   The problem is I knew it was something I did and I was about to spin a lie just to get the board replaced.",Negative
AMD,I have an Asrock B850 and it hasn't grilled my CPU so far. Am I the chosen one?,Neutral
AMD,"1st time, shame on ASRock. 2nd time, shame on you",Negative
AMD,Using asrock b650 pg lightning + 9800x3d for almost a year and so far no problems but i do not feel comfortable at all,Negative
AMD,I just built an am5 build and ASRock was completely off my list of motherboards to consider buying.,Neutral
AMD,Im thankful that my ASRock X870 is working fine without a hitch for my 9800X3D since last November,Positive
AMD,Glad I went with ASUS b650,Positive
AMD,No sympathy.,Negative
AMD,"Recently went AM5, no way in hell was I getting an Asrock board though.",Negative
AMD,Aaaaand you Jinxed it,Neutral
AMD,"9800x3d and x870e Nova chiming in, still rocking steady since February and hoping it stays that way..... we'll see how long it keeps on trucking though",Positive
AMD,That's what I have.,Neutral
AMD,The Nova's specs are sooooo good but I still won't chance it,Positive
AMD,"GN should request a video showing what case is in use and what all is connected, because what if it only occurs when some specific class of peripheral is plugged into some rear port, or something? Just getting the board itself with no context will potentially make that sort of issue likely to never occur when GN tests it",Neutral
AMD,That called being AssRock'ed,Negative
AMD,If only we had a word for this,Negative
AMD,> let me buy another one of them  Sigh.. Did you even read the article before posting?,Negative
AMD,"It's probably because the problem isn't that widespread.  While Asrock boards are more likely to kill CPUs for some unknown reason, the actual chance of a CPU dying is still tiny.",Negative
AMD,"price compels alot of people  but i agree, I wouldnt",Neutral
AMD,Love my X670E Steel Legend. Best board with best features for the price. Has been great since day one.,Positive
AMD,"It's such a bummer because the ASRock B450M Pro4 in my HTPC/NAS/Home Assistant server has been one of the most no-BS boards I've owned since I was on X58, it just works, and it had me planning on an ASRock board for Zen6.  My buggy ass ASUS Prime X470 along with ASUS's recent anti-consumer garbage has turned me off using an ASUS board ever again after using nothing but ASUS in my main for 20 years, and it seems like every single board partner has some discouraging shit going on for some reason or another.  I wish DFI would come back and show all these scrubs how to make good AMD motherboards again. I would do depraved and heinous things for a new AM5 LanParty board.",Negative
AMD,"It's been a couple of years since I've built a new PC, but between myself, spouse, children, siblings, and other extended family, I've ended up using somewhere around 7-8 ASRock boards over the last decade and never had any problems with them.",Positive
AMD,It's the B870x RS pro wifi specifically that seems to be shipping with a faulty bios by default. But it might be a problem with the power regulation as well.,Negative
AMD,You're in the majority.,Neutral
AMD,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Negative
AMD,"That used to blow up 7800x3d, though they have since fixed it.",Neutral
AMD,He knocked on wood he should be fine for a while longer.,Neutral
AMD,The best way to get your ASRock board run stable with a 9800X3D is to start an email thread with GN about sending the mobo over.,Positive
AMD,"I had Gigabyte before (x570 Aorus Elite) and that was a bit messy BIOS wise. It kinda worked, but each BIOS update was a bit of a gamble.  So I paid some extra really wanting ASROCK this time as they had such good reviews. And the board has been solid, I just didn't expect it might be a time bomb :D",Positive
AMD,"Absolutely, though it feels like I barely even use it. I plopped a single 4 TB SSD in there and have 3 more slots open (:",Positive
AMD,Hot!     For the CPU.,Positive
AMD,*Tylenol*,Neutral
AMD,[https://www.youtube.com/watch?v=rKMMCPeiQoc](https://www.youtube.com/watch?v=rKMMCPeiQoc),Neutral
AMD,Insanity,Negative
AMD,Bad acpi compatibility  on top of cpu degrade,Negative
AMD,"I was eyeing with that board too. Only 1 NVMe on cpu lanes, the other 4 lanes are reduced from 5.0 to 3.0 for the bottom x4 slot, no gpu quick release, no Thermal Sensor header.",Neutral
AMD,"I wouldn't trust it, especially  with their bad ACPI",Negative
AMD,"I'm also on an AM4 AsRock.  It's one of those things where every company has it's duds.  Maybe their good engineers left, or their processes got rushed, or they have some defective components.  If it was an easy to reproduce problem I'm sure they would have already fixed it.  As an Engineer, I feel bit bad for them cause it's rarity is at the level where it's killing their name, but not enough to easily replicate it.  My guess is it's some weird software bug.",Negative
AMD,I miss asrock  from their z77 boards which had floppy connector. Now their legacy support sucks. Best for amd now is Gigabyte,Negative
AMD,ASRock has been my go-to for over a decade as well. They've always seemed like a great value for money and a reliable brand,Positive
AMD,"Did he knock on wood, or knock his wood? Only one of those brings good luck.",Neutral
AMD,You giving my computer autism?!,Negative
AMD,"Yep, I love it. I don't need or own 4 PCI-E 5.0 NVMEs, GPU never needs to be released quickly, don't need an external thermal sensor.  Only thing I wish it had is a 7seg display.",Positive
AMD,Been rock solid for a few years now. Think I'll live.,Positive
AMD,"I dont feel bad for them at all, the way they have ""behaved"" during this whole saga is simply unacceptble. Still they selling boards like they have no issues, despeite knowing they fry CPUs left and right.   Hopefully people ditch them completly and other companies see what happens when u dont do a re-call and stop selling an obviously flawed product.   But people just want te cheapest, and ASrock is cheapest, so they will probably continue like nothing happens, and buyers will still line up to save 10-15% compared to boards with similar features.  Im sure many redditors has picked an ASrock despite knowing about all the issues, because they save a few $$$ and gamble it wont happen to them. Look how many happy ASrock owners you have, just because their setup hasnt fried. Its like they think they did something to not have it happen. Like ""never OCd"", ""undervolted always"", ""this-that-setting"", ""updated bios offcourse"" etc etc.  Nah they just got lucky so far.....",Negative
AMD,Stroke his wood 3 times and bring good feeling?,Positive
AMD,I just feel bad for their engineers.  The brand is on my no buy list for AM5.,Negative
AMD,"Much like the MSI counterpart, too late to the party",Negative
AMD,Oh so that's what AYW stands for...,Neutral
AMD,If only any of these 2 DIMM slot overclocking boards were actually available around the 9800X3D launch timeframe. No idea why they waited so long.,Negative
AMD,Nothing about global release.,Neutral
AMD,Whoa a new 1DPC board,Positive
AMD,"Could've slapped the princess bride on it, just saying",Negative
AMD,It's actually Ask Your Wife and is them encouraging you to make responsible purchasing decisions (or Ask Your Waifu for redditors).,Neutral
AMD,Not really a surprise when the collective AMD community and especially techtubes decided BeCaUsE MeMoRy sPeEd dOeSnT MaTtEr,Neutral
AMD,"Haven't heard about MSI's B850MPOWER in that respect, either :(",Negative
AMD,Asia Pacific only,Neutral
AMD,"“Dearest Sakura, my precious waifu and crunchiest body pillow, would you be terribly offended if I spent this amount of money on something other than your figurines with removable clothing?”",Neutral
AMD,It is not like it doesn't matter at all but on X3D chips matter a lot less than usual.,Neutral
AMD,It's confirmed to come in October to USA via their discord,Neutral
AMD,Even the MSI B850M Mortar is late in US market.,Negative
AMD,> crunchiest body pillow  Are body pillows supposed to be crunchy? Genuinely curious.,Neutral
AMD,Are tube socks?,Neutral
AMD,¯⁠\\⁠_⁠(⁠ツ⁠)⁠\_⁠/⁠¯,Neutral
AMD,¯⁠\⁠\_⁠(⁠ツ⁠)⁠_⁠/⁠¯,Neutral
AMD,amd should release FSR4 on 6000 series as well at this point,Neutral
AMD,"Once the drivers are installed with the swapped files, do i get the option to enable fsr4 on any game that supports it ? its that easy ?",Neutral
AMD,"WTF, AMD?  Let's get some official support for RDNA 2 out there!",Negative
AMD,Also works on Linux (and seemingly does so better than on Windows).,Positive
AMD,"Neat, hopefully they make it official. I would love to try it on my 6950XT",Positive
AMD,"Just a heads up for anyone unaware, file swapping/changing signed files in drivers *CAN* trigger anti-cheats in online multiplayer games. If you live in a bunker and play only Single Player, can do this.",Neutral
AMD,sounds like AMD could just allow the features available on RX6000 later. i get they not do it due to marketing of newer gpu but any chance this could happened officially in future?,Neutral
AMD,I'm just gonna wait for the official support with Redstone. Supposedly launching before the end of the year,Neutral
AMD,Is there any malware issues if I try using FSR4 files?,Neutral
AMD,It looks a bit worse than FP8 model.,Negative
AMD,amd need to figure out a official solution come on,Negative
AMD,even the mfg ?,Neutral
AMD,I wish fluid motion frames worked with FSR4. Would be a game changer,Positive
AMD,FineWine (TM) strikes again?,Neutral
AMD,"Is there any performance penalty when using a modded Driver?   For example;   25.9.2 Driver on Dying Light The Beast (2025 Game) - 65FPS (Without any upscaler)  25.9.2 Modded Driver on the same game with the same setting - 55FPS (Without any upscaler)  Because I saw someone report that using modded drivers makes MH Wilds unable to launch.   So, perhaps there is also a performance penalty because we swap the DX12 swap chain to the older version.",Neutral
AMD,"For what I’ve understood fsr4 on 6000 requires more power draw, this mean more heat compared to fsr3.x",Neutral
AMD,"Heads up to anyone doing that u are effectively using 2 year old driver on dx12 applications, so performance In some games might be pretty bad.  Other than that anticheats might be an issue but since file is actually from amd and properly signed it shouldn't cause troubles",Negative
AMD,"I've been using it for almost two weeks, I haven't had a problem with any online game, not even with valorant and Vanguard (only modified driver, but if you try to mod FSR4 maybe they will ban you).  I've tested FSR4 in Exp33, KCD2, Cyberpunk, Silent Hill. It's a blessing for my RX 6700 XT",Positive
AMD,What about rx 7900 series,Neutral
AMD,What’s next? FSR4 running   on an RX 580 with absolutely zero performance loss ?,Negative
AMD,I'm on 6700xt and the game I play require an upscaler to look decent.  I'm currently on fsr3 and it looks so bad - can't wait for them to give it to use officially since the game has anti cheat and I don't want to risk it.,Negative
AMD,"It's not worth it guys. It loses too much performance on rx 6000. Also, they would waste time with an older family for low gains and chance of people trash talking fsr4 for it.",Negative
AMD,Does this work with mobile stuff like 890M?,Neutral
AMD,Can I use this on multiplayer?,Neutral
AMD,Anything but official fsr4 on 7000 series,Neutral
AMD,"This broke AFMF, RSR and my PC started crashing. I just switched to 23.9.1and use optiscaler fg. Everything is fixed and no more crashing.",Negative
AMD,Doesn't work with newer games. The game just doesn't launch with 23.9.1 files,Negative
AMD,"""Works""",Neutral
AMD,"No, I think you still need to DLL swap it into a game that officially supports FSR 4 or use Optiscaler for games that don't. And it has to be the FSR 4 INT8 version.  For RDNA 3 you can do that and it will just work. With RDNA 2 you need to mod the drivers like so or it will ghost   DLL swapping is really easy though.  https://youtu.be/U4B3vmWg9bg",Neutral
AMD,online games might ban you,Negative
AMD,"Hardware Unboxed posted a nice video showing how to do this and comparison shots between both versions of FSR4, FSR3 and XeSS.  Link: https://youtu.be/yB0qmTCzrmI?si=uDwOIn22RImA0ZN0",Positive
AMD,Yeah I'd like to do this with my 6800 but I'm not mucking around with unsigned drivers and online anti-cheats. I'll stick to XeSS for now.,Negative
AMD,"> If you live in a bunker  That's an unnecessarily unpleasant way to put it, but ok.",Negative
AMD,I can't think of a game with anti-cheat that requires upscaling to run well. Most of the popular online games have potato graphics because they need to support the vast install base of 1050's  Even the worst RDNA2 card is ~ a 1060,Negative
AMD,"I haven't heard any cases of this happening,but I get your point.",Neutral
AMD,With the redstone update is probably when they will put it out,Neutral
AMD,"Ask Sony.  If AMD was working on something like this, I bet it was because they wanted FSR4 to run on the PS5. If that is still the plan, I think RDNA2 gets FSR4 after Sony launches the same thing. If the plan changed and Sony no longer wants to do FSR4 on the PS5 (to push upgrades), I don't think it is coming.",Neutral
AMD,"no, but online games might ban you",Negative
AMD,"The swapped files are from official drivers ain't it? should be fine if you do it yourself, but yeah anti cheat might pick it up and trigger it",Neutral
AMD,There is no FSR 4 FG yet.,Neutral
AMD,Optiscaler allows injection of different frame gen techniques if the games allow it. You can also consider Lossless Framegen app,Neutral
AMD,It does?,Neutral
AMD,Buddy it never left,Neutral
AMD,"Well, it's as heavy as DLSS 4",Neutral
AMD,"I've been curious about this as I plan to play Battlefield 6... if I modded the driver but I'm *not* using Optiscaler in a game, does it still carry the ban risk? Or is that only if I'm actively using Optiscaler in the game to enable FSR 4 and such?",Neutral
AMD,Not saying this will happen....but you do need to be careful.  Just because you didn't get banned instantly is no guarantee you are safe. Online games like to collect up a bunch of people they want to ban and then ban them all at once.,Negative
AMD,You mind posting a vid or guide you used for this? Something I’d like to try out at some point.,Positive
AMD,.dll swap is needed only for RDNA 2,Neutral
AMD,"No. But it should run well on Radeon VII, which supports INT8.",Positive
AMD,Yes. It supports INT8.,Neutral
AMD,"LMAO, I remember buying a 6700 XT 4 years ago because of his videos. Glad to see he is still at it.",Positive
AMD,Can you point me to a simple tutorial to do this on RDNA 3 on a 7800XT. Thanks in Advance😁,Positive
AMD,Marvel Rivals. It's super unoptimised,Negative
AMD,"The Finals is a newer one that benefits from ray tracing shadows, so any upscaling does help the game there.",Positive
AMD,I wanted to enable Radeon Chill on BF6/BF2042 and their anti cheat wouldn't even let me launch the game,Negative
AMD,"Its not about running well, its about the image quality.",Negative
AMD,Squad,Neutral
AMD,Fortnite runs like shit even with upscaling,Negative
AMD,"Forsure my friend said that they are signed so they could be malware if I download somewhere or something.   Idc about online games, I usually need upscaling with single player haha.",Neutral
AMD,Do you link?,Neutral
AMD,"Just use Lossless scaling, its for like 4USD on steam right now.",Neutral
AMD,rdna3 was looking pretty abandoned there for a while to be completely real with you  i mean it still is but community tools are doing amd’s job for them,Neutral
AMD,"You're right, if you really care, you should be careful. I mentioned Valorant because it has one of the strongest anti-cheats, if not the strongest, and it would be able to detect things like this, but Riot usually does ban waves for bot accounts. If they suspect you're a cheater, they ban you almost immediately.  If you play CS2, you shouldn't worry (because they don't even ban real cheaters). VAC \*currently\* doesn't have the ability to detect these types of changes unless you try to inject them into the game.  I don't know enough about Battle-Eye or Easy Anti-Cheat to give examples.",Neutral
AMD,"The video I watched was in Spanish and was a bit more complicated. Now it's easier to do by adding the files to the driver.  You can watch this video: [https://youtu.be/U4B3vmWg9bg?si=YqZdIOGXYh3vsjAG&t=818](https://youtu.be/U4B3vmWg9bg?si=YqZdIOGXYh3vsjAG&t=818)  Keep in mind that this will install the driver as unsigned, and you may experience issues with online games.  I think because of the way I did it, Windows still thinks my driver is signed and I haven't had any problems.",Neutral
AMD,Meaning ?,Neutral
AMD,"Still using it, still loving it, might get a 9060XT soon though...",Positive
AMD,300fps on an RX6400 is shit to you?  Even 1440p low is 100fps on that.,Negative
AMD,"They are signed because they are official from amd, signed by him accidentally in the last SDK, the optiscaler and FSR 4 did not work in vulkan games and with anti cheat",Negative
AMD,"I have it and use it, great software",Positive
AMD,AMD FSR 4 on Radeon RX 7000 “RDNA3” works with latest drivers without using simple file swap workaround,Neutral
AMD,Perhaps the 10600xt (?  I think the 9060xt would be a side grade The 9060xt is between 18 and 25% better that the 6700xt which is kinda sad  Considering that que 6700xt came out in 2021,Negative
AMD,a 9060xt over a 6700xt?  not really a meaningful upgrade besides more ram,Negative
AMD,Constant FPS drops on 9070 xt aren't good,Negative
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,Meaning fsr4 works ?,Neutral
AMD,"I’m not gonna lie boss, I have started seeing  9070XT’s for less than what I paid for my 6800XT. It’s tempting man. I mean, if I was willing to spend that money on it once, surely I’m willing to spend *less* on the current most obvious AMD upgrade lol.  We’re only talking like €30,- less than the 6800XT I have still north of €600,-, but still.",Positive
AMD,yeah kinda sad. not for me tho from 480 4gb it was massive lol. FSR4 looks really great to me,Positive
AMD,"The idea is to use 6700xt for Lossless scaling frame gen, while 9060xt runs the game. Could wait longer, for whatever the next equalent card will be called. 10600 xt is fucking rediculous name though, hope they don't go with that..",Negative
AMD,"Was gonna say ""FSR4 though"" then remembered the topic of this thread",Neutral
AMD,Lower your settings mate. The game runs great.,Positive
AMD,AMD FSR 4 on Radeon RX 7000 “RDNA3” works with latest drivers,Neutral
AMD,"It does seem like RDNA4 prices are finally slipping down towards the fabled """"MSRP"" level",Negative
AMD,If I wanted to play the game with potato graphics I'd play it on my Switch or my phone. It stutters much harder than any other UE5 game,Negative
AMD,It's FORTNITE. Its baseline is potato. Turning raytracing or whatever on is a waste. Its just applying fancy lighting to potato assets. I guess if you like putting lipstick on a dog butt and kissing it.  How are you getting bad fps on a 9070 lmao. The game runs really good on RX6400 and REALLY good on RX6600,Negative
AMD,the game has stutters what are you not understanding about this ?,Negative
AMD,"Impressive gains on the SD3, a 2x improvement, and about 40-50% on the other models. Do these gains apply to RDNA 3? Did the update improve performance on LLMs as well?",Positive
AMD,"This is drawing a lot of power for a 9070, I assume the consumption measurement is inaccurate?",Negative
AMD,"haven't tested on rdna3, but since 7800XT was outperforming the 9070 in the beginning I guess there is less room for improvement.",Neutral
AMD,"it is accurate, 9070XT Bios",Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,BiFrost having 3 fans doesn't sit well in my brain 😅,Negative
AMD,https://i.redd.it/lz9xieyxnhtf1.gif,Neutral
AMD,Powercolors spectral white is so much better looking.  https://i.imgur.com/A1fDKqs.jpg,Positive
AMD,"They used to have 2 fans, one regular fan and one blower fan.",Neutral
AMD,Should've named it TriFrost then.,Neutral
AMD,"I understand your comment.  My autistic brain reads 'bifrost' as the mythological connection between the realms in Norse mythology where 'bif' means shining, whereas 'bi-' meaning 'two' is from Latin.  So, homophonic, sure, but completely distinct in gloss.",Neutral
AMD,[Agreed brother](https://i.imgur.com/MCHoQ7S.jpeg),Neutral
AMD,pitcairn is the unsung greatest of all time tbh,Positive
AMD,this is why linux rocks the llamas ass,Negative
AMD,"8th gen console GPUs, 8 GB shared RAM for 2013 PS4/Xbox one. While average midrange PC gpu in 2013-2015 were running in 2 GB VRAM  Best midrange value for that console generation: 2017 RX 500 series. With 4 GB minimum VRAM. 8 GB VRAM for 10 year usage.",Positive
AMD,"They were very good gpus, unfortunately lack of funds caused limited development for quite a bit of time on the Radeon side, while amd tried to survive. My own 7970 was only retired from frontline use the year the new 7000 series came out, but was replaced with a 6800. I had bought a few other used gpus the last few years before for trouble shooting … issues ended up being the cpu",Negative
AMD,"Very fond of those cards, HD7770GHz was my first GPU, incredible the gaming experience £100 got me back then.",Positive
AMD,based i wish i kept my 390x that thing was a beast,Negative
AMD,o7,Neutral
AMD,This is THE FineWine(tm). :),Positive
AMD,"Saying ""HD 7000"" makes it sound old, but it's AMD Radeon R9 200 series",Neutral
AMD,"I was running FSR 3 frame generation in Cyberpunk on a FirePro W700 (equivalent to an HD 7850) and it was working really well actually. I also got XeSS running, but that tanked performance understandably, because these GPUs don't natively support SM 6.4 but it seems that they've received driver updates to support SM 6.5 or something. GCN 1.0 only natively supports SM 5.7 iirc. I've been doing some tests on that HD 7850 FirePro GPU because it has 4GB of GDDR5 which is a minimum nowadays for most games. But I was even playing Doom Eternal at native 1080p at over 60fps at low settings. I've got a video about it in my channel if anyone's interested in [that.](https://youtu.be/5rcLg7Xo2rs)",Positive
AMD,"Ah yes , like back when the 7990 was the tHe mOsT PoWeRfUl gPu iN ThE WoRlD!!!!1 and the shitty driver forced me to use Compiz over KDE!  I member  Guess is good thing for whoever still in series HD 7000 but that’s an era I would like to forget.  (I wasted my hard earned money on that shit of a card)",Negative
AMD,"I see it's gonna be added to the kernel, but do you need to do anything to take advantage of it? I tried installing Nobara on a 7970, and yeah it was...odd.",Negative
AMD,Best miner ever,Positive
AMD,my r7870 overclocked to 1475MHz gpu 1510MHz mem.,Neutral
AMD,I owned both the 7870 and the 7850 They still run to this day as display adaptors for a friend's and his dad's computers.,Neutral
AMD,All 5 people are pretty excited,Positive
AMD,See this 1 week after my 8990 decided to die. One of the worse cards I've owned. Wonder if these updates would of made it decent,Negative
AMD,precious developers time wasted on obsolete hardware...,Negative
AMD,They rebranded that shit soooo many times. That's how good it was.,Positive
AMD,The 7970 GHz edition was a beast for GPU mining back in the day.,Positive
AMD,"Sir, I think you're mixing that saying with Winamp.",Neutral
AMD,"Yup, as soon as anti-cheat widely adopts linux to the point where all my fav multiplayer games run on it, I'm fully switching to linux and never looking back cause an OS that just does what I tell it to (and also has low-latency audio, unlike windows audio, turns out our audio reaction time is 3 times faster than even the top esports players visual reaction time, making audio latency actually kinda important) without any weird extra steps sounds like heaven.  Like seriously, only installing new things myself that I actually want and use? That's something I look forward to, might actually get to try out some AI softwares when a new one isn't popping up every other day like some kind of FNAF jump-scare.  Fingers crossed the steam deck and steamOS gets widespread linux compatibility over the final hurdle, game compatibility is literally the only reason I'm not running it (though R6 is kinda ass now so I might end up switching anyways).   Of course banning kernel-level anti-cheat and any other kernel-level software that has any ulterior function other than anti-virus, is the ultimate goal since anything with access to your kernels is fully capable of using your computer to commit crime. However the general public doesn't understand that kernel-access = prime crime time, so I'm not holding my breath for that.",Positive
AMD,Such a disappointment as a generational hardware… compared to how powerful the 360 was to pc gpus at launch. AMD cut a sweetheart deal to stay alive and we had an underwhelming generation that impacted game development till about 2 years ago,Negative
AMD,"I also had the 7770, about US$70 from TigerDirect(rip)... Was a different time where I didn't care about chasing graphics. It just worked solidly. Nowadays I'm complaining to myself about my 5070ti not running E33 smoothly at Epic settings in 4k lol",Neutral
AMD,I still have my 7970ghz! Maybe I'll actually get around to building a Linux box with the littany of parts I've collected from upgrading various pcs over the last 13 years haha.,Positive
AMD,o7  indeed,Neutral
AMD,"No. These are older than that. It literally says the 7000 series and that’s what they were called - 7970, etc.   The R9 290X was released in 2013.",Neutral
AMD,"Wrong. GCN 1.0 was 7000 series (with exception of 7790 which was GCN 1.1). R9 200 series was a mix of 1.0, 1.1, and 1.2 (Hawai’i and Bonaire were 1.1, Tahiti, Pitcairn, and Oland 1.0 and Tonga 1.2).",Neutral
AMD,"The bug-fix addresses Tahiti and Pitcairn GPUs, which as far as I remember, are GCN 1.0.",Neutral
AMD,"Actually now that you mention it, yes. You need to enable the ""new experimental"" version of the installed driver with kernel boot options (included in the main amdgpu driver as part of the kernel).[AMDGPU - ArchWiki](https://wiki.archlinux.org/title/AMDGPU)  radeon.si_support=0   amdgpu.si_support=1 radeon.cik_support=0 amdgpu.cik_support=1",Neutral
AMD,"Yeah, I wonder how many people are still using a 2013-vintage GPU these days?",Neutral
AMD,They did all this work for 5 people. Moron,Negative
AMD,"Why do you want to see working hardware, go to waste.",Negative
AMD,well that and AMD was struggling thats why the CPU team had to go to Lisa Sue to be like listen we gotta stop focusing on the next reiteration so we can do a whole new design. then ryzen was born. that was actually a make or break moment for the company Lisa whent for it. cool videos with the engineers at amd offices on gamers nexus.,Positive
AMD,...which doesn't exist for Linux btw.,Neutral
AMD,WinAmp best.  Windows is in the name.,Positive
AMD,"Kernel-Level-equivalent Anticheat will never come to Linux. It might come to some ""Android""-esque Linux kernel based desktop operating system over which you have no control over though.  I wouldn't call that ""Linux"" though.  Sorry, if you want to give games companies full control over your computer in order for them to let you play their games then you'll just have to stick to closed platforms.",Negative
AMD,Been using Bazzite for a few months. Not going back...,Neutral
AMD,Easy to leave now then .. Not ALL your games will work.. All mine do however :),Neutral
AMD,"anti cheat companies are paid under the table to not support linux, like how intel paid dedl land hp to not use AMD chips for their flag ship pcs for all those years.",Negative
AMD,the moment linux starts to become more popular it will attract hackers too..,Negative
AMD,Xbox 360 has 512 MB total RAM.  Midrange/entry PCs were bleeding in 7th gen consoles era (2006 PS3) with Sub-512 MB graphic card and Windows Vista demanding memories (above 1 GB RAM requirement).  until 2009 Windows 7 hardwares came affordable with 1 GB VRAM in midrange graphic card pricing and 4 GB DDR3 RAM. Able to get 60 fps HD resolution.,Neutral
AMD,"PC hardware has only been marginally better than consoles from the PS4 onward, come on now. We're seeing diminishing returns on this tech altogether, not ""stagnation"". There isn't much room for a new Crysis sort of project anymore.",Negative
AMD,The 270X is a rebrand of the HD 7870 and the 280X is a rebrand of the HD 7970,Neutral
AMD,Yep and those were rebands of the 7000 series.,Neutral
AMD,"What you're mentioning makes it sound like it was a whole sleuth of cards in the 200 series that weren't rebrands, when realistically it was two or three cards that were actually new.",Neutral
AMD,"We can call them GCN 1-2-3 now, it's fine. Everyone calls Polaris and Vega GCN4 and 5 after all.",Neutral
AMD,"R9 280x says hi.  Edit: I didn’t read the post properly, and that it was mentioning GCN 1.0.",Neutral
AMD,"https://www.techpowerup.com/gpu-specs/amd-tahiti.g120  Both, as 280 is a rebranded Tahiti.  7970 was a hell of a card though. HD 7850 is doubtlessly the best card I've ever owned. Absolutely wonderful experience coming from a GTX 460 and many Nvidia cards before.  Edit: To add I think it's the last truely great generation from ""ATI""/AMD. 4000 series was pretty banger too, HD 4770 was major value.",Positive
AMD,Dozens. GPUs are expensive.,Negative
AMD,I got it on my second machine at home. I actually finished a playthrough of Cyberpunk2077 with it and my daughter uses it when it comes to my home mostly to play GtaV.  The 7970 has served me well and still does.,Positive
AMD,We have our own tho. Look up XMMS.  100% compatible with Winamp skins iirc.,Positive
AMD,"True, but Winamp works just fine through Wine or Bottles.",Positive
AMD,"It very nearly got a Linux port way back in the original v3/v5 days. Sadly it was canceled. But then XMMS, and later Audacious, were created, possibly as a response.  The developers of the current Winamp successor, WACUP, also stress their Wine support. WACUP would *like* to have a native port to Linux, but they don't really have anyone willing to do it right now. Most of WACUP development is still just one guy far as I know.",Negative
AMD,"Not a great reason. Putting ""Win"" in the name was to differentiate it from DOS-based players. That's how old Winamp is. Winamp even had a DOS version at first.  There was supposed to be a Linux port, though it was canceled. There were successful Mac ports since the beginning.",Neutral
AMD,"Yeah, I guess, though there are still a ton of games I play that are windows exclusive unfortunately.  What's funny is I'm waiting for windows to ban kernel level access, because once that happens linux compatibility will probably become universal (or at least, the games that don't work on linux, also won't work on windows).",Negative
AMD,"There are already plenty of hackers for Linux, they just target servers and data centers.",Negative
AMD,"Memory was a weak point yes, but in terms of processing power the 360 gpu was arguably top of the market when it hit, being pushed down to second place a month or so later. PC hardware was moving very quickly then, but the concepts placed in its gpu carried over to next gen gpus in the pc quite well.   I would also say shared memory on the Xbox one was deceptive as more was reserved (10% originally) for the system lowering useable pool by the gpu and cpu, home towers by the point often had 8gb of memory with gpus between 2 and 4gb, though quickly moving to 6 to 8. The 2 and 3 gig gpus left at moderate settings quite quickly.   Further I would say the evidence of hindering next gen games can be seen in what we know were axed from titles supporting it, like infinite local co-op, and comments by developers like the Xbox one/ Xbox series gen. Vs how the games that made the jump from the 360 generation to the one generation faired.   The series S while a sales darling is another boat anchor around game development (see baldurs gate 3) for the Xbox brand, it’s truly unfortunate how often they knee cap themselves… I say this as an original Xbox owner and until about 2 years ago gold/ultimate player.",Neutral
AMD,"we all ran XP till 7 beta came out.  we also had more than 1.5gb of ram, at least the peeps i hung out with on IRC.",Neutral
AMD,"I would say on the hardware front we have seen great gains, it’s more the development front that has stagnated relying too much on middleware and not leveraging hardware properly… or where they do for features who’s returns are dubious…. It’s not the 00s where each generation was a wizbang upgrade, nor the early 10s where resources for resolution popped…. Nvidia is focused else where, and is still recovering from almost dieing as a company, and gpus paid a heavy price… and intel is dealing with a decade of poor choices…. Despite all of that hardware has advanced pretty well, while your hardware can probably last a decade at this point with settings degrading until playability is lost, it’s not a hardware progression issue. Check out what they were doing with the 360 at the end of its run… compared to what came out the end of the Xbox one run… not to much of a change outside of some major temples like rdr2, and that quality of a game is far between release as to what we should have…. Developers have been leaning on performance improvements to put less work into games",Neutral
AMD,Doesn't change those were release in 2011,Neutral
AMD,The 290X is not a rebrand of anything.,Neutral
AMD,"Not really sure how that's relevant since my point was that it goes back even further than the RX series, but it would apply to all of them if they are using the same GPU.",Neutral
AMD,"The 290X was about 50% faster than the 7970 predecessor.  It provided the same performance as the new $1000 GTX Titan at a bit more than half the price.  Yeah, ""slightly improved"" sounds reasonable.",Positive
AMD,There also was the RX 285 and R7 250. Other than that they were all rebrands.,Neutral
AMD,He literally said a mix of GCN 1.0-1.2 on the 200-series though. So his comment isn't wrong. But it kinda contradicts his statement.  The 2XX series had a lot of rebranded cards.,Neutral
AMD,Isn't that just an overclocked and rebranded HD 7970 GHz?,Neutral
AMD,My 270X was faster and had more memory than my HD7x50 class card.,Positive
AMD,ran dual 7870s for 4-5 years. Crossfire was overhated when it worked it was amazing.,Positive
AMD,"I still love my 5700xt, it was kinda dud at launch (it still had driver issues after like 1yr lol) but it's treating me super well even today",Positive
AMD,"Aha, well... I even got a gifted MSi nVidia 4070 super something from friend who used it, but my Silver-certified BeSilent 400W PSU wont ever power that thing.    Maybe I should sell that thing for bux. Hm....",Negative
AMD,Missed opportunity to call it Linamp,Negative
AMD,Thank you.,Positive
AMD,tweaks and fonts required? new rebooted winamp or the original?,Neutral
AMD,"Well, it is Windows AMP.  But a DOS version existed too. So... upvote?",Neutral
AMD,a ton ?,Neutral
AMD,I’ve always been curious how 360/PS3 games would look if the consoles had double or triple the ram. 1gb shared for 360 and 512mb/512mb for PS3,Neutral
AMD,"That's quite generous, it was a paper launch on december 22nd, 2011, with availability starting january 9th",Positive
AMD,Yeap.,Neutral
AMD,"Rebranded, yes. But 7970 GHz is higher clocked than 280X. The 7970 GHz edition is therefore faster, marginally though. [280X](https://www.techpowerup.com/gpu-specs/radeon-r9-280x.c2398) [7970 GHz](https://www.techpowerup.com/gpu-specs/radeon-hd-7970-ghz-edition.c365)",Neutral
AMD,"Aye, and the dual GPU cards was pretty cool. Nvidia also had a few! Not to mention 3DFX which actually created SLI.",Positive
AMD,"I never used that generation, but it is infamous for the driver issues.    I never had any more issues with ATI/AMD than with Nvidia. Far less black screens with AMD/ATI, especially early on with ATI as their ""VPU"" recover was far superior to Nvidas ditto. To be fair it took a few years for Nvidia to make a similar function that worked, if I remember correctly. More issues on new releases with AMD though. Which is expected as developers would be stupid to not focus optimizing for Nvidia due to market share. But in general hardware has been really stable last 10-15 years IMO, at least compared to late 90's early 2000's :D",Neutral
AMD,"Calling it Linamp would imply it only runs on Linux.  The creators of XMMS were thinking big. XMMS not only runs on Linux, but also BSDs, Solaris and more. It can even run on Windows using Cygwin or MingW. As long as it supports Unix C and has an X server, it runs XMMS.",Neutral
AMD,Original.,Neutral
AMD,"No tweaks needed, no fonts required unless you want them. You just pick whatever font you want for Winamp.  Both the original and WACUP work under Wine.",Neutral
AMD,Most multiplayer games that use kernel level anti-cheat cause Linux says no to anything touching its kernels.  Unfortunately I predominantly play multiplayer games so I'm SOL when it comes to linux gaming.,Negative
AMD,Like 5... Mayyyyyyybe 6.,Neutral
AMD,"I would say better draw distance, and slightly better texturing at the end of the generation…. It’s really amazing what they pulled out of that hardware to the end of the generation. The extra memory would have probably been very nice for gta/skyrim … I think a bigger question is what would PlayStation be like if the second cell chip as gpu had worked out, and programming for cell was a bit easier… the kissing cousins of powerpc architecture from the g5 to cell to xenon is an interesting read",Positive
AMD,"Oh yeah? Well, your mom is a rebrand of your grandma!",Neutral
AMD,"I had pretty regular crashes (maybe twice a week) for a year and a half or a so after i got it lol, even after ""the drivers have been fixed"" for the third time hahaha  It's a rock for me now, and I was never frustrated with it really because I got it knowing about driver issues and getting it cheap, and I was so happy with it's performance uplift vs my 970ti. I wasn't rich enough to get anything better and it was a steal, liek £150 off, becuase of driver issues I think. so I was really happy.   I get nauseous at low fps/hz, so I think that's probably teh biggest reason why I didn't care about the crashes - getting stable high frame rates in more games was way more significant than any crashes. I think if i was a normie I might've regretted it lol  it did feel like vintage hardware though lmao, it was so shitty the first couple months I had it especially",Positive
AMD,Posixamp doesn’t have the same ring to it,Negative
AMD,I play multiplayer too,Neutral
AMD,Was there plans for a second cell chip?  I know initially they didn’t have the Nvidia GPU but I thought it was a “this single cell chip and SPEs can do everything!”,Neutral
AMD,"Dang. I would've been fairly annoyed with that amount of crashes to be honest :P But I also sacrifice all I can to keep a high refresh-rate.  I've refunded games locked at 60FPS many times. But since AFMF2 released some have been spared if it worked well, like SOMA.  Edit: Wow, double posted apparently ""error 500""..",Negative
AMD,Also there’s already a separate project called Linamp. Apparently it’s some touchscreen in car entertainment system that runs Linux.,Neutral
AMD,"I could have sworn it was, but it’s been a bit since I read through it I know the ps3 gpu was a very late addition to the hardware",Neutral
AMD,If I remember well the CELL itself was suppossed to be used for graphics processing on it's own too.  After all it is a CPU with a single core with SMT and seven SPEs as co-processors   But it wouldn't panned out well so they added an NVidia GPU late in development.  Still the CELL had some strong points compared to the GPU so some devs used it to improve graphics too.,Neutral
AMD,"Cell was a very interesting chip, especially if you had been following PowerPC before it. So much possibility and power, but required a lot of the dev team, and I don’t think Sony ever got the dev tools really worked out as well as they wanted. Huh, I wonder where I got the second cell chip from going to have to research that now",Positive
AMD,And largely against the non-x3d lmfao.,Negative
AMD,Aren't they just showing that AMDs CPUs are better for gaming?,Neutral
AMD,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Neutral
AMD,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Neutral
AMD,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Negative
AMD,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Neutral
AMD,I assume they compared with CPUs in a similar price range,Neutral
AMD,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Negative
AMD,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Positive
AMD,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Neutral
AMD,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Neutral
AMD,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Negative
AMD,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Positive
AMD,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Neutral
AMD,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Negative
AMD,"Now install windows 11, lol",Neutral
AMD,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Negative
AMD,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Neutral
AMD,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Neutral
AMD,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Neutral
AMD,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Positive
AMD,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Neutral
AMD,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Neutral
AMD,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Neutral
AMD,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Neutral
AMD,I did same performance on all processors.,Neutral
AMD,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Neutral
AMD,That sounds like an AMD Stan argument circa 2020,Negative
AMD,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Neutral
AMD,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Negative
AMD,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Neutral
AMD,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Neutral
AMD,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Positive
AMD,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Neutral
AMD,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Negative
AMD,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Negative
AMD,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Neutral
AMD,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Neutral
AMD,Expand ?,Neutral
AMD,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Positive
AMD,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Negative
AMD,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Negative
AMD,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Neutral
AMD,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Negative
AMD,did you do it,Neutral
AMD,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Neutral
AMD,can you reset settings then choose ray tracing ultra preset.,Neutral
AMD,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Neutral
AMD,because they exclusively exist in DIY build your pc enthusiast bubble,Neutral
AMD,Pricing was aggressive. A 12 core 3900x was 400 usd.,Neutral
AMD,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Neutral
AMD,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Neutral
AMD,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Negative
AMD,"Okay, I did it",Positive
AMD,"No, I didn’t remember good",Negative
AMD,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Neutral
AMD,Thanks for solidifying opinion that your benchmarks are fake,Negative
AMD,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Neutral
AMD,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Neutral
AMD,"What’s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If you’re comparing score screenshots to OBS-recorded videos, you shouldn’t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. I’ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think they’d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since I’ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Negative
AMD,Cam someone confirm or is this gas lighting?,Neutral
AMD,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Positive
AMD,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Positive
AMD,Intel comeback real?,Neutral
AMD,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Neutral
AMD,3D v-cache has entered the chat.,Neutral
AMD,Take it as a grain of salt. Intel marketing LOL,Neutral
AMD,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Neutral
AMD,Thats cool ...but lets talk about better pricing.,Positive
AMD,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Neutral
AMD,Tech Jesus has entered chat :).,Positive
AMD,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Neutral
AMD,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Positive
AMD,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Neutral
AMD,What do you mean by gaslighting in this case?,Negative
AMD,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Neutral
AMD,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Neutral
AMD,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Negative
AMD,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Negative
AMD,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Negative
AMD,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Negative
AMD,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Neutral
AMD,Nova Lake bLLC about to ruin Amd X3D party.,Negative
AMD,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Negative
AMD,I always wondered if Intel marketing budget is higher than the R&D budget,Neutral
AMD,Intel Arrow Lake is much cheaper than Amd Zen 5.,Positive
AMD,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Negative
AMD,only an AMD fan would worry about replacing their shit CPUs under 3 years,Negative
AMD,Hardware unboxed isn't a reliable source.,Negative
AMD,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Neutral
AMD,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Negative
AMD,Telling people that its performance is better than it actually is?,Negative
AMD,The ones with similar pricing not performance,Negative
AMD,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Neutral
AMD,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Negative
AMD,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Positive
AMD,Quite common for AM4 in my experience.,Neutral
AMD,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Neutral
AMD,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Negative
AMD,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Positive
AMD,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Neutral
AMD,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Neutral
AMD,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Neutral
AMD,Sooo they are in the YouTube space for the money not for the love of tech,Neutral
AMD,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Negative
AMD,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Neutral
AMD,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Negative
AMD,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Negative
AMD,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Negative
AMD,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Negative
AMD,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Negative
AMD,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Neutral
AMD,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Neutral
AMD,Sure but charts seem about right to me,Positive
AMD,APO is game specific. I'm referring to what has changed overall.,Neutral
AMD,"purchased a qualifying intel cpu with the Newegg bundle last month, part of the promotion was a free copy of battlefield 6.  Intel messed up the offer by not providing a code that aligned to the sale, which they then re-issued the coupon codes.  My problem is now the BF6 code does not work and I’ve raised it with customer support who have not responded.  Anyone have similar issues and got a resolution?",Negative
AMD,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Negative
AMD,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Negative
AMD,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Neutral
AMD,Thank you,Positive
AMD,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Neutral
AMD,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Positive
AMD,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Negative
AMD,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Negative
AMD,"i think it's bad for us, consumers",Negative
AMD,Was the team up really to crush AMD or Nvidia's answer to enter China?,Neutral
AMD,AMDware unboxed only cares about AMD anyway,Neutral
AMD,This hurts the arc division way more than this could ever hurt amd.,Negative
AMD,They will crush user's wallet,Negative
AMD,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Neutral
AMD,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Positive
AMD,Remember Kaby Lake G? No? This will also be forgotten soon.,Negative
AMD,Yes.,Neutral
AMD,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Neutral
AMD,Foveros baby!,Positive
AMD,AMDUnboxed on suicide watch.,Negative
AMD,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Neutral
AMD,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Positive
AMD,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Negative
AMD,Ohh noooerrrrrrrrr,Negative
AMD,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Neutral
AMD,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Neutral
AMD,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Neutral
AMD,welcome to the Nvidia and amd duopoly,Positive
AMD,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Positive
AMD,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Neutral
AMD,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Neutral
AMD,a partnership doesnt mean they get free reign over license lol,Neutral
AMD,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Neutral
AMD,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Neutral
AMD,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Negative
AMD,Why would it?,Neutral
AMD,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Neutral
AMD,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Negative
AMD,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Negative
AMD,Past != Future,Neutral
AMD,"nvidia also used to make motherboard chipset, with mixed success.",Neutral
AMD,Arc’s offerings aren’t far behind either.,Neutral
AMD,FSR 4 looks like the later versions of Dlss 2 did,Neutral
AMD,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Neutral
AMD,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Negative
AMD,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Neutral
AMD,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Neutral
AMD,Never said that.,Neutral
AMD,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Neutral
AMD,Why do so many people think that this will kill ARC?,Negative
AMD,The market for Arc is the same as for Nvidia.,Neutral
AMD,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Positive
AMD,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Positive
AMD,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Negative
AMD,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Neutral
AMD,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Negative
AMD,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Negative
AMD,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Negative
AMD,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Neutral
AMD,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Negative
AMD,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Negative
AMD,Nvidia does not have an A310 competitor.,Neutral
AMD,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Neutral
AMD,I have not lied,Neutral
AMD,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Neutral
AMD,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Negative
AMD,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Negative
AMD,Intel doesn't have a current gen A310 competitor either.,Neutral
AMD,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Negative
AMD,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Positive
AMD,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Negative
AMD,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Positive
AMD,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Negative
AMD,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Neutral
AMD,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Negative
AMD,The later versions of Dlss 2 look like Dlss 3,Neutral
AMD,Nvidia probably feels the same about their low end SKUs.,Neutral
AMD,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Neutral
AMD,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Negative
AMD,Yeah lol,Positive
AMD,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Neutral
AMD,"""Everyone I don't like is biased""-ass answer",Negative
AMD,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Neutral
AMD,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Negative
AMD,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Negative
AMD,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Neutral
AMD,Just hodl until you get the biscuits,Neutral
AMD,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Negative
AMD,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Neutral
AMD,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Neutral
AMD,"why even panther laker when it was only for mobile, cancelled it and just released nova lake next year",Negative
AMD,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Neutral
AMD,I thought Arrow Lake refresh was in the cards for 2025.,Neutral
AMD,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Neutral
AMD,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Negative
AMD,This entire thing is a mobile roadmap so why are you here?,Negative
AMD,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Neutral
AMD,"Yeah if it's Surface roadmap, it's a nothing burger.",Negative
AMD,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Neutral
AMD,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Neutral
AMD,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Neutral
AMD,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Neutral
AMD,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Neutral
AMD,"I like how they just throw random words around to pad their ""article"".",Neutral
AMD,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Neutral
AMD,"""leaks""",Neutral
AMD,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Negative
AMD,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Negative
AMD,Scared of their rumor?  Lets release our rumor!,Neutral
AMD,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Negative
AMD,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Neutral
AMD,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Negative
AMD,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Neutral
AMD,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Negative
AMD,bLLC is not stacked cache,Neutral
AMD,What do you consider random? The article was perfectly clear.,Neutral
AMD,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Neutral
AMD,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Neutral
AMD,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Negative
AMD,Yeah. Definitely just you,Neutral
AMD,You could literally make that claim with any CPU performance increase.,Neutral
AMD,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Neutral
AMD,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Neutral
AMD,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Neutral
AMD,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Negative
AMD,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Neutral
AMD,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Negative
AMD,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Neutral
AMD,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Negative
AMD,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Neutral
AMD,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Neutral
AMD,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Negative
AMD,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Negative
AMD,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Negative
AMD,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Negative
AMD,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Neutral
AMD,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Positive
AMD,It's not sorcery. Its just Intel doing the game developers work.,Neutral
AMD,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Negative
AMD,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Negative
AMD,Never count out Intel. They have some very talented people over there.,Positive
AMD,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Neutral
AMD,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Negative
AMD,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Neutral
AMD,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Neutral
AMD,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Negative
AMD,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Positive
AMD,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Neutral
AMD,I will follow all!,Positive
AMD,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Positive
AMD,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Negative
AMD,How is this doing the game developers work?,Neutral
AMD,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Negative
AMD,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Negative
AMD,The intel software team is pure black magic when they allowed to work on crack.,Negative
AMD,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Neutral
AMD,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Negative
AMD,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Positive
AMD,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Neutral
AMD,Balanced in full load will just do the same thing as High Performance.,Neutral
AMD,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Neutral
AMD,Or... Just process lasso.,Neutral
AMD,Because it’s optimizations on how it can efficiently use the cpu.,Neutral
AMD,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Neutral
AMD,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Positive
AMD,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Neutral
AMD,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Neutral
AMD,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Neutral
AMD,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Positive
AMD,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Positive
AMD,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Neutral
AMD,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Negative
AMD,You need to download the Intel Application Optimization app from the Windows store,Neutral
AMD,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Positive
AMD,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Positive
AMD,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Positive
AMD,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Neutral
AMD,Where to download this APO,Neutral
AMD,Except its THE game devs job to optimize games for multiple cpus and gpus.,Neutral
AMD,It’s literally their job to do so? wtf you talking about?,Negative
AMD,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Neutral
AMD,"Will do, I got a 265K. Performance is already great tbh.",Positive
AMD,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Neutral
AMD,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Neutral
AMD,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Neutral
AMD,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Negative
AMD,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Negative
AMD,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Negative
AMD,That's not simply what APO does.,Neutral
AMD,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Neutral
AMD,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Positive
AMD,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Positive
AMD,i am not talking about older games. i am talking about newer games.... really dude?,Neutral
AMD,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Neutral
AMD,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Neutral
AMD,Thanks a lot.,Positive
AMD,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Negative
AMD,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Neutral
AMD,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Negative
AMD,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Negative
AMD,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Negative
AMD,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Neutral
AMD,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Positive
AMD,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Positive
AMD,Did the DP4A version also improve from 1.3 to 2.0?,Neutral
AMD,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Neutral
AMD,Okay but why would I want to use that instead of NVIDIA DLSS?,Neutral
AMD,It’s the least you should get after not getting FSR4.,Negative
AMD,You'd use it over FSR if that's available too?,Neutral
AMD,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Neutral
AMD,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Negative
AMD,And they expect people who bought previous RDNA to buy more RDNA,Neutral
AMD,Not by a significant amount.,Neutral
AMD,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Neutral
AMD,for the games that dont support DLSS,Negative
AMD,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Neutral
AMD,10 series cards will benefit from this,Positive
AMD,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Negative
AMD,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Positive
AMD,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Positive
AMD,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Neutral
AMD,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Negative
AMD,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Positive
AMD,DP4a is cross vendor.   XMX is Arc only.,Neutral
AMD,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Neutral
AMD,1080ti heard no bell,Neutral
AMD,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Neutral
AMD,where did amd touch you bud?,Neutral
AMD,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Neutral
AMD,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Neutral
AMD,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Neutral
AMD,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Positive
AMD,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Negative
AMD,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Negative
AMD,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Neutral
AMD,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Negative
AMD,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Negative
AMD,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Negative
AMD,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Neutral
AMD,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Neutral
AMD,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Negative
AMD,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Neutral
AMD,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Neutral
AMD,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Positive
AMD,Thank you :),Positive
AMD,Wouldn't the 300 series actually be Arrow Lake Refresh?,Neutral
AMD,"Videocardz is wrong      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Neutral
AMD,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Positive
AMD,This is pat's work,Neutral
AMD,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Neutral
AMD,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Positive
AMD,Is Intel finally making a comeback with their cpus? I hope so,Positive
AMD,Large L3 cache without reducing latency will be fun to watch.,Positive
AMD,the specs sure do shift a lot..,Negative
AMD,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Positive
AMD,it's just a bunch of cores glued together - intel circa 2016 probably,Neutral
AMD,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Negative
AMD,Bout time,Neutral
AMD,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Negative
AMD,Will it be available in fall of this year or 26Q1?,Neutral
AMD,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Negative
AMD,Noob question:  Is this their 16th gen chips?,Neutral
AMD,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Neutral
AMD,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Neutral
AMD,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Neutral
AMD,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Positive
AMD,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Positive
AMD,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Negative
AMD,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Negative
AMD,"So alder lake on cache steroids ?  If the heavier core count chips don’t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Neutral
AMD,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Neutral
AMD,"""E-Cores"",  Ewww, Gross.",Negative
AMD,"**Hi,**  I’m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPU’s clearly starting to bottleneck a bit, but it’s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Neutral
AMD,NVL will definitely be the 400 series. PTL is 300.,Neutral
AMD,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Neutral
AMD,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Neutral
AMD,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Neutral
AMD,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Neutral
AMD,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Neutral
AMD,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Positive
AMD,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Negative
AMD,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Neutral
AMD,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Negative
AMD,More like *despite* him.,Neutral
AMD,All the SKUs rumored so far are BLLC,Neutral
AMD,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Neutral
AMD,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Negative
AMD,Why not 8 P cores with HT + even more cache and no e cores at all,Neutral
AMD,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Positive
AMD,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Neutral
AMD,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Neutral
AMD,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Negative
AMD,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Neutral
AMD,Nova lake? More like 26Q4.,Neutral
AMD,Only Pantherlake for mobile,Neutral
AMD,"It doesn’t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Neutral
AMD,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Neutral
AMD,It is really happening. The only question is when? Or can they release it on the next year without delay?,Neutral
AMD,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Negative
AMD,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Neutral
AMD,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Negative
AMD,"E cores are the future, P cores days are numbered.",Neutral
AMD,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Negative
AMD,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Neutral
AMD,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.     You can definitely wait a bit.",Negative
AMD,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Neutral
AMD,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Neutral
AMD,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Negative
AMD,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Neutral
AMD,The former ceo,Neutral
AMD,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Negative
AMD,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Positive
AMD,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. You’re using dated info.,Negative
AMD,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Positive
AMD,No different to 12-14th gen then.,Neutral
AMD,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Negative
AMD,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Neutral
AMD,E core is 30% faster than hyper threading,Positive
AMD,HT is worse than E cores,Negative
AMD,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Neutral
AMD,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Neutral
AMD,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Neutral
AMD,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Negative
AMD,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Neutral
AMD,ARM chips regularly do this sometimes on a yearly basis.,Neutral
AMD,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Neutral
AMD,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Neutral
AMD,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Negative
AMD,"> They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Neutral
AMD,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Neutral
AMD,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Neutral
AMD,"Lmao, some of my games still runs on e cores",Neutral
AMD,People are still disabling e cores for more performance.,Neutral
AMD,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Neutral
AMD,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Neutral
AMD,fym no different they're 50% faster,Positive
AMD,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Negative
AMD,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Neutral
AMD,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Neutral
AMD,"As a advice, Nova Lake will further increase memory latency.",Negative
AMD,In which gen iteration?,Neutral
AMD,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Neutral
AMD,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Negative
AMD,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Neutral
AMD,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Neutral
AMD,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Neutral
AMD,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Neutral
AMD,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Neutral
AMD,They don't pay attention,Negative
AMD,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Neutral
AMD,That is more so bc the 9950x3d isn’t really a 16 core cpu it is two 8 cores,Neutral
AMD,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Neutral
AMD,How do you think they're doing 2 compute tiles if the memory controller is on one?,Neutral
AMD,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Neutral
AMD,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Positive
AMD,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Neutral
AMD,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Neutral
AMD,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Neutral
AMD,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Positive
AMD,That didn't stop Intel with N3B,Neutral
AMD,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Neutral
AMD,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Neutral
AMD,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Positive
AMD,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Neutral
AMD,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix the problem. It doesn't happen very often but when it does it's very annoying.",Negative
AMD,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Neutral
AMD,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Neutral
AMD,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Neutral
AMD,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Neutral
AMD,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Negative
AMD,Will it also introduce Lunar Lake successor?,Neutral
AMD,I wonder if this could become avaliable on LGA 1954 because that'd be a good alternative for budget builds potentially if the iGPU is potent enough.,Positive
AMD,Is anyone left?,Neutral
AMD,I would love to see a Nova Lake-A (strix halo like) APU with an 8+16 tile + 20Xe3 cores which is equal to 2560 FP32 lanes or 40 AMD CU's with 16-32mb of memory side cache to handle igpu bandwidth demands,Positive
AMD,"Wonder what node such a product, if it does exist, would use for the iGPU tile.   AFAIK, rumor is that the iGPU tile for the standard parts would be on 18A or an 18A variant. However, if this is a flagship part, I would assume they would be willing to pay the extra cost to use N2...   However that would also presumably involve porting the architecture over to another node, which I don't think Intel has the resources, or wants to go through the effort, to do so.   Also wonder if such a product will finally utilize the ADM cache that has been rumored since like, forever.   And this is a bit of a tangent, but I swear at this point in the leak cycle for a new Intel product, there's always usually a ton of different skus that are leaked that may or may not launch - right now we have NVL bLLC, NVL standard, and now this, and a bunch of them end up not launching (whether they were cut internally, or the leaks were just made up). So it would be pretty interesting to see if any of the more specialized rumored NVL skus end up actually launching.",Neutral
AMD,These designs are going to be the go-to for creator and mid-range gaming at some point in the future. Having it all integrated together should provide opportunities for optimization and form factors that are difficult with discrete cards,Positive
AMD,How long do I have to wait for something that is more powerful than an AMD 7840HS and still cheaper?,Neutral
AMD,Good to hear! I have a strix halo asus tablet the 390 one and its only got 32GB ram but its fast!  Would love to see an intel version…,Positive
AMD,"This is great move by Intel since they aren't making dGPU for laptop so they can get some marketshare by making strong Intel ecosystem. If Nova Lake AX released then people don't need to buy overpriced laptop with Nvidia dGPU anymore, not to mention with Nvidia shitty small vram.",Positive
AMD,"I'm still using Alder Lake, I suppose all these chips will be released in 2026 ?",Neutral
AMD,That's Panther Lake in a few months,Neutral
AMD,Lunar lake was always confirmed even by Intel as a 1 off design with soldered memory and many of its design choices.,Neutral
AMD,"No, the memory config wouldn't work.",Negative
AMD,No not really.  It's pretty f'n bleak atm.,Negative
AMD,"That isn't that different from the AI MAX 395+ already released, which has 16 Zen 5 cores and 40CUs and some MALL cache (I think 20MB) to help with bandwidth.  For this to be a meaningful upgrade, you'd want more than 20 Xe3 cores, which means you need more bandwidth, which means you need probably LPDDR6 memory to get it high enough as otherwise the cache demands would be prohibitive and LPDDR6 memory isn't coming to client until probably 2027.",Neutral
AMD,It's N2.  Sadly Intel Inside has almost entirely become TSMC Inside,Negative
AMD,"Nova Lake-AX will not be released, and DLLC will not be released, and nova Lake will not be released.",Negative
AMD,"Unless they can find some way to bring down the cost.....these aren't gonna get much traction, the same applies to Strix Point of course. While more efficient than your typical 2000$ laptop with a decent GPU.....the traditional laptop is still gonna outperform them based on pure specs.  You get more battery hour and less heat but people who use these kind of laptop are used to having it plugged 24/7. Feels more like experimental models but they have a long way to go before they see proper adoption. Particularly the cost.",Negative
AMD,"> mid-range gaming  Strix halo is only in $2000 systems. high end price, mid-range performance.  Good for stock margins i guess.",Positive
AMD,I'm sure that Xe2 in Lunar Lake is faster than the 7840HS iGPU on average if you run it with the max performance mode in the OEM software that comes with your laptop.,Positive
AMD,The 7850HS can't compete with the Xe2 igpu because it's bandwidth starved.   The 8mb of memory side cache insulating the 4mb of L2 from main memory allows the Xe2 Arc 140V igpu in LL clocked at 1950mhz to beat the RDNA 3.5 890m clocked at 2950mhz as it only has 4mb of L2 as a last level of cache before hitting memory.   TLDR: RDNA 3.5 is bandwidth starved on strix point due to lacking memory side cache,Neutral
AMD,PTL's not really a LNL successor.,Neutral
AMD,"Yeah, it was just to prove a point that ARM is overrated.",Negative
AMD,"I hear you, questioning my decision to return under Pat’s hire-back spending spree. Dodged this one… but this is hitting differently.",Negative
AMD,What about an 8 + 16 big LLC tile and 32Xe3 cores (4096 FP32 lanes or ~60 AMD CU's) + 64mb of memory side cache?   Or even better a mid range part with an 8+16 tile and 16-20Xe3 cores + 16-32mb of memory side cache,Neutral
AMD,Just read about the JEDEC spec for lpddr6 of 14400mt/s.  That is wild!,Neutral
AMD,Who said it's It's entirely TSMC ? they have been pretty open about majority of tiles Intel the CPU Tile will be shared with N2 ofc though,Neutral
AMD,"It should be Intel here, TSMC there.  Or a little bit of Intc, a little bit of TSMC.",Neutral
AMD,IDM 2.0: Where we proudly declare our fabs are world-class—while quietly handing the crown jewels to TSMC.,Positive
AMD,Source?,Neutral
AMD,"The BOM is lower, so the question is where the markup is coming from.",Neutral
AMD,Consoles have big APUs and they have better value than anything. I know they're subsidized however.,Positive
AMD,A mid range Nova Lake-A SKU with a 6+12 tile and 16/20Xe3 cores with 16-32mb of memory side cache would be sick,Neutral
AMD,Thats the problem with AMD sticking an additional 8 cores instead of infinity cache to fix the bandwidth bottleneck.,Negative
AMD,>still cheaper,Neutral
AMD,"It's the closest that we will get to a Lunar Lake successor   Really, panther lake is a combined successor to both Arrow Lake-H and Lunar Lake",Neutral
AMD,"Low volume and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference.  Combine that with not moving a lot of them compared to the rest of the lineup, and each one of those units has to make up more of the costs from things like bad dies or general spin up costs for a new chip.",Negative
AMD,It already has 32MB infinity cache.,Neutral
AMD,The 7840HS is cheaper because it is older.,Neutral
AMD,"It competes in the U/H lanes, so what is today ""ARL""-U and ARL-H. But yes, in practice it should bring *most* of the LNL goodness over, but PTL-U is still not a true successor in the original ~10W envelope LNL was designed for.",Neutral
AMD,"> and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference   It's no bigger than the equivalent dGPU, and you save on memory, package, and platform costs. Half the point of these things is to use the integrated cost advantage to better compete with Nvidia dGPUs.",Negative
AMD,The new Ryzen AI 250/260 with 780M is still cheaper because it doesn't have copilot+ .,Neutral
AMD,I will see the performance envelope of PTL U and decide should dump my LNL or not,Neutral
AMD,"The low volume part of that quote is important. If you're making heaps of them, each can take up a little bit of the startup costs for that design. If you only make a few, those costs get concentrated on what you do make.  Low volume also means a single defect is promotionally more of your possible units, and hits the margins harder. You do get more total defects in a larger run, but at sufficiently high quantities of those defects, you start making lower bins to recover some of the defects. If you hardly make the volume to cover one widespread SKU, you aren't likely to have enough to make a cheaper one with any degree of availability. At some point you hit a threshold where it's not worth selling what would be a lower bin because there's not enough of them.",Neutral
AMD,Because putting a 40 TOPs NPU for the Copilot+ certification is costly.,Negative
AMD,"It doesn't have a 40 TOPS NPU, it's the same 16TOPS one in the 8840HS it's literally just another rebrand. It's cheap because it's from 2023.",Neutral
AMD,Exactly. Hence your Xe2 lunar lake being faster is irrelevant to the question because it won't be cheaper regardless of age when they have a NPU that takes close to 1/3 of die space.,Negative
AMD,I think I didn't say anything that deviates from what you just said.,Neutral
AMD,"Maybe I misread or misunderstood, I thought you said the 260/250 had a 40TOPS NPU",Neutral
AMD,enjoy rhythm aware outgoing practice bike attempt library versed cake   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
AMD,Haha wow that is hilariously bad.   And here I thought ASUS newer bios update was bad due to some higher temps,Negative
AMD,"The new Gigabyte BIOS also has booting issues for me, and memory is even more unstable.  At least the UI is entirely in English now.",Negative
AMD,"who needs quality control, what can go wrong?",Negative
AMD,"This is why you don't rush to update software, let others do the testing for two weeks",Neutral
AMD,Gigabyte is trash,Negative
AMD,The Elon Musk method,Neutral
AMD,"> “Yeah just push it to production nothing’s gonna happen, no need to QA/QC this”  Seems to be the motto of a lot of the tech world",Neutral
AMD,"Yup indeed.   I would even go as far as one full month   Hence, why I learned to avoid installing new Windows Update automatically",Neutral
AMD,Intel really needs to be able to compete with X3D or they're going to continue getting dominated in the enthusiast consumer market. I like Intel CPUs and was happy with my 12600K for awhile but X3D finally swayed me to switch over.,Positive
AMD,"Intel has had plans for big ass L4 cache for almost a decade now, just that it never made it past the design board.  Supposed to be marketed as Adamantium. But it got ZBB’d every time I suppose due to cost.  For Intel to implement Adamantium, regular manufacturing yield has to be good enough I.e cost is low so they can splurge on L4.  Of course now they are forced to go this way irrespective of cost. I’d love 16p + L4 CPU.",Negative
AMD,"Honestly, good. I've been using AMD for a while now but we need healthy competition in the CPU space for gaming otherwise AMD will see a clear opportunity to bring prices up",Positive
AMD,"Something interesting is that the extra cache isn't rumored to be on a base tile (like it is with Zen 5X3D), but rather directly in the regular compute tile itself.   On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.   I think Intel atp desperately needs a X3D competitor. Their market share and especially revenue share in the desktop segment as a whole has been ""cratering"" (compared to how they are doing vs AMD in their other segments) for a while now...",Neutral
AMD,Hasn’t this been on their roadmap for a while now? I’m pretty sure they said 2027 is when they’ll have their version of x3D on the market,Neutral
AMD,"These core count increases could be a godsend at the low end and in the midrange. If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming, Intel will have a massive advantage on price.  That being said, if leaked Zen 6 clocks (albeit they’re from MLID, so should be taken with a grain of salt) are accurate, Nova Lake could lose to vanilla Zen 6 in gaming by a solid 5-10% anyway.",Positive
AMD,"Funny how non of this news posted on reddit hardware sub or even allowed to be posted. Guest what? R amdhardware will always be amdhardware! It's painfully obvious that unbearable toxic landfills sub is extremely biased to Amd. Meanwhile all Intel ""bad rumors"" got posted there freely which is really BS!  I still remember i got banned from that trash sub for saying ""People need to touch grass and stop pretending like AMD is still underdog because they aren't"" and the Amd mods sure really mad after seeing my comment got 100+ upvotes for saying the truth, but that doesn't matter anymore because i also ban those trash sub!",Negative
AMD,Intel should be ahead of the curve on things not looking to compete on previously created tech,Neutral
AMD,"Very fine-tuned ARL-S almost reach 9800X3D performance. Extra cache could help to close the gap   Given people are willing to overpay for price-inflated 9800X3D, I wonder if it could work given buyers need an entirely new platform. 9800X3D users are fine for a pretty long time like 5800X3D users did",Positive
AMD,"Lol, requires a new socket. Intel is such trash.",Negative
AMD,Intel will simply always be better than amd,Positive
AMD,"AMD gains tremendously from X3D/v$ because the L3 cache runs at core speeds and thus is fairly low latency, Intel hasn't seen such low latency L3 caches since skylake, which also has much smaller sizes, so the benefits of this could be much less than what AMD sees.   Only one way to find out, but I advise some heavy skepticism on the topic of ""30% more gaming perf from 'intel's v$'""",Positive
AMD,"Either more cache or resurrecting the HEDT X-series... Doesn't matter, as long as there is an affordable high-end product line.",Neutral
AMD,"The 12600k was a fine chip, but AMD had the ace up Its sleeve. I upgraded from a 12600k to a 7950x3d and it was one of the best PC upgrades I ever made.",Positive
AMD,I mean 9800x3D and 14900K offers basically the same performance in the enthusiast segment. Going forward though it would be nice to have more cache so normal users doesn't have to do any sort of memory overclocking just to match 9000x3D in gaming.,Neutral
AMD,4070 ti won’t cut it man - upgrade!,Negative
AMD,Broadwell could have been so interesting had it planned out.,Positive
AMD,"I want a 32 Core/64 Thread 3.40 GHz Core i9-like CPU. Not Xeon like with Quad-Channel and stuff, just 40 PCIe 5.0 lanes and 32 Power-Cores instead of little.big design. 😬",Neutral
AMD,">Otherwise AMD will see a clear opportunity to bring prices up  AMD already did, as you can see zen 5 x3d is overpriced as hell especially the 8 core CPU. Zen 5 is overpriced compared to zen 4 which is already more expensive than zen 3. Not to mention they did shady business like keep doing rebranding old chip as the new series to fools people into thinking it was new architecture when it wasn't and sell it with higher price compared to chip on the same architecture in old gen.  Intel surely needed to kick Amd ass because Amd keep milking people with the same 6 and 8 cores CPU over and over with price increases too! Not to mention radeon is the same by following nvidia greedy strategy.  Edit: Some mad Amd crowd going to my history just to downvote every of my comments because they are salty as hell, i won't be surprised if there are from trash sub r/hardware. But truth to be told, your downvote won't change anything!!",Negative
AMD,"Even though it's not stacked, I believe it's still going to fix the last level cache latency issue MTL and ARL have.   Ryzen CPUs have lower L3 latency than Intel because each CCX gets their own independent L3, unlike Intel's shared L3. Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3, so possibly giving the existing cores/tiles their own independent L3, improving latency and bandwidth over shared L3.  But one thing intrigues me. If this cache level has lower latency than shared L3, wouldn't this more properly be called L2.5 or something below L3 rather than last level cache? Will NVL even still have shared L3 like the previous Intel CPUs? I know the rumor that it will have shared L2 per two cores, but we know nothing of the L3 configuration.",Neutral
AMD,"> On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.  It is already a non-issue since AMD moved the 3D V-Cache to underneath the compute tile.",Neutral
AMD,"Adamantaium was on the interposer, did they change plans?",Neutral
AMD,"Don't remember them saying anything like that, but by around that time their 18A packaging is supposed to be ready for 3D stacking.",Neutral
AMD,"Nova lake= skip of it's just as good as zen, you would be looking at 2 gens after that and then swap from AM5 to intel.",Positive
AMD,"> If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming  Doubt that since it'll probably lack hyperthreading and the E-Cores are slower, even 6C12T CPUs are starting to hit their limits in games in the last few years, faster cores won't help if there's much less resources to go around, it kinda feels like intel went backwards when they removed hyperthreading without increasing the P-Core count.",Negative
AMD,Intel managed to run Sandy Bridge's ring bus clock speeds at core clocks which resulted in 30 cycles of L3 latency.   Haswell disaggreated core and ring clocks allowing for additional power savings.   Arrow Lake's L3 latency is 80 cycles with a ring speed of 3.8ghz,Neutral
AMD,"I'd like to see the HEDT X-series come back too, but Intel would have to come up with something that would be competitive in that area.  It's not hard to see why Intel dropped the series when you take a look at the Sapphire Rapids Xeon-W lineup they would have likely been based off of.  I think AMD would also do well to offer something that's a step above the Ryzen lineup, rather than a leap above it like the current Threadrippers.",Neutral
AMD,Well it was a downgrade on system snappiness as intel have way higher random reads than amd.,Negative
AMD,> I mean 9800x3D and 14900K offers basically the same performance  LMAO,Neutral
AMD,"Huh? 9800x3d is universally known to be like 20-25 percent faster, even in 1 percent lows.   https://www.techspot.com/review/2931-amd-ryzen-9800x3d-vs-intel-core-14900k/",Neutral
AMD,Maybe that is your experience.  Neverthelesss if you compare most gamers who switched to 9800x3D they report a significantly noticeable uplift in fps and 0.1 fps. Maybe a negligible few reported a decrease. And this has very likely nothing to do with the x3D CPU but other causes.,Neutral
AMD,"Ah you’re missing the final piece. As far as i’m aware this pretty much requires controlling the OS as well (or at least solid OS support). Consoles get their own custom operating system, Apple built a new version of MacOS for M chips. Intel and AMD though don’t control windows.",Neutral
AMD,UMA is such a hassle That's why I don't see it much except for calculation purposes (HPC/AI)...,Negative
AMD,"Application developers are supposed to try to avoid copies from GPU memory to CPU memory, instead letting it stay in the GPU memory as much as possible",Neutral
AMD,">so there is still the cost of useless copies between system RAM vs allocated GPU ram.    There is none, AMDGPU drivers have supported GTT memory since forever, so static allocation part is just to reduce burden for app developers but if you use GTT memory you can do zero-copy CPU+GPU hybrid processing.",Negative
AMD,"Intel needs something decent because AMD has taken a page out of intel (up to gen7) playbook, same cores no changes. Intel now provides more cores but it's the 100% core increase Vs AMD 50% and bLLC that should shake things up, hopefully they keep the temperature down as I don't want to have to replace case and get a 360mm rad just to not throttle, and not ever again do a 13th and 14th gen degradation show.   If all goes well going back to intel for a few years then AMD, brand loyalty is for suckers, buy what's best for performance and value. Hopefully intel i5 has 12P cores and i7 18-20P cores that would be nice to have",Neutral
AMD,"bLLC is just a big-ass L3$ and since Intel does equal L3 slices per coherent ring stop, it'll be 6\*12 or 12\*12 with each slice doubling or quadrupling. The rumor is 144MB so quadrupled per slice, probably 2x ways and 2x sets to keep L3 latency under control.",Neutral
AMD,"Intel and AMD have effectively the same client L3 strategy. It's only allocated local to one compute die. Intel just doesn't have any multi-compute die parts till NVL.   > Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3  8+16 is one tile, in regardless of how much cache they attach to it",Neutral
AMD,It is a massive issue for amd. You're voltage limited like crazy as electron migration kills the 3D cache really fucking fast. 1.3V is already dangerous voltage for the cache.,Negative
AMD,"I still think there's a slight impact (the 9800x3d only boosts up to 5.2GHz vs the 5.5GHz of the 9700x), but compared to Zen 4, the issue does seem to have been lessened, yes.   And even with Zen 4, the Fmax benefit from not using 3D V-cache using comparable skus was also only single digits anyways.",Neutral
AMD,"Adamantium was always rumored to be an additional L4 cache IIRC, and what Intel appears to be doing with NVL is just adding more L3 (even though ig Intel is calling their old L3 the new L4 cache? lol).   I don't think Intel can also build out Foveros-Direct at scale just yet, considering they are having problems launching it for just CLF too.",Neutral
AMD,"I'm an e-core hater but arrow lake e-cores are really performant and make up for the loss of HT. arl/nvl 4+8 would wildly beat 6c12t adl/rpl.  HT was always a fallacy anyway. If you load up every thread, your best possible performance is ~60% of a core for a games main-thread.  I would much rather pin main-thread to best p-core in a dedicated fashion and let the other cores handle sub threads. Much better 1% lows if we optimize for arrow lake properly (still doesn't hold a candle to 9800X3D with HT disabled though).",Negative
AMD,"Yeah, I somewhat agree with this. I suppose it depends if Intel’s latency problem with their P+E core design is at all a fixable one - 4c/8t is still shockingly serviceable for gaming, but 4c/4t absolutely is not.",Neutral
AMD,It's the same ratio as 285K 8P+16E vs AMD 16P and we know that 285K is competitive despite no hyperthreading,Neutral
AMD,"Sooo a few months ago, I helped a buddy of mine troubleshoot a black screen issue on his newly built 9800X3D and RTX 5090 rig, a fairly common issue with Nvidia’s latest GPUs.  While working on his PC, I'd notice a series of odd and random hiccups. For example, double clicking a window to maximize it would cause micro freezes. His monitor runs at 240Hz, and the cursor moves very smoothly, but dragging a window around felt like it was refreshing at 60Hz. Launching League of Legends would take upwards of 10+ seconds, and loading the actual game would briefly drop his FPS to the low 20s before going back to normal. Waking the system from sleep had a noticeable 2-3 seconds delay before the (wired) keyboard would respond, which is strange, considering the keyboard input was what wake the system up in the first place.  Apparently, some of these things also happen to him on his old 5800X3D system, and he thoughts that these little quirks were normal.  I did my due diligence on his AMD setup: updated the BIOS and chipset drivers, enabled EXPO profile, made sure Game Bar was enabled, set the power mode to Balanced. Basically, all the little things you need to do to get the X3D chip to play nice and left.  But man... I do not want to ever be on an AMD system.",Neutral
AMD,did they measure responsiveness and timed the click to action? and was it significantly different? how much difference are we talking about?,Neutral
AMD,"can you explain exactly what you're talking about here? are you talking about a situation where the system needs to do random reads from an ssd? aka: boot time, initial game load time?",Neutral
AMD,"How was ""system snappiness"" measured?",Neutral
AMD,No.,Neutral
AMD,"Now both AMD and Intel chips are “disaggregated “ which means between cpu and other agents like memory controllers, pcie, and storage there is higher latency than the 12/13/14th gen parts. AMD has higher latency due to the larger distances involved on the package.  Also Intel is not really improving the CPU core much. There won’t be a compelling reason to upgrade from a 14700 until DDR6 comes out. At least not in desktop. Nova lake high cache parts will cost $600 or more so value/dollar will be low.",Negative
AMD,So? Major upgrade for everything else,Neutral
AMD,"I had an 12600 not k. I had the opposite experience, I upgraded to a 7800x3d and the snappiness was a night and day upgrade. I can recommend a x3d to anyone. Pair that cpu with Windows 11 IoT LTSC and you have a winner <3",Positive
AMD,"Meant to say ""Gaming Performance""  >Higher avg on X3D  >similar or same 1% lows on both platforms >Higher .1% lows on Intel.",Neutral
AMD,"any comment that starts with ""I mean..""  I never go any further, its like some weird reddit think where everyone with ignorant comments seems to start out with this,  at least often anyway.",Negative
AMD,"""Enthusiast Segment"" my good sir. All the benches you see are poorly configured or stock 14900K. With tuning it's a different story. Intel craptorlake scales with fast ram.",Negative
AMD,"As a long time AMD user I know that Intel needs to be tuned to perform best. So when you tune the 14900K or even 285K you get like 20% performance uplift vs stock. X3D just performs great out of the box because of the huge L3 Cache. At the very least if you do not like microstutters or frame drops and want consistent gaming performance Intel 14th gen is superior vs current AMD's offering. Anyone with a specific board like Apex, Lightning, Tachyon, or even Gigabyte Refresh boards + i7/i9 13-14th gen with decent memory controller can achieve similar gaming experience. I'm speaking from experience since I also have a fully tuned 9950x3D/5090 on my testbench. For productivity task Intel feels much better to use as well. I feel like Intel is just better optimized for Windows and Productivity too.",Positive
AMD,"Actually Intel thermal is already better than Amd ever since Arrow Lake and Lunar Lake released. Even Core Ultra 7 258V is arround 10c cooler than Amd Z2E and Strix Point on the same watt.   On MSI Claw 8 AI+, Lunar Lake temp at 20w is just arround 62c while the Amd version is arround 70c. I wouldn't have a doubt Nova Lake and Panther Lake will also have good thermal because it will have 18A node with BPD and RibbonFET GAA which is more advance than traditional silicon when it comes to power delivery and efficiency.",Positive
AMD,Ah so bLLC on both tiles is a possible configuration? Any chance Intel actually goes for this?,Neutral
AMD,You can very simply get 9800x3D to 5.4 with little effort,Neutral
AMD,I haven't seen any of those issues on AMD where the underlying cause wouldn't also cause those issues on Intel.,Negative
AMD,"7800x3d here and never had these issues, came from intel",Neutral
AMD,Difference between 85MBps and 140MBps in q1t1 random reads and writes.,Neutral
AMD,Lets just ignore the whitepaper WD and Intel did about this.,Negative
AMD,So what configuration (tuning and ram settings) can a 14900k match a 9800x3d?,Neutral
AMD,That (Apple Silicon is good) and UMA are different stories I already know that Apple Silicon is good,Positive
AMD,On which benchmark(s) / metrics?,Neutral
AMD,"Haven't kept up with mobile since AMD 5000 and intel 10th gen, all I remember is intel needing XTU undervolting and then intel blocking XTU so using third party undervolt programs, AMD like I said 5000 never needed undervolting.   Desktop side, AMD 7000 is a hot mess, like seriously ridiculous. Seems that was sorted on 9000,    Let's wait and see on nova lake and zen6, like I said, brand loyalty is stupid, bought into AM5, so it's cheaper for me to go for zen6 with 244Mb of L3, but no am6-7 that will be intel.",Negative
AMD,"In theory, yes. For packaging reasons and market segmentation, probably not.",Neutral
AMD,"what's that on in terms of percentage, or seconds to person? was it noticeable?  i'm not techy enough, but is random reads and writes for clicking things and accessing data, and less so on copying and pasting a file?",Neutral
AMD,where is this whitepaper,Neutral
AMD,"You can send white papers all day but if most people buy these for gaming or productivity, AMD is winning in both categories.",Positive
AMD,Stock clocks 5.7/4.4/5.0 HT Off With DDR5 7600-8000 on Windows 10 22H2 or Windows 11 23H2 is enough to match 9800x3D at 5.4ghz with 6000 c28/6200 c28,Neutral
AMD,So MLID leaked in his most recent video about this (and he ranks it with a blue color code - 'very high confidence')  What do you think about this?,Neutral
AMD,[https://youtu.be/0dOjvdOOq04?t=283](https://youtu.be/0dOjvdOOq04?t=283) This explains it.  Gonna find the whitepapers link again.,Neutral
AMD,Based on what evidence?   I looked online for a few moments and found:  (1) Buildzoid from actually hardcore  overlocking did a 12 hour live stream where they couldn't even get a 8000 mhz overclock stable on the 14900k. No wonder why people haven't benched this lol  https://www.youtube.com/live/bCis9x_2IL0?si=ht3obVoBLcRFCyXI  (2) Plenty of benchmarks where an overclocked 9800x3d is about 10 percent faster than an overclocked 14900k with 7600 ddr5,Neutral
AMD,"Haven't you been listening? The conversation is strange (Confused) You first brought up the story of UMA, right?",Neutral
AMD,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Negative
AMD,"That channel has a lot of videos and even on this specific video it would help if you point the specific time you're referring to.  Now regarding AI, I assume you are talking about token generation speed and not prompt processing or training (for which Macs are lagging due to weak GPU compute).  I happen to have expertise in optimizing AI algorithms (see https://www.reddit.com/user/Karyo_Ten/comments/1jin6g5/memorybound_vs_computebound_deep_learning_llms/ )  The short answer is that consumers' PCs have been stuck with dual-channel RAM for very very long and with DDR5 the memory bandwidth is just 80GB/s to 100GB/s with overclocked memory.  Weakest M4 starts at 250GB/s or so and M4 Pro 400GB/s and M4 Max 540GB/s.  Slowest GPUs have 250GB/s, midrange about 800GB/s and 3090~4090 have 1000~1100GB/s with 5090 having 1800GB/s bandwidth. Laptop GPUs probably have 500~800GB/s bandwidth  LLMs token generation scales linearly with memory bandwidth, compute doesn't matter on any CPU/GPU from the past 5 years.  So by virtue of their fast memory Macs are easily 3x to 8x faster than PC on LLMs.  The rest is quite different though which is why what benchmark is important.",Neutral
AMD,"I'm highly suspicious, for packaging reasons if nothing else. I'm not going to call it impossible, but that would put enormous strain on the X dimension. Think about it, it's something on the order of 3-4x what the socket was originally for.  And obviously goes without saying, but MLID ""very high confidence"" doesn't mean shit.",Negative
AMD,"Based on my own testing , the games I play and benchmark results. Bullzoid is not relevant to this conversation. You can keep downvoting me for speaking the truth it's okay. I can't blame you because youtube and mainstream techspot are 99% misinformation. If you believe 9800x3D is 30% faster then go buy it, nobody is stopping you. I have investments with AMD and you're doing me a favor by supporting them.",Neutral
AMD,Just to be clear compute and hardware support for things like tensor cores have a massive impact. HBM is king but on older cards like the mi100 (released five years ago) can be out paced by a mid range card like the 4070.  All I wanted to convey is llm and token generation is a complex topic with limitations and struggles beyond memory bandwidth.,Neutral
AMD,"I haven't downvoted you at all, what are you even talking about.   You want people to believe in some conspiracy theory where any other information is a lie, and only you provide the ""real truth"".",Negative
AMD,"Besides, UMA wasn't first developed by Apple. Even if Intel introduces it, the software side or the software framework… Moreover, the OS side has to deal with it, so it is necessary to consider it a little. That's what you said earlier",Neutral
AMD,"I take no side there. I'm a dev, I want my code to be the fastest on all platforms  I have: - M4 Max so I can optimize on ARM and MacOS - Ryzen 9950X so I can optimize with AVX512 - in the process of buying an Intel 265K so I can tune multithreaded code to heterogeneous architecture.  The problem of Intel and AMD is segmentation between consumer and pro.  If Intel and AMD want to be competitive on AI they need 8-channel DDR5 (for 350~400GB/s), except that it's either professional realm (Threadripper's are 8-channel and EPYC's are 12-channel) with $800~1000 motherboards and $1500 CPUs and $1000 of RAM.  Or they make custom designsvwith soldered LPDDR5 like the current Ryzen AI Max 395, but it's still a paltry 256GB/s.  Now consumer _need_ fast memory. Those NPUs are worthless if the data doesn't get fetched fast enough. So I expect the next-gen CPUs (Zen 6 and Nova Lake) to be quad-channel by default (~200GB/s with DDR5) so they are at least in the same ballpark as M4 chip (but still 2x slower than M4 Pro and Max).  I also expect more soldered LPDDR5 builds in the coming year.",Neutral
AMD,"Have you not seen HardwareUnboxed 9070 XT Finewine video? Tell me why should I trust someone like that? I'm just sayin if you want real information test it yourself. I just don't trust product influencers like GamersNexus, HUB , Jayz2cents and other mainstream channels. Id rather buy the product and test it myself. Im not forcing you to believe what I said, its for people that actually knows what Raptorlake is capable off. If you want to see a properly tuned 14900k check out facegamefps on youtube. This is a very capable platform.",Neutral
AMD,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
AMD,Is it safe to install an Arrow Lake CPU without a third-party contact frame over the long run?,Neutral
AMD,"Edit: solved. Windows is capable of disabling e-cores on boot. The setting to turn them back on in MSCONFIG, boot tab, cores dropdown. Whatever feature causes this, I do not know but would love to find out, and furthermore their is no legitimate purpose for this feature, and Intel should prevent Microsoft from doing this, in my opinion, if there is any legal means available. Unless it interferes with Intel’s revenue.  Poor performance, incredibly low benchmarks. Beginning to suspect bad CPU.  I have tried bone stock with cleared CMOS settings. I have loaded multiple overclocking profiles, clearing CMOS in between. I have tried undervolting, and adjusting Load Line. I have changed Lite Load settings. I have tried with and without XMP enabled. Changed windows power plans. Nothing gets my cinebench r23 score above 11k. CPU-Z benchmarks it as 43% as fast as the previous generation CPU. I never overclocked this cpu, and I changed to the updated bios with new microcode the day it was available. CPU Purchased from Intel, retail box.  Windows 11 Build 26100.4484  CPU: Intel I9 14900K, stock clock, 360 AIO liquid cooling  RAM: Patriot Viper Venom DDR5 32GB (2x16) 7200MT/S CL34  GPU: MSI Ventus NVIDIA RTX 4080S  Motherboard: MSI Z790-S Pro Wifi with most recent bios  Storage: WD Black 2TB NVME on CPU; Toshiba 20GB X300 Pro  PSU: NZXT C1000 PSU (2022)  Display: Samsung Odyssey G93SC  I am at my wits end with this thing. I first noticed games crashing to desktop a few months ago. I thought it was just poorly coded (Helldivers 2). I checked my FPS in the game, and while previously I had to frame limit it 10 144FPS, I was now getting close to 60-80FPS.  Is my CPU a toaster or is there something I'm missing?  CPU-Z output, and HWiNFO64 sensor readings:  [https://imgur.com/a/ROuZOKS](https://imgur.com/a/ROuZOKS)",Neutral
AMD,"I bought a 265kf CPU from Amazon. On the checkout page it clearly stated that my purchase qualified for the Intel Spring Gaming Bundle and that I would receive an email with a Master Key. Well, I didn't receive anything and I spent all day being transferred from one Amazon support staff to another to no avail.  The promotion is literally still active and if I try to buy the CPU again it shows the same offer.  But somehow no one on Amazon or Intel support can tell me why I didn't get the email.",Neutral
AMD,Does anyone know if the upcoming Bartlett Lake-S 12 p-core no e-core CPU will suffer from the same stability issues as Intel 13th gen and 14th gen CPUs?,Negative
AMD,Any news to share regarding this link? https://www.reddit.com/r/intel/s/Bg4QnVzIdD,Neutral
AMD,"Bug report: Latest ARC driver 32.0.101.6972 causes crashing using Speed Sync  I get crashing in ALL games when attempting to use the new Speed Sync option on this driver.  Specs are:  Ryzen 9700x, Arc B580, 32GB DDR5 RAM, 2tb SSD.  Monitor is non VRR compatible (older gsync monitor ASUS PG348Q)  Hope this gets resolved as it sounds like a great feature.",Negative
AMD,"Hello,  I own a i9-14900K, mobo MSI MAG Z790 Tomahawk. I am wondering if it’s possible a specific program can cause my cores to be power limit exceeded, and is there any fix to that? Currently it drops the cpu speed to 0.8 GHz. This one program LeagueClient.exe for a game, League of Legends, has recently started to cause this problem, the only fix is to restart the computer. I have updated my bios, changed power limits within the bios, disabled c state, disabled EIST, disabled e-cores, tried to go into safe boot but the program won’t launch, reinstalled multiple times.",Negative
AMD,Has the degradation of the 13th and 14th gen CPU’s been fixed yet?,Neutral
AMD,"Been getting the following error A LOT while playing Expedition 33, and now after 18 hours I can't start the game without it crashing:  `LowLevelFatalError [File:C:\Hw5\Engine\Source\Runtime\RenderCore\Private\ShaderCodeArchive.cpp] [Line: 413] DecompressShaderWithOodleAndExtraLogging(): Could not decompress shader group with Oodle. Group Index: 760 Group IoStoreHash:52bcbf8ac813e7ee35697309 Group NumShaders: 29 Shader Index: 9301 Shader In-Group Index: 760 Shader Hash: 3BF30C4C9852D0D23B2DF59B4396FCC76BC3A80. The CPU (13th Gen Intel(R) Core(TM) i7-13700KF) may be unstable; for details see` [`http://www.radgametools.com/oodleintel.htm`](http://www.radgametools.com/oodleintel.htm)     Am I just screwed because I bought the wrong generation of Intel CPUs 2 years ago?",Negative
AMD,"I'm not sure if this matters, but when running `garuda-inxi` on my laptop, it tells me that my i3-8130U is Coffee Lake Gen 8, not Kaby Lake Gen 9.5. I've seen similar problems reported with users using CPU-Z, is this just a known bug or is there something else going on?  I'm going to be tweaking my CPU performance soon, so I'd like to make sure of the CPU capabilities/options first.",Negative
AMD,"I've been planing to build a PC from scratch for video editing and Ultra 7 265k and Arc B580 are good in my price bracket. Now, I'm worried with all the talk of Intel potentially going bankrupt or having layoffs, is it a good idea to still buy their products? Will we lose support with drivers and stuff like that?",Neutral
AMD,My gaming laptop Intel core i7-12700h runs at a constant temperature 95 . Doesn't matter if I'm playing a high end game like cyberpunk or some indie game like hollow knight. Is this thing suppose to always run at this temp?,Negative
AMD,"con el nuevo microcodigo de intel , los juegos de ubisoft (ASSASINS CRREED ODYSSE) tienen tiempos  de carga excecivamente altos, diria que al menos 10 veces mas de lo normal, Al devolverr la bios al microcodigo anterior todo funciona bien, cuando lo van a parchar?",Neutral
AMD,is it worth the upgrade for ai preformance cus my 5070 ti wont work for some reason so now my cpu is my main accelertor and user so should i upgrade to ultra 9? and i  will remove 5070 ti from my flair soon so yeah .,Negative
AMD,"ok now this is something. my Intel HD Graphics Control Panel is no longer there? no idea how long its been gone but i clearly remember it being there at a point.  using a Lenovo G510(i7-4700MQ, HD Graphics 4600)   Windows 10 Home 22H2 (Build 19045.6216)  windows apps in settings shows the intel driver but not the control panel   drivers from [lenovo's website](https://pcsupport.lenovo.com/in/en/products/laptops-and-netbooks/lenovo-g-series-laptops/lenovo-g510-notebook/20238/downloads/ds103802-intel-vga-driver-for-windows-10-64-bit-lenovo-g410-g510?category=Display%20and%20Video%20Graphics) are dated 16jul 2015. version seems to be 10.18.15.4240   drivers from [intel's website](https://www.intel.com/content/www/us/en/download/18388/intel-graphics-driver-for-windows-10-15-40-4th-gen.html?wapkw=intel%20hd%20graphics%204600) are dated 9jan 2015. version is 15.40.7.64.4279  now whats funny is that my device manager shows that my driver is dated 8mar 2017 which is version 20.19.15.4624   i also have another driver that i can see in the update drivers menu(drivers already present on my device) along with this one which is dated 29sep 2016 version 20.19.15.4531  i have tried reinstalling the driver from the update driver menu using the 8mar 2017 version, no change at all.  my drivers dont seem to be DCH drivers.   intel also says that they [discontinued the ms store version of the control panel](https://www.intel.com/content/www/us/en/support/articles/000058733/graphics.html) anyway.  this is all that i could think of writing here. any other details required just ask.   any help would be good lol",Neutral
AMD,"my i5-14600KF is being throttled at low temps and refuses to go past 0.8ghz of clock speed. Nothing I do seems to get it to stop throttling. According to throttlestop i have a red EDP OTHER ongoing throttle under CORE and RING. My average CPU temp is 31 degrees C across all cores and im getting 0.69 Voltage to my CPU  CPU: Intel i5-14600KF stock settings no overclock   GPU: Intel Arc B580 ONIX Odyssey BAR resizing enabled   Motherboard Gigabyte Ultra Durable Z790 S WIFI DDR4   RAM: Corsair Vengeance DDR4 16 GB x 2 (32GB)   Storage: 1TB Corsair MP600 CORE XT SSD + 2 TB WD Black SN770   PSU: Cooler Master MWE Gold 850 V2  EDIT: NEW INFO ACQUIRED   When running in safe mode and when booting into BIOS settings my CPU acts normally and receives typical voltage. Something running on my computer is throttling my CPU as i boot into windows. If i open Task Manager quickly after booting, I see system interrupts consume a mild amount of CPU before quickly going away, rather than sticking around when their is an ongoing hardware issue. I have reason to believe a program, either maliciously or due to error, is fucking with my CPU. Also worth noting is that Intel Graphics Software reports my CPU utilization as far higher than task manager, anywhere between 2-50% higher.",Negative
AMD,"Currently in the planning/purchasing phase of a small NVR/Steam Cache server. Information on VROC on X299 is pretty limited. so far I've seen mixed information on the drives supported. Before I purchase x4 Intel P4510 drives, I was hoping someone on here has a similar configuration that works.  The mobo manual states that only Intel based drives are supported but doesn't clarify which intel drives. Also saw on the intel forum that X299 CPU raid is further limited to only Optane based NVME drives. This drive will not be booted to, and I dont want to do a windows based raid.  My planned specs are:  CPU: 10900X  Mobo: X299 Taichi CLX - One of the few that seems to support bifurcation & VROC  Drive: Intel P4510 1TB  VROC Key: VRoc Standard  Are the Intel P4510 supported for Vroc on the x299 platform?",Neutral
AMD,"My Intel I210 ethernet device has device id 1531 meaning unprogrammed. The freebsd ethernet driver does not work with 1531. It needs device id 1533 meaning programmed. (Can I use a different driver? No, it's an embedded system that only supports this driver.)  I was linked this:  https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html  but 1) have no idea how to do it 2) cannot access the .bin file and tool it requires  Does anyone have ELI5 steps for getting the device to show devid 1533? Where can I get the .bin?",Negative
AMD,"The RMA process for Intel is absolutely atrocious. I don't know if  anyone can give advice on this but here is what is happening:  Based in Germany, for geolocation info. I was one of the early adopters of the 13th gen processors, but as I don't follow tech news too strongly I didn't find out about the issues with these chips until autumn 2024 when the issues I was having with my PC escalated to a point I couldn't ignore any further. Identified the CPU as the likely culprit and started the RMA process.  Firstly, Intel would not offer any solution where I could continue to use my PC whilst they analysed my CPU. As I use my PC for work, having it out of action for weeks/months was simply not an option, so I was forced to pause the RMA ticket whilst I saved up for a new cpu way ahead of my expected timeline.  With that aside, I reopened my ticket and the requirements they lay out are near impossible to meet - they wanted a clear photo of the matrix on the front of the chip and the matrix on the pcb itself.  The front of the chip was simple enough but on this series Intel printed the matrix in dark grey on a dark green pcb. It's barely visible just with the naked eye and I've tried so many ways to take a picture of the matrix with my smartphone and nothing I do is getting a clear picture.  I don't understand how your average consumer can possibly meet this requirement - solutions online apparently suggest purchasing a special type of expensive scanner or a macro lens for your smart phone? Which is ridiculous to me.  There is no way they cannot verify my chip with the rest of the information I have been able to give them, as well as far as I am aware, the matrix on the side of the chip on the 13th gen is literally the same as the one on the front of the chip.  The whole experience is proving to be awful, time consuming, feels like it should be illegal and has completely put me off ever using Intel again, or recommending it to anyone I help spec builds for.",Negative
AMD,"Hi, I have a Lenovo Ideapad with an Intel CPU and Intel GPU.  I find Windows font rendering unreadably faint. I have an astigmatism, and a light sensitivity, so even with prescription sunglasses I can't use dark mode, or bright screens, respectively.  I've tried finding Intel Graphics Software app settings which might help. The contrast settings quickly get too bright for my eyes, so they don't fix this. The right gamma settings might help though, one profile to make medium shades darker to make text bolder, and one to make them lighter to make images bolder.  I tried the support site here, but it has buggy scrolling which triggers my migraines, and it doesn't work with Firefox's reader view:  https://www.intel.com/content/www/us/en/support/products/80939/graphics.html#211011  How can I find or create these profiles? The Intel Graphics Software app doesn't seem to have an option to create profiles.  P.S. I can use the Windows Display Calibrator; if I ignore the instructions, and turn it as dark as possible during gamme, I get readable text at the end; if I turn it as light as possible, I get clearer images. But I can't see  a way to save those and switch without going through the whole rigamarole again.",Negative
AMD,"Can someone explain to me the differences between all the different names of the Intel CPUs, I’m new to laptops and am trying to learn. What’s the difference between 155H, 255H, and Meteor/Lunar/Arrow Lake and which one is “better”? All the different names get so confusing to me",Neutral
AMD,"I've been chasing down a crashing issue in a Unity program. Memtest86+ ran for 24 hours and cleared 18 passes so I was confident it's not RAM, and I started setting core affinity in an elimination pattern to see if it might be CPU related. I have discovered it crashes within minutes if I set affinity to Core 8, but it's rock solid on Cores 0-7 (haven't individually tested any of the cores past 8 yet but I have it on 9 now and it seems fine so far).  I have a 13900KF (not Overclocked) which is part of the batch that had the microcode issue. My BIOS was updated, but I am now suspicious of this core. Is it likely I should RMA this? Is there something else I should try first?  So far the system itself has been stable but this one Unity program crashes consistently, and I was having tab crashes in both Firefox and Chrome that also resolved when I removed Core 8 from their affinity...  The CPU passed the Intel CPU Diagnostic tool but because the crashes are so specific to core 8 it makes me suspicious.  [edit] RMAd the CPU and the system is rock solid stable now. WHEW!",Neutral
AMD,"i'm trying to get a specific driver for the storage but i can't findd a site with the driver, only setup exes (my old computer will not let it work and the new one needs the driver to install windows). is there a place where i could find the driver itself?",Negative
AMD,"Hi There. I been trying to get a replacement cpu for my 14700 and i chose option 2 to pay the 25 dollor fee (nonrefundable) etc. but my case now has a new number, a differnt worker. and he said that a credit card specilist is gonna called me and im supposed to give him my credit card infomation  edit 1: He also said the intel website isnt secured/safe for creditcard transaction",Negative
AMD,"Hi, I have Intel AX200 160Mhz Gig+ in 3 PC's (on board Gigabyte B550 x 2 & X570). My Router supports 160Mhz, it's enabled and was working getting me up to 900Mbps on my Gigabit plan. and in the last few months I've noticed the 160Mhz option in the intel wifi drivers has disappeared. I discovered this as I noticed the household PC's were struggling to get above 600Mbps most of the time. I double checked the router setting, nothing changed there 160Mhz still eneabled, only changes over the last couple of moths were 2 bios updates each for the motherboards and Intel wifi driver updates. Has anyone else had the 160Mhz option disapear? Any tips on how to get it and my full network performance back?",Neutral
AMD,"u/Progenitor3  yes, it is generally safe to install an Arrow Lake CPU without a third-party contact frame over the long run. CPUs are designed to function properly with the standard mounting mechanisms provided by the manufacturer. Third-party contact frames are optional and may offer additional stability or cooling benefits, but they are not necessary for the safe operation of the CPU. Always ensure proper installation according to the manufacturer's guidelines to maintain optimal performance and safety.",Neutral
AMD,"u/TerminalCancerMan  Intel cannot comment or interpret results from third party benchmark tools. Run [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) to confirm if there are any issues with the CPU. You may try this, If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor need a replacement.",Neutral
AMD,"u/Progenitor3  Just fill in your info, and it’ll automatically create a ticket for you. Our team that handles those items will get in touch within 3–5 business days.   [Software Advantage Program](https://softwareoffer.intel.com/Support)",Neutral
AMD,Gotta wait for real-world tests to know for sure tho.,Neutral
AMD,"u/Special_Ad_7146 To stay on top of Intel news, **visit** our [Newsroom](https://newsroom.intel.com/).",Neutral
AMD,"u/Hippieman100  just checking can you confirm which software you're using? From what I’ve seen in the latest [ReleaseNotes\_101.6972.pdf](https://downloadmirror.intel.com/861295/ReleaseNotes_101.6972.pdf), there doesn’t seem to be a “Speed Sync” feature in the current version that comes with this driver. It does support V-Sync and Adaptive Sync though. Just wanted to clarify are you referring to the older Intel Arc Control software or the Intel Graphics Command Center? Just making sure we’re on the same page!",Neutral
AMD,"u/TheCupaCupa to better understand and isolate the issue, I kindly ask for some additional information. Please find the details requested below:   1. When the CPU drops to 0.8 GHz, do you notice any **error messages or warnings** in Windows or BIOS? 2. Does the issue happen **only with LeagueClient.exe**, or have you seen it with other programs too? 3. Have you checked **CPU temperatures and power draw** when the issue occurs? 4. Can you check **Event Viewer** for any critical errors or warnings around the time of the slowdown? 5. Are you using any **custom power plans** in Windows, or is it set to Balanced/High Performance? 6. Is **Intel Turbo Boost** enabled in BIOS? 7. **When did this issue first start happening?** Has it occurred before? 8. Have you made any software or hardware changes to the system recently?   Once I receive this information, I will be able to properly assess the situation and provide further assistance.",Neutral
AMD,"u/Alloyd11 Not all 13th and 14th generation processors show instability issues. Just to better assist you are you planning to buy or use one of these processors, or do you need help with your current system?  Let me know how I can support you!",Neutral
AMD,"u/amitsly “crashes” is a pretty broad term, and not every system issue points directly to the CPU. There are quite a few steps we go through to fully isolate the problem before concluding it’s a processor fault.  To help us assist you better, could you please share a bit more info about the crashes?  * When did the issue first start happening? * Have you made any recent changes to the system either hardware or software? * Is there any visible physical damage to the system? * What troubleshooting steps have you already tried? * Have you noticed any signs of overheating? * Have you tested the processor in another working system, or tried swapping it out to see if the issue follows the CPU?  The more details you can provide, the quicker we can get to the bottom of it!",Neutral
AMD,"u/Jay_377 its  likely comes from Intel’s naming convention. The i3-8130U is part of the 8th gen, but it falls under “Kaby Lake Refresh” (for mobile chips), not “Coffee Lake” (which is for desktops). Tools like `garuda-inxi` or `CPU-Z` might label it differently based on how they categorize architectures, not a bug, just naming differences.  [Intel® Core™ i3-8130U Processor](https://www.intel.com/content/www/us/en/products/sku/137977/intel-core-i38130u-processor-4m-cache-up-to-3-40-ghz/specifications.html)",Neutral
AMD,It is because Kaby Lake CPU's are 7th gen processors for laptops and 8th gen to 9th are Coffee Lake check rhis link for better understanding:https://www.intel.com/content/www/us/en/ark/products/codename/97787/products-formerly-coffee-lake.html,Neutral
AMD,"u/Reality_Bends33 Intel has been around for a long time and is known for making solid, reliable products, so you can feel confident about choosing them for your PC build. The Ultra 7 265k and Arc B580 are great picks for video editing, offering the performance you need without breaking the bank. While there’s been some discussion about Intel facing challenges, remember that big companies like Intel usually keep up with driver updates and support, even if they’re going through changes. The tech world is always evolving, and Intel is investing in new technologies to stay ahead. So, you’re likely to get the support you need for your products.",Positive
AMD,"u/unknownboy101 It’s normal for your processor to heat up during heavy tasks like gaming. Intel CPUs are built to manage heat by adjusting power and speed, so they stay safe and avoid damage. However, running at a constant **95°C** on your Intel Core i7-12700H even during light gaming is **not ideal** and could indicate a cooling issue. While Intel CPUs are designed to handle high temperatures and will throttle performance to avoid damage, consistently running near the thermal limit can shorten the lifespan of your components and affect performance. **Feel free to check out this article for more info or steps to try. Just a heads-up this is specifically meant for boxed-type processors. You can still take a look, but I strongly recommend reaching out to your laptop’s manufacturer to get help with the overall system configuration.**  [Overheating Symptoms and Troubleshooting for Intel® Boxed Processors](https://www.intel.com/content/www/us/en/support/articles/000005791/processors/intel-core-processors.html)  [Is It Bad If My Intel® Processor Frequently Approaches or Reaches Its...](https://www.intel.com/content/www/us/en/support/articles/000058679/processors.html)",Neutral
AMD,"u/Frost-sama96 Tenga en cuenta que solo puedo apoyarlo en inglés. He utilizado una herramienta de traducción web para traducir esta respuesta, por lo tanto, puede haber alguna traducción inexacta     **To help me dig a little deeper into the issue, could you share a few details?**  * What’s the **make and model** of your system? Is it a **laptop or desktop**? * Do you remember **when the issue first started** happening? * Which **BIOS version** are you referring to, the one that works fine? If you can share the exact version , that’d be super helpful.",Neutral
AMD,"u/Fluid-Analysis-2354 If your 5070 Ti isn't functioning and your CPU is now your main accelerator, upgrading to the Ultra 9 could be beneficial. The Ultra 9 offers enhanced AI performance, which can significantly improve your computing tasks. If AI capabilities are a priority for you, the upgrade is worth considering.",Positive
AMD,"u/BestSpaceBot , As with all good things, your product has reached the end of its interactive technical support life. However, you can find [Intel® Core™ i7-4700MQ Processor](https://www.intel.com/content/www/us/en/products/sku/75117/intel-core-i74700mq-processor-6m-cache-up-to-3-40-ghz/specifications.html) recommendations at [Intel Community forums](https://community.intel.com/) and additional information at the [Discontinued Products](https://www.intel.com/content/www/us/en/support/articles/000005733/graphics.html) other community members may still offer helpful insights or suggestions.. It is our pleasure to continue to serve you with the next generation of Intel innovation at [Intel.com](http://www.intel.com/). You may also visit this article for more details [Changes in Customer Support and Servicing Updates for Select Intel®...](https://www.intel.com/content/www/us/en/support/articles/000022396/processors.html)",Positive
AMD,"u/ken10wil   To help figure out what’s going on with your CPU throttling issue, I’d like to ask a few quick questions that’ll help me dig deeper:  * When did you first start noticing the problem? * Have you made any changes to your system recently like installing new software, updating drivers, or swapping hardware? * Is there any visible damage to your PC or loose connections? * Have you updated your BIOS to the latest version for your Gigabyte Z790 board? * Did you try resetting the BIOS to default settings to see if that helps? * Have you tried reapplying thermal paste to the CPU? Just to rule out any cooling contact issues. * Are there any startup programs or services that might be messing with your CPU? * What background processes pop up right after boot? You can check Task Manager or use Reliability Monitor to trace anything unusual. * Can you run the [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) and let me know if it passes or fails?  Let me know what you find happy to help you troubleshoot further once we have a bit more info!",Neutral
AMD,u/PM_pics_of_your_roof   Interesting thanks for pointing that out! Let me check this on my end and I’ll post an update here once I have accurate information.,Positive
AMD,u/UC20175 Let me check this on my end and I’ll post an update here once I have accurate information.,Neutral
AMD,"I don't think that there it is awful, I mean in the past where the issue blew out and many consumers are reporting. People at Intel can barely accommodate them but they still give the replacement needed for the consumer.   If i am right, that image is necessary for them to get your serial number and other stuff. Unless you have the box of the processor, or you did not purchase it as a tray then I think you'll be fine.",Neutral
AMD,"Hi, I think you'll need to connect with your laptop manufacturer or Microsoft to help you customize an ideal setting for your display. But could you share your full laptop model?",Neutral
AMD,I think this will help you https://www.intel.com/content/www/us/en/processors/processor-numbers.html,Positive
AMD,"u/Tagracat  May I confirm the exact error message you're seeing during the crash? Also, have you had a chance to contact the developer of the software in question? Regarding the BIOS, could you double-check if you're using the patch addressing instability issues specifically version 0x12F provided by your motherboard manufacturer? Since the Intel Processor Diagnostic Tool passed, not all crashes may be CPU-related.   Looking forward to your response!",Neutral
AMD,"u/Maleficent_Apple4169   What specific storage device are we talking about here? Is it an NVMe SSD, RAID controller, or something else? And what's the make/model of the unit in question? Knowing the exact hardware might help me point you to a more specific solution.",Neutral
AMD,Guess checking with your motherboard manufacturer,Neutral
AMD,"u/ChiChiKiller Just to be clear, what you've described sounds like a possible **fraud or phishing attempt**. Please **do not share your credit card information** with anyone claiming to be from Intel unless you're communicating directly through our **official support channels**. Intel will never say that our website is not secure we take security very seriously, and this kind of messaging is often used by bad actors trying to steal personal information.  I’ve sent you a **private message in your inbox** please check it when you can. Also, could you kindly provide the details of your **initial ticket** and the **new case number** you mentioned through inbox? I’ll investigate this immediately on my end.  Don’t worry I’ll keep you updated as soon as I have more information. For your privacy, I’ll continue communicating with you through inbox so you don’t have to post sensitive details publicly.  Thanks again for bringing this to our attention!",Negative
AMD,"u/Amazing_Watercress_4  Kindly check your inbox, I’ve sent you a personal message. Thank you",Neutral
AMD,"u/PeakHippocrazy  This is completely normal behavior for your Intel Core Ultra 5 235U processor. Modern Intel processors, especially those in laptops like your Dell Pro 14, are designed to automatically park (turn off) cores when they're not needed to save battery life and prevent overheating. Your processor has 10 cores total, and Intel's smart technology decides which ones to use based on what you're doing, it doesn't need all cores running just for web browsing or light tasks. The cores will automatically wake up when you run demanding software that actually needs them. This core parking feature is intentional and helps your laptop run cooler, quieter, and with better battery life. Unless you're experiencing actual performance problems during heavy tasks, there's nothing wrong with your system, it's just working efficiently as designed.  *",Neutral
AMD,I fixed the problem. It was windows msconfig disabling e-cores on boot. You should forbid them from doing this.,Negative
AMD,"I can't remember which (not at my pc), I think it's intel graphics command center. I get a list of v-sync options, off, on, smooth, smart and speed (from memory). Speed was not available in previous versions of the software/driver. Underneath there is an FPS limiter and a control for latency improvement (I can't remember the name) with off, on, and on + boost options available.",Neutral
AMD,"Just got back to my PC, it's actually neither Arc Control or Intel Graphics Command Center. I'm using Intel® Graphics Software (25.26.1602.2), should I be using something else?",Neutral
AMD,"Hello, thank you and I'll try my best.  1. No error messages or warnings in Windows, unsure how to check BIOS. 2. Yes, currently I have only found this happens when LeagueClient.exe is started. 3. Using HWInfo64, right when I open LeagueClient.exe, all P and E cores have ""Yes"" in the Current column for ""Power Limit Exceeded"". Core temperatures are: Current- 33C Minimum- 31C Max- 72C Average- 42C. CPU Package Power, minimum is 65.699 W, maximum is 129.230 W. Upon starting the program, the maximum value doesn't change. 4. For Event Viewer, no warnings come up when LeagueClient.exe is started. 5. I have tried setting it to Balanced, High Performance, but I'm mainly on Ultimate Performance. 6. Yes, Intel Turbo Boost is enabled. 7. The issue first started happening 2 days ago, 07/25/25. No it has not occurred before. 8. I have not made any new hardware changes to the system. I did have a windows update, 2025-07 Cumulative Update Preview for Windows 11 Version 24H2 for x64 based Systems (KB5062660) (261.000.4770) installed on 07/25/25, however I installed this update later on during the day after the problem had already started.",Neutral
AMD,"Well, I didn't immediately blame the CPU. The crash message specifically mentions the issue that point the finger to the CPU, plus the provided link a saying that's the root cause.  Anyhow, here are more details:  1. The issue first started happening about 2 hours into my Expedition 33 playthrough and has happened at least 20 times since (in about 18 hours of gameplay) 2. I have not made any changes whatsoever to software or hardware before it started happening. Last night, after the 20th crash, I did run an update for various drivers & the BIOS (including the 0x12F update) using Gigabyte's CC software. 3. No physical damage that I could observe through the PC's glass window 4. I have tried everything the internet had to offer about this specific issue regarding Expedition 33. This includes verifying game files, changing config and settings, reinstalling, lowering the CPU clock speed and more.  5. I didn't notice the temperature when the crashes happened but I'll be on the look out. 6. I don't have a way to swap out the CPU nor do I have a replacement CPU",Neutral
AMD,"Weird, mine isn't in that list.",Negative
AMD,"I5 14600k desktop  When i try Odysse ACC, \*game\* I notice the start up and loading in to the game was so slow, like 8 or 10 minutes to Load.    \--Versión 182011.06 MB2025/05/21SHA-256 ：493D40A2351EED41FCF60E51346B9065880E87F986C1FD1FB1A3008E8C68DA26  ""Update the Intel microcode to version 0x12F to further improve system conditions that may contribute to Vmin Shift instability in Intel 13th and 14th Gen desktop-powered systems.   Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later. ""-- With this BIOS frrom ASUS page, The game do that   But, when i rollback to>   Versión 181211.04 MB2025/03/28SHA-256 ：98528167115E0B51B83304212FB0C7F7DD2DBB86F1C21833454E856D885C7EA0  ""Improve system performance and stability      Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later.""  Intel microcode 0x12B t  The game Load A lot faster and run well.  I dont know what this happens, but is a error caused by New microcode",Negative
AMD,"\- I started noticing the problem between august the 19th and august the 20th.  \- I updated the system around the time I \*noticed\* the problem, but I am not sure if that is when the issue started. I reverted the machine to a state prior to the update and the problem still occurs. Any other driver updates or software installs happened after I had already noticed the problem, and were initiated trying to solve the problem  \- There is no visible damage to my pc or wires  \- my Bios is up to date, and aside from turning on resizable BAR months ago for my GPU, my bios settings are default. I reset to default and turned resizable BAR on again just to be sure, and the issue still occurred.  \- I have not, mostly because it would be a hassle and because the CPU has remained very cool. I have manually overclocked it as a temporary solution to the problem and even under these conditions it is averaging 35-40 degrees Celsius when idle and hovers around 50 degrees when under stress  \- While the issue did not occur in Safe Mode, I am not sure which program is causing the throttling. I have disabled all installed startup programs and still get throttled.  \- No unusual processes pop up right after boot, though the sum of all processes hits 15% CPU utilization, stays there for a while and then the CPU throttling begins (all happens in less than 30 seconds after boot). I could see what happens after disabling even the security related startup programs and see if there is any difference  \- I pass the PDT tests but I have abysmal performance on various benchmark tests, and the PDT takes a long time to complete. Overclocking leads to more expected results.",Neutral
AMD,"Thank you. I spent some more time looking and it appears VROC on x299 was discontinued sometime in the past two years? Seems intel pushed people towards RST. I guess the question still stands since VROC on X299 has moved into sustaining mode, so drives that maybe came out during that time should still work.      [https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html](https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html)",Neutral
AMD,What I've gathered from support so far is documents needed by section 2.14/2.15 of https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html are behind a premier account registration. I'll try registering an account.,Neutral
AMD,"I'm mostly referring to the fact they've designed the matrix to be printed near invisibly with it being absolutely miniscule and basically the same colour as the pcb, then made it a mandatory requirement for the RMA process.   It reeks of a company trying to dodge responsibility for faulty products by making the hoops so difficult to jump through theres no reasonable expectation the average consumer will be capable of complying.   Why do I need to go out and purchase a macro lens for my smart phone? Its putting me out of even more money than I already am.",Negative
AMD,Ideapad 1 15IAU7.,Neutral
AMD,"No error message per se... the program just closes and there is an event in the event viewer. I chatted with the dev and it looked like it pointed to a virtual memory issue, but it ONLY occurs when bound to core 8. (or all cores including 8)  turns out I was on BIOS 16.02, NOT 17.03 which specifically addresses 0x12F. I will update that next!",Neutral
AMD,it's a NVMe m2 SSD. i don't think it has a specific make/model because I built the computer myself,Neutral
AMD,"u/Hippieman100 **Intel Graphics Command Center** and **Intel Arc Control** have been the go-to software for many users, but they’re being phased out soon, and support will be limited moving forward.  Since system (not your PC) includes an **Intel Arc B580**, I highly recommend switching to **Intel Graphics Software** instead. This newer software is bundled with the graphics driver package, which you can find at the link I’ll provide-[Intel® Arc™ & Iris® Xe Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html). Before making the switch, please take a moment to read through all the details and driver descriptions on that page.  Also, based on my checks, it looks like you're currently using **driver version 25.26.1602.2 on your PC**, which is outdated.  Let me know if you have any questions.",Neutral
AMD,"u/TheCupaCupa If the motherboard BIOS allows, disable Turbo and run the system to see if the issues continues.  If the instability ceases with Turbo disabled, please let me know.",Neutral
AMD,"u/amitsly For further analysis, please provide the crash dump or log files generated by the game. You can follow the guide and scroll down to the section titled **""Crashing/Freezing Issues/BSOD""-**[Need help? Reporting a bug or issue? - PLEASE READ THIS FIRST! - Intel Community](https://community.intel.com/t5/Intel-ARC-Graphics/Need-help-Reporting-a-bug-or-issue-with-Arc-GPU-PLEASE-READ-THIS/m-p/1494429#M5057) for instructions.  Once you’ve obtained the files, kindly notify me so I can send you a private message to collect the logs.  For isolation purposes, please try the following step and let me know the outcome:   **If your motherboard BIOS allows it, disable Turbo Boost and observe whether the system crashes continues.**",Neutral
AMD,Yeah that's weird try checking qith laptop manufacturer about it,Negative
AMD,"u/ken10wil  Thanks for the detailed info - this is really helpful for narrowing things down. What I'm seeing here points to a software issue rather than hardware failure. The sudden onset timeline, passing PDT tests, and the fact that safe mode works fine all suggest you're dealing with a software or driver problem.   Your CPU temps are totally normal too, so I can rule out thermal throttling. That 15% CPU usage spike right before throttling kicks in is actually a big clue - something's definitely triggering this behavior.     [Information about Temperature for Intel® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [How to Know the Idle Temperature of Intel® Processor](https://www.intel.com/content/www/us/en/support/articles/000090343/processors.html)  [What Is Throttling and How Can It Be Resolved?](https://www.intel.com/content/www/us/en/support/articles/000088048/processors.html)  For overclocking: Note that the motherboard does have an impact on the ability to overclock. Some are better quality and more capable than others. Furthermore, the way the motherboard is set up also impacts the overclocking ability of a particular system (such as liquid cooler vs fan). Please note that if the system was overclocked, including voltage/frequency beyond the processor supported specifications, your processor voids warranty.   **Next steps to try:** • **Check Windows power settings** \- sometimes updates mess with power profiles. Set to ""High Performance"" and see if that helps •   **Look at that 15% CPU spike** \- open Task Manager right after boot and sort by CPU usage to catch what's eating those cycles •   **Try disabling Windows security temporarily** \- sometimes antivirus can cause weird throttling behavior •   **Check Event Viewer** \- look for any error messages around the time throttling starts • **Check Reliability Monitor** \- go to Control Panel > Security and Maintenance > Reliability Monitor to spot any anomaly issues or critical events around August 19-20thThe fact that overclocking fixes it temporarily suggests your CPU is being artificially limited by software, not hardware.   Since you've already tried the obvious stuff like BIOS reset and driver rollbacks, you're probably looking at a Windows service or background process gone rogue. Keep me posted on what you find with that CPU usage spike and reliability monitor - those are likely our smoking guns!",Neutral
AMD,"u/PM_pics_of_your_roof  You're absolutely correct about VROC on X299 being discontinued. Intel moved VROC for X299 platforms into sustaining mode within the past two years and has shifted focus toward Intel RST (Rapid Storage Technology) for consumer applications. While VROC is no longer actively developed, it remains supported in sustaining mode for existing users, which means NVMe drives that were released during VROC's active development period (roughly 2017-2022) should still function properly. However, newer drives may work but won't receive official validation or certification. For anyone building new systems, Intel recommends using RST instead of VROC, but existing X299 users can continue using VROC with supported drives from the qualified vendor list. ***Support is now limited to sustaining mode with no new features or drive certifications planned, as confirmed in the support article you referenced.***",Neutral
AMD,"u/UC20175  Per your inquiry:  The necessary tools and firmware files, but they are only accessible through Intel’s Resource and Design Center (RDC), which requires a Premier account.  1. Register for an Intel RDC Premier Account 2. Visit: Intel RDC Registration Guide [https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html](https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html) 3. Use a corporate email address for faster approval. 4. From the RDC, search following Content IDs:  EEPROM Access Tool (EAT) – Content ID: 572162  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162)  Production NVM Images for I210/I211 – Content ID: 513655  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655)",Neutral
AMD,I guess you are the only one who is having trouble on that part. I mean I haven't seen anyone get mad about it. So I don't really seem to find any fault I mean it is their preventive measure for fake processors that are out there. Just ask for some help to take a picture it's not that hard,Negative
AMD,"I think you should revert the Gpu setting to default. Then start changing some settings in the Microsoft setting. There are settings for fonts, brightness, and such. There is also some setting for the smoothness of the animation in the Windows",Neutral
AMD,I suggest getting a replacement if the issue is the same. But make it sure you update your bios first to the latest version and set it to intel default settings.,Neutral
AMD,"u/Tagracat I’m okay to proceed with a replacement if the recommendation has already been tried and you're still experiencing the same issue.  Just let me know if you're good to move forward, and I’ll send you a personal message to help facilitate a possible RMA.",Neutral
AMD,"u/Maleficent_Apple4169 NVMe drivers are typically **chipset/motherboard-based**, not SSD-specific. The driver you need depends on your motherboard's chipset, not the SSD brand.     Here are my recommendation;    Identify Your Motherboard Chipset:  Check motherboard manual/box for chipset info (Z690, B550, X570, etc.)    Universal NVMe Driver Sources:  Windows 10/11 usually has built-in NVMe support  Try Windows installation without additional drivers first  If needed, use Microsoft's generic NVMe drivers you may contact Microsoft for additional guidance.    Motherboard Manufacturer Support:  Visit your motherboard brand's website (ASUS, MSI, Gigabyte, etc.)  Download chipset/storage drivers for your specific board model    Most likely solution, download Intel RST drivers or AMD chipset drivers based on your motherboard's chipset  this will provide the NVMe controller drivers needed for Windows installation.",Neutral
AMD,"You have misunderstood, I am using intel graphics software already as I said, 25.26.1602.2 is the version number of that software. My driver's are up to date according to intel graphics software.",Neutral
AMD,"Hello,    great news, it solved itself. I opened the program this morning and everything was fine. If this happens again, I will try disabling Turbo and see what happens. Thank you for the help!",Positive
AMD,So what is the QVL list for x299 VROC? I can’t find that information. I can find QVL for C621,Neutral
AMD,"So I was able to download the files from the RDC. Currently when I run        eeupdate64e.efi /NIC=2 /DATA Dev_Start_I210_Copper_NOMNG_4Mb_A2.bin  It says ""Only /INVM* and /MAC commands are valid for this adapter. Unsupported."" This is for three 8086-1531 Intel(R) I210 Blank NVM Device. Presumably in order to program them with device id 1533, I need to run a different command or use a different tool?  Thank you for your help, I feel like it is very close to working.  Edit: I believe it works, using /INVMUPDATE",Neutral
AMD,"[This is literally what I am dealing with.](https://imgur.com/a/WBowyjO)  I'm not even worried about posting this image because please tell me how to get a better photo than this with a regular smart phone, and you can barely see there's anything there.",Negative
AMD,"Yes, I've turned off the system animations, and the blinking cursors.  There aren't settings to switch fonts, to replace thin/faint fonts with bold ones. The folks developing Windows like Segoe Ui, maybe they can read it. There are regedit hacks, but I'm not sure how to do them. There is also Winaero Tweaker, which can switch some fonts, but doesn't work on some older apps with unreadable text. There is MacType, which can bolden text without switching fonts, but doesn't work everywhere either.  There are settings to scale fonts, or scale everything, or reduce resolution. I've tried every combination of these. I've reduced resolution, since it does work everywhere. But I can't go below 1280x720, and I end up breaking some interfaces anyway. There is also the magnifier, but it gives me a migraine.  Here's the thing. If text is too thin/faint, making it 2x as bold will take less than 2x the screen space; making it 2x as big will take 4x the screen space.  I'm surprised how well tweaking gamma has worked. So far it has worked everywhere with dark text on light images, and it's worked very well.",Negative
AMD,I'm now on the new BIOS (with Intel default settings) and the crashing is still occurring on Core 8. Alas... I was really hoping that would fix it.  What are the next steps?,Negative
AMD,"u/Hippieman100 Ah, I see I overlooked that part! Looks like this is the installer version, my apologies for the confusion since we were discussing three different software options earlier.  Alrighty, since you're using the latest version now, how can I help? Are you running into any issues with the new application, or is there a specific feature that's not working as expected?",Neutral
AMD,"u/TheCupaCupa Great to hear it fixed itself! If it happens again, trying Turbo off sounds like a good plan.",Positive
AMD,"u/PM_pics_of_your_roof  You're encountering a common issue, Intel doesn't maintain a centralized QVL (Qualified Vendor List) specifically for X299 VROC at the platform level. Since X299 is a consumer chipset, the VROC compatibility lists were typically maintained by individual motherboard manufacturers rather than Intel directly. The C621 QVL you found is for Intel's server/workstation chipset, which has more formal validation processes. For X299 VROC compatibility, you'll need to check with your specific motherboard manufacturer (ASUS, MSI, Gigabyte, etc.) as they would have maintained their own compatibility lists during VROC's active period. However, since X299 VROC is now in sustaining mode, many manufacturers may no longer actively update these lists. Your best bet is to search for your specific motherboard model's support page or contact the manufacturer directly, though given the discontinued status, this information may be limited or archived.",Neutral
AMD,Relax 😅 and follow this:https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html,Neutral
AMD,"Yeah switching fonts, I haven't tried it. But in windows settings you can change its size and such. Try looking at the Microsoft community or support changing it. But I am glad that adjusting the gamma helps.",Neutral
AMD,u/Tagracat  Kindly follow the article linked below to request a warranty replacement. You can use this Reddit thread as a reference for faster processing.  [How To Submit an Online Warranty Request](https://www.intel.com/content/www/us/en/support/articles/000098501/programs.html),Neutral
AMD,"Thank you for the reply. Sadly asrock doesn’t have a QVL for nvme drives in U.2 form factor or enterprise grade drives, just m.2. Let alone a QVL for VROC support. Looks like I’ll be the test subject for this.",Negative
AMD,"That is what I'm following, I provided support everything on that list, including decoding the matrix on the front of the chip but they don't proceed with the ticket until they get a picture of the code on the pcb which is nigh on impossible.  I wound up taking it into the office to use a high resolution scanner thats used to digitise paintings and just about got something readable. I remain firm in my stance that whilst wanting to verify a serial number isn't outrageous, making the process in which you verify said information virtually impossible is.   I simply don't understand how regular people can take such a detailed photo using smartphone technology. My phone is 4 years old and was absolutely not capable (Samsung S20+)",Negative
AMD,u/PM_pics_of_your_roof  **Best of luck with your configuration!** Your pioneering work might just pave the way for others looking to implement similar setups.,Positive
AMD,"u/Danderlyon  **just sent you a message, check your inbox when you get a chance!**",Neutral
AMD,"Just wanted to follow up for anyone seeing this or is in a similar boat. I can confirm that Intel DC P4510 drives will work in a VROC Raid using a standard key.      Not sure if intel support can confirm but it appears the ASROCK X299 Taichi CLX supports VROC Raid across multiple VMD Domains, which is supposed to be locked behind Xeon scalable CPU/Chipsets. I need to buy two more drives and try to add them to my array to confirm.",Neutral
