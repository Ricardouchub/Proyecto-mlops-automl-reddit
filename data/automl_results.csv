brand,text,sentiment
Intel,"It's good they launching this, this card adds some competition to the landscape, but before anyone buys it they should figure out if their drivers are lighter weight.  It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  The more ARC cards out there the more developers get familiar with them, the more XeSS v2 gets added to titles, the more the drivers get matured and the better future ARC cards will be.  I'd happily pick up an ARC card... once they've proved themselves in terms of driver maturity and overhead.",Positive
Intel,This should be a great upgrade from my A750 if B580 performance is anything to go by. I hope it's under $400.,Positive
Intel,I hope they fixed the drivers CPU overhead problem or that GPU's gonna need a 7800X3D or 9800X3D to feed it fully.,Neutral
Intel,Too late for me I already went with a 9060xt but hell I had dreamt of it!,Negative
Intel,"Unless it's super cheap, it's not gonna be sold well at all.  Even here with all the ""enthusiast"" and people are saying ""make sure you have this hardware combo, that driver, these settings,..."". The average buyer would just simply pay 50$ more for an nvidia card and not have to worry about all that.",Negative
Intel,"I wish they'd get the drivers past the point of frequently broken, but also they haven't produced enough cards for any previous launch to make any dent in the market regardless.  It's pretty much guaranteed the upcoming super refresh will make much more of a difference in terms of bang for your buck.",Negative
Intel,The problem with arc is you need the latest and greatest CPU to go with or you lose 1/4 of performance,Negative
Intel,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Depending on the price I might give it a shot.,Neutral
Intel,Love it going to get one if I can scratch some money together,Positive
Intel,Intel has the ball in it's court   If you released a New GPU..  that is pretty much a 5070.... add on 24gb of ram...  and price it at 399   u will   make  boatloads.  it will play pretty much any game at max settings at 1440p..  They must really be  hating on turning down sony though at making the SOC for the PS6 cause the margins too low..they really would need that money now lol,Neutral
Intel,Wouldn’t there be a risk that future drivers will not be supported and that it comes with US government back doors?,Negative
Intel,"LOL, preparing ""box"" packaging   I immediately thought of advanced silicon packaging like CoWoS or whatever",Neutral
Intel,Who is gonna tell them G31 would be celestial die since B580 was G21 and A770 was G10?,Negative
Intel,"Ah yes, finally the 4060ti 16gb/4070 killer, only 1.5 years too late! Ig at least this will force price drops on the rx9070",Positive
Intel,Aren't the driver overhead issues really only seen on older processors that are six cores or less?  Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course,Neutral
Intel,"> It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  Is it really though? Powerful CPUs are comparatively cheap, powerful GPUs are expensive. I know plenty of people who went with a 9700X/7700X or even 9800X3D but 'cheaped out' on the GPU because spending $1200 on a 4080 (at the time) was simply too much.",Neutral
Intel,"It's a hardware problem, not a driver problem. The cards have insufficient DMA capabilities and uploads must be handled by the CPU, no driver will fix it, and as a consequence the B770 will have even worse overhead.",Negative
Intel,"I agree with everything you said.  However, I myself will buy one just because I want more competition and so I am just going to give Intel a sale. Sure it doesn't move the needle much, sure Intel's probably not going to make any money out of it and I personally probably won't use it much, but I am just doing it out of principle. I sure am in the minority, but at this point I can't sit idle and allow this duopoly to continue without trying something.",Neutral
Intel,Even as an enthusiast I don't want a second job troubleshooting infinite permutations of issues on immature hardware and drivers with my very limited free time.  I want to maximize my enjoyment in the free time.  A +$50-100 (hell even much more$) spend to solve/minimize that is always going to win my vote and my wallet.   I don't care to pay both time *and* money to recieve hassle.,Negative
Intel,I have a B580 and the driver seems pretty stable to me at this point.,Positive
Intel,"The super refresh is only for 5070, 5070ti and 5080. I doubt the B770 will compete with the 5070 to begin with so those cards are more upmarket.",Neutral
Intel,my b580 has been stable,Positive
Intel,"I’ve had a B580 for 6 months and have experienced one game-specific issue with Tarkov. Everything else, old, new or emulated has worked fine.",Positive
Intel,"I’ve been using Arc on both windows and Linux since alchemist, it’s powering 3 rigs for gaming, transcoding, etc.   Initial few months was rough but drivers are absolutely serviceable and have been for a while, and continue to get better each release.  I play lots of different games on steam btw, very rarely do I have issues.",Positive
Intel,to our knowledge... i wonder what kind of uplift we'll see it have with next gen cpus,Neutral
Intel,It's BMG-G31.  https://videocardz.com/newz/intel-confirms-bgm-g31-battlemage-gpu-with-four-variants-in-mesa-update  https://videocardz.com/newz/intel-ships-battlemage-g31-gpus-to-vietnam-labs-behind-past-arc-limited-edition-cards,Neutral
Intel,"It isn't the number that determines the generation, it's the prefix.  A770 was ACM-G10 (alchemist G10), while the B580 is BMG G21 (Battlemage G21). The shipping manifests that have been floating around for the better part of a year have been for the BMG G31. Unless new leaks I'm not up to date with are discussing a G31 with a different prefix, everything points towards it being battlemage, not celestial.  Now I pray that Intel have found a way to mitigate the driver overhead. If not, the B770 will be utterly useless for gaming. Nvidia is bad in the overhead regard, but the B580 is damn near an order of magnitude worse.",Neutral
Intel,> Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course  Not even close.,Negative
Intel,"HUB showed the b580 lost like 20%+ perf between the 9800x3d and the 7600 or 5700x3d and actually fell behind the 4060, as the 4060 lost minimal performance on the weaker cpu vs using the 9800x3d. And the 7600x and 5700x3d can certainly power much stronger gpus like the rtx 5070 without bottleneck.",Neutral
Intel,Where do you find such information?,Neutral
Intel,Source: I made it the fk up,Negative
Intel,"I was going to say, I put my sister in a b580 and she has had no driver issues in 6 months.",Positive
Intel,"That's sort of my point, they'll probably still exert more price pressure across the stack than the b770, despite being a totally different segment.",Neutral
Intel,https://youtu.be/00GmwHIJuJY?si=z4wU05sJx2SeS7K1  How can people get on here and lie and literally no one questions them? The 7600 only lost anywhere near that performance on Spider-Man Remastered,Negative
Intel,"Inference, there was a blog post tracing and comparing what the Arc driver does with the Radeon driver. The radeon driver just sends a few pointers to buffers, the Arc driver sends large amounts of data. Assuming the driver programmers at Intel aren't idiots, it's because something is seriously wrong with the cards and DMA.",Neutral
Intel,"No, I inferred it from tracing what the driver does, and assuming the programmers aren't idiots.",Neutral
Intel,Try Mechwarrior 5: Clans on high and say there are no problems again.,Positive
Intel,"https://chipsandcheese.com/i/154252057/driver-cpu-usage-vs-application  By the bars, it looks API specific which hints driver problems.  But maybe there is some command protocol inefficiency. I'll certainly hope at least one of the Steves will look into B770 driver overhead.",Negative
Intel,"that game runs on my 3080 ti like ass, just like all early UE5 games...  even with RT turned off, to hit solid 4k60 I needed DLSS and if I wanted 90+ fps I need to use DLSS performance / ultra performance.",Negative
Intel,Steve from HUB and Steve from GB both lack the technical knowledge to look into the underlying issues,Negative
Intel,"It doesn't even run, it crashes left and right on an Arc.",Negative
Intel,"Not looking for a chipsandcheese analysis, just whether the issue exists.",Neutral
Intel,"that I have no idea, on launch I did have crashing issues on my 3080 ti, but they did get resolved over time.  but if you are now seeing it still then welp  PGI is a small team that may not have gotten help / getting to it themeslves to make their game arc capable.",Negative
Intel,"Yeah, still doesn't work at the latest patch (and latest Arc driver) with anything other than low.",Negative
Intel,SR-IOV at that price. Who cares about anything else.,Neutral
Intel,Intel would be stupid to axe there Graphic card division if this proves to be successful.,Negative
Intel,"Single-slot variant or custom cooler please, my MS-A2 running proxmox is demanding this card.",Neutral
Intel,About 66% overall performance of a B580 it looks like. That's really nice for a 70W card.,Positive
Intel,"This is exciting, definitely looking forward to the b60 as well.",Positive
Intel,"Obligatory ""Intel is exiting the GPU business any moment now"".",Neutral
Intel,how hard are tehse to actually buy?,Neutral
Intel,"Buying one, this is impressive",Positive
Intel,Its better than a 1.5 year old bottom of the range card....well done i guess.,Positive
Intel,Better than NVIDIA? lol .... oooookay,Positive
Intel,"Haven't seen the video, but I'm already buying one if that's the case",Neutral
Intel,Literally it could be a damn arc a310 or rx6400 and people would buy that card at $350 without licencing bs. For anything VDI related the B50 is huge.,Negative
Intel,Intel’s “MOAAAAAR CORES” in the GPU space???,Neutral
Intel,what is that? SR-IOV?,Neutral
Intel,Super interesting!  Wonder how well it would handle AI tasks like Frigate while another VM uses it for inference or a third doing video transcoding with Plex.,Positive
Intel,16 GB VRAM too,Neutral
Intel,They will eventually axe it.,Negative
Intel,Instead of axing it maybe spin it off like AMD did with Global Foundries?,Neutral
Intel,And if it isn’t successful?,Negative
Intel,They do have 3rd party vendors for ARC PRO Cards this time around so it most likely will happen.,Neutral
Intel,"The B60 is more exciting to me just for that 24GB VRAM. Still, at this price point the B50 is a pretty compelling buy tbh.",Positive
Intel,I think it would be really stupid for them to do so.,Negative
Intel,You can preorder from newegg now. They ship later this month.,Neutral
Intel,"One Swedish retailer I checked has them coming into stock next week (10 September) and open to orders, how much stock there will be however, I have no clue.",Neutral
Intel,Same. I put my preorder in. Plan to put it into one of my sff builds.,Neutral
Intel,What did bottom of the range cards cost 1.5 years ago?  How much VRAM did they have?  Did they support SR-IOV?   Just think for a bit sometimes.,Neutral
Intel,It quite literally is. Watch the fucking video.,Negative
Intel,"Wendell confirmed as much in the comments, looking forward to his future testing of the card.",Positive
Intel,What does AMD have in this product segment?,Neutral
Intel,"Ingen av de kortene greier LLM, hvis du tror det.",Neutral
Intel,"Ingen av de kortene greier LLM, hvis du tror det.",Neutral
Intel,"I didn't know either so I looked it up.   ""SR-IOV (Single Root Input/Output Virtualization) is a PCI Express technology that allows a single physical device to appear as multiple separate virtual devices, significantly improving I/O performance in virtualized environments by giving virtual machines direct access to hardware. This bypasses the overhead of a software-based virtual switch, resulting in lower latency and higher throughput for demanding applications by dedicating virtual functions (VFs) to guest VMs.""",Neutral
Intel,Everyone (including Nvidia) is moving toward APUs with large GPUs onboard. Why would Intel kill their chance at competing in that market?  They've already withstood the most painful part of the transition. There's no point in stopping now.,Neutral
Intel,They are keeping their Fabs which is even more expensive to maintain why would they sell GPU not to mention their iGPUs are pretty Damm good nowdays not like meme in Intel HD4400 even though they could play any game /jk.,Neutral
Intel,Doubt they have the revenue to spin out successfully without significant investment from outside sources.,Negative
Intel,Sadly Intel has a recent history of making poor life choices.,Negative
Intel,"Maybe it's just me, but this reads as AI generated.",Neutral
Intel,"I dunno man, I was building a PC for work and the 3050 was the cheapest Nvidia card I can get and the 7600 is the cheapest from AMD. Huge price gap between the two, by about 100 USD. AMD really needs to buck up their APUs to render cheap GPUs surplus or have something cheaper than a 7600 to price match the 3050.",Neutral
Intel,"They're comparing it to an entry-level NVIDIA GPU, the A1000. Saying that Intel GPUs are ""better than NVIDIA"" as a universal statement is flat-out wrong. Let's see some competition to the RTX 5070 Ti, 5080, or 5090. NVIDIA has zero competition on mid-range and high-end GPUs.",Negative
Intel,Radeon Pro V710 and you can't even buy it retail.,Negative
Intel,thanks 🙏,Positive
Intel,Because intel shareholders are super short sighted.,Negative
Intel,this comment is the weirdest version of 'corporations are people' that i've encountered,Negative
Intel,Lmao seriously the formatting and the amount of bolded words just screams AI,Negative
Intel,It's AI generated in your mind,Neutral
Intel,"because AMD has a bad habit of leaving a bunch of their older cards in the channel and having them become the low end...  CPU and GPU, AM4 lives for so long because there are still piles of the stuff in the channel and just gets slow tiny discounts till its gone in full  its like their demand forecast is too optimistic or something but at this point I think its deliberate",Negative
Intel,Because this is not a gaming GPU and thus the A1000 is the correct card to compare with.,Neutral
Intel,Good luck using those super gpus to host multiple gpu accelerated vm with one card. Nvidia won't let you.,Negative
Intel,"Yes, compare an Arc Pro to a GeForce, totally the same market.",Neutral
Intel,"That seems more mid or high tier rather than these relatively low tier gpus, the b50 is a cut down b580...  Also the v710 seems like the kind of ""passive"" gpu that's ""passive"" as long as it's next to several 7,000 rpm fans.  So it would probably not work very well as a retail car because it's rack server focused.",Negative
Intel,"I think the really loud short sighted shareholders have quieted down a bit after it became clear they helped drive the company to where they are. Hell, they're probably not even shareholders anymore.",Negative
Intel,The weirdest version was Citizens United,Negative
Intel,Did you not figure out why they’re bolded?,Neutral
Intel,I'm aware but it's the only current gen GPU for graphics workloads that has virtualization support from AMD.,Neutral
Intel,"The current Chairman of the board, Frank Yeary, is one of these stupid short sighted people. He REALLY wants to sell the fabs, and is probably the reason Intel went through their latest round of layoffs (Lip-Bu Tan wanted to raise money from Wall Street, Yeary apparently sabotaged it).",Negative
Intel,"if corps are people, they should be allowed to vote, right ?",Neutral
Intel,"Not only that, but because they are people, they should also be able to fund gigantic super PACS to get a candidate into office. I love America!",Positive
Intel,"u/michaellarabel It would be super cool to have Molecular Dynamics benchmarks for these kind of cards, since you already use them for CPU testing and a few of them (e.g. GROMACS) support APIs from all three vendors (CUDA, ROCm, SyCL, + OpenCL)",Positive
Intel,"Same price as B580 with lower performance, 4GB less vram and 128 bit bus.  A round of applause for Nvidia",Neutral
Intel,"GB207 being slower than AD107 is pathetic, what's the point of these x07 dies again? They're not thst mich smaller than recent x06 dies.  They're spaffing design cost on these barely different dues.",Negative
Intel,"Now that we have a third-party review, it pretty much confirms what Inno3D said the other day, it's definitively slower than the RTX 4060 by about 5-7%.",Neutral
Intel,The RTX 5050 is 2.5% slower than the Arc B580.   It's also a 50 series card that costs $250.,Negative
Intel,> the system used a Ryzen 7 9800X3D  If the B580 only wins by 2.6% with this CPU then it's going to lose when you use something weaker because of that CPU overhead problem with Intel.,Negative
Intel,It's actually surprising that it's that close to a 4060 considering the 5050 only has 2 GPCs as opposed to the 4060's 3,Neutral
Intel,IDK but that performance is actually not bad. It should've just been cheaper.,Positive
Intel,The only positive about this is they're likely going to make a 5040 with a cut down chip that should pretty easily fit <75W.,Positive
Intel,"Honestly for people who don't need a lot of GPU power, not the worse.   I have two work computers, one with a 4090 and an old one with a 3090Ti.  These GPUs sit idle just taking up space, would have made more sense to get the something less performant.",Negative
Intel,272mm\^2 for B580 vs \~150 or less for RTX 5050 and perf within 5% lmao,Neutral
Intel,"I think to lose in sales, it would have had to lose a little more convincingly.  Yes, it's worse. But not enough to make up for features and brand-loyalty / nvidias massive reputation with gamers who are not into tech.  This thing will sell. It will sell A LOT. Because Average-Kevin who just wants to play Fortnite and some League will have a great time with it. His YT videos will get upscaling, his shitty mic-quality will get fixed (mostly) by Nvidia Broadcast... and he DGAF why we think it's the wrong move.",Negative
Intel,"Does it actually fall behind the b580 or is it due to vram bottleneck in certain instances?  Either way, this should open up people’s eyes about the b580. The b580 is just marginally above the 5050, which is a terrible product, with 4gb more vram… even at its 250 dollar price point, it’s never been a good deal. Not to mention how it’s going for 400 these days.",Negative
Intel,"People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.  Edit: r/hardware is dumb CPU overhead is a thing and budget GPU buyers need to know which of these two cards is effected the most by it. Everyone already knows that you are going to be GPU limited on top tier CPU's its not valuable information for a budget card.",Negative
Intel,Looks like people already forgot that testing the B580 with a top end CPU gives unrealistic results for actual budget buyers.  Every trick in the book for a nvidia sucks article.,Negative
Intel,Don’t make Jensen sad,Negative
Intel,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Will it also fall behind them in the Steam hardware survey?,Neutral
Intel,the 3060 is still a better value than this card bc its about same speed but 12gb vram and higher bus,Positive
Intel,"Maybe a product for non-gamers who want/need multiple display setup, like me. Even then, if you bother to spend that amount of money for 5050, better add some more to get 5060, and if bother to get 5060 just better get 16GB one. Furthermore, you may feel why not add bit more to get 5070 series rather than 5060 16GB. That's how these things are priced.",Neutral
Intel,Value engineering to extract as much wealth as possible while unable to match performance conditions.,Neutral
Intel,When you gimp the memory bus of course it is not as good,Negative
Intel,"No 75 watt gpu, pin connectior, yeah this is death of arrival.",Negative
Intel,"Trying to convince people to buy a 8gb card in 2025 should be illegal. Yet, people will line up to buy it because of the Nvidia logo...",Negative
Intel,"It's a Nvidia Rx 7600, actually good recommendation over that card if it released for the same price in India. Rx 7600 is awful compared to rtx 5050 in pretty much everything.",Positive
Intel,Humiliation for the RTX 5050 is well deserved. It's good this gets curb stomped by the Intel B580.  More market share for Intel because AMD also introduced a useless 8 GB card.,Positive
Intel,">128 bit bus  Seems like this isn't the main issue with 5050's performance. 4060 has lowest memory bandwidth out of all 3, yet it's the fastest card. The die is just too cut down.",Negative
Intel,Unfortunately it will still sell like hotcakes in pre-builts to people who don't know much about computers.,Negative
Intel,"Their brand name is so strong that despite being worse in basically every way, including having poor drivers which was one of their major selling points in the past, they will still sell super well",Positive
Intel,"I just looked at PCPartPicker and the cheapest B580 on there, in the US, is $299 (and the brand is Onix). The Intel Limited Edition is $340. If you can get the 5050 at MSRP it would have a decent price advantage.  I mean, I wouldn't get a 5050 but in the US at least, the $249 price for the B580 isn't real.",Neutral
Intel,"Where I live none of the main sellers stock the B580, a round of applause for Intel.  The B580 isn't faster on the CPU's owned by the people who are in the market of a budget GPU.  People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.",Negative
Intel,Intel apparently hasn't shipped any GPUs last quarter. The shortage is already been seen as prices of b580 are going up.,Negative
Intel,They're really pushing what they can get away with these days. Less VRAM and narrower bus but want the same money... bold strategy.,Neutral
Intel,"> with lower performance,  even on lower end cpu? Holy crap nvidia wtf",Negative
Intel,Because it will sell a shit load. Probably will be one of the highest selling Nvidia GPUs this generation,Negative
Intel,"At these small die sizes, yields can be really high. High enough that if they wanted to have supply for a 5050, they might not have enough GB206 dies that can be cut down to 20/36 SMs.  The 5060M is probably catching the bulk of the bottom-level GB206 dies at 26/36.  There's also the memory difference, with the 5050 having gddr6 while the rest of the lineup uses gddr7. With dies this small, that cost difference in memory could actually make it worth taping out a new chip with controllers for the cheaper stuff. Gddr6 is dirt cheap at this point.  You could absolutely then argue that there's no reason for the 5050 to be so cut down that it needs a new, even smaller die, and that Nvidia can eat some margins on a budget card to give it better memory. And I'd agree. But I'm sadly not anybody who can make that decision.  Personally, I can't wait to see what becomes of the cut-down GB207s. The 5050 is a ""golden"" fully-enabled die. There will be some with only 18, or 16, or even fewer working SMs. Hell it could lose an entire GPC and be down to half-working. Those will be some absolutely sad little GPUs.",Neutral
Intel,> It's also a 50 series card that costs $250.  I'll believe that when I see it in stock for 250. MSRP is meaningless if the product isn't available for that price.,Neutral
Intel,And will be even worse since it's vram is 8gb. In many many games the b580 would Perform even better,Negative
Intel,Yeah it would be interesting to test with a more real-world CPU pairing.,Positive
Intel,"Yeah, that was my question, did that ever get fixed? It seems like nobody remembers that anymore",Negative
Intel,"TBH, they should have made THIS card a 75W SKU. It probably easily can be, without losing much performance. That would make it a very viable upgrade path for low-end systems. And kind of make the price more attractive, because it would require zero additional changes.",Positive
Intel,Margins go crazy,Neutral
Intel,4x the performance with MFG though.,Neutral
Intel,In mkst cases comparison would make sense since you testing gpu's not anything else. But when you toss in b580 complaints about using too powerfull cpu make much more sense,Neutral
Intel,I have a system with a 9950x3d and a 1060. I’m thinking of getting a 5050 to go with it.,Neutral
Intel,> Every trick in the book for a nvidia sucks article.  That's just a standardised testing suite.   Someone will have low-end CPU testing once this gets any distribution.,Negative
Intel,They're using the same cpu for the 5050 too. What a weird comment to make.,Negative
Intel,"Don’t forget how everyone is shitting on the 5050, calling it the worst nvidia card ever, but were praising intel non stop for the b580.  Even using purely stats from the article, b580 is 3% better with 4gb more VRAM than “worst gpu ever”, so I don’t see how this translates to a win for the b580. Being better than a terrible product by a hair doesn’t exactly make it good.",Negative
Intel,Same can be said for the 5050 no? Both have CPU overheads unlike AMD which any mid-range AMD GPU can easily outperform Nvidia's 5090 at 1080p.   The 12GB VRAM however are better suited for people who are stuck at older Gen3/Gen4 PCIE speeds.,Neutral
Intel,"It'll run nearly every game in existence at 1080p with playable framerates with the right settings. IDK why it would be a product for non-gamers. Calling it that means the B580, 4060 and 7600 are products for non-gamers, too.  Only game I could think of it wouldn't run properly would be oblivion remastered, which has terrible performance on anything.",Negative
Intel,3050 released with the exact same specs and msrp. The 70w variant only came out 2 years after. The first variant was in top 10 of steam's hardware chart.,Neutral
Intel,"This is not a card for 1440p, as it would have issues with getting even 60 fps there, and in 1080p 8gb is mostly enough.  There is a difference between choking on VRAM with 80 base fps (5060ti) and 50 base fps (5050). Yes, second case is still annoying, but you might want to dial down the settings anyway, and then you would be fine.",Negative
Intel,Nvidia RX7600? That a new GPU? 😂,Neutral
Intel,"Funnily enough the 8gb 9060 XT is the best brand-new alternative at $300 and below, hands-down, and that includes the B580. It's x16 interface means it's less restricted by the vram compared to the 5060/Ti 8gb. The B580 still has overhead issues, VR issues, game issues and is rarely available at MSRP.  But people trash it anyway and praise the B580 without actually looking at its performance. In the benchmark linked by the OP alone it loses to the 4060 lol",Positive
Intel,"Performance is fine I'm sure it'll perform just as good as the 3060 maybe slightly if you lower the power to 75W, call it a 5030 and sell it for $150. But these are different times and Nvidia doesn't do reasonable things like that anymore.  Bottom line, it's just the price and TDP that sucks.",Neutral
Intel,TBF a pre-built you're just looking at:   * What's the price?  * Does it have a graphics card or not?  * Overall package/size,Neutral
Intel,"or even people who have a baseline knowledge, but think ""dlss and framegen"" will make this abomination better than its competition",Negative
Intel,"It's not so much the brand name but their near monopoly in the prebuilt market. There will be 1 prebuilt with a 9060XT 8 or 16GB for every 20 prebuilts with a 5050/5060. It's so bad that even the prebuilts with the 16GB 9060XT will be outsold by prebuilts with a 5050 at the same price range, as companies like to pair trash GPUs with ""high"" end CPUs like a 13700F to clear stock but the ""i7"" allows them to mark up the price.",Negative
Intel,"It was in stock at newegg monday for 3+ hours at $249.99, I picked one up after seeing a post that was 3 hours old on /r/buildapcsales   I've stopped into microcenter a couple of times and they said they get them, but I just haven't been lucky enough to be there when they were restocked.  EDIT: it is back in stock as of 11:40 EST at Newegg",Neutral
Intel,"In Germany you can find Arc B580 for €269-283 low-end, and B570 for €218-235.",Neutral
Intel,Her in Canada they are $360 CAD. Cheap cheap. And ways to find. Might be a issue in your country perhaps,Neutral
Intel,"Went on Shopee and the lowest I could find is $280, which is not bad considering there's usually taxes which increases prices of GPUs by 10-20% from global USD MSRP.",Neutral
Intel,">the $249 price for the B580 isn't real.  Yes it is, it just gets sold out as soon as it's restocked - https://www.nowinstock.net/computers/videocards/intel/arcb580/",Negative
Intel,Doesn't it just need a 7600?,Neutral
Intel,Not in Canada. Prices are exactly as launch. Could be your countries problem,Negative
Intel,"Eh, they recently shipped a new batch at MSRP to Newegg.  Don’t know what’s going on with the partner cards. Probably the typical shenanigans",Negative
Intel,You forgot about the smaller dies.,Negative
Intel,"The chip is tiny, so a single wafer -> more chips. The demands on power delivery, cooling etc. are nothing to speak of. And the GDDR6 memory is cheap and readily available.  If anything will ever be in stock, it's this card.",Positive
Intel,5060 has been in stock at MSRP everyday since launch. This should be the same. Even the 5060ti 8gb and 5070 are at MSRP.,Neutral
Intel,MSRP has always been meaningless because it stand for suggested retail price.,Negative
Intel,"But if the recent LTT video is halfway accurate, AMD would murder both of them in that scenario. The 9060 XT 8GB might want to have a word with those two cards.",Negative
Intel,negative margins for intel,Neutral
Intel,The B580 showed we need to test in the systems the cards will end up being used in. The restricted PCIe lines on the lower tier cards i.e. only use 8 also means they need to be tested on older systems.,Neutral
Intel,You really believe you are an average 9950x3D owner? Really?,Negative
Intel,"Out of curiosity, what is your use case in such a setup? I find myself cpu bottlenecked quite often on a 7800x3D but i use a 4070S, when i used 1070 GPU was bottlenecking me.",Neutral
Intel,"But Intel Arc drivers have a really bad overhead, while nvidia does not, so in gpu bound games the 5050 will have around the same performance but the b580 will lose a lot of it, so it's not a weird comment, it's a very important point.",Negative
Intel,>in 1080p 8gb is mostly enough.  The narrative was pushed so much that this fact became an unpopular opinion.,Negative
Intel,nvidia Rx 7600 equivalent or Rx 7600 alternative by nvidia. Ig people can't even get a silly joke nowadays,Negative
Intel,The 12 GB of RAM in the B580 will age more gracefully for a truly low end gamers. On purpose of 8 gb is retro gaming.,Neutral
Intel,"Hate to be that guy but to call this a 30-class card is insane. The 1030 was 25% the performance of the 1060, and the 1630 was around 45% the performance of the 1660. This card is 80% the performance of a 5060, which should probably be called the 5050.  To take it a step further, the GT 1030 was 13% of the 1080ti, the 5050 is 20% of the 5090. The 1050 was 25% of the 1080ti, so it would be reasonable to call this a 5040 or I could even be satisfied with 5040ti.",Neutral
Intel,That would be an interesting card,Positive
Intel,I think a big thing about prebuilts are the aesthetics. Does it look good? is the cables managed? Is the RGB controllable?  Slapping a PC together is easy but building something that looks beautiful through the side panel is not as easy.,Neutral
Intel,The 4060 is 2% and DLSS did plenty of work for the even slower 3060. DLSS is going to be great for this card. Frame gen will be good in games this card can run at good base framerates.,Positive
Intel,"cool, it's in stock now",Positive
Intel,A 2-year old CPU ushering a new platform at the time of B580's launch. Far from a budget user's upgrade timeline-wise.,Neutral
Intel,"For a brand-new truly budget build, AM5 is still restrictive pricewise to people eyeing GPUs of this level. The 3050 and 6600 were well-known inclusions in $500 builds, building with a 7500F is going to cost $280-300 just for the cpu, mb and ram alone. AM4 or LGA1700 otoh can be as low as $180-200.",Neutral
Intel,Depends on the game  The 7600 can see some serious performance deficits in some games like Spider Man  https://youtu.be/00GmwHIJuJY&t=521  and Hogwarts' minimum FPS is affected  https://youtu.be/00GmwHIJuJY&t=468,Negative
Intel,I mean tariffs in the US are a thing...,Neutral
Intel,"It wasn't always meaningless. Overall deviation from MSRP used to be smaller, and it wasn't hard to find the basic models at MSRP once upon a time.",Neutral
Intel,more like we need to test with different processors. Like with high-end to see absolute maximum and some lower grade to see how good/bad driver overhead is,Neutral
Intel,I never said I was,Neutral
Intel,"I'm looking into this now, and you are correct. I am just reading this for the first time.",Neutral
Intel,Lol no. For truly low-end gamers the B580 will perform worse because of its CPU overhead that still hasn't been fixed more than half a year after release.,Negative
Intel,"I mean not really. The b580 is low end enough to the point where it'll become obsolete in performance long before it runs out of VRAM.  It's the equivalent of saying buying a 16gb 7600xt in 2025 is ""more futureproof"" than buying a 12gb 3060 in 2025.",Negative
Intel,an 8 GB card is obsolete at day one for modern games.,Negative
Intel,"It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup. You can find those numbers on Gamer's nexus latest shrinkflation video on this topic (https://www.youtube.com/watch?v=caU0RG0mNHg). Considering those things, the 5060 is essentially a 5050 and the 5050 is a 5030.  The GT 730 had 13.3% as many cores as the full GK110 die, with a 50W TDP.  The GT 1030 had around 10.3% as many cores as the full GP102 die, with a 30W TDP though this one was impressive.  The GXT 1630 had around 11% as many cores as full TU102 die, with exactly 75W TDP.  The RTX 5050 has 10.7% as many cores as the full GB202 die (RTX PRO 6000 Blackwell), and around 11.7% vs the 5090 (which is the number you'll se on Gamer's Nexus video, he rounds it up to 12%, I think that was a mistake on his part? I'm not gonna try and correct him though), but it has a TDP of 130W somehow, just 15W less than the 5060, that's absurd.  Hence why I'm saying it SHOULD be a sub 75W TDP card, and priced way below that. My theory is that it was OC'ed past its efficiency curve to make it at least moderately better than the 3060 and still be able to call it a XX50 card.",Neutral
Intel,Nah most people who buy prebuilts just want a good working PC with minimum effort,Neutral
Intel,Yep. That's a problem for that country. Countries with normal leadership have much more viable options,Negative
Intel,Price is only driven by demand and supply. See how the crazy expensive GPU still sell like a hot cake during the COVID lockdown period?,Neutral
Intel,It's well known by Arc owners.,Neutral
Intel,"The 16gb 7600xt  is a giant pile of crap even with 16 GB , so a 12 GB 3060 is the better choice.  If you go to techpowerup you'll see that the b580 is 17% in relative performance than the ""12 GB 3060 """,Negative
Intel,What if you’re playing at 720p?,Neutral
Intel,"> It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup.  That is a absolute dog shit methodology to use.   The 1080 Ti was 471 mm², 5090 is 750 mm².  Those two products are not the same tier. And can therefor not be used as equal reference points. When it comes to die size the top Pascal card is closer to the 5080 than 5090.",Negative
Intel,The point is more VRAM =/= aging better. The b580 having 12gb isn't saving it from becoming obsolete in a few years.,Negative
Intel,"They re not the same price tier either bro... They could have called the 5090 a 5095ti if that makes you happier.  The guy you quoted is right.   Also comparing the performance diff between the 1030 vs 1060 because guess what the 5060 should have been the 5050 looking at die size and memory bus.   Not sure why people keep defending these naming schemes, do you think the engineers use them internally lol? Its bullshit that marketing comes up with",Negative
Intel,"Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍 Doesn't matter if die sizes get bigger, that's up to Nvidia. Just because the halo product got bigger doesn't mean they can get away with moving their product stack (below the XX90) one tier up in prices.   I guess it is a dog shit methodology to use, only Gamer's Nexus uses it and I'm sure #1 Nvidia apologist u/Alive_Worth_2032 knows more than him.   It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree.  And it's a shit methodology to use if you're an Nvidia executive.  If you can't see that Nvidia is putting all the effort into binning high end chips for AI cause it's 90% of their income, then I don't know what to tell you. Obviously it's a good move for them, and it works, but there's 0 reason for a gaming customer to defend them for it. We used to get much more out of their chips in the gaming cards for more reasonable prices, why's it wrong to want that?   You can't just draw a conclusion based on relative performance between 2 underpowered products, you compare them to the ""best"" Nvidia COULD give us (which is the full top die for each generation) and go down from there.  Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm^2. That's not even an excuse.",Neutral
Intel,"> It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree. And it's a shit methodology to use if you're an Nvidia executive.  You can chose one methodology.   You can decide on comparing what hardware you get at a certain price point.   Or you can ignore price/bom and look only at arbitrary model numbers as if they mean something.  You cannot do both at the same time.  Personally I prefer to look at die area. The 5050 today is roughly comparable to the 1050 Ti, comparing it to the top cards that are not comparable is irrelevant.  Pascal did not have a analogue for the 5090, period.  >Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍  That changes nothing. The Pascal Titan are using the same die as the 1080 Ti. The whole tier of die that is used in today's consumer top SKUs and for the RTX 6000 Blackwell, DID NOT EXIST back then.  >Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm2. That's not even an excuse.  What are you even trying to say? Ampere also did not have a analogue to the 5090. The 3090 Ti, even it is sitting almost a tier below the 5090. The 2080 Ti and 5090 are unmatched in other generations.",Negative
Intel,"You’re still missing the point by focusing purely on die area and pretending model numbers are arbitrary. Model numbers mean something, nvidia knows that so that’s why they kept them consistent for so long (unlike AMD that's hella inconsistent). Because they rely on the perception of tier consistency from generation to generation, even if they’ve worsened the specs behind the scenes.  You say you “prefer to look at die area,” but that’s irrelevant unless you’re building the chips yourself. Customers don’t game on silicon real estate they game on actual performance and hardware capabilities. And the cuda core count + bandwidth vs top-die approach directly shows how much nvidia is offering relative to what they could offer if they weren't prioritizing AI margins.  So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  > Pascal did not have a 5090 analogue  That’s just semantics. Every generation has had a full-fat top die, whether branded as “Titan,” “RTX 6000,” or whatever, which where I drew the comparison. Comparing lower-tier cards to the top die is how you reveal how much of the architecture's potential is being offered to gamers.  And yes, 3090 and 3090 Ti still preserved proportionally higher specs vs top-tier dies. What changed now is not just die sizes, it’s NVIDIA reserving most of the silicon for AI and throwing scraps to gaming.  > You can choose one methodology...  Sure, and I chose one: compare the lowest-tier GPU to the top die, which accurately shows how nvidia has been shrinkflating consumer value over time. You're welcome to look at BOM and TDP too the 5050 still loses. It should be sub 75W and priced accordingly.  I think it's okay to call it an XX40 series card even if we keep the TDP at 75W but the XX30 series would need to disappear. What I don't agree with is using the die area excuse to justify shrinkflation just because the 5090 is such a halo product.",Neutral
Intel,"> So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  Do you even listen to your own madness?  You realize that what you are saying. Is that the lower end 5000 series would be ""better"". If Nvidia removed the 5090 entirely. And renamed the 5080 the 5080 Ti, and made the 5070 Ti into the new 5080.  Suddenly, you would praise the lower end cards for being ""better"". Purely because the high end is less powerful. Nothing else changed, no one is getting more hardware for their money.  Jesus Christ the mental gymnastics you people go trough.   The hardware Nvidia gives you for your money, is all that matters. How large the top die is and what core configuration it has. Is irrelevant for the value for cards further down the stack.",Neutral
Intel,End of Life is not the correct term for this. End of manufacturing is,Negative
Intel,Was about to shit on Intel for such a terrible product lifecycle time and how its GPU division was not going to do well if a GPU only has a ~2 years of updates until I read the article...,Negative
Intel,"End of life typically means end of support, not end of manufacture, or am I wrong?  Anyhoo I blame the article for bad wording",Negative
Intel,"> This announcement marks the beginning of the end for a model that arrived just two and a half years ago, and it offers partners a clear timetable for winding down orders and shipments. Customers should mark June 27, 2025, as their final opportunity to submit discontinuance orders for the Arc A750.",Neutral
Intel,"looks like Intel didnt fire enough of its incompetent staff. EOL usually means end of support/drivers/Developmemt.  [Intel® Arc™ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",Negative
Intel,"Why did anyone buy these garbage Arc cards to begin with? The performance/$ was never ever ever ever ever not even for a single second better than comparable Nvidia and AMD cards.  I've never understood the point of the Arc cards. It was like a company in 2020 saying ""we're officially into console gaming and just created the PlayStation 2 for only $399!""",Negative
Intel,But did it really even live to begin with? 🤔,Negative
Intel,is this the shortest life cycle of recent GPUs? AMD was notorious for that... wasn't expecting Intel to top that....,Negative
Intel,"So, for the distribution process this is also known as End-of-Sale. Since this is a B2C business, they can't really control when their resellers reach the end of their stock, but at that point Intel has stopped selling the product.  EoM and EoL are both dates that usually do not coincide with the End of Sales.",Neutral
Intel,"The article is poorly written, and the headline misleading.   The card is discontinued meaning no more orders will be accepted. It says nothing about software support.   Admittedly the miscommunication is Intel's fault because they specifically use EOL in their notification, but I also put this somewhat on the article writers because they didn't do a good job of clarifying that intel meant discontinued. They could have used more appropriate wording in the headline but instead chose to follow intel's lead likely knowing it would sow confusion, but lead to more clicks.",Negative
Intel,"Least likely reddit user behavior, actually reading the article probably puts you in the top 5 :)",Positive
Intel,"Don't worry, Iris Xe GPUs are still ""supported by the driver"", but their last fix was in 2023.  Intel doesn't notify when an architecture is actually dead, you're just left stranded for years until they make it finally official.",Neutral
Intel,"The incompetence source is actually intel themselves ...  [Intel® Arc™ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",Neutral
Intel,> Anyhoo I blame the article for bad wording   Intel chose the wording in its own notice.,Negative
Intel,"Bruh did you even read the article?  Btw it costs half of a 3060 and it outperforms it, no idea why you are saying it is a garbage product lmao shitty redditors don't know the difference between end of manufacturing and end of life/service.  The article is garbage, but it was talking about the end of manufacturing, not the end of the service/life, meaning it will still het driver support.",Negative
Intel,"No because they are still supporting it, just ending manufacturing of new cards.",Neutral
Intel,"For a website that's predominantly text-based, a shocking amount of its users can't read for shit",Negative
Intel,"The job of a journalist is to provide the translation and context for their readers, not copypasta and regurgitate headlines that laypeople will immediately misunderstand.",Neutral
Intel,That's just for the hardware manufacturing.,Neutral
Intel,"ah, i thought it meant end of driver support.",Neutral
Intel,We all did. unconventional use of EOL,Neutral
Intel,"Unfortunately it’s been like that for decades with Intel, I can count numerous times in the past decade we’ve had this same conversation about their processor lines being EOL.",Negative
Intel,End of Sale,Neutral
Intel,"Tbf arc pro battlemage gpus are supposedly going to be fairly cheap, so its not that out of reach for consumers",Neutral
Intel,"Imo, the lockdown of virtualization started before AI, as Nvidia and AMD didn't want people buying cheap GPUs to virtualize corporate environments.      At least that's the context that I'm most familiar with for this. A lot of small to midsize companies barely need the GPU for their individual VMs so being able to split a single GPU against a beefy CPU would be a good cost saver to avoid the professional GPU tax.",Neutral
Intel,"Intel has said that SR-IOV will come to consumer. Even if it doesn’t, their GPUs aren’t super expensive.",Neutral
Intel,"Splitting a GPU is so that you can run multiple VMs with each GPU. Any AI training and any inference on LLMs or similarly large models wants to run *one* application across many GPUs. There's nothing in common between those use cases. The market for GPU virtualization for remote desktop, etc. kind of stuff is still there, but it never was a big chunk of the datacenter GPU market and still isn't.",Neutral
Intel,"Yep, I have to use Intel for my virtualization server because it's impossible to get anything to work the way I want on more powerful AMD APUs. I can easily have a VM using part of the iGPU while the host shares it.  Also kind of sad that a pretty old Intel iGPU still has more video transcoding features in QuickSync like HDR tone mapping and better stability than modern APUs.    I don't think it has anything to do with AI it's been like this  forever.  AMD is nice consumer hardware that they just don't provide the tools to use in any advanced capacity.",Negative
Intel,"i thought nvidia grid got semi replaced by MIG. then again, MIG is only supported only on a specific subset of datacentre cards.",Neutral
Intel,"Hyper-V supports GPU partitioning without the need for SR-IOV, which is almost perfect on nvidia cards (at least for what I need). It doesn't support VRAM partitioning though, so every VM has full access to all of the VRAM. It also disables checkpoints for the VM (or at least says it does, apparently there's a way around this), and doesn't support dynamic RAM allocation.  In my limited use of other brands of GPU, it has some infrequent but major bugs with AMD IGPUs and frequent but minor bugs with Intel IGPUs (I've never tried a dedicated card from either brand with Hyper-V).",Neutral
Intel,"I think the future way around is to make the vm look like any other application from the gpu drivers point of view. On Linux (both host and guest for now) there exists virtio native, but for now it only works on Amd and is still in Beta and not yet really well documented how to get it working.",Neutral
Intel,"I know AMD GPU passthrough is supported by HyperV virtualization and LXC containers without MxGPU, but that might be Linux-specific.",Neutral
Intel,"AMD appears to be heading towards allowing SR-IOV on consumer Radeon soon, but we'll see if it actually happens.  https://www.phoronix.com/news/AMD-GIM-Open-Source",Neutral
Intel,"I'm a bit late to the conversation, but I have more faith in Virtio-GPU Native Context landing than whatever the big three are planning  But VirtioGPU NC is, at the moment, Linux specific.",Neutral
Intel,"I think the real question is availability and drivers. If it was like the initial Intel Arc GPUs launched in 1 laptop shop in S. Korea, good luck trying to expand market share.",Neutral
Intel,"Yeah, i'd reckon this mostly. Initially, more than a decade ago by now i guess, we were seeing this, with consumer grade GPU's being used or tried to power workloads on virtual desktops, RDS deployments and even research clusters.",Neutral
Intel,Surprised AMD just acted like Nvidia while Intel is the one actually doing this for so much GPU stuff.,Negative
Intel,The problem is they're not super available either,Negative
Intel,"PCIe Passthrough is not the same as GPU splitting. All 3 have support for PCIe Passthrough. I use it all of the time. The main goal of SR-IOV is to remove the need for multi-GPU setups in virtualization which from a market segmentation point would be detrimental to profits at the consumer level because a host may only need 1-4GB of VRAM and the rest can be allocated to the guests. This would be very impactful in ""Prosumer"" markets where someone may need more than 2 GPU outputs (typical for iGPUs) but may not want a multi-GPU setup as a multi-GPU setup has higher power draw.",Neutral
Intel,Intel Linux drivers are solid for everything except games,Positive
Intel,"Its an incredibly small market for this among consumers.  Most people asking for this is in a professional environment, and everyone wants to sell pro cards.",Neutral
Intel,TLDR:    Geomean: RX 9060 XT 16GB is 46% faster (mixed settings).     Raw performance closer to 30-40% faster.          Cheapest RX 9060 XT 16GB in EU is ~400 euros (VAT included)     Cheapest Arc B580 12GB in EU is ~294 euros (VAT included),Positive
Intel,Does the B580 still suffer from the massive CPU overhead or was is ever fixed?  Considering that noone reasonable will pair something like a 9800X3D with a B580 something like a 7600 or even 5600 or 5700X3D this is something a lot of people would have to keep in mind when it comes significantly worse on the kind of CPUs you would actually see in a budget build.,Negative
Intel,average 58fps 45fps 1% lows with 9800x3d system will be unplayable for most people with zen3 era speeds.   Turn off rt- use fsr/xess/dlss + fg for 90fps+,Negative
Intel,"Since 9060 xt is $390, cheapest 5060 Ti 16gb at $430 might as well be included in this calculation",Neutral
Intel,I hope B770 delivers at a price that old-heads like me can appreciate,Positive
Intel,I own both. The 9060xt is better use of money.,Positive
Intel,hot take   intel arc b580 is a better value then the 9060xt 16gb reason   your going entry level gpu   9060xt is a weird spot because if you can find at retail 9070 smashes it at 100 150$ more it makes it EXTREMELY HARD TO SUGGEST YES some people supposendly cant afford the extra 100 150 (although its very very likely not off the bad just a extra month of work or so) especially when your going entry you can argue fps is higher but my argument is 250$ for a 12gb gpu that competes properly against what its trying to compete against the 4060 while the 9060xt is a messy pricing and i also argue no one wants to give intel a realistic chance which is another issue if more people gave intel a chance i fondly believe it would be great for most people,Negative
Intel,"Question is:  They have a deal in few days where i can get BF6 free with intel GPU. It's a game i would have bought anyway so it's like getting a 90$CA off the GPU price    Currently, i can get a Arc B580 for 360$CA which would mean an effective price of 270$CA. A 9060 XT, the cheapest comes at 490$CA (but it's on sale right now so the real cheapest would be 540$CA).   At this theorical price, i guess i can't go wrong with the intel one right? I will pair it with a Ryzen 7 5800x probably.   I'm currently running a Ryzen 7 1800x and a GTX 1070ti and playing at 1440p and i really don't care about running 4K ultra. BF6 was working incredibly well at low on my Nvidia so if graphics are higher i can only be happy but again, dont need ultra and fancy graphics that make real world look dull.    What's your thought people?",Neutral
Intel,"Prices differ between EU countries, in Germany the cheapest B580 is currently 270€ and 369€ for the 9060 XT.  For the US it is 310$/370$ currently.",Neutral
Intel,"Yes, but as he says in the video, there are some major outliers in the results due to things like Vram going past 12GB, and certain games just not liking Arc GPUs. Looks like a more ""typical result"" is 30% to 40% faster when not going over 12GB VRAM and not in a game that Arc has issues with. It's always hard to sum up the difference with a percentage, but especially hard with Arc GPUs, upscaler differences, etc.",Neutral
Intel,"Australia pricing (AUD, GST incl.):   9060 XT 16GB: $629   B580 12GB: $449",Neutral
Intel,What's VAT?,Neutral
Intel,Never heard of any more news about this either,Negative
Intel,"Yes, but the conclusion was that even in this best case scenario for the B580 it offers worse value. So not looking good for the B580, especially when it's way above MSRP.",Negative
Intel,I don't think it can be fixed. The card seems to have been designed to utilize extra CPU resources as much as it can.,Negative
Intel,"have a b580 and 9800x3d combo, and am perfectly happy with it. it even does some 4k gaming stuff just fine.",Positive
Intel,"As long as the zen3 era CPU can hit 58gps average and 46fps 1% lows in whatever game you are referring to, then it would make no difference vs having the 9800x3d. You only see a difference if the CPU frame times take longer to compute than the GPU frame times.",Neutral
Intel,"Yeah, as someone trying to build a new PC for 1080p/1440p it makes no sense to go for the 9060xt since the 5060 Ti is only slightly more expensive and gets me the new DLSS, better ray tracing, etc. I guess for pure raster performance one could also include the 7800xt but it's not as ""future proof"" that's for sure.",Neutral
Intel,"Got a 3080 ti for 550$ yesterday, man that hits hard for value against 7900 xt (750$) and 5060 ti   A 5060 ti, 4070 here costs 600 used.",Neutral
Intel,260€ for the B580 in july...,Neutral
Intel,"Also, has Intel fixed the driver over head issue? People buying lower end gpus probably dont have top of the line CPUs to play with.",Negative
Intel,Value Added tax. Think a better version of sales tax.,Negative
Intel,"It's better then *any* 8 gig model, but that is the definition of damnation with faint praise.",Negative
Intel,"Hell no, the 9800x3d is doing a lot of heavy lifting with 1% lows and 0.1% lows. If you ran same tests with 3600-5700x. It wont be same experience at all especially on B580",Negative
Intel,dlss is only dealbreaker if they're extremely similarly priced otherwise I'm not really sold on the difference just for the features    especially where i live,Negative
Intel,"I don't consider a 25% price difference to be ""slightly"" more expensive.",Negative
Intel,Also hits hard on the power draw,Neutral
Intel,Is it fully a overhead issue or were tests done on CPUs that predate RE-BAR support? It looks like Arc is basically have RE-BAR and it performs close to expectation or its in the dumpster without it,Neutral
Intel,Oh,Neutral
Intel,"debatable. 4060/5060, 9600xt on 3600x-5600x are faster than b580 on similar stepup",Neutral
Intel,"I'd rather have a 5060 ti 8GB than a B580 if you offered me them for the same price. It'd be way faster as long as I tune my settings. That being said, id never buy the 5060 ti 8GB.",Neutral
Intel,I would rather take the 5060 and play with lower textures than the b580 and be cpu bottlenecked with driver issues and no DLSS.,Negative
Intel,"You still aren't saying which test you are referring to, so my answer has to say general rather than specific, but you seem to not fully understand how this works. If the CPU computes the frame faster than the GPU, then an even faster CPU will not improve the frame time. If in this particular test a lower end CPU could not compute the frame faster than the GPU, then the faster CPU is helping. But a faster CPU does not always make a difference. It only matters if the weaker one is actually limiting the performance, which is often not the case.",Negative
Intel,"HUB tested very thoroughly and even did a follow up video about it because some russian tech site called HUB liars. TLDW of it is, yes rebar enabled, any game with decent CPU usage seriously hampered GPU performance, not all games but enough that it is something that anybody reviewing an Intel GPU should test for and advise the viewer about.",Negative
Intel,"Running a B580 with a 5700X3D, rebar enabled, tweaking a few settings, and it runs well for what I need it to do.  Basically iRacing and other sim racing titles.",Positive
Intel,"Nah theres definitely a drop if you pair 9800x3d with b580 and 5600 & other gpus as well. You'll see lower average & 1% and 0.1% lows.  Setups with b580, 4060s, 9600xt could see averages of: 52fps 40fps 1% lows. Just turn off the rt, optimize and run fg for \~100fps",Negative
Intel,"That is not true as a blanket statement. It is situationally true in the cases where the lower end CPU  can't keep up with the GPU frame times, but it is not true when it can.",Neutral
Intel,... its disingenuous to only test with 9800x3d on b580. Average person wont get those perf. Adequate step-up for me would've been a midrange cpu,Negative
Intel,The conclusion was that the b580 is still worse performance and value even in this best case scenario,Negative
Intel,Intel isnt serious about dgpu anyways. arc still has less than 0.15% marketshare on steam and the competitors are thriving...well only Nvidia. B580 probably 0.04%,Negative
Intel,"I will say that Intel's naming is incredible. Now we have *Battlematrix*, what a badass name. Can we, as a society, *please* stop pretending that the rule of cool is ""cringe""?",Positive
Intel,700$ for the B60 is pretty cheap,Neutral
Intel,"Intel badly needs a presence in enterprise or the data center, so these Pro Arc GPUs are to be expected.  Hopefully these cards can act as a lifeline for the struggling Arc division, but I hope that data center revenue doesn’t compel Intel to abandon the consumer segment altogether.",Neutral
Intel,The passively cooled one looks interesting.,Positive
Intel,"Original title: THIS is the Most Important GPU of 2025  Genuinely could be exciting stuff, both in terms of AI/pro use cases, and in terms of giving Intel's discrete GPU arm more sales / runway.  Also includes commitments from Intel towards the gaming side of GPUs, so all in all pretty good news, if this doesn't bomb.",Positive
Intel,Interesting video downvoted to oblivion  GN spam upvoted endlessly  Make it make sense...,Neutral
Intel,It’s nice that Intel will let people run the gaming drivers on these instead of artificially segmenting the consumer and professional products. Could see it being a cheap 24GB people’s champion once games start requiring obscene amounts of VRAM and companies offload their old stuff.  I want to see how the 70w model compares to the 6GB 3050.,Positive
Intel,The problem is it is a bit slow even compared with 5070 sigh,Negative
Intel,"But think of all those nVidia cards that could have been sold to gamers instead being sold for AI...  And think about what will happen when there is a $500 24GB AI capable board the AI guys can buy instead!   Or a dual GPU 48GB!  There are many AI cases where the 48GB card is going to trounce any offering from nVidia, and every card Intel sells to AI folk is one more nVidia card available to gamers.   So whether or not you are personally interested in buying a B60, it IS going to be a crazy important card to gamers because this is going to permanently shift the supply/demand for nVidia cards as well as force nVidia to start increasing vram to compete.  Bonus, because Intel has their own foundries, the upcoming celestial line will not be competing with nVidia for foundry time, so this will also increase the total number of video cards produced!",Neutral
Intel,"This is legit, I can see a lot of companies adopting this to use for local AI. Now if only Ollama IPEX can update their support and not lag behind for Intel ARC graphics, compared to the regular build for AMD and Nvidia.",Positive
Intel,I will be watching out for B50 though.,Neutral
Intel,"Great troll clickbait title tbh, the same day that 5060 hits shelves. People will of course think this is a 5060 review and LTT knelt the knee on Nvidia if they don't watch the video.",Negative
Intel,TL:DR ITS ALL DATACENTER GAMERS MOVE ALONG.,Neutral
Intel,"No, we gotta have AI Pro Max Ultra+   /s",Neutral
Intel,I still feel their architectures called after D&D things is so cool.,Positive
Intel,I'm not a fan of *Sparkle*,Negative
Intel,"What data center revenue, they have pretty much none. Their roadmap also slid, they won't have anything competitive for data center till 2027.",Negative
Intel,"Why is Arc 'struggling'? Every release so far, going back to the first one, have sold out very quickly. B580s haven't been in retailer stock much at all since it came out, until the new AMD GPUs that recently released. But, whether it's scalpers or consumers, Arc never stays in stock for long.",Negative
Intel,"I mean I think that's what the dGPU for Intel is changing for Xe3P and onwards; Intel's fabs producing these GPUs which make money with AI/Workstation. I mean the original Xe produced on TSMC under Raja is certainly gone but feels like they've re-calibated with Xe3P quite a bit.  Honestly if Xe3P performs \*very\* well in labs I'd do a N44>N48 style die upscaling of a probable 256-bit part they're working on and just double the spec of it. If it still struggles in gaming Vs RTX 60 & UDNA/RDNA5/GFX14 PPA wise then they can just reuse said dies for AI easily. Especially could fill in the gap left by Falcon Shores to an extent.  An ""affordable"" 96GB Clamshell card for AI/Professional Vs Nvidia sounds pretty tempting.",Neutral
Intel,"> Original title: THIS is the Most Important GPU of 2025  seeing a title like that would instantly get me to click ""not interested"" in youtube.",Negative
Intel,It's Reddit. How often does it really make sense? XD,Neutral
Intel,I NEED to see this in gaming xD  I don't think gamers should be walled off from having a GPU that can do ai stuff and gaming as well. The GPU should be able to do both IMO,Positive
Intel,No one will actually think that 5060 is important,Negative
Intel,No they won't?,Neutral
Intel,"not all datacenter, but prosumers and businesses that need to leverage stuff for workstations also have a space here. plus, while nowhere near as performant as a full fledged gaming GPU, i bet the B60 with gaming drivers can be a respectable gaming card for those off work hours gaming sessions.",Neutral
Intel,"It was clickbait, I editorialized the best I could. Although I do kind of buy their reasoning for why it _could_ be a very significant GPU overall - if it lands well.",Neutral
Intel,Where's my Chaotic Neutral Rogue GPU Intel?  WHERE!?,Negative
Intel,Sparkle is an AIB who makes GPU boards. Intel didn't pick that name.,Neutral
Intel,Their first generation had some issues and was not great even for the low price but in the past year they have been killing it in the budget card space. Its really nice to have some options on the lower end again.,Positive
Intel,"VR development could be an interesting topic here, assuming one GPU die can not access the other GPU's VRAM, it could still be a decent option for development of VR games on the internal testing front. VR is very much staying, along with AI so i can see this being a nice tool for both, just not all at once.",Positive
Intel,you underestimate human stupidity and overestimate their ability to read.,Negative
Intel,Youll have to wait for R generation for the Rogue. Alignment optional.,Neutral
Intel,Chaotic Neural Rogue.,Neutral
Intel,"I see, ya they do have cool codenames. Also TIL Intel Arc GPUs are named after character classes from D&D. Alchemist and Battlemage for example, no idea where Battlematrix comes from though",Positive
Intel,"$299 and I can do Forza high settings 120fps at 1440p? Sold. ...don't care about RT, I turn it off anyway.",Negative
Intel,I'm just trying to find an affordable card that will run 4k vr at 60 to 90 fps without any dlss; so many cards don't have enough vram for vr. Excited to see if the B60 will be able to do this.,Positive
Intel,below $1000 USD? Sounds amazing value for those that want 48GB of VRAM and BMG okay for that kind of stuff?   One way to recuperate any losses on B580s in the dGPU division.,Positive
Intel,"sure, whatever. welcome back sli  edit: I KNOW ITS NOT SLI",Positive
Intel,i wonder how well will this perform for LLMs,Neutral
Intel,"Nice to finally see BMG X2 in the wild. We need more weird GPUs like this, now more than ever.",Positive
Intel,"I'm in, I would buy these.  Bring back big cases with big stacks of GPUs.  Let's do this.",Positive
Intel,"The shroud design oddly reminds me of 9800GX2.   But of course, the GX2 had a... 'sandwich' form factor with dual PCBs!  The late 2000s were such a great time to be a nerdy teenager. Technology was still trying to find its footing and everyone seemed to be experimenting with different ideas. We had weird smartphones, weird GPUs (with CGI mascots), and even weirder CPU coolers (Thermaltake SpinQ, Cooler Master Mars/Eclipse, anyone?).  Everything just feels *too* mainstream and 'serious' nowadays... but I digress.",Positive
Intel,Am I dreaming a dual GPU in 2025.,Neutral
Intel,he's wearing the shirt to computex lmao,Neutral
Intel,"From the article I read, it does the processing work of handing out the data from both gpus in the video card itself.   Kind of cool if it's fast enough.",Positive
Intel,Only 2 words  Fuck Nvidia!,Negative
Intel,Can you buy a single one or do you have to buy it in Battlematrix form like the other B60?,Neutral
Intel,"Finally something for modern day workstations. If you are working on a desktop these days, the most important thing is access to AI models.   With these, you can easily buy 2-4 B60s and throw them in your machine to not be reliant on external services all the time. Could be a real productivity booster.",Positive
Intel,Is that Richard Stallman,Neutral
Intel,This would be perfect for lossless scaling multi gpu framegen.,Positive
Intel,Steve trying a mandarin ad. Cute I say...,Positive
Intel,"This is awesome, but... are we ever going to get a B770?",Positive
Intel,Dual gpu. 2012 amd...is that you?,Neutral
Intel,"Hey, dumb question: can a motherboard with 4 pcie slot runs 4 of this card?  That would make 48\*4=192gb, pretty doable for really large langu model.",Neutral
Intel,I remember when he said that Intel were scumbags  did he change his mind?,Neutral
Intel,Don’t get fooled by 48GB. The card actually behaves as 2 cards of 24GB with pcie x8 lanes each of gen 5.0 in bifurcation mode. So for gaming most likely you will be getting only 24GB of vram (which is still not bad). But this card really shines in AI or other professional loads because they are claiming that you can have one big shared memory across multiple cards with their drivers and without any physical connectors like crossfire or sli. This means you can run full fat deepseek completely locally on your physical system with something like 4 of B60 on a threadripper system. God damm never even imagined that intel would be shining in GPU segment.,Neutral
Intel,this technology is from ASUS Mars II 3 GB Dual GTX 580 maybe from 14 years ago?,Neutral
Intel,they wont sell these discretely [source](https://www.youtube.com/watch?v=F_Oq5NTR6Sk),Neutral
Intel,no software support + half of the memory bandwidth of the 5 year old 3090,Negative
Intel,"Sli never left... Nvidia just hide it away from customers and sell it as nvlink (with some upgrades) for high end component for commercial side because they didn't want people stacking their cheaper cards.    Edit: got it, seems like I misunderstood what Sli does with the vram as it didn't pool.",Neutral
Intel,Unfortunately this is just a way to get more GPUs into one system. There’s nothing like SLI.,Negative
Intel,No need for SLI to use multiple GPUs.,Neutral
Intel,"no this particular card does not support sli. this is just to increase the gpu density in a given system. Increased from 1 gpu per slot to 2 gpus per slot.   According to linux tech tips, the memory sharing between cards are done on the software level, which in my opinion is not a good thing. the pci-e gen5 x8 bus has a memory bandwith of 32GB/s in one direction (latest nvlink support up to 900GB in one direction). More inter-card bandwidth = better performance on large models (assuming the model support multi-gpu setup). No dedicated inter-card interface certainly will harm the maximum theoratical performance in certain workloads.",Neutral
Intel,"Me too. Based on the TOPS, the inference speed seems to be roughly comparably to a 3060TI for the B60 and the Dual being roughly comparable to a 4080, though i think realistically more like a 3080/4060 due to the bandwiths. I've run a good bit of local inference on a 3060 12GB, and while it isn't hyper-performant, the speed is slower but ""acceptably slow"" in my opinion. You won't be waiting 10 seconds per token, but it's also not going to be spitting out a novel per minute. For me, that extra memory is really what matters because it mean less quantization at roughly the same speeds I'm used to, which means less errors in the output. I'm really more hopeful that this will finally light a fire under the NVIDIA execs who thought punishing consumers with less memory to prevent corporate customers/datacenters from using consumer cards in their servers was the right move. You can see they are reversing that somewhat with the 5090, but they can kick rocks with that 3k\* price tag that is once-again intended to get more money from data centers who decide to try to use them. It's pure market manipulation and artificial deflation of the specs to drive pro customers to more expensive hardware at consumer's burden.",Neutral
Intel,"Tokens/second won't be as good as a high end Nvidia card obviously, but you'll be able to fit a much larger model onto it than you could for any consumer and almost any Nvidia card. And for $1000, you could run four of these and still come out way cheaper than one A6000.",Positive
Intel,"Since I've run a bunch of tests on Xe2 (and of course plenty of Nvidia and AMD chips):  * A 70B Q4 dense model is about 40GB. w/ f16 kvcache, You should expect to fit 16-20K of context (depends on tokenizer, overhead etc) w/ 48GB of VRAM. * B60 has 456GB/s of MBW. At 80% (this would be excellent) MBW efficiency, you'd expect a maximum of 9 tok/s for token generation (a little less than 7 words/s. Avg reading speed is 5 words/s, just as a point of reference most models from commercial providers output at 100 tok/s+ * For processing, based on CU count each B60 die should have about ~~30~~ 100 FP16 TFLOPS (double FP8/INT8) but it's tough to say exactly how it'd perform for inference (for layer splitting you usually don't get a benefit - you could do tensor spliting, but you might lose perf if you hit bus bottlenecks). I wouldn't bet on it processing a 70B model faster than 200 tok/s though (fine for short context, but slower as it gets longer.  Like for Strix Halo, I think it'd do best for MoE's but there's not much at the 30GB or so size (if you have 2X, I'd Llama 4 Scout Q4 (58GB) might be interesting once there are better tuned versions.",Neutral
Intel,"Reminds me of the old days when brands would try weird stuff to see what sticks, or just for the market data/rnd.  Kudos for Intel for trying something a bit different.",Positive
Intel,Thankfully not,Neutral
Intel,"yes, they are even showing servers with 4 of those [https://geeksynk.com/intel-rises-all-upcoming-intel-high-vram-gpus-with-their-specs/](https://geeksynk.com/intel-rises-all-upcoming-intel-high-vram-gpus-with-their-specs/)",Neutral
Intel,“24gbs of VRAM is not bad for gaming”   It’s the bare minimum these days,Neutral
Intel,If it has Vulkan it can be used for inference,Neutral
Intel,It has all the software stack it needs and works great.,Positive
Intel,Good luck fitting 48GB of anything into a single 3090 VRAM.,Negative
Intel,OpenVINO is fully supported on Arc discrete graphics and supports both Windows and Linux.,Positive
Intel,"Man you don't know what are even talking about, these GPUs are one 192 bits per unit combined you will get 384 bit through 2 GPUs and the 3090 well it's half the amount of VRAM and it is powerful but in workstation use it is not the only thing you need, and software support, there is not support for 50 Series cards on many applications like premiere, Da Vinci and most of the work based applications and the Game ready drivers are more than capable enough to run LLMs on Windows and Linux",Negative
Intel,"Tbh that is probably for the best. Sli was never that great for gaming, all that leaving sli open as an option would bring is greater AI demand for consumer cards.",Negative
Intel,"SLI, as implemented in the later years of its existence, has the same latency cost as framegen, with worse variation.  With how much rage there is about framgen... can you imagine how the public would respond if they had to buy two GPUs to get it?",Negative
Intel,">because they didn't want people stacking their cheaper cards.  because they’re evil and greedy. No other reason, right? 🙄",Negative
Intel,starting to find out why people use /s on this website,Negative
Intel,"According to linus tech tips, the memory sharing between cards are done on the software level. The pci-e gen5 x8 bus has a memory bandwith of 32GB/s in one direction (latest nvlink support up to 900GB in one direction). No dedicated inter-card interface certainly will harm the maximum theoratical performance in certain workloads.  Also, according to linus the purpose of this 2 gpus on 1 board is just to increase the server density, and nothing to do with bandwith. Each gpu is connected to host via pci-e 5.0 x8 interface, and the system will seperate them via pci-e bifurcation.",Neutral
Intel,"Pretty disappointing tbh. This card will not be a nvidia replacement by anytime soon. I think the cheapest option to run LLM locally is to buy decomissioned nvidia tesla v100 16G SXM2. Each cost around 60-70 USD and availability is pretty good atleast in China. And pair with SMX2 to pci-e conversion board.  6 of them can be linked together using nvlink, providing 96GB vram. The downside though, is this setup will drain 1.8kw of power when maxed. And the HBM2 vram inside the core is very prune to failure due to old age. The embedded vram can't be repaired, so once vram died whole gpu is pretty much gone.",Negative
Intel,between AMD and Intel which is more stable?,Neutral
Intel,"Intel's official figure is 192 INT8 TOPS. I guess this is with sparsity, so 96. Then FP16 should be 48 TFLOPS (or 4x the FP32 perf).   So essentially, a 3060 with 24GB VRAM and 25% higher bandwidth (conveniently available in a dual-gpu version for a 48GB total).",Neutral
Intel,Thanks!!!,Positive
Intel,That’s true and product segment itself focuses on AI. But the fun part is bifurcated dual GPUs. And unlike Nvidia they are claiming to have support for both game-ready and professional drivers at same time. This means you can be professional video editor by day and gamer by night (or gamer by weekends or vice-versa 😄) all with one card without tinkering drivers or monitor ports. And by the way unlike AMD they have great support for video encoding and decoding. I am really excited to see all these various combinations of things on this card.,Positive
Intel,Yeah 24GB has become bare specially for 4K and above. This is good intel has hit a good sweet spot each GPU. 😗,Positive
Intel,source??,Neutral
Intel,Sure but there’s no Vulkan needed.,Neutral
Intel,"this isn't a single card. it's two mid cards on one PCB, you are still splitting vram across two GPU's",Neutral
Intel,"It's an option...? Meaning that you don't have to use it if you don't like it.      And plenty of people use their commercial cards for gaming, literally some of earliest adaptors of nvidia cards were chief tech leads playing games on their workstations in afterhours, as described in many interviews and books such as the recent The Nvidia Way.   And Sli was fine once they made G-Sync, which was from the discovery of microstutters as a big problem.",Neutral
Intel,"Yes, that's why I said:    >...sell it as nvlink (with some upgrades)...     Also, I bet that most people would gladly take just the standard sli without the nvlink improvements simply for the larger vram.    Edit: I was wrong, see below. Cunningham's Law",Neutral
Intel,"The question is less about stability and more about support.  AMD's ROCm support is basically on a per-chip basis. If you have gfx1100 (navi31) on Linux you're basically have good (not perfect) support and most things work (especially over the past year - bitsandbytes, AOTriton, even CK now works. I'd say for AI/ML (beyond inferencing) I'd almost certainly pick AMD over Intel w/ gfx1100 for the stuff I do. If you're using any other AMD consumer hardware, especially on the APUs then you're in for a wild ride. I am poking around with Strix Halo atm and the pain is real. Most of the work that's been done for PyTorch enablement is by two community members.  Personally I've been really impressed by Intel's IPEX-LLM team. They're super responsive and when I ran into a bug, they fixed it over the weekend and had it in their next weekly release. That being said, while their velocity is awesome, that causes a lot of bitrot/turnover in the code. The stuff I've touched that hasn't been updated in a year usually tends to be broken. Also, while there is Vulkan/SYCL backends in llama.cpp that work with Arc, you will by far get the best performance from the IPEX-LLM backend, which is forked from mainline (so therefore always behind on features/model support). IMO it'd be a big win if they could figure out how to get the IPEX backend upstreamed.  I think the real question you should ask is what price point and hardware class are you looking for and what kind of support do you need (if you just need llama.cpp to run, then either is fine, tbt).",Neutral
Intel,"Hmm re-reading, I may have brain-farted the CU math, Arc 140V (Lunar Lake) is I believe 32 TFLOPS so obvs G21 should be higher.  B60 ([official specs](https://www.intel.com/content/www/us/en/products/sku/243916/intel-arc-pro-b60-graphics/specifications.html)) uses the full BGM-G21 which has 20 Xe2 cores, 160 XMX engines and a graphics clock of 2.4GHz (a bit lower than B580).  Each Xe2 core can support 2048 FP16 ops/clock ([Intel Xe2 PDF](https://cdrdv2-public.intel.com/824434/2024_Intel_Tech%20Tour%20TW_Xe2%20and%20Lunar%20Lakes%20GPU.pdf)).  ``` 20 CU * 2048 FP16 ops/clock/CU * 2.4e9 clock / 1e12 = 98.304 FP16 TFLOPS ```  This lines up if Intel is claiming 192 INT8 TOPS (afaik XMX doesn't do sparsity and they claim 4096 INT8 ops/clock, so double FP16/BF16).  These cards seem super cool! My main bone to pick is that the retail plans (uncertain retail release in Q4) makes it less interesting. I guess we'll see what else hits the shelves between now and then.",Neutral
Intel,But you get 48GB of VRAM for that price.,Neutral
Intel,"Intel has a currently working but not released software to share video memory across multiple GPUs for the purpose of AI and such with minimal performance loss. It is effectively 48GB worth of video memory for the tasks these GPUs are designed for.  These cards might be slower than other solutions, but they will be thousands or even tens of thousands of dollars cheaper. Stuff like this (and AMD's AI MAX+ 395) allow companies to have each dev be able to host their own full size(or at least near full size) AI model on their own workstation instead of fighting over time on the big server. They also allow hobbyist to work with much larger models than they usually would be able to.",Neutral
Intel,"The nvlink improvements are what *allow* for larger vram.  SLI's biggest problem was that GPU memory had to be duplicated across all cards, meaning if you put 2x 8gb cards in SLI, you still only had 8GB effective vram. NVLink allows for memory pooling, SLI did not.",Neutral
Intel,thank you,Positive
Intel,"If they really have 98 FP16 TFlops (i.e., 70% of a 3090), they will be pretty cool and better value than a heavily used 3090 (if we ignore the CUDA advantage)",Positive
Intel,with the memory bandwidth of a 5060... too slow for dense models and not enough memory for large MoE models,Negative
Intel,"Unless they have some custom interconnect, it will be slow (PCIe 5.0 x8 tops out at 32GB/s)",Negative
Intel,"Thx, TIL",Positive
Intel,"Faster tham RAM, which is what you'd need to use with single 3090 when you go above 24GB.",Neutral
Intel,"What the hell, even a B580 has 192 Bit bus compared to 5060's 128 bit bus, and when it is combined you are getting aboout 384 bit bus accross the 2 GPUs and they are not pooled but you won't get that much VRAM even on used markets",Negative
Intel,That’s about the bandwidth of the Nvlink bridge on a 2080(/ti) (which obviously doesn’t do vram sharing),Neutral
Intel,Then just get Strix Halo if you don’t care about bandwidth and performance,Negative
Intel,"Seems like I have to go through this again, but with you this time. So, again. For the price of Strix Halo, if you actually use the memory, you are going to get much higher performance than a system with better GPU but less VRAM, because you'd run out of it and had to go for RAM. Do you guys just not understand that if you are going to use the VRAM, it will have much better performance than something that would run out of it and had to use RAM for the same price?",Negative
Intel,"Disappointing that we're not seeing the B770 in computex. Guess we'll have to ""stay tuned"" until intel decides to release or scrap the B770  The Arc Pro B50 is a B570 with 16gb of VRAM on a 128bit bus. It has a $299 msrp  But we DO get the Arc Pro B60 which has a single 24gb GPU model and a dual GPU B580 which has 48gb of VRAM shared between both GPU's   Intel designed it's software so that you can scale up workloads across multiple Battlemage GPU's up to 8. Intel calls this Battlematrix.  A fully equipped Battlematrix setup would have across 8 GPU's or 4 of the dual gpu B60 cards: 192gb of VRAM, 1280 XMX Engines and 160 Xe cores. This setup comes with what I assume is a Granite Rapids Xeon CPU and MSRP range from $5000-$10000. It includes a customized linux distro with software and frameworks optimized for AI workloads.  Unfortunately, Battlematrix workstations are the ONLY way to get an Arc Pro B60 as they're not selling them as individual cards yet.   EDIT: You can actually buy the Arc Pro B60 individually in Q1 2026.",Neutral
Intel,I’m just happy Intel is still thinking about GPUs at this point.,Positive
Intel,"How likely are these to support SR-IOV?  I want to buy one for GPU Virtualization. I am willing to go through my work to buy one, I don't care if they're not going to be available at retail. Hell I want the 48GB model GN just tore down.",Neutral
Intel,Do I still need to stay tuned for a possible B770?,Neutral
Intel,"A dual GPU B580 with 48GB of VRAM? That's a spicy meatball, especially if the price is low. IIRC a B580 12GB is something like only $399 CAD or was $249 USD at launch.  That's the kind of cooking-with-gas product that would have the AI open source dev crowd marching to a blue tune. Especially if you could get a handful of them for the same price as a 5090 32GB card and run them together.",Neutral
Intel,"Is doing 'AI' actually simple on this hardware though? as in wide compatibility, no headaches trying to make everything work like it just does on Nvidia etc.  I have no clue, not trying to sealion. I just know that for a long time actually doing certain tasks on non-Nvidia hardware involves a lot of extra steps just to make things work.",Neutral
Intel,MFW When Intel GPU Department is dishing out good stuff and CPU department is kina meh.,Negative
Intel,"Nothing on the B770 after saying ""stay tuned"" when asked about it? Seriously?",Negative
Intel,"So much for ""UNDERPROMISE OVERDELIVER"". They all but said to expect B770 and nothing...",Negative
Intel,Are the ARC GPUs any good or not. I've seen some saying they're crap while others say they give the most bang for your buck. So which is it?,Negative
Intel,"The B60 has 24 GB of VRAM for a $500 MSRP (but I imagine it will be difficult to find at that price/single GPU). The memory bandwidth (which will likely be the inference limitation) is 456 GB/s.  For inference, its closest Nvidia competitor is the $550 MSRP 5070, which has just 12 GB of VRAM but bandwidth of 672 GB/s. So if you're running an 8B model (which honestly I can only recommend for small tasks like tab code assist, not image generation) it is better to get the 5070. But if you're running a slightly larger model like Deepseek R1 14B at Q8, you will only be able to run it with B60 or something significantly more expensive, like a 3090.  https://apxml.com/tools/vram-calculator",Neutral
Intel,"You gotta love competition, Nvidia would never do something like this, even though they say they want to democratize AI.  If they really wanted to boost AI research, you flood the world with affordable VRAM so everyone can run big models locally.",Negative
Intel,It wouldn't surprise if the reason they aren't selling the GPUs with Battlematrix ala carte initially is that the code hasn't been mainlined into the Linux kernel yet.,Neutral
Intel,Depending on Price and Software Support I might actually grab myself the 24 or 48gb VRAM Variant for AI workloads. If the gaming performance is improved I might be able to bench my 6700XT.,Positive
Intel,Hey I want to say good on you for updating every comment you’ve made with the new information; it was a pretty upstanding thing to do.,Positive
Intel,"> Disappointing that we're not seeing the B770 in computex. Guess we'll have to ""stay tuned"" until intel decides to release or scrap the B770 >  >   That was never going to happen before RTX 5060.",Negative
Intel,"they probably know that they can't just start making dGPUs and immediately get half the market, but that it's a long battle that they gotta continue for quite a while",Neutral
Intel,"From Chips and Cheese's video, it should be comming in Q4  [https://youtu.be/F\_Oq5NTR6Sk?si=A6cXIrKbRquAsNEm&t=280](https://youtu.be/F_Oq5NTR6Sk?si=A6cXIrKbRquAsNEm&t=280)",Neutral
Intel,They are guaranteed to be support SR IOV by Q4,Neutral
Intel,Yes https://x.com/tekwendell/status/1924417394115858735,Neutral
Intel,"You can't buy the Arc Pro B60 by itself. Instead, Intel is bundling up to 4 of them with a xeon cpu as part of a ""Battlematrix"" workstation.   Intel must be desperate to move unsold Granite Rapids inventory  Each workstation will cost $5000-$10000 with a custom linux distro tailored to running ai workloads  Edit: ypu will be able to buy the Arc Pro B60 as an individual card in Q1 2026",Negative
Intel,"I guess so. Intel did throw gamers a bone, though, with the Arc Pro b50 with 16gb Vram with a 128bit bus using the cut down b570 die (16 Xe cores)",Neutral
Intel,"You can't buy the Arc Pro B60 by itself. Instead, Intel is bundling up to 4 of them with a xeon workstation cpu as part of a ""Battlematrix"" workstation.  *sigh* Intel pulling an Intel again. Intel must be desperate to unload Granite Rapids inventory that's sitting in warehouses  EDIT:  intel is going to sell the Arc Pro B60 separately in Q1 2026",Negative
Intel,%100,Neutral
Intel,"According to a news article: ""Intel is working to deliver a validated full-stack containerized Linux solution that includes everything needed to deploy a system, including drivers, libraries, tools, and frameworks, that's all performance optimized, allowing customers to hit the ground running with a simple install process. Intel will roll out the new containers in phases as its initiative matures. """,Positive
Intel,"I own an A770, and the answer is absolutely not. I bought it with the expectation that the experience would be much worse than nvidia/amd, and they blew my expectations out of the water.  My favorite example is that, until [this release 3 weeks ago](https://github.com/intel/intel-extension-for-pytorch/releases/tag/v2.7.10%2Bxpu) ([the fix was merged ~march 3](https://github.com/intel/intel-extension-for-pytorch/issues/325#issuecomment-2694277026)), using large image generation models on Alchemist cards wasn't possible because they have a 4GB VRAM allocation limit (fixed in Battlemage). Now it mostly ""just works"", but a lot of optimizations are nvidia-only.  LLM support isn't quite as awful as image/video generation was prior to the aforementioned release, but it's still substantially worse than nvidia/amd (I also own a 6900xt). For instance, none of the inferencing tools support Flash Attention on Intel's compute backend (a [SYCL](https://en.wikipedia.org/wiki/SYCL) implementation), which _severely_ limits context length / initial generation speed because memory usage scales quadratically with context size.  Intel maintains a [repository](https://github.com/intel/ipex-llm) with forks of most of the popular tools (vLLM, llama.cpp, Ollama, etc). It vastly improves performance when using intel's backend relative to the upstreams, but it's also extremely annoying to use. It's buggier, releases lag behind upstream, there's less model support, it requires a hyper-specific environment setup, etc. I have no idea why they don't just upstream their changes.  At this rate, Vulkan will end up with better support and performance than SYCL, despite the former not being designed with compute workloads in mind at all, while the latter was explicitly built for it (I think).",Negative
Intel,"Been researching what my options are outside of NVIDIA, but it honestly seems Intel cards may be a more viable solution with IPEX than AMD. There are reddit posts floating around with the B580 showing comparable inference performance to a 4060ti for text and image generation.  Getting 3-4it/s on SDXL models would be a huge improvement to my 4-5sec/it with my 1060 ti bottlenecked by RAM.",Positive
Intel,On a new budget computer build they're really good but there's issues putting them in older systems as upgrades.,Positive
Intel,Decently competitive. They do have driver quirks and last I heard they also had a CPU overhead issue.,Neutral
Intel,For gaming the B580 seems like very good value if you're prepared to encounter a few scenarios where it falls behind. In general it seems to offer a decent bit more than AMD and Nvidia at the same price point.,Positive
Intel,">For inference, its closest Nvidia competitor is the $550 MSRP 5070, which has just 12 GB of VRAM but bandwidth of 672 GB/s.  No, the closest NV competitor is the $429 5060Ti 16GB with 448GB/s bandwidth.",Neutral
Intel,I just realized that you can actually put 4 dual GPU b60's in a large case to get 8 gpu's into 1 pc,Neutral
Intel,"You can’t, they’re not sold individually and are workstation only.",Neutral
Intel,"Why? If they plan to release it at all, they should do it before the 5060, not after.",Neutral
Intel,This makes me very erect.,Neutral
Intel,They better make these available.  I have been waiting since 2018/2019 for SR-IOV to become available on GPUs ever since I tried it with enterprise NICs.,Positive
Intel,"128 bit on Intel is entry level. Around A580/RX 6600 (which was $190 like 3 years ago). I don't think it makes sense for gaming.  The extra VRAM is nice, but it's too slow to be worth even 199 for gaming, let alone the MSRP of 299. Might make sense for video editing or AI or smth.",Negative
Intel,"Yeah which seems to translate as no, not right now.",Neutral
Intel,"B580 is probably better, but I get closer to <2 it/s on my A770. There might also be something wrong with my setup, given how jank the whole thing is though.",Negative
Intel,And 192 gigs of vram. Thats enough to run almost all of the open source language models out right now. I wonder how that kind of setup would stack against an Nvidia setup.,Neutral
Intel,Proper posture is important.,Positive
Intel,"If you want this setup, then you'll have to buy a ""battlematrix"" workstation with what I assume is a Granite Rapids Xeon cpu.  Guess they're really trying to get move unsold Granite Rapids inventory  as you can't buy the Arc Pro b60 on it's own.  Each workstation will cost $5000-$10000 with a custom linux distro tailored to running ai workloads  Edit: You will be able to buy the Arc Pro B60 as an individual card in Q1 2026",Neutral
Intel,It's definitely a posture alright.,Neutral
Intel,"Wow, what...  I need one of those.  I just wonder if they are already redy to run, or just dont have the software ready like it is for CUDA, can't be writting all the code from scratch.",Negative
Intel,I looked into the situation more and they're apparently going to sell the Arc Pro B60 as an individual card in Q1 2026. Before that it's exclusive to battlematrix,Neutral
Intel,"I'm convinced that Intel is launching the B770 at this point, because if they don't, this would be an *insane* marketing blunder.",Negative
Intel,"So we have gone from radio silence since the B570 to possibly seeing a B770, B580 24gb, Pro B60, and B580 sli 48gb. Wonder how many of these will actually appear at Computex.",Neutral
Intel,"everytime someone says ""stay tuned"" about their product it's like hitting a 3 month snooze button.",Negative
Intel,"I mean this has to all but confirm it unless someone in their social media team is having a very uncomfortable meeting with management today.  Either B770/80 is going to be a thing in one configuration or another, or they've managed to move up the schedule for Celestial somehow.  I would expect that the former is much more likely than the latter.  Personally, I'm still likely to keep my money in my wallet until next gen at least, but if Intel disrupts the game a bit with a 32gb vram at prices well below 5080 prices, it would be pretty cool for people who want to run small but not tiny ML models locally to have a budget option.  Some AI workloads, and particularly for learning and developing could accept the trade off of having bigger models with slower performance.",Neutral
Intel,Hell yeah! Might pick up a B770 if the drivers and stock aren't too bad.,Positive
Intel,Wooo Go BMG-G31. It should be an interesting to compare the b770 to the 9070xt and the 5070,Positive
Intel,"My guess is <10% slower than a 9070(non XT), or a 5070. For ~30% less moneys.  PS: would still be nice it was an exact equivalent to, or slightly better than, a 9060XT too. But I expect more.",Neutral
Intel,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"Doesn't make sense to release models within six months of the next-generation which is Celestial, so if Intel was going to release more Battlemage models it had to be sooner rather than later. B580 was six months ago already.",Negative
Intel,they meant tuning to the Novideo channel,Neutral
Intel,"I think for the B770, they could try adding 80 Xe engines, along with 32GB VRAM. These within a sub-$800 price tag will destroy any Nvidia GPU",Neutral
Intel,"Wonder what their profit margins are going to be with a massive B770 32Xe2 die, which is most likely also going to be decently power hungry.   Don't get me wrong, I would love to see one, but I'm just curious how Intel is going to actually capitalize on a potential B770 launch.",Neutral
Intel,"But, being Intel, if someone can toss the table and put Nvidia and AMD on their place they are the ones.  My i7 6700K still boots faster and is more stable than my r5 7600 after almost 10 years.  Intel has been doing poorly these years but their legacy and their inhouse power can absolutely turn the tides.  B770 a 192 bit and 16Gb Vram for less than 380€ and it kills the duopoly.",Positive
Intel,"They saw how absurd graphics card prices are now and decided to release everything. Even old cards are more expensive than they used to be, like you can't buy a new rx6800 or equivalent at 360$ like you could in 2024. Even the b580 while not perfect easily sells out at <=$300 because there is nothing else in that price range with 12gb+ vram, av1 encode, dp2.1 support, and good RT/production performance.",Negative
Intel,They definitely wanted to see what the competition had to offer. Intel knew Nvidia wouldn't offer anything that good at $250 and released them before others.,Neutral
Intel,They don't want to kill the hype so they are still quiet.,Negative
Intel,They do experience some very unique and often unfortunate time dialation,Neutral
Intel,"The B770 is likely a 16GB config, but even that stands a chance at being very disruptive against a 9060XT and 5060ti if priced to undercut them.",Neutral
Intel,32gb would be insane for ML workloads. That’s ChatGPT-4o quality these days. A B770 with 32gb running Qwen-3-32b is a better programmer than 99% of computer science undergrad students.,Neutral
Intel,They were never bad compared to Nvidia's 50 Series drivers,Positive
Intel,The B770 would be nowhere near the 9700XT performance wise.  I would guess it would be a 5060ti/9060XT rival.,Neutral
Intel,Celestial dGPU is 2H 2026 on Xe3P using their internal foundries.,Neutral
Intel,I think Intel should also give you a Ferrari with every purchase.   Also the Ferrari gives you a blowjob every 10 miles.,Neutral
Intel,"hold there cowboy, do you have any idea how big that gpu would be  B580 has 20 CUs, be glad if B770 is as big as 40CUs, I would be surprised if they make anything bigger than that if celestial is coming end of the year",Neutral
Intel,It won't be 80 Xe engines.  Just looking at the data width difference between a 12GB B580 vs a 16GB B770.  You are looking at 3:4 ratio for available memory bandwidth.  It won't be able to feed the bandwidth of 80 Xe you claim which is 4X of B580.  Note: 32GB is for a clamshell configuration. i.e. the data bus width is same for the base 16GB units.,Neutral
Intel,"B580 is already a decent sized die. If they quadrupled the size of the die, it would hit the tsmc 5nm reticle limit.",Neutral
Intel,"You're smoking the good stuff, huh",Positive
Intel,Best they can do is probably $1599,Neutral
Intel,Bro is in Delulu,Neutral
Intel,Power hungry and  probably a huge die considering how big the B580 is.,Negative
Intel,"RX 6800 has been EOL for months, that's why the last few remaining are priced up.",Neutral
Intel,Oh shit I got called out,Negative
Intel,"Yeah the B580's had pretty decent drivers afaik. Nvidia really dropped the ball this gen, but they don't care because they make 10x the money elsewhere.",Negative
Intel,"WTF The B580 is close to 4060Ti in many scenarios when GPU is choked aka fully leveraged, and 5060Ti is a 4060Ti with GDDR7, it will be theoretically close to 7800XT, but no it wont be close to 9070 but will be better than 9060XT which is supposed to have 32 CUs",Neutral
Intel,Performance was meant to be comparable to the rtx 3080 or 4070,Neutral
Intel,"I guess <10% slower than a 9070(non XT), or a 5070. For ~30% less moneys.  PS: 9060XT is much better than the 5060Ti, so would be nice it was an exact equivalent to, or slightly better than, a 9060XT too. The B580 is already comparable to a 4060Ti in most scenarios, and 5060Ti is just a 4060Ti with GDDR7.",Positive
Intel,"The biggest planned battlemage die was BMG-G10 with 56-60Xe cores with 112-116mb of L4 Adamantine cache with a 256bit bus  But it got canceled much earlier on in development than BMG-G31 along with L4 Adamantine cache for Meteor Lake's igpu  BMG-G10 would've been very close to the maximum possible size for the Battlemage ip. It would've likely competed with the rtx 4090 in performance  If meteor lake released with L4 ADM cache, then we would've likely seen G10. Die size would've likely been massive from 600mm2 to 750mm2 of TSMC N5 or N4",Neutral
Intel,"If the rumors from before G31 got supposedly killed and resurrected are still relevant, it should have 32 Xe cores and a 256-bit bus (16GB).",Neutral
Intel,"They might do 32 GB since it would make it lucrative for the AI and Professional Workstation markets. I could see them not bumping the core count by that much and using the memory as a selling point. iirc, b580 is the largest that could be done on the current die, and they had one higher that was cancelled. That one might have the bus width to do 24GB non-clamshell.",Neutral
Intel,But there is nothing that replaced it available at the price either,Negative
Intel,Exactly.,Neutral
Intel,"Was the performance of the B580 ""meant"" to be comparable to the RTX 4060 and RX 7600?",Neutral
Intel,Much better how?,Neutral
Intel,"IIRC Battlemage can enumerate 64 Xe cores, so G10 would've pretty much been a maximum possible config. That die may have also been planned for an internal node if going with Adamantine. I'm not in ARC so this is pure speculation and pondering, but it would've been cool to see something like an Intel4 node big-BMG GPU.",Neutral
Intel,Yes,Positive
Intel,Yes and even reached 4060ti performance in certain games,Positive
Intel,That’s pretty obviously not true if you look at the spec sheet. Nobody sane would bother green lighting the B580 if the **intent** was to compete with xx60 tier products.,Negative
Intel,"Intel has not reached the maturity where they can match N/A performances with the same spec, it's pretty open that they're expecting it to be at least 1 or even 2 class lower performance with the same hardware spec.  They spoke about this in multiple podcasts since the beginning, even back to the alchemists days, that they're looking to close the gap but not completely catchup in the next gen even.",Neutral
Intel,Just make a B770 please,Neutral
Intel,Cool. Wonder if there ever was a time when AIB makers had this flexibility with AMD or nVidia cards?,Positive
Intel,Why can't a man get a 24GB memory in a 5070 Ti?   Too much to ask?,Negative
Intel,"Interesting! I suppose they will be doing a clamshell design for the memory to double the modules, since b580 was 12 GB GDDR6, they just need to double the memory chips.  Or perhaps there is already enough PCB space, if they are using 2 GB modules, to just add 6 more.",Neutral
Intel,"Hello fatso486! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"There was a BMG-G10 die planned with 56-60Xe core die with 112-116mb of L4 Adamantine cache as MALL cache with a 256bit bus.   But the die was canceled during development along with L4 Adamantine cache, which was also planned to be used in Meteor Lake's igpu.  BMG-G31 was the planned 32Xe core core. It had a 256bit memory bus and so likely memory configurations were likely 16-32gb of GDDR6  BMG-G10 and BMG-G31 would've likely been a bloated die if it targeted 2850mhz clock speeds like the B580. Less so if they targeted lower clocks.    We'll likely never see the G10 die, but we could still see BMG-G31 (32Xe core die)",Neutral
Intel,Which game on what resolution would require 24 GBs VRAM though?,Neutral
Intel,Im sure they already made but it doesn't look worth releasing for intel. The 5070 is almost **%80 faster than the The B580 while being a smaller chip** . Intel are  probably barely breaking even with $50 above MSRP with the B580.  \--  Edit: fixed math based on TPU charts,Negative
Intel,"Might be one on the way, but unfortunately, an expensive Pro version.    https://www.techpowerup.com/336528/intel-teases-upcoming-unveiling-of-new-arc-pro-gpus-insiders-predict-battlemage-b60-card",Negative
Intel,Yes,Positive
Intel,Event recently as Geforce 900 series AIBs were launching their own double VRAM cards unofficially.,Neutral
Intel,HD 7970 6GB from 2012 can still play things. Even supports Vulkan 1.3 on Linux,Positive
Intel,"This has even happened more recently. Towards the end of its production, Sapphire produced an 8GB version of the AMD Radeon RX 6500XT using a clamshell design, and the price was basically the same £150-200 (I don't know the US price, sorry).  From the past, my favourite of these (although I guess it's a different, more unique case) is the Nvidia GTX 295x2. For context, the 295 was a dual-GPU version of the GTX 280 which worked through the now-defunct SLI protocol. EVGA's engineers had the bright idea of making a dual-GPU version of this dual-GPU card, and so they essentially made their own SKU where they linked up two of these cards to form a quad-GPU card. Absolutely insane stuff, and the heat produced from it and the power usage to run it was insane, but in games that supported SLI (which was a bit shoddy) it was an absolutely insane card.",Neutral
Intel,"YES?  lots of examples of double memory cards in the past.  there is an 8 GB 290x for example to mention rarer ones.  and having dual memory options are simple and easy to do.  to be perfectly clear in case you aren't aware, amd and nvidia could tell aibs tomorrow to make double or 1.5x memory (for gddr7) cards and we'd see them in 2 months time maybe.  it is cheap, it is extremely low effort, there are no issues here, except amd and nvidia PREVENTING partners to produce double/higher capacity memory cards.  again this was common practice back then right from the start even.  we had polaris 10  (rx 470, 480, etc... ) in 4 and 8 GB versions  and for basically just the vram cost difference.  that is how it should be.  but again amd is preventing partners from doing this. that is why we don't see 32 GB 9070/xt cards for example.  we the customers want them,  the partners want to make them, but amd DOES NOT LET IT HAPPEN!  and again the same goes for nvidia, but harder, because they push broken amounts of vram way more.  the 5070 12 GB for example with 3 GB memory modules, without having to change the pcb even  (we can assume)  would be a 5070 18 GB  card then and acceptable at least.  and a 5060 would be a 5060 with 16 GB (clam shell), or at bare minimum 12 GB with 3 GB modules, or hell 24 GB if you go clamshell + 3 GB modules)  \_\_  the only reason we see double memory config options  extremely rare these days, is because companies are trying to scam us harder by preventing us from getting what we want, or hell preventing people from getting a working graphics card even (see all 8 GB cards)",Positive
Intel,I'm getting old.,Negative
Intel,"He can and will. Its called 5080 Super (or ti) . They already released it as Laptop ""5090"" so its just a matter of time.",Positive
Intel,">Which game  This would not be sold for gaming, it's for applications or AI workloads.",Neutral
Intel,It's 2025 grandpa. Gaming is only one thing GPU's are used for these days.,Neutral
Intel,GPUs aren't made to play games anymore.,Negative
Intel,"Lots of games will push past 12GB, and even 16GB with new games at particular RT settings and high resolutions.  Not necessarily a big problem to avoid right now by lowering settings, but you'll need to know which settings to lower. I don't feel all that comfortable getting a new card with 12GB as you'd want it to last for a good while when you pay that much to begin with.",Neutral
Intel,Path tracing and mods can go over 16GB at 4k.,Neutral
Intel,"This shouldn't be considered for  gaming.   Considering the gaming performance level of b580, I'm sure 24GB is totally useless.",Negative
Intel,"The b580 is only 20% as performant as a 5070?  -  E: Ah, op just phrased it oddly. Thanks for the info below. OP: percentage differences don't work in both directions, please see the info below.  E2: op fixed it, we can stop busting their chops now but I'm glad we all had the opportunity to discuss how stats work (and I'm not joking, it's good for us all to have a refresher).",Neutral
Intel,I wonder how the RTX 5070 is so much denser than the B580.,Neutral
Intel,"If intel targeted 2850mhz clock speeds for BMG-G31 then you would likely be correct, however if they targeted lower clock speeds closer to 2000mhz like we saw with the dense Xe2 implementation seen on the Lunar Lake igpu then BMG-G31 could end up being more area efficient than BMG-G21",Neutral
Intel,> The 5070 is almost %80 faster than the The B580 while being a smaller chip .  Chip size doesn't matter as much as you think.,Positive
Intel,That's the last time it was allowed. Nvidia wisened up with the 10 series.,Neutral
Intel,"Do you have more information about this EVGA card? I'm An Old and don't remember them doing sandwich style dual-GTX 295, quad die card, but would love to see more about it. There were some tongue in cheek satirical riffs on products like that though, like the obviously not real *Nvidia GTX 295X2 ATI GTFO Edition*.  The only 295X2 cards I know of were the Radeon R9 295X2 (dual-die, downclocked R9 290X on one PCB), with the earlier GTX 295 cards being similar. The older 7950GX2/9800GX2s had a dual PCB, but were also only dual die. Asus' various Mars and Ares series cards were all single PCB affairs, as were ye olden Radeon HD 5970/6970/7990/8990s and stuff like the GTX 460X2, GTX 590/690, etc.  *Edit: Ares, not Aries*",Neutral
Intel,"I had 4GB GTX 670s in SLI in 2012, the extra vram carried hard when I got an early 4K monitor",Neutral
Intel,Clamshell ain't cheap tho,Neutral
Intel,This conspiracy theorist again...,Negative
Intel,"The Polaris/Pascal launches were the last great value launches I can think of. Sure, you can argue that the 9070 series had an okay MSRP, but it's not without qualifiers.   Reddit was more Nvidia fanboyish than now, and largely wrote off Polaris. History shows the 470/570/480/580 aged better than the 1060 models.",Neutral
Intel,"So why is the source saying it is for ""gamers too""?",Neutral
Intel,GPUs are still made to be generalists in their application.,Negative
Intel,"So why is the source saying it is for ""gamers too""?",Neutral
Intel,TPU lists the 5070 78% faster than the B580 on the B580 page. They also list the B580 as 44% slower than a 5070 on the 5070 page.,Neutral
Intel,5070 has 80% more performance than B580,Positive
Intel,"According to an Intel rep, density can be influenced by how you count transistors, and everybody has their own way of counting that we don't get good information on.",Neutral
Intel,It matters for profitability,Neutral
Intel,It matters when the performance delta is so big because performance doesn't scale linearly with the hardware.  Intel would need a humongous die to reach the 5070/ti tier which would make it financially unprofitable.,Negative
Intel,Well it matters a heck a lot more than you make it sound. There is a reason why the B580 is a paper launch and doesn't really exist in the wild close to its MSRP. Its not sustainable and is  just too expensive to make.,Negative
Intel,"I tried searching for it, and for some reason I couldn't find it even though I've seen it before (including on Wikipedia, which isn't the most trustworthy source but still). I think it might've only been an engineering sample that never hit the market and was then rumoured about on forums for years?  If not, then this is one hell of a Mandela Effect.",Negative
Intel,"It's not free, but it's pretty cheap. Looking at the 5060 Ti, 50 bucks is apparently enough to make up for clamshell + 8GB GDDR7 + margins which almost certainly won't be lower than on the cheaper SKU.  Hard to explain how cards that cost 2x-3x as much don't come with clamshell configs by default other than pure product segmentation.",Neutral
Intel,"says who????  please show me an actual reference, that clearly shows the added cost to use a clamshell design to increase memory.  not some vague statement by a gpu maker, but an actual cost breakdown from a graphics card maker like let's say sapphire.  showing how much using a clamshell design ads in cost  vs a single sided memory design.  if you don't have that data, then you yourself are randomly assuming, that it would be very expensive, or someone told you WITHOUT ANY EVIDENCE, that it ""is very expensive"".  the CORRECT assumption to make here is, that it is DIRT CHEAP to use a clamshell design, unless actually proven otherwise.  \_\_\_  and even more crucial here is, that IF it were very expensive, it would NEVER be an excuse for selling broken graphics cards.  nvidia CHOSE to put a 128 bit on the 60 tier cards and a 192 bit bus on the 5070.  they CHOSE this, same as amd CHOSE to put a 128 bit bus on the 9060 xt.  they chose this, knowing the costs of clamshell designs, knowing that 8 GB vram is already broken when they chose the memory bus (yes that includes the massive time, that dies are in development)  so bringing up a reasonably assumed FALSE idea, that clamshell designs are expensive is playing towards the bullshit from the gpu makers.  but hey we can have partners figure the cost out if they want by letting them double or 1.5 x the memory to their liking.  CHOICE, that is what the gpu makers don't want you to have.",Neutral
Intel,"I wished amd had continued with their strategy with Polaris. Repurpose old production nodes for lower end markets.    The rx570 was the perfect entry level cards during 2020. Way better value than 5500, and was so readily available that during the mining boom, it remained a stable price despite and finally discontinuing production.    Imagine if the rx6600 still in production today for $200.",Positive
Intel,Because you're not limited to using it for applications or AI workloads.,Neutral
Intel,[https://langeek.co/en/grammar/course/866/too](https://langeek.co/en/grammar/course/866/too),Neutral
Intel,"They're saying that you can play games on your AI rig with it. It's not a ""compute"" card and is capable of running modern graphics libraries. This card's primary purpose isn't gaming, but it'll be able to handle it if you want to use your computer for both.",Neutral
Intel,[https://langeek.co/en/grammar/course/866/too](https://langeek.co/en/grammar/course/866/too),Neutral
Intel,"That is not how math works (not you the guy you replied to)  100% 5070 = 180% B580  1 5070 = 1.8 B580  1/1.8 5070 = 1.8/1.8 B580 = 1 B580  1/1.8 ~= 0.56 or 56% the performance  Given the 550 price tag on the 5070 and the 250 on the B580, the B580 is 45% the price. It’s better value per dollar, if that performance is acceptable.  EDIT: this reply was made to the comment above that said the B580 was 20% of the 5070",Neutral
Intel,It's not. Bigger issue for them is 30% for retailers alongside the % for board partners.,Neutral
Intel,> There is a reason why the B580 is a paper launch and doesn't really exist in the wild close to its MSRP.  [Lots of stock in Europe.](https://geizhals.eu/?cat=gra16_512&xf=9816_01+01+09+-+B580)  Maybe there's other reasons it's not in the US.,Neutral
Intel,"Definitely could have been a semi-custom or engineering sample, but if you find something send me a ping. I love obscure stuff like that! There are some wild products that end up powering oddball simulators and whatnot - remember the old Quantum3D or Intergraph simulator stuff? Aww, yiss.  [http://www.thedodgegarage.com/3dfx/q3d\_mercury\_brick.htm](http://www.thedodgegarage.com/3dfx/q3d_mercury_brick.htm)",Positive
Intel,50 euro on a 400 euro card is a big chunk of percentage.,Neutral
Intel,">please show me an actual reference, that clearly shows the added cost to use a clamshell design to increase memory.    Consumers aren't privy to this data either showing it's expensive or not.",Negative
Intel,How does that disagree with what he said?,Neutral
Intel,"You make it sound that the $250 b580 is a real product that can be bought by normal people.  Personally,  even if it was available widely at that price id still opt for a used 6700xt for less than $200 instead which is exactly what i did  last year.",Neutral
Intel,"what...  AMD and Nvidia both fabs at TSMC, now that intel is doing it too with arc, that means their BoM is more or less the same  having a smaller chip means they get way more profit, and selling it at 5070 prices vs b580 prices means its a HUGE profit diff, even if we account for GDDR7 vs 6 and all that but the price gap is like 300 whole ass dollars the price of a full b580 lol.",Neutral
Intel,"Yes, which is why that last line is focusing on more expensive cards. It makes sense for it to be optional on midrange and below SKUs.",Neutral
Intel,Because it used to say B580 has 20% the performance of a 5070,Neutral
Intel,"I mean I bought it at that price, just wait for the restocks which are pretty frequent. Not saying that it’s easy, just that it’s possible",Neutral
Intel,"You're not wrong, I'm seeing it pop up on buildapcsales in the neighborhood of $300 though, a far cry from the 5070 that's usually in the neighborhood of double.",Neutral
Intel,The transistor count for the RTX 4060 and b580 are same.,Neutral
Intel,Oh I think you were supposed to respond to the comment above the one you responded to,Neutral
Intel,"Shit, mb",Neutral
Intel,Based. We need more vram,Neutral
Intel,Professional lineup is probably gonna have professional prices,Neutral
Intel,"The B580 version 24GB is relatively easy to do as it would need a PCB layout with double side VRAM and may be a new BIOS and driver.  Very little R&D needed.  There is no point to have both 20GB and 24GB cards as they won't worry about the tiny price saving in the Pro market for a slower card with 4GB less VRAM.  The B770  32GB on the other hand is unlikely.  All that R&D for a new B770 ASIC needs to be recouped, so it would be a waste to not also available as a 16GB card for the consumer market.  tl;dr The info is highly BS.",Neutral
Intel,Other than Local AI enthusiasts who is this for?  And at that price cheaper non rich startups would probably be in the market for it as well.,Neutral
Intel,This will sell for 1200 and fly off the shelf at that price imo,Neutral
Intel,Wonder how it'll do in Blender rendering workloads.,Neutral
Intel,Where’s the 32GB Radeon cards?,Neutral
Intel,"There are rumors of Intel exhuming the G31 chip, but no indication of it releasing so soon. Reads more like the author's wishful thinking.",Neutral
Intel,Great for workstation use  Nothing to be excited about for gamers,Positive
Intel,"""Pro"" means $2000+ I guess...",Neutral
Intel,"Battlemage kinda reminds me of Zen-1. Back in 2017 Zen1 wasn't as polished as Kaby Lake, wasn't as fast in single core performance, but it DID have good performance per dollar.",Neutral
Intel,That would be brilliant.,Positive
Intel,32GB? It's going to be out of stock forever.,Negative
Intel,NVidia shitting themselves about RAM (NVidia sell RAM as premium package - very greedy company),Negative
Intel,Can you play games on the Pro GPUs?,Neutral
Intel,Damn was counting on the 32GB on the 5090 to hold its value for resale when 6090 comes out.,Neutral
Intel,Considering getting Battlemage dGPU performance for gaming seems way too much of a hurdle. Turning those G31 dies for Professional AI work seems the best bet.,Neutral
Intel,Ideally even more memory,Neutral
Intel,"just triple the B580 in every way including price and i'll buy it.  60 xe cores, $749usd.",Positive
Intel,Inb4 instantly sold out to AI companies,Neutral
Intel,We need a 69GB Vram SKU For LOLZ,Neutral
Intel,Call me a data hoarding prepper but I have an LLM model set up locally so that if I lose complete internet connectivity for a while I have at least something I can run simple queries against. A big 32gb card at a good price makes it possible to run a bigger LLM during times of need.,Neutral
Intel,"They're being referred to as ""Workstation cards."" That means big $$$$ premiums over base product.",Neutral
Intel,64GB would be great.  MORE!,Positive
Intel,"Yeap, these are RTX Pro (Quadro) competitors meant to go in workstations, so they will be more expensive than the gaming cards. Still they should be significantly cheaper than what Nvidia charges, the Quadro 4060 (AD107) equivalent was $649 but came with 16GB instead of 8GB.",Neutral
Intel,"Ehhhhh, depends.  The B770 will not have enough computing power to use all that VRAM in games for example.  But for AI workloads? Yeah it will help loads.",Neutral
Intel,and if there even remotely decent for AI they won't exist they will be vaporware like the 5090.,Negative
Intel,"At this point my only hope is that games will start incorporating AI features (that barely do anything) in their game engine (NPCs, AI graphics enhancement etc). That might be the only way to pressure Nvidia to finally release affordable 32GB+ GPUs.",Neutral
Intel,"True, but hopefully it will be in line with their pricing strategy which means it will still be   $ Intel Consumer < Intel Professional < large gap < NVIDIA anything $$$",Positive
Intel,The B770 parts of the article are all author conjecture. There is no solid evidence of such a card. Either way 24GB Arc card is pretty awesome and sets up the board for Celestial to improve it further.,Neutral
Intel,"them and anyone doing video editing, lots of vram is really good for that, and they don't typically need a whole lot of processing power like say a 5090 tier.  not sure if this is enough or with the right decode or w/e, but that is one big reason why 3090 prices were higher than normal while 4080 or 4070ti were on the market, despite those matching or exceeding 3090 performance.",Positive
Intel,"Many businesses would love to get that much VRAM on the cheap imo. Not even necessarily small ones, it’s a huge amount of value if it can be properly utilized",Positive
Intel,"Computational physics needs tons of VRAM. The more VRAM, the more stuff you can simulate. It's common here to pool the VRAM of many GPUs together to go even larger - even if no NVLink/InfinityFabric are supported, with PCIe.   In computational fluid dynamics (CFD) specifically, the more VRAM the more fine details you get resolved in the turbulent flow. Largest I've done with FluidX3D was [2TB VRAM across 32x 64GB GPUs](https://youtu.be/clAqgNtySow) - that's where current GPU servers end. CPU systems can do even more memory capacity - here I did a simulation in [6TB RAM on 2x Xeon 6980P CPUs](https://youtu.be/K5eKxzklXDA) - but take longer as memory bandwidth is not as fast.   Science/engineering needs more VRAM!!",Neutral
Intel,"These are workstation cards that compete against the RTX Pro (Quadro) Nvidia cards. The Nvidia cards come with ECC memory and are built for production workloads (Blender, CAD, local AI etc).",Neutral
Intel,Game artists like me. UE5 uses a shit tom of vram. I'll be able to run UE + 3dsMax + Zbrush + Painter without having to close any of them,Neutral
Intel,Local AI enthusiasts will help build the tooling/ecosystem for you so that down the road you can more easily sell the high-margin data center products.   Just need VRAM and a decent driver.,Positive
Intel,Local AI enthusiasts will quickly become working professionals whose businesses don't want them to use big tech AI,Neutral
Intel,4K video editing for cheap.,Neutral
Intel,"How well do local AI models run on Intel GPUs, though? There don't seem to be that many benchmarks out there. Tom's Hardware has a [content creation benchmark](https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html#section-content-creation-gpu-benchmarks-rankings-2025) partially but not entirely comprising AI where the 12 GB Arc B580 sits slightly below the 8 GB RTX 4060 for a similar price. And I don't think Intel has made it a priority to optimize and catch up in that area.",Negative
Intel,The 32GB B770 is just conjecture by the author. But it does look like a professional 24GB Intel card is coming based on the B580.,Neutral
Intel,Which means that it will MSRP for $2100.,Neutral
Intel,A 3090 is $1000 used so it better be less than that,Neutral
Intel,only if its better than a 5080,Positive
Intel,2 5060 Ti' 16Gb will be way faster for AI workloads.,Positive
Intel,"32GB is great for local AI. It's the best a reasonably affordable card can provide atm (5090). Basically the more the better, if the 5090 has 48GB it would be an even better card, if it has 96GB like the RTX Pro 6000 then it would be better still.",Positive
Intel,"With that specific card maybe not, but it could do two things to help gamers.   If it's successful, Intel's dGPU gets more cash infusion and leads to better cards down the road which might compete in the high end gaming market. Having another player is always a good thing.   It might force NVIDIA to compete by lowering prices so not to lose market share on the Ai and workstation side of things, which means the better gaming cards do get cheaper.",Positive
Intel,The thing is that Intel dGPUs have a major architectural issue with the CPU overhead. Hopefully they'll be able to do something about it soon.,Negative
Intel,Yes.,Positive
Intel,Womp womp,Neutral
Intel,"There was a BMG-G10 die planned with 56-60Xe core die with 112-116mb of L4 Adamantine cache as MALL cache with a 256bit bus.   But the die was canceled during development along with L4 Adamantine cache, which was also planned to be used in Meteor Lake's igpu.  BMG-G10 would've likely been a bloated die if it targeted 2850mhz clock speeds like the B580. Less so if they targeted lower clocks.   We'll likely never see the G10 die, but we could still see BMG-G31 (32Xe core die)",Neutral
Intel,"with 3GB chips we may see that. It would take 416 bit bus width, which is unusual, but technically possible.",Neutral
Intel,If the internet is down for an extended period of time I doubt having access to an LLM will be high on the list of priorities.,Negative
Intel,Cheaper to just buy some acid if you want to hallucinate when the power is out.,Neutral
Intel,"> I have at least something I can run simple queries against.  I genuinely cannot see how that would be useful if the internet connectivity went down. Unless, what, you're using it for assists in coding?",Negative
Intel,If it's a desperate scenario you'll survive with slow ram.,Negative
Intel,Wikipedia + local AI + voice control would be a cool doomsday support agent.,Positive
Intel,"A well indexed RAID setup full of survival PDFs and guide videos, entertainment and independent power will do that job far more competently, at longer independent power up time.",Positive
Intel,"«Dear LLM, should I drink water or Coca Cola? Much appreciated !»",Positive
Intel,LTE/5G card + failover router with recursive routing setup does not work ?,Neutral
Intel,"Haha, when nvidia is selling $11k GPUs for workstations, even a large gap would still be thousands of dollars for the intel pro, but I mean it's all rumor anyway, conputex isn't for another week still haha.   I won't be holding my breath and pinching myself at every rumor in the next week lol.",Neutral
Intel,"There was a shipment of the chips (which Intel already fabbed) to one of the factories that makes the special edition Arc cards, but that's the last that has been heard. It's not much, but it is something.",Neutral
Intel,"HA. Hahahaha, that's hilarious.",Positive
Intel,They run models that need 32gb of VRAM way way faster than cards without 32gb of VRAM.  Though 2 5060Ti 16Gb will run them faster.,Neutral
Intel,It would atleast be able to run some models albeit slowly. Versus not being able to run at all on even high end GPU’s like a 5080,Neutral
Intel,Sorry I should have been more careful with my phrasing based on the leak culture for tech news.   I'm not an insider.     I predict this could easily sell for 1200 usd,Neutral
Intel,A 3090 has less ram,Neutral
Intel,$700-$800 USD used.,Neutral
Intel,3090s at this point are all in danger of finally stopping working. Some have been in datacenters for what 5 years?,Negative
Intel,nah that vram will be unmatched for the price,Positive
Intel,"This card is for AI, not gaming.  I do want a gaming version, but *that* would have half the VRAM and can't be $1200.  Intel isn't getting into the GPU business to save gamers.",Negative
Intel,not everything is about gaming if there decent for AI they will fly off the shelfs.,Neutral
Intel,Battlemage was a big improvement over Alchemist in architectural adaltation. Im hoping Celestial will also be a big improvement and reduce the overhead.,Positive
Intel,Didn't you know LLMs is one of the basic human needs?,Neutral
Intel,Lol but how else can we justify having a card with 32gb of vram when ones with 12-16 crush virtually any game these days...,Neutral
Intel,"The person you replied to might live in a rural area or somewehre with frequent outages. You're making it as if the only possibility for losing internet access is for the internet to go down globally or some shit.   I've solar/batteries/generators for my house because outages are frequent. I also have two internet providers because electrical posts get crashed frequently by trucks (maybe not thaaat frequently, but 3-4 times a year, at least) and that leaves me with no internet sometimes for days on my main provider.",Negative
Intel,Unless it's shockful of useful survivalist tips and strategies... Which it is.,Negative
Intel,I think its debatable. A local database LLM could provide a lot of useful infromation that you wouldnt otherwise think of asking in a survival scenario. Do you know how to home-make water filter for drinking from a local river? You could ask LLM that.,Neutral
Intel,", until it “hallucinates” some fatal advice.",Negative
Intel,That 11K GPU has 3X VRAM and 2X performance.  A lower end is likely not leaving too big a gap.   More importantly is how the Intel pro compares with AMD pro,Neutral
Intel,2 5060Ti 16Gb will run them faster and probably for less money.,Neutral
Intel,"In addition, used prices are normally lower than for similar new items, accounting for the relatively higher risk involved and shorter (on average) remaining lifespan.  For example, you can find a used 4060 8gb for significantly cheaper used on Ebay than the same card new on Newegg.",Neutral
Intel,"2 3090's have 48Gb of VRAM, AI models don't really care how many cards they run on, the cards don't even need to be in the same machine, network is fine.",Positive
Intel,Why would they stop working?,Neutral
Intel,They killed the Flex line. Gaming is the primary market for this class of GPU.,Negative
Intel,when did i say anything about gaming?,Neutral
Intel,Why would you need to prep human needs over your internet being down?,Negative
Intel,"I read guns were basic human needs, according to /r/preppers  ?",Neutral
Intel,"Cool, but what does that have to do with *whether or not you can locally host a bigger LLM*?",Neutral
Intel,"Yeah, but for that job you're better off with a RAID made of the cheapest reliable HDDs you can find full of survival guides and entertainment. Thing will stay up longer too once it has to run on local generators.",Neutral
Intel,Just buy some survival book lmao. If you are in a survival situation you probably dont have electricity either.,Negative
Intel,/r/BoneAppleTea,Neutral
Intel,If it gets them right.,Neutral
Intel,It's useful for having a tool that actively lies to you at complete random and will likely get you hurt or killed just as much as help.,Negative
Intel,"You could also purchase survival books that wouldn't use precious electricity, and they wouldn't hallucinate and lie to you like the AI could.",Neutral
Intel,Thats why the advice is filterd by the human intelligence listening to it.,Neutral
Intel,"And given how juice-frugal modern HDDs are, a local archive of wiki and survival data on an in house NAS RAID and generator, connected to a laptop running in low power mode will stay working much longer.",Neutral
Intel,well you may want to double check the survival knowledge before attempting it XD,Neutral
Intel,2 of these would have 64gb of VRAM.,Neutral
Intel,"Hey if Intel want to release this 32GB B770 in the gaming segment where it's going to be judged primarily on how well it renders frames (and has to be priced accordingly) then they can go nuts. I'll be happy to consider it as an option.    I just think ""Arc Pro"" and 32 GB indicates a different goal and different customer in mind.",Positive
Intel,"To be fair, an LLM could be a pretty nice tool if shit hits the fan.   I have a 8kw generator and solar panels that will work as backup power if things goes sideways. Of course I have books too, and a full copy of Wikipedia on a small usb drive that I refresh once a year. Oh, and I live pretty remotely.   LLMs are not a substitute for actual knowledge, but in a scenario where the internet is turned to mush, time is of essence and maybe it can allow me to do stuff I won’t have time to fully learn on my own, especially for one-off tasks. I don’t expect to get an LLM to guide me through how to remove my own spleen, but it would probably be able to help me with spot knowledge. Could even load it with schematics, manuals, books on a myriad of subjects.",Neutral
Intel,"I dont know whats in that sub, but given what scenario typical preppers think will happen guns would be useful to hunt food if nothing else.",Neutral
Intel,You are questioning the other person's priorities based on your perceived impact of an internet outage.    Maybe LLMs are their hobby and they experience frequent internet outages. Why would it be so weird to want to run LLMs when the internet goes down.,Negative
Intel,Most pre-made survivalist packs now come with a portable solar panel. Good enough to charge stuff like phone/flashlight/radio.,Neutral
Intel,>If you are in a survival situation you probably dont have electricity either.  You could buy a single single solar power station and couple of panels for not much more than a grand. Which would be able to run a desktop PC for a couple of hours a day.   Sure wont be enough panels and storage to run it continuously. But makeshift off grid solution like that are getting crazy affordable these days. It's starting to get to the price point where it should simply be something you should have in some form if you live somewhere where you can expect power outages every now and then. Since then you can use it for things like running a freezer and basic lighting.,Neutral
Intel,Or even just a very basic RAID full of high quality survival data and a bit of entertainment if you want to go high tech.,Neutral
Intel,"And thus is the crux of problems with **ALL** LLM projects. Either yo already have the resources and experience enough to double check its work...in which case, it'll have been faster to just look it up yourself **without** the LLM. Or, you do not and are trusting in something that lies or makes up sources at literally any time and you are putting ticking time bombs into your day-to-day life, work, etcetera.",Negative
Intel,"You found an issue of the ""Wasteland Survivor's Guide!"". You permanently take 5% less damage from insects.",Neutral
Intel,"> I just think ""Arc Pro"" and 32 GB indicates a different goal and different customer in mind.  Agreed, but there are other markets than LLMs. And my main point was their client dGPU line was driven primarily by gaming and productivity, not AI. As for their AI chips, well, who knows what's going on with that clusterfuck.",Neutral
Intel,Are you taking into account that this person is a self proclaimed data hording prepper and the context that is implied?,Neutral
Intel,"There's a dozen+ more important things you'd want to use your limited electricity for in such an event, like a fridge/freezer, cooking, heating water, hell even much more efficient computers like phones or laptops.",Neutral
Intel,"Why on earth would you run a desktop PC in an outage. It's insanely inefficient and lacks portability. A laptop is smaller, portable, and uses less power. If you care about ""knowledge"" then just download a bunch of information and guides in advance, and make a copy on a separate flash drive if you want redundancy.",Negative
Intel,"an AI could help narrow down what you're looking for  Imagine someone is sick with a couple of different symptoms  Searching every disease wikipedia page to try and find a match could take ages  But an AI could help point you in the direction of the most probable ones  And then if you wanted, you could just double check those pages yourself",Neutral
Intel,"Like, HDDs are a fraction of the power to work of *any* GPU that could run one of those, and frankly, in that case you need your wits about you anyway.",Negative
Intel,"It wold help you find relevant articles quicker, I don't need it to try and summarize or distill information into easy to read summaries.",Neutral
Intel,"The point is to use it as a tool to find the information you need, not to regurgitate information it's been fed, at least in this example.",Neutral
Intel,"An LLM is meant to construct plausible sentences that it.  “Hallucinations” are just a way to explain to the layman that an LLM has ZERO concept of what is logical, factual, or ethical. It just builds sentences but people treat it like it’s the librarian to the grand archives.",Negative
Intel,"If an LLM told you to jump off a bridge, would you?",Neutral
Intel,I use LLMs to help with creating my TTRPG. The only doublechecking needed is to check against my own homebrew fantasy.,Neutral
Intel,Why is it relevant what they are?,Neutral
Intel,We dont know that in the case of that user those arent already accounted for.,Neutral
Intel,Right? I saw someone's recommendations for downloading all of Wikipedia and all maps onto a $30 phone that could be recharged from a wood burning lantern that used the heat to funnel power to a USB port.  All that was way less than $500.   This guy's LLM is the stupidest thing I've ever heard of. Reaks of cryptobro mindset.,Negative
Intel,We already have mayo clinic and doctors hate it lol,Negative
Intel,a good PDF and CTRL-F does that at a fraction of every price.,Positive
Intel,This is the answer to all the snarky BS people want to say   https://www.reddit.com/r/hardware/comments/1kkb32p/intel_might_unveil_battlemagebased_arc_pro_b770/msjj3mo/,Negative
Intel,that sounds needlesly complicated. You can buy portable solar panels for less that are good enough to charge phone/flashlight/radio.   That guys LLM sounds more like what person i know does. He works on ships that cross the atlantic ocean. Internet access is still rare there. So he pre-downloads enough stuff for local use. Power isnt an issue on a ship.,Neutral
Intel,"Which is understandable in our current situation  But this comment chain started as a discussion of a doomsday scenario, where very few or no doctors might be available   Google and mayo clinic servers would most likely be down, so i think a local AI would probably be better than nothing",Negative
Intel,Now imagine a situation where doctors are not available and wont be available. Would you rather have mayo clinic or not?,Negative
Intel,No risk of them pulling that off but the stock correction and long term damage to software stacks from this bubble is gonna be extremely bad in its own right.,Negative
Intel,"I guess you could do that but just the English wikipedia is about 90GB of xml files, could be quite time consuming.",Neutral
Intel,"The thing I was referencing was in like, 2015 or shortly after. I dunno what the status of portable solar was back then. In any case, he was an avid long-distance hiker so it was tailored for his use case. He walked during the day, burnt some tinder into his fire lantern while camping at night to recharge his phone.   So, yeah. Add in specific use case scenarios and the arguments make more sense. Like your friend's situation.",Neutral
Intel,If doctors aren't available I think I'd have bigger problems to worry about than getting an opinion from a hallucination bot,Negative
Intel,"Then get a inexpensive but quality PCIE 3.0 SSD for it to live on, maybe a cache as part of the RAID.",Neutral
Intel,"so these are RDNA 5 / UDNA and Celestial, right?",Neutral
Intel,FYI post mentions SWC for GFX13 (UDNA). [The patent for the Streaming Wave Coalescer](https://patents.justia.com/patent/20250068429) mentions it reduces thread divergence. This is AMD's implementation of DXR 1.2's SER functionality and will be used to boost thread coherency *for thread incoherent workloads like* ~~in~~ neural rendering and path tracing *for example similar to* ~~just like~~ NVIDIA's Shader execution reordering.,Neutral
Intel,"Posts like that make me wonder if it'd be still worthwhile for me, at end of this year (5-4+- months), to buy a 9070xt or not?   These posts are speculative, so nothing to make decisions on, I just wonder if it'd take them a year, or 3 years, to get to actual production...",Negative
Intel,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,I need Celestial in my PC for free now!!!!!! 😍😍😍😍😍😍😍😍😍,Positive
Intel,"This is Druid, the successor to Celestial.",Neutral
Intel,Xe4 is Druid.,Neutral
Intel,The title literally says Druid.,Neutral
Intel,Apparently does it automatically and not just ray and path tracing.  Hopefully they'll do a 6090/90 Ti competitor.,Neutral
Intel,Less drugs might help you,Neutral
Intel,Druid is no specific IP. It's just whatever next-next gen dGPUs would align with.,Neutral
Intel,"My bad, so these are Druid and RDNA 6/UDNA 2?",Neutral
Intel,"Edited the above comment for less confusion and indeed any workload with thread incoherency would benefit from it.  Automatic, really? So games devs don't have to implement it unlike NVIDIA's SER? If it ""just works"" then I sure hope AMD has found a way to address the reordering overhead issue or only enable it when certain criteria are met.",Neutral
Intel,"It was a joke post. ☠️   If I don't buy a 9070XT, then I'm going for Celestial myself",Negative
Intel,"Troll making fun of people who want cheaper gpus. Pri""cks. The only actual circle j""rk ive seen on reddit regarding pricing",Negative
Intel,Intel Alchemist has automatic SER. Maybe there is a reason why their GPUs have worse overalll overhead,Negative
Intel,Well the thread with Kepler on Twitter mentioned it. Would be damn cool and help go for the performance crown hopefully.,Positive
Intel,"Your 'joke' fell pretty flat there, mate. Screaming nonsense and spamming emojis isn't much of a joke..",Negative
Intel,"> So saying Druid uses Xe4 is correct for dGPU   Not even that. It's an assumption. Not an unreasonable one, but if Intel skips Xe3 for Celestial, then Druid could be Xe5, for example.    For that matter, Druid could not exist at all even if Xe4 lives.",Neutral
Intel,"To make it easier to understand, it is similar to AMD: RDNA1/2/3/4 is the underlying architecture and Navi 1x,2x,3x,4x are the implementations.",Neutral
Intel,Interesting. Someone also told me that SWC is a lot closer in design to Intel's TSU than NVIDIA's SER.  The inconsistent gaming performance of Battlemage could be related to a broken TSU implementation. Might explain why B580 in new games (as per HUB reivew) best case performs like a 4060 TI 16GB and worst case 20% slower than a 4060. Something isn't adding up.,Neutral
Intel,Indeed another halo gen would be nice. Haven't seen anything competitive since 6950XT. At the very least hoping on UDNA being a massive architectural upgrade clean slate as rumoured last year.,Positive
Intel,"It's an interesting read, clearly shows that the gap at higher resolutions with memory intensive features (upscaling ,frame gen, rt etc) is very small, however it does appear that that headline is a touch disingenuous, as the 5060ti does appear to handily beat the B580 in lower resolutions, basically all scenarios where the 8gb frame buffer isn't the bottleneck...  Can't fault the conclusion though that this card should never have been made with an 8gb option or it should have been priced as a much more direct price competitor with the b580 so customers could choose whether they wanted the better 1080p raster performance or the better 1440p and 4k upscaled performance.",Positive
Intel,"I read the article, they mention they cherry picked examples - which is fine and recommend neither.  Then they say esport players shouldn’t pick one up either without giving an excuse.  Why? Esport players typically play on lower res with low settings to maximize fps, cpu is more prevalent in these titles. No serious esport game (CS, Valorant, Dota, League) is reaching 8GB on 1080p low  For CS atleast even if you max it out on 4k its not gonna go beyond 4GB",Negative
Intel,And more expensive too.,Negative
Intel,This gen Nvidia is a massive joke,Negative
Intel,Would it have been cheaper to make it 192bit 12GB GDDR6? Even 160bit 10GB GDDR6 might be just enough to be acceptable.,Neutral
Intel,And here I am perfectly happy with my humble RTX 4060 ti 8GB,Positive
Intel,"Correct me if I'm wrong, but if I'm in the market for a low to mid range card, it's VRAM volume the first consideration?",Neutral
Intel,oh this [guy](https://www.techspot.com/community/staff/steve.96585/) wrote this?  no wonder the headline is bs & clickbait,Negative
Intel,"The 5060 Ti is also 180W vs. the B580's 190W. All things being equal you'd expect it to be slower, but it sounds like it's only slower because of the RAM (and you're getting the cheap version at 8GB.)",Neutral
Intel,Title definitely comes off as clickbaity. Of course a card that isn't VRAM bottlenecked will be faster than one that is. 5060 Ti 8 GB will get destroyed by any card with higher VRAM than it under certain workloads.,Negative
Intel,I feel like it is only 8GB to justify Nvidia's lack of vram across the stack.,Neutral
Intel,"HUB is very clickbaity, with their focus on ""elitist"" PC gamers. Basically Steve advertises himself as someone who only cares about high refresh rate gaming, to the point where I think he's said more or less that anything under 240fps at 1080p is unacceptable for him now.",Negative
Intel,Yeah it's really just confusing buyers even more. The B580 literally came 10% worse against the 4060 over a 50 game sample size.,Negative
Intel,Sensational article headline that nobody reads past?  Can't be...,Negative
Intel,"Their excuse was that esports players may want to play a single player game occasionally. If they choose to buy the 8 GB variant of the card they will have a bad experience in said game (assuming it's a modern AAA title with a high VRAM requirement). At that point they're screwed so they will have to limit themselves to older single player games and esports games only, which is something nobody spending $400 for a GPU should have to go through. Obviously there are people who exclusively play multiplayer games so the 5060 Ti 8 GB would be totally fine for their use cases but I'd say they are of the minority.",Negative
Intel,"Idk if I ever met an eSports player that never played other games.   Real e-sports player buys expensive GPUs for higher refresh rate because they can afford it and they have to. Broke gamers buys these to mostly play competitive games, then also play single-player games just for fun.",Neutral
Intel,"2 reasons:   1. Being an esports gamer does not mean that is all you will ever be for the useful life of the card. The price difference of 60 bucks is worth it to give yourself options.   2. Resale value. When you compare both cards TCO, the 16 GB model may end up cheaper overall (as observed by the 4060 ti versions).",Neutral
Intel,> they mention they cherry picked examples - which is fine  Why is that fine?   If it made NVIDIA look good everyone would be mad that they cherrypicked.,Negative
Intel,The joke is on all of us.,Negative
Intel,"The MSRP prices aren't bad, it's just the actual prices suck, but that applies to literally anything rn, including amd gpus, and even Intel b580s are going for 400-500.   Dlss 4 and frame Gen is surprisingly good, but they're lacking in raster performance. Think of the 50 series as a refresh of the 40 series, which wouldn't be bad at all",Negative
Intel,"That would require an entire new die, or cut down the one used in the 5070 which is clearly not going to be cheaper. The alternative solution would be staying on 128 bit bus but using the illusive 3GB memory chips.",Negative
Intel,Personally I think the 5060Ti should just be a 16GB card with the 128 bit bus.  The 8GB variant should have never existed at all.  Below that the 5060 could have been a $330 card and NV had the option of using 3GB GDDR7 chips to make it 12GB on a 128 bit bus. If supply or cost of those chips reduced the margins such that NV did not like that option then they could have gone 96bit 12GB with the 5060. It would be a good upgrade over the 4060 and they could have commanded a higher ASP than the 8GB 5060 model they are going to end up selling. It also would have done pretty well overall with reviewers.  Still NV chose not to so it is what it is.,Neutral
Intel,"Not for long, that's the problem.",Negative
Intel,"Not really, always check benchmarks. VRAM alone in isolation isn't the only number to bank on.",Neutral
Intel,"If you had asked this question four years ago, you'd be told that 8GB of VRAM was the floor for a mainstream card. 16GB of system memory was also ideal. You could get away with 6GB at the time, which is why the RTX 2060 6GB was so popular.   Today those requirements are changing quickly as UE5 and other contemporary game engines are switching their attention to current and next gen console specs.   If you want to run games with full texture quality at 1080p or 1440p, 12GB of VRAM and 32GB of system RAM is required. 16GB of VRAM is ideal if you're playing at 4K.   While you can get away with less (and games will still run), the presentation will be ass and the performance will be lower.",Neutral
Intel,"No. How well a card performs is totally separate from vram. VRAM is one of those things where you either don't have enough and it will cripple your performance or you do and everything works properly. Performance itself is determined by the actual GPU though, which you need to look at benchmarks to figure out.   Like, adding 32GB of VRAM to a low end card would not make it perform better.",Neutral
Intel,"That's kinda like asking whether the amount of windows is first consideration when picking a house, in a thread about a house that doesn't have any at all  No, in a typical situation VRAM is not the first consideration, especially in low-mid range. But it can very quickly become one if the card in question has a ridiculously small amount, which this one does",Neutral
Intel,why what's wrong with him,Negative
Intel,You don’t actually believe this right? Nvidia doesn’t care about gaming GPUs at all. 50 series is just their low effort attempt at throwing some scrap chips at their roots in gaming. This could very well be the last gaming GPUs we get from nvidia for a while.,Negative
Intel,No I wouldn't expect it to be slower considering the B580 released as a 4060 competitor lmao. We already saw with the 4060 ti the massive difference 8 vs 16 gb can make even with the same exact GPU core config.,Neutral
Intel,>  Of course a card that isn't VRAM bottlenecked will be faster than one that is.    There seem to be no shortage of people who can't comprehend it though.,Negative
Intel,any GPU below 10 gigs is a dinosaur as a new gaming card.,Negative
Intel,"The point being, of course, that these ""certain workloads"" may easily include popular recent games at reasonable settings.",Neutral
Intel,HUB as in hardware unboxed? This article is [techspot.com](http://techspot.com),Neutral
Intel,Other way around,Neutral
Intel,It be,Neutral
Intel,"Depends on the circle I guess. When I used to play CS religiously I had friends who just had 1 game in their account - CS, they’d have mid end PC’s and would only play CS. Maybe diverge to other esports games like R6 Siege, Overwatch but mainly just play CS.  There’s a lot of people like that, maybe not in North America but definitely in Europe and Asia.",Neutral
Intel,"Completely missed the point.  When you say ""Esports gamers shouldn't buy this card"" the obvious implication is that it is bad FOR ESPORTS, not just in general.  That's like saying ""Olympic swimmers shouldn't eat this food"" and then finding out that the food increases the general populations chance of getting diabetes at 65.",Negative
Intel,Real esports players play esports games (known for very low specification requirements) on lowest settings (for maximum clarity and fps) so there is NO way in hell you would even consider buying latest GPU for esports... how much does fucking SC2 or CSGO require LOL,Negative
Intel,The rationale they used - a $400 card should not be losing out to an older $300ish card.   Even ignoring the 4k results the 5060ti8g lost at 1080p to B580 in some games - which is inexcusable,Negative
Intel,The point is today's worst case is a few years from now's typical case. In other words 8GB cards are going to age like milk.,Negative
Intel,And you bet your ass the execs are all laughing their asses off at you while the billions keep raking in,Negative
Intel,"I mean yes it's a joke because of pricing and bad generational gains, not because they aren't still the best GPUs. It's not like 5000 series is worse than 4000 series, it just didn't make the gains we'd have hoped.",Negative
Intel,"They're not really elusive, Nvidia is just consuming all of the 3GB GDDR7 supply for their enterprise AI products, and reserving a tiny portion for the RTX 5090 laptop GPU.",Neutral
Intel,"?       I've been running it for a year and a half now - lots of VR games (Alyx, Star Wars Squadrons, Subnautica) and Cyberpunk, Doom Eternal, Robocop Rogue City - and have been very happy with the results.     But I've been playing since the late 70s so I'm pretty easily pleased. I don't really care about 4K or 120 fps.",Positive
Intel,This. There are many gray spots like framgen vram requirement. Texture popin or failure to load because of out of memory. Bus widths cache sizes of architecture etc.,Negative
Intel,"*Nothing. Stans just get mad when people objectively point out the logical flaws in their emotional reasoning. I truly don't understand why people get so defensive about their purchases or brand fandom.   Nvidia is a multi-trillion dollar company, they don't need simps.",Negative
Intel,Bullshit. It's a $10B/year industry for them.,Negative
Intel,"The 4060ti was a 160W card. Saying a 190W card competes with a 160W card... they're very different. 180W is closer but it's still significant.  And yes, of course, if your workload needs more than 8GB of RAM it's going to be slower, but that's obvious and it doesn't negate it if the 180W card is faster than the 190W card despite using less power (but it can only operate on 8GB of RAM.) And sometimes that's all you need so it's just better.",Neutral
Intel,ok.... but should it be 50% more if it's going to lose so easily? Like I think people's issue with this card is it's MSRP $399. After you get the OEM's involved this is a $450 card verse a $250 card.,Negative
Intel,"I would say 12 GB, honestly.  The 1080 Ti had 11 GB. Eight years ago.",Neutral
Intel,"Ehh even 10 GB is pushing it. I'd say it's just barely enough to use maximum texture quality settings with no RT, based on what I've heard from 3080 owners. 12 GB is the new minimum for RT + maximum textures. 16 GB is optimal for all of that plus path tracing.",Neutral
Intel,This is just the written version of the video that was posted a few days ago.,Neutral
Intel,"Can confirm (my benchmarks are only 10 raster + 5 ray traced though). I have the 4060 at 79% of the b580 at 4k, 89% at 1440p and 96% at 1080p on average. B580 ranks 31 out of all gaming gpus, 4060 at 37.6",Neutral
Intel,"For the price it's bad for eSports, none of the features are used for competitive games and the price is garbage.  Just buy something used for as cheap as possible then, it will probably run everything anyways because you set everything to medium or lower.",Negative
Intel,"> ""Olympic swimmers shouldn't eat this food""  A better example would be a marathon runner running on  flip-flops for the least amount of weight, or a cheap but heavy running shoes.   If you're doing things competitively and sometimes for a living, why not buy the best gear possible to give the most advantage? The best runner I've seen are running on Carbon Fiber shoes that costs as much as a 5060, they yield the most advantage.",Neutral
Intel,You get my point or you still missed it?,Neutral
Intel,You buy the best hardware possible to get the most FPS possible.,Positive
Intel,"Yes you're right, maximum FPS, it's like 500fps on games in 1080p.   There's also newer gen eSports title such as Overwatch 2, new R6 Siege, Marvels Rivals, The Finals etc. Is a 8GB card enough for these?   I game at these titles on the lowest settings, there is a point where even I have problems seeing people at distance and the best settings I used are around medium for some level of details, but I lose some FPS.",Neutral
Intel,"In a million gamers, how many of those eSports players are exclusively playing eSports games and absolutely nothing else? Assuming costs isn't a factor for them.",Neutral
Intel,One with a notoriously... iffy... software stack at that. edit: I am talking about the *Intel Arc* here!,Neutral
Intel,What I meant was comparing VRAM across architecture/generations. Say a 6gb RAM card from 2025 would likely outperform an 8gb RAM card from 2014.   So VRAM alone isn't a clear indicator whether one card is sureshot better than another. Just the way Ghz isn't the only indicator whether a CPU is better than another. It's a major component but not only component that matters.   Which is why you should check benchmarks before deciding on whether a GPU is better than another.,Neutral
Intel,"The CPU processes all the games’ physics and logic per frame. More powerful CPUs can process more physics and logic per second.   GPUs process the graphics per frame and more powerful GPUs can process higher resolution and higher quality textures more times per second.   If you play at 1080p, the GPU can be set to higher textures since it doesn’t have to work as hard as 4K so it can process graphics faster. This allows you to get higher fps but the fps may be limited to how powerful the CPU is. This is “CPU limited” or the CPU is the bottleneck.   The opposite would be “GPU limited”. No e of these terms are objective, they are just a result of GPU/CPU combos and monitor resolutions. It’s all just preferences, perceptions and projection.",Neutral
Intel,"Intel's GPUs have needed more power to compete for a while now thats nothing new. The A750 and A770 both consume over 200 watts but are within a 4060 performance tier. The base model RX 7600 was in a 4060 performance tier while using more power than the 4060 ti. The watts don't say a lot besides the fact that nvidias 40 series cards were extremely power efficient.  Maybe for a card aimed strictly at 1080p 8 gb could still be mostly okay but I consider a 5060 ti to be a 1440p capable card and the GPU surely has good enough raw power for 1440P, it just seems the vram is holding it back",Neutral
Intel,No it frankly just shouldn't exist. It is under-specced for today's requirements and for its own capability.,Negative
Intel,"12GB is plenty of headroom for 1080p and 1440p gaming. There’s always going to be horribly optimized games that use too much vram. I really wish people would learn how games and vram work, to cut down on some of this repeated vram fear.",Negative
Intel,and Nvidia learned their lesson. They'll never make that mistake again.,Neutral
Intel,3080 owner here and can confirm. Still not giving up on the card given that anything that provides a meaningful upgrade is stupid expensive.,Negative
Intel,"10 is 6, 12 is 8, 16 is ten.",Neutral
Intel,"Oh yes, I had no idea Steve was a contributor for Techspot. Makes sense that the headline is a touch click baity. Still an overall valid point though.",Neutral
Intel,"As resolution increases, the GPU takes on more of the load while the CPU becomes less of a limiting factor. The CPU-related bottlenecks that hold back the B580 gradually disappear, allowing its true performance to emerge. That’s why, even without running into VRAM limits, the B580 can outperform the 4060 at 4K—and might even surpass the 5060. This is exactly why those who hype up X3D CPUs tend to avoid comparing gaming performance at 4K resolution.",Positive
Intel,"....on what settings, what resolution and which game XDDDDDDDDDDDD",Neutral
Intel,A surprising amount of people play 1 game only or only dabble in games outside of that.,Neutral
Intel,Ok so then are you an esports player or everything player?...,Neutral
Intel,"A lot of us LoL players almost literally only play LoL. Ofc we'll play something different here and there, but LoL players in general don't seem to care much for other games until they abandon LoL. Even then, they're likely to go to another esports title with low requirements because LoL players have a strong competitive itch. Look at Tyler1 moving to chess for an extreme example lmao. The most graphically demanding games tend to be single player or coop and those are generally less attractive to LoL players.",Neutral
Intel,"Basically not even Nvidia can path tracing unless few cards, not the same thing",Negative
Intel,"Oh, I see.   Agreed on checking benchmarks for the ultimate answer.",Neutral
Intel,"Agreed but think of it like this GTX 3050 is 8gb VRAM and 3070 is also 8gb VRAM, does that mean their performance will be similar?   The above answer was for someone who doesn't have a lot of idea about GPUs and was looking for a an easy way to identify the capability of a GPU.   What you're discussing is moreso about balancing a build between CPU and GPU budget allocation as per resolution. Which is perfect and valid, but not related to the question asked by the user as to how to identify a better GPU.",Neutral
Intel,"plenty might be a bit strong, i've noticed a lot of games using more than 12-14/24 on my card at 1080p. I've got the settings at max when at that resolution in testing but still it's still concerning that modern entry level - mid range products aren't being packaged with enough Vram for 1080p the standard display resolution since about 2008/9.",Neutral
Intel,"You're missing the 'for now at 1440p' - don't buy a 12 gig card for 1440p; the 5070 current model and 9070 GRE are elite 1080p cards and no more.  If you're at 1440p, get the 16 gigs.",Neutral
Intel,I have a 3080 10GB and I keep following all of the new GPU news but I can't justify even an upgrade to a 9070xt my card just does everything I want at 3440x1440 so why would I need to upgrade?,Negative
Intel,"You play on low settings yes, but you don't buy a card that's ""good enough"" at low settings. If they can get 1000 FPS with a 5080 in CS2 compared to 500 FPS with a 5060 Ti, they are going to buy the 5080.",Neutral
Intel,And when they dabble it might be a crap experience for no good reason,Negative
Intel,"The person I replied to didn’t ask the specific vram question that you originally responded to.   I am responding to their first paragraph, and the “higher frames at 1080p are more CPU dependent” idea. Which is related to my comment.   For people doing cursory research, they will undoubtedly run into the hordes of commenters repeating that CPUs work harder at lower resolutions and take away the wrong impression. Which is why it gets repeated so much here to begin with.",Negative
Intel,"You’re seeing allocation and texture storage. If you have 24GB available most games will store textures in vram up to about 75-80% of total vram. If you put a 12GB GPU in place of yours at the exact same settings it’ll “use” about 7-8GB. It just scales the usage to what’s available for faster texture streaming. COD black ops for instance has the vram buffer adjustable. You can set it to how much vram you want it to keep available, but if you max the settings out at 1080p it shows you the game only needs about 4-6GB to run. I can set cod to use 16GB of 16GB available at 1080p, even though it doesn’t actually need it. That’s just an example of a game that lets you control it.",Neutral
Intel,"And it's particularly pressing in this economic climate, you want the SKU with extra VRAM to stretch the life of your card.",Neutral
Intel,Flawless logic bro. Are you a salesman by any chance?,Negative
Intel,> no good reason  saving $1700 not buying an msrp 5090 is a good reason,Neutral
Intel,"Damn people didn't like the truth I guess.  You're correct and you can often see people complain about their OS ""eating up RAM out of nowhere"" because modern OSs will similarly pre-cache commonly used binaries and assets based on how much unallocated memory you have.",Negative
Intel,"In general you are probably right as I have seen these points made before by reviewers, however the specific games in question (CP2077, HL, and Horizon FW were the ones I noticed this on) were using the full 11 GB buffer at 1080p on my 1080ti before I upgraded. May well be examples of poorly optimised games but it was a major reason I elected to go for a gpu with more than 16 for my personal PC, since I was upgrading my monitor at the same time and prefer not to upgrade GPU every generation. I can definitely believe CP2077 and HL in particular would use more than 80% of the available buffer and I'm sure there are more examples with the lack of optimization effort some studios put in these days.",Neutral
Intel,No way I've had my gpu running out of VRAM with 12gb.   Ratchet and clank at max settings 1440p ultrawide.,Negative
Intel,"They want the most FPS and best performance they can get, so they buy the best hardware they can get. This is not hard to understand.",Positive
Intel,Because the only 2 options are an 8GB card or a 5090  What,Neutral
Intel,"I understand what you think you probably saw; but cyberpunk just does not actually use 11GB at 1080p even with RT on. That game came out 5 years ago, and does fine with 6GB GPUs. Cyberpunk was in development when the 1080ti was the top GPU available. You think they developed a game that maxed out THE top available GPU at 1080p? When the game came out the 2080 ti could max it out at 4k with 11GB.",Neutral
Intel,"Talk all you want, you will never convince me that pros need a 5090 to play Starcraft Brood War.",Negative
Intel,"you maybe right (perhaps I'm remembering the stat for the time I tried path tracing with a non rt card for science [https://www.techpowerup.com/review/cyberpunk-2077-phantom-liberty-benchmark-test-performance-analysis/5.html](https://www.techpowerup.com/review/cyberpunk-2077-phantom-liberty-benchmark-test-performance-analysis/5.html) ) guess it was probably closer to 8, Hogwarts Legacy was definitely well over 10 though.",Neutral
Intel,That's probably largely due to the game using low quality textures which is made worse through noise reduction.,Negative
Intel,Bro it's 2025 no one is playing a game from 1998 except the like 20 Koreans left in ASL. It's not an eSport anymore. It's also not even a game where FPS count matters. Quit your larp.,Negative
Intel,"Wait, do you think ""esports players"" just means people that play them professionally?",Neutral
Intel,I'd personally suggest the 9060xt.  The 9000 series are spectacular tbh...,Positive
Intel,If it's the 16gb 9060 xt I'd say the upgrade is worth it,Positive
Intel,If its the 16gb 9060xt then absolutely that. Thats the one i just got for my first pc,Neutral
Intel,"It honestly depends on what your needs are... Fps, solo campaign, rpg, streaming, vid rec., etc ... Do u have an interest in A.I. or M.L.? RX is amazing for raw frames n streaming. ARC is amazing for AI n ML so it depends on what your use cases are",Positive
Intel,9060 XT and get the 16GB version.,Neutral
Intel,is the current price still good with it?,Neutral
Intel,the pricing would be my real question as it seems to be too much for both,Neutral
Intel,"they're both overpriced, is this USD?",Negative
Intel,Nah. MSRP is 350. +/-50 is what I'd accept. 100+ more is crazy. That's 30% upcharge.,Neutral
Intel,What country are you in? It would depend on local pricing and what alternatives are priced at. Sometimes countries are just expensive unfortunately,Negative
Intel,usd equivalent of php,Neutral
Intel,where can we get msrp ones im from the philippines.,Neutral
Intel,"if I had to overpay for something, it'd be amd over Intel.  Especially because their new CEO has been threatening to discontinue their GPUs",Negative
Intel,I see. Are the 9060 xt around those price?,Neutral
Intel,thanks for that info ill keep that one in mind,Positive
Intel,on stores here yeah around those price,Neutral
Intel,I see. Then I guess it's fine to buy it. It's a much better card that the b580 by 25%.,Positive
Intel,"If you go with a smaller case you can get a better GPU.  [PCPartPicker Part List](https://pcpartpicker.com/list/RDQQv4)  Type|Item|Price :----|:----|:---- **CPU** | [\*AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $178.51 @ Amazon  **CPU Cooler** | [Thermalright Assassin King SE ARGB 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/9Gstt6/thermalright-assassin-king-se-argb-6617-cfm-cpu-cooler-ak120-se-argb-d6) | $18.59 @ Amazon  **Motherboard** | [\*Gigabyte B650M GAMING PLUS WIFI Micro ATX AM5 Motherboard](https://pcpartpicker.com/product/9HTZxr/gigabyte-b650m-gaming-plus-wifi-micro-atx-am5-motherboard-b650m-gaming-plus-wf) | $125.00 @ Amazon  **Memory** | [\*Silicon Power Value Gaming 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/cCKscf/silicon-power-value-gaming-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-sp032gxlwu60afdeae) | $85.97 @ Silicon Power  **Storage** | [TEAMGROUP MP44L 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/2x4Ycf/teamgroup-mp44l-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-tm8fpk001t0c101) | $54.99 @ Newegg  **Video Card** | [\*PowerColor Reaper Radeon RX 9060 XT 16 GB Video Card](https://pcpartpicker.com/product/fzh2FT/powercolor-reaper-radeon-rx-9060-xt-16-gb-video-card-rx9060xt-16g-a) | $379.98 @ Newegg  **Case** | [Cooler Master MasterBox Q300L MicroATX Mini Tower Case](https://pcpartpicker.com/product/rnGxFT/cooler-master-masterbox-q300l-microatx-mini-tower-case-mcb-q300l-kann-s00) | $39.99 @ Amazon  **Power Supply** | [\*Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $89.90 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$972.93**  | \*Lowest price parts chosen from parametric criteria |  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-09-07 23:06 EDT-0400 |",Neutral
Intel,"Looks like you hit the nail on the head for a budget ddr5 gaming system, maximising performance per dollar IMO.   You could try a 9060 xt, to get 16gb of vram to help give your GPU more legs to play longer since a 7500f should still be well suited for that GPU. It is likely about $100 more though.   I see nothing wrong with this build for what your assumed goal is, you could probably squeeze more out of your budget with a different mobo, case, and cooler and upgrade the GPU at most and stay in budget.",Positive
Intel,Bit over budget  https://pcpartpicker.com/list/HfKWh7   In budget  https://pcpartpicker.com/list/jJCGC8,Neutral
Intel,"you can shave of like $25 off the SSD with a 2tb $100 SDD instead. You can also shave 17$ off the cooler by getting a regular assassin instead of the double radiator version, since you don't need a double radiator to cool the 7500f. And you can save money on the power supply by opting for a 650w supply instead. Gives you plenty of wattage overhead as well. All those savings can allow you to get a better GPU like the Radeon RX 9060XT 16gb",Neutral
Intel,"Could also do a 7700xt, slightly worse performance and 4gb less vram, $50 cheaper  https://pcpartpicker.com/product/97kH99/xfx-speedster-swft-210-core-radeon-rx-7700-xt-12-gb-video-card-rx-77tswftfp",Neutral
Intel,"buy a better gpu because it sounds like you're really only interested in playing games at max fidelity, so a good GPU will take you way further than any monitor currently can. IMO, a good/great monitor should be the last thing that you get because if max frames is the goal, it's smarter to see where your personal cutoff is (in regards to resolution/fidelity with your current setup) and whether or not you're really wanting to buy pricier hardware to max out your current monitor or upgrade to a monitor with higher specs or a different panel type like W/QD-OLED",Positive
Intel,I mean in short i have 2 choises intel arc B580 and QD/W-Oled monitor or Rtx 5070/5060 ti and a normal not good not bad monitor but is the monitor worth it? cause im not sure is an OLEd that big of an diffrence,Neutral
Intel,"There is definitely a noticeable difference between a nice IPS panel monitor and a QD/W-OLED monitor, there's absolutely no debating this. But where objectivity ends, subjection begins and this is where your personal standards come into play. For me personally, growing up playing games on low resolution CRT monitors, I only care that I can play my games of choice, so I would personally be happy with a decent IPS monitor because the games won't care if I'm playing on a nice monitor or not, but they will care what hardware I'm playing on. So I would go with the 5070/60 Ti and a decent monitor. The B580 will struggle more with a nicer OLED panel anyway due to the more demanding specs.",Neutral
Intel,A B580 won't be able to properly show off such a nice monitor because it will run so slowly and you won't be able to turn the graphics settings up.,Negative
Intel,I mean i can test the oled and refund it if its not good enough,Negative
Intel,I mean with some ai FG i have 80 frames with high/cinematic settings in BMW its not that bad,Neutral
Intel,"You can, but you'll end up with initial disappointment because you don't know what you want yet. I'm giving you advice that will help you figure out what you want. But I guess I can't force you to take it. Good luck on your search.",Neutral
Intel,Thanks for Gl ill propably test it and refund if its not good enough,Neutral
Intel,"Everybody was talking about it when it came out. At MSRP, it’s a good value   It was just a bitch to get because it sold out instantly everywhere. I’m sure that’s changed now. It’s a good purchase   It also requires a decent CPU, that’s the only issue",Positive
Intel,They make up something like 2% of the total GPU market.  You don't see a lot of talk about them because so few have them.  They are ok for what they are.,Positive
Intel,They’re really good for what they are and are pretty universally well received.   They’re budget tier cards with a limited lineup that simply don’t have the same recognition and acclaim as Nvidia or AMD.,Positive
Intel,"The Intel arc GPUs used to be avoided because of the drivers. Some games straight up don't work with them, some had way worst performance than the competitions. Intel's driver has improved leaps and bounds, it's a great one to get unless you have a really dated CPU.",Negative
Intel,"I just got one and it’s awesome, $395 AU and I got a nice increase at 1080p. Made it very cheap to upgrade!   I think my end game is to try get the next line of graphics card at early retail price, or just hop on the next Intel… tbh done me great. The only problem I have had so far was tabbing out of helldivers caused a few glitches on my mates end for the 15 seconds I was gone.",Positive
Intel,"Their A series gpus had some major issues alongside drivers still being pretty raw. The a310/380/580 have some niche uses in video encoding or for driving 1080p displays.   The B series cards have turned out to be really good budget gpus for 1080/1440p. The drivers have come a long way since launch, theyre still not as refined as Nvidia but theyre mostly stable now. They both require more modern platforms to perform well but not the top end hardware alot of reviews claim they need. The b580 has had alot of issues staying in stock since it offers 12gb of vram at a price point that almost exclusively offers 8gb cards.  Ive had my b580 since around launch. I got it on launch day for MSRP and aside from an unstable driver update i had to roll back its been a great upgrade from the 1070 i had previously. If you play on 1080/1440p displays at high settings theyre good cards",Neutral
Intel,"Intel arc a310 is one of the best cards for transcoding videos, can handle multiple 4k streams. I love mine, using it for plex server and immich",Positive
Intel,It's good,Positive
Intel,No they aren’t. The b580 in particular is praised for its great value at its price range,Positive
Intel,As long as your CPU and motherboard supports RE-Bar it's a great option for 1080p/1440p for the price,Positive
Intel,Idk but I take it as a good sign because people will only post if there is something wrong with the product. This also applies to other subreddits.,Neutral
Intel,"They're good and I have one that I use for a rig that only plays games like esport titles and multiplayer steam games. They aren't the best for any type of AAA game.  The bigger deal is that intel has announced some pretty heavy changes and cuts coming and there is good speculation that intel's gpu sector might be cut. If it doesn't happen, it's no big deal, but if it does get cut or restricted, any type of updates or driver updates that are needed for future releases will probably be unlikely to come out. That's probably the only reason I wouldn't recommend it to anyone, but for the price its great for someone who only enjoys current multiplayer games or indie games.",Neutral
Intel,"Intel Arc has physical hardware that works ok, but it's not high performance yet.  But MOST importantly, this is a new company in terms of graphics drivers and industry cooperation in software development. So what? NVIDIA and AMD partner with game developers to optimize graphics, sample and super-sample textures in AI for optimization for best performance. This is an ongoing development of specialized software and tools to improve performance to the maximum with the given hardware. This software is NOT available to Intel and they will have to spend years catching up to their competition.  In short, Intel graphics cards will work fine for low demand popular programs. They may eventually get there for high end performance, but it will take years for them to catch up.",Positive
Intel,"Have you considered 9060 xt at all? There has been a price decrease recently, you might be able to get one for the same price as a b580 (8gb model 9060 xt).  Except for the vram department, the 9060 xt is pretty much better in every way and performs better too.  Was on the same boat with you. The b580 was $360 CAD, and the 9060 xt 8gb was $380. After going through countless benchmarks (especially at 1440p), I eventually decided to pick the 9060 xt sapphire.   In only like 1 or 2 games the b580 scored higher, and everything else was in 9060 xt’s favour. Idk much about the driver issues on the b580, but at least the 9060 xt seemed like the safer bet. I wish it was at least 10gb though…. Amd really missed an opportunity. Otherwise this gpu is a beast! I’ve never seen an 8gb gpu do so well at 1440p. Amd did score with that.",Positive
Intel,They are good for 1080p gaming also if you have a really low budget and want to game. For slightly more money you can pick up a 9060xt 16gb which is good for 1440p and has better long term returns.,Positive
Intel,"They're okay, if you hate yourself get an Intel Arc GPU otherwise get an AMD Radeon GPU or if you have the money to burn literally and figuratively speaking an Nvidia GPU.",Neutral
Intel,I think better price to performance can be found on a used card.,Neutral
Intel,"Has driver issues with more dated and/or less popular games, but they've made it a point that major AAA titles and esports games usually have good drivers. It also basically requires a fairly beefy CPU and it will at least slightly negatively impact the CPU performance too. But when I say fairly beefy I basically just mean AM4 5000 series or basically any AM5 CPU even the 7500f, I personally don't know the Intel comparison but I imagine any Core Ultra or 12th gen would absolutely crush it and be fine.  But the B580 is basically competing in the 60 series tier and is alright, but definitely if you can get it for MSRP or within I'd say even $30 over MSRP it's a steal. Something of note too is at least from my experience and seeing benchmarks, the Arc GPUs normally do this split the middle kind of thing, where they tend to outraster/render Nvidia GPUs of their tier, but get outrastered/rendered by AMD GPUs of their tier, but on the other end XeSS upscaling/frame gen performance and raytracing usually outperforms the comparable AMD GPUs, but loses to the comparable Nvidia GPUs. Although FSR4 and the better raytracing on the 9000 series might've changed that, a lot of those comparisons were from when we were still comparing to the RTX4000 and RX7000 cards.",Neutral
Intel,B series is getting a bit old now. Eagerly awaiting what the C series will bring.,Neutral
Intel,No idea about the drivers but B570 and B580 which released some months ago are very good value and also very efficient.,Positive
Intel,"The B580 is actually very good for a $250 card as it performs similarly to the 4060 and more importantly has 12GB memory instead of 8GB. Even today it's still better value compared to the 5050 at the same price.  The main issue with the card is availability in certain regions, which impacts it's price. Where I'm from (Bangladesh) availability is actually pretty decent and so most B580s can be had for at or near the $250 msrp. The other issue with the card is it's driver overhead, which mostly shows up on the AM4 platform where the performance delta can sometimes be so massive that the card ends up slower than the 3060 12GB.  The issue is far less rampant on AM5. It performs around the 4060/5050 on a cheap R5 7500F, and is already beating the Nvidia cards in terms of value. It can still go faster if you pair it with the fastest CPU on the planet, but not by much and I personally don't think it's that big of a deal.   Alternatively you can play at 1440p and eliminate the driver overhead; the card has the performance and memory for it anyway, so why not. You can even go all the way upto 4k Balanced with XeSS 2.1 which is actually pretty good and supports frame gen. Kinda insane for a $250 card if you ask me.  Driver support and game compatibility is nowhere near as bad as Arc Alchemist, and people have even played the Battlefield 6 Beta on day 1. Older games such as those on DX9 or DX10 may still struggle compared to a 4060/5050, but the card is so fast you should still get highly playable framerates. We're talking 150 fps instead of 300fps, big deal.",Positive
Intel,No one buys them because people truth internet myths even from years ago but the reality is that those are very good gpus and near to MSRP are a great value.,Positive
Intel,"They're decent GPUs but for gaming yo'ull want something a lot more powerful, like an RTX 5070 or RTX 5070 Ti.  [https://www.youtube.com/watch?v=VcKPJvGNr0c](https://www.youtube.com/watch?v=VcKPJvGNr0c)",Positive
Intel,Nobody talk about the A series anymore. Is it still worth to buy?,Neutral
Intel,"You must think all of us can afford Nvidia. In Europe you guys probably can, but in America all of us have millions in debt from medical bills and we make minimum wage!",Negative
Intel,So for exemple an 7500f is ok ?,Neutral
Intel,"> unless you have a really dated CPU.  Read: more than a generation or two old. So, not actually dated at all, which is why the overhead issues with Battlemage were so criticized.",Negative
Intel,"I ran an a750 in Linux until Monday. It worked well.  Rarely had problems.  I just wanted more vram so I got a 9060xt. If Intel had an upgrade path for the a750, I would have bought one.",Positive
Intel,Idk if I'd call Nvidia refined either.   I was still using the 566.36 driver until the Battlefield 6 beta made me update and since then I've encountered the flickering display and sudden black screen issues a few times that have been going on with pretty much every driver version since the 50 series came out.,Negative
Intel,"Rebar isn't all it needs. It needs a good CPU. I bought one to replace a 7600, paired with an i5-10400. After extensive testing it didn't really offer any improvement, and even performed worse in CPU-heavy/multiplayer titles. I thought the overhead issue was overblown, it isn't.",Negative
Intel,It really cant,Negative
Intel,Like which one ?,Neutral
Intel,B series launched literally under a year ago.,Neutral
Intel,Depends. For gaming? Not really. Better bargains. But for a home server the a310 is the champ of media encoding,Neutral
Intel,"In gaming, the B series will generally be better in pretty much every way. It just runs faster, cooler, and cheaper. Outside gaming it gets a bit weird, but actually favors A series. The 310s are *very* good at media encoding for the price. The A770 can also outperform the B series in some non gaming tasks (generally its inferior, but it can get 4gb more vram and has a wider bus, which can make it better, or sometimes close enough that it being older and thus cheaper can make up the difference.) For gaming, B series is strictly better",Positive
Intel,The b580 is close to the a750 in performance. Not worth buying that. The a770 is better but I wouldn’t recommend them now with the amd 9060xt out.,Negative
Intel,"The median net worth in the US is 200k and the US accounts for more of Nvidia’s revenue than the rest of the world. Speak for yourself, big bro.   Regardless, what even prompted you to say this? I’m just answering OP’s question about Intel’s popularity lmao",Neutral
Intel,Totally fine.,Positive
Intel,"Given you dont need to run DDU or (in some cases) do a fresh install of Windows to get the cards to run decently id argue theyre pretty well refined.   Even compared to AMDs drivers Nvidias just tend to work without much effort, from what ive been seeing it sounds like bf6 has had a ton of issues regardless of which gpu youve got. Having a set of rough updates off the back of 20+ years of development doesnt discount the amount of time theyve had to work on getting things working well. Intels 1st dgpus came out in 2022 IIRC",Neutral
Intel,Sure it can. You can get a 3070 at the cost or a 3070 Ti for a little more and it beats the B580.  I just sold my 3070 Ti for $290 a month ago because that's all people were willing to buy at. Some sold the card for even less.,Positive
Intel,"A used b580 is better price to performance, they’re just kinda speaking arbitrarily",Neutral
Intel,3070 or 3070 Ti. One may cost more. I sold my 3070 Ti for $290 and others are selling for less.,Neutral
Intel,"Wrong, the B580 outperforms the A770 in every case i know.",Negative
Intel,You’re comparing the used market to the new market. You can get a used B580 for $170 on ebay as the first listing I see.,Neutral
Intel,"Not arbitrary with my assumption they want to spend $250 on a new B580. I don't know for sure they wanted to go new, but in case it was their plan, I'm saying instead of spending $250 on a new one if that's what their plan is, just spend $250 on a used card which will have better performance. It's really common sensical as to what I am saying. Don't understand what is arbitrary about that.",Neutral
Intel,"Wrong. The A770 has more VRAM and works better for AI/ML workloads.  That's why they're releasing other cards to help with loading models.    As for gaming performance, the A770 and B580 are very close in most benchmarks.  It's game dependent which one does better, but in scenarios more vram is needed the A770 wins.  12GB isn't terrible for 1080p/1440p workloads but it's just OK.",Neutral
Intel,"Indeed, that was my intent and my assumption that OP was going to go for new. $250 spent on a used card is spent better than on a new B580.  I don't think this point is invalid to the point where it should be downvoted.",Neutral
Intel,"Ok… but if OP is willing to buy used cards, $170 is better spent on a used B580 than $250 on a 3070 as you can spend the extra $80 to get other components that are better as well",Neutral
Intel,"Sure this is true, but I was only talking about going for a brand new B580 versus a used GPU. I naturally assume at such a low price point that people will go brand new on that GPU rather than used.  You can argue different scenarios with a different good answer, but what you stated wasn't exactly relevant to my assumption.",Neutral
Intel,"Yes, but your assumption is inherently flawed is all I’m pointing out.",Negative
Intel,"No, my assumption isn't flawed when I had no evidence to suggest that OP was going used on that card. How can I possibly know what OP intends on doing without it having been explicitly stated? I would personally expect them to go for a new card and not used as the card is already cheap enough to get it new (without considering budget constraints, but they also never stated any details on a budget and other essential info).   Hindsight and what comes after the comment was made is different, but my comment was based entirely on a very specific scenario. I think it's clear from my comment that I was specifically discussing differences between a new B580 and a different used card. I wasn't talking about anything else.",Neutral
Intel,"Your comment here already dismantles the scenario you thought of. You claimed you assumed they would be going for a new card. This assumption automatically means that going for a used card is out of the question, so comparing the new card to the used card doesn’t make sense. If OP were to be considering going used on cards, why would they not consider going used on the B580? The problem is that you didn’t think to expand the logic you presented beyond one specific card, which is inherently flawed.",Negative
Intel,For what purpose? Because if gaming is your goal this is not the play unless you’re refusing both AMD and Nvidia graphics for ideological reasons.,Negative
Intel,"So my goal is to get you to a RTX 5070 which requires $280 of reallocation.  - Downgrade the CPU to a 9600X. $160 saved  - You have three M.2 slots on your motherboard, you don’t need 4TB to start. Drop to 2TB $130 saved.  - Replace the Arc with an RTX 5070  Alternatively you can make a less severe CPU cut with the 7700X ($60 saved) and get the other $100 from your peripheral budget because frankly you don’t need $500 for all that. You could also downgrade the case to something with less fancy rgb (Montech XR, Phanteks XT View) but I understand some folks are very passionate with their case selection.",Neutral
Intel,It is a little cheaper and I do a little bit of video editing on the side!,Positive
Intel,"This is honestly really good advice, I might make the jump to a 5070! Thank you!!",Positive
Intel,"The first release might be sold with a new PC. If you live in US, you might as well go to China and buy one there (or one of their online stores). Tarif and scalpers might jack up the price by a lot. This is not Intel, but he went to Hong Kong to buy the rare 5050 and found a few of them easily. https://youtu.be/-jXMaS8e8oo?si=6M1XiK54SrVDI3h9",Neutral
Intel,"For the 48GB Maxsun dual variant of the Pro B60, I was quoted $3,300 New Zealand Dollars (NZD) per GPU. Not available in retailers, only to business customers through limited suppliers. The supplier who services my area doesn't even have a website.   For comparison, the RTX PRO 5000 with 48GB is $8,800 NZD, and the cheapest RTX 5090 is around $4,650 NZD at the moment.   I was interested initially because of the RAM in the dual Maxsun card, but was hoping for a price closer to $2,000 NZD.",Negative
Intel,Why specifically the Intel when plenty of GPUs aren't going to interfere with AI.,Neutral
Intel,That's an interesting idea :) I'm not in the US but going to China to buy some hardware sounds very cool! Maybe I'll do it some day. Thank you.,Positive
Intel,"My idea was to buy a singular one now and another one later when I have the money for it. But dual variants look cool too if you can afford it. $2000 USD sounds too much though. I feel like the supplier in your area is jacking up the prices since it is difficult to find them.  I asked a friend of mine who does business hardware deals. He couldn't even get a quote. I guess it is not even available on many regions now.  Anyway, thanks for the info :)",Positive
Intel,24GB VRAM for 500$ sounds super nice and not offered by any other GPU manufacturer. Plus they advertised a card with dual B60s (48GB VRAM total) sharing one PCI-E x16 slot (x8 each).,Positive
Intel,"Then price the GPUs that have Tensor cores. I'm just wondering because Intel GPUs aren't traditionally for gaming, though that is slowly changing",Neutral
Intel,"Tensor cores is not the only thing that matters when it comes to performance. B60 performance was looking pretty good in the demos but now there is nothing on the internet, no benchmarks, no prices, no people with crazy connections showing off a hardware others cannot buy, nothing.",Negative
Intel,You have a very narrow vision of speed. What you are saying is like your truck is faster than my audi because it has more horsepower. Nvidia is the top dog right now but that doesn't mean you get more from nvidia for 200$. VRAM size and speed is something you cannot ignore. If you have to load half of the model to system memory it is gonna decrease your speed significantly. And guess what? If you can't fit the model into your memory you won't be running it even if you have a billion cuda cores. This is why intel is offering 24GB VRAM for 500$ while you need to pay 2500$ for the same VRAM size in nvidia (rtx a5000 for example).,Negative
Intel,"But compared head to head with an nVidia or AMD/ATI of the same price and the Intel is not the top performer, last I checked.   I looked into this recently because I was thinking about upgrading.  $200 for that versus $200 for an nVidia/AMD and the non-Intel GPU was a slightly better performer.",Negative
Intel,"But compared head to head with an nVidia or AMD/ATI of the same price and the Intel is not the top performer, last I checked.   I looked into this recently because I was thinking about upgrading.  $200 for that versus $200 for an nVidia/AMD and the non-Intel GPU was a slightly better performer.",Negative
Intel,"No i don't have a narrow vision of speed. Maybe ask questions first. Like i said, i read reviews, Intel didn't break through. Maybe I'm incorrect, but i could care less. Reviews change daily. And i could find a dozen that support my opinion. Not like i care. Intel said it wasn't primarily intended for gaming.  What's worse is your rudeness.",Negative
Intel,"Yeah, sure mate. You do you.",Positive
Intel,9060xt 16gb,Neutral
Intel,Thats about 100$ more expensive that the two I mentioned. Is the improvement worth it for the price? If not I would rather go for the lower price range.,Negative
Intel,yes you can run games at 1440p fine,Positive
Intel,"The P3 Plus uses QLC flash, which has low durability and becomes noticeably slower after a few years. Get a better drive with TLC flash like the Kioxia Exceria Plus G3, TeamGroup G50, Intenso MI500, WD Blue SN5000, or Verbatim Vi5000.",Neutral
Intel,ASrock b850 motherboards have killed x3d cpus recently. While your 9600x should be fine a future x3d chip that you may want to upgrade too years down the line may not be. I’d look at a different board for your “future proofing” spec.,Negative
Intel,"pcpartpicker is not great with german pricing imo, use geizhals instead.  [https://geizhals.de/wishlists/4616811](https://geizhals.de/wishlists/4616811)  here i have assembled a very similar setup with a slightly better cpu and bigger ssd for \~1050€ with shipping.",Negative
Intel,"Thanks m8, I didn't know this. Already looked up your suggestions and they have a decent price, I will adapt my build accordingly.",Positive
Intel,"Oh my, thats an important information 😅, thanks for the info, I will look for an alternative",Positive
Intel,"Oh sweet, thanks m8👌I really like the cpu upgrade",Positive
Intel,"yes just get a new gpu. The cpu is still alright. Also make sure the psu is good, check the model.",Positive
Intel,Do you have a suggestion on a good gpu should I get one of the newer ones I mentioned in my post or something older I searched around and found some decent deals on gtx 1080 gpus.,Neutral
Intel,"the 1080 is outdated. The one you listed are good, see what deals you can find",Negative
Intel,"Okay, need to do some price checking thanks for the help.",Neutral
Intel,"Driver support in games is very good these days, I have had considerably more issues on my NVIDIA PC than on my arc PC, over the last six months or so (I've also used that one more but still felt like a worse overall experience in terms of gaming drivers). Productivity software is a different story and a bit hit or miss, in my experience especially Adobe Photoshop and to a lesser extent After Effects don't play nicely with Arc.  XESS 2 is also really good, I think right now it's generally the worst upscaler between itself, FSR4 and DLSS 4 but it's by a very small margin to a point where it depends on a game by game basis. Biggest problem is the lack of widespread adoption. XESS framegen works great.   Ray tracing compares pretty well to competing AMD and NVIDIA cards, in terms of raster performance to RT ratio it seems to land somewhere in the middle between RDNA 4 and Blackwell. That being said it's still a low end card, so you're not gonna be running heavy RT workloads in it.  I think the biggest issues are the higher CPU overhead, requirement for rebar and only supporting an x8 PCIe connection, somewhat limiting it as a viable budget upgrade for older systems.  It's more of a 1080p card for modern AAA games, that can work with a 1440p monitor in a pinch. For $260 I think it's decent and NVIDIA and AMD don't really offer anything better in the sub $300 range.",Negative
Intel,"Using my 3-fan B580 OC for 1440 gaming and it does a good job. I love its non existent noise level. Back in march I bought it for 330€. Updated driver is released like every 1-3 weeks. I appreciate intel's clean and intuitive driver GUI. I love the way the driver handles crashes, it's exemplary. The driver literally invites to try out some OC. Don't regret my B580. If I'd buy a new card right now, I'd be torn apart between B580 or 9060 XT.   But my main goal was/is a decent gaming experience with a  reasonable power consumption. I don't have a problem to fine tune settings to get stable 60 FPS and a flatline frametime. And it was a decision against NVIDIA, against their dominant position, which they exploit and hurt the market.",Positive
Intel,It has bad driver overhead and isn't great for now end systems.  It also doesn't have that great price to performance now that both AMD and Nvidia have released newer gpus.  I would take the 5060 over it at 1080p.  Far less driver overhead for Nvidia.  Dlss 4 is far better than xess,Negative
Intel,"To some degree, yes. But don't expect amazing energy efficiency with the card, its somehow the same performance as the RTX4060, but is 190 watts. Even if the 12GBs of VRAM makes it suitable for 1440p gaming",Neutral
Intel,"While I’m sure you want to take advice from someone with that username all day, yes it’s a good card. It’s certainly better for 1080 but especially with a string CPU is going to do most things very well. I have no issue at all with mine and it often stays so cool the fans don’t even turn on.",Positive
Intel,"I'd say find a used card if you can as you'll usually get better value out of a used card. B580 is not a 1440p card, it's a 1080p card.",Neutral
Intel,its a 1440p card wdym,Neutral
Intel,"Looking at benchmarks, it is really quite underwhelming compared to my old 3070 Ti (which I just sold for $295), which was enough to do 1440p at 144FPS at medium settings. It's definitely not a 1440p card that's going to give you high fps on modern titles. imo a proper 1440p card would be more like a card like a 5070 Ti which can actually do 1440p 144+ fps at highest settings, but 1440p and above with highest settings aren't really budget territory. You'll find that a majority of PC gamers are still playing at 1080p because 1440p isn't offering a great experience on cards lower than a 3070. And I would do the same myself if I had such a low end card.",Negative
Intel,"The 3070ti is a good card, but not that high in performance. Also that card WAS a 1440p capable card, now it can't go any higher than 1080p due to a lack of VRAM, not unless you had that rare 16GB version (which only a few were made)  The B580 can still very much do 1440p 60fps ultra quality, since ultra quality in games is more suited for 1440p compared to maximum settings",Neutral
Intel,"Most tasks in 3D/CG/VFX are predominantly single threaded, until you simulate (not always), render or export something when more multithreaded performance is nice to have available. A 7600 has solid single core performance and in day to day performance will be great.  2x8GB is a mistake though, at least get 2x16GB - preferably 6000cl30 (cl32 and cl36 are fine if the price difference is big). You can save on the cooler for now, use the stock one (comes with a 7600) until she has enough saved for an upgrade.  The Intel Arc A750 is certainly a contentious choice. The only reason I'd go arc at that price point is if she'll be doing a lot of high res video editing, otherwise I'd look at alternatives, possibly second hand. A 3060 12GB would be a nice to have for 3D as NVidia is completely dominant in that space, but you'd probably need some second hand luck; I find them for around 200 EUR in the Netherlands. It'll perform a bit better than the A750 in gaming too, and has way better software support.  You can always save on the case and may be able to shave off a bit going for an mATX board, though less than 140 EUR for a wifi one is unlikely.",Positive
Intel,"All parts look good but something to consider is some software just runs better on nvidia than others. I know this is true with video editing so maybe look at any applications she’ll need to run and see how the performance is before going with the intel card.   This is as good as I could manage being 100 over budget assuming the applications for vfx, cgi etc would play nicer with an nvidia card but don’t know for certain.   [PCPartPicker Part List](https://fr.pcpartpicker.com/list/G3VgWc)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 7500F 3.7 GHz 6-Core OEM/Tray Processor](https://fr.pcpartpicker.com/product/ms4Zxr/amd-ryzen-5-7500f-37-ghz-6-core-oemtray-processor-100-000000597) | €160.97 @ Amazon France  **CPU Cooler** | [Thermalright Peerless Assassin 120 SE 66.17 CFM CPU Cooler](https://fr.pcpartpicker.com/product/hYxRsY/thermalright-peerless-assassin-120-se-6617-cfm-cpu-cooler-pa120-se-d3) | €45.90 @ Amazon France  **Motherboard** | [MSI PRO B650-S WIFI ATX AM5 Motherboard](https://fr.pcpartpicker.com/product/mP88TW/msi-pro-b650-s-wifi-atx-am5-motherboard-pro-b650-s-wifi) | €135.83 @ Amazon France  **Memory** | [Patriot Viper Venom 32 GB (2 x 16 GB) DDR5-5600 CL36 Memory](https://fr.pcpartpicker.com/product/mbJgXL/patriot-viper-venom-32-gb-2-x-16-gb-ddr5-5600-cl36-memory-pvv532g560c36k) | €92.99 @ Amazon France  **Storage** | [Timetec 35TTFP6PCIE 1 TB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive](https://fr.pcpartpicker.com/product/GPPQzy/timetec-35ttfp6pcie-1-tb-m2-2280-nvme-solid-state-drive-35ttfp6pcie-1tb) | €54.99 @ Amazon France  **Video Card** | [Gigabyte WINDFORCE OC GeForce RTX 3060 12GB 12 GB Video Card](https://fr.pcpartpicker.com/product/8d4Ycf/gigabyte-windforce-oc-geforce-rtx-3060-12gb-12-gb-video-card-gv-n3060wf2oc-12gd) | €301.27 @ Amazon France  **Case** | [KOLINK KLA-003 ATX Mid Tower Case](https://fr.pcpartpicker.com/product/QKQKHx/kolink-kla-003-atx-full-tower-case-kla-003) | €25.00 @ Amazon France  **Power Supply** | [Gigabyte P750GM 750 W 80+ Gold Certified Fully Modular ATX Power Supply](https://fr.pcpartpicker.com/product/RhH8TW/gigabyte-750-w-80-gold-certified-fully-modular-atx-power-supply-gp-p750gm) | €82.95 @ Amazon France   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **€899.90**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-08-03 15:03 CEST+0200 |",Positive
Intel,well better go for 3060 that gpu is fine but it has some issues for a new users that can be challenging,Negative
Intel,"Most if the 3D / vfx software are optimized (written) for nVidia cards. I would avoid intel and AMD because of that. I've even seen some used RTX 3070 cards under 200 €. To avoid frustrating moments, i would rather choose something like that.",Negative
Intel,Which country in the EU is your friend currently in?,Neutral
Intel,"Hot take here.   That first year, I can guarantee she's not going to do anything heavy duty, if at all. That's what the school computers are for.  I would say save and purchase a quarter of the way or half way through her first year, when the needs actually come to head. It'll be less money spent overall for something nicer.",Neutral
Intel,* intel gpu is likely bad idea for someone who will use it for CGI etc - usually people recommend nvidia gpus for that with more then 8gigs of ram   ram - highly recommended getting 2x16gb as it will effect performance of everything and not wait for future upgrade,Negative
Intel,"You can get a ryzen 7 7700 from aliexpress for €150(i did have to pay 11 euros customs charge though) but it'll take 3 weeks to get to you.  As for the gpu, take the advice here and just get the 3060 12gb model. It's a great card for the money.  She will almost certainly need 32gb of ram. I have a laptop with a 4060 in it so nothing too intensive with 16gb of ram and was getting out of memory errors just playing marvel rivals so had to upgrade.  Get a cheaper case, you can get decent cases for 50 ish euros and money saved there will give you the extra for 32gb of 6000/30 ddr5 ram(think patriot viper 32gb(2x16)is like 90 euros on amazon.",Neutral
Intel,"that motherboard can take 4 RAM sticks so I thought that she will just add another 2 in the future.    As for the GPU - I forgot to check if the software has some preferences or not. I was considering either A750 or RX6600XT, but I guess she will have to pay a bit more for gpu.   Thank you!",Neutral
Intel,"Faster, but not white.  [PCPartPicker Part List](https://pcpartpicker.com/list/RDQQv4)  Type|Item|Price :----|:----|:---- **CPU** | [\*AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $179.99 @ Amazon  **CPU Cooler** | [Thermalright Assassin King SE ARGB 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/9Gstt6/thermalright-assassin-king-se-argb-6617-cfm-cpu-cooler-ak120-se-argb-d6) | $18.59 @ Amazon  **Motherboard** | [\*MSI PRO B650M-P Micro ATX AM5 Motherboard](https://pcpartpicker.com/product/LdHqqs/msi-pro-b650m-p-micro-atx-am5-motherboard-pro-b650m-p) | $109.99 @ MSI  **Memory** | [\*Patriot Viper Venom 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/4cCCmG/patriot-viper-venom-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-pvv532g600c30k) | $84.99 @ Newegg  **Storage** | [TEAMGROUP MP44L 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/2x4Ycf/teamgroup-mp44l-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-tm8fpk001t0c101) | $61.99 @ Amazon  **Video Card** | [\*ASRock Challenger OC Radeon RX 9060 XT 16 GB Video Card](https://pcpartpicker.com/product/BxRnTW/asrock-challenger-oc-radeon-rx-9060-xt-16-gb-video-card-rx9060xt-cl-16go) | $369.99 @ Newegg  **Case** | [Cooler Master MasterBox Q300L MicroATX Mini Tower Case](https://pcpartpicker.com/product/rnGxFT/cooler-master-masterbox-q300l-microatx-mini-tower-case-mcb-q300l-kann-s00) | $39.99 @ Amazon  **Power Supply** | [\*Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $89.90 @ Newegg   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$955.43**  | \*Lowest price parts chosen from parametric criteria |  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-08-09 15:39 EDT-0400 |",Neutral
Intel,The motherboard already has a heatsink for the M.2 drive (and it's larger and will work better).,Positive
Intel,"can you post the full link of the build from pcpartpicker.com?   I see some potential here to improve on the build and will address your questions, however I'd need to know how many VMs you'd be running to determine if more CPU-cores would be useful in your case. Also, are your programs dependent on CUDA?  edit: also, do you have a microcenter nearby? They have some very nice bundles which can save you a good bit of money",Neutral
Intel,"It looks like you may have posted an incorrect PCPartPicker link. Consider changing it to one of the following:  * [Use the Permalink](https://i.imgur.com/IW0iaOm.png). note: to generate an anonymous permalink, first click [Edit this Part List](https://i.imgur.com/uqDIcdt.png).  Or, make a table :    * [new.reddit table guide](https://imgur.com/a/1vo0GHH)   * [old.reddit table guide](https://imgur.com/C86vdxB)         *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",Neutral
Intel,"I have the same processor and they’re known to run pretty hot, as a result this makes my bedroom pretty warm. It’s a good cpu at its price but knowing what I know now I’d check for alternatives that are as powerful or better for the same cost or slightly more. Just something to consider",Positive
Intel,Thank you!,Positive
Intel,"Here is the link to my [pcpartspicker build](https://pcpartpicker.com/list/zNm8gn). In my studies, I've never needed more than one VM at any given time, and the programs I use are somewhat reliant on CUDA, but it's not essential",Neutral
Intel,"wrong assumption, just because it runs hot does not mean it dumps a ton of heat into your room. The used electricity is what determines how much it heats up the room, and GPUs use 3-6 times the wattage of a modern AMD CPU.",Neutral
Intel,"Thanks, I appreciate that insight! Tells me I probably would need more fans too",Positive
Intel,"That Montech PSU is likely only a tier C, doesn't deal well with sagging voltage on mains.  I'd go with something better and likely a bit more expensive such as one from Corsair.",Negative
Intel,"ok now it worked - there you go  [https://pcpartpicker.com/list/js2RPJ](https://pcpartpicker.com/list/js2RPJ)  cheaper parts, same performance. Unless you want the AIO for looks I wouldn't keep it or just use a cheaper Thermalright model with a 240mm radiator. So if you just need one VM a 6 core CPU is more than enough, if it isn't essential the extra VRAM of the card should also boost productivity.",Positive
Intel,your list is private,Neutral
Intel,"Thank you, I appreciate your help",Positive
Intel,whoops try now [https://pcpartpicker.com/list/zNm8gn](https://pcpartpicker.com/list/zNm8gn),Neutral
Intel,9060 XT is just like 10% better. Stay with the B580 until 2027,Positive
Intel,9060xt is prolly faster but for the price/p b580 prolly is the best rn  if u have the b580 then just stick with it until the next intel card like celestials or something unless ur running into issues with ur games i guess,Positive
Intel,"I saw you bought the b580. Considering it, can you tell me how it's doing so far? Any issues with any games?",Neutral
Intel,I wouldn’t fidget if I already had B580 coming in. It’s got 12GB of VRAM and quite a bit cheaper than 9060XT 16GB and not that much worse of a performance. Especially if you’re planning big upgrade in 2027,Positive
Intel,I have the Steel Legend B580 and Ive had corruption and cold crashes with Hitman 3 and the latest drivers removed my optimizations as the features were gone!  So it has been frustrating if not annoying however it makes me sad to get rid of it like the others say when it works nice thats nice.  I'm calling the B580 a hassle but its not junk its simply not getting top tier support for layered driver support.   Bought the Sapphire Pulse 9060XT OC 16Gb and it matches is little siblin' the 6600XT OC i still use on my second PC.  Hitman 3 for me went from 127 FPS down to at least 60 fps with the former Tessellation off.  The Radeon is everything the B580 promises actually.  The AI cores on the B580 were interesting. I lost about $30 in the mix but Ive had the B580 all Summer.  Sad,Negative
Intel,"If you are not having any issues, then why change? The B580 is a pretty good card (I recommend watching LinusTechTips video on it, 2 people changed their 4090s or high end cards and used ARC for some time). Plus, its performance is just getting better wich each driver update, and more games are adapting XeSS faster than FSR (lol).",Positive
Intel,"okey thanks for the suggestion, good day mate!",Positive
Intel,"Daniel Owen did a comparison and it's like 30% or more difference between the two, not 10%, so imho if he can, just get 9060xt 16gb, he get 30% better performance with more vram for 40% more cost if we use MSRP pricing for both, but yeah if he choose to get B580 now then there's no point in upgrading to 9060xt, 30% is hardly a worthwhile upgrade, and B580 already has enough vram at least until next gen console arrived",Neutral
Intel,"well I had no problems so far, I played quite a few games so far already, never had an issue",Positive
Intel,Its pretty weak with tarkov and stalker 2 other games run well,Negative
Intel,..and if you haven't made the buy yet the extra $70 is well worth it. Likely $85 difference.  Count the extras. This thing trades hit with a 5060 Ti and the B580 somewhere well under and can perform nice (not missing much)  for you there for the $ but the hassle!,Positive
Intel,"Sounds like I made a good deal. Actually, I am yet to experiment with the Arc card, as I am getting the card today. Can you tell me what should be the ideal way to test the new card and what should be the ideal temps while gaming? (ASRock B580 Steel Legend 3 fan) I got",Positive
Intel,"Nice..i just bought one and tested it on BF2042 and it's incredibly better than my 1070ti. For such a low price, I'm happy.    I don't agree to pay ridiculously overpriced price for Nvidia stuff. They have a kind of Monopoly and allow themselves to squeeze everyone like lemons.. It's about time this change. Too bad if Intel cards are not as fancy. As long as they are not expensive and can run games decently well. I'll boycott Nvidia until they stop this madness. But when i see that people are crazy to accept 2000$ for a card, they really have no reasons to stop their scam!",Positive
Intel,Actually purchased b580,Neutral
Intel,..B580 had it all on paper.  B580 CAN play ball but will it today ....  Doesn't deliver well enough for your money.  First game that is annoying you will be doing what I did.  Its pretty nice at $250 MSRP only as others declare simply because of the cheap feature set. Should have performed around the $350 mark and we all expected it to but a long time ago.  This is where the Enthusiast market was excited.  $369 Sapphire Pulse 9060Xt OC 16Gb is about as good as it gets right now.  Then again I drive a 2018 Dodge Challenger V6 AWD not the V8 HEMI.  lol  Two different franchises!,Negative
Intel,"I should not worry for temps, the B series improved efficiency... Just play your games, and actually, the B580 has better performance on 1440p. Not literally, rather than it performs better in higher resolutions.  And look into OC the safe way. Which is basically increasing power limits and adding boost, which makes the card to clock higher with same voltage, isn't dangerous.  And INSTALL ONLY WHQL DRIVERS. Basically this one's are like the Polished version of the first release. Like, they send v1, people download and test it (it's not a beta driver, is supposed to be stable), complain about issues and later they send v1 WHQL, which is like the 100% stable version.  I never had issues with any WHQL drivers. People that installed the newest (not WHQL) always have problems.. I suggest joining the Arc sub and ask any question you have.",Positive
Intel,12gb is more than enough for 1080p,Positive
Intel,"you arent the sharpest tool in the shed are you mate, the B580 doesnt have cpu bottleneck issues, it just has compatibility issues with AM4, I run it with a 7600x so this guy with a 7700 wouldnt have the slightest of issues.",Negative
Intel,What did you pay?  I will miss mine a pinch as it was weird. I am currently marketing my 3d Voodoo 5 6000 for auction. Good luck you may have exactly what you need.,Positive
Intel,"you're the champion, thanks for explaining it much detailed appreciate it, and [https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html)  is this one you talking about?",Positive
Intel,"I paid more than 300 USD because the market is just like that and for 9060xt it's over 500 here   So I had no options, I didn't played hitman 3 yet, btw did you tried the cracked one or from steam?  Game crashes for many reasons, so far I played 5 games in my PC, valorant, vanguard, carxstreet and the finals  I got no issue of crashes yet, temps are at 50 at idle.",Negative
Intel,"Yes, sorry for the late response, reddit was having problems and didn't send my message. As long as you use that one, everything should go smoothly. Feel free to dm me or ask on the arc subreddit for any questions/help.",Neutral
Intel,"Yeah, something which op clearly doesnt care about",Negative
Intel,You wont have temp problems unless you live in the desert with no AC,Negative
Intel,Mine was exactly 300. I like its AI cores.  It produces way better images than my 6600XT and noone but noone simply comments on overall image quality ..anyway I have hundreds of legal Steam games no cracked eh I also own the Epic games version.  See this damn card had a driver update while I had it that improves the performance and that's good its what we want but once the features drop and performance drops (for me anyway) after the next driver update I had to call it.  Its the poor driver software stack.  The hardware has serious potential!  A770 is maxed.  I see this B580 ownership  as I leased the card for the summer +$60.      Can you buy from the [Amazon.com](http://Amazon.com) USA website for our cost?? Shop our stores in the USA.   I should remind you the Steam cold crashes suck.  I have an Asus 850w Platinum for it and I know there's no power fail issue sooo its a goner.  Im old enough to say ive seen this before.  Dont hang out until its done right so long after its release.  Not a hot buy now imo.  There's the AsRock Challenger or the Sapphire Pule within $10 of each other. $359-369.  Newegg and Amazon,Positive
Intel,Thanks man,Positive
Intel,"Honestly? If I were you I’d get a 9060xt 16gb for 350-370 and then enjoy your $400 saved.  Reason being that with your setup, once you start moving up in GPU, you’re not gonna see as big of a performance uplift because of your 5700x, which will start to slightly but noticeably bottleneck a 9070/5070 or above  Which if you did go with a 9070/5070, say you get one for $600, you’d need a new PSU, which is around $70-$100. And at that point, it’s not worth paying $300 extra for a 25-30% performance uplift at 1440p because thats a 50-60% price increase.  You could upgrade your CPU sure, but then at that point, with a new CPU/RAM/MOBO/GPU and Power Supply, that’s basically just building a whole new PC, and you’re not gonna be able to do that for under $800, nor should you because again, the price increase isn’t worth the performance increase.  If I were in your shoes, I’d get the 9060xt 16gb and ride out the rest of my PC for 4-5 more years playing at 1440p. After that, if Im unsatisfied with the performance and frames im getting in games, I’d sell it/keep it as a spare and start from scratch a brand new build ground up. And with the $400 youre saving today, why not get some games, a peripheral upgrade, a second monitor, or shit you could always just pocket and save it. More money on hand is always a good thing.",Neutral
Intel,"9070 non xt and a psu, nvidia prices for the same perf and vram amount is nuts. I say that running nvidia.",Negative
Intel,Is this now the best PCIE power only card?,Neutral
Intel,Context is important,Neutral
Intel,WTF is a B50...  https://preview.redd.it/o20yqe9r07of1.jpeg?width=659&format=pjpg&auto=webp&s=3217fc593219caf45ae9aa9e13cfcbd17893336e,Negative
Intel,Is it better than a RX 580?  Asking for a friend.,Neutral
Intel,It will continue to have absolutely no effect on the market or this subreddit.,Negative
Intel,"So this is the Intel Arc B50. It's an *extremely* power limited BMG-G21 with only 16 Xe2 cores (B580 has 20) and only a 128 bit memory bus, limiting memory performance to just 224 GB/s: B580 has 456 GB/s.  To get power down to 70 watts and be slot powered, it has utterly castrated the BMG-G21. Only 4 MB L2 cache is enabled (of the 18 MB on the silicon) and it runs 1,000 MHz slower than it does in B580. Some sources claim 8 MB L2 is enabled, so this appears to be a bit unsure at the moment. It's not remotely B580's 18 MB and Intel's dGPU caching architecture is still a train wreck from its IGP origins.  What Intel has done is given it a similar memory config to AMD's 7600XT, 128 bit GDDR6 and 16 GB of it and cut the BMG-G21 down so far it can barely do anything. It should perform around RTX 3060 Ti, RX 6600, RTX 2070 kind of level.  It could potentially be useful for AI, since Battlemage has strong INT8 performance, hobbyist level AI is all about CUDA and PyTorch. PyTorch will run on AMD fairly well, but Intel has pretty shit compatibility here and the card's performance will be inadequate for anyone wanting to play games on it. Sure, it'll beat an RTX 3050, but what wouldn't?",Neutral
Intel,"Intel please just set aside the CPU game for a bit, focus all your might on GPUs and save us all.",Neutral
Intel,The SR-IOV is the real catch here.,Neutral
Intel,Will it run on ARM CPU’s? :P,Neutral
Intel,"Not available on r/etailers in EU for common folk, had to send requests and only I got were ""mpq: 10, and we sell it only to companies"". So I could become a lord of B50 over here, but no thanks, I only need 1 and don't want to bill more hrs for my accountant.",Negative
Intel,Got that 2000s Linksys router look to it.,Neutral
Intel,Would this work well on a Plex VM in Proxmox for video transcoding?,Neutral
Intel,Not sure if it's just the A370M on my work/travel laptop but Arc drivers past 2024 make about half of my DirectX games unplayable.,Negative
Intel,I wouldn’t trust installing anything Intel due to it being a state-sponsored company,Negative
Intel,I didn't even realize it was slot power,Neutral
Intel,"In any decent price point? Yes, in ANY PRICE POINT no we have rtx 4000 ada or w/e the name is now.",Neutral
Intel,Not even close. The Ada 2000 is probably faster and the Ada 4000 is definitely faster.,Positive
Intel,It’s a well priced workstation gpu (rare),Positive
Intel,If you have a newer system... yes. There are a lot of reasons this is a bad choice for gaming regardless.,Negative
Intel,"Yes, by miles.  Answering for a friend.",Neutral
Intel,"This, unfortunately.",Negative
Intel,"Not a gaming card. Workstation card mostly for architects, engineers etc.   Decent value for what it is. I grabbed an arc a380 for video encoding and it’s phenomenal.",Positive
Intel,"Wait I’m confused. You said it can barely do anything, then that I will perform similarly to an RTX3060ti. That’s still a fairly capable card for AAA gaming, will it really perform that well on only 70w?",Neutral
Intel,"A lot of of LP slot powered cards won't beat the 3050 6GB. In fact, nothing in the gaming market. The A2000 does, but it was marketed as a workstation card as well. It's not the nothing burger you're making out to be. Granted, at $350, it's a stupid purchase to play games with when we have the 5060 LP.",Negative
Intel,Does it still have the same video functions as it's big brother? AV1 encoding and decoding would make this a decent plex/jellyfin card.,Neutral
Intel,Pretty much just workstation,Neutral
Intel,"Even if it came with a side of fries, people would still grab that new nvidia GPU",Neutral
Intel,"genuine asking, have ARM CPU in desktop space mature enough to not suffer the consequences of x86 translation? like performance issue or bugs",Neutral
Intel,It should be possible on Linux: [https://www.phoronix.com/news/Intel-Arc-Graphics-On-ARM](https://www.phoronix.com/news/Intel-Arc-Graphics-On-ARM),Positive
Intel,Passively and by 10% share. You can argue that somewhere in the future the government will own it,Neutral
Intel,Either did I. That make it way more impressive considering it's performance.,Positive
Intel,"Keep arguing please, I am scribbling notes like my nephews gaming career depends on it",Negative
Intel,Blackwell pro 2000 and 4000 sff beg to differ (those cost a fortune though),Neutral
Intel,"Great for what it is, hopefully even better as new product prices wear off and more deals start to show up. Not meant for gaming so comments comparing it to gaming cards is a different subject. You shouldn't buy it just for gaming at these brand new product prices.  [Here's a video](https://www.youtube.com/watch?v=QW1j4r7--3U) from level 1 techs on it and you'll see all the nerds gushing about SR-IOV at this price point in the comments.",Positive
Intel,It's definitely not made for gaming both in value and performance but it's still gameable if happen to steal one from intel headquarters,Negative
Intel,"A lot of people bought the A2000 to play games, because there weren't a lot of LP options at the time. The best was the 1650, and the A2000 was substantially faster. However, that is not the case anymore.",Neutral
Intel,"If you're running the likes of Bentley MicroStation or Autodesk Civil3D (some of the apps I look after at work) on this little thing, you're taking your workstation back to IT, telling them to ram that little toy up their arses, and to get you a real workstation.  Engineering and architecture workstations are about rasterisation performance. The designs and drawings have to be rendered quickly and smoothly on your two 4K monitors, which a stripped back and power-throttled 70 watt BMG-G21 isn't going to do.  For very entry level visualisation, we use RX 7600XTs with 16 GB, they have AutoDesk and Bentley certified drivers and enough VRAM and rasterisation to handle it. The RTX 4060 Ti isn't that bad there either, again you need the 16 GB version. On the next refresh of the low end workstations, they're probably going to end up being 9060XT and 5060Tis. Both of those would smash this little thing silly.  To that, I'm not sure who Intel wants to sell this to. $350 isn't serious money and it's competing with gaming-intended GPUs (which are utterly fine for visualisation and design) of much higher performance. It doesn't have substantially more RAM than competing products, it doesn't ECC protect the RAM and workstations generally don't care for slot powered cards either way. The ones we use come as standard with 800 watt PSUs and our choice of ""gaming"" CPUs like Ryzens or Cores, or Threadrippers and Xeons - Xeons are a bit less popular these days.  I'm seeing only an argument here for it being a solid SFF video card. It's the most powerful thing you can run in a 75 watt slot power budget... ~~but it isn't half-height.~~  [Yes it is.](https://www.servethehome.com/intel-arc-pro-b50-review-a-16gb-sff-mini-gpu/) So yeah, a solid SFF card.  Intel's inability to execute on the CPU side may be bleeding over to the GPU side and that would be very tragic.",Neutral
Intel,"~~From what I can tell here, B50 isn't even LP.~~  Strike that. [It is.](https://www.servethehome.com/intel-arc-pro-b50-review-a-16gb-sff-mini-gpu/)",Neutral
Intel,"sad but true, I just thought if Intel is struggling atm why not focus on the thing that they're doing well in.",Negative
Intel,"The answer is kind of long, but most ARM chips (probably all) can’t emulate/simulate x86, however there is lots of software that does this. Under Windows it has kernel-level ARM to x86/64 translation for Windows binaries. Compatibility is pretty decent but it’s hard to tell under gaming since there’s no GPU drivers for anything but embedded ARM graphics.  Under Linux, it’s a totally different story. You have Box64 and FEX-EMU. I switched to all ARM years ago but it came with a learning curve. Compared to three years ago, there’s many high performance games and performance reaches about 50-70% of native x86 performance.  If you wanna see some games running on ARM you can check out some videos I made almost 3 years ago below. I cover native ARM games, ARM games running under Box64, games running under Box64/Wine, and games running under Box64 and Steam Proton.  I really should make more videos going over the pros, cons and evolution/reality of the landscape… Anyways!  https://youtu.be/OQrDQmONm6s?si=mDm-hcw1BeNeVLUK",Neutral
Intel,And neither are on the market yet.,Neutral
Intel,Hey - I resemble that.    In all honesty it makes me sad that my VM hosts only have single slot PCIE.,Negative
Intel,Toché.,Neutral
Intel,Give or take a month or two,Neutral
Intel,"b60, the beefier workstation gpu they announced, might interest you whenever they officially release specs and prices then. Supposedly 24GB model and a 48GB model that's basically two combined on one board for what will hopefully be very competitive prices in the space as well.",Positive
Intel,"I wasn’t clear, I only have a single height full length slot, so I’m pretty limited.",Neutral
Intel,"Full height/length single slot with a blower fan I think is supposed to be in the mix for b60, if that's what you got.",Neutral
Intel,I’ll have to look into that,Neutral
Intel,"GPU requirements are fine, but the CPU requirements are insanely high. 7800x3d just to play at high ?? My poor ryzen 7600x   Is this game CPU demanding ?",Negative
Intel,wtf is a 5800f?,Negative
Intel,why would you need a 7950x3d for 4k?? what the fuck is going on,Negative
Intel,Use fg to get 60fps lol,Neutral
Intel,They recommend frame gen to get to 60fps...,Neutral
Intel,Cpu requirements are insane... Something doesn't look right..,Negative
Intel,That must be a mistake considering B580 is listed under medium.,Negative
Intel,"These recommendations are so cursed lmao. Also, the fact they're recommending a stronger gpu for high than they are for ultra is insane  Also, fucking crazy that they say you need a massively stronger cpu to get 60fps as resolution goes up",Negative
Intel,Did AI make this holyshit. B580 for recommended and ultra but not high. Cpu requirements fucking astronomical.,Negative
Intel,Ryzen 9 7800X3D.... yeah aint no way that this is real,Negative
Intel,When u recommend a B580 at 4K Ultra with RT but you only put the 4070 Ti on high 💀,Negative
Intel,"Wtf is this sys req.? The vast majority of players are interested of 1080p60fps. Then we get a 1080p30fps, and a 1440p60fps instead. Lol. What a fail",Negative
Intel,when you upscale from 360p i think it could run that,Neutral
Intel,That table is including a huge amount of bullshit.,Negative
Intel,Probably hiding that you will need XESS on ultra performance. Like every other shitty dev out there,Negative
Intel,"14900k and 7950x3d for ultra? What the fuck?  I understand it's Ultra but then if you need the top 0.1% tier CPU why not say you need a 5090 and 7900xtx for the GPU?  Calling it right now this game will be a stutter fest because the CPU will be absolutely hammered.  If not then these specs are absolutely fucked as other have already said a B580 for the ""recommended"" 1080p30 and also for the 4k60...",Negative
Intel,"Me looking at the chart: Oh, minimal gpu recommendations are quite lo... ooooooh, it's 30fps, not 60.",Neutral
Intel,cpu requirements seem insanely high i think my ryzen 5 5600 will struggle with this one what a shame,Negative
Intel,I have no idea where my 3090 fits on that chart tbh..high i assume,Negative
Intel,Another game slated to run like sublime crap,Negative
Intel,"Do love me a good Ryzen 9 7800 X3D. Can’t find any for sale though, what am I doing wrong?",Positive
Intel,"This game doesn’t look that much improved over the predecessors graphically and other games on the market shit on it visually with lower or same spec requirements. Frame gen to hit 60fps on ultra with ray tracing is insane, if I had to guess they probably have really heavy ray tracing features but the actual models and textures and other tech is not fancy enough to bring enough total eye candy/detail to the game to make it seem worth it.",Negative
Intel,Am I seeing things or is there a ryzen 9 7800x3d 💀,Neutral
Intel,My cpu isn't even on minimum requirements WTF,Negative
Intel,"This low key sucks ass, unless the game is genuinely impressive with graphics, physics, and AI.  Also using frame gen to get 60 is a terrible idea.  You shouldn't be using frame gen with a native 30fps.  Good thing I got this game for free.",Negative
Intel,"The feeling, when you see your CPU, is mentioned as recommended.",Neutral
Intel,This makes absolutely no sense…so I could play ultra settings with 4080 super but not really because I have 7800 x3D? Also isn’t this CPU faster in games than 14900K? Somebody use chat gpt to make this I swear 😂,Negative
Intel,"The GPU requirements for low were ok until i read ""30 fps"" + paired with the somewhat elevated CPU requirements for all settings...  Lets not even talk about FRAME GENERATION FOR ULTRA 60FPS > im tired of companies abusing this technology to skip doing a proper optimization work.  Based SOLELY on this requirements chart, the performance will be shit.",Negative
Intel,i almost never buy games on launch day.  10$ says this is going to be buggy and broken initially so i better just ignore it and wait a year,Negative
Intel,30fps should be deleted from being minimum in any game. No one wants to play a jittery slideshow.,Negative
Intel,They're recommending a 7900GRE and 4070Ti for 4K HIgh. I think that's pretty good isn't it?,Positive
Intel,Ill just play on high lol. My 9070xt isnt even going to try ultra 4k 😅,Neutral
Intel,The Beast was meant to be a DLC for a 2022 video game.  Don't get too excited by the low system requirements 😂,Neutral
Intel,"And my 7900 GRE will need FSR, because why not?",Neutral
Intel,"It all depends on how intensive the game is. We routinely used the RTX 2080 Ti for 4K 60FPS gaming, and the B580 on that same kind of level.  Given its requirement for 4K60 at ""Ultra"" is just 12 GB VRAM, this probably isn't a hugely intensive game.  All that said, however, this is likely a mistake. The B580 is already in the 1440p 60 boxes and it isn't keeping up with an RX 9070 or the RTX 5070 alternative.  What's weirder than this are the CPU requirements. The 7950X3D (Ultra) is **slower** than the 7800X3D (High) unless the game is using utterly silly numbers of threads. The ""Minimum"" Ryzen 7 5800F isn't even a real thing!  Nobody has put much thought into this at all.",Neutral
Intel,70gb storage  https://i.redd.it/hbfuai741tmf1.gif,Neutral
Intel,"This seems like some nonsense. B580 for both recommended and ultra? Cpu requirements are insanely high too. The best option, like always, is to wait for the game to come out and check out some youtube videos w.r.t. to your specs and then make a decision.",Negative
Intel,Why do you need a top X3D 16c chip for 4k Ultra with RT + FG  That's asking for too much even for 4k High,Negative
Intel,ohhhkay so my 1080 should do 1080p60 at medium-high.. as it does with everything else. yay.,Positive
Intel,I have an RX 6600 and an i7-11700 and a 1080p monitor  I have absolutely no bloody idea how this game will run for me. But I'm hoping for at least medium settings at 60FPS. Probably wishful thinking on my part.,Negative
Intel,"as a guy with 4070tis and i7 12700kf, if im not going to get 140+ on 2k, I'll be disappointed",Negative
Intel,My poor 3070 😢,Negative
Intel,Never knew there was a 5800F,Neutral
Intel,"If its legit. Then thats called a great optimization. If not, well. You know the answer",Positive
Intel,"Any modern instruction CPU with 8 ""P"" (normal hyper-threaded/SMT capable with sufficient CPU speed) should be able to keep up with the GPU recommendation for 4K (if truly optimized well enough to max out with an Intel B550 - the vMem and raw horsepower of this card shouldnt be underestimated).   My $0.02",Positive
Intel,Is 32GB the new 16GB?,Neutral
Intel,14900 still kicking lol🙏🏾,Positive
Intel,"Btw they also released laptop gpu requirements as well , which feels even more wierd  https://preview.redd.it/knpbietfeumf1.jpeg?width=1080&format=pjpg&auto=webp&s=4cf82a2c75a69b25e1c3d2e0b5fe52aff9722204",Negative
Intel,First time I've ever seen the GRE mentioned in one of these lol,Neutral
Intel,This game is supposed to be basically a DLC of DL2. so why the fuck a 6750xt is for 1440p medium? If it can max out DL2 easily at 1440p native. Cut the bullshit,Negative
Intel,Man the specs are cooked low 30 fps even with a ryzen 7.,Negative
Intel,"I'd imagine cpu specs are that high because of how many zombies can be rendered at once and if they have more than the most basic of AI save for the mutations.  Still, box specs aren't always a great metric to go by.",Neutral
Intel,I’d guess it’s a mistake?,Neutral
Intel,"the Ryzen 7 5800F doesn't exist. why is the B580 both in 1440p medium and 4k ultra? why does 4k ultra (with frame gen, so technically the cpu renders 30fps) require a Ryzen 9 7950X3D when 4K high requires a Ryzen 7 7800X3D (here cpu renders 60fps), those cpus have the same perfomance. why does 4K high require a 4070 Ti and 4K ultra a 5070, which is slightly weaker.",Negative
Intel,"Talks about the 7900 GRE (Sold way less then an XTX) and 9070xt but not the 7900xtx? 7800x3d which is faster in gaming recommending for lower then the 7950x3d which is for higher recommendations but performs worse? Then Intel Arc B580 recommended for 1440p60 and also for 4k Ultra Ray Tracing 60fps (THIS CARD BARELY DOES 1440P)? Huh? And then a 5800f? What is that? this CPU doesn't even exist! THEN A 7800X3D FOR 4K HIGH? THE DIFFERENCE IN 4K FOR A 5800X3D AND A CREAM OF THE CROP 9850X3D IS LIKE 5-10% (In BF6 it's 7%) WHO TF MADE THIS? WITH FRAMEGEN AND UPSCALING FOR THIS?  This genuinely makes no sense, this cannot be real. This also has to be the most demanding ""traditional"" game for these requirements I've ever seen. Framegen to get 60fps on Ultra 4k? What is going on? This started as a DLC for a 2022 game? This has to have been made by an unpaid intern using ChatGPT. Monster Hunter Wilds spec chart is like half as demanding as this games and we all know how that played out. Rip Techland, this game wont' sell because no one can play it.",Negative
Intel,Wtf I have a 5600g I'm cooked,Negative
Intel,I’m sorry 32gb of ram for high and ultra? What the hell!  Don’t tell me this game is horrible optimised?,Negative
Intel,wow,Positive
Intel,Must be using the upscaling at like ultra mega performance mode with 8x frame generation,Neutral
Intel,[https://www.youtube.com/watch?v=UELrfslYU8s](https://www.youtube.com/watch?v=UELrfslYU8s)   RTX 3060 Ti | Dying Light: The Beast,Neutral
Intel,"This shit is so wrong on so many levels, they don't even care to put real hardware on it.  ""Ryzen 9 7800X3D"" doesn't even exist, 7950X3D isn't faster than 7800X3D in most games, so why it's included for 4K with RT/FG, shouldn't it be 9800X3D?.  ""Ryzen 7 5800F"" doesn't exist. Arc A750 Is 87% faster than 5500XT and 1060 and has more VRAM and it's in the same category for 1080p 30fps low.  The Intel I5 13400F doesn't change from ""30fps low"" to 60fps medium"" but the jump from 5800F (X)?? to Ryzen 7 7700 is pretty big.  Arc B580 being for ""4K with Ultra RT/FG"" and for medium 1440p/60FPS is insane. The 9070 and 5070 are 80-100% faster than that.",Negative
Intel,"this chart doesn't make sense to me. my ryzen 5600x cpu and 7800xt gpu should be more than enough for 1440p 60fps, and should be enough for far more than that, but this chart is making it seem like the cpu will be a bottleneck for performance?   Looking at the chart again, the recommendations just aren't adding up to me. Im thinking this chart has, possibly, misinformed or incorrect information right now, and that ill probably be fine for 1440p 60fps.",Negative
Intel,Ryzen 7 5700F ???,Neutral
Intel,There is no way its not a typo. Otherwise this game is like made for arc gpus,Negative
Intel,Where would I scale with a 3900x and a rtx 2080? Im thinking like a mix of high and medium?,Neutral
Intel,"The only conclusions i can make are the storage requirments are set, theres supported tech in the bottom left, and everything else if confusing and in need of more certified benchmarks.",Neutral
Intel,In what world is the b580 equal to the 5070/9070?,Neutral
Intel,"Is this AI generated? These are rarely consistent or but this one is really bad. An A750 can only do 1080p 30 but a b580 can do 1440p 60? A b580 can do their medium, can't do high but ultra works again? A 5070 being significantly better than a 4070 Ti? And what the hell is a 5800f? Ultra including frame gen?   What were they thinking?",Negative
Intel,People makes games in 15 min at UE5.  No optimisation.  Game sector went fast food from fine dining.,Negative
Intel,"FSR 4, hell yeah.",Positive
Intel,want this game!,Positive
Intel,Does a Ryzen 7 5800F exist? It's listed under 1080p/30 or did they mean 5800X?,Neutral
Intel,"These days, resolution and framerate targets are both useless... We need to know if this is at NATIVE resolution and with or without the fake frames...",Negative
Intel,"i dont trust the cpu requirements, i would definitly take something newer than a 14xxx series from intel",Negative
Intel,The CPU requirements are nonsensical. They recommend a 7950X3D for 4K Ultra with RT and the 7800X3D for High 4K when the 7950X3D is slower for gaming than the 7800X3D lol.  I wouldn't worry too much about it. Your CPU will be more than fine.,Negative
Intel,"Its funny how games like cyberpunk and kcd2 recommend a 7800x3d, but a 7600x is perfectly fine and will give you high fps even in cpu intensive areas, yet a game like stalker 2 which recommends a 3700x, you will be bottlenecked at 85 fps",Negative
Intel,Yea it looks to me that these have to be typos.  Can’t imagine it being that intensive especially at 4K when it comes to the CPU and then regarding the intel gpu……..maybe it’s the persons first day on the job?  This seems wrong that’s all I’m saying.  However optimization these days is rather lack luster so who knows?,Negative
Intel,That’s what I was thinking… seeing something other than 5700x3D or 5800x3D makes me wonder…,Neutral
Intel,Does cou requierement ever actually matter?  So manny times it says I will need better than my 13600k but there are never any issues,Negative
Intel,GPU req are also insane,Negative
Intel,Another non optimized game of 2025. Game of the year should have awards for unoptimized games of the year winners.,Negative
Intel,"Idk man , but yeah cpu part seem insane, while we can get away with GTX 1060 but we need 13400 for cpu lol",Neutral
Intel,"Same with ""Ryzen 9 7800 X3D""  Also who tf use frame gen to reach 60 FPS?",Negative
Intel,"At 30 fps no less (60fps fg, so the CPU only sees 30 fps). This tells you all the people who made/approved this chart are clueless.",Negative
Intel,So over all these companies targeting 60fps for everything and everyone being ok with it for some reason.,Negative
Intel,4k 60 with RT... With a 5070...,Neutral
Intel,With ray tracing though.,Neutral
Intel,ppl forget ultra usually is future proof settings. using frame gen to achieve it makes sense,Neutral
Intel,Stop being one of those people that just says that like you ignore all context. Especially when you're talking about Ultra settings. You scream that you have an unoptimized system and are already want to blame devs.   ![gif](giphy|71LVGkH37wAyRKHMAl),Negative
Intel,Probably ai related. X4 also requires crazy cpu power. 9800X3D recommended,Neutral
Intel,Yeah listed at medium and ultra but not high,Neutral
Intel,"I don't know if it's true but I think it's intel sponsored game(people in dying light sub claim it is) , but still even that won't push b580 for 4k ultra gaming lol , and it was posted by someone official from techland in dying light sub  Edit: why down votes?",Negative
Intel,For real. Guess my rig is now trash 🤷🏽‍♂️,Negative
Intel,Well.........yeah u had me here,Neutral
Intel,And look at cpu recommended. They feel insane   My poor 3600xt crying in the corner,Negative
Intel,"Where does it say that?   It usually says if the requirements are with upscaling taken into consideration, as with the FG for 4K Ultra RT.",Neutral
Intel,The Ultra requirements are aiming for 60 fps with frame gen so actually 30 so it kinda makes sense...  There's also no talk of upscaling here which I find hard to believe.   This is probably made my an intern.,Neutral
Intel,Do you run it native 2k or do you do DLDSR + DLSS ?,Neutral
Intel,I've noticed more games using 17GB so it's getting there.,Positive
Intel,"It's intel sponsored game I am guessing it's a situation very similar to amd's insane advantage in cod games, in cod even 70 class amd gpu beat the hell out of  rtx 4090",Neutral
Intel,"Nope , it's real, they even shown it in their pc build video on YouTube",Neutral
Intel,https://preview.redd.it/b2sy4jzrttmf1.png?width=705&format=png&auto=webp&s=bbe7c8675adc5441f773fc8137282673c2ff1adb  [https://www.cgmagonline.com/articles/previews/dying-light-the-beast-event/](https://www.cgmagonline.com/articles/previews/dying-light-the-beast-event/)  ??????????????,Neutral
Intel,I think it’s a typo.,Negative
Intel,"tbf it is possible that it needs the extra threads for it to perform better. I heavily doubt it, but it *is* possible.",Neutral
Intel,Why is this x950X3D slower than x800X3D shit still being thrown around everywhere. It is incredibly easy to configure x950X3Ds so they run as just upgraded x800X3Ds.,Negative
Intel,"IIRC, the 7950X3D not using the right cores in games was mostly fixed.",Neutral
Intel,"Ok good I ran to the comments worrying about mine, I usually have no problems with my 7800x on ultra but I'd like this game to work for me well",Positive
Intel,"This is from [9800x3D review](https://www.eurogamer.net/digitalfoundry-2024-amd-ryzen-7-9800x3d-review?page=4) done by Eurogamers (Digital Foundry). Captured in a CPU heavy area that they use in all their CPU & GPU testing. You can easily see the differences between CPUs. Even my 9800x3D is CPU often limited in many ways. Not so much when playing with locked 120Hz, but definitely when going any higher.   Edit. One addition. When using high GPU intensive path tracing, it will also tank the CPU performance. Something that actually got my mind blowed when I tested this on my 5800x3D & 9800x3D. Already CPU intensive titles have a hard time when using the PT.  https://preview.redd.it/yyw9oejud0nf1.jpeg?width=1216&format=pjpg&auto=webp&s=fef5205dddcf03ee0df8972aad5e3b6c8da0f6b9",Neutral
Intel,"3070ti for native 1440p 60 fps at medium doesn’t seem to be horrible. I’m sure there is many settings that don’t change fidelity a lot but eat up fps, so putting some to low and some to high may be a better experience. This also seems pretty damn general of a spec sheet here, almost like it’s useless based on some things here.   Throw on dlss with the 3070 ti and enjoy probably like 80 fps.  It does seem to be a gorgeous looking game too.   However, I’m not trying to defend it, and I would expect it to be optimized for the money that they’re asking. Like any game these days. And if it’s not it should be hated on.",Neutral
Intel,"To be fair, dying light has always been a very AI heavy game, so requiring a beefy CPU makes sense. Specially if any of the zombies can parkour at all.",Neutral
Intel,60 fps target with raster performance is a very sensible target.,Positive
Intel,and in 4K,Neutral
Intel,I don't mind if they recommend frame gen for something more logical like 120+ fps where the latency won't be super horrible. But requiring frame gen just to get 60 fps means the experience is going to suck. I'd rather play at 30 fps than with the weird feeling latency that frame gen causes when working with a low starting frame rate.,Negative
Intel,It still is the input latency of 25fps,Neutral
Intel,"With cheese Mr Squidward, with cheese.",Neutral
Intel,"No it’s a perfectly valid thing to point out, there is a massive difference between 4k 60 and 4k 60 with frame generation.   This is coming from someone who generally enjoys frame generation, but what has allowed me to enjoy it is knowing how to use it properly, and going from sub 60fps up to 60 is not the right way to use frame generation.",Neutral
Intel,fuck the devs,Negative
Intel,"If it's not a mistake, maybe frame gen and upscaling make it technically possible, but with unbearable latency.",Negative
Intel,"I was being ironic. Every UE5 game I’ve played needed upscaling to hit 60+ fps, e.g. MGS3 Delta, Oblivion Remastered, KCD 2 (real good game). But that's just me being annoying, wanting everything on ultra.",Neutral
Intel,"yeah ""intel sponsored game"" but Arc A750 that is 87% faster than the 5500XT/1060 is for 1080p low 30fps, and that argument is dead. B580 doesn't have enough power to play at 4k, the difference between B580 and 5070/9070 is so massive it won't matter anyway that it's better optimized for battlemage architecture. That's just cope",Negative
Intel,"Oh great, my 14900 is going to overheat just like in bf6",Negative
Intel,Isn't the non-X3D CCD basically turned off when gaming? By the X3D boost setting?,Neutral
Intel,"It’s not ‘it’s possibly’ it’s most likely what it is, for some reason many people think cpu > next to irrelevant but it absolutely isn’t for *specific* games, devs put this out there for a good reason probably",Neutral
Intel,Multithreaded optimizations are notoriously difficult for most developers. I doubt this game needs that much throughput.,Negative
Intel,9950X3D is notably an exception to the rule. The 7950X3D was and is slower than the 7800X3D.,Neutral
Intel,"Emmm the game looks just like DL2, and that gpu maxed out the game easily, so why now barely 60 at medium?  crap optimization",Negative
Intel,"But as we're told by Jensen, AI runs on the (Nvidia) GPU, right?",Neutral
Intel,It's with the Ray Tracing ON that's why it requires frame gen especially on Ultra 4K. Stop rushing and read more carefully.,Neutral
Intel,... The B580 definetley has 12GB of Vram wdym.,Neutral
Intel,If they implement every single optimiztaion intel recommends then its more than possible to get 4K 60K on the B580 with FG.,Positive
Intel,"Ah, I guess it's not out of the realms of possibility these days anyway 😂  Yeah UE5 is the worst. I've not *had* to use upscaling on my 9070 XT at 1440p ultra for a playable experience but since I prefer a higher refresh rate experience then I've usually had it stick it on to quality to get that experience when other games/engines I can easily play native and do that.  The newest version of XeSS is decent if you're on RDNA 3.",Negative
Intel,"To be fair, Bf6 is particularly CPU heavy. My wife's 5600x was at 100% usage the entire time the game was open. My 9950x3d would be as high as 60% avg on one or 2 maps",Neutral
Intel,"No, it is still active! The problem with the 7950x3D (and 5950x3D) is that some processes can go to the part without the 3D-Vcache. So you could say the problem is that it isn't turned off, or rather that the scheduler doesn't prioritise correctly.",Negative
Intel,Not if you bother spending 10 seconds properly configuring your $900 CPU.,Negative
Intel,You can literally just turn off CCD1 on the 7950X3D and it'll run as a slightly upgraded 7800X3D. Proper CPPC and Process LASSO configuration will make it always run better or the same.,Positive
Intel,I got like 85 fps in ultrawide with a mixed/ ultra settings. What are you talking about?,Negative
Intel,Why are you assuming I didn’t know it was with RT on?,Neutral
Intel,"The issue is that FG is a “win more” tool. You don’t use FG for sub 60 to get to 60, as it has poor latency. Generally not a good experience.",Negative
Intel,"The issue is I’m expecting 4K Ultra out of a 1440p card; as I game at 5120x1440, about 20% shy of 4K.      But honestly, I don’t care that much, I’ve played games at 40/45 and still had fun with awesome graphics. I just need stability.",Negative
Intel,My 5070 was bottlenecked by my 5700x3d in bf6,Neutral
Intel,"I throttled my cpu and had no dips in performance, temps went back down to 60ish. I don’t think they were utilizing the cores correctly or something.",Negative
Intel,"To be perfectly fair, a lot of people who are spending $900 on a CPU are fairly reasonably expecting it to just work best out of the box.  A lot of the ""value"" of most super premium consumer products is the fact that everything is handled for you.",Neutral
Intel,Wow sounds like a great deal for hundreds more lol,Positive
Intel,"but you have a 5070 not a 6750xt/3070ti..................................   Edit: also, barely 80fps on a 5070 on and old gen game is HORRIBLE",Negative
Intel,Man do you even realize how demanding the RT is? they put a 5070 there not a 5090.,Negative
Intel,"Makes sense, my 7800xt was about right with my 5700x3d. There was no bottleneck since all my components were fully utilised",Positive
Intel,"If money isn't an issue, yeah actually. It's at parity or slightly better in gaming with massive headroom, and completely shreds it in productivity.",Negative
Intel,That’s what I got with my 3070ti and 5600x. I just recently upgraded. I get far more fps now on max settings with raytracing.,Positive
Intel,"Yes, I use RT often so I know how heavy it is, in fact I used it in the DL2 base game, but that’s neither here nor there when it comes to my point.   They really shouldn’t be labelling it as 4k 60 if they are using frame generation to get there. Nobody will realistically want to used FG to get from sub 60fps up to 60, so they should have just put 4k 30fps in that category.",Neutral
Intel,"You won't need frame gen to hit 60 if you have a current gen card like 5080.   5070 has the performance of a 3080ti or 4070ti, both cards for 2K nowadays. Look at it as a sign of good optimization for those that have 4K monitors.  I also don't use FG, but the graphic looks better rather than slapping 5080 there to brute force it.",Positive
Intel,"Yeah I’m aware of all of that, but again it’s not really my point, my point is that they really shouldn’t be marking it up at 4k 60 for those cards, probably in the hopes that a lot of people will just skim over it and not see the bit further down in brackets that you actually need to use frame generation to achieve that with those cards.   It’s misleading at best, as those cards won’t get near 60 fps with those settings without it, and it’s not the way FG is supposed to be used, it would be an awful experience, they either should have listed the actual cards you will need to get 4k 60 with those settings, or marked that category as 4k 30fps.",Negative
Intel,you're probably CPU bottlenecked though the slow RAM isn't helping you at all either. Get a 2x16 3600 speed RAM kit and/or grab a 5000 series AMD CPU,Negative
Intel,"Yeah, you're probably being hit by the driver overhead issues. You want at least a 5000-series Ryzen to mitigate that, as the other commenter suggested.",Negative
Intel,Intel cards have a terrible CPU overhead issue with lower end processors.,Negative
Intel,![gif](giphy|3WmWdBzqveXaE)  Nana watching you regain your inheritance,Neutral
Intel,Yes,Positive
Intel,Thank you,Positive
Intel,"Sounds like that should work, I feel like the GPU would be the bottleneck actually. Depends on what you use the PC for. Maybe a used 3060 instead if you can find it, but I like new hardware due to being burned in the past.",Positive
Intel,"I find this page useful when comparing recent GPUs: https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html#section-rasterization-gpu-benchmarks-2025  They have a handy visual table showing how the cards line up in their benchmarks.   The B580 looks to come close to a 4060 / 5060 in these benchmarks. Nvidia cards look to be a similar price where I am so maybe worth stretching your budget to a 5060 if that’s the case where you are.",Positive
Intel,Doesn't the B580 out perform the 3060 quite considerably though ?,Neutral
Intel,My buddy has one and loves it,Positive
Intel,I hope you enjoy the new card. Are you gaming at 1080 or 1440?  I would highly suggest you update your [BIOS](https://www.gigabyte.com/Motherboard/H610M-H-DDR4-rev-10/support#dl) says version F4 but the release date has the date of F29 current version is F31 from June?,Positive
Intel,B580 is only a risk if your CPU is relatively old. Anything LGA 1700 or AM5 will be fine.,Neutral
Intel,1080p @ 100 hz,Neutral
Intel,I have an i5-12400. I seen someone say you need Intel 10th Gen on up.,Neutral
Intel,awesome it will be completely capable of handing that and more so you can upgrade your monitor at a later date.,Positive
Intel,"Or just keep the same monitor. I had 1440p @ 180 hz at some point, but I'm quite happy with 1080p now because 1080p will last a lot longer.",Positive
Intel,"That's a no to 5060ti and RAM. Move the budget towards the 5070 (Recommend the Asus Prime which is at or below MSRP now), which will also then qualify for free copy of Borderlands 4 to eat up some of that extra you pay for 5070. MUCH faster card, even with 12gb VRAM....that extra 4gb on the 5060ti isn't gonna do diddly squat for you on that tier of card. In the end it's gonna cost you $50 more since you won't be spending $70 on Borderlands 4.  I would not drop any more money into that system besides GPU. 16gb RAM is fine.  Then you can start saving for a MOBO/CPU/RAM upgrade...but what you have should hold over fine for now (ASSUMING your PSU is decent).",Neutral
Intel,"Hey so this game is gonna run like shit on every system, I hate to break it to you. I breaks my heart",Negative
Intel,"Already have my free copy of BL4 from the laptop promotion best buy was doing, but thanks for letting me know!  Alright, thanks! Do you have a link for the 5070 you're talking about?  My PSU is 400W.. :/",Positive
Intel,People keep saying this and then I play the game they are talking about and it plays perfectly .,Positive
Intel,"That changes things.  5070 will need PSU upgrade, which if you didn't already have BL4, is what I'd suggest and moving that RAM budget to PSU budget.  If still want to upgrade you can, but if budget is restricting and you don't want to spend that much now but still want to upgrade GPU then other option might be considering 9060XT 16gb at $350 (but only if you can get it at $350, any more and 5060ti is better buy). Otherwise I would move to 5070.  Not sure if I can post links on this sub (they always remove my post) but it's on BestBuy search SKU: 6614787  Keep in mind if you do get the 5070 and the BL4 code, it can only be redeemed by those running 5070 or above cards since that's what the promo applies to.",Neutral
Intel,I see the GeForce RTX 5070 12GB at $550. Is the 5060ti in my post the better buy? And do I actually need more ram or is that just what steam recommends for maxed out settings,Neutral
Intel,"What resolution are playing at?  As for RAM, it's recommended but isn't priority for you. You have to manage everything else first, especially GPU... if you have a weak graphics card, it doesn't matter if you have 32gb or 64gb of RAM... you'll be limited by the GPU before you run in RAM issues.  Depending on your monitor resolution, and resolution you play at.... 'maxed out settings' is going to be all relative. For example if you have 1440P monitor, if you get a 5060ti/9060XT tier card.... you're not going to be playing maxed out settings without help of upscaler (DLSS or  FSR4).. 16 or 32gb RAM becomes irrelevant. I wouldn't worry about that right now.  As it stands... your choices are 9060XT @ $350, 5060ti @$430 and 5070 @$550. The 5070 will requires PSU upgrade, but others may not. 5070 comes w/ game code but limited to who you can sell to if you don't need it. Might be able recoup some money though as plenty of people who bought before promotion. w/ that in mind.. if you can get say $40ish for the game code.. the 5070 is better buy over the 4060ti. If you don't want to spend that much then it's a tossup between 9060XT and 5060ti. The 5060ti is about 5% better raw performance wise but does have a more widely supported DLSS software stack which at that caliber of card you are more likely to use.....so to you it may or may not be worth the extra $70.... if not and you really don't want to spend as much then 9060XT @ $350 is better value card in that budget.  If you prefer Nvidia card, then 5070 is overall better value card as it is 30-40% faster than 5060ti and why I'd pick it over the 5060ti if one can budget for it. Difference in performance is too big.  I'm not even bringing up (since someone else will) the AMD 9070 and 9070XT because they're well over MSRP and out of your budget and not worth it at the prices they sell at.... and at those inflated prices, you might as well move to 5070 ti (which is well over your budget as well).",Neutral
Intel,"2560 x 1440. Doesn't the game code need to be activated with the graphics card through the nvidia app?  If not, then it looks like I'll be picking up the 5070!",Neutral
Intel,"at that resolution, you need minimum 5070 (and 550W PSU) if you want any enjoyable experience at the target 'maxed out settings' and even that is going to be a tough ask for this Unreal Engine 5 game without DLSS.  Yes, the game code does have to be activated via nvidia app...but doesn't have to be the 5070 you buy... it could be any 5070, 5070ti, 5080 etc. So if your buddy has a 5070 or above, they can redeem it etc.",Neutral
Intel,"Alright, lets say I don't max out my settings, maybe medium? High even? Would that be an enjoyable experience? (if i don't upgrade my psu) And that's interesting, I didn't know that. Thanks!",Positive
Intel,"Yes. Should still be able to do mix of Ultra/high/medium settings with 5070 and DLSS upscaling @ 1440p resolution. To me, enjoyable means it never dips below 60FPS... and target around 90FPS+.... to me that is enjoyable.  Unfortunately, People often have this misconception that because they spent X amount of money on a card, they can crank everything to ultra setting and the GPU should just be pushing out 120FPS constant. It's not how it works... you always have to tune your settings to what the card/hardware is capable of. I still have to tune settings for my 5090 in lot of AAA games. At least you are being reasonable about adjusting your settings lol.  Same thing applies to amount of VRAM a card has.... same misconception exists, ESPECIALLY on this sub, that you need LOTS of VRAM and anything below 16gb VRAM on GPU is useless... again, that's wrong. Just because 5060ti/9060XT have 16gb VRAM doesn't mean you can crank up settings and  it's gonna be faster.... it could have 96gb VRAM, it's still going to be 30-40% slower than a 5070 w/ 12GB VRAM at it's intended resolution (1080p/1440p).  Yah would have been nice if 5070 had more VRAM but in no world is a 16gb XX60 class GPU ever going to be a faster gaming card than xx70 class. Period. Putting 16GB on weaker entry level cards was an FU homage from Nvidia and AMD to everyone raging about VRAM and people ate it right up.",Positive
Intel,"Lol yeah I used to play maxed out, but then over time I had to swap to high and now I play on medium, so I've been through it 😂. Thank you for all the quick responses! I appreciate it",Positive
Intel,"Just checked on google and it says the recommended psu for the 5070 is 650w, would I not be able to use the 5070 until I upgrade my psu then?",Neutral
Intel,"https://preview.redd.it/a2lk5vrnl2of1.png?width=506&format=png&auto=webp&s=6bc8a62ed77bf4cf8c1f2dba6ce15ad44ed085e6  The A650BN has a single cable, splitting to two 2x 6+2-pin. The GPU has two 6+2 headers, meaning you are splitting power from a single cable. That is not optimal, but it is ok to use a single cable for lower-end GPUs.  Why are you not considering the B580 instead? [The B580 has a bit better gaming performance than even the A770](https://youtu.be/aV_xL88vcAQ?t=681)",Neutral
Intel,I did some research on that GPU and what’s recommended for that card is minimum of a 650W gold certified. If you plan on overlocking the card it’s recommended a 750W gold certified to future proof the build and have more headroom for overclocking. Models recommended are Corsair and Cooler Master.,Neutral
Intel,"So let’s cut that to the absolute bone: https://pcpartpicker.com/list/H8DRWc  Then get a non-bomb PSU and a better GPU.   https://pcpartpicker.com/list/LYkQJn  Don’t have to worry about VRAM throttling on a PCIE 3.0 board if you have enough taps forehead.   Also check second hand for a 5600 should be available. Use the stock cooler until you have $20 spare to get a cooler.  Side note, an AM5 build isn’t that far away: (130 7500F, $5+ Cooler, 1x 16gb stick at $30, $99 no wifi board)   Also you need storage. MP44L is like another $80.",Neutral
Intel,"Thank you! This is also My first use of pcpartpicker.com, so I appreciate the Feedback.",Positive
Intel,just get the cheapest,Neutral
Intel,"It just depends on the game tbh. Its a good GPU, alot of improvements over the last generation. A gamble, but when it works it works well.",Positive
Intel,"If you have a budget for a computer on AM5, you might consider building a specification with a better graphics card on AM4.",Neutral
Intel,"i'm mostly going to play the following: warthunder, teardown, people playground (not that gpu dependent but still) cs2, and roblox, but i will also probably buy new games such as ready or not, and some offroad explorer game i saw my friends play but i have no idea what its called (i think its snowrunner? or over the hill, not sure). would it run most of these games fine?",Neutral
Intel,You don't really need to apologise if you pick Intel.,Neutral
Intel,Reason for intel cpu: I know I picked Intel. But this is because my sister doesn't really play demanding games and she is a student who needs to edit videos for school and more. Any future builds i will use an AMD cpu obv. But this build will get her things done. The total price is about £550 ($743 for Americans).,Neutral
Intel,Probably makes it more simple too for her cpu and gpu to work together in a menu for overlocking or something. Idk if they have anything like amd with their cpu and gpu.,Neutral
Intel,yeah she isnt really good at pc's so thats why I chose this combo,Negative
Intel,"if you don't have a 3rd gen ryzen or 10gen intel or newer CPU you're kind of SOL on intel GPUs, they need reBAR pretty much to perfrom well",Neutral
Intel,If you already have Rebar on your system.  Then BUY the B580! If I remember the B580 is around 36K! Potaka IT & SkylandBD got the card within the price.,Neutral
Intel,"Not much reason to go for the a770 when the b580 improves performance by a decent margin and consumes less power. Yeah, less vram, but I doubt you’ll be running out of vram before you’ll be running into a gpu performance wall.  Also, as the other commenter said, make sure you have a relatively newer cpu, not just because it requires rebar but because the drivers can have an impact on cpu performance.",Neutral
Intel,"check for any of the rtx 3060 / 4060 / 5060   i can see some in similar price, you will save yourself a lot of trouble",Neutral
Intel,Some 9th-gen Intel chipsets got the ability to use reBAR in later BIOS updates. I’m building a 9600kf media PC with it enabled right now. Z390 chipset.,Neutral
Intel,"Eh, B580 was a real improvement in terms of software and hardware so I'd say it's ok  But then again, seeing Intel currently bleeding money left and right I wouldn't be surprised if they were forced to drop the GPU market again which would be a real shame but that's what happens when shareholders run a tech company into the ground by not pushing innovation but profit above everything else I guess",Neutral
Intel,"What a disgusting build, I love it",Negative
Intel,the content we crave,Positive
Intel,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",Neutral
Intel,What GPU are you using in your build?  All of them,Neutral
Intel,you're one hell of a doctor. mad setup!,Positive
Intel,The amount of blaspheming on display is worthy of praise.,Negative
Intel,Brother collecting them like infinity stones lmao,Neutral
Intel,I'm sure those GPUs fight each others at night,Negative
Intel,Bro unlocked the forbidden RGB gpus combo,Neutral
Intel,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,Neutral
Intel,What the fuck,Negative
Intel,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,Positive
Intel,Yuck,Negative
Intel,Wait until you discover lossless scaling,Neutral
Intel,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,Neutral
Intel,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",Negative
Intel,Now you just need to buy one of those ARM workstations to get the quad setup,Neutral
Intel,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,Positive
Intel,Love it lol. How do the fucking drivers work? Haha,Positive
Intel,What an amazing build,Positive
Intel,wtf is that build man xdd bro collected all the infinity stones of gpu world.,Negative
Intel,You’re a psychopath. I love it,Negative
Intel,This gpu looks clean asf😭,Positive
Intel,The only setup where RGB gives more performance. :D,Positive
Intel,Now you need a dual cpu mobo.,Neutral
Intel,Placona! I've been happy with a 6700xt for years.,Positive
Intel,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",Neutral
Intel,"Brawndo has electrolytes, that's what plants crave!",Positive
Intel,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",Positive
Intel,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",Neutral
Intel,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",Neutral
Intel,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",Neutral
Intel,Team RGB,Neutral
Intel,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",Neutral
Intel,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",Negative
Intel,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",Positive
Intel,"OpenCL works on all of them at once, and is just as fast as CUDA!",Positive
Intel,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",Positive
Intel,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,Neutral
Intel,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",Positive
Intel,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",Neutral
Intel,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),Negative
Intel,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,Neutral
Intel,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,Neutral
Intel,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",Negative
Intel,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",Positive
Intel,Thank you so much for the very detailed response!,Positive
Intel,Well worth it!,Positive
Intel,Thank you my man!! Looking forward to run some tests once I get home.,Positive
Intel,That's awesome!,Positive
Intel,"Yes, but SLI is a bad description for it.",Negative
Intel,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",Neutral
Intel,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",Negative
Intel,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",Negative
Intel,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",Neutral
Intel,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",Neutral
Intel,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",Neutral
Intel,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",Neutral
Intel,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",Negative
Intel,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",Positive
Intel,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",Neutral
Intel,Why are you connecting the monitor to the gpu and not the mobo?,Neutral
Intel,"👍   thanks for the info, this'll definitely come in handy eventually.",Positive
Intel,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,Neutral
Intel,No worries mate. Good luck,Positive
Intel,"For some reason I switched up, connecting to the gpu is the way to go. I derped",Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,It's alive. Rejoice.,Positive
Intel,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",Neutral
Intel,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",Negative
Intel,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,Positive
Intel,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,Neutral
Intel,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,Negative
Intel,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,Positive
Intel,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",Neutral
Intel,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",Negative
Intel,I'm fairly sure they use dxvk for d3d9 to 11.,Neutral
Intel,Could just be a cache issue,Neutral
Intel,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,Neutral
Intel,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,Positive
Intel,"According to the graphs, AMD has slightly less overhead than NVIDIA.",Neutral
Intel,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",Negative
Intel,"Lowest with DX11 and older, but not with the newer APIs",Neutral
Intel,And when is the last time HUB did a dedicated video showing the improvement in overhead?,Neutral
Intel,or it's just a cache/memory access issue,Neutral
Intel,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",Neutral
Intel,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",Negative
Intel,"Intel uses software translation for DX11 and lower, so it does matter for them.",Neutral
Intel,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",Neutral
Intel,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",Negative
Intel,That's not true. Intel's issue is being too verbose in commands/calls.,Negative
Intel,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",Negative
Intel,HUB used DX12 games that also showed the issue.  It's something else.,Negative
Intel,"The comment to which I am replying is talking about nVidia, not Intel.",Neutral
Intel,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",Negative
Intel,That's actually... just worse news.,Negative
Intel,I always dreamt of the day APUs become power houses.,Neutral
Intel,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",Neutral
Intel,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",Negative
Intel,Damn Why is AMD even involved in iGPU,Negative
Intel,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",Positive
Intel,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",Positive
Intel,almost there,Positive
Intel,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",Positive
Intel,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",Positive
Intel,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,Negative
Intel,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",Neutral
Intel,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,Negative
Intel,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",Negative
Intel,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",Negative
Intel,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",Neutral
Intel,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",Negative
Intel,yes its so bad. better go buy some steam deck or ally x,Negative
Intel,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,Negative
Intel,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",Positive
Intel,How are they going to feed all those CUs? Quad-channel LPDDR5X?,Neutral
Intel,That's considerably faster than an XSX.,Positive
Intel,>That's tapping on 4070/7800 levels of performance.  What is?,Neutral
Intel,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",Neutral
Intel,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",Positive
Intel,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,Neutral
Intel,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,Neutral
Intel,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",Neutral
Intel,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",Negative
Intel,It's called satire. You're just salty because you're the butt of the joke.,Negative
Intel,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,Neutral
Intel,Praying the blade16 gets it.,Neutral
Intel,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",Neutral
Intel,256 bit bus + infinity cache.,Neutral
Intel,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,Neutral
Intel,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",Positive
Intel,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",Positive
Intel,The rumored 40CU strix halo chip. Not the actual chips released this week.,Neutral
Intel,7500mhz ram and the 780m,Neutral
Intel,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",Neutral
Intel,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",Positive
Intel,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",Neutral
Intel,Literally where did you see 40-60% uplift at half the power?,Neutral
Intel,> 40-60% performance uplift at half the power  Source?,Neutral
Intel,"i chuckled, then again im not a fanboy of anything",Neutral
Intel,Dont expect 40CUs in a handheld anytime soon,Negative
Intel,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",Negative
Intel,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",Neutral
Intel,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",Neutral
Intel,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,Positive
Intel,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",Neutral
Intel,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,Positive
Intel,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,Negative
Intel,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",Neutral
Intel,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",Negative
Intel,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",Negative
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Neutral
Intel,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,Positive
Intel,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",Negative
Intel,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",Negative
Intel,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,Negative
Intel,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",Neutral
Intel,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",Neutral
Intel,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,Positive
Intel,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,Positive
Intel,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",Positive
Intel,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",Neutral
Intel,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",Negative
Intel,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,Neutral
Intel,"Installs beta software, proceeds to complain about it",Negative
Intel,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,Negative
Intel,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",Neutral
Intel,What Ghost of Tsushima issue?,Neutral
Intel,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",Neutral
Intel,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",Neutral
Intel,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",Negative
Intel,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,Negative
Intel,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",Neutral
Intel,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",Neutral
Intel,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,Neutral
Intel,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",Neutral
Intel,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,Negative
Intel,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,Negative
Intel,The documentation for it would still be in their archives,Neutral
Intel,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",Neutral
Intel,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",Negative
Intel,Wish Arc cards were better. They look so pretty in comparison to their peers,Positive
Intel,Thats actually a pretty solid and accurate breakdown.,Positive
Intel,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,Positive
Intel,3080 still looking good too,Positive
Intel,What they have peaceful then 4k series?,Neutral
Intel,Just get a 4090. I will never regret getting mine.,Positive
Intel,i miss old good times where radeon HD 7970 as best single core card cost around 400$,Neutral
Intel,"Damn, the A770 is still so uncompetitive...",Negative
Intel,"It's like the free market priced cards according to their relative performance. How weird, right?",Negative
Intel,How is that possibly annoying,Negative
Intel,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,Positive
Intel,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",Positive
Intel,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",Negative
Intel,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,Positive
Intel,8gb perfectly fine today :),Positive
Intel,"Ah yes sure, now where did I leave my 1500 euros?",Neutral
Intel,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",Negative
Intel,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,Positive
Intel,"Yeah, i like the black super series.",Positive
Intel,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",Negative
Intel,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",Neutral
Intel,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,Neutral
Intel,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",Negative
Intel,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",Neutral
Intel,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",Negative
Intel,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",Positive
Intel,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",Negative
Intel,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",Negative
Intel,"Yo, I saw the title and thought this gotta be Gnif2.",Neutral
Intel,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",Negative
Intel,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",Negative
Intel,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",Negative
Intel,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",Positive
Intel,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",Neutral
Intel,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",Negative
Intel,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",Negative
Intel,Long but worth it read; Well Done!,Positive
Intel,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",Neutral
Intel,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,Negative
Intel,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",Neutral
Intel,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,Negative
Intel,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,Negative
Intel,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",Negative
Intel,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,Negative
Intel,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",Negative
Intel,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",Negative
Intel,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",Negative
Intel,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,Negative
Intel,100% all of this...  Love looking glass by the by,Positive
Intel,How does say VMware handle this? Does it kind of just restart shit as needed?,Neutral
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",Neutral
Intel,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",Positive
Intel,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",Negative
Intel,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,Negative
Intel,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",Negative
Intel,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",Neutral
Intel,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",Neutral
Intel,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",Negative
Intel,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",Neutral
Intel,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,Neutral
Intel,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",Negative
Intel,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",Negative
Intel,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",Negative
Intel,TL;DR. **PEBKAC**.,Neutral
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",Negative
Intel,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,Negative
Intel,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",Negative
Intel,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,Negative
Intel,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",Negative
Intel,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,Negative
Intel,"Thanks mate I appreciate it, glad to see you here :)",Positive
Intel,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",Positive
Intel,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",Negative
Intel,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",Neutral
Intel,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",Negative
Intel,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,Positive
Intel,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,Neutral
Intel,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",Neutral
Intel,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,Neutral
Intel,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",Positive
Intel,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",Neutral
Intel,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",Negative
Intel,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",Neutral
Intel,"Funny, I saw the title and thought the same too!",Positive
Intel,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",Negative
Intel,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",Negative
Intel,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,Neutral
Intel,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",Neutral
Intel,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",Negative
Intel,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",Negative
Intel,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",Negative
Intel,ursohot !  back to discord rants...,Neutral
Intel,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,Negative
Intel,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",Negative
Intel,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",Negative
Intel,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,Positive
Intel,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",Negative
Intel,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",Negative
Intel,"It doesn't handle it, it has the same issue.",Negative
Intel,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),Neutral
Intel,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",Neutral
Intel,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",Positive
Intel,Me neither. I use a RX580 8GB since launch and not a single problem.,Positive
Intel,Because they're talking absolute rubbish that's why.,Negative
Intel,You are one of the lucky ones!,Positive
Intel,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",Negative
Intel,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",Negative
Intel,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",Positive
Intel,lol your flair is Please search before asking,Neutral
Intel,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,Negative
Intel,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,Neutral
Intel,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",Negative
Intel,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",Negative
Intel,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,Neutral
Intel,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,Neutral
Intel,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",Negative
Intel,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",Neutral
Intel,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",Positive
Intel,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",Negative
Intel,"""NVIDIA, it just works""",Positive
Intel,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,Negative
Intel,What is the AMD Vanguard?,Neutral
Intel,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",Negative
Intel,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,Negative
Intel,You misspelled $2.3T market cap....,Neutral
Intel,"Okay yeah fair enough, hadn't considered this. Removed it from my post",Neutral
Intel,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",Neutral
Intel,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",Neutral
Intel,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",Neutral
Intel,This is not a fix. It's a compromise.,Negative
Intel,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",Neutral
Intel,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,Negative
Intel,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",Negative
Intel,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,Neutral
Intel,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,Negative
Intel,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",Negative
Intel,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",Negative
Intel,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",Negative
Intel,The comment I quoted was talking about people playing games having issues.,Neutral
Intel,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,Neutral
Intel,The thing I quoted was talking about people playing games though.,Neutral
Intel,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",Negative
Intel,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",Negative
Intel,"Idk, I don't use Linux",Neutral
Intel,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",Neutral
Intel,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",Neutral
Intel,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),Neutral
Intel,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",Neutral
Intel,Because adding a feature for a product literally gives users more control for that product.,Neutral
Intel,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,Neutral
Intel,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",Negative
Intel,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,Negative
Intel,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",Neutral
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",Negative
Intel,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,Negative
Intel,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",Neutral
Intel,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",Neutral
Intel,*wayland users have joined the chat,Neutral
Intel,You're falling for slogans.,Negative
Intel,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",Positive
Intel,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,Negative
Intel,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),Neutral
Intel,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",Neutral
Intel,Honestly after a trillion I kinda stop counting 😂🤣,Neutral
Intel,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",Neutral
Intel,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",Negative
Intel,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",Negative
Intel,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",Neutral
Intel,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,Negative
Intel,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",Neutral
Intel,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",Neutral
Intel,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",Negative
Intel,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",Negative
Intel,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",Neutral
Intel,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",Negative
Intel,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",Neutral
Intel,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",Neutral
Intel,Oh then just ignore my comment 😅,Negative
Intel,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",Positive
Intel,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,Negative
Intel,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",Positive
Intel,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",Negative
Intel,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",Neutral
Intel,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,Negative
Intel,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",Negative
Intel,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,Negative
Intel,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,Negative
Intel,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",Negative
Intel,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",Neutral
Intel,"Too soon to tell, but hopes are high.",Positive
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",Negative
Intel,"Agreed, they cannot rest on their laurels.",Negative
Intel,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",Positive
Intel,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",Neutral
Intel,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,Neutral
Intel,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,Neutral
Intel,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",Negative
Intel,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",Negative
Intel,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",Negative
Intel,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",Neutral
Intel,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",Neutral
Intel,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,Negative
Intel,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,Negative
Intel,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,Negative
Intel,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",Negative
Intel,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",Negative
Intel,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,Negative
Intel,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",Negative
Intel,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,Neutral
Intel,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",Neutral
Intel,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,Negative
Intel,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",Neutral
Intel,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",Negative
Intel,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",Negative
Intel,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",Neutral
Intel,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,Neutral
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",Negative
Intel,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",Neutral
Intel,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",Negative
Intel,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",Negative
Intel,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",Negative
Intel,Oh and XE also have bug feature reporting.  Omfg!!!!,Negative
Intel,Nobody is 100% right ;),Neutral
Intel,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),Neutral
Intel,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",Negative
Intel,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",Negative
Intel,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",Neutral
Intel,What about using a DP to HDMI 2.1 adapter for that situation?,Neutral
Intel,"2021 my guy, it's right there on the date of the article.",Neutral
Intel,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,Neutral
Intel,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,Neutral
Intel,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",Neutral
Intel,And I guess infallible game developers too then. /s,Neutral
Intel,So you decide what criticism is valid and what not? lol,Neutral
Intel,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,Negative
Intel,"Yup, but do you see them making a big press release about it?",Neutral
Intel,that is not how it works but sure,Negative
Intel,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,Neutral
Intel,>whine about Redditors.  The irony.,Negative
Intel,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",Neutral
Intel,learn to comprehend.,Neutral
Intel,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,Neutral
Intel,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",Negative
Intel,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",Negative
Intel,"No, that would be you obviously /s",Neutral
Intel,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,Neutral
Intel,"Yea, given the state of XE drivers every major update has come with significant PR.",Neutral
Intel,Why not ;),Positive
Intel,Go word salad elsewhere.,Negative
Intel,"I have replicated the issue reliably yes, and across two different systems.",Neutral
Intel,If discord crashes my drivers.. once every few hours. I have to reboot,Negative
Intel,Discord doesn't crash my drivers  I don't have to reboot.,Positive
Intel,Really love how the 6000 series radeons look.,Positive
Intel,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",Neutral
Intel,That's a good looking line up,Positive
Intel,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",Negative
Intel,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",Positive
Intel,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",Positive
Intel,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,Positive
Intel,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",Negative
Intel,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,Negative
Intel,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",Negative
Intel,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",Negative
Intel,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,Positive
Intel,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",Positive
Intel,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,Negative
Intel,That 7900xtx sale number is insane,Negative
Intel,That just shows that most people that buy GPU's don't know a thing about them.,Negative
Intel,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",Negative
Intel,best discounts were 6750xt 6800 and 7800xt,Positive
Intel,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,Neutral
Intel,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",Positive
Intel,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",Neutral
Intel,"They're not out of stock there, duh",Neutral
Intel,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,Positive
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",Negative
Intel,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",Neutral
Intel,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",Neutral
Intel,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",Neutral
Intel,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",Neutral
Intel,the card is pretty bad if you missed that somehow,Negative
Intel,AMD probably ships leftover to countries in which they know it will sell,Neutral
Intel,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",Neutral
Intel,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",Negative
Intel,"As you notice the photoshop version differs, so you can't compare them really",Neutral
Intel,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),Positive
Intel,So basically PC games are never going to tell us what the specs are to run the game native ever again.,Negative
Intel,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",Neutral
Intel,The true crime here is needing FSR to reach these requirements.,Negative
Intel,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",Positive
Intel,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),Negative
Intel,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,Neutral
Intel,"well, at least the chart is easy to read, not a complete mess",Neutral
Intel,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,Positive
Intel,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,Negative
Intel,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,Negative
Intel,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",Neutral
Intel,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",Neutral
Intel,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,Positive
Intel,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",Negative
Intel,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",Negative
Intel,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",Negative
Intel,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",Neutral
Intel,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,Positive
Intel,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,Negative
Intel,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",Neutral
Intel,I hope they will bundle this game with CPUs/GPUs,Positive
Intel,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,Negative
Intel,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,Negative
Intel,This game better look like real life with those specs. I does looks beautiful!,Positive
Intel,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",Negative
Intel,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,Negative
Intel,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,Negative
Intel,I love it how absurd these things are these days.,Negative
Intel,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,Negative
Intel,Does anyone know if it supports SLI or crossfire?,Neutral
Intel,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,Neutral
Intel,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",Negative
Intel,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,Negative
Intel,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,Negative
Intel,Why in the f*ck is upscaling included on a specs page?,Negative
Intel,no more software optimization and full upscaling  bleah,Negative
Intel,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,Positive
Intel,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",Positive
Intel,"I've never even heard of this game, nor care about it, but these system requirements offend me.",Negative
Intel,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,Negative
Intel,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",Positive
Intel,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",Negative
Intel,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,Neutral
Intel,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,Negative
Intel,Nice,Positive
Intel,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",Neutral
Intel,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",Negative
Intel,Native gaming died or what ? Wtf they turning pc gaming into console gaming,Negative
Intel,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",Negative
Intel,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,Neutral
Intel,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,Negative
Intel,"Omg, it would be a graphic master piece or  bad optimized thing.",Negative
Intel,First time I see matches recommendations for nv and amd GPUs...,Neutral
Intel,Rip laptop rtx 3060 6gb,Neutral
Intel,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",Positive
Intel,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,Neutral
Intel,*NATIVE* resolution gang ftw!,Neutral
Intel,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",Negative
Intel,Looks capped at 60fps?,Neutral
Intel,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",Neutral
Intel,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,Neutral
Intel,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",Negative
Intel,Is it using UE5?,Neutral
Intel,"Ubisoft, rip on launch.",Negative
Intel,guessing no DLSS3 then?,Neutral
Intel,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,Positive
Intel,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",Neutral
Intel,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",Negative
Intel,Farewell 1660ti… looks like it’s time for an upgrade,Neutral
Intel,well my 3300x is now obsolete for these new AAA games...,Negative
Intel,4k ultra right up my alley 😏,Positive
Intel,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,Neutral
Intel,7900xtx will do 4k 120fps with FSR 3 then I guess?,Neutral
Intel,What must one do to achieve a higher rank than an enthusiast? A demi-god?,Neutral
Intel,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",Neutral
Intel,Is vrr fixed with frame gen then?,Neutral
Intel,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",Negative
Intel,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",Positive
Intel,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,Neutral
Intel,They recommend upscaling even at 1080p.. disgusting ew,Negative
Intel,pretty much this we all knew they would start using upscaling as a crutch.,Negative
Intel,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,Negative
Intel,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",Negative
Intel,My thoughts too…,Neutral
Intel,Thanks to all of you that were screaming dlss looks better than native lmao.,Positive
Intel,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",Neutral
Intel,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,Negative
Intel,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",Negative
Intel,Nope we as a community abused a nice thing,Negative
Intel,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",Neutral
Intel,Didn't take them long to make upscaling worthless.,Negative
Intel,i smell a burgeoning cottage industry of game spec reviewers!,Positive
Intel,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",Negative
Intel,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",Neutral
Intel,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",Neutral
Intel,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,Negative
Intel,yeah this is the new standard,Neutral
Intel,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",Positive
Intel,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,Neutral
Intel,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",Negative
Intel,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",Positive
Intel,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",Neutral
Intel,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,Negative
Intel,The actual true crime here is even having fsr to begin with. It should just have dlss,Negative
Intel,Seems like they tried to cover every basis with these.,Neutral
Intel,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",Negative
Intel,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",Positive
Intel,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,Neutral
Intel,"GCN support is over, RDNA1 is the lowest currently supported arch.",Negative
Intel,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",Negative
Intel,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",Negative
Intel,What do you mean forced raytracing?,Neutral
Intel,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,Positive
Intel,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",Neutral
Intel,It's actually 960p :(,Negative
Intel,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",Neutral
Intel,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,Neutral
Intel,We'll see in a year.,Neutral
Intel,AFAIK  &#x200B;  It is with RT,Neutral
Intel,Seems pretty good to me given it is at 4K with RT,Positive
Intel,"Likely includes the RT features , its also 4k",Neutral
Intel,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",Negative
Intel,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",Negative
Intel,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",Negative
Intel,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,Negative
Intel,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",Neutral
Intel,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",Negative
Intel,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,Neutral
Intel,Exactly! It even got xess so Intel users also can use xess,Positive
Intel,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",Negative
Intel,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",Neutral
Intel,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,Neutral
Intel,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,Negative
Intel,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",Negative
Intel,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",Neutral
Intel,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",Neutral
Intel,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",Neutral
Intel,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",Negative
Intel,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",Negative
Intel,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,Negative
Intel,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",Negative
Intel,"Mirage is PS4 game, Avatar is PS5",Neutral
Intel,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",Negative
Intel,Timed epic exclusivity? Aww man.,Positive
Intel,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,Negative
Intel,You... are.. joking... right..?,Neutral
Intel,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,Negative
Intel,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,Negative
Intel,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,Negative
Intel,Try ubisoft achievements :),Positive
Intel,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",Neutral
Intel,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",Negative
Intel,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,Negative
Intel,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),Neutral
Intel,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",Positive
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",Neutral
Intel,yup that is why I will play 1440 UW native with a 7900XTX.,Neutral
Intel,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,Neutral
Intel,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",Neutral
Intel,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",Negative
Intel,Very similar performance I guess,Neutral
Intel,Ye,Neutral
Intel,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,Neutral
Intel,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",Neutral
Intel,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,Negative
Intel,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,Neutral
Intel,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,Neutral
Intel,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",Negative
Intel,"with AFMF it works , didnt test with FSR3 now.",Neutral
Intel,Reading before raging :),Positive
Intel,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",Negative
Intel,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,Negative
Intel,For 30fps even lol,Neutral
Intel,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",Negative
Intel,Or be happy your shitty video card is still supported.,Negative
Intel,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,Negative
Intel,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,Positive
Intel,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,Negative
Intel,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,Negative
Intel,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,Negative
Intel,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",Negative
Intel,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",Neutral
Intel,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",Negative
Intel,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,Negative
Intel,But it does in many ways.,Neutral
Intel,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",Neutral
Intel,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",Negative
Intel,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,Negative
Intel,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",Negative
Intel,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",Neutral
Intel,speaking facts my guy,Neutral
Intel,You shouldn't have downvote dood wtf redditors ?,Negative
Intel,People with AMD cards dislike upscaling more because FSR sucks ass lol.,Negative
Intel,"Uhm, me btw...",Neutral
Intel,"> Who actually plays games at native these days, if it has upscaling?  I do.",Neutral
Intel,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",Negative
Intel,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",Neutral
Intel,onto absolutely nothing.,Negative
Intel,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",Negative
Intel,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,Negative
Intel,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,Neutral
Intel,The game is raytracing only with a ridiculous ammount of foooliage.,Negative
Intel,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",Negative
Intel,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,Neutral
Intel,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,Positive
Intel,False.  guy blocked me lmao,Negative
Intel,"I thought that was on Linux, though I might be wrong",Neutral
Intel,1070 doesn't have hardware RT though.,Neutral
Intel,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),Neutral
Intel,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,Negative
Intel,Completely agree.,Positive
Intel,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,Negative
Intel,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),Neutral
Intel,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",Negative
Intel,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",Negative
Intel,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,Negative
Intel,"Derp, derp.",Neutral
Intel,Sounds like John on Direct Foundry Direct every week.,Neutral
Intel,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",Positive
Intel,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",Neutral
Intel,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,Negative
Intel,Avatar is RT only. There is no non RT mode.,Neutral
Intel,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,Negative
Intel,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",Neutral
Intel,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",Neutral
Intel,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,Neutral
Intel,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,Neutral
Intel,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",Positive
Intel,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",Positive
Intel,"Next gen seems promising so far, low native rez hidden by upscaling, 60 FPS target on +500$ GPUs, traversal stuttering thanks to the glorious UE5. But hey, we have great reflections on puddles so at least we can take good looking screenshots.",Positive
Intel,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,Negative
Intel,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,Negative
Intel,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,Negative
Intel,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",Negative
Intel,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",Neutral
Intel,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",Neutral
Intel,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",Positive
Intel,This is what I am thinking too.,Neutral
Intel,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",Neutral
Intel,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",Neutral
Intel,Let's hope so. 30fps is far from recommended for FSR 3,Negative
Intel,Yeah I heard it works with that hoping it works with fsr3 now,Positive
Intel,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",Negative
Intel,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",Negative
Intel,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",Neutral
Intel,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,Negative
Intel,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,Neutral
Intel,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",Positive
Intel,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",Neutral
Intel,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",Negative
Intel,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",Negative
Intel,Assassins creed origins and odyssey side quests/collectibles oh my god,Positive
Intel,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,Negative
Intel,I appreciate your insights and opinions. Thank you.,Positive
Intel,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",Neutral
Intel,console games started upscaling way before PCs .,Negative
Intel,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,Negative
Intel,you forgot who owns one of the most popular engines out there?,Neutral
Intel,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",Negative
Intel,downvoted by devs lol,Neutral
Intel,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,Positive
Intel,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",Neutral
Intel,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,Negative
Intel,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,Neutral
Intel,Why is there a dialog message about unsupported hardware when you try and run a 390X?,Negative
Intel,It can do software based RT just like every other modern GPU out there.,Positive
Intel,Well then no wonder rx 5700 can't manage 30 fps lol,Negative
Intel,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",Positive
Intel,Avatars uses a Lumen like software RT solution.,Neutral
Intel,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",Negative
Intel,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",Negative
Intel,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),Negative
Intel,"Yeah, thatsl happened.",Neutral
Intel,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",Negative
Intel,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",Negative
Intel,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",Negative
Intel,You didn't understand. It's another Swiss knife engine,Negative
Intel,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",Negative
Intel,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,Positive
Intel,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,Positive
Intel,The PS5 has a 6700.,Neutral
Intel,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",Neutral
Intel,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",Positive
Intel,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,Negative
Intel,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,Neutral
Intel,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,Neutral
Intel,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,Neutral
Intel,TAA,Neutral
Intel,Temporal anti aliasing.,Neutral
Intel,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",Negative
Intel,"Ahh, welcome to r/FuckTAA",Negative
Intel,Even 4K looks blurry with some implementations of TAA,Negative
Intel,they're giving you the bare minimum until your upgrade!,Negative
Intel,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,Neutral
Intel,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",Negative
Intel,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",Neutral
Intel,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",Neutral
Intel,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",Negative
Intel,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,Negative
Intel,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",Negative
Intel,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",Neutral
Intel,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,Neutral
Intel,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",Neutral
Intel,No I havent. AMD is bigger than Epic.,Neutral
Intel,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",Negative
Intel,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",Negative
Intel,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",Neutral
Intel,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,Positive
Intel,It's software ray tracing which isn't accelerated by hardware.,Neutral
Intel,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,Neutral
Intel,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,Negative
Intel,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,Negative
Intel,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",Negative
Intel,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,Neutral
Intel,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,Negative
Intel,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",Negative
Intel,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",Neutral
Intel,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,Negative
Intel,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",Positive
Intel,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,Neutral
Intel,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",Neutral
Intel,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",Negative
Intel,Both excellent 👌 Down the Rabbit Hole was another solid one.,Positive
Intel,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",Neutral
Intel,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",Neutral
Intel,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,Negative
Intel,I have to turn it off in borderlands 3. Ugh.,Negative
Intel,r/FuckTAA,Negative
Intel,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",Neutral
Intel,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",Positive
Intel,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,Negative
Intel,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",Neutral
Intel,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,Negative
Intel,epic is tencent...,Neutral
Intel,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",Negative
Intel,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",Neutral
Intel,You don't need RT hardware to do software RT. That's what I'm saying.,Neutral
Intel,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",Neutral
Intel,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",Neutral
Intel,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),Neutral
Intel,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,Negative
Intel,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",Neutral
Intel,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,Negative
Intel,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,Negative
Intel,Yes hah,Neutral
Intel,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",Positive
Intel,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",Neutral
Intel,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,Positive
Intel,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",Neutral
Intel,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",Neutral
Intel,Oh man the hair... its so crazy how much upscaling kills the hair..,Negative
Intel,i mean they already arent the smartest bulbs considering they went team green.,Negative
Intel,Reading comrehension dude.,Neutral
Intel,"I do, but thanks for your interest.",Positive
Intel,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,Neutral
Intel,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",Positive
Intel,Nice. It’s on the list. Thanks man,Positive
Intel,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,Neutral
Intel,Probably because TAA is (unfortunately) more prevalent than it ever was.,Negative
Intel,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",Neutral
Intel,Oh my bad if I read that wrong,Negative
Intel,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,Negative
Intel,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,Negative
Intel,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",Positive
Intel,imagine being mad that 4 year old cards arent high end,Negative
Intel,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,Negative
Intel,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",Neutral
Intel,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",Negative
Intel,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",Neutral
Intel,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",Negative
Intel,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",Neutral
Intel,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",Negative
Intel,"It's for textures, not object edges from FSR use, lol. Two completely different things",Neutral
Intel,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",Negative
Intel,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",Negative
Intel,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",Negative
Intel,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",Negative
Intel,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,Neutral
Intel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",Neutral
Intel,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,Neutral
Intel,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",Negative
Intel,5700 was probably the lowest AMD card they had to test with.,Neutral
Intel,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",Positive
Intel,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,Neutral
Intel,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,Positive
Intel,AMD CAS is aimed to restore detail,Neutral
Intel,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",Neutral
Intel,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,Negative
Intel,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",Neutral
Intel,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",Negative
Intel,Yeah but a 3060ti didn't,Neutral
Intel,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",Neutral
Intel,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",Positive
Intel,Confirmed Can it run CP2077 4k is the new Can it run Crysis,Neutral
Intel,It'll be crazy seeing this chart in 5-10 years with new gpu's pushing 60-120 fps with no problem.,Positive
Intel,In 1440p with 7900 xtx with fsr 2.1 quality it doesn’t even get 30fps,Negative
Intel,"Makes sense. Is almost full path tracing, it’s insane it’s even running.",Positive
Intel,good. this is how gaming should be. something to go back to before the next crysis shows its teeth,Positive
Intel,3080 falling behind a 3060? what is this data?,Negative
Intel,wake me up when we have a card that can run this at 40 without needing its own psu.,Neutral
Intel,5090 here I come!,Positive
Intel,"Playing the game maxed out with path tracing, FG, RR and DLSS set to balanced at 1440p. Over 100fps 90% of the time. Incredible experience.  *With a 4070 ti and 13600k",Positive
Intel,"I mean, Path tracing is to Ray tracing, what Ray tracing is too rasterization.",Neutral
Intel,When Cyberpunk first came out the 3090 only got 20 fps in RT Psycho mode.  https://tpucdn.com/review/nvidia-geforce-rtx-4090-founders-edition/images/cyberpunk-2077-rt-3840-2160.png  Still does  Fast forward just one gem and you don't see anyone saying its demanding with many able to get RT Psycho on thier cards as new cards got faster.  Give it 2 gens and you are going to get 4k60 here.  Gpu will improve and get faster.,Neutral
Intel,"Look at that, my 6700XT is just 19fps slower than the RTX 4090 in this title.",Negative
Intel,It's a good thing nobody has to actually play it native.,Neutral
Intel,Well good thing literally nobody is doing that…,Negative
Intel,The future is not in native resolution so this is really pointless information.  Cyberpunk looks absolutelt mindblowlingy insane with all the extra graphical bells and whistles it has gotten over the years and with Nvidia’s technology it runs so damn smooth as well.,Positive
Intel,4.3fps lol. No amount of upscaling is going to fix that and make it playable. People were saying the 7900xtx had 3090ti levels of RT when it launched. A 4060 ti is 50% faster then it.,Negative
Intel,That's actually pretty amazing for any GPU to get a metric in frames per second instead of minuites per frame.,Positive
Intel,"Meh I get like 90+ fps with everything cranked with RR, FG and DLSS(Balanced) toggled on with my 4090 @ 4k.  Path tracing does introduce ghosting which is annoying buts not really noticeable most times but at the same time with RR enabled it removes shimmering on 99% of objects that is normally introduced with DLSS so I am willing to compromise.   Honestly as someone who used to have a 7900XTX I am disappointed with AMD its clear that AI tech in gaming is the way forward and they just seem so far behind Nvidia now and even Intel(going by some preview stuff). FSR is just not even comparable anymore.",Negative
Intel,Who cares when it looks worse than with dlss and ray reconstruction on top of running a lot worse? Native res 4k is pointless.,Negative
Intel,"Well the upscaling in this game is really good, DLSS with ray reconstructions AI accelerated denoiser provides better RT effects than the game at native with its native Denoiser.  Also Path tracing scales perfectly with resolution so upscaling provides massive gains. using DLSS quality doubles performance to 40fps, and dlss balanced gives 60fps on average, performance about 70-80 or 4x native 4K, that includes the 10-15% performance gain RR gives as well over the native denoiser as well.  I've been playing with about 60fps on average 50-70. with DLSS Balanced and RR and its been amazing. I don't like frame gen tho since it causes VRR flickers on my screen",Positive
Intel,"Running Ray Tracing or Path Tracing without DLSS or Ray Reconstruction is like intentionally going into a battlefield without any gear whatsoever, it's absolutely pointless and suicidal, what we can clearly see here though is top of the line 7900 XTX losing against already mediocre mid-range 4060 Ti by over 50%, which is just beyond embarrassing for AMD Radeon.  All this says to me is AMD Radeon need to get their shit together and improve their RT / PT performance, otherwise they will continue to lose more Marketshare on GPU department no matter how hard their fanboys thinks that they are pointless features, just like DLSS was back on 2020 right?  Also, with my 4070 Ti OC i can run it at average of over 60 FPS at 1440p DF optimized settings with DLSS Balanced + Ray Reconstruction without even using DLSS Frame Gen, with it on i can get over 80+ FPS",Negative
Intel,Running this way means you lose RR…why in the world would you run at native 4k?  It’s completely pointless now with RR.,Negative
Intel,Im having 80 fps  thanks to dlss 3.5  and its looking better than ever,Positive
Intel,"Why would I run native? I have amazing DLSS, ray reconstruction for huge image gains and frame gen. Nvidia offering all the goodies.",Positive
Intel,"I have a 13900KF-4090 rig and a 7800X3D-7900XTX rig. They are connected to a C2 OLED and to a Neo G9.  I've been holding on to play the game till 2.0 update. I've tried many times with different ray-tracing options and they all look good and all. But in the end I closed them all, turned back to Ultra Quality without ray-tracing and started playing the game over 120FPS.  This is a good action fps game now. I need high fps with as low latency as possible. So who cares about ray-tracing and path-tracing.  Yeah ray-tracing and path-tracing are good. But we are at least 2-3 generations away for them to become mainstream. When they are easily enabled on mid-range gpus with high-refresh rate monitors, they will be good and usable then :)",Positive
Intel,"If you have the HW, go all out. That's why we spend on these things.  I'm getting the best experience you can get in the premiere visual showcase of a good game.   It's path tracing. It isn't cheap but the fact that we have DLSS and FG with Ray reconstruction is a Godsend. It looks stunning and it's still early in development.",Positive
Intel,This doesn’t seem right…. 3080 performing worse than a 2080TI?,Negative
Intel,How tf is a 2080 ti getting more fps than a 3080?!?,Negative
Intel,I'm not seeing the RTX A6000 on here...,Neutral
Intel,4 fps for 7900 XTX  Oof. Shows you how much of a gap their is between AMD & Nvidia with pure ray tracing.,Negative
Intel,"Everything maxed, PT, 1440p, DLSS+RR+FG, input feels good, game looks and runs great, 100+ fps",Positive
Intel,"""Get Nvidia if you want a good ray tracing experience""  Yes Nvidia GPUs give a better ray tracing experience but is it really worth it if you are required to turn on DLSS? Imo, the more AI-upscaling you have to turn on, the worse the Nvidia purchase is.   I have a 6900XT and I will readily admit that the RT experience (ultra RT, no path tracing) at native 1080p resolution is ok, like 30-45 FPS (around 50 on medium RT settings), but if I turn RT lighting off (so RT shadows, sunlight, and reflections are still present) suddenly I get pretty consistent 60 FPS (i left my frames tied to monitor refresh rate, so 60 FPS is my max) and i can't tell the damn difference at native 1080p compared to RT medium or Ultra.   So would I spend another $400-$1000 to get an imperceptible outcome (imperceptible to me that is)? Most definitely not.",Negative
Intel,Does anyone actually play this game? Its more of a meme game imho,Negative
Intel,4.3 fps 😂😂😂,Neutral
Intel,"RTX 4090 DESTROYS 7900XTX with over 400% increased FPS, coming in at an astounding…. 19.5FPS 😫😂",Positive
Intel,"Overclocked RTX 4090 can (there are factory OC models that run up to 8% faster than stock). A measly 2.5% overclock would put that at 20 FPS.  Native is irrelevant though, DLSS Quality runs much faster with similar image quality.",Neutral
Intel,[85-115fps (115fps cap) in 4k dlss balanced](https://media.discordapp.net/attachments/347847286824370187/1154937439354368090/image.png) ultra settings,Neutral
Intel,Lol the 3060ti is better than a 7900xtx...crazy,Positive
Intel,Why would you use it without DLSS? Upscaling is the way of the future and AMD better improve their software to compete. Future GPU's aren't going to be able to brute force their way to high frames.,Neutral
Intel,Who cares about native though.  Cyberpunk PT at 4k DLSS Balanced + FG is graphically so far ahead of any other game ever released that it literally doesn't even matter if other games are run at native or even 8K. Cyberpunk is just in league of its own.,Neutral
Intel,ThE gAMe iS PoOrLY OpTImiSed!!!,Positive
Intel,Who cares about 20 fps native? Use the tools and tricks provided by the game and card manufacturer and you are good. The whole native rendering argument can be thrown in the garbage bin.,Negative
Intel,"The 7900xtx losing NATIVELY to the 4060ti 16gb is embarrassing, that card is half the price and overpriced, nevermind Nvidia's far superior upscalers. AMD falls further and further behind in RT, sad.",Negative
Intel,"99.7fps  with everything maxed out at 4k balanced with frame gen.   64.3fps with everything maxed out at 4k balanced w/out frame gen.   29.4fps with everything maxed out at 4k native.   That’s a big difference from this chart what I’m getting vs what they’re getting. For record, I’m using a 4090 suprim liquid undervolted to 950mv at 2850mhz. Stock is 2830mhz for my card.",Neutral
Intel,"Even with frame gen and DLSS quality, Ultra+Path Tracing looks incredible. I get solid 90 FPS. Funny thing is I can't get into playing CP 2077, even with incredible graphics and a QD-OLED monitor. It's just not my type of game :-D",Positive
Intel,I was wondering why a thread like this with the name like this is in this sub. And the I see the name of the OP. And it all makes sense,Neutral
Intel,"""4090 is 4 times faster than 7900XTX""",Positive
Intel,"So? Native 4k is such a waste of compute hardware, only an idiot would use it.",Negative
Intel,People need to change their mind frame. AI is here to stay and will be a major part of graphics rendering going forward.,Neutral
Intel,I played 2.0 yesterday with 2k ultra and path tracing with 155fps (DLSS3.5),Neutral
Intel,"The 4060 Ti literally better than the 7900 XTX in RT, cope snowflakes!111!11! /s",Positive
Intel,"So Nvidia only needs to increase their RT performance 3x while AMD needs to do 14x. We are not seeing that within a decade, if that.",Neutral
Intel,Sounds like Ray tracing isn’t ready for prime if nothing can run it and you have to use AI smoke and mirrors to fake it playable. I’ll worry about Ray tracing when it’s a toggle on or off like shadows and there is no major performance hit. See you in the 9000-10000 series of Nvidia or 12000-13000 series for AMD.,Negative
Intel,Native resolution is a thing of the past. DLSS looks better than native anyway.,Neutral
Intel,Is this with DLSS + FG?,Neutral
Intel,"Ray tracing already wasn't worth it imo, that's why I saved money buying an AMD card. It's not worth the cut in frame rate that then has to be filled with upscaling that makes frames look blurry anyway, so what's the point of even using it?",Negative
Intel,"Meanwhile me using medium settings + performance dlss on 3080 ti to get that locked 120fps. I just can't go back to 60fps. The game still looks amazing, especially on blackstrobed oled.",Positive
Intel,What is even with with all these path traced reconstruction bullshit lately? Ray tracing itself isn't even that mainstream yet.,Negative
Intel,No one is using these settings.,Negative
Intel,"Ray tracing is fun and all, but games have really improve much.. Same animations.. Same type of quest... Same listening to notes.. There have to be better ways....",Positive
Intel,"4K rendering for games like this is pretty pointless unless you're upscaling it to a massive 8K display via DLSS Performance.  Ultimately, the final image quality is going to be more affected by how clean the Ray / Path Tracing is than whether you're rendering at Native 4K or not.",Negative
Intel,"I'm honestly kind of shocked my 4070 is that far above the 7900XTX. I'd have thought it would be no where near close, regardless of ray tracing.",Negative
Intel,cyberpunk sucks don't worry about it,Negative
Intel,3080ti?,Neutral
Intel,"Yes, that is true, but what the numbers do not tell you is that if you tweak some settings, you are able to get 90 fps on that resolution with path tracing with no problem.",Neutral
Intel,I feel like the 4k resolution is here to stay for a couple years boys. Get yourself a decent 4k monitor or OLED TV and chill.,Neutral
Intel,"This is why DLSS3 and frame generation are so important. 4x the performance at over 80FPS with a relatively minor amount of silicon dedicated to AI, with only minor visual degradation.",Positive
Intel,"It's way too early for using ray / path tracing in games comfortably, maybe in 10 years it'll be different, but for the foreseeable future I'm more than happy with normal rasterization.",Neutral
Intel,Who cares tho? Its not even the way it's meant to be ran. This tech is is a synergy. No point in trying to 'flex' native at this point. A 4090 gets very high FPS and amazing visuals when doing things right.,Negative
Intel,can wait in 2030 with the 8030RTX low profile 30 watt card run this game with full path tracing in 4k 120fps lol (in 2030 a low end card will run this like butter don't down vote me because you didn't get it lol),Positive
Intel,"Seems like this tech is only around to give Nvidia a win. Also path and ray tracing doesn’t always look better. If the one true God card can’t run it, is it reasonable tech or is it a gimmick? Especially seeing how like no one has a 4090. Like a very small percentage of PC gamers have one. Most aren’t even spending $1600 on their whole build!",Negative
Intel,What body part do you think will get me a 5090?,Neutral
Intel,"Once they introduced this dlss upscaling anti-lag all these gimmicks I don't think we've seen true 4K native and almost anything lately.  This is maybe one game that does look that good but many of the games don't look that great and have some really serious system requirements when there are games that look much better and are much better optimized.  Companies are taking shortcuts, instead of optimizing they give you suboptimal and use FSR dlss all these gimmicks.  Don't get me wrong I always thought it was great for people who wanted to reach a certain frames per second someone with a weaker system to allow them to play at least the minimal at a steady frame rate.  It just seems like this is the normal now non-optimized games.  The 4090 is a beast if you could only get about 20 frames native with Ray tracing it tells you everything you need to know about Ray tracing.  It needs to be reworked that needs to be a new system I believe one real engine has it built in in a certain way probably not as good as Ray tracing directly.  To me it seems like ray tracing is hampering many games that tried to add it, too much focus on that instead of actual making a good game.  Let's daisy chain 4 4090s ..gets 60 fps.. and then house goes on fire!  Obviously I'm being sarcastic but realistically it's actually pretty sad a 4090 20 frames per second.",Negative
Intel,"Its not really surprising, raytracing is decades away from being truly mainstream. I applaud the misguided pioneers who betatest the technology, their sacrifice will be honoured",Neutral
Intel,"bro, for who are they making such intensive/badly optimised games? for aliens with more powerful pcs? i trully dont get it.",Negative
Intel,That not even proper path tracing.,Negative
Intel,And I thought that ray tracing was a needlessly huge performance drop for a barely noticeable difference...,Negative
Intel,"Can't wait to buy a $1,600 GPU to run a game at 20fps!!!  &#x200B;  Nvidia fans: i cAnT wAiT 🤑🤑🤑",Positive
Intel,Native is a thing of the past when dlss produces better looking image,Neutral
Intel,Lmfao ... where are all those nvidia fan boys who prefer eye-candy path trace over 60fps?,Negative
Intel,Sounds like some efficient tech. Lmao. What's the fucking point if it makes the game unplayable? RT just washes out the image and looks like shit. Kills the contrast. Can't wait for this fad to end,Negative
Intel,In my opinion it's too early for Ray / path tracing it's computationally expensive and if you are using rasterization at ultra settings not only does look nearly identical but it's easier to do the work for the graphics and that gives you more fps...,Negative
Intel,"Rather than having 2-3fps, I would go and see a video of path tracing.",Neutral
Intel,"Does this fact really matter tho? Dlss in cyberpunk is known the look on par or even better than native, frame generation will further smooth it out. I can definitely see the 7900xtx being able to do path tracing at 1440p at least once amd release fsr 3 and better once they release something to now match ray reconstruction which also give a small boost in fps making path tracing even more doable.",Positive
Intel,cool idc i wont play that shit at such shit settings,Negative
Intel,Do we need this BS? Seriously? Gpu’s will end-up costing over 2 grand soon,Negative
Intel,"And everyone loose their minds about the last ports that came out! Starfield, TLOU, etc. It´s not that the gpu´s are not good enough. Ray Tracing and Path tracing are just not optimized.",Negative
Intel,I don't even get the hype around ray tracing most games don't have it and when they do it either makes the game look blurry or isn't noticeable unless you're in a weirdly lit area or looking at a reflection,Negative
Intel,"Am replaying it with a 6950xt and a 7700k(defo bottleneck).  All maxed out(no RT) at 1440p. If I wanted 4k I would have to enable FSR.  Its completely playable imo.   Mostly 50-60fps.  For me RT vs off vs path tracing.   The game doesn't look more real or better, it just looks different.  But ignorance is bliss.",Positive
Intel,This chart is bull. Since when is a RTX 3060 faster than a 3080. Someone needs to seriously question the source of the chart,Negative
Intel,Just goes to prove once again that Raytracing was pushed at least 4-5 gens too early as a pure marketing gimmick by Nvidia,Negative
Intel,"The Tech Power Up article says:   >Instead of the in-game benchmark we used our own test scene that's within the Phantom Liberty expansion area, to get a proper feel for the hardware requirements of the expansion  So it's very hard to reproduce their data at the moment, but I've run 3 tests with the built in benchmark (run to run variance below 1%, so 3 runs are enough) and my config is averaging 25.7 fps at 4K native with Path Tracing, so the title's premise is not correct. Still, the game is not playable without upscaling at 4K with Path Tracing, so the basis of the sentiment remains correct. Benchmark data available [here](https://capframex.com/api/SessionCollections/279f9216-bdb8-4986-a504-91363328adbe).",Neutral
Intel,RTX 4090 1600€ 19FPS 👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼,Positive
Intel,"Yay i will get 13,8 FPS with my 4080 👍😂 Someone want to borrow some frames? I don't need them all!",Positive
Intel,The first card on this list a normal person would consider buying is a 4070. 8.5 fps lol.   Even with upscaling that's basically unplayable.   Then you have all the UE5 and path trace type demos where a 4090 is only getting 60fps with upscaling.. at 1080p lol.   We are not remotely ready for these technologies yet. They're at least one if not two console generations away. So multiple years.,Negative
Intel,"Looks like 4K is mainly just for videos, not gaming for the time being.",Neutral
Intel,How can you bring out such technology and no hardware can process it properly.  an impudence,Negative
Intel,the question is who need path traycing? i mean that cool and beautiful but 20 fps with a 1600dollar price tag that wild.,Negative
Intel,You could just you know... disable path tracing and get 70 fps or go below 4k,Neutral
Intel,"That what I expect when a game gets built specifically to market Nvidia, but Nvidia over estimated their technology.",Neutral
Intel,"um...im running cp 2077 on a 4090 at 70-90fps, 5k monitor with path tracing/ray reconsturction on and most settings maxed out...",Neutral
Intel,Yo guys let's make this really unoptimized game then rely on hardware manufacturers to use AI to upscale the resolution and insert extra made-up frames to make it playable on a $1600 GPU lmao.,Negative
Intel,"so it is settled, Devs have zero optimization in games.",Negative
Intel,"Love how shit the 3080 was lol, even when it came out it was shit. But somehow everyone loved it",Negative
Intel,"Or, hear me out, developers could just do better. I've seen far better looking games without the constant frame hit.   Cyberpunk hasn't looked good enough to justify the frame rates from the get go.",Negative
Intel,Once again proving RT is useless,Negative
Intel,Cyberpunk w/ path tracing at max settings seems even more demanding than Crysis was at launch.   20fps with a 4090 is insane.,Negative
Intel,"""only gamers get this joke""",Negative
Intel,"People have forgotten or are too young to remember when Nvidia had hardware tessellation features when that was first emerging. They had tessellation maps under the ground and water in Crysis that were doing nothing other than tanking AMD (ATi) performance. I am not saying this whole Cyberpunk thing is exactly the same, but it's essentially similar. In all honesty, turning everything up to 11 on my 3090 doesn't net much visual gain beyond some mirror like reflections and nothing I really care about when actually playing the game (compared to standard High/Ultra) and not pixel peeping, which seems to be the new game now for people buying expensive GPUs, so they can justify their purchase. Meanwhile, everyone else just gets on with enjoying video games instead of living vicariously through someone's 4090 at LOL 1080p",Negative
Intel,Starfield is peaking around the corner.,Positive
Intel,starfield as well,Neutral
Intel,I remember when physx was so demanding people had a dedicated 2nd Nvidia graphics card just to turn the setting on in Arkham City. Now it's considered so cheap to calculate that we just do it on the CPU lmao,Neutral
Intel,The new Crysis,Neutral
Intel,"I wouldn’t be so optimistic. Transistor shrinking is crazy hard now, and TSMC is asking everyone to mortgage their house to afford it.",Negative
Intel,How long until a $200 card can do that?,Neutral
Intel,Most improvement is probably going to AI software more than hardware in the next few years.,Positive
Intel,"8800GT giving advice to 4090:  “I used to be 'with it. ' But then they changed what 'it' was. Now what I'm with isn't 'it' and what's 'it' seems weird and scary to me. It'll happen to you!""",Neutral
Intel,GPU need to have its own garage by then,Neutral
Intel,Yep! Insane how fast things change.,Neutral
Intel,"Most likely there will be no gpu that supports path tracing and gives you native 4k 120fps in 5 years, maybe even in 10 years.  The technology has slowed down a bit. It’s increasingly more challenging to make more dense chips.  That’s why Intel has been struggling for many years already and every iteration of their cpus gives only minor improvements. AMD went with chiplets but this approach has its own problems.  Nvidia stands out only because of AI. Raw performance increase is still not enough to play native 4k even without ray tracing.",Negative
Intel,Yeah rtx 12060 with 9.5 gb Vram will be a monster,Positive
Intel,I'm willing to bet hardware improvement will come to a halt before that.,Negative
Intel,"In 10 years AI based upscaling will be so good, no one will want to natively render unless they are generating training data",Neutral
Intel,10 is too much. Give it 5.,Neutral
Intel,Transistor density advancements have been declining for a good while now. We can't expect hardware performance gains of old to continue into the future,Negative
Intel,Like the 1080 barely doing 4k30 and now we have gpus that do 4k120 id way heavier games.  Its still weord to me to see 4k120,Negative
Intel,But then the current gen games of that era will run like this. The cycle continues,Neutral
Intel,TRUE! i think 5-10 years was the actual point in time where anybody should have paid their hard earned dollar for raytracing gpus. instead ppl dished out $1000s for the RTX2080/Ti and now are sitting on them waiting for raytracing to happen for them xD,Neutral
Intel,Who knows what new tech will be out in even 4 years lol,Neutral
Intel,"And that’s when I’ll turn on this setting in my games. Fuck having to pay $1600 for a sub 30fps experience. Path tracing is far from being a viable option, it’s more like a proof of concept at this point.",Negative
Intel,"You wish, GPU makers have stagnated like it's crazy, even the graphics have peaked. I expected to see CGI level graphics like Transformers now",Negative
Intel,"...but will cost your everlasting soul, body, firstborn and parents. If it's on sale.",Negative
Intel,It is path trancing though. The technology used by Pixar to make Toy Story 4 (though they spent up to 1200 CPU hours for one frame) Path tracing used to take up to a day per frame for films like the original Toy Story. And they had their supercomputers working on it. It is a miracle of modern technology that it even runs real time.,Positive
Intel,"You basically need a 4090 to crack 60 fps at 1440p w/ dlss on quality without frame gen. It looks good, but not good enough to run out and buy a 4090.",Negative
Intel,"Same card, I just turned off RT at 4K. 75-120fps is better than 40 with muddy but accurate reflections",Neutral
Intel,"The 7900xtx is one of the worst gpus iv ever had lol on paper it looks so so good, In games it just play isn’t. Fsr is garbage, 4080 mops the floor with it :( plus the drivers from amd besides starfield have been awful this generation",Negative
Intel,I think that most of the progress will go together with software tricks and upscalers.,Neutral
Intel,"lol. And you missed the 2080Ti.   Every result under 10 fps is just to be ignored, it isn't representing anything outside of ""woot, the card managed to chuck a frame our way"".",Negative
Intel,That's VRAM for you,Neutral
Intel,we are counting decimals of fps its just all margin of error.,Neutral
Intel,If you buy a card with low ram that card is for right now only lol,Neutral
Intel,Wake me up when a $250 GPU can run this at 1080p.,Neutral
Intel,Well DLSS isn't best. DLAA is,Negative
Intel,Reviews have been saying that the game looks better with DLSS than native. Not to mention runs extremely better.,Positive
Intel,Can’t you just disable TAA? Or do you just have to live with the ghosting?,Negative
Intel,TAA is garbage in everything. TAA and FSR can both get fucked,Negative
Intel,"Yeah. 70 to 100fps on a 4080, but with 3440x1440 and DLSS-quality-FG-RR (nvidia needs new nomenclature....)",Neutral
Intel,"same, high refreshrate at 4k with optimized settings + PT + FG. With a 4080 of course, it's insane that it can look and run this great.",Positive
Intel,"Not exactly. Path tracing is just ray tracing, but cranked up to 99% with the least amount of rasterization possible.",Neutral
Intel,"> Give it 2 gens and you are going to get 4k60 here.  Assuming the 5090 literally doubles a 4090 (unlikely), that only gets us to 4K 40hz.  Assuming a 6090 doubles that, 80. which won't be bad.  Going with more conservative 50% boosts. 5090 will give 30. 6090 will give 45.  And i feel like 50% is being very generous, as nvidia have claimed moores law is dead and they can't advance beyond a 4090 by much. I'd guess we get 30% uplift in 5090 and maybe 10-15% uplift in 6090. So we'd still be under 4K30.",Neutral
Intel,Imagine buying a 4090 and then using upscaling.,Neutral
Intel,Path tracing is so much more demanding than ray tracing due to light scattering being modelled. It is a marvel it even runs.,Positive
Intel,PC gaming is in Crysis.,Neutral
Intel,Well it's a good thing 99.999999% of PC gamers don't give a shit about ray or path tracing.,Negative
Intel,You're the first other person other than myself I've run into that had an XTX but ended up with a 4090 in the end.,Neutral
Intel,"Plus frame generation works very well in Cyberpunk in terms of image quality. In some games, you need to get closer to ~80 fps output for an acceptable image quality with FG. But the FG in CP2077 is decent with 60 fps output, ~~and I get ~65-70 fps output with quality DLSS + FG at 4k on a 4090.~~ EDIT: I misremembered what I was getting. With path tracing, DLSS quality, frame generation, and ray reconstruction, I got 80.1 fps with the benchmark!  Of course there's the matter of latency, and the latency of CP2077 with FG output of ~65-70 fps isn't great. So I'll often use DLSS balanced + FG. Thanks to ray reconstruction, this now looks very close enough native 4k (to my eyes), with acceptable latency (to me), at a high framerate output.",Positive
Intel,"Nvidia features are always useless until AMD copies them a year or two later, only then they become great features 😁",Positive
Intel,"Tbh, list the games that have ray tracing right now. Pretty few.  It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it). I would personally go amd over nvidia because those 50 or so euros more that nvidia has against the amd counter parts simply are too much for just better rt and  dlss. I could spend those for a better amd card with generally better performance.  Regarding dlss, personally I would just lower the graphics before resorting to kinda bad generated frames, fsr even worse. But that's my point of view.  I find that amd still has to fix some driver issues but other than that, they are a fine brand. (so is nvidia)",Neutral
Intel,Yeh because native 4k looks worse than dlss + RR 4k,Negative
Intel,"Yeah the game with DLSS, RR and path tracing at 1440p looks amazing and with very high fps",Positive
Intel,What's the point of having $5000 PC when you're still gonna have literally the same graphics as $1000 PC then?,Negative
Intel,"This is a tech demo. That's the whole point. It's not really playable yet, but the game really is meant to showcase what is possible in the future and how close we are getting. That's what Nvidia is doing here by funding this whole project.  Crysis who many people are comparing this to, was in itself quite revolutionary for its time. The destructible environment in Crysis to this day holds up, and that was it's killer feature really.  You're gonna have swings at the future that miss as well, and that's ok.",Positive
Intel,"Did you try DLSS?  Imagine getting down voted for asking if he was using DLSS. What a fragile community. Sometimes it's okay to use DLSS, other times you better not or people will come to your house with torches",Negative
Intel,VRAM,Neutral
Intel,"Depends on what you are after i guess, is it worth it for me? Absolutely, DLSS+FG+RR with PT is a great experience.",Positive
Intel,still better than 90% of games in 2023,Positive
Intel,Arguably better now Ray Reconstruction has been added. It's quite a big image quality upgrade.,Positive
Intel,"…no it does not, unfortunately. Have you seen the artifacting in Cyberpunk?",Negative
Intel,"Someone ate up the marketing. I often see people wonder why games are so unoptimized today, your answer is right here. \^",Negative
Intel,All graphics you see on your computer screen is fake,Negative
Intel,Path tracing is more demanding than Ray tracing,Negative
Intel,"That's why I downscale from 1440p to 1080p, running DLSS and using Contrast Limited Sharpening. It looks better than native resolution and still performs well.",Positive
Intel,I guess between the 3090 and the 4070,Neutral
Intel,Yep hahaha,Positive
Intel,RemindMe! 7 years,Neutral
Intel,"No, this tech is around so game developers can sell more copies of their games by making them look better. The only reason why Nvidia is winning is because their GPUs are much better at intensive ray-tracing.",Positive
Intel,"This is an absurd, fanboyish thought lmao",Negative
Intel,dont let novidia marketing see this youll get down voted into oblivion,Negative
Intel,I would still argue it's still in proof of concept stage. The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native. That will the allow the lower end GPU'S to get 4k 60 or 1440p 60 upscaled.,Neutral
Intel,I mean I have no problem playing at 30fps dlss balance at 1080p DF optimized settings on a 2080ti with pathtracing idc what anyone says RT/PT is the future it just looks so much better than Rasterization,Positive
Intel,"Possibly a little more. Assuming that the leaked 1.7x improvement is right and path tracing performance scales the same, that would be about 34 fps at 4k native. Might be possible to hit 60fps or close to it with DLSS Quality.",Neutral
Intel,It could as well do 60fps if Nvidia adds a lot of path tracing HW into the GPU.,Neutral
Intel,Fanboyism of both kinds is bad,Negative
Intel,"It won't end, but hopefully we see games that have art designed with RT lighting and reflections in mind.  Cyberpunk isn't it. We'd need a full rebuild of the game, every location, every asset and surface remade, so it looks like originally intended by the artists.",Neutral
Intel,Since it needs more than 10gb vram,Neutral
Intel,Cyberpunk is more optimized than majority of games that came out in last two years. Just because you can't run it at 4k 120 fps with path tracing doesn't mean it's not optimized.,Positive
Intel,It’s not unoptimized at all. Game runs great for the quality of the graphics. You guys just don’t understand the insane amount of power path tracing takes. The fact you can do it at all rn is insane but once again people just throw “unoptimized” at it because they don’t understand the technical marvel happening in front of their eyes.,Negative
Intel,"No, this game is actually very well optimized. It's just extremely demanding at 4k native, which is why DLSS exists.  It would be unoptimized if it ran like this while looking like nothing special, but this is probably the game with the best graphics, period. At the very least, it's certainly the game with the best lighting.",Positive
Intel,"and people will be like ""well RT is the future""  raster was fine for over 15 years and raster games look great but we get it we must let incompetent game devs and their bosses make shit quality games because average gamer is ready to spend $100 to pre-order electronic version of horse shit and then justify it for years because they can't admit they wasted money",Negative
Intel,It's settled that you have no idea what you talking about,Negative
Intel,Its full path tracing u cannot really optimize this much.,Negative
Intel,"It always did. Most games don't scale well, Cyberpunk does.  Look at the last two years of pc ports and you will understand how cyberpunk looks and performs like it's black magic.",Neutral
Intel,"Baked illumination requires you to choose where to add light sources and how the lighting is applied to everything, it's a creative choice of the author, the AI needs instructions to know what to do, so it would need constant handholding, on the other hand RT uses simple physics to calculate how light bounced and NN to apply the light on surfaces.",Neutral
Intel,But the destructable environment and the water graphics and other things in Direct X 9 made high end pcs kneel as well.  I remember playing on my 9800gtx/+ with my Intel Q9300 quad core (lapped and oc'ed to 3.0ghz - EDIT: checked some old evga posts got it to 3.33ghz) with 2x4gigs DDR2 @1000mhz cas 5 trying to maintain a sustained 30fps at 900p resolution on my 1680x1050 monitor. And I oc'ed the crap out of that 9800gtx 835 mhz (cant recall if it was unlinked shadder or not now) core on blower air (won the silicon lottery with that evga card).  Tweaking settings was mostly user done and guided with old school limited forum help. Ahh the good old days of having lots of time during school breaks.,Neutral
Intel,"The equivalent of a 4090 at the time, 8800 ultra (768MB) got 7.5 fps at 1920x1200 very high quality. The other cards were not able to run it. Even the lowest tier last generation card runs this, even though it's at 5% speed.",Neutral
Intel,No and no again lol  The 8800gtx could do 1152x864 high and that was the top dog. DX9 cards could do that at medium but high shaders was like a 25fps average tops and that dropped into the teens on the snow level.   It took things like the gtx480 and HD 6970 to do 1920x1080 30fps max settings. That's before getting into Sandy Bridge being the first cpu that could sustain 40+ fps if the gpu power was there.   Crysis came during the core 2 duo/k8 athlon and first wave dx10 cards era and it took *2 more generations* of *both* gpus and cpus to do it some justice.,Neutral
Intel,"My 8800ultra was getting single digit fps at 1080p ultra, so CP isn't quite as demanding.",Neutral
Intel,Amd had a tessellation unit. It went unused but it was present,Neutral
Intel,"My fun story for PhysX was Mirrors Edge. I don't remember what GPU I had at the time but the game was pretty new when I played it. Ran fine for quite some time until one scene where you get ambushed in a building by the cops and they shoot at the glass. The shattering glass with PhysX turned on absolutely TANKED my framerates, like single digit. I didn't realize that the PhysX toggle was turned on in the settings. This was at a time when PhysX required a dedicated PCIe card in your system.   Once I turned it off it ran fine. Now I can run that game at ridiculous framerates without my system getting warm.",Positive
Intel,"I remember around about 2008 when companies like Asus and Dell were selling ""Physics Processing Units"" and some claimed that these would be commonplace in gaming machines just like Graphics cards had become 10 years previously.",Neutral
Intel,"Hardware accelerated physics (as in on the gpu), is different than what's going on the CPU.",Neutral
Intel,Yeah. People with Windows 7 were using AMD to play the game with old Nvidia card for Physx. Nvidia didn't like that and blocked it via driver,Negative
Intel,"People used to have ATI/AMD for main and a lower-end NVIDIA for PhysX.  When NVIDIA found this out they pushed out drivers that disabled PhysX on their cards if an ATI/AMD card was detected, limiting you to the intentionally piss-poor CPU implementation of PhysX.  Think about that crap.  One day everything's going fine for consumers, the next day NVIDIA decides they don't like how consumers are legitimately using their cards and gimps everyone, weaponizing a physics engine company that they bought in 2008.",Negative
Intel,"Oh damn, I forgot about those cards.  I wanted one so badly.",Positive
Intel,Remember when companies tried to sell physics cards lol,Neutral
Intel,>PhysX  It never was a dedicated Nvidia card - it was a [dedicated psysx](https://www.techpowerup.com/review/bfg-ageia-physx-card/) on which the tech later was bought by Nvidia and implemented in it's own GPU's.  But the things never became really populair.,Neutral
Intel,"This makes me wonder if we'll ever see something similar with Raytracing, where we get a 2nd GPU purely for RT, and then the main GPU just does all the other stuff. Would that even be possible?",Neutral
Intel,I remember when people had a dedicated PhysX card.,Neutral
Intel,The ray/path tracing in this case done by specialized hardware which has more room to grow faster.,Neutral
Intel,"I would, transistor shrinking isn't the only method of increasing performance, and honestly, these companies have to keep putting out better cards to make money.  There have been many breakthroughs over the last few years. I give it another 5 as both amd and nvidia are pushing ai accelerated ray tracing on their cards, nvidia is in the lead for now but amd will eventually catch up.",Positive
Intel,There is still a big leap possible since all lithography processes at the moment are hybrid euv and duv.   But the moment everything is done euv things will drastically slow down.,Neutral
Intel,"No those are just pennies in the pocket of Nvidia, but as the consequence you as the customer need to take a mortgage on a brand new GPU.",Neutral
Intel,"Yeah the next 10 years are gonna be spent getting budget cards up to 4090 level performance, rather than bringing the bleeding edge to 5-10x the 4090 performance. As long as 4K is king, there’s not much incentive to do the latter.",Negative
Intel,"In 10 years we will be begging one of several thousand test-tube created Musk Family members for $200 so we can buy a cheeseburger.  But the jokes on them, we’re just gonna spend it on space crack.",Negative
Intel,"Never. Even if performance can be pushed that far, by the time it happens there won't be such a thing as a $200 graphics card anymore.",Negative
Intel,It's not happening unless the market crashes and they start focusing on offering good price/performance cards instead of bumping up prices every generation.,Negative
Intel,25 years,Neutral
Intel,"The software needs to run on hardware, right now it eats through GPU compute and memory.",Neutral
Intel,And sadly ended up with 450 Watt TDP to achieve that performance.,Negative
Intel,This. We'd be lucky to see more than 3 generations in the upcoming decade.,Positive
Intel,"Having seen a few newer games on relatively low resolution CRT display, I can't help but think it might come down to improved display tech and embedded scaling. Like DLSS3 features in the display instead of the GPU.",Neutral
Intel,Not with that attitude,Negative
Intel,Love how it's still gimped on memory size 😂,Positive
Intel,"They just need to render the minimum information needed , and let the ai do the rest",Neutral
Intel,"Don't get upscaling, to me it looks shite. Why spend hundreds or thousands on a GPU to not even run at native resolution.   It's something Nvidia are pushing because they've got a better version of it at the moment.  Maybe it'll improve, it's pretty much snake oil currently imo.",Negative
Intel,"Feels pretty good with 120 frames. Playing with a controller, i don‘t mind or even feel the input lag on a projector screen.",Positive
Intel,"It's mad that even with a 4090 running games at 4k, they still look less realistic than the low-res FMV sequences in The 7th Guest from 30 years ago.",Negative
Intel,"Do keep in mind that despite both being named the same, the devil's in the details. Movies use way more rays per scene and way more bounces too.   Path tracing in CP2077 shows temporal artifacts due to denoising, something that doesn't happen in movies. It is being improved with the likes of DLSS 3.5, but it is still quite off when compared to said movies.",Neutral
Intel,"Keep in mind the systems that rendered toy story 1, costing upwards of like 300k IIRC, have less power than your cell phone today and were super delicate/finicky machines. There's a dude on youtube that got a hold of like the second best one that was made at the time and the machine honestly was really impressive for when it came out, but it pales in comparison to even a steam deck really.",Neutral
Intel,"> Path tracing used to take up to a day per frame for films like the original Toy Story  Toy Story didn't use path tracing though, A Bug's Life was Pixar's first movie to use ray tracing (not path tracing) and only for a few frames in the entire movie for specific reflections, they started using ray tracing more generally for Cars and I can't find exactly when they started using path tracing but it should be around the early 2010s which is also when the other Disney animation studios started using path tracing",Neutral
Intel,No it's not a miracle. Because it's downscaled a lot and also uses lower quality math and piggybacking on AI to make up for it. It's basically running through 3 layers of cheats to produce results that aren't that much better than just traditional raster.,Negative
Intel,"That’d be some next level consumerism, paying 1500$ minimum to turn on a single setting in a single game just to play it wayyyyy slower than you would otherwise",Negative
Intel,"Hell playing path tracing with my 2080ti at 25 FPS is still looks absolutely fantastic, I would absolutely play with pathtracing on a 4090 constantly. Idc dlss looks great with RR even at 1080p. I won’t upgrade for another year or 2 (just bought a phone so I’m broke right now)",Positive
Intel,4090 getting 60fps at 4k balanced dlss with no frame gen. 100 with frame gen. I can make it drop into the 20s if I stand right next to a fire with all the smoke and lightening effects or if I go to a heavily vegetated area it’ll drop to mid 40s. But it’s stay consistently at 55-65 and even goes Into the 90s if I head out of the city.   Haven’t tried it at quality or with dlss off though. May go do that now that it says only 19fps lol. Have to try it to see for myself,Neutral
Intel,"https://www.tomshardware.com/news/nvidia-dlss-35-tested-ai-powered-graphics-leaves-competitors-behind  Well it's so close (54fps) it's more like a 3080Ti or higher from 3000 series or a 4070TI + from 4000 series it seems? The old 3000 series is punching way above it weight vs the 7900xtx, which was meant to deliver similar RT performance to the 3090.. which it doesn't.",Neutral
Intel,At some point the RT pixels are so expensive that native resolution w/o DLSS and frame gen is just not gonna work for the time being.,Negative
Intel,"On the other hand, if you set the new DLSS 3.5 to performance (which you should in 4k), and just enable frame generation, you get 90+ in 4k with basically zero issues unless you pause to check frames.",Positive
Intel,So rendering at 960p? Oof...,Negative
Intel,"you take that back, I literally returned my 4080FE for a 4090 liquid x for more frames in cyberpunk.",Neutral
Intel,I almost see no difference.   And to see the little diffidence I need to take two screenshots and compare them.,Neutral
Intel,The game look is ruined on PT it makes everything look completely different.  Regular RT keeps the style of the game and performs good.,Negative
Intel,Dont use useless raytracing and you wont have any problems lol,Negative
Intel,"Because PC gaming blew up during a time where you could buy a mid range GPU and not need to upgrade for 5-6 years. Now those sample people buy a GPU, and 2 years later it can't run new games. At least that's my theory.",Negative
Intel,AW2 looks insane. Can't wait to play soon,Positive
Intel,Well the 3050 managed to hold with just 8GB while the 3070Yi crashed?,Neutral
Intel,"Yeah, and people insisted on defending the configurations at launch lmao. The cards just won't be able to handle heavy loads at high resolution such as this game, regardless of how fucking insane the actual processing unit is. You can't beat caching and prepping the data near the oven. Can't cook a thousand buns in an industrial oven at the same time if there's trucks with 100 coming only once an hour.",Negative
Intel,My 3080 in shambles,Negative
Intel,"It looks better than native even with fsr quality, the Taa in this game is shit",Negative
Intel,You dont need to increase raw performance. You need to increase RT performance.,Neutral
Intel,Imagine!,Positive
Intel,DLSS unironically looks better than native TAA even at 1080p in this game because of the shit ghosting.,Negative
Intel,Only people who don't give a shit about high quality graphics don't care about ray tracing.,Negative
Intel,I got a stupidly good deal on the 4090(buddy won an extra one from a company raffle) and it just so happened my XTX was one of the cards suffering from the mounting issue on release.  Honestly very happy that all of that happened. Sure I would have been happy with the 7900xtx if I got a replacement but the 4090 just kinda blew me away especially with all the DLSS tech behind it.,Positive
Intel,"Nvidia used cp77 as a testing grounds for their dlss3 and rt tech, it'd no surprise it looks and performs relatively good, they literally partnered with cdpr for that.  Do not expect these levels of RT on most games anytime soon, probably in a couple years (with rtx 5000 series) the tech will be more mature and not as taxing for most cards, because needing to upscale and use framegen to get 60fps on an RT PT setting is kinda absurd.",Positive
Intel,">only then they become great features  Watch this happen with ""fake frames"" FSR3 in real time",Neutral
Intel,>Ray tracing as of right now is a gimmick   This line was already false years ago when control launched. Now it’s just absurd,Negative
Intel,"There are a lot of new games that features Ray Tracing on them as of now, too bad most of them doesn't look as visually as good as they could be though as most of the time their RT effects are dumbed down or reduced to the point it's pointless.  But the thing is that wasn't the point even from the beginning, RT Overdrive on Cyberpunk is literally considered as a Technological showcase  What matters here IMO for all of us is PC platform in general is showcasing here what a real high-end computers can achieve beyond what current gen console can do, and they are showcasing it really well here on Cyberpunk with much better visuals and using plenty of tricks to make them more than playable, where they shouldn't be considering how much demanding they are over standard, that is impressive enough to me and shows a good sign of technological engineering innovation.   As what Bryan Catanzaro said on his interview on DF Direct, work smarter not harder, brute force is not everything, of course it still matters a lot but doing it alone is starting to become pointless just like how it is on real life.",Negative
Intel,Much more fps :) Newer generation cards have at least 50-60% better raster performance then prev gen.,Positive
Intel,The Evangelical Church of Native,Neutral
Intel,"So just to be clear, you don't like running games at native resolution? Because the purpose of DLSS is to improve performance by specifically not running at native resolution",Negative
Intel,Nice but ive been playing video games since the 70s and its Shit end off..,Negative
Intel,The future of frame generation is going to be huge. I can definitely see 360p frame rendering upscaled to 4k with 80% maybe even 90% of the frames being generated. All with equal to or even better visual quality.   A 20 FPS game like this will become a 100-200fps game.  We may be looking at the end of beefy graphics cards being required to play games. Just a good enough mid or low tier chip (granted 5-10 years from now) with frame generation might be enough.,Positive
Intel,"After 2.0, it's around 100""fps"". Just 60""fps"" (meaning with FG enabled) is far from being playable due to latency. You need at least 90""fps"" for playable experience.   Cyberpunk PT vs RT is a huge difference in terms of graphics, while DLSS Quality vs Balanced is really slim in comparison. What you say would be a really bad trade-off.   At 1440p DLSS Quality it's easily 120+""fps"" which is perfectly comfortable.",Neutral
Intel,Cyberpunk is greatly optimized. It looks fantastic for the hardware it asks. Starfield looks like game from 5 years that asks hardware from the future.,Positive
Intel,"Yeah, sure, now go back to play starfield.",Neutral
Intel,"There literally is a /s, what else do you need to detect sarcasm?",Negative
Intel,"Someone made a really cool comment yesterdays on Reddit that even a generated frame using path tracing is less fake than a rasterized frame. It is effectively closer to reference truth. I had never thought about it that way, but people really are clutching their pearls with this shit",Positive
Intel,"I know, which makes path tracing even worse off imo.",Negative
Intel,I will be messaging you in 7 years on [**2030-09-23 15:26:41 UTC**](http://www.wolframalpha.com/input/?i=2030-09-23%2015:26:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/16pm9l9/no_gpu_can_get_20fps_in_path_traced_cyberpunk_4k/k1v42p3/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F16pm9l9%2Fno_gpu_can_get_20fps_in_path_traced_cyberpunk_4k%2Fk1v42p3%2F%5D%0A%0ARemindMe%21%202030-09-23%2015%3A26%3A41%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2016pm9l9)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,Neutral
Intel,haha this is gold 🥇,Positive
Intel,"At the moment, it’s definitely what it seems like.  Especially seeing how there’s only one card on the planet that can run this. Do you have a 4090?",Neutral
Intel,">I would still argue it's still in proof of concept stage.  Not at all, in fact another AAA game with ""path tracing"" is coming out next month.  >The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native  Forget ""native"", that ship has already sailed. Nvidia, AMD and Intel are all working on machine learning techniques, and several game developers are starting to optimize their games around DLSS/FSR.",Neutral
Intel,"Nope, not even close.  Shaders 3.0 - that's improvement. Ray tracing is noticeable, but nowhere near to justify the hit.  I've looked techpowerup comparison RT vs LOW (lol). Low looked far better and cleaner, then blurry mess of RT (Overdrive).   I don't need ""realistic"" lighting, I need pleasent bisual part with good gameplay. For ghraphics I have a window in my appartment.",Neutral
Intel,"In a screenshot, sure. When actually playing the game, unless the fps is low enough to remind them, 99,99% of people will forget if they left it on or not.",Negative
Intel,Pixar movies will sometimes take the better part of a day to render a single 4k frame on a render farm. So the fact that Cyberpunk takes more than 0.05 seconds to render a 4k frame in its path-tracing mode on a single 4090 clearly means that it's unoptimized garbage! /s  EDIT: Apparently people in this thread don't understand sarcasm (or don't understand how a path-traced game can take longer than 0.05 seconds to render a 4k frame with high-poly path tracing and still be optimized).,Negative
Intel,Man you had to lap your q9300 to hit 3.0ghz? Those chips really weren't OC friendly.  My q6600 hummed along 3.2 no problem on 92mm arctic freezer and a slight voltage bump.  I was so pumped when I upgraded to a gtx 260. Crysis was a dream.,Positive
Intel,Funnily Crysis still have better destructible environments than Cyberpunk has tho,Positive
Intel,"This. People losing their shit either weren't around when Crysis launched or have forgotten just how demanding it was . PT CP kicks the shit out of a 4090, Crysis murdered my 8800ultra.",Negative
Intel,crysis did not do 1080p 60 on maximum setting and dx10 on a 8800GT.   I had a 1280*1024 monitor back then and barely had 30 fps with my 8800GT.   Even the 8800 Ultra did not do 60 fps on full hd.  https://m.youtube.com/watch?v=46j6fDkMq9I,Negative
Intel,"Man, I remember how hype the 8800gt was. Thing was a hell of a big jump compared to previous cards that came out, prolly our first REALLY powerful card.  Still remember the old xplay video where they build a PC to play crysis and Bioshock. Put 2 8800GTs in sli in there with a core 2 quad which costed one thousand dollars back then!",Positive
Intel,"My poor HD 4850 got pushed to the limit to give me 1280x1024, lol",Negative
Intel,I remember thinking $300-400 (IIRC) was so much for a GPU back around those days.  7800XT is the closest we've seen in a while and not even close to the top end for today.,Negative
Intel,This is still the case to this day because the game includes a really old DLL file for PhysX. The other day I followed the instructions on the PCGamingWiki to delete some DLL files in the game directory and only then it ran perfectly smooth on my RTX 3070.,Neutral
Intel,LOL I remember this exact scene also.,Positive
Intel,"CPU PhysX back then was single-threaded and relied on ancient x87 instructions if I recall correctly, basically gimped on purpose. Even with a 5800X3D the shattering glass reduces the frame rate to the low teens. Sadly an Nvidia GPU is still required if you want to turn PhysX effects on for games from that era, though I hear that it's possible again to use it with an AMD GPU as primary.",Negative
Intel,"OMG I had the exact same experience, hahaha I remember it vividly, I was so confused why that room would just destroy my FPS until I figured out PhysX was enabled LOL",Negative
Intel,"I'd like to see ray tracing addon cards, seems logical to me.",Neutral
Intel,"TBH, I could probably run some of the old games I have on CPU without the GPU.",Neutral
Intel,Had the exact same experience. One scene in the whole game that actually used PhysX,Neutral
Intel,You could probably run Mirror's edge with physx on today's hardware without gpu fans turning on,Neutral
Intel,Last time I tried on an R7 1700X and RX580 I still couldn't turn on PhysiX without it being a stutter party 2 fps game.,Negative
Intel,What gpu you have,Neutral
Intel,Now it even runs fine on a Ryzen 2400G.,Positive
Intel,"And they were right, PhysX and systems very much like it are still used but things have advanced so much nobody even thinks about it and it no longer requires dedicated silicon.",Neutral
Intel,"Yeah, it’s been common knowledge for many years now that Nvidia are the most ruthlessly anti-consumer company in PC hardware, and it’s not particularly close.",Negative
Intel,Ah good old Ageia before nvidia bought them out https://en.wikipedia.org/wiki/Ageia,Neutral
Intel,"No, you could actually install two Nvidia cards and dedicate one of them to only PhysX.",Neutral
Intel,"It would certainly be possible, but it wouldn't really make sense. Splitting it up on multiple GPUs would have a lot of the same problems that sli/crossfire had. You would have to duplicate memory, effort, and increase latency when you composite the final image.  It may or may not make sense to maybe have a raster chiplet and a ray tracing chiplet on the same processor package on a single gpu. But, probably makes more sense to have it all on one chiplet, and just use many of the same chiplet for product segmentation purposes instead.   A separate PPU did make sense tho, I'm still annoyed that the nvidia ageia deal essentially kill the PPU in the cradle. Our gaming rigs would cost more if PPUs became a thing, but we could have had a lot better physics then we do today. There is still a revolution in physics to be had some day...",Negative
Intel,would be cool if 2000/3000 series users could get a small tensor core only PCIe card to upgrade to framegen,Positive
Intel,"That's true, but if he's talking about *the equivalent* of a current $200-class card, I'd say about it's 10 years, what do you think?",Neutral
Intel,At least with the 4090 you can run 70% power target and still hit insane FPS while only pulling around 300w which is the same as my old 3080. The gains with that extra 150w are a couple percent at best. Not worth it to me.,Negative
Intel,Intel design will be a little bit different as far as now.  If I understood it right AMD chiplets communicate via lines on pcb but Intel wants to make something like chip-on-a-chip.,Neutral
Intel,bedroom ring library cows summer thought aspiring worm north joke   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"It's also worth noting that those early movies don't use path tracing either, Pixar switched to PT with Monster University around 2013 IIRC.",Neutral
Intel,"There is a lot of room for improvement, both in the software and future generations of hardware.  It's coming along though!  Overdrive mode looks nice, but there's just a lot more ghosting than the regular RT mode.",Positive
Intel,"They are still the same thing and should be named the same, your disclaimer is just semantics about the implementation.",Neutral
Intel,"Weird argument.  Traditional raster is cheats upon cheats upon cheats already.  In fact, all of the lighting entirely in raster is essentially cheats.",Negative
Intel,It will not work well/look good using a single frame worth of data either due to the low ray counts. Digital Foundry has shown Metro Exodus builds the lighting over ~20 frames or so and it seems like Cyberpunk is even more. Even Cyberpunk ray pathtracing doesn't look as good in motion due to that same build-up effect.  You need to cast 20-50x the rays to have enough data for a single frame but then you will be measuring the game performance by seconds per frame.,Negative
Intel,"RT looks significantly better than raster when it’s not just using simple reflections like a Far Cry 6. Raster is all cheats, the so called reflections are essentially rendering the scene or part of the scene being reflected in reverse then applying fake material effects to mimic metal, water, etc. Raster also takes HUNDREDS to thousands of person hours to get the fake reflections and lighting to look decent to good. That’s why devs love RT and want it to grow, as RT/hardware does the work for them.",Negative
Intel,It's significantly better than raster. It kills fps but the quality is great,Positive
Intel,Better graphics needing more expensive hardware is hardly a hot take.,Neutral
Intel,"Yes, better graphics costs performance. SHOCKING",Negative
Intel,People do it!,Neutral
Intel,It’s not way slower. I get 110 FPS at 4k with all DLSS settings turned on and honestly it’s insane.,Positive
Intel,"I shamefully admit I bought a (inflation adjusted) $400 console in the past to play one game. That's marginally better than buying a GPU for one setting in one game, but still.",Negative
Intel,Sounds like most nvidia fanboys,Neutral
Intel,"If you're into the eye candy, I can see a lot of people that have the money doing it.  Not everyone though. That's around a mortgage payment for me.",Neutral
Intel,And yet people will tell you how nvidia is mandatory. Like you want to overspend to play 1 game with shitty fps? I don't understand these people.,Negative
Intel,Nvidia fanboys being happy with fake frames and fake resolution will never not be funny to me.,Negative
Intel,What? I thought amd was always more tuned to raster as opposed to reflections,Neutral
Intel,Which is why nvidia is rabidly chasing AI hacks,Negative
Intel,This and rumors are saying that the 5090 is gonna be 50-70% faster than 4090 which wont be enough for native 4k either.,Negative
Intel,Say that to the people playing upscaled games at 4k (540p) on PS5.,Neutral
Intel,"I understand some RT effects might not catch everyones eyes, but seriously, path tracing cyberpunk vs 0 RT cyberpunk IS a big difference",Neutral
Intel,Huh? [Watch this is in 4K on a big screen.](https://www.youtube.com/watch?v=O7_eHxfBsHQ&lc=UgyO2vRiSI6CRU5kYmV4AaABAg.9uyI5P8oamz9uylTN_rdmL) The differences are really apparent in each version.,Neutral
Intel,"PT looks way better in the game to me - it makes everything in the environment have a more natural look to it.  It adds, not ruins it...",Positive
Intel,"That's how I feel as well. It looks awesome don't get me wrong, but it doesn't look like cyberpunk to me.",Negative
Intel,It's not a perfect way of measure but you can clearly see how (at least on Nvidia GPUs) the 8/10GB cards are way behind the >11Gb cards. Meaning you need 11 or 12GB of VRAM for this scenario which cripples the 3080 but not the 3060.  We said from the start these configurations are shit but no-one listened. There you go.,Negative
Intel,The crypto miners did me a comical solid by preventing me from acquiring many countless times and hours I wasted (came from a gtx1070 before finally being able to upgrade). Was able to get my 6900xt eventually for around $600 with 2 games. Becoming increasing thankful for the extra Vram these days.   Once I start getting through my game backlog and into the gard hitting ray tracing ones will hopefully upgrade to something with at least 24gigs of GDDR# lol.,Positive
Intel,"I mean, Alan Wake 2 is also releasing with path tracing so my guess is we're definitely gonna see more games featuring heavier RT, although not necessarily path tracing due to the performance hit.",Neutral
Intel,"I expect path tracing support to be the exception for a while because most developers will consider the non-trivial dev time to implement it not worth it. However, I'm sure Nvidia would be willing to throw dev help at most developers who would be willing to implement path tracing with their help.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode. Presumably, a less complex world helps, but they may also have implemented opacity micro maps, which I'm not sure if Cyberpunk has done in the 2.0 update. Perhaps they're cherry-picked examples for the trailer.",Neutral
Intel,Raytracing now is a gimmick *  ( * - in AMD sponsored games where RT effects are so tiny you have to zoom on to notice them),Negative
Intel,"Except this is a tech demo. CP77 has lots of cut corners to achieve that, NN was trained to keep the interface stable in that game and even then, ray reconstruction creates horrible artifacts at some angles and distances. And that's with Ngreedia's engineeers literally sitting at CDPR's office.",Negative
Intel,You can already have 120fps on $1000 PC,Neutral
Intel,It improves both looks and fps so it is a win win,Positive
Intel,"The issue with this is the inputs will only be registered at 20Hz. This would feel awful. The benefit of high FPS is having very low latency. This will just make it look smoother, not feel any better than 20 FPS",Negative
Intel,Considering there is no input on the generated frames that is going to feel horrible to play. Base FPS (incl DLSS) really still needs to be 50-60fps to feel playable imo.,Negative
Intel,I'm thinking will there ever come a day where 100% of frames are generated by AI. Like the data inputs directly from the card into the neural network and it no longer requires physical hardware limitation to generate frames.,Neutral
Intel,"Don't forget that all NPCs will be controled by AI to simulate a real life.  They'll wake up in the morning, shower, commute to work, work, then go home.  All this to do it over and over and over until they die...  mhhh... this is kinda sad to type out.![gif](emote|free_emotes_pack|cry)",Negative
Intel,It absolutely is. I’ve been playing at 1440p with DLSS Balanced and Ray Reconstruction and it is absolutely the worst version of DLSS I’ve ever seen. It’s basically unusable.,Negative
Intel,There are horrible artifacts with RR. And ghosting. It replaces artifacts with different ones. Needs a lot more work tbh. Or training.,Negative
Intel,"The thing about ray and path tracing is that they are demanding on their own, not because they are badly optimised",Negative
Intel,"I have a 4070. It runs the game just fine with the tools that have been set in place.  This is absolutely not a pissing match. Cyberpunk is demanding, and takes advantage of lots of tech available for upscaling and ray tracing and frame generation. AMD card’s currently do not match Nvidia’s in those categories, not even close. This us not an attempt to showcase Nvidia tech, but it does take advantage of it.",Positive
Intel,Because it's not about the flagship card. Only if the flagship can attain good performance at native then you will have headroom for lower end cards. Think about it. Of course upscaling is very important. But you also need some headroom to upscale.,Neutral
Intel,"Yeah. I wanted better cooling performance after upgrading to the artic freezer 7 as well with those weird crapy plastic tension clips you pushed in manually, fearing bending them (worse than the stock intel cooler tension twist lock). Kept having random stability crashes until after i sanded it down for better thermals...Good old sandpaper and nerves of steel fearing an uneven removal of the nickel coating.  Was trying to maximize performance as back then core 2 duo and the extreme versions were king and they had way higher single core clocks and were easy to oc. Wanted the multhreaded for Crysis, LoL (back when you had to wait 5min+ to get into a game), BF2, and was eventually playing TF, Day of Defeat and Natural Selection.  My upgrade back then was to the Evga 560ti DS, which I ended up installing a backplate and  a gelid cooler. They had wayy better thermal tape/individual heatsinks for the memory/vram chips for the heat transfer. Evga back then told me if I ever needed to RMA it that I would just need to re-assemble it as it was shipped. Remember using artic solver ceramique instead of as5 due to it potentially not degrading as fast as well.   Good times =)",Negative
Intel,"Yea same here, didn't play crysis until I got a hold of a 260 after having 2 8600gt's in sli. Played mostly wow at the time and those 2 8600gt's in sli got blown away by the gtx 260, but man that card was hot!  Remember in 2011 upgrading to a gtx 570, the first gen with tessellation and playing dragon age 2 which was one of the first games to have it. Ended up turning off tessellation cause it hit the card too hard, least until I got a second 570 in sli, which sadly died like a year later due to heat while playing Shadow Warrior.",Positive
Intel,Q6600 was the bomb. I ran mine at 3.0GHz all its life and it's still alive today. Had it paired with an 8800GT which was IMO one of the best GPU value/performance of all time.,Positive
Intel,Had 3.4ghz on my q6600 didnt want to hit 3.6ghz on modest voltage but 3.4 all day and was quite the lift in fps in gta4 compared to stock 2.4ghz. Run 60fps 1440x900 no problem those where the days when OC gave actual performance boost,Neutral
Intel,> I was so pumped when I upgraded to a gtx 260. Crysis was a dream.  In those days I was a broke teenager and I remember upgrading to a GTS250(rebadged 9800GTX+ with more vram) and crysis still hammered my Athlon x4 965(?) system.  While my richer neighborhood buddy upgraded to a 260 just to play Left 4 dead.,Positive
Intel,"> q9300  Yes, it was not a good bin. I was at 4.0GHz with my Q9550, with just a copious amount of voltage.",Negative
Intel,"After checking sone old Evga posts, (mod rigs server is done, along with old signatures), I was able to get to 3.33ghz.",Neutral
Intel,"Yeah my lapped Q6600 was my daily driver at 3.2 GHz with a Tuniq Tower 120 Extreme. It ran it pretty cool. I think when Crysis came out it and my 8800 GTX could do 20-25 frames at 1680x1050 with drops into the teens. EVGA replaced it with a GTX 460 when it died, big upgrade.",Positive
Intel,moving data between the two is the issue,Neutral
Intel,grey cagey sharp detail handle north grandiose connect sparkle hungry   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"I wonder the same thing. But I guess if Nvidia would put it all in an extra card, people would just buy more amd to get the best of both worlds.",Neutral
Intel,Game physics doesn't seem to be a focus anymore though,Neutral
Intel,I'm ashamed I even own one NVIDIA card.    Unsurprisingly it's in a laptop that I bought during the [illegal GeForce Partner Program](https://hothardware.com/news/nvidia-geforce-partner-program) before the public knew about the program.  Not making that mistake again.  Screw NVIDIA.  Every other card I've ever gotten has been ATI/AMD (and obviously the low-end integrated Intel GPUs on some office laptops)  EDIT: Keep them downvotes coming.  You've all been bullshitted by NVIDIA's marketing.  Your boos mean nothing; I've seen what makes you cheer.,Negative
Intel,Anyone who owns an Nvidia GPU should be ashamed of themselves tbh.,Negative
Intel,"There was more nuance to that ""bugfix"" than just ""it was a bug that got fixed""   Modders were able to re-enable the AMD/NVIDIA PhysX combo basically immediately via editing DLLs.  Not driver replacements, but actual DLL tweaks.  The speed at which modders were able to fix what NVIDIA ""broke"" combined with the public outrage put them in a funny legal spot.  If they continued with PhysX disabled and claiming it was a bug then the bug can easily be fixed, as shown by modders doing it first.  So either fix it and get everyone back to normal, or leave PhysX disabled even though it can be easily enabled and open the company up to potential anti-competitive lawsuits.  Obviously not many modern games use the GPU implementation of PhysX anymore and the performance difference between current GPU and CPU implementations are negligible anyway, but their intent at the time was clear once it was shown how quickly modders were able to get everyone back to normal.",Neutral
Intel,"Yes, but not after Nvidia bought Ageia.",Neutral
Intel,"If the $200 cards of today aren't 3x better than the flagship cards of 10 years ago (they're not), then you shouldn't expect a $200 card in 2033 to be 3x faster than an RTX 4090. Average annual performance gains are more likely to decrease between now and then, rather than increase.",Negative
Intel,I like how you said: static image  because in motion upscaling is crappier,Positive
Intel,"It isn't a weird argument. Rasterization is premised on approximating reality, while RT is simulating it.  The extreme few amount of rays we trace right now, including bounces, means effects are often muted and the performance hit is enormous. To compensate, we're using temporal upscaling from super low resolutions *and* frame interpolation.  Temporal upscaling isn't without it's issues, and you know how much traditional denoisers leave to be desired. Even Nvidia's Ray Reconstruction leaves a lot to be desired; the thread on r/Nvidia shows as much, with ghosting, artifacts, etc. all over again. It's like the launch of DLSS 2.0.  All of that, for effects that are oftentimes difficult to perceive, and for utterly devastating traditional rasterization performance.",Negative
Intel,"It's a shame raster is so labor intensive, because it looks so interesting when done right.     I look at screenshots from cyberpunk, Raster / Ray Trace / Path Trace, and in most of them the raster just looks more interesting to me. Not constrained by what would be real physical lighting. The 100% RT render versions of old games like Quake gives a similar feeling.     But I imagine there will be more stylistic RT lighting over time, it saving a ton of labor is all things considered good, freeing it up for actual game design.",Positive
Intel,">That’s why devs love RT and want it to grow, as RT/hardware does the work for them.  It's the marketing departments that love RT. Devs hate it because it's just yet more work on top of the existing raster work.",Positive
Intel,"It *is* way slower, you're just compensating it by reducing render resolution a ton",Negative
Intel,If it was bloodborne i am guilty of that myself,Negative
Intel,price zealous rinse detail yam rainstorm groovy automatic snails fact   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,4k max without dlss or fsr or frame gen I still get 30fps. Sorry you can’t even hold 10. Guess I’ll shill some more to at least not be spoon fed shit from amd,Negative
Intel,"Ray tracing and path tracing aren't just ""reflections"" , but this thread is specifically about the new path tracing modes in CP2077.",Neutral
Intel,Rasterisation is a “hack” too,Negative
Intel,If it works it works....computer graphics has always been about approximation,Positive
Intel,"4090 is \~66% more powerful than 3090 overall, but it has 112% more fps in this test.     The RT capabilities of NVIDIA cards has grown faster than general capabilities.",Positive
Intel,"I guess on the internet, you can just lie. There’s no game on ps5 that upscale from 540p to 4k.",Negative
Intel,"I guess I was wrong on that one, although SWJS upscales from a resolution as low as 648p, which is not much.",Negative
Intel,It's big in certain lighting situations. In others it's not as noticeable as you'd hope,Neutral
Intel,"PT vs no RT comp is silly.  U should compare PT vs RT, and there, the difference is minor and not even subjectively better. Just different.",Negative
Intel,Natural? It over saturates the light bleeding into the entire scene. Everything becomes so colorful. Even when floors or ceilings are not supposed to be reflective. Literally tiles with grainy texture suddenly becomes a mirror..  It's ridiculously overdone.,Negative
Intel,Guys I'm trying to suffer while I play the game what settings will let me suffer the most,Negative
Intel,"I mean this is native RT/TAA. Was never meant to be played this way. You need to use DLSS and Ray Reconstruction. With that setting, i get about 40-50fps at max settings with my 3080. Not too bad.  The thing about AMD is that they don't have any of this AI technology (yet). You have to rely on raw power, which won't get you far.",Neutral
Intel,"Until the next generation of consoles, I think path tracing will be added to something like a couple major games per year with Nvidia's help. If I'm right, that's wouldn't be a lot, but it's definitely be a bonus for those who have GPUs that are better at ray tracing.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode (which could be for a variety of reasons, including CP2077's larger world).",Positive
Intel,Alan Wake 2 is also an outlier since the same dev made Control which was a test bed for DLSS 2.0 and one of the earliest games to fully embrace ray tracing. Alan Wake 2 is also NVIDIA sponsored and looking to be another test bed for them.,Neutral
Intel,"Redditors trying to have reading comprehension (impossible)     >It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it).",Neutral
Intel,"Every single game has cut corners, starfield has multiple cut corners. Tech demo would imply there's no great game under great visuals.",Negative
Intel,Get that fps with a 5120*1440 240Hz monitor,Neutral
Intel,"It's almost like crushing your fps with all these fancy new graphic features isn't worth it, even on the best cards. It's user preference in the end.  That said, many gamers, once they play at high hz screen and fps, find it very difficult to go back to low fps for any reason. Games feel slow by comparison, no matter how great the lighting looks.",Negative
Intel,"Both DLSS and FSR have weird image artifacts (FSR is worse though) so i guess if you don't see them then DLSS only works in your favor then.  I personally want as little noticeable image artifacts from DLSS/FSR when I play so opt for native resolution, generally.",Negative
Intel,Have you tried dlss quality. Dlss balanced sacrifices image quality and dlss quality doesn't/can enhance it sometimes,Negative
Intel,">a lot of the comparisons just make it look like it gives too much bloom  This is due to colors being compressed to SDR. In HDR that ""bloom"" looks awesome as it's super bright like it would be in real life but still not losing any details. You lose those details only on those SDR videos.  PT just looks and feels way more natural than just RT, when only raster literally looks like it was two different games from two different hardware generations.  Once I saw Cyperpunk PT there is no way I'd imagine playing this game in any other way. All other games look absurdly ugly next to it now. It's something like when you get a high refresh screen and can't look back at 60Hz anymore.",Negative
Intel,"Trust me, I know how the technology works. That doesn't change the fact that it's not worth it imo. I'm not  paying 4090 money to have the newest fad that just makes games look over saturated and glossy.",Negative
Intel,We are talking about running the game just fine now are we? Because rasterization is just fine. I will consider ray tracing reasonable when you can do it maxed out for $1200(GPU) native. Until then it’s a gimmick. IMO,Positive
Intel,"So even you, someone who is an enthusiast of this sort of thing, i.e. a small minority of the userbase, had to spend \*minutes\* benchmarking to realize you didn't have RT turned on.   Yeah, you would lose that bet.",Negative
Intel,Those wolfdale C2D were beasts. I had a buddy with one and we took it 4.0. And it kept pace/beat my C2Q as most games didn't utilize MC as well back then.  Man XFX and EVGA all the way. Shame they both left the Nvidia scene.,Positive
Intel,"Ah, WoW. That started it all for me on an C2D Conroe with ATI x1400 on a Dell inspiron laptop freshman year of college.",Positive
Intel,8600gt in SLI man you are Savage!🔥,Positive
Intel,"I had the same setup, and i managed to get my Q6600 stable at 3.2.  Was a blazing fast machine and it still shit the bed in some parts of crysys",Positive
Intel,there there’s no way there will ever be another 8800 GT. you got so much for your money.,Negative
Intel,"Those 9800gtx+ were solid cards, it's what I upgraded from.  You make due with what you had.  My first system was a Craigslist find that I had to tear down and rebuild with spare parts from other systems.",Neutral
Intel,Seemed like a perfect use case for the sli bridge they got rid of.,Positive
Intel,Why would it need to send the data to the other card? They both feed into the same game.,Neutral
Intel,Big up the Vega gang,Positive
Intel,"That is because destructible buildings and realistic lighting REALLY does not go hand in hand. Realistic looking games use a ton of ""pre-baked"" light/shadow, that might change when ray tracing is the norm but it has a delay so things still look weird.",Negative
Intel,"I remember playing Crysis, flipped a wheeled garbage bin into a hut, which came down on top of it. I blew the wreckage up with grenades and the bin flew out, landing on it's side. Took pot shots at the castors and the impact made them spin around in the sockets.   Here we are 15 years later and game physics have barely moved an inch from that",Neutral
Intel,Cause regular stuff is so easy to simulate or fake simulate that super realistic complex items are not really worth the trouble.  &#x200B;  water still looks like crap in most games and requires a lot of work to make it right.  and even more processing power to make it truly realistic.  Cyberpunk is perfect example.  the water physics is atrocious.,Negative
Intel,It's because it's not the big selling point now and as the comment you're responding to... Things have gone so far no one really thinks about it anymore. Ray tracing is what physx used to be or even normal map and dx9/10 features you don't consider now.,Negative
Intel,"Honestly, same here - I hate Nvidia so much for what they do so shamelessly to the average consumer that I’d struggle to recommend their products even if they *were* competitively priced.",Negative
Intel,"People are just stupid, if game companies just slap ray tracing on and don't tweak the actual look of the games lighting, gaming graphics is doomed. Rt is not the magic bullet people seem to think it is, everything looking 'realistic'=/= good art design.",Negative
Intel,"I thought RT was loved by devs, as it allowed them to avoid the shadow baking process, speeding up dev time considerably?",Positive
Intel,We are guilty of the exact same sin.,Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"All rasterization does is approximate colors for all the pixels in the scene. Same thing does raytracing, path tracing, AI reconstruction and whatever other rendering techniques there are.   The final result is what's most important, doesn't really matter how it's achieved",Neutral
Intel,would you care to explain ? Kinda interested to here this,Neutral
Intel,jellyfish follow simplistic plucky quarrelsome unwritten dazzling subsequent mourn sable   *This post was mass deleted and anonymized with [Redact](https://redact.dev)*,Negative
Intel,"It isn’t minor. The PT with DLSS 3,5 is quite a bit more accurate and reactive, as in dynamic lights are reacting closer to instantaneous than without. The best way is to watch on a 4K monitor or to actually play it on someone’s 4090/4080 or wait for AMDs FSR 3 to see if that helps it do path tracing.",Positive
Intel,> Literally tiles with grainy texture suddenly becomes a mirror  PT doesn't change what the materials are bro. You are just seeing what the materials are meant to look like.,Neutral
Intel,Turn on path tracing. Embrace the PowerPoint.,Neutral
Intel,Control also has some of the best implementations of RT I've ever seen. It's one of the very few games I've turned on RT and felt like it was worth the performance hit. Everything else I've played has looked very good with RT on but not good enough to be worth the performance hit.,Positive
Intel,"Of course it is not perfect but it also allows max RT, PT, RR which improves the graphics greatly...and i do not enjoy below 100 fps either",Negative
Intel,"I tried 1080p DLSS Quality and it was just as bad, but I’m not really happy with the performance of 1440p DLSS Quality (like, 30-40fps), so I didn’t want to use it.",Negative
Intel,"You're is coming off as salty that you *can't* afford a 4090. If other people wanna spend the extra cash and turn on all the bells and whistles, why does that bother you?",Negative
Intel,"Mate, you can't fucking run starfield at good fps, a game that's raster only that looks like from 5 years.  Look at how many games scale and run badly in last two years and compare that to cyberpunk",Negative
Intel,"This just absurd and frankly entitled. Not everyone can afford a PC strong enough to run the most demanding games absolutely maxed out.   I never could, couldn’t even afford a PC until this past summer.   This obsession with performance metrics is ridiculous. Like, less than 5% of PC gamers even have 4k monitors. Almost 70% still have 1080p.   CDPR made a VERY demanding game that looks GORGEOUS completely turned up. Modern hardware cannot handle it natively, it can’t. It is not a gimmick that a company has developed a superior technology that allows the game to be played at almost the same quality as native, significantly smoother and faster. You’re in denial if you think otherwise.  I’m not tied to either brand. I have an AMD CPU bc it was the best option for what I wanted, and a Nvidia GPU for the same reason. I care about RT, frame gen, all of that. The Witcher 3 still is my favorite game, and the entire reasoning for building a gaming PC over a console is graphical quality and speed while doing so. That game is absolutely gorgeous with RT on. Same with Cyberpunk, which I can enjoy thanks to DLSS.   You don’t care about those things? That’s fine, but it is solely a personal opinion.",Negative
Intel,"Yea well I was defining an arbitrary performance. It's not about having native performance. But, but if a flagship is able to get 4k 60 with path tracing at native then that allows headroom for 60 tier or 70 tier cards to perform well.  In this example the 4090 gets 19.5 fps at 4k. The 4060ti gets 6.6 fps. Let's assume a next gen flagship is able to reach 60 fps at 4k. That would theoratically allow the 60ti card to reach around 20 fps. Once you do your various upscaling and frame generation techniques you can then reach 60 fps ideally with a mid range card.",Neutral
Intel,"The quad core showed up the core 2 duos like 2 yrs after because of emerging support and I was on cloud 9. After evga left, i jumped ship with my XFX 6900xt and I think I am here to stay with team Red until value/longevity says otherwise.",Neutral
Intel,Shame you don't. They did it for a reason.,Negative
Intel,The 9800GTX was a bad ass. I'd consider it the biggest upgrade in generation until the 1080 and then the 4090.  It was far more powerful than anything near it.,Positive
Intel,"I do miss it, but it was a pain in the ass to get working with my custom cooler due to the HBM.",Negative
Intel,They've actually gone a bit backwards. Devs dont seem to care about implementing physics anymore. It's just an afterthought. And you can forget about destruction,Negative
Intel,"I gobble up the downvotes every time I say it and I don't give a crap.  Every other corporation sees through their bullshit.  Otherwise every console would have NVIDIA cards.  No console has an NVIDIA card anymore.  But the general population is more than willing to get bullshitted into buying their cards against their own interest.  EDIT: Forgot about Nintendo Switch, as is tradition.",Negative
Intel,"Anyone can say anything.  Anyone can change their intent after the fact.  Happens all the time.   Judge them by their actions alone and ignore what they say.  Going solely by **events** :   1) People use (mostly used) NVIDIA cards for driving PhysX   2) NVIDIA disables this ability, locking everyone to the god-awful CPU implementation at the time   3) Modders edit DLLs to get functionality back   4) NVIDIA THEN says it was a bug that will be fixed    It's not foil hat because I'm coming to this opinion based solely on what's measurable and real; events and actions.   It's not conclusive, but I'm not going to rely on any company's PR statements to sway me one way or another because they will (obviously) say whatever they're able to prevent lawsuits and public image damage.  Based strictly on **events** that looks like damage control on a bad decision.  They rectified it, even more recently made PhysX open source, I give them credit for making it right, but the intent at time time was clear.",Neutral
Intel,This is why I'm excited for Alan Wake 2. It is coming out with path tracing always having been part of it. This will definitely mean they built the art style around that.,Positive
Intel,"Yeah, I can't really blame people for not recognizing great design if they are not interested in it. I don't think most people are interested in design, even though we all benefit from it without knowing it.     People might get some sort of subtle feeling when they see some great craft made by a experienced professional, but they can't really put it into words or evaluate the quality of the design without experience. They likely can't tell what cause the feeling to begin with even if they can notice it.     But everyone can instantly judge how real something looks, which is the go-to standard for ""good"". And bigger numbers are better, even if they are fake. Niche features that nobody uses also sells, as long as it's a new feature with a captivating name.     This reminded me. Live action lion king, what a travesty, stripping away all the artistry and expression for realism.",Neutral
Intel,"If you're using RT exclusively maybe, but no one in the games industry is - nor will they be any time soon.",Negative
Intel,"A lot of raster techniques reduce the complexity of lighting into very naive heuristics. Like shadow mapping renders the scene from the light's pov to create a depth map, then the objects that are at a greater depth have darker colors drawn (I'm simplifying that but roughly how it works). It's like teaching a kid how to illustrate shadows on an ball vs the how light actually works. Raster techniques have evolved a lot since then, and use raytracing in limited contexts, screen space reflections and ambient occlusion for example, but they're still operating with extremely limited data in narrow, isolated use cases, which is why they often look awful. There's just not enough information for them to look correct, just enough to trick you when you're not really paying attention.",Negative
Intel,All lights in a rasterised scene are “fake”.   Someone explains it much better here:  https://reddit.com/r/nvidia/s/eB7ScULpih,Negative
Intel,Or a bag of fake tricks.,Negative
Intel,">Every scene is lit completely differently with PT enabled.  It's really not, and it's very disingenuous of you to try and pretend it is. There are plenty of screenshot comparisons showing the difference in certain areas if you want me to link them, and I have the game myself and use a 3090.  There are differences, but saying ""every lit scene looks completely different"" with PT Vs regular RT is false.",Negative
Intel,"I'm not talking about DLSS 3.5. I'm talking about PT vs regular RT in CP2077. The visual difference is subjective, it looks different, is it better? Not in my opinion, it over saturates the scene.  DLSS 3.5 is a separate issue, it fixes some of the major flaws of PT, the slow reflection updates.",Negative
Intel,Have you tried Metro Exodus Enhanced?,Neutral
Intel,At less than 4K resolutions Quality is the only sensible option. The resolution it upscale from becomes too low otherwise and DLSS then struggles to have enough data to make a good image.,Negative
Intel,"It doesn't, I have no problem with people buying what they want. My problem is people assuming I can't afford something...",Neutral
Intel,What is this? A reasonable comment in this dumpster fire of a sub?,Negative
Intel,Thinking that a $1200 graphics card should be just strong enough to run everything Max is entitled. I wear that crown.,Positive
Intel,"I've got an evga 3080ti now. I suspect my next card is either going to be an XFX 8800xt, or catch an XFX 7900xtx on clearance.",Neutral
Intel,"I don't blindly support a single company, such as simply switching to AMD because my chosen vendors left nvidia. That's how competition stalls.  That being said the card I have now is a product of what I could get my. Hands on during the crypto boom, and is EVGA.  My next card is likely to be an XFX 8800xt/7900xtx.  So not sure why you're in here attacking me.",Neutral
Intel,Except it was just a rebadged 8800GTX/Ultra,Neutral
Intel,"I mean, the Nintendo Switch is using Nvidia graphics, albeit it’s a tiny 28nm iGPU from 2014… but I get your point. They’ve opened up a large technological lead over AMD thanks to the RDNA 3 debacle, and I’m *still* recommending only Radeon GPUs because Nvidia is just that hubristic.",Neutral
Intel,Pixel art go brr,Neutral
Intel,Yeah they will. Lumen and lumen style gi systems will have to exist for fallback so the industry can move on.,Neutral
Intel,"Yeah, I agree. It’s just a shame because it’s not really playable otherwise. DLSS with ray reconstruction is a little mixed.",Negative
Intel,"It just doesn’t make sense. Game developers do not set GPU prices.   The game runs well on a $1200 GPU. Perfectly well. Not completely maxed though, and it’s weird to think the game shouldn’t be able to make full use of available technology if it wants to. My GPU cost less than half that and runs the game very well with RT on thanks to DLSS.",Negative
Intel,"RR is not perfect, but I think overall Cyberpunk 2.0 looks so much better than it did previously - and it looked pretty damn good earlier too!  [Digital Foundry's video](https://www.youtube.com/watch?v=hhAtN_rRuQo) is again the yardstick for how many small things they point out where RR improves the situation, even if it has some things where it fails atm. But maybe DLSS 3.6 will solve those.  I do feel DLSS itself has also improved in the past few years in terms of ghosting and performing at lower resolutions.  On a 1440p monitor these are the starting point resolutions:  - Quality: 1707x960p - Balance: 1485x835p - Performance: 1280x720p - Ultra Perf: 853x480p  So even the highest quality option is sub-1080p. I wish Nvidia introduced a ""Ultra Quality"" preset that would be say 80% per axis resolution, something between DLAA and DLSS Quality. At 1440p this would be 2048x1152.",Positive
Intel,"Speaking of the AMD overlay, am I the only one that gets near zero CPU utilization every time they pull it up? GPU utilization fluctuates up and down depending on resolution and graphical settings, so it seems to be fine, but my CPU utilization readings never go above more than a few percent tops. I have a 5800X3D.",Neutral
Intel,GN has already made a [video about it.](https://www.youtube.com/watch?v=5hAy5V91Hr4),Neutral
Intel,Honestly this is everything that I wished Adrenalin overlay had. It's pleasantly lightweight too.   Hats off to Intel's development team. They didn't need to publish this at all.,Positive
Intel,I wonder if this will get added to Mangohud and Gamescope.,Neutral
Intel,This is quality. Great work.,Positive
Intel,Doesn’t capframeX uses presentmon as its monitoring tool?,Neutral
Intel,I can finally see if it really is the ENB taking down my Skyrim gamesaves,Neutral
Intel,"FYI, the open-source version has been available for years, was last updated 9 months ago, and does not contain an overlay or GPU busy stats. (It also has version number 1.8 while this new thing is v0.5)  Maybe they're planning to release source code later, but right now the source is closed.",Neutral
Intel,This is pretty damn awesome and will be incredibly useful when they develop the future features Tom mentioned. (insightful metrics),Positive
Intel,"This is not bad at all. But it's not replacing Radeon overlay for me... yet.  1. Needs an option to minimize to system tray.  2. Many important Ryzen CPU metrics (temperature, power consumption etc.) aren't available without an ability to access Ryzen Master SDK. 3. Radeon Overlay can continue to monitor on the desktop, which I use constantly. I tried manually hooking DWM.exe and it won't. I might just be dumb, but I couldn't get it the overlay working without 3D running. 4. Will give credit where credit is due, unlike Radeon overlay, it works right with OpenGL. 5. And the credit is immediately removed, because unlike Radeon Overlay, it doesn't work right with DXVK.  6 out of 10.",Negative
Intel,This is so good. The only thing I dislike is that there isn't a way to scale the whole thing up an down.,Positive
Intel,"At this rate Intel will have a more polished driver and software stack than AMD in 2 or 3 years, if not before.  Seriously, I suck for being able to buy an AMD GPU like the 6990 I owned, but I can't get near any hardware they made while using my system for work.  I need a CUDA alternative that works on more than 6 GPUs on specific linux distributions when the red moon shines above my building.",Negative
Intel,it's sort of misleading to call it opensource    when whatever intel beta version released is just MSI installer and binaries inside ...    i see no source of what is available for download ...   what's on gihub is something old w/o overlay,Negative
Intel,Thanks Intel! I will try this out at least since I hate MSI afterburner.,Positive
Intel,The download link gives a warning in Windows:  >This site has been reported as unsafe   >   >Hosted by intelpresentmon.s3.amazonaws.com   >   >Microsoft recommends you don't continue to this site. It has been reported to Microsoft for containing harmful programs that may try to steal personal or financial information.  ???,Negative
Intel,"I really wanted to test with it why I was getting stuttering in Apex. Unfortunately, due to some work, I would be possible on Sunday.",Negative
Intel,Doesn’t work for me. It crashed at the start with an error message and made Dolphin run way worse.,Negative
Intel,And AmD gives far more than Nvidia.,Neutral
Intel,I can't get this to work. Is anyone having the same issue?  CPU Temperature (Avg) NA C  CPU: 5800X3D,Negative
Intel,I have a 5800x3D and PresentMon does not recognize it. Says UNKNOWN\_CPU.  Anyone?,Negative
Intel,People have reported that cpu usage in Radeon software is not very accurate.,Negative
Intel,I saved this because it is posted so frequently lol  https://www.reddit.com/r/AMDHelp/comments/14n55gd/cpu_usage_percentages_differ_between_task_manager/jq66gop?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button,Positive
Intel,"Probably overlay wasn't updated for W11 22H2.  Microsoft updated something and CPU reading was broken for MSI Afterburner also, after working the same for a decade or more before. They fixed it in the meantime.",Negative
Intel,"I had the same problem, googled around and found the problem to be c state in the bios. Just turn that off and it should be accurate.",Negative
Intel,Just use afterburner as OSD.,Neutral
Intel,I had that issue with my 5600X both with Nvidia and AMD overlay. More recently it randomly fixed itself.,Neutral
Intel,"Yeah adrenalin was useless for it. It was displaying 1-2% usage for me, but msi afterburner, hwinfo, and rtss were showing much more, like 30-50, which made more sense.",Negative
Intel,You beat me to it :),Positive
Intel,"Yup, ""new Intel"" definitely seems to be getting nicer and with a ""younger"" company culture. Also, it's a smart move. They're building a nice little ecosystem around Arc cards, which is eventually what is going to drive sales when performance and stability matches NVidia and AMD.",Positive
Intel,And it's open source!  I'm liking Intel more and more since they're getting their ass kicked.  Apparently it was one guy's pet project.,Positive
Intel,Technically it should be possible to add in MSI afterburner because it's open source,Neutral
Intel,"The new PresentMon shows more information.  Since it's open source, capframeX can implement this as well.",Positive
Intel,It was a pet project of one of the Intel engineers.   6/10 is not bad!,Positive
Intel,It's still beta. I'm sure they'll fix it.  Intel has amazing engineers.,Positive
Intel,Intel is working on their 2nd generation GPU and it's said to compete with Nvidia highest tier.  And now AMD isn't planning to release a big RDNA4 chip.,Neutral
Intel,"didn't nvn get merged with mesa recently edit: sorry, I mean NVK not NVN",Neutral
Intel,I hate afterburner and RTSS. This is way better,Negative
Intel,"people probably spammed microsoft because this PresentMon also gives you the option of enabling amd, nvidia, or intel telemetry.",Negative
Intel,"Thank you for contributing exactly NOTHING to this discussion.  Truly, the world is a better place because thanks to you.",Positive
Intel,"Tell that to anyone that needs CUDA. Or GPU accelerated workloads in general.  Id suck for being able to get something like the 6990 I owned back then, but between lack of support for almost all workloads and the driver being shit since like forever... Nope.  Yes, yes, nvidia's control panel is old and looks like a windows 98 app. But it at least works and actually saves youre settings, unlike AMD's one that half the time simply forget about everything.  I currently own 5 systems. 2 nvidia ones, 3 amd ones. And all of the amd ones had driver related issues.  You can't simply work with that. Is not viable. And as a gamer you should not need to see if the drivers are working or not.  Fuck, amernime drivers exists just because how hard amd sucks at it.",Negative
Intel,"Well, everyone uses RTSS anyway and it gives you basically everything.",Neutral
Intel,Similar issue here with an i5 12600kf. Can't get temperature readings from it. I'll need to look at it more.,Negative
Intel,"Really? Good to know it's not just me, then. Nonetheless, this seems like a pretty egregious oversight on AMD's part. If I'm not mistaken, CPU metrics are on by default in the overlay and every time someone switches it on, it's an extremely visible reminder of work that AMD still needs to put into the drivers.",Negative
Intel,Speaking of said software why does it keep setting gpu max frequency to 1200MHZ?,Neutral
Intel,"Thanks for sharing your findings. It's pretty damning if even the Windows Task Manager does a markedly better job than Adrenalin. That's probably an understatement since, as I found out via other helpful Redditors here, the Adrenalin overlay may as well be broken. I'm just glad it's not something wrong with my system.",Negative
Intel,Afterburner fucks with my settings in adrenaline,Negative
Intel,"NVIDIA's highest tier? Nah, but around RTX 4080 is the target, which might be bad news for AMD.",Negative
Intel,"By NVN do you refer to Nintendo's API for nvidia? Or you wanted to refer to NVK that got into mesa?  By CUDA alternative I mean stuff like ROCm. CUDA have so many years of development on top of it and a stupidly widely support from software vendors.  While yes, NVK could help to alleviate the issue, is not even close to what native CUDA can do regarding GPU usage for general computing.  The issue with ROCm is mainly a lack of support on a lot of software and lack of support for a lot of hardware.  On the nvidia end you can get the cheapest trashiest GPU and it will have the same level of CUDA support as the most expensive enterprise product.  Yes, performance is not the same, but I can use a 3090 Ti for loads that needs more than 12 GB vram without needing to buy a dedicated AI GPU, and if a given task needs serious shit, then again, I can buy a dedicated solution that will run the exact same software with the exact same code I ran on the ""weak"" 3090 Ti.  That is not possible with AMD, if you buy an enterprise grade GPU from them, you need to build all your software stack for it instead of just moving it and keep going.  And while yes, developers from tools like PyTorch COULD improve their ROCm support, I totally get why they are not doing that. ROCm works on like what? 7 GPUs? On specific linux distros?  Is a matter of scalability and costs. ROCm needs to be supported on ALL AMD GPUs in order for it to be something worth developing for, otherwise there is not really a use case where the end user wont be writing their own stack.  And again, if you can start small, then scale up, nvidia is the only option, so AMD on this space is either ""I got a stupidly great offer where AMD provides the same hardware power and performance per watt at like half the prize so I justify building all my software from scratches"" or ""Ok, I'll go for nvidia and just use all the software already built for it and call it a day""  I really, really want to being able to use AMD GPUs aside of trivial tasks, but this is a very, VERY gigante issue that appears a lot, especially on the profesional space.",Neutral
Intel,Except PresentMon seems to not be compatible with Ryzen CPUs at all... :/,Negative
Intel,I get downvoted for asking a valid question?,Negative
Intel,Thank you for continuing to contribute Nothing to this conversation.,Positive
Intel,Clearly not more than this beta of presentmon,Neutral
Intel,I use amds for overclocking and that's about it really. But it's still far more tools than nivida gives,Neutral
Intel,"To defend AMD here as loathe as I am lately. CPU utilization isn't a very useful metric to begin with, so it misreporting would be super low priority.  The stat tells you nothing useful about the hardware, just the thread scheduling. CPU could be stuck in wait and reporting a high number while doing no work or it could be bottlenecking somewhere else while reporting a low %.",Negative
Intel,Where are you seeing this?,Neutral
Intel,"The Adrenalin overlay is good for really everything except usage. But yeah, this has seemed to have been a problem for a good while now. Ive been using AMD for a little over a year now at least Adrenalin and the CPU usage was always wrong on two different CPUs (AM4 and AM5) as well. Me and that other person tested it on my system and he had 3 systems running different AMD CPUs and all were the same. Ill find the thread one day but its been a while so it is way down the list of my comments.",Negative
Intel,"False setup.  Disable the button in the ui ( not settings)  For ""load on startup""  You literarily told afterburner to load it's settings on start up.  Huge mistake many people do because most people think it's ""boot on startup"" but in reality it's loading it's oc settings and stuff effectively overwriting adrenaline if you leave that enabled.",Negative
Intel,3070 Also was their target for their first generation. I don't expect Intel to suddenly be great just because that's what they aim for.,Neutral
Intel,And XeSS 1.1 looks better than FSR.  XeSS 1.2 was just released and it has even more improvements.,Positive
Intel,Great news to gamers though,Positive
Intel,ah sorry I meant NVK,Neutral
Intel,"I don't know, I didn't downvote you.",Neutral
Intel,My point is if you've got nvidia card you simply use 3rd party soft and got everything and more. So comparing that AMD's software to Nvidia's is kind of meaningless.,Neutral
Intel,"I actually agree with you, but I don't need the metric to be some scientifically accurate reading of performance or whatnot, I just think it'd be a useful tool for adjusting game settings. For example, if I'm trying to optimize performance in a game, does this setting make CPU utilization go up or down? How much almost doesn't matter. Is there a bottleneck, where is it, and am I relieving pressure on it with these settings or not?",Neutral
Intel,It’s where you oc in adrenaline,Neutral
Intel,"There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Not because upscaling, but also because how important is to have drivers that work in tandem with specialized hardware units in the GPU.  Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  AMD fails hard on 2 of those things.  Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.",Positive
Intel,"Yep, XeSS is just great, I happily use it in MWII on my Arc A770. But I still use DLSS on NVIDIA. But both are better than FSR.",Positive
Intel,"Then you should be looking at ***GPU*** usage, because CPU usage will not help you answer any of those questions",Negative
Intel,"Problem is, by itself it still tells you little to nothing. If GPU utilization drops like a brick you know something is bottlenecking. If CPU utilization changes in the absence of other data you know absolutely nothing of value.   The real kicker is the CPU utilization may not even be ""wrong"", it may just be polling at a very low rate because that's the other thing with these OSDs if you poll too frequently for up-to-date data that can kill performance from the overhead.",Negative
Intel,"> There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Agreed, not to mention that when they made Alchemist, yes their target was 3070 and they underperformed that, but they will eventually hit their targets as they learn to improve their architectures. For a first shot, getting to around 85-90% of their target is actually quite good. It's not like they undershot by 50% of the target. They got close.   But like you said, software is super important and these days having good software is half of a driver. I have an A770, the driver actually pretty good, like the software menu. It used to suck when it first came out because it was some crap overlay, but over time it's become a separate program and it works really good considering they're not even a year into Arc's release. They also have a good set of monitoring tools built in and the options are all simplified. They don't have super unnecessary things like a browser integrated into the driver suite, or a bunch of other stuff. I'm sure by the time Celestial is around if Intel does stick with Arc, that they will be ahead of AMD on the software game, assuming AMD continues down the same path they have now.  > Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  Yep, tools like [this one from NVIDIA](https://youtu.be/-IK3gVeFP4s?t=51) have made it easier than ever as a dev to optimise for NVIDIA hardware.  > AMD fails hard on 2 of those things. Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.  Yep because Intel recognises that to get close to NVIDIA they not only have to make great hardware, but also great software to go along with it. You can only do so much with hardware to match NVIDIA. You have to make great software along with in-game solutions like XeSS to match NVIDIA. FSR is behind not only DLSS but now also XeSS. AMD is just not competitive with NVIDIA on all fronts. Arc is just getting started, give Intel 5-10 years of time in GPU and they will outpace AMD for sure.",Positive
Intel,"There are many settings that directly impact CPU usage, so of course CPU utilization reading can be useful.  Not everything needs to be precisely accurate, this is a lousy defense. Getting 0% reads all the time is entirely worthless but some accurate reads is still useful information.",Neutral
Intel,"You are going out of your way to defend a broken feature.    7800X3D shows up as 1% CPU utilization during games such as Watch Dogs, Stellaris, Age of Empires. Clearly broken. Undefendable.",Negative
Intel,"And with a bit of luck, they will also hit AMD in the console market.  Right now AMD stays afloat GPU speaking because consoles, they were during a lot of years the only vendor offering both CPU and GPU in a single package, reducing costs A LOT.  Nvidia was not able to compete against that with the lack of CPUs and Intel with the lack of GPUs.  Now that Intel is working on dedicated GPUs they are going to eventually create APU like products, its just the natural outcome of producing both CPUs and GPUs.  And then AMD will NEED to compete against them instead of being the only option.  And if that happens, unless FSR gets turned into a hardware specific tech like XeSS that falls into different techs depending on the GPU its running on, AMD will be murdered.  Better software stack, better general hardware to accelerate and improve upscalling keeping costs down AND being a second player pushing prices down towards Sony and Microsoft?  AMD will be screwed up if they don't up their game. And hard.",Neutral
Intel,"Right, so what kind of useful information can you tell me if I raise graphics settings and CPU utilization goes from 12-16% to 16-17% while framerate goes down from 120 fps to 110 fps?",Neutral
Intel,"All CPU utilization tells anyone is thread scheduling, and nothing about what the threads are or aren't doing. Like the only situation that it marginally tells someone anything is how many threads a program uses. But that usually doesn't fluctuate with games, games usually scale up to <x> number of threads and that is that. GPU utilization is the one that is far more useful for spotting bottlenecks, general performance observations are far more useful for tweaking.  Even Intel and co. will tell you that with modern processors and all the elements they contain as well as hyperthreading/SMT CPU utilization isn't a very useful stat anymore. Meant a whole lot more when CPUs were one core unit and didn't also include the FPU and memory controller and everything else.",Neutral
Intel,"Cause unfortunately even if it wasn't broken, it's not useful in any meaningful capacity. Should it be broken? No. Would anything notable come of it if it were fixed? Also no. Given the bugs they got and room for improvement elsewhere it's one of those things that should be super low on the priority list or just removed wholesale.",Negative
Intel,"That whatever setting you are changing does not have much of an impact on CPU utilization, and if your GPU allows, you can use it.  What is the purpose of this question, are you really not aware of any settings that impact CPU usage? Some RT settings have big impacts on CPU usage too.",Neutral
Intel,"The problem you're missing here is that settings that are CPU demanding generally don't have an easily observable impact on CPU usage, if they have any impact at all.  The reliable way to see if a setting is CPU demanding is to turn it up, see the frame rate go down, and then look at how **GPU** usage changed.",Neutral
Intel,"I'm aware that some settingd impact CPU usage, but I think looking at the CPU usage is about as useful as your daily horoscope, or AIDA64 memory benchmark",Neutral
Intel,Congratulations on the new gaming laptop. Don’t let people get you down and not everyone has to have the very best to be happy so if it makes you happy then I’m happy for you.,Positive
Intel,"Congrats the 30 series isn't half bad. Laptops aren't bad but gaming desktops add more umph to their performance. My first gaming rig was a gaming desktop with 64gbs ddr4 3200mhz, rtx 3060 12gb, 850w platinum psu, b660m gigabyte motherboard and a intel i5 13600kf. Couldn't have had a better first rig.my current build now is a hellhound raedeon rx 7900xt,gigabyte  z790x ax gaming motherboard,  850w platinum psu, 64gbs ddr5 6000mhz,Samsung 980 1gb ssd, intel i5 13600kf(watercooled).best upgrade I made for 1440p gaming.",Positive
Intel,"Congratz!  Also, what's that wallpaper? Looks awesome",Positive
Intel,I don't understand the stickers. Ryzen CPU Check. Both Radeon and GTX graphics? Is one discrete and the other dedicated? Just needs an Intel sticker to make it more confusing.,Negative
Intel,"That is not a ""gaming rig"" not even close my friend. ;)",Negative
Intel,3050ti?  Gaming rip?  No,Neutral
Intel,"5 or so years ago i built my first real gaming rig. Few years later i bought new laptop, because i already had a gaming pc i wanted something capable to game but didnt want to spend too much money for high end. So i bought Lenovo legion with same gpu - 3050ti.   What i soon realized was that i love the comfort of being able to browse the net and game from the couch, being able to put my legs up, put my back against a big pillow etc... Despite having much more capable pc, i was lazy to play on it and was using gaming laptop instead 90% of the time.   Problem is, 3050ti in the last year or so really starting to show its limitations, most of new games are straight up unplayable anymore. Even on lowest settings it doesnt run newest games at playable standards, probably because of only 4gb of vram. Hogwartz, Rachet and Clank, Remnant 2 all pretty much unplayable unless you drop resolution to 720... I really regret not putting few hundreds more for something like 3070. (unrelated, but why stepping up laptop gpu tiers are so expensive, for example 3060 to 3070 is like 300e here for essentially same system otherwise, 3070 to 3080 is like 4-500e extra more, its even more expensive overall than desktop gpu tiers for way lesser performance improvements...).   So my advice would be, if you can get at least 3060, its worth it to pay up a bit more, ideally 6600m or 4060, those few hundreds increase in price will be worth in a few years.  ALSO: not sure if OP knows, but these laptops come with shittiest ram, upgrading it to a dual channel dual rank is pretty much a must as it nets 10-20% more performance.",Neutral
Intel,There is nothing wrong with laptops I'd build my own if there was a market readily available for it like a desktop.,Neutral
Intel,No!,Neutral
Intel,"This isn't exactly what I'd imagine hearing the words ""gaming rig"" but I'm glad you like it. Happy gaming!",Positive
Intel,Ignore the haters dude. Congratulations on your new gaming laptop. Gaming is something that is supposed to make us happy not cater to snobs. Not everyone needs 4k 60fps. If it satisfies your requirements it's the best rig for you. I have a Ryzen 5 2500u and I game on that so I am super happy for you.,Positive
Intel,"People take things too seriously, and tend to forget that every enthusiast was once a novice.  This is OP's first gaming setup; they deserve to feel that excitement.",Positive
Intel,who even buys the 'very best' laptop anyway?,Negative
Intel,Lenovo Ideapad Gaming 3,Neutral
Intel,"It's just the default one that came with the laptop, theres a red and a blue version probs from AMD and Intel lol.  You should be able find them online.",Neutral
Intel,Its a ryzen cpu with integrated radeon graphics and an nvidia dedicated gpu. Its completely normal and not really confusing.,Neutral
Intel,APU + dGPU,Neutral
Intel,"You know what I mean.   It's my very first PC/Laptop set up so I'm definitely calling it a rig, it's way more powerful than anything I've owed before it.",Positive
Intel,"It games, it's a gaming rig. :)",Positive
Intel,you do realise not every wants or needs a 4070 to play games they like,Neutral
Intel,I upgraded the ram. Has 16gb ddr 5 dual channel now,Neutral
Intel,"Thanks dude! I think people forget that not everone needs the same kinda thing from a computer which is why they sell multiple models.  This is great for what I need it for, and I feel like a kid at Christmas, happy to be able to dip my toe into PC gaming, finally!",Positive
Intel,You get my point at least you should,Neutral
Intel,"Just to let you know , these laptops have different versions , one I have is something like Ar15mhqp or some shit like that   It is less powerful than yours but decent enough",Neutral
Intel,That is okay if you see that way and congrats.,Positive
Intel,"There's nothing ""rig"" about a laptop. Laptops aren't built to be rigged.",Neutral
Intel,"No, it's a notebook.  Also have a low end gpu dude.  Not a game rig. It's nice but is not that.",Negative
Intel,"I know, but the same laptop or any other with 6800m or other AMD gpu would have been a much better choice lol",Neutral
Intel,All that matters is that your happy with it,Positive
Intel,Glad you’re enjoying it. People something think that unless you get something with everything maxed out then you couldn’t be happy lol,Positive
Intel,"Yeah I'm just happy to be able to start playing around on some games on PC now. I've wanted to play flight simulator on pc instead of my xbox for ages now becuase of all the addons,and now i'll have access to those :D",Positive
Intel,"3050ti isn't really low end. And it will do for everything I need it for. I have an Xbox too for the most modern stuff.  People think that everyone has to have 4k 60fps in every game these days and not everyone needs or wants that, especially in a first machine.",Positive
Intel,"for you maybe, this fits my needs perfectly.",Positive
Intel,"Yup I am more than happy!  I've only ever had and played with integrated graphics on any other laptop I've had so this is a huge step for me.  I will play most brand new stuff on my Xbox for the best ecperience but stuff like KF2 I can now play a 1080p ultra and get 100s frames, could only play that before at 900p low.",Positive
Intel,Well you say that right now but one day a game you really wanna play will come and your pc will be shitting itself trying to run it. If I had the money I would buy the best rig.   But if you don’t have a lot of money like me it’s all about price/performance,Negative
Intel,That is the important that you are happy but still the reality is one.  Just enjoy the notebook.,Positive
Intel,"not too similar but I can recommend grabbing dirt rally 2.0 on a steam sale with all DLC for 8€ / $10 or whatever it is, generally games that simulate some things are pretty much at home on PC",Neutral
Intel,"Yes, the 3050 ti is a low end card.",Neutral
Intel,I have an Xbox too so I'll be able to play it on there anything that is out of spec.,Neutral
Intel,I have a friend playing Baldurs Gate 3 on a 980/ You don't know what low end is apparently lmfao,Neutral
Intel,It's better than the 1650 and the 2050 though. And they were the other choices.,Positive
Intel,And that is normal the game don't requiere a big gpu to run the game is a mid / low graphics.  Besides the 980 was the top of that generation. The 3050 is the low end of that particular generation.,Neutral
Intel,"Dont let the haters bring you down dude, if you are happy, then enjoy your new laptop!",Positive
Intel,"Both cards are the lowest version of his generation.  Again the notebook is nice but is a low end ""gaming"" notebook and again the point is that you ENJOY IT and play many games.",Negative
Intel,He needs to know what he have but also I congrats OP for your purchase. The point is that you enjoy it.,Positive
Intel,"It's small but a decent performer, if you ask me. Intel needs to keep them coming.",Positive
Intel,A single slot version would've been nice.  70w doesn't need a dual slot cooler.,Neutral
Intel,12W idle power... here does the dream of low idle server... :I,Neutral
Intel,"I should have waited, i could have gotten two!",Neutral
Intel,they need if they want to survive,Neutral
Intel,I was thinking opposite ... can we have full passive model?,Neutral
Intel,nvidias alternatives here are also dual slot,Neutral
Intel,"Yeah, a multi-billion corp that's been dominating the CPU market for decades , with a 75% market share needs an entry level Pro card to survive. Take your meds.",Neutral
Intel,Something akin to a KalmX 3050,Neutral
Intel,"company that is falling apart, that had very bad run recently, firing a lot of crew, canceling huge investments that already costed billions.        Having foundries ... that no one want to use and not having load because products don't sell.             Big fall quickly as they have huge costs that cannot be easily scaled down.               Yes Intel is in very bad shape, it needs a good product that will be competitive and what is most important rebuild the brand that is in shambles now.",Negative
Intel,"There is good news, the employees got their free coffee back",Positive
Intel,"Way to go Intel, keep them coming.",Positive
Intel,[https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw](https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw)  From Wendell Level one tech   **The Intel Arc B50 is the Better Alternative to Nvida's A1000**,Neutral
Intel,"[The Intel Arc Pro B50 is designed for professional users, architects, engineers, AI developers, and video professionals, who need a stable, memory-rich, and energy-efficient tool for demanding tasks.](https://timesofindia.indiatimes.com/technology/tech-news/intel-expands-professional-gpu-lineup-with-new-arc-pro-b-series/articleshow/121288086.cms)",Positive
Intel,three fiddy,Neutral
Intel,>16GB VRAM  Fuck yes.,Positive
Intel,"$350, SR-IOV and 16 GB of VRAM   Freedom from green for workstation users",Positive
Intel,"I got a Dell Micro workstation with a 285, 64gb of ECC memory and their fastest SSD.  But I didn't get an Nvidia card as they seemed overpriced and the A1000 doesn't support 6k monitors!  The built in Intel graphics support higher resolution than the cheap Nvidia cards...  I'm going to buy this.",Positive
Intel,Anyone have a link to where I can buy one at ?,Neutral
Intel,Where is link for it I been looking for this card 😭will micro have it,Neutral
Intel,"I would curious how this Card can perform vs RTX 3050 6GB in terms of gaming.  Since Slot Powered GPU are very rare these days on release which the demands are high for those.     And yes, i know, Workastion arent designed for gaming",Neutral
Intel,"Waiting for a single slot low profile card. Screw me I guess for choosing a case with such requirements. I already got the a310, but it is limiting to just transcoding with its small video buffer.",Negative
Intel,god do i wish this was an option for me,Positive
Intel,I want 4 of these.,Positive
Intel,I wonder to ask who is main audience for this GPU?,Neutral
Intel,WHERE GPU,Neutral
Intel,"That's pretty attractive, need more benchmarks on it.",Positive
Intel,How would it compare to Blackwell RTX PRO 1000 and Radeon Pro W7400?,Neutral
Intel,"Sad to not see any gaming benchmarks, yes not intended but still would be nice to see",Negative
Intel,"8K resolution, but with compression? No, thank you!",Neutral
Intel,Just work on the memory bandwidth guys. Quadruple it (or at least double) and these will sell out like hot cakes.,Neutral
Intel,For gaming Nvidia RTX 4000 SFF will run circles around this. The only con is price. It's a titan of small form factor.,Neutral
Intel,"Looking forward to his Linux testing of this card, it could be interesting (And on that note I'm also looking forward to the B60 and the dual B60 cards partners are allowed to make).",Positive
Intel,was just thinking 70W and 16GB of ram no way is this a gaming card lol. awesome value though for an AI dev.,Positive
Intel,"Check out the level 1 techs video, on Newegg right now",Neutral
Intel,"Pre-ordered on Newegg yesterday (9/4), expected delivery 9/18 +-. Just checked back a few minutes ago (12:30am, 9/5) and shows Out of Stock. Don't really need it (and I ordered a ARC A750 on release day that I didn't need ...lol), so I guess it's a me thing.",Negative
Intel,"Still a valid question though. Good enough is, well.. good enough when you're on a primarily business oriented machine. Would be nice if it could run some titles at min or close to min quality settings.",Neutral
Intel,Timespy scores:  3974 - rtx 3050  4269 - hx370 890M  4485 - intel arc a380 with a310 cooler mod  4833 - intel arc 140t  6041 - rtx a2000 12gb  6551 - rtx a2000e 16gb  8514 - Intel ARC b50  9329 - 4070m 8gb @ 70W  11003 - rtx 4060  11261 - ryzen 395+  13564 - rtx 5060,Neutral
Intel,"this the card where that might be achievable,   id chuck the next gen equivalent into that sexy minisforum ryzen ai max pc thats coming soon..",Positive
Intel,"People running professional applications that require a lot of VRAM and precise compute.  CAD software, photo editing, video editing, digital painting, transcoding, broadcasting, servers.",Neutral
Intel,People who use shared remote login who do need gpu processing power while logged in remotely.,Neutral
Intel,https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007,Neutral
Intel,"This card is not designed for gaming. Or if it was, it's a joke to compare it to an A1000, considering that the A1000 is for a low end CAD or pro video workstation. Intel is not even trying to compete in the gaming market.",Negative
Intel,"There isn't much of a point to it, it's a slow card. It will be slower than a B570. It will be slower than an RTX 5050. For *gaming*, find a review for something that gets called a waste of sand for being too slow and too expensive, and this will be more expensive and slower than that.",Negative
Intel,"Based on specs and power draw, it should be around a GTX 1070  Edit: Based on Level1Techs' review it's probably more like a 1070 Ti, or somewhere around these two",Neutral
Intel,That's like asking someone to run a marathon in basketball shoes.,Neutral
Intel,level 1 tech did them,Neutral
Intel,This is not gaming gpu...,Negative
Intel,The gaming benchmarks have already been posted here.   [https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw](https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw),Neutral
Intel,"The B60 is supposed to have double the memory bandwidth (456 GB/s vs. 224 GB/s for the B50), and the B60 comes with 24 GB memory (And 192 bit vs. 128 bit for the B50).  Will be interesting to see the dual B60 cards that were shown previously that partners can release.",Neutral
Intel,"Shockingly, the modern [$1250](https://marketplace.nvidia.com/en-us/enterprise/laptops-workstations/nvidia-rtx-4000-sff-ada-generation/) workstation card beats the $350 workstation card... and at gaming...",Positive
Intel,"RTX 2000 is pretty close though, but that is still double the price of this card.",Neutral
Intel,I would hope it would at that price point.,Neutral
Intel,Release Date: 9/18/2025  https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007,Neutral
Intel,Fly by Night is one of the best,Positive
Intel,Nice profile pic,Positive
Intel,"I don't give a shit what its intended purpose is, still doesn't hurt to have game benchmarks.",Negative
Intel,\>but muh gaming,Neutral
Intel,"Agreed... But the B570 is 150w, and the RTX 5050 is 130w. What about SFF PCs without a PCI-E power connector? What about low power desktops running on battery (RV, etc.)? What about a cheap, compact eGPU that can use a PicoPSU as the power supply?  Currently, for slot powered, the options are slim:  * Geforce RTX 3050 has solid performance at 70w... but only 6 or 8GB of RAM. * Radeon RX 6400 at 53w is a little beefier than a Steam Deck, but not much. * Radeon RX 7400 just came out, is 53w, and is better still - but still limited to 8GB of RAM and appears to be OEM-only. * Alchemist A310 at 75w and 4GB is a joke. * Alchemist A380 at 75w and 6GB isn't amazing, and trades blows with the RX 6400.  An Arc B50 Pro ticks a lot of boxes. 70w means slot powered and not even pushing to the very limit. 16GB of RAM means you won't have massive framebuffer limits. [Geekbench 6 Benchmarks from Toms Hardware](https://www.tomshardware.com/pc-components/gpus/intel-arc-pro-b50-is-up-to-20-percent-slower-than-the-arc-b570-gaming-gpu-in-early-geekbench-tests-almost-doubles-the-a50-in-synthetic-tests) show 69890 OpenCL and 78661 Vulkan, which is slightly better than the 3050's 63488 and 62415 respectively.  The prebuilt eGPUs with a 7600 XT are decent. Only 8GB RAM, but the 7600XT easily beats the B50 Pro and 3050 at GB6 with scores of 83093 and 99525. But... it's $550-$600 for these. They aren't upgradable. And that 8GB of RAM is rough.  And sure, the Ryzen AI 395 with the Radeon 8060S is really solid... but Mini PCs with that are at least $1000, and that's getting a whole new machine. Thunderbolt 3+/USB4 ports are now quite common. Taking an existing laptop or mini PC from '720p low only' to '1080/1440 with moderate settings' is huge,",Neutral
Intel,It's slot powered no PCIe power cable needed,Positive
Intel,What 2080ti is low profile and slot powered?,Neutral
Intel,"Intel had to trim pretty far to get the power down. below 70W.  Really nifty, but it does hamstring the memory more than I'd hoped.  Definitely looking forward to seeing what it's capable of, especially for the price.",Positive
Intel,Does rtx 2000 even exist. Didnt rtx become a thing around 4000s,Neutral
Intel,Already gone.,Neutral
Intel,Also an excellent album,Positive
Intel,"A380 is definitely more powerful than RX 6400, unless the game has driver issues or poorer optimization on Intel arc. But yeah, nothing amazing really, except for the codecs. RTX 3050 LP is pretty much the only viable choice today.",Neutral
Intel,"Rtx 2000 ada sff was released at the same time as rtx 4000 ada sff. So yes, it does exist.",Neutral
Intel,I agree,Neutral
Intel,"I'm not aware of anything else that's this small, doesn't need external power, performs this well and has 16GB VRAM, and this isn't for gaming but it can game, as shown in the video. Would make for an interesting ultra SFFPC.",Positive
Intel,"I have a 5090 thanks, again its slot powered.",Positive
Intel,Oh yea okay.,Positive
Intel,No you don’t.,Neutral
Intel,"Actually I do..... https://m.youtube.com/@TnTTyler   And other gpus and other systems, so try again",Neutral
Intel,"You love to call people kid, kid.",Negative
Intel,Brave attack there kid. Btw. You do know kid has more than one definition. It’s your actions and attacks that are childish. If you weren’t ignorant you’d know this but instead you’re accusing people of things and crying about AMD everywhere with one of your many accounts. Lame. Try again.,Negative
Intel,Whats this product codename? Still Battlemage?,Neutral
Intel,I miss the days when entry level and mid end GPU doesn't need external power.,Neutral
Intel,forgive my ignorance but this isn’t made for gaming right?,Negative
Intel,A is for Alchemist.  B is for Battlemage.   C is for Celestial.  D is for Druid.,Neutral
Intel,"It's not made for gaming, however Intel drivers allowed it to run game. Kinda like what Nvidia did with their Quadro card.",Neutral
Intel,"Why does it always have to be ""gaming...  No, it's an entry level Pro card, for AI etc. While I'm at it, it is also cheaper and faster, has twice as much VRAM as the nVidia A1000 card.  From what I've seen, it can do 70/90fps at 1440p medium details in CP2077.",Neutral
Intel,">Why does it always have to be ""gaming...  because 'gaming' is a single task that is generally representative of realworld hardware performance in all aspects due to its unparalleled diversity, no single number will tell you as much about a gpu as the framerate will.",Neutral
Intel,"This day and age with crappy game optimizations, buggy engines and early access slop being sold as AAA full price ""masterpiece"". Highly debatable.",Negative
Intel,Not surprising  It has 16Xe Cores (equivalent to 32CU/SM)   14Gbps GDDR6 memory   ~2.3Ghz core clocks   B580 has:  20Xe cores   19Gbps memory  2850mhz core clocks,Neutral
Intel,It’s a workstation so,Neutral
Intel,This is a workstation card.  Its drivers won't be optimized for games -- they will be tuned for things like Autodesk Maya and AI.  This rumor is totally immaterial.,Neutral
Intel,That colorway is actually... Cool.,Positive
Intel,$90-$100 MSRP? 🤔,Neutral
Intel,hope this helps drive down the prices of LP cards on the used market,Neutral
Intel,"I've one pulling from my uncle's workstation for testing before returning it to his case.    Cons: Even though it has 16GB Vram but the card can only match with 2060 Super/Vega 64 at raw perfomance (I've know it's using for training AI or workstation task) and laking driver, even MSI Afterburner was not reconized with this card...    Pro: The Benmark score on Superposition 8K optimized is 'bout 2100 to 2250 depend on how cooler the card is. God of War 2018 2K High Setting has 30-55FPS and play very smoth without lagging, Cyberpunk has bout 30-40FPS at 2K High Setting so it's playable.    So will you get one for yourself?   _Yes: It's card with 70W TDP (But runing at full benmark consume about 55W) so if you are using under 5 Litters sff case like NV10, A09M with small PSU like Flexguru 250W or Pico PSU 200W above, 250W Gan HD Plex,... or you wanna train small AI or developing or any workstation task that consume much Vram... Buy this card.   _No: The perfomace is not match with it price so better buy 4060 LP, 5050 LP, 5060 LP or much money buy A2000 Ada, A4000 Ada SFF...",Neutral
Intel,Arc Pro? Who's the target audience? Doesn't look like a gaming oriented card to me.,Negative
Intel,"They're comparing it against the B570, not B580.",Neutral
Intel,And just 70W  https://www.intel.com/content/www/us/en/products/sku/242615/intel-arc-pro-b50-graphics/specifications.html,Neutral
Intel,Why do people who visit this sub assume everyone is a gamer?,Negative
Intel,"This used to be true decades ago (back then even the hardware was significantly different in some cases), but nowadays pro cards are quite similar. The ""special drivers"" usually just exist for certification purposes and are made from the same code as the gaming drivers. It's mostly product segmentation these days. So the pro cards tend to have more memory, sometimes have ECC memory, different form factors and different power/performance optimization. That's it.",Neutral
Intel,Doesnt really matter. This is just another cut down version of Battlemage. It will scale however they want to chop it up to. Like the Radeon Vega Frontier workstation  card came out and had mediocre performance so everyone outcried saying just wait for the gaming “optimized” Radeon Vega 64. It was the same card and performed identical.,Negative
Intel,Which part of geekbench said it is a gaming benchmark to you?,Neutral
Intel,MSRP for the Arc Pro B50 is $299.,Neutral
Intel,Professionals / AI freaks,Negative
Intel,Low-end workstations and lab equipment. I had a similar low-end professional GPU installed on an [X-ray diffractometer.](https://www.malvernpanalytical.com/en/products/product-range/empyrean-range/empyrean),Neutral
Intel,The term “pro” is short for “professional”,Neutral
Intel,"Also depends on the price, if its under $300 then it's a pretty good deal as far as workstation cards go.",Positive
Intel,Because the vast majority of redditors aren't very smart people to put it mildly.,Negative
Intel,"That's its typical usage.  But you're right, Geekbench is a shoddy cross platform bench, despite its typical use for gaming by bad reviewers.  That said, it is very unlikely that this card's drivers are tuned to Geekbench, unlike the mature drivers in the B570.",Negative
Intel,I bought my B580 2 weeks ago :(,Negative
Intel,"Good enough a reason to get my buddy a b570 for his birthday, might just pocket the game!",Positive
Intel,You gotta be joking. Literally got a 14600K for $150 a few days ago. :(,Negative
Intel,Hopefully this means BF6 will get an APO profile.  u/Aaron_McG_Official can you confirm?,Positive
Intel,I got Borderlands 4 with my 5090. Would rather trade it for this.,Neutral
Intel,Maybe its a good time for them to drop big battlemage on the market??,Neutral
Intel,I'm surprised hardware companies are still doing this. People can easily abuse this by getting the game for free and returning the product they bought. Well it works in places that have strong consumer protection laws.,Negative
Intel,I got assassin's creed shadows with my $140 Intel CPU.  Good deal indeed.  I like the AC games.,Positive
Intel,"I bought my intel ultra 7 less than two weeks ago, damn",Neutral
Intel,Deep thought stimulation.,Neutral
Intel,"I wonder what retailers, will amazon be one of them?",Neutral
Intel,"That's a pretty sweet deal, especially for pc builders on a budget.",Positive
Intel,Are latest Achemist/Battlemage GPUs faster than my 3080 at 4k?,Neutral
Intel,Is this live now?,Neutral
Intel,I bought 13700K on Jan 2023 can I get a serial as a compensate ?,Neutral
Intel,"Quick question: I got a laptop from Amazon with an Ultra 9 CPU, and Amazon sent me a code. However, no useful instructions on how to redeem it. Usually they have you download nvidia app or something like that, but this says to add the item to your cart and try to use the promo code? Tried that and nothing. Anybody else having trouble with this promotion? Is there an Intel app I need to download to redeem this code?",Negative
Intel,"Bought i7 14700 from amazon uk, yesterday, did not reciece master key from them to redeem offer",Neutral
Intel,Dang I just found out about this today but got my i9 cpu on the 24th of august. Luckily I just got a laptop so I get the code from there but it would've been nice to give one away.,Positive
Intel,"Bought a 14600K, only to be hit by a ""This offer is not available in your country"". There is NO region restrictions outlined in the promotion Terms and Conditions. Very sketchy.  Something that can be looked into, or do I need to return the processor?",Negative
Intel,I bought it at the height of the voltage controversy when the fix wasn't even known. Nothing for me then? 😐,Negative
Intel,Even if it's free it's not worth it.,Negative
Intel,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
Intel,Price scheme or they are that desperate.,Negative
Intel,I’ll do this deal when my stock sells ha,Neutral
Intel,Mine just got delivered on Friday too….,Neutral
Intel,Return it and than rebuy it,Neutral
Intel,"That's still a great deal, even without BF6!",Positive
Intel,Same here lol. I got one to upgrade my sisters build.,Neutral
Intel,Can’t return it?,Neutral
Intel,Check you probably got the previous deal,Neutral
Intel,"Per Intel policy, I can't comment on unreleased products. Historically, APO has tested the top superlative and popular titles; if the team gets them to show improvement then they are added.",Neutral
Intel,"That's fair. I can trade you a B580 with Battlefield 6 for a 5090, you can even keep the copy of Borderlands 4.",Positive
Intel,"That would be amazing, but I doubt it.",Negative
Intel,It will likely come in Q4 2025 or Q1 2026,Neutral
Intel,I have a buddy at bestbuy and asked if this would work and he told me that when you go and return the product the price of the game will be withheld from your refund if the code has been redeemed. He did also say that would happen when nvidia included a code for star wars outlaws and I had no problems getting a full refund with the game redeemed,Neutral
Intel,Same man 😅,Neutral
Intel,Nope. 3080 is faster by a long shot.,Neutral
Intel,Yes,Positive
Intel,Anyone solved how to get/redeem the BF6 code?,Neutral
Intel,Is it fixed now?,Neutral
Intel,A megacorp that has almost 70% of CPU market share is so desperate that they're giving away a game with a purchase of one their CPU's. You're a genius.   I got Dying Light 2 with my old 12th gen.,Negative
Intel,"I remember getting FarCry 6 with my Ryzen purchase a few years ago. Right now looking at a local pc shop website, you can get Borderlands 4 with a 50 series gpu purchase.  I guess by your logic, both AMD are desperate for CPU sales and Nvidia is desperate for GPU sales ?",Neutral
Intel,If only there was something called a return period....,Neutral
Intel,How generous. But no thanks. 😅,Negative
Intel,"I'll up it to a 1070, or my current 3070.",Neutral
Intel,So unlucky lol,Negative
Intel,Yup,Neutral
Intel,"These idiots also act like AMD never bundles games, I got Star Wars Jedi Survivor with my Ryzen 7 7700X.",Negative
Intel,"I got ""Dying Light: The Beast"" and ""Civilization VII"" as a gift from Intel with my Core Ultra 7 CPU.",Positive
Intel,Let's not pretend Intel isn't in desperation mode right now. The company is in the worst shape they've been in 30 years.,Negative
Intel,"Intel: here kid, have a balloon   Kid: what's in it for *me?*",Neutral
Intel,"> A megacorp that has almost 70% of CPU market share  Since you mentioned your 12th gen, on steam it's under 60% intel now, an 11% reduction in intel cpus in just 1 year. On amazon, there's [0 intel cpus in the top 10 best sellers.](https://i.imgur.com/LGwqOZs.png) Anecdotally the only high end cpus I see recommended in gaming discords is 7800x3d/9800x3d.  Obviously intel still has the lion's share in oem/laptop/server but that's been falling rapidly too, despite intel illegally bribing oems to not use amd products. Apple ending their partnership+their own laptop chips have been chipping away at intel's share too.  One company's been rolling losses year after year with falling market share, one is the complete opposite, the writing was on the wall a long time ago.  It took amd the better part of a decade to claw back from bankruptcy, intel needs to stop their attitude of looking for short term gains if they want to even start a comeback.",Neutral
Intel,"TBF, you shouldn't be looking at marketshare but percentage of new CPUs sold/ profit.  Marketshare just tells us intel was number 1 for a long time. Change in marketshare tells us they're losing ground fast.",Negative
Intel,Yep that’s exactly what I did since they wouldn’t honor the promotion even though it’s during the return period,Negative
Intel,Well you said you would trade so…,Neutral
Intel,AMD literally set up a store at one point to give away games and GPUs with the purchase of a bulldozer.,Neutral
Intel,"What are you talking about. AMD has been on a brink of going extinct since the 90's with the exception of AMD64 (Clawhammer etc) days. 2005-2008 or thereabouts.  Second, every heard of anti-monopoly laws? It would be cheaper for AMD to pay from their own pocket to keep Intel afloat rather than let them go bankrupt. Why do you think AMD is still around after all these years. Intel will never fail.",Negative
Intel,"AMD's zen architecture was the company's final hail mary which is their last decade, ryzen being a literal play on words for risen (from the dead). Their own engineers admitted if it failed the company was going to go under.  >Second, every heard of anti-monopoly laws?  Same laws that intel has been consistently losing for decades?  >cheaper for AMD to pay from their own pocket to keep Intel afloat rather than let them go bankrupt.  That's not how anti trust/monopoly laws works. Company A going bankrupt because they kept shooting themselves in the foot doesn't mean that Company B automatically become subject to anti trust lawsuits. Nvidia isn't exactly paying AMD even when they hold 90-95% of the market. Legislative scrutiny tends to come from manipulative practices, not from just selling a better product.  Since you might bring it up, Microsoft propped up Apple because they wore forcing their own software through Windows, not because they were just simply selling Windows. In a twisted turn of fate, now Apple is going through anti trust lawsuits/legislation around the world despite holding a minority share of operating systems worldwide.",Neutral
Intel,That is surprisingly affordable for how much vram it has. I expected close to 3.5K.,Positive
Intel,"Yep, I totally need this for plex",Positive
Intel,Perfect for AI inference market Lip Bu Tan is targeting,Positive
Intel,Would this be a competitor to the Framework Desktop?,Neutral
Intel,You can now buy CORE 1 series cpus on desktop motherboards from ali.,Neutral
Intel,How did I not see this? Has anyone tested the B60? There seems to be extremely limited supply.  Maxsun does not have good support in the US?,Negative
Intel,48GB Vram,Neutral
Intel,It's just two B60 Pros. They're supposedly priced at $500 each if they'll ever actually be available for purchase.,Neutral
Intel,"Biggest thing in this update for me is finally they allowed iGPU memory allocation for Intel Core Ultra Series 1 & 2. This is huge!      I haven't tested this drivers on my MSI Claw yet, really excited to use this features because it can solve a lot of texture issue and low memory warning in the game where it only detect Arc integrated graphics with 128MB vram.",Positive
Intel,"I get crashing in ALL games when attempting to use the new Speed Sync option on this driver.  Specs are:  Ryzen 9700x, Arc B580, 32GB DDR5 RAM, 2tb SSD.  (Will make an actual post for more visibility)",Negative
Intel,Are the iGPU memory allocation settings in the BIOS or the Intel ARC control panel?,Neutral
Intel,"intel dynamically scale IGP usage and has a hard cap that is 57% of system memory. As per reporting by CapFrameX, Vulkan) maybe DX12 too, not sure) will allocate dynamically as high as the aforementioned hard cap but DX11 (legacy API) will stuck at 128MB VRAM reporting   I shall test this update on my MTL-H laptop",Neutral
Intel,Where is this settings at? Doesn't appear on my U125H,Neutral
Intel,Just used this by accident and i think i accidentallx bottlrnecked CPU on the 258v lmao,Negative
Intel,"It's on Intel Graphics Software. On BIOS usually it will be greyed out and written as 128MB. Last time i remember Intel allowed iGPU memory allocation on BIOS is at skylake era, nowadays it's locked and it's done automatically until the recent update.",Neutral
Intel,"What do you mean bottlenecked CPU? iGPU memory allocation isn't affecting how CPU works because it only changed the amount of ram reserved to iGPU, not the speed.",Negative
Intel,Because the CPU had too little memory to work with!,Negative
Intel,"CPU has its own memory called as cache (L0, L1, L2, L3 and L4), it's nothing to do with RAM. The only thing RAM affecting overall performance is latency and speed.  The amount of RAM doesn't matter much, as long it has enough RAM for OS and application then it wouldn't make the system runs slow, let alone to make the CPU runs slower which is not possible.",Neutral
Intel,I am also questioning why is Intel still focusing so much on battlemage when celestial is supposed to be just around the corner.   It feels more and more like Lip bu tan axed Celestial.,Negative
Intel,The weird thing is when he did a keynote a slide noted dGPUs was part of Intel.,Neutral
Intel,"Huh?  They launched one die, and haven't said anything about launching more.  How is that being focused on Battle mage?",Negative
Intel,Celestial IGP is pretty much done with `force-probe` option has on its way to be removed (driver promoted from experimental to mainline)   Celestial dGPU is pretty much unknown. If Tan wants to bet dGPU because Gaudi accelerators didn't sell (or badly marketed? too expensive? dunno) then he better not kill it. Arc Pro B50 and B60 received good AIB reception that they can go crazy and make whatever from it,Neutral
Intel,There is big battlemage coming in Christmas apparently.   Which worrying since celestial supposed to be 2026.   I guess we are lucky if we get celestial early 2027,Neutral
Intel,"This is some good news after the layoffs  Hopefully, the Arc DGPU Division didn't get cut too deeply and that there's still gaming Xe3 Arc cards and Arc Pro cards in the pipeline  Insane that Lip Bu Tan would gut so much of the workforce   The board must've been itching for layoffs after firing pat or Lip Bu Tan might think this is how you save Intel.  Either scenario is bad.  Deep across the board cuts like this is how you destroy a company as talented people and lazy underperformed are caught in the dragnet.  Irreplaceable talent will go to companies with better stock options, 401ks and compensation like Nvidia, AMD Qualcomm, ARM, Apple.  The board and Lip Bu Tan are fools for thinking this is a good idea.",Neutral
Intel,I wish that they just added way more memory instead of messing around with dual GPU,Neutral
Intel,Seems like a decent option for a low power PC,Positive
Intel,Gorgeous.. love these builds.   How hot did she get?,Positive
Intel,I would buy HX cpus adapted to lga1851 with an interposer like the chinese 12900hx and 13950hx es chips.,Neutral
Intel,I. . .WANT. . .THIS!,Positive
Intel,The 192-bit bus and density of gddr6 limits them to 24GB per GPU chip max. They have to go multi-gpu to have more than that.,Neutral
Intel,Yes. You are right about the memory bus .  I completely forgot about that,Neutral
Intel,When compared to this  AMD intel and Nvidia with thier supposed Ai GPU's are just creating E-Waste . https://support.huawei.com/enterprise/en/doc/EDOC1100285916/181ae99a/specifications,Negative
Intel,"Just like motherboards have 4 RAM slots for 128 bit bus and only two channels, GDDR6 can also use clamshell mode to double the VRAM without going multi GPU.",Neutral
Intel,"Kind of a shame honestly. If bigger Battlemage does materialize, that 256-bit bus gets you a 32GB max capacity instead. 64GB per dual-GPU card.",Negative
Intel,"Yes, and that tops out at 24GB for a 192-bit bus.",Neutral
Intel,Yeah..Shave really because there's a company which has developed an ai GPU which will have an ability to add more memory just like we do with system ram..  It's still in works and very long time to go until an actual release .   But they have demoed their FPGA card which very soon will be turned in to an ASIC,Neutral
Intel,AI inferencing would save intels ass if they'd pump out high bandwidth high VRAM cards. They don't even need massive chips to do it. Maximum profit.,Positive
Intel,We need Intel to be a viable competitor to AMD and nvidia. That means better prices all across the board and more options for us consumers.   I’m rooting for Intel in the gpu sector. If only they would get their shit together for CPU.,Positive
Intel,Amd is competitor with intel? XD,Neutral
Intel,… yes.,Positive
Intel,"What the hell are you even talking about? CPUs? GPUs? Both? The lack of specificity is really irking me, and it shows you don't care enough about the topic to even be right about it.",Negative
Intel,"I work with AI, comfyui and other heavy things, the AMD gpu are 💩💩 there, good luck if you amd fanatic",Negative
Intel,The unintelligible ramblings of a madman.,Negative
Intel,"Sure, a madman not amd fanatic 🤡",Negative
Intel,"Intel needs to fix their Arc supply issue. They really need to start pumping out Arc cards on US soil.  However this is good news to publicly announce Arc BattleMage will expand into AI markets.  So now Intel Arc, a discrete graphics card has a consumer line, a pro line and venturing into edge/AI.  Nvidia across the board has become too expensive and AMD continues to limp along in typical mediocre fashion. Intel needs to execute persistently.   I can’t give financial advice, but Intel is at a good price. if Intel can get the foundries up and running sooner, its stock will be a buy for most.  Intel is getting ignored by the White House, dominated by TSMC with the Taiwanese government, backing them all the way, and seen as a corrupt entity by AMD famboyz.   There are people to this day, who keeps complaining that Intel didn’t innovate back in the day and charged so much for their CPUs. They live in this alternate reality where AMD can do no wrong,  as Amd 9800 X 3-D chips melt along with other AMD CPUs and Radeon cards are only mediocre at best.  We are seeing many 9000 series Ryzen chips start to overheat, especially the high end chips like the 9950 X 3-D and the 9800 X 3-D. There could be more issues on the horizon. Keep your eyes open.",Neutral
Intel,LBT will fix this.,Neutral
Intel,"Interesting strategy compared with Nvidia which absolutely refuses to let consumer GPUs be used for enterprise (spoilers, everyone still do tho) That's why even brands that make local AI training desktops, which are in effect entry level workstations, like Gigabyte and its AI TOP www.gigabyte.com/Consumer/AI-TOP/?lan=en need to pretend these are just gaming rigs that happen to be really good at AI to keep Nvidia from getting on their case.",Negative
Intel,"Good, good, keep them coming, flood the market with them cards.",Positive
Intel,"I was actually hoping for this, but looks like the software support for intel died for a bit. They cut development for their Npu libraries and theres limited support for their gpus if you dont want to use openvino",Negative
Intel,"you are finished intel, i'm going to start a class action lawsuit regarding having to FINISH OUR PAID FOR INTEL PRODUCTS OURSELVES, THERE IS NO INSTALLATION SUPPORT THAT ACTUALLY WORKS, USERS HAVE TO MANUALLY IMPLEMENT IT? that's A CRAZY LAWSUIT I AM SURPRISED NOBODY NOTICED. FALSE ADVERTISING, REASONABLE ENJOYMENT, AND OF COURSE, THEY STOLE OUR MONEY!",Negative
Intel,"go ahead and ban me, i'll add that fact to the lawsuit, instead of help customers you decided to ban them.",Negative
Intel,"the entire intel corporation is owned by some asian guy.   he's obviously SABOTAGING IT, as they take all the tech back to china.   LIKE THEY DID WITH NORTEL in the 90's. WAKE UP INTEL, you are being DESTROYED INSIDE OUT.",Negative
Intel,"china loves the new ai from intel, nice how it works for china, BUT NOT FOR NORTH AMERICANS.   LAWSUIT TIME.",Neutral
Intel,"u/intel WILL BE INTERESTING WHAT US AND CANADIAN SUPREME COURTS BOTH THINK ABOUT CHINA RUNNING YOUR AI, BUT ALL OF YOUR OTHER INTERNATIONAL CUSTOMERS CAN'T SEEM TO GET IT RUNNING AT ALL.",Negative
Intel,"Intel #TRASH.   hundreds of gigabytes later,   still no working #AI",Negative
Intel,"u/reddit i'll sue you guys too. literally you are letting u/intel lie through their teeth about unsupported products they knowingly sell without support and then expect the end user to build said support-THEMSELVES. THAT'S A LAWSUIT GUYS, i'm sure you don't want to be known as a COLLABORATOR for intel's illegal activity, ya? STEP OFF.",Negative
Intel,"if i can't use what you advertised, that's false advertisement, you made it almost impossible for normal average people to use your hardware and software, and expect not to be sued? GET REAL :D u/intel",Negative
Intel,i can't even stand anything intel now. 3 years of this disappontment. i'm finished with u/intel besides to sue them.,Negative
Intel,"they are firing employees around the world, intel is finished. DROP your intel stuff. it's about to lose support forever. LOL.",Negative
Intel,"proof intel is dying. forget intel, forget their ai. turn your backs on them, like they did when it was time to support their gpu u/intel-ModTeam u/intel you deserve what comes your way, INTEL.   [https://www.youtube.com/watch?v=pWT5eyto\_P0](https://www.youtube.com/watch?v=pWT5eyto_P0)",Negative
Intel,"they can't even make their ai work properly. which is why people went intel in the first place, all of those false promises. 3 years later. my intel gpu is pretty much useless, all those advertised features, still waiting on those. there is no solid way to install their ai suites, it's a joke. since when did we have to script and code our own support for SOMETHING SOLD TO US>NEVER. sue intel, move on. u/intel your own fault guys. YOUR FAULT. [https://www.youtube.com/watch?v=pWT5eyto\_P0](https://www.youtube.com/watch?v=pWT5eyto_P0)",Negative
Intel,"the 12700k is nice. but my A770 16gb was a bad buy. having to run scripts and code myself just to utilize features advertised, that's a big mistake on their part. they should have made a no-brain solution for installing their garbage. it certainly will always be garbage in my mind, none of it has worked after hundreds of gb of git/python downloads.",Negative
Intel,"intel deserves what comes their way. their AI support for their GPUS is literally NON EXISTENT. a whole lot of links to ""do it yourself"" who sells a product like that: HERE'S THE HARDWARE, now go BUILD the SOFTWARE support, YOURSELF. how STUPID can a COMPANY GET. u/intel",Negative
Intel,"Intel #intelai is trash: HUNDREDS OF GIGABYTES OF GIT/PYTHON and INTEL SOFTWARE,   and none of it worked, NONE OF IT. u/intel",Negative
Intel,"Intel #intelai is trash: HUNDREDS OF GIGABYTES OF GIT/PYTHON and INTEL SOFTWARE, and none of it worked, NONE OF IT.    FALSELY ADVERTISED HARDWARE CAPABILITIES. that's a lawsuit. #IntelGraphics #Intelai <- ARE WORTHLESS PURCHASES. u/intel-ModTeam u/intel",Negative
Intel,i want my money back u/intel    and you can pay for my wasted 3 years of time.   (that's a huge lump sum of money) works for me.   u/intel <= SUE THESE CROOKS WHILE YOU CAN.,Negative
Intel,"HARD TO FOLLOW REDDITS POLICY, WHEN INTEL WON'T EVEN RELEASE PROPER INSTALLATION SOFTWARE, AND IS NOW RUNNING AN ILLEGAL RACKET THAT SCAMS CUSTOMERS. u/intel",Negative
Intel,"PEOPLE: are dropping billions into online ai generators(myself included)   that could have been ALL u/intel-ModTeam \#intel u/intel revenue. but nah,   intel literally is self sabotaging itself. how peculiar indeed.",Negative
Intel,DO NOT BUY u/INTEL there is no SOLID WAY to DEPLOY the AI SOFTWARE. aka none of it works.,Negative
Intel,"just glad i found out the real truth, intel still has no solid ai solution for their gpus. the real truth saved me from wasting money on max sun 48gb gpus. honestly what a relief really, to go back to nvidia. this intel a770 GPU sucked the whole f-ing time. it honestly can't do any of the stuff they claim. it's one of intel's biggest lies going. no wonder they are going out of business. YA YOU GUYS: u/intel-ModTeam  u/intel",Negative
Intel,"the longer your stuff doesn't work, the longer i encourage everyone to sue you u/intel-ModTeam  u/intel",Negative
Intel,nobody likes their job at intel u/intel [u/intel-ModTeam](https://www.reddit.com/user/intel-ModTeam/) or else you'd see them jumping miles high to make this stuff work flawlessly. they're obviously tired of their jobs or are too wealthy to be concerned with their futures. the ceo included. #INTEL is synonymous to other words like TRASH and NON-FUNCTIONAL. their stuff just DOES NOT WORK as they CLAIM it does. period.,Negative
Intel,"good try intel, but i'm not buying any max sun 48gb anymore u/intel u/intel-ModTeam i was going to get two of them. BUT YOU CAN'T EVEN USE THE AI FUNCTIONS on INTEL GPU, every ui wants CUDA. really sad intel has NO CHANCE in the ai market, their stuff DOES NOT EVEN FUNCTION. whereas nvidia's is a cinch to setup, and works flawlessly.",Negative
Intel,"> There are people to this day, who keeps complaining that Intel didn’t innovate back in the day and charged so much for their CPUs.   When AMD launched Zen 3 and later Zen 3 3D, they were fast but also very expensive compared to what Intel offered. Intel, AMD, Nvidia, same shit, different maker. The moment they will make a product better than the competition it will be more expensive. It's called a monopoly.    Back then when Intel charged a lot for their chips they did it because they had no competition. Don't understand why Intel was in the wrong and somehow AMD doing the same was justifiable. Every company does these things.",Negative
Intel,">Intel needs to fix their Arc supply issue. They really need to start pumping out Arc cards on US soil.  Margins have to be way too low for Intel to do this, especially since Intel also said they would be cutting low margin product development.   They might still end up biting the bullet, but I don't think they should at all. There are tons of better areas where that money can be spent.   >I can’t give financial advice, but Intel is at a good price. if Intel can get the foundries up and running sooner, its stock will be a buy for most.  Honestly, I still think the risk is too high. No external 18A customers is a terrible, terrible sign. Intel can bear the economics of 18A internally, but they outright said that doesn't apply anymore for 14A.   >They live in this alternate reality where AMD can do no wrong, as Amd 9800 X 3-D chips melt   Tbf a pretty small scale issue, which hasn't stopped the popularity of those chips at all... unlike the RPL fiasco.",Negative
Intel,"Different strategies. X3D has its place in time. X3D chips have to have another separate line to build and now test. No mobile X3D chips so far.   Silicon cost is greatly lowered if the design is the same and desktop/mobile chips can be binned to create separate product lines.    AMD I will argue absolutely needed to produce stellar products because of their past history before Zen 1. No one will defend AMD products before Zen 1.   I will definitely use a newer better AMD chip today but back then he'll no. There GPU line has driver issues and its why all of us just use NVIDIA gpus. There is a good reason.    Intel's new chip design strategy and leading edge fab is solid. The problem is foundry capital expenditure and foundry capacity. They have to build excess and train excess personal. The Ohio Silicon Heart Land. A whole new campus, town, new families, and Silicon ecosystem.    That takes time and a ton of money. And investment (aka the money levers) just dont want to pay for this. They rather get there money's worth of AI and self driving vehicles today. They know Chinese EV investment is bad and Chinese AI is bad. So rather than invest in Chinese companies and not make money, they want to use there leverage to pump up American companies.    This way American companies like Tesla and AI will succeed and win this AI race.   In other words, China ain't in the leading edge foundry game. None at all. No chance. So why pump money into that? They instead are attacking the EV and AI race. So our American companies will need capital to defend that space.   Intel design products are absolutely solid. 1000% they maintain better MT performance without SMT. SMT has security vulnerability.    Intel products are wafer Silicon efficient with p and E cores. Intel design is solid. Just foundry is extremely expensive and there isnt a China competitor. Taiwan is a very safe island.   If Taiwan was like a Hawaii, easily conquered, then for sure TSMC is at risk. But Taiwan today is like an island carrier/fortress. Too difficult to go against. At least not yet.   Its only value is in its tropical fruits and strategic island location.",Neutral
Intel,"It's such a small market, and it honestly doesn't really matter EXCEPT as practice for their data center GPUs. Intel has bigger fish to fry (like the fact that they're ceding significant server and laptop market share, and are in the process of building several 10 billion dollar+ facilities on the foundry side).",Neutral
Intel,"I hadn’t seen that there was a problem with 9000 series chips apart from with some Asrock boards, has there been another issue identified?",Neutral
Intel,"intel is dying, and i doubt things are going to get better.   [https://www.youtube.com/watch?v=pWT5eyto\_P0](https://www.youtube.com/watch?v=pWT5eyto_P0)   really too bad they made their ai stuff almost impossible to install and use.   they literally KILLED their company, themselves, with CRAP SUPPORT.",Negative
Intel,What is LBT?,Neutral
Intel,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Negative
Intel,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Negative
Intel,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Negative
Intel,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Negative
Intel,">Don't understand why Intel was in the wrong and somehow AMD doing the same was justifiable.   It's a cultists thing. Amd for some stupid reason has a lot of braindead people in their community who gonna support Amd even though they blatantly doing shaddy business practice, even though those people got robbed so bad, same as Nintendo and Linux.",Negative
Intel,"i can't get any video generator to work in comfyui, meanwhile xpu generation has been advertised, but i see no one using it (seems like no one can, hahaha)  i hear all kinds of positives about NVIDIA tensor   even their GTX can run it.   i pity intel and their slow fall down the drain, but it's their own fault.   nobody wants stuff that doesn't work as advertised.",Negative
Intel,"was straight up working towards 2x max sun 48gb, now i'm just gonna get a RTX 6000   i'm not playing games with u/intel (and that's all intel seems to want to do, lie to customers)",Negative
Intel,"intel AI doesn't EVEN EXIST.  PROVE ME WRONG.    PROVE COMFYUI WRONG.  u/intel your stuff doesn't work, at all.",Negative
Intel,gave u/intel 3 years to do a consumer grade ai theme like nvidia's.   still looking around wondering where it is (nothing works on huggingface or comfyui)   intel gpus are a lie.,Negative
Intel,"the moral of the story...(as intel goes out of business)   don't lie to customers, and make stuff you can't properly support, out of box.   nobody wants to get dirty deep in script and code or batches, for something that still won't work anyway. u/intel 100% team ""ULTIMATE-fail"", 2022-2025. u/intel-ModTeam",Negative
Intel,"if intel could have delivered something like what nvidia has going now,   they'd be topping the charts, in everyones ""to buy"" lists.   the proof in their failure, well just look around.   you can see many more people are nvidia fans, than amd or intel fans.   the reason: NVIDIA'S STUFF ACTUALLY WORKS.    u/intel u/intel-ModTeam it's no wonder it's over for all of you. enjoy the streets :D",Positive
Intel,"and until i can generate a video via xpu over in my comfyui, i'll continue to tell the truth about intel products, and how dysfunctional and unacceptable they are, by today's industry standard, set by NVIDIA CORPORATION.    u/intel u/intel-ModTeam  there is no intel, at least not in my mind, not anymore.",Negative
Intel,"i'm getting a RTX 6000 96gb and that new CPU nvidia is making, and their motherboard. hopefully ddr6 is out by then. RIP, INTEL and AMD. (in fact i hear both the gpu and motherboard will both use HBM2 ram or something similar) more stuff intel and amd can't bring to the table. like i said, there are no such things as INTEL or AMD in my mind, anymore. times sure have changed.",Negative
Intel,Lithography But Tinier,Neutral
Intel,"New Ceo Lip Bu Tan.  Though I think Celestial might come to US soil, there is no way battlemage ever will.",Neutral
Intel,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Negative
AMD,The big thing in this update is that you can force FSR 4 via driver software in any game that suppports FSR 3.1 or newer.,Positive
AMD,"kinda surprising to see ue5 titles still dropping and not supporting it  edit: oh anything after 3.1 is money, nevermind",Negative
AMD,"They have really stepped up their game, they are on the same trajectory as DLSS 4 adoption.",Positive
AMD,"Since The Last of US - Part II uses FSR 3.1.3 , Possible to use FSR 4?",Neutral
AMD,is it FSR4 or just repurposes FSR3.1 DLLS?,Neutral
AMD,"6 or so months ago they launched with 30 odd games. They are close to 90 now.   Not bad at all but they have to keep at it. Nvidia has dlss in close to 800 games iirc.  Edit:  DLSS 4 can be swapped into any of the DLSS 2 and above games. I do not have the numbers for the NVidia APP override, but you can drop in the dll.  This was back in April:   [https://www.tomshardware.com/pc-components/gpus/nvidias-dlss-tech-now-in-over-760-games-and-apps-native-and-override-dlss-4-support-has-broad-reach](https://www.tomshardware.com/pc-components/gpus/nvidias-dlss-tech-now-in-over-760-games-and-apps-native-and-override-dlss-4-support-has-broad-reach)  This article mentions that including overrides (vulkan FSR 3.1 games are apparently not compatible yet) at over 90 (from the link)  >  >",Positive
AMD,Anyone who bought RDNA3 should not expect AMD to support future features on RDNA5 to be supported on RDNA4,Negative
AMD,"It is not forcing a change via driver, its changing the dll like how dlss 2+ works. When AMD was whitelisting games that was forcing it via driver.",Neutral
AMD,"> that suppports FSR 3.1 or newer.  Welp, Alan Wake 2 is still on FSR 2.2.  One of the few titles that could *really* make good use of FSR4.   Come on, Remedy :/",Neutral
AMD,"DX12 only - So it’s most of them, but some of the key omissions would be Vulkan based games without official FSR4 support like Indiana Jones, Doom TDA, and (perhaps most importantly of all things) No Man’s Sky",Neutral
AMD,And expect this trend to carry on with FSR 5 and etc.,Neutral
AMD,"If you're in second place and it's not even remotely close, you have to go spend money to get developers with limited time and budget to add features only a very small group will be able to use. Remember unlike FSR2 FSR4 only benefits RDNA4 owners right now and the Steam survey recently revealed how little actual market AMD has netted.  Next to none.  AMD needs to start spending on marketing.",Negative
AMD,"Now they need to get on the DLSS trajectory in terms of image and motion quality. FSR4 is a big step in the right direction, but they're still about a generation behind Nvidia. The difference is very noticeable even in screenshots, let alone actual gameplay.",Neutral
AMD,It's listed with [FSR4 support](https://www.amd.com/en/products/graphics/technologies/fidelityfx/supported-games.html).  Pretty much all the Sony games are a safe bet.,Neutral
AMD,"Well, the whole point of FSR3.1 was that it was forward-compatible. So yes, AMD just followed through with their plan and enabled a driver-level override to turn all 3.1 (DX12) games into FSR4 games",Positive
AMD,Vulkan and also games with anti cheats are problem for optiscaler.,Negative
AMD,DLSS 4 is at around 125 games not 800,Neutral
AMD,This article doesn't mention that they also now have it where any DX12 game with FSR 3.1 can be automatically switched to FSR4. So it's in much more than 85 games or so.,Neutral
AMD,Thats a dumb take.,Negative
AMD,"Yep, waiting on this one as well!",Positive
AMD,"it’s a bit tricky but apparently Alan Wake 2 is Optiscaler-compatible through DLSS to FSR4 replacement complete with frame generation. Actually better than Vulkan based games without direct FSR4 support, if you ask me.",Positive
AMD,We haven't even gotten FDR 2 yet and the next election is in 2028,Negative
AMD,"nah fsr4's image quality trades blows with dlss4, theyre each better/worse at some things but overall roughly equal. (worth noting I focus on 4K results since that's what I use)",Neutral
AMD,so you just described how they're on the trajectory of image quality of dlss...,Neutral
AMD,"> FSR4 is a big step in the right direction, but they're still about a generation behind Nvidia. The difference is very noticeable even in screenshots, let alone actual gameplay.  No.  FSR 4 is basically caught up.  The differences are small enough that determining which one is better will depend on personal preference / sensitivity.  Even if you prefer DLSS, to call it a ""generation behind"" is an exaggeration.  You may as well tell me that G-Sync is just so much better than FreeSync that it's worth spending an extra $150 on the same monitor.",Neutral
AMD,"it can be swapped into any of the DLSS 2 and above games.      This was back in April:    [https://www.tomshardware.com/pc-components/gpus/nvidias-dlss-tech-now-in-over-760-games-and-apps-native-and-override-dlss-4-support-has-broad-reach](https://www.tomshardware.com/pc-components/gpus/nvidias-dlss-tech-now-in-over-760-games-and-apps-native-and-override-dlss-4-support-has-broad-reach)  This article mentions that including overrides (vulkan FSR 3.1 games are apparently not compatible yet) at over 90 (from the link)  >FSR 4 is not compatible with FSR 3.1 titles that run on the Vulkan® API or use non-standard methods of integration of FSR 3.1 into their games, such as using third-party plug-ins or methods that do not use the signed FSR 3.1 DLL.  >For a full list of AMD FSR 4 titles confirmed to be compatible with our driver upgrade feature, check out this [link](https://www.amd.com/en/products/graphics/technologies/fidelityfx/supported-games.html).",Neutral
AMD,This is not so. Only games that use the signed 3.1 DLL can be switched to FSR4. Games that use custom implementation cannot.,Neutral
AMD,"No, it's not. There are in total 85 games that either support it or can be made to support it by using the override in the driver.",Neutral
AMD,Its about 90 ish  From [https://www.amd.com/en/products/graphics/technologies/fidelityfx/supported-games.html](https://www.amd.com/en/products/graphics/technologies/fidelityfx/supported-games.html)   [This includes driver overrides.](https://gpuopen.com/news/amd-fsr4-over-85-games/#:~:text=For%20a%20full%20list%20of%20AMD%20FSR%204%20titles%20confirmed%20to%20be%20compatible%20with%20our%20driver%20upgrade%20feature%2C%20check%20out%20this%20link),Neutral
AMD,"Yes, it does.  > #Expanding horizons: over 85 supported games With the latest AMD Software: Adrenalin Edition™ 25.9.1 driver update, AMD FSR 4 extends its reach to cover more than 85 popular gaming titles  Adrenalin 25.9.1 is the exact driver that allows FSR 4 in all DX12 FSR 3.1 titles.",Positive
AMD,AMD has yet to prove me otherwise with FSR4 on RDNA3,Neutral
AMD,Digital Foundry disagrees. FSR4 is better than DLSS CN model but does not compete with DLSS4. Noticeably blurrier and significantly more artifacting when compared side by side.,Negative
AMD,"I love when redditors do that thing lol  Person A: My statement  Person B: No, because (person A's statement repeated)",Positive
AMD,They're still a generation behind. They need to catch up faster. At the rate they're going they'll always lag behind DLSS.,Negative
AMD,"No, they're not basically caught up. I use both on my PCs (one PC with a 9070 XT and one with a 5070ti) and DLSS is still very very clearly better. And comparing this to arguing for Gsync over Freesync? That's a wildly disingenuous thing to say, upscaling and adaptive sync are not comparable things.",Neutral
AMD,And fsr 4 can be in game with dlss or fsr as well using same method.  We talking official support.   By your logic also fsr 1 has over 6k Game support as its in all emulators   Or 20k since it can be forced on entire pc,Neutral
AMD,"I'm not saying that, I am saying the number is much more than 85.",Neutral
AMD,FSR4 on RDNA3 works on linux.,Neutral
AMD,They fully intend to support it on rdna3. You're just being impatient.,Neutral
AMD,Maybe its due to the cards lacking a certain hardware feature...,Neutral
AMD,"But as also noted by DF - and other outlets as well - DLSS4 does suffer from worse disocclusion artifacting compared to FSR4. So the above commenters point about trading blows is correct, even if you can definitely argue (and I'd think you'd be right to) that DLSS4 does have more advantages over FSR4 and is still better overall.  That being said, your original point that AMD ""need to get on the DLSS trajectory"" and FSR4 being ""still about a generation behind Nvidia"" don't really line up with reality. The two are reasonably close to one another at this point in terms of upscaling, it's really the other stuff (e.g. ray reconstruction) where AMD needs to put effort in catching up now (and that is what Redstone is supposed to do).",Neutral
AMD,Did we watch the same DF video? Because the ones i've seen they clearly say it's between DLSS 3 and DLSS 4 in terms of quality.,Neutral
AMD,"tell me you don't know what a trajectory means without telling me...  you say they need to catch up FASTER and then you say they will always lag behind. so they're either catching up or not, make your mind.",Negative
AMD,"> And fsr 4 can be in game with dlss or fsr as well using same method. >  >  >  > We talking official support.  Setting DLSS Override to Latest globally in the NVIDIA app isn't exactly the same as using a third party tool like OptiScaler. The NVIDIA support IS official, the AMD support is not.",Neutral
AMD,">And fsr 4 can be in game with dlss or fsr as well using same method.  >We talking official support.  DLSS overrides for any game using DLSS 2.x+ are officially supported through the Nvidia app and have been since DLSS4 came out. Meanwhile AMDs official FSR override just came out and requires a game has FSR 3.1 + DX12, which is a significantly smaller pool considering no vulkan support and that FSR 3.1 only came out late last year.  >By your logic also fsr 1 has over 6k Game support as its in all emulators  Sure but FSR1 is also garbage relative to modern upscalers and it's just a basic spatial upscaler. Also kind of irrelevant anyway because Nvidia has NIS which is practically the same thing as FSR1 and it works with every game too.",Neutral
AMD,I’m confused about how there would be much more than 85 FSR 3.1 games when FSR 3.1 isn’t even that old?,Negative
AMD,You mean those “AI Accelerators” and it being “Architectured to exceed 3.0Ghz”?,Neutral
AMD,"I use both FSR and DLSS as I split time between two places and keep a PC at each. I definitely wouldn't say they're reasonably close yet. It's not the canyon wide gap that was DLSS3 vs FSR3, but it's still quite significant in my experience.  And as far as I can tell the disocclusion artifacting issue is highly dependent on the game. I've only encountered it a couple of times, whereas I encounter quite a lot of artifacting with FSR every time I use it.",Neutral
AMD,"My takeaway was that it's very marginally better than DLSS3. Not close to DLSS4. Though to be fair that's still a big jump forward for FSR. FSR3 was straight up bad, 4 is definitely not. Hopefully they can keep it up because I'd like nothing more than for AMD to be a more competitive option on the software front.",Positive
AMD,Dlss for ever game is not officially support via override go try to use it in greyzone you cant it has dlss 4 fg but 3.0 upscaling  The support for DLSS 4 transformer model is a white list and its very limited. Most people using optiscaler but u dont use that in a anticheat game so its limited in use.   FSR 1 is still the objectively best upscaler its just that DLSS/FSR replace TAA and because FSR 1 doesn't you get the shitty built in taa.  NIS is no where near as good as FSR 1 or as good as RSR btw. Its not even comparable NIS is just a low tap Laczos with no edge refinement or anything not to mention the sharpening filter in NIS is the worst sharpener I have ever seen. I don't know why Nvidia doesn't hire some reshade devs who have made good filters because AMD has the best sharpeners and Nvidia has the worst it would help DLSS greatly if they had a sharpener even on the same ballpark as rcas.,Negative
AMD,"I have been running mine at 3.15GHz for 2.5 years. It was literally architected to exceed 3GHz or I couldn't do this as a daily all-games, all-apps thing. Unigine Heaven will run 3.4GHz stable. And 7900 XTX performs just fine in AI for anything where a dev has bothered",Neutral
AMD,"No idea what the 3ghz is supposed to represent but the lack of fp8 is the reason. There is a big performance hit when using it on rnda 3 and you have people running it on Linux as proof. Rdna 3 will probably get a in-between version, same one the ps5 pro will upgrade to if I had to guess.",Negative
AMD,"I personally disagree, MHW and Spider Man 2 looked equally as good in my experience on both a 4070 super and a 9070 xt. Maybe the latter's power helps that, but I was very impressed with FSR4 outside limited support on launch. The tech itself is nice.",Positive
AMD,"Is FSR3 better than DLSS1? What about DLSS2?  I sometimes think about how something which was worse than FSR3.1, which is not great, made people so hyperbolic, like it was such a big deal",Negative
AMD,"It is officially supported in every game if you use the global override, you can confirm it with the DLSS indicator overlay. Even then though if you draw the line at only counting manual per-game overrides for some arbitrary reason DLSS4 overrides are supported in \~175+ games (as of last month) while FSR4 overrides are in \~85 games (as of today) which is still significantly more  >FSR 1 is still the objectively best upscaler   lol, no.",Neutral
AMD,Dlss override does not support every game.   The dlss 4 chart is incorrect as it counts any Game with any dlss 4 support.  Greyzone has no dlss 4 upscaling but has dlss 5 fg its on the dlss 4 list.,Negative
AMD,">Dlss override does not support every game.  My guy I just tested this a few days ago with the DLSS indicator @ global Preset J override, it does.  >Greyzone has no dlss 4 upscaling but has dlss 5 fg its on the dlss 4 list.  Even the game you're literally using for an example supports DLSS upscaling overrides, you can even find tutorials for it from [months ago on YouTube](https://www.youtube.com/watch?v=_pxZyWJp68c&t=403s). Do you even have a Nvidia card capable of DLSS or are you just pulling stuff out your ass?",Neutral
AMD,That guy is using profile inspector not Nvidia app that will get you banned,Negative
AMD,"> The TA95X3D brings the power efficiency of a mobile form factor, but it sacrifices upgradability.  Except those two soldered CPUs have the same chiplet layout as their desktop counterparts, with the same idle power issues.  Is there any sizeable difference to getting a socketed model and setting a power limit?",Negative
AMD,Minisforum is currently selling this type of motherboard but for the 7000 series,Neutral
AMD,"Hello kikimaru024! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,I think price is also a factor the 9955hx3d model costs 661 usd while the 9950x3d alone costs 670 usd.,Neutral
AMD,AOOSTAR's model seems to have 4x SATA but in mATX form factor; Minisforum is only mITX with 0x SATA.,Neutral
AMD,"You can look up reviews of similar CPUs from previous generation (7945HX3D), it's very capable - especially considering it uses ~~lots~~**less** power than the desktop models.   If Minisforum or whomever could make an ITX model with 2x SATA I'd be all over that.",Positive
AMD,Yes but based off desktop,Neutral
AMD,I hope their models will use dimm rams instead of sodimms,Neutral
AMD,"SODIMM is a very sad state of affairs for the minisforum board, considering the (lack of) quality DDR5 SODIMM memory vs the great kits you can get in normal DIMM form factor now.",Negative
AMD,Cheapest sodimm is 75$ for 32gb 5600mhz cl46 model while you can find deals for 6400mhz cl32 dimms 32gb,Neutral
AMD,Yep :'(. 2/3rds the latency!,Negative
AMD,I'm waiting for the AMD 3100x3d,Neutral
AMD,Waiting for a special edition die-shrink of the 5800x3d as a last hurrah of AM4.,Neutral
AMD,it's not available worldwide :(,Negative
AMD,"I don't really get what AMDs strategy is here. They have gone 5800X3D, 5700X3D and down the stack to 5500X3D while the those earlier higher performing models are largely gone and not made anymore we get lower performing ones introduced.  We get these while X3D is not coming to lower end CPUs in the 7000 and 9000 series. It can not be the case that this is a higher margin way to sell the cache silicon add-on of X3D.  Are AMD just trying to shift old poor quality dies they have laying around?",Negative
AMD,I'm lazy why doesn't gamers nexus just have an average data slide,Negative
AMD,give me 5950x3d to replace my 5950x so it can catch up with my rtx4080 somehow,Neutral
AMD,"AM4 : ""I didn't hear no bell"" 💪",Neutral
AMD,"Charts are missing the 5800XT which is currently around the same price as this ($205 USD). Would've been nice to include it to see if the 8-core non-X3D part is a better value. The 5950X is in the charts, but you never know whether games will act funny due to the 5950X having 2 CCDs.  The 5500X3D has a very poor showing in productivity so perhaps the 5800XT would just be a better all-around pick especially if playing in 4K.  All-core sustained clocks:  * [8C] 5800XT: 4500MHz (via HUB) * [8C] 5800X3D: 4300MHz * [8C] 5700X3D: 4000MHz * [6C] 5600X3D: 4350MHz (This is why the 5600X3D is better than the 5700X3D in some game benchmarks despite the core deficit.) * [6C] 5500X3D: 3950MHz  It seems that the 5500X3D is just poor quality silicon if it can't even hit 4GHz. That's a huge clock speed deficit compared to the 5800XT and 5800X3D. When the 5800X3D first came out, it spanked the 5800X even though it was clocked 200MHz lower all-core (4500MHz vs 4300MHz).  But 5500X3D vs. 5800XT? I'm not convinced the X3D CPU is better. The 5800XT is top-binned silicon and could be OC'd to 4.9-5.0GHz all-core. The 5500X3D is stuck with 2 fewer cores and 3950MHz (you can't OC X3D CPUs). That's a ~1GHz clock difference and I don't believe the +64MB 3D V-cache makes up for this. The 5800XT still has 32MB L3 cache; it's not a Celeron. For those on a budget, the 5800XT looks even more attractive because it has gone on sale for $125-150 and comes with a decent cooler.",Neutral
AMD,"With those crazy fps we currently get, reviewers should focus more on 0.1% lows instead of 1%. 1% were good enough when average framerates were <60.",Negative
AMD,"Its awesome to see AM4 still alive, while Intel requires a new mobo for every generation...",Positive
AMD,"Remember the thread from a couple days ago when everyone said that Hardware Unboxed cherry picked the benchmarks to show that 6 cores CPUs keep up with 8 core CPUs in gaming and even when there's an improvement, it usually wouldn't be large enough to be noticeable with the naked eye? Funny how nearly all the benchmarks here show the same thing.",Neutral
AMD,"I have it, it's really good.",Positive
AMD,Looks like AMD's marketing department figured out how to make product focused videos instead of drama again.,Positive
AMD,how has there been no 7600x3d or something yet thats not a best buy exclusive  i think a 199 USD  x3d chip on the 7 series would be an AMAZING seller,Negative
AMD,Alternative title:   YouTuber who never used AM4 long-term declares AM4 is the GOAT platform because AMD keeps releasing new defective CPUs to maximize profits.,Negative
AMD,"Factory reject CPU that should not exist, on a stone age platform.",Negative
AMD,Backport 3D cache to Bulldozer.,Neutral
AMD,I'm waiting for the budget Ryzen 3 2200G3D,Neutral
AMD,Single core,Neutral
AMD,I'm waiting for the AMD 3150c3d. Let's see how zen1 holds up,Neutral
AMD,"If I'm not mistaken, Zen 2 actually had TSVs and was designed with 3D stacking in mind.  It's possible, possibly with moderate tweaks, that Zen 2 could get 3d vcache.   No idea if the actual implementation on Zen 2 was bugged or not and Zen 3, 4 and 5 are a lot more modern.",Neutral
AMD,"nah, zen4 or zen5 on am4 package. 100% it is doable.",Positive
AMD,Yet.   AMD has done this in the past with regional or retailer specific launches before quietly releasing to the wider market after a few months.,Neutral
AMD,"> Are AMD just trying to shift old poor quality dies they have laying around?  Pretty much. All of the newer AM4 CPU releases are low-binned CPUs. The exception is the 5800XT which is a slightly faster 5800X. (The 5900XT is a slightly worse 16-core 5950X, not a better 12-core 5900X, adding to the confusion.)  The 5600X3D has higher clock speed than the 5700X3D, but 2 fewer cores. So the silicon is better on the enabled cores, and it wins on some games where clock speed matters more. The 5500X3D has lower clock speed _and_ 2 fewer cores, making it the lowest quality silicon of the bunch.",Neutral
AMD,>Are AMD just trying to shift old poor quality dies they have laying around?  Pretty much. Stockpile dies not good enough for 5700X3D and sell them late.,Negative
AMD,"Given that I thought the x3D packaging has a non zero failure rate.   Like, does breaking the CPU dies by putting the vCache on them allow them to write the broken dies off on taxes? While letting them sit in inventory unsold doesn't?   Like, obviously being able to sell them as working products is better, but. I mean, if the issue is they have unsold 5500 level CPUs, I would be more worried about. Breaking them with the die stacking, which I thought was something that could happen.",Negative
AMD,"My guess is because their business relies on YouTube metrics and if people just skipped to the slide, it could be financially bad for them.   Also, there's nuance that overall average performance can miss. Maybe a part excels/sucks in one genre of game specifically and it skews the average",Negative
AMD,"Better value for gaming - i don't think so  If productivity is a real concern, a high core count dual CCX cpu is probably needed anyway (I also don't think many who used am4 for productivity need/want to change cpu at this point)",Negative
AMD,"Why would you spend a 200 on a slightly binned non-x3d zen3 8-core? Like only ppl who think about am4 are ppl wth zen1/+/2 cpu:s and at that price point, why would you make that upgrade. The whole meme about the XT cpu:s is that it's $50 for letter and nothing else, can just manually OC a 5700X to be near the same or more/less cores for more/less money if you care/don't acre about productivity.  Just don't get a 5700 it's a 5700G without the IGPU, great naming amd!",Neutral
AMD,Yea 0.1% is better for sure. Frame time health would be best but only DF does that.,Positive
AMD,The 0.1% lows are still crazy fps. At this point fps is a solved problem in non RT games.,Negative
AMD,I mean this is no different than if intel released a 14599K with a 5% performance reduction to the 14600k… simply repackaging worse dies on a bad yield CPU isn’t anything special,Negative
AMD,"proper ddr4 ram is very expensive though if u want that, or if u need a new am4 mobo well there are no good left to be bought. only a couple of meh models left in retail. At least when I sourced the retail channels so to speak.",Negative
AMD,"LGA 1700 supported 2 gens, not bad. Raptor Lake was a really strong upgrade except for the RMA rate. Yeah it's not like AM4 which supported 4 gens, but i5 Raptor Lake beats i9 Alder Lake in gaming. Very rarely we see such leap within a single gen.    LGA 1851 was supposed to support 3 gens but became a single gen platform because Meteor Lake-S and Panther Lake-S were cancelled, perhaps for a good reason, which is the chips were very mobile focused and scaled poorly on desktop as shown by ARL-S only achieving low 5GHz and poor ring clock speed and latency.   Considering intel's current financial difficulties, I'm sure it was considered a waste of resource to design and validate desktop chips that aren't amazing on anything but power efficiency. LGA 1851 still supports 3 gens on mobile, where power efficiency matters the most.   LGA 1954 will likely support 3+ gens again, like LGA 1851, but with all the products coming to desktop. The first desktop CPUs for it, Nova Lake-S, are definitely coming. The future gens reportedly will feature unified cores, which is good news for desktop. No more p-core/e-core gap.",Positive
AMD,This is the main issue Intel needs to fix for me. Core2 duo was my last Intel they'll need to do what AMD does,Neutral
AMD,There is Bartlett Lake. Same deal as this. Double interesting too that no one calls AM4 a dead end platform when talking about buying it's low end CPUs,Neutral
AMD,"HUB has consistently claimed that new gen 6 cores are always faster than previous gen 8 cores, dont try to twist facts by changing it to ""keeping up"". And in this video we can clearly see that's not the case in BG 3 where a 5700x is slightly faster than a 7600 even though the latter has higher frequency and uses way faster memory with higher bandwidth.",Neutral
AMD,"The people claiming otherwise literally provided 0 evidence from their own hardware or any benchmarks from any reviewer.  ""You're suppose to have 30 tabs open, 4k utube video, rendering a vid, discord, spotify, netflix and benchmark at 720p""  But 14600k is better than 7600/7500 if you're going to keep ur gpu under 4090 for the next 5 years because you're going to be gpu bound.",Negative
AMD,"I thought that was known for a while now, in even some CPU intensive games only utilize up to 4 cores. That's why it was notable when BF6 beta showed capabilities of  actually using all cores when playing.   I'd understand it if people are more afraid of the future when games suddenly release that has that capability, but in the meantime any 6 core CPU with similar spec should be similar in games with higher core CPUs.",Neutral
AMD,"There is 7600X3D available, maybe not in 'murica, but in Europe it's very available. Obviously not $199 cause why would it be the normal 7600 seems to be $185 USD and the launch price was $299 USD a year ago and it's currently ~$307 converted when removing tax here.",Neutral
AMD,"well it is not that far off, but I would call it more like. Manufacturers gimps the model u want so that they can release a cheaper version of it instead of offering us the older model for less several years later because it would cut into the sales of the current gen models.     The binning costs money so I guess it is cheaper to just not trying to bin the old am4 cpus and just release them as the new sku with the new lesser binning.",Neutral
AMD,I really hope that title shows up on dearrow because it's the only accurate title,Positive
AMD,"That is possibly the most unfavorable, disingenuous way to frame this.   If I ran a successful YouTube channel, and my career was to benchmark hardware and then edit the content - i wouldn't use AM4 either. I'd run a 5090 and at minimum a 9950X3D - possibly even Threadripper. But my career requires Office Suite, a web browser, and RDP, so I use a ThinkPad issued by the company. I'm also sure that GN/HUB probably don't have tons of free time to play games because they have to work more than 40 hours a week to hit their current release schedules.   As for ""releasing new defective CPUs"" - that describes pretty much everything that *isn't* a 9950(X3D) or 285K.  A more reasonable take like ""the release of a 5500X3D doesn't mean AM4 isn't dead. It's just a limited release bin of an existing CPU to clear out the remaining inventory that didn't meet spec"" - I'd 100% agree with you.",Negative
AMD,>releasing new defective CPUs to maximize profits.  You say that like it's a bad thing.,Negative
AMD,and still matching Intel latest in games,Neutral
AMD,Release a `Socket 7` Ryzen CPU with massive 3d cache.,Neutral
AMD,lol. ive forgotten about bulldozer. I dont think ive ever seen one in person.,Negative
AMD,Would have saved bulldozer tbh.,Neutral
AMD,opteron 144,Neutral
AMD,On ddr4 speeds??? You cant balance that even with l3 cache,Negative
AMD,"Ehhh, Ryzen 5 5600X3D was Microcenter-exclusive and never became worldwide. If they had the stock to do a worldwide release I imagine they would have.",Neutral
AMD,Maybe Steve needs to be more entertaining so people want to watch the video for more than just the slide at the end.,Neutral
AMD,"It's impressive how Google is acting more and more like a stereotypical monopolistic mega corporation of a cyberpunk dystopian world as of late.  Google search is absolute sheet, YouTube is getting sheetier, side loading (as we know it) is being killed off on Android next year, and Chrome is... well, Chrome.  That's one reason I've migrated to FireFox, switched to DuckDuckGo, and already looking into installing a custom ROM on my Android with zero Google crap.   And YouTube isn't half bad with UBlock + YT Control Panel but... I've digressed enough!",Negative
AMD,"> The whole meme about the XT cpu:s is that it's $50 for letter and nothing else  The 5800XT has gone [on sale for $159](https://www.reddit.com/r/buildapcsales/comments/1lty6e5/cpu_5800xt_included_1tb_mp44l_m2_ssd_159/) as late as last month. That's why. We're not talking about MSRP prices. The 5800XT hasn't been at MSRP price for a long while now.  The last 5700X sale on r/buildapcsales was 9 months ago. So no, the 5700X isn't easily available for cheap anymore, just like the 5700X3D and 5800X3D. Obviously everyone wants the 5800X3D as the endgame AM4 CPU but now they're $450+ even on Aliexpress.",Neutral
AMD,"Ye, for all intents and purposes AM4 died with the 5800X3D launch. That was the last actual meaningful launch.   Intel will still be selling LGA1700 CPUs for years to come. Doesn't mean it isn't a dead platform, unless Bartlett Lake is actually launched for desktop.",Neutral
AMD,"yep, remember that amd was not willing to enable support for the zen3 cpus on older mobos until the very end of the am4 platforms life. That was something that many including I thought was such a f-you to us that bought their platform.",Negative
AMD,"Imo, the big draw is that AMD keeps manufacturing old parts for a long time, so new parts cause pricing to be pushed further down.  This puts pressure on the used market and older parts. It also means that there will be a much larger amount of Zen 3 in the used market relative to anything else. That reduces the ""end of the line"" proce creep that the strongest parts for a socket often have.",Positive
AMD,"Intel even did that a while ago, with their non e Core CPUs which were announced at the beginning of this year. So, basically the same, just a slightly worse version of the existing product. But in there no one cared",Negative
AMD,But they don't,Neutral
AMD,That is the most negative thing I have heard about a platform having a 10 year longevity. You are good at finding small details in a sea of positive news. lol,Negative
AMD,"> Raptor Lake was a really strong upgrade except for the RMA rate.  Aside from that, Mrs Lincoln, how was the play?   > but i5 Raptor Lake beats i9 Alder Lake in gaming. Very rarely we see such leap within a single gen.   That's pretty normal, if not underwhelming, for a generational upgrade though? The gap between i5 and i9 in gaming is pretty small to begin with. Hell, depending *which* i5 you get, it's literally just the same ADL silicon.  > LGA 1851 still supports 3 gens on mobile, where power efficiency matters the most.  LGA 1851 is a desktop socket. Mobile uses something entirely different. And what 3 gens are you talking about there? It's just MTL and ARL.   > The future gens reportedly will feature unified cores, which is good news for desktop.  That's probably well after this socket's lifespan.",Positive
AMD,"> Very rarely we see such leap within a single gen.   Not really. For Intel that happened with 11->12->13 gens, for AMD non X3D chips Zen+->Zen2->Zen3->Zen4.",Neutral
AMD,"The 9700X is only 0.5% faster than the 9600X on average at gaming  https://www.techpowerup.com/review/amd-ryzen-5-9600x/18.html  >Compared to the Ryzen 7 9700X the 9600X loses by a small 2% at 720p, but it keeps on gaining as resolution increases and beats it at 1440p and 4K by a wafer thin 0.3 and 0.4%. While that's not exactly conclusive, it's strong evidence that gaming performance between those two processors will be virtually identical.   https://www.techpowerup.com/review/amd-ryzen-5-9600x/29.html",Neutral
AMD,"Yeah, I looked into this very closely many years ago when choosing PC parts and found that the 5600X performed nearly the same as the 5700X and even the 5800X in most games. So too did the 5600X3D, 5700X3D and 5800X3D. And while games may have gotten more CPU intensive, the 9600X is still very good against 9700X.",Positive
AMD,"i play with 2500tabs open with a video playing in the background and playing at the same time, even on the hexa core systems and the perf penalty is not at all that bad, pretty much the same perf as an octa core or if I enable the e cores, heck even worse with the e cores on.  But if u render a video or something with the cpu then I guess it gets a tiny bit better on the higher core count cpu, even though it would still make the game unplayable on a higher core count system but for less time than on a hexa core.  People dont understand how it works.",Neutral
AMD,"Even if the main event loop in those few games you play do not take advantage more than a handful of cores, there are lots of games now doing shader compiles.    Some new games (e.g. Sony, Unreal) do that at first run or after new GPU driver install,  while better ones run it at the background (can cause stutters) and some older games do that between levels.  Shader compile is using most of my threads in my 5800X and cut back of the wait time and possible stutters.  There are other things people do that requires CPU cores. e.g. emulation, virtual machines, video encoding.  Silly to buy/build a PC for single use case and restrict yourself.",Neutral
AMD,The 7600X3D launched in America as well.,Neutral
AMD,>Look guys I know I only do reviews where I test CPUs for a few days with like 1-3 motherboards that are rarely updated with their BIOS revisions and are rarely on but let me tell YOU how great AM4 is.  Absolute cinema. Any sane person with a brain would ignore these tech reviewers.,Positive
AMD,It's also matching the AMD latest 9600x according to the graph.,Neutral
AMD,"does it? 5800x3d lost pretty badly in hubs own testing when they tested against an 12900k and faster ddr5 sticks compared to ddr4 sticks it used before.  I loved my 5800x3d and had no issues what so ever with what same on the net would call amd-dip, but my lga 1700 cpus including my 12700k tuned outperformed my 5800x3d which had tuned b-dies so even higher perf that we see here.",Negative
AMD,My first cpu was an amd Athlon 1.53ghz single core.   Then a phenom X 4 9850.   Then a fx 6100. (Bulldozer),Neutral
AMD,"IIRC, there were Excavator CPUs for AM4. The A12-9800 I think.",Neutral
AMD,"Having any L3 at all could probably have helped Bulldozer, yeah.   At a certain point though, the question becomes, if you make a big enough change to the architecture, when does it stop being that architecture and start being a new one?",Neutral
AMD,"Because Infinity Fabric is bottlenecking hard, there is limited benefit from DDR5 for Zen4 and 5. Keeping old Zen3 era I/O die would cost ~10% gaming performance for non X3D chips, even less for X3D.",Negative
AMD,"I've been wondering for a while if Zen4 or higher could be back ported to AM4 by adding a HBM module as an L4 cache, but I'm sure the engineers at AMD already ran the numbers and it wouldn't be cost effective.",Neutral
AMD,"same as 7600x3d, sad times",Negative
AMD,"If I'm going to pull out my own wallet I don't need entertainment, I want facts and figures  It was websites but now it's skipping around in YouTube videos and tpu reviews   Ltt and gn are both techtubers but hit different markets imo  But yeah I don't daily drive gn and go there when I buy or well major news breaks",Neutral
AMD,"I feel like he missed an opportunity to poach Emily Young out of LTT or hire them after quitting. They're a great presenter who has a similar tone but feels more engaging and speaks at a more measured pace. Look at the videos where Emily was host or the foil to Linus' energy.  Of course, I'm not sure Steve's ever gonna pull from LTT at any point in his career, but it's quite surprising how, despite his and his team's improvements in writing, the hosting stagnated.",Neutral
AMD,You're allowed to say shit.,Negative
AMD,"4.3 petabytes get uploaded to YT *every day*. I don't think Google's actions surrounding ads or YT Premium have been unreasonable considering that metric.   I pay for YouTube premium because its YT is my most used service. Don't mind the price vs the value.   I also dont think YT's stance on minimum watch time to count as a view is unreasonable. YT's algorithm is based around clicks - and as much as people say we hate click bate titles, they work. Thats more to blame on human nature than Google specifically imo.   But I do also think Google as a company sucks. They lack a unified, creative vision. They extract value from the few hits they had years ago while being unable to replicate past success. They dont care for a tight UX and overall attention to detail, and care only about having a ""good enough"" product to extract user data....  But realistically, only a massive company like Google that has other revenue streams based primarily on user data can even make a service like YT viable at all.   Also I played around with custom ROMS a lot back in the day. If I ever reach the point of taking the next step in de-Googling by life, I'd just switch to iPhone.",Neutral
AMD,"I mean there is a reason Google lost three different anti-trust cases in like the last year or so (one over web search, one over their ad network, one suit filed by Epic games over treatment of third party stores); trials right now are going on to determine the penality Google will face, hopefully the DoJ follows through on some of their more hefty proposed remedies like forcing Google to sell off Chrome.",Negative
AMD,Your just being contrarian hopefully that's just youth as irrational contrarianism isn't a good look for an adult.,Negative
AMD,"They've always been like this, the moment they made Chrome people called this shit from a mile away. They make money from ads they need as many people to see ads and all the things you listed can get in the way of that. Fuck, Google that's why I never used chrome.",Negative
AMD,breh your neck beard needs a trim,Neutral
AMD,"The 5700x is pretty cheap here in Brazil right now through amazon, in comparison to the 5500x3d that is costing more making It the x3d not worth it here",Neutral
AMD,Nah the 5700x3d because it was actually affordable.,Neutral
AMD,At least you can still find AM4 boards at retail at reasonable prices now. You cannot say the same about Intel platforms from 2017.,Neutral
AMD,You are forgetting all the 1600X owners that still see any 5XXX series CPU as a great upgrade.,Neutral
AMD,"they did, Q1 this year.",Neutral
AMD,"there was a lot of negativity before am4 got obsolete. The biggest thing is that AMD did not want us to run zen3 cpus on older boards which they let us when the platform was end of line.  That was much more negative than the fact that fast ddr4 sticks and the vast amount of am4 boards that once were avaible are not anymore. How many owners of earlier am4 boards were forced to upgrade to a new board when they went with zen3 cpus when they did not really needed that at all.  That was negative, not that some components are rare now.",Negative
AMD,Why are you bringing up averages when HUB literally brought one game?,Neutral
AMD,omg people were down voting you...wth...,Negative
AMD,"yea, pretty good huh",Positive
AMD,"never owned a bulldozer/piledriver as at that time a phenom was just as good at the start of those fx cpu life and at the end intel was the only way.     But, my first proper pc was an amd k6 200mhz cpu, which was barley faster than my friends 120mhz pentium. But then athlon came out and omg... those were the times.",Neutral
AMD,Isn't Infinity Fabric speed tied to memory speed though? At the same rate or half the rate or something? Or has there been some big change when I wasn't paying as much attention over the past few years?,Neutral
AMD,7600x3d can be bought in Europe,Neutral
AMD,"YouTube was self-sufficient the last time I checked.   Google isn't doing any of this out of the goodness of their hearts.   There's no such thing as free lunch, a simple fact most people seemingly lack the capacity to appreciate.",Negative
AMD,"> But I do also think Google as a company sucks. They lack a unified, creative vision. They extract value from the few hits they had years ago while being unable to replicate past success. They dont care for a tight UX and overall attention to detail, and care only about having a ""good enough"" product to extract user data....  That description made me think of Microsoft. Strikeout Google and replace with Microsoft and it's just as accurate.",Negative
AMD,"Don't forget constantly killing good products and somehow having 7 overlapping calendar or task services.  But yeah, I don't think YouTube could exist as an independent company. Not unless they dropped above 1080p resolutions and compressed everything to hell. It's insane that a free service provides 8K60 HDR upload and playback.",Negative
AMD,it was?,Neutral
AMD,"what no.... they are crap boards and ddr4 are getting more expensive and they are crap sticks as well. no good am4 stuff left, at least when I looked.",Negative
AMD,"So what, so that the 10 people who has a motherboard that breaks long enough after purchases that had they gone Intel it would be EOL can get one?  That is a extremely niche positive. While motherboards do break more often than CPUs. They as all electronics follow the bathtub curve. If it didn't break in the first year of you owning it, it is not likely to break within the reasonable life time of the product either. Baring some glaring issue like the capacitor plague or specific design flaws with that model.  And if you are using that 2017 platform and upgrading it along the way. You will pay a price for it. A 5800X3D will be held back by PCIe 3.0 before it becomes obsolete as a CPU. Do you then get a new board for a dead platform to gain 4.0? Nullifying the main advantage vs going with a newer contemporary Alder Lake platforms or just getting a at the time new 4.0 AM4 board?",Neutral
AMD,"And why is that? Because AMD started at a very low point. The 1000 > 5000 series jump is not normal in that time period. AMD started at similar performance level of Haswell, which was 4 years old at the time.   Reddit is heavily biased towards entusiasts when it comes to PC hardware discussions. People upgrading every other generation is not the norm.    Had you instead bought a 8700K back in 2017, you would still be using a chip that is perfectly passable today. Sure the fastest AM4 has to offer is considerably faster for things like gaming. But it is not nearly the same leap as someone who jumped on Ryzen 1 and is now comparing vs a 5800X3D. The jump from a 8700K to 5800 series is closer to ""normal"" progress in the 5 year period from 2017 until the last major AM4 launch.  Normally, most consumers would see no reason to upgrade in that time frame. If you bought a 7800X3D at launch, by the time that thing is obsolete for a normal consumer and they would consider a upgrade. You would be looking at a whole new platform. Because the last X3D chip released for AM5 will itself be old news by then.",Neutral
AMD,Which CPU are you referring to?,Neutral
AMD,"Most of the criticisms were that the benchmarks were cherry  picked because the video in question showed one game. Therefore, Gamer's Nexus above as well as the 9600X review from TechPowerUp show an average of games which corroborate HUB's claims. This addresses the criticism of ""cherry picking"" data.  In addition, HUB has made countless of those videos over the years and they had an average of the games. There is a clear trend in the data.  https://youtu.be/7L9rPNSuPCA?si=4zAgnP8t2z1OKEJN",Neutral
AMD,They all bought 9800X3D's and play at 4K and can't admit it was a waste of money.  X3D's only make sense if you are playing on a 5090 at 1080p and only play esports titles. For sane gaming it makes zero difference over a 9600X. We are all GPU limited at the resolutions and settings people actually use.,Negative
AMD,"I went with my dad to buy a Compaq in 98 or 99.. it had an amd k5 533 mhz cpu. Lol. He spent 2500 on that pc lmao. It had 128MB SD ram. We had computers before that, like ibms and stuff, but I wasn't into computers then. But yeah, the thuban amd x6 six core cpu i remember was a true 6 core and people liked it more than bulldozer",Neutral
AMD,You can change the mem clock to IF bus ratio around however you'd need to.  It'd be fine.,Neutral
AMD,not worldwide,Neutral
AMD,TIL the world is US and Europe,Neutral
AMD,">But yeah, I don't think YouTube could exist as an independent company.   Yet their business model is somehow self-sufficient...",Negative
AMD,"I remember it hitting 170 euros in the netherlands, the 5800x3d was never below 300.",Neutral
AMD,It was like $130 for Ali express at one point.,Neutral
AMD,it was selling for 200CAD at one point (~160USD?),Neutral
AMD,"Yeah the dual rank, Samsung b-dies are long gone. Also, there are not much AM4 mobo options at this point except for budget B550 mobos out there.",Negative
AMD,"> A 5800X3D will be held back by PCIe 3.0 before it becomes obsolete as a CPU.  Doubt.  [5090 loses ~8% at 3.0x16](https://www.techpowerup.com/review/nvidia-geforce-rtx-5090-pci-express-scaling/11.html) in the ~worst (eyeballed) tested game.  [5800X3D vs 9800X3D is down 24% in the same game](https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/18.html).  You could quibble about ""held back"" being a different standard than ""obsolete"" (I am typing this on a Haswell!), or construct scenarios with under-VRAMed GPUs in heavy swapping, or an high-queue-depth IO-bound workload.  I think only the last one would be honest, though, and such workloads are rare.",Negative
AMD,My msi x370 carbon has a word for ya https://www.3dmark.com/3dm/120628256,Neutral
AMD,"Intel released the Raptor lake CPUs, this time without e Cores. Just look at their website",Neutral
AMD,Show me that trend on the 9600x HUB review vs 7700 where the former is significantly faster than the latter on average.,Neutral
AMD,"yep, I only play at 1080p/1440p lowish with an high end gpu/cpu. Actually had two 4090 but now I swapped over both of mine desktop system both intel and amd x3d to 9070xt because of how well they perform at lower res in bf6 beta and warzone.",Neutral
AMD,"It was common here in Sweden as well, to get a pc via the job, here the biggest employer was Volvo/Saab and people from Volvo got to buy/lease whatever it was IBM Aptiva machines. A freind got a pention 75 I think and omg what a machine. But then the development started to move in a blistering fast pace. from 486 to pentium at max 120mhz to 1ghz and accelerator cards and then proper 2d/3d video cards. Now it kinda is at a standstill and only the Halo gpus are actually worth the upgrade but they are locked by the mega extreme pay-wall.  yep, those fx cpus were very meh, and even my friend that hates everything that is intel or nvidia used his phenom until am4 came out.",Neutral
AMD,"Not worldwide yes, but not microcenter exclusive",Neutral
AMD,The point was that its not microcenter exclusive unlike 5600x3d,Neutral
AMD,"For whatever reason mine was going to be 140 but they had it even cheaper with klarna, So I got mine out the door at 123, then I sold my 5600X for 80 bucks   All in all, a $40 upgrade, which is insane",Positive
AMD,"Grats on losing performance on a $1500 graphics card I guess? And before you say ""it's only marginal"". The thing with PCIe is that it does not hit every title evenly.  Rather most are not affected at all while some titles are [severely](https://tpucdn.com/review/nvidia-geforce-rtx-4090-pci-express-scaling/images/metro-exodus-3840-2160.png) impacted.",Negative
AMD,"Are you talking about Bartlett Lake? So far, at least, those are just lesser versions of the 8+16 RPL die. Maybe some ADL mixed in there as well. Also not available at retail.",Neutral
AMD,You can see it in the TechPowerUp benchmarks.  https://www.techpowerup.com/review/amd-ryzen-5-9600x/18.html  A 5600X also consistently outperforms the 3700X.,Positive
AMD,"You missed the crucial ""never became worldwide"" part of that comment.",Negative
AMD,"So you call a 12% loss at the very worst severe although it still goes beyond my 4k144 monitor ? Okay, I'll keep that in mind and regret my purchase in silence. 👋",Negative
AMD,"Yes, that’s what I’m talking about.  And no, they are available in retail",Neutral
AMD,Zen 2 is 4 cores per CCX while Zen 3 is 8 cores per CCX. That means the benefits of more cores were nulled by die-to-die latency. All the 6 cores on 5600X exists on one CCX while 3700X is 2x4 cores.,Neutral
AMD,"Techpowerup is a garbage source for cpu benchmark results because they benchmark on low settings instead of max that are more cpu demanding and also hiding % lows metrics per game. Even taking into account those results, 3% difference can barely be called a difference in the first place, way off the mark of significantly faster and more like leaning into flop gen territory.",Negative
AMD,"i didnt miss anything.    I never stated that its worldwide available. I only pointed out that its not microcenter exclusive, so i dont understand why are you trying to correct me so much...",Neutral
AMD,"> And no, they are available in retail  Where?",Neutral
AMD,"Very likely that the doubled effective L3 cache size on Zen 3 is the cause of the difference, not CCX-to-CCX (two CCXes are on a single die) latency.",Neutral
AMD,"nah man, it is not that simple. I have not found that many games that actually impact the cpu the higher settings u chose. not in say like bf and wz and the like.   Some games like say gta5 and cp2077 have settings that affect the cpu by increasing the crowd/ai/object population but that issue is not here in mp fps games.",Negative
AMD,Pretty much everywhere. Just search for them.,Neutral
AMD,"I'm pretty sure Ryzen 5700 (5700G without iGPU) which has less L3 performs better than 3700X too, in gaming at least.",Positive
AMD,RT/PT require more cpu power aside other settings like crowd density.,Neutral
AMD,> AMD can confidently ship products across multiple nodes and product families without having to worry about over- or undercapacity at a single fab  Do you remember the chip shortage just a few years ago?  https://en.wikipedia.org/wiki/2020%E2%80%932023_global_chip_shortage  My company is fabless.  You can tell TSMC we want 10 million of this chip and 20 million of that on these dates.  That doesn't mean you are going to get the quantity on the date you want.,Neutral
AMD,"Only Alder Lake and Raptor Lake at Intel are tied to their process node; Intel's later products - Arrow Lake and Lunar Lake - and those that come later, aren't tied to any specific node.  Intel 7 is fully amortized and will likely be functional only to meet obligations for Alder Lake and Raptor Lake over their entire product life cycles.  Intel 3 is being used for their Xeons, and the Ireland facility that makes them recently announced upgrades for additional capacity. Intel 3 will also be used for the smaller iGPU tile in Panther Lake.  Panther Lake will be on 18A, future Xeons will be on 18A, and Nova Lake will also have 18A as well. Their upcoming E-core Xeon - Clearwater Forest - will also have base tiles for cache and interconnects made on Intel 3.  Seems flexible enough to me.  AMD's situation with TSMC isn't as advantageous as you think. For example they always deprioritize GPUs in favour of CPUs when both of them use the same node. RDNA4 is the most clear-cut example of this.",Neutral
AMD,">AMD can confidently ship products across multiple nodes and product families without having to worry about over- or undercapacity at a single fab.  They cant. This is why AMD had both CPU and GPU underproduction this year unable to fulfil demand for either. They simply didnt buy enough capacity in advance, which needs to be done years before you actually want the chips.",Negative
AMD,When Node Development was not a money hog it was good idea to be IDM but as Nodes got expensive and more difficult people started to drop and brings to our present condition of only 3 companies with fabs.  Now for Advantage of fab is margin stacking if you do it correctly you can get  > Margin Stacking   > Volume and you control your supply chain not other companies.  > Custom Process tightly coupled with design allowing you to bend design rules a bit.  Cons > Without enough volume it's a net loss to run fabs. >  Very risky and if you fail you end up like Intel.  Fabs less > Lower risk > Outsourced manufacturing means low Capex requirements. Cons > You are at the mercy of your partner if he screws up your product screws up as well. > IP Issue if your outsourced fab is also a competitor.,Neutral
AMD,"Non existent.  That was a thing about a decade ago.  First of all, Intel can still produce in different nodes.  Intel had the problem that they developed both architectures and process nodes together which both had advantages and disadvantages.  You can’t just switch out the process node for an architecture, every architecture is designed for some nodes specifications. The more you focus on one node the better you can use its advantages which is one of the reasons why Intel was typically superior with their chips. But that also comes with drawbacks like Intels 10nm issues.  Intel didn’t stay with basically Kaby lake for years because they wanted to but because they couldn’t switch to ice lake without the 10nm node. And that’s the reason why Intel fell back on the desktop and servers. If you look at their roadmaps, what became Tiger lake H was supposed to come out as Ice lake in like 2018. But with Rocket Lake they backported Ice lake to 14nm which worked more or less well, depending on who you ask. Since then, Intel developed their Architectures with no specific node in mind which makes them as flexible as any fabless company.   I’d say if anything it’s a huge advantage to have your own fabs today because you can determine how much volume you want for yourself.  TSMC is basically a monopoly at that point which also means that all three, NVIDIA, AMD and Intel compete over capacities and that drives up cost.  And then there is Apple. Apple is like a first class customer of TSMC (for good reasons) and they’re the ones to decide how much they leave for the rest.",Neutral
AMD,"> How big of an advantage is fabless flexibility in practice?  Huge. Not being tied to a fab's success or failure, not being forced to make a direct investment in that yourself, not having to worry about maximizing utilization of that fab, it all adds up. You can see this playing out with Intel real time. They've written down billion of dollars worth of equipment for their newer nodes because of low utilization, and their old nodes are so expensive to manufacture it's a major reason their foundry is so deeply in the red.",Neutral
AMD,It cuts the other way as well. A independent Intel foundry would have it a lot easier attracting customers.   Right now a customer wouldn't know if it isn't always a second priority to whatever intel deems necessary to their own products.   Also there is the question of exposing your roadmap to a company you are directly competing with on products.  A Intel with both design and foundry is only attractive to those who want monopolistic advantages. If a presumably better node were on intels hands it would only serve them and their products.,Neutral
AMD,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,">How big of an advantage is fabless flexibility in practice?    This is kind of bagging the question. That there is an advantage or even a choice here.   If you don't have 10 billion to invest in fabs you are a fabless. To basically everyone.   There is no flexibility on leading nodes in this industry. You get TSMC one of the few alternatives.   I imagine TSMCs customers pay what TSMC tells them to, only constrained by the reality they can't get too crazy lest they help support Intel when the customers revolt.   This is not a real thing in practice. Design teams are targeting a node whatever it is.   The cost to switch or support multiple nodes is what it is too. You can switch or hedge different options but there are costs.",Neutral
AMD,"I have a couple of follow-up questions since I’m not an expert on nodes:  * When you say **Intel 7 is fully amortized**, what does that really mean in this context? I get that fabs are expensive and equipment gets “paid off” over time, but since these machines are tightly programmed and occupy massive amounts of space, wouldn’t they still represent opportunity cost if they’re just kept alive to meet obligations? * Also, on the point about products being **“tied” to a node** — could you expand on what that practically means? For example, AMD has shown flexibility in the past, like when they re-lithographed Polaris (RX 580 → RX 590 on 12nm). Would that kind of move count as being “not node tied”? * And for Intel specifically, how should we think about this? Are their designs today still bound in ways that limit shifting across nodes, or is the tile-based approach (like with iGPU/cache tiles on Intel 3 vs. cores on 18A) essentially giving them AMD-like flexibility?  Would love to hear more of your perspective on that.",Neutral
AMD,"So basically with IDM you gain flexibility in design rules and stronger IP security, but you carry all the risk. If you have Intel-level missteps — like missing the mobile wave while PC demand was already declining — the fab overhead turns into billions in write-downs each quarter.  What I find interesting is TSMC’s strategy. They’ve always stuck to the principle of never competing with their customers, unlike Samsung, which builds both its own chips and fabs. That seems to be a big part of why everyone from Apple to AMD has enough trust to let TSMC handle their lithography.  Do you think TSMC’s “pure-play” stance is really the genius move here — the key reason they’ve become the dominant foundry while so many others dropped out?",Neutral
AMD,It's a huge risk to have an in-house fab that doesn't get any outside business. 2 points of failure on design and fab side that stop the whole thing.  Intel got stagnant on the design side and then stumbled on the fab side (which causes more churn on the design side) to end up in the huge mess they are in today.,Negative
AMD,"ADL/RPL were designed specifically for Intel 7. Intel 7 was design with ADL/RPL in mind. The two are closely related. It would require a lot of work to bring ADL/RPL to a different node.   One of the main improvements that ARL/LNL brought was internal design changes that make Intel designs in line with industry standards, using industry standard design tools vs previous designs being custom-made for specific internal nodes. This doesnt matter (directly) to the customer, but it does help Intel's engineering efforts easier going forward.    And while yes there's an opportunity cost to continuing to use Intel 7, a lot of Intel 7's cost structure was paying off the massive NRE costs due to its multi-year delay. These costs were fully depreciated H2 of last year, making Intel 7 much more cost effective.   But besides that, fabs need different types of nodes at different price points. The lithograph equipment used for Intel 7 can't just be reporposed for new nodes which use EUV. Its either use it on Intel 7 (or 16) or let it sit unused. Bartlett Lake exists almost entirely just for this reason.",Neutral
AMD,"1. That is basically what it means. As for tooling and machinery, some of them that were intended for the fab expansions that are now either cancelled or have been slowed down, have been repurposed as upgrades for existing manufacturing, so expect those to pay themselves off as well in the future. 2. Since Arrow Lake and Lunar Lake, Intel's core IP, at least on the CPU side, is ""99% process node agnostic"", to quote one of the lead engineers who worked on the Lion Cove P-core that goes into Arrow Lake and Lunar Lake. What it means that they don't have to design with a specific process node in mind, and their workflows and design rules reflect that.  3. Basically the choice of node boils down to performance-power-area - which are intrinsic to the nodes themselves, and cost of wafers, time-to-market, and volume availability. The economic aspect as characterized by the latter, can sometimes be more important in determining which product uses which node. For example, Nova Lake, the desktop successor to Arrow Lake, will use both 18A and some advanced TSMC node for the compute tiles.",Neutral
AMD,TSMC is not unfailiable no one is it's just they are executing better currently if Intel hadn't shot itself so many times in the foot TSMC wouldn't be so dominant. There Is only one rules in semi manufacturing the best node(PPA and yield) wins that's it.,Neutral
AMD,"Exactly, TSMCs last node, N3, was essentially an L. It took a while (and some alterations) for it to really get to healthy yields and performance. If they had competition, there absolutely could’ve been some customers willing to switch. IF. Instead, Intel was on fire and Samsung was busy huffing glue, so the status quo remained.",Neutral
AMD,Many of the big players never touched N3 and are planning to go straight from N4 to N2. If thats not a failure of N3 i dont know what is.,Neutral
AMD,Intel went to N3 cause Intel fumbled and looked like TSMC caught Intel dealy flu for 3nm lol,Neutral
AMD,"Thermalright's bigger coolers tend to perform marginally worse with lower TDP CPUs, if you bench with a 250W CPU I'd expect it to beat the Spirit slightly.",Negative
AMD,The Phantom Spirit 120 is GOAT.,Positive
AMD,"Note they're testing DeepCool's older Assassin IV, not the Assassin VC Elite WH, which is their latest and greatest vapor chamber cooler and has a 300W TDP rating. Would be interesting to see how that one fares.",Neutral
AMD,Phantom Spirit 120 SE is the goat. It’s better than the evo and that’s better than the royal pretor ultra. But all good options from thermalright.,Positive
AMD,"TBH I don't care about results on X3D CPUs, only noise samples.  Put these coolers on Ryzen 9 or Intel i7/i9 for proper comparisons.",Negative
AMD,What a useless review.  They all crossed 90C and were probably thermal throttling.  FWIW the Phantom Spirit 120 Evo perform better at low heat loads the the amazing Frost Commander 140. When the heat load gets turned up the tables turn. The bigger heat sinks were simply not evaluated properly.,Negative
AMD,I enjoyed the action music during the results. Maybe other reviewers like GamersNexus could learn something from this review.,Positive
AMD,Sounds a bit like the bottleneck might lie at the contact point/thermal interface then. Possibly something like pressure differences or even heatpipe layout if heat can't get distributed quick enough.  Would be interesting to see how both perform with a top-tier TIM and a good offset mount fitting Ryzen's hotspot profile.,Neutral
AMD,Not really interesting since DeepCool supported the direct invasion of Ukraine by Russian forces,Negative
AMD,"Going by the TPU review, the 9800X3D [can hit ~160W at stock](https://tpucdn.com/review/amd-ryzen-7-9800x3d/images/power-per-application.png) for proper MT apps like Blender.   Cinebench from this channel's test doesn't push it quite that much.   Here's the 9950X comparison, [~220W at stock](https://tpucdn.com/review/amd-ryzen-9-9950x/images/power-per-application.png).  But I agree, test these on some serious power hogs.",Neutral
AMD,"Any processor that auto boosts based off temperature matters and the x3d are no different.  And given the 9800X3D is the fastest gaming processor on the planet, respectfully disagree,  the data very much matters for the CPU",Neutral
AMD,"The best performer here, the Phantom Spirit, has 7 heat pipes. I wonder if that leads to slightly better alignment with the CCD. The added capacity of one more heat pipe would explain better temps in a high TDP part, but shouldn't make this big of a difference in a lower one like the 9800X3D on its own.  The Assasin IV also has 7, but didn't perform as well. They may not ""land"" in the same relative position over the CCD(s), or maybe it's some other thing going on here (contact, airflow, fin geometry, etc).",Neutral
AMD,GN has pointed out that TR has QA issues of late.,Neutral
AMD,And half of Europe bankrolls Russia by purchasing gas from them lol,Neutral
AMD,It's less about the total power usage & more about the fact X3D has different heat profile due to the X3D stack.,Neutral
AMD,">Any processor that auto boosts based off temperature matters and the x3d are no different.  > And given the 9800X3D is the fastest gaming processor on the planet, respectfully disagree, the data very much matters for the CPU  [The data completely disagrees with you.](https://www.youtube.com/watch?v=_Bv7Tn4zqRc)  A 1.5% delta in gaming framerates (226.5 - 223.0 fps)  despite a 20% delta in temperatures (63.2 - 52.4 °C) .    **6 (six!) MHz delta** in clock speed.",Neutral
AMD,"Yeah, it's wild how many small variables can play into optimizing this. Even things like optimal heatpipe distance to the baseplate, the method of bonding heatpipes to the surrounding copper, wick material and so on.  I used to be 'meh' about air coolers, but this outwardly simple topic continues to fascinate me with all its tiny engineering details.",Neutral
AMD,I had issues with both of my TR water coolers within a year. Both on the pump producing an excessive amount of noise. I recommend running the pump at 30-40% or in my experience they suffer issues quite quickly.,Negative
AMD,They have been having QA/QC issues ever since the Taiwanese owner sold the company to Shenzhen DIRISE,Negative
AMD,"Good to know, did they specify any product groups or was it more of a general heads-up?  Do you remember the source?",Positive
AMD,The U.S. government sanctioned entity: BEIJING DEEPCOOL INDUSTRIES CO LTD,Neutral
AMD,"Does that really matter now? For the 9000 series, the 3D V-Cache is below the die, not above as it was prior.",Neutral
AMD,"In fairness, the X3D chips are great at chasing pointers, so they tend to do well with programs that do a lot of business logic (ie. games). They can hit higher maximum framerates as a result, though that doesn't generalise to all workloads - just those where the working set fits into cache and relies on low-latency memory access.",Positive
AMD,"Unless I'm missing it somewhere the only workloads performed were blender and cp2077. And only a 5080 was used. So it's entirely possible the processor wasn't pushed to 100% with the workloads. Given the gaming temps were much lower than blender workloads, I'd wager their gaming suite doesn't sufficiently push the processor, and thus their data is limited.  Id want more info before I based a complete conclusion",Neutral
AMD,"I wouldn't touch ANY AIO from that operation; Air coolers, fine but better is possible. They're cheap and don't have the same catastrophic failure modes, but water? The cheapest thing that I can recommend in good faith is Arctic's lineup for reasons that are fairly obvious.",Negative
AMD,Doesn't some company have a monopoly on aio pumps?,Neutral
AMD,"Caused some sizable discrepancies with the PA140, disqualified it from their best list last year. Some of the swinginess of reviews on a number of products suggest to me that it's a wider spread problem then we might think IMO.",Negative
AMD,"It matters *less* on the 9800X3D, but that chip also doesn't care too much about heat anyway.  It will hit 5'200 MHz on any decent cooler.",Neutral
AMD,>Unless I'm missing it somewhere [...] it's entirely possible the processor wasn't pushed to 100% with the workloads.   Blender is 100% CPU rendering.   Games rarely utilize 100% CPU rendering.    Dozens of other reviewers corroborate this.,Neutral
AMD,"The problem with Arctic's solution is their fans are failing with noise issues as well. Its a sorry state of affairs really. I am looking forward to the Noctua AIO next year, that will be cost all put into the right places, reliability and performance and not screens and other stuff I don't care about.",Negative
AMD,"I may be misremembering, but I think it's that Asetek has a patent, which has since expired.",Neutral
AMD,"Can't really say. they tested it with a 5080 rather than a 5090 so even though they could push the CPU Harder, They didn't",Neutral
AMD,"Arctic's AIOs perform even better with better fans. For less than the price of an re-branded Asetek AIO slathered in RGB and LED screens, you can buy an Arctic LF3, replace the fans with Noctuas G2s, and have some pretty great performance.",Positive
AMD,I used my ML120s that i already had laying around on my LF2. The fans cost about as much as the AIO.   But i would not recommend anyone that dont use a 250W CPU to use one.,Neutral
AMD,"I'd look into In Win, Iceberg and maybe Enermax's lineup as well. Fans are less troublesome there anyway as Arctic has a fairly good warranty period anyway, IIRC?",Positive
AMD,"That might effect new, redesigned models of aios.  Ones made with non asetek pumps?",Neutral
AMD,"And with that, we might see a few new players enter the AIO design market, such as maybe a return of that Canadian outfit that got burned last time?",Neutral
AMD,"Whatever you say, buddy.",Neutral
AMD,"There's also the good ol' eLoop if you're in Europe, as it's a well proven low noise general purpose fan and that includes radiator applications and has ARGB if you like that sort of thing.  Use a spacer if you push-pull mind.",Positive
AMD,"Again, I may be mistaken, I don’t follow AIOs closely, but I don’t think there’s many that didn’t pay a fee to Asetek to make a pump derived from their patent.",Neutral
AMD,"idk, but I'm looking forward to the Noctua AIO that's coming out next year.. it'll be in time for the next build (probably Zen 6 X3D but maybe Intel Nova Lake if it's better than expected) I'm planning on doing.",Positive
AMD,"They do have a new dual impeller design, but that patent seems less... all inclusive... then the old one.",Neutral
AMD,make sense. just look at both Nvidia and AMD Q2 revenue. Nvidia gaming revenue was 4.3 billion (10% of Nvidia’s total revenue) while AMD gaming revenue was 1.1 billion (14% of AMD’s total revenue). Q1 gaming revenue was 3.7 billion (nvidia) and 0.6 billion (AMD),Neutral
AMD,"As we closely approach the alleged 50 Super with more VRAM, my guess is Radeon won't appear at all this gen (maybe down the line when prices drop further). AMD trick in its sleave is that 9070 has 16GB of VRAM and its a bit faster than 5070. But that its just not enough to overcome Nvidia ecosystem of ML goodies.  A potential 18GB 5070 at 549$ will absolutely kill anything AMD wise (I also don't think AMD will do refreshes).",Negative
AMD,Top gains this month:  * RTX 4060 - 4.85% (+0.46%) * RTX 5060 - 1.01% (+0.41%) * RTX 5060 Laptop GPU - 0.69% (+0.27%) * Nvidia Graphics Device - 0.67% (+0.26%) * RTX 5070 - 1.57% (+0.25%) * RTX 4060 Laptop GPU - 4.62% (+0.19%) * RTX 5070 Ti Laptop GPU - 0.18% (+0.18%) * Intel Graphics - 0.17% (+0.17%) * RTX 3060 - 4.79% (+0.17%) * RTX 5060 Ti - 0.74% (+0.16%) * RTX 3060 Ti - 2.84% (+0.15%) * RTX 5070 Ti - 0.75% (+0.12%) * RTX 5080 - 0.74% (+0.09%) * Radeon RX 7800 XT - 0.64% (+0.08%) * Radeon RX 7600 XT - 0.27% (+0.07%) * RTX 2070 - 0.66% (+0.06%) * RTX 5090 - 0.26% (+0.04%) * RTX 4050 Laptop GPU - 1.41% (+0.04%) * RTX 4070 Ti - 1.10% (+0.02%) * RTX 2080 - 0.33% (+0.02%) * RTX 4070 Ti SUPER - 0.87% (+0.02%),Neutral
AMD,9070 is just never going to be able to compete with the 5070 at 525-550 and then the 9070 xt at -50 of the 5070 ti is just never happening outside of the 5 linux users.  I understand outside of the US prices are different but in the US they can't compete,Negative
AMD,[9070XT users.](https://www.youtube.com/watch?v=lKie-vgUGdI),Neutral
AMD,Babe the monthly reality check for AMD users came out  It's wild to see that the hecking 750 TI has double the users of the 9070,Negative
AMD,Rdna4 still not found. 6% market share,Neutral
AMD,An RTX 4060 an 8GB GPU is now officially the most popular GPU in the whole world. Even when you go to internet and the vast of Tech YouTubers hates it and doesn't recommend their audiences on buying it. It just clearly shows us how the pc hardware enthusiast community is such a small fraction compared to your average joe PC Gamer who doesn't need more than 8GB of Vram.,Positive
AMD,"Just like the last time Steam hardware survey got posted and [people said the 9000-series isn't in it](https://www.reddit.com/r/hardware/comments/1mfe48z/comment/n6gvlzc/?context=3), I'd like to once again point out that the 9070 does show up on this page: https://store.steampowered.com/hwsurvey/directx/",Neutral
AMD,Redditors and tech influencers in shambles as 4060 and 4060 laptop are 1st and 3rd in the rankings.,Negative
AMD,That fake MSRP hurt AMD more than I thought,Negative
AMD,Who is running DirectX 8 GPUs and below,Neutral
AMD,7800xt gaining share when rdna4 cards existed fuels my beliefs even more that rdna4 was more limited stock than rdna3. AI card boom distracted amd to print money.,Neutral
AMD,I assume the Radeon graphics are 2 igpu AMD SKUs. Not bad and I'm guessing those are the handhelds,Neutral
AMD,Top increases in August 2025:  - NVIDIA GeForce RTX 4060 +0.46% - NVIDIA GeForce RTX 5060 +0.41% - NVIDIA GeForce RTX 5060 Laptop GPU +0.27% - NVIDIA Graphics Device +0.26% - NVIDIA GeForce RTX 5070 +0.25%,Neutral
AMD,"Pretty cool seeing the 5070 climbing that fast. Kinda surprised RDNA4 still isn’t showing up at all, guess folks are sticking with Nvidia for now. Wonder how long till AMD makes a dent.",Positive
AMD,"the survey is rigged, and prebuilts/laptops don't count because like fake frames, they are fake gamers. real gamers are enthusiasts who buy great value gpu like AMD and not high performance slop. so in reality amd won. hardware unboxed already posted facts that 9070xt outsold blackwell.",Negative
AMD,"Why is nobody buying the 9060XT 16 gig? I got one and it's outstanding, and early reports from reviewers were good, too. I really thought they'd claw back some market share this year but it doesn't seem to be materializing.",Positive
AMD,How is this more reliable than mindfactory.de? Valve sucks Nvidia's dick. My favorite youtuber told me the 9070xt alone outsold the whole rtx 5000 line ups combined. This must be fake.,Negative
AMD,"Nvidia by far still sells more GPUs than AMD... And anyone who thinks otherwise is delusional.   But the steam hardware survey was, and still is, a severely flawed dataset. And anyone who believes otherwise is also just as delusional.",Negative
AMD,"I got the Steam Survey pop up on my AMD system (7900XTX) for the first time. Historically it only showed up on my Nvidia box (5080, 4080, etc.).  I wish it would just ask everyone.",Neutral
AMD,"I have had a 9070 since launch and I've even reinstalled windows once, yet I've never been prompted to do the hardware survey on my 9070 PC, but I have been prompted to do it on my old GTX 1070 PC that I only used for about a week over the summer.  I really don't think the RDNA 4 cards are that uncommon, I just think the hardware survey is weird, and likely that the few RDNA 4 PCs that do get prompted end up lumped into Radeon graphics instead of individual RDNA 4 GPUs.  More anecdotally, the 9070 XT and 9060 XT seem to be selling really quite well when I look at the stock levels at hardware vendors.",Neutral
AMD,Still never seen this survey in months of using a RX 9070,Neutral
AMD,">RDNA 4 Radeon GPUs are still missing from this survey showing that AMD’s newest generation hasn’t yet gained measurable adoption among Steam users.  It doesn't mean that. We, the public, **don't know how many machines were surveyed**.",Negative
AMD,"Note that Nvidia's Gaming is just Geforce (they listed things like switch 2 SoCs under OEMs) while with AMD console SoCs are also under Gaming, so Radeon is a much smaller part of that 1.1B.",Neutral
AMD,does gaming include CPUs too or just GPUs?,Neutral
AMD,"AMD is rumoured to do a 9070GRE refresh with 16gb vram. but other than that, nothing much",Neutral
AMD,"The 5070 Super is rumoured to have 6% more cores, as well as 18GB GDDR7 and possibly a small clock speed bump. It should be reasonably powerful, probably matching the 9070 in raster. If that is the case, the 5070 Super could be a no-brained.  But I would guess that AMD will refresh the 9070 if Nvidia refreshes the 5070. It won't get a VRAM bump, but I could see them bumping the power limit up to the same wattage/CU as the 9070XT, which would be 266W. The 9070 is currently so power limited that simply pushing more power is a completely viable option for a refresh.  266W would enable AMD to bump the clock speeds by 300-400 Mhz, which would keep the 9070 firmly in contention with the 5070 Super, while costing AMD and their board partners nothing, since 9070s usually have 9070 XT coolers specced for 304W or higher, I think the Sapphire pulse 9070 is the only exception.  AMD would simply have to push a new BIOS and/or unlock the power limiter in Adrenalin. They could potentially even push an update in Adrenalin that swaps the vBIOS.  I'd probably still pick the 5070 Super over the 266W 9070, but it still wouldn't be a complete no-brainer, since the 5070 Super would probably be worse in raster, but better in RT. If AMD Redstone launches before the 5070 Super and gives a nice bump in ray traced (and especially path traced) titles, I think the 9070 could remain competitive.",Neutral
AMD,closely? dont expect SUPERs till January.  >9070 has 16GB of VRAM and its a bit faster than 5070.  and also cost 30-50% more so....,Neutral
AMD,It says a lot about the current AMD lineup that their main selling factor is that their power connector isn't known for catching on fire - and becomes its biggest selling point.,Positive
AMD,"As for 50 series:  * RTX 5060 - **1.01% (+0.41%)** * RTX 5060 Laptop GPU - **0.69% (+0.27%)** * RTX 5060 Ti -  **0.74% (+0.16%**) * RTX 5070 -  **1.57% (+0.25%)** * RTX 5070 Ti -  **0.75% (+0.12%)** * RTX 5070 Ti Laptop GPU -  **0.18% (+0.18%)** * RTX 5080 - **0.74% (+0.09%)** * RTX 5090 -  **0.26% (+0.04%)**  RX 90 cards are still not present in meaningful enough numbers to show up in overall share (>0.15%)   However you can find:  * the 9070 at 0.20% and 9070 GRE at 0.02% if you sort by Windows (Vulkan) systems * the 9070 at 0.10% and 9070 GRE at 0.01% if you sort by Windows 10 (DX12) systems * the 9070 at 0.10% and 9070 GRE at 0.01% if you sort by Windows (DX11) systems * the 9070/GRE/XT at 1.05% if you sort by Linux systems, where Linux accounts for 2.64% of the steam ecosystem",Neutral
AMD,"Does anyone know where ""Nvidia Graphics Device"" comes from? Is it people that don't install drivers? Or are they workstation cards or something.   Also wasn't there an issue where 9000 series cards were showing up as generic ""AMD graphics""? That's still fairly high up on the list. Either way though, the fact they aren't even gaining more than 2080 is rough lol. Makes you realize how circlejerkey PC building subs are.",Negative
AMD,>9070 is just never going to be able to compete with the 5070 at 525-550 and then the 9070 xt at -50 of the 5070 ti is just never happening outside of the 5 linux users.  >I understand outside of the US prices are different but in the US they can't compete  [Newegg Best Selling GPU list disagrees. ](https://www.newegg.com/p/pl?N=100007709%204814&PageSize=36&Order=3)AMD is selling just fine for the DIY crowd.,Negative
AMD,I don't really care what anyone else is using beyond the potential for a monopoly to form if AMD leave the graphics market. AMD needs to offer better value and features for a wider audience to have any hope.,Neutral
AMD,"> It's wild to see that the hecking 750 TI has double the users of the 9070  ""It's wild that then-priced $149 GPU, that was popular for the last 11 years, is still being used today.""",Negative
AMD,Why would AMD users care what the rest of Steam is using? I never understood this sentiment lol.,Negative
AMD,"Too bad AMD sell more on DIY market that is what matter the most, this survey need more time, i never been prompted",Negative
AMD,"“Dont buy the 8gb 5060 its terrible. Also not the 5060ti 16gb because now you are too close to the 5070, go for it instead. Better yet, go for the 5070ti, 12gb is kinda sus these days.”  Yeah but you gotta remember people have limited money. 5070 is double the price of the 5060 in my country.",Negative
AMD,8gb 4060/5060 gpus are the bread and butter for prebuilts especially in stores like Best Buy or Costco,Positive
AMD,"Worst, for me it showed how deaf tone Youtubers are and propagated a very elitist mentality in which products said Youtubers didn't like were ""ewaste"" or ""waste of sand/silicone"" which the ""enthusiast' community started to parrot while all looking like imbeciles once sales reports come in.  The markets keep slapping these people in the face, and now Steve of GN is willing to risk his whole channel/livelihood to put NV it it's place and HUB is still waiting for AMD to outsell NV based on their sources and insider info, any day now!",Negative
AMD,"Not ""doesn't need more"" but can't afford the GPUs that have more vRAM. If you give those people a 4060 8 GB and a 4060 Ti 16 GB at $300 each, they will buy the latter. GPUs are expensive, much more in most of the world than in the US. Where I'm from, the cheapest low-end 4060 costs $466 and the average salary is $333",Negative
AMD,"Well, yeah. Most people buy a pre-built with a 4060 and 90% of their decision is price. They may or may not notice they're stuttering like a MFer during gameplay, but it's what they could afford.",Neutral
AMD,"The fact that people aren't buying/can't afford more than 8GB GPUs doesn't change the fact that many games are starting to need more than 8GB, some even at 1080p.  Reviewers wouldn't be doing their job if they didn't inform you that 8GB cards are quickly becoming obsolete. There are still countless older and simpler new games to play, sure, but some new games will have unbearable performance issues.",Negative
AMD,"It turns out that for most people, testing non-high end cards at ""ultra"" presets is not a realistic usage case and people just use the auto-quality settings or turn he quality a step down when the game runs too slow.",Negative
AMD,Or people have no choice....,Negative
AMD,"It probably also means that people, no matter how enthusiastic, can’t justify the price of GPUs more expensive than the 4060 series.",Negative
AMD,4060 has been the top GPU for awhile now. 3060 was the top GPU before it and the 5060 will be the top GPU after it.,Positive
AMD,"It's not just about needing Vram. It's about money as well. GPUs are expensive. Nvidia also knows this, that's why they don't future proof their current GPUs by providing enough vram for future proofing.",Negative
AMD,"It always was like this: ""I'd rather buy $300 GPU or PS5, even PS5 Pro instead of $600+ GPU only to see it dying in 2-3 years and struggling with crappy PC ports at upscaled 1080p"" - average joe gamer.",Negative
AMD,"A card being popular doesn't mean it's good. People who are informed don't like it because it's junk, people who are uninformed buy prebuilts that come with junk.",Negative
AMD,"It's different, they just don't have a clue and buy whatever is popular  Like this headset   https://www.amazon.it/Cuffie-Microfono-Cancellazione-Surround-Comfort/dp/B09DPR2LZW/ref=asc_df_B09DPR2LZW?mcid=1179ab65abc0389386bea28b066c86d6&tag=googshopit-21&linkCode=df0&hvadid=700790378073&hvpos=&hvnetw=g&hvrand=752363944309456689&hvpone=&hvptwo=&hvqmt=&hvdev=m&hvdvcmdl=&hvlocint=&hvlocphy=9209372&hvtargid=pla-1426585270475&psc=1&hvocijid=752363944309456689-B09DPR2LZW-&hvexpln=0  Is one of the most popular, and is plain sh** same with 8gb gpu  Popularity=/Quality",Neutral
AMD,"Its missing from the main one because its percentage is too low, same reason its been missing before.  The last thing in the general list is the UHD 600. The 9070 is 12 places below that in the Vulkan (technically its most popular) list.  Its not same grand attempt at hiding them, they literally just aren't popular. The assessment in OP isn't far enough off to really argue.",Negative
AMD,"RX 90 cards are still not present in meaningful enough numbers to show up in overall share (>0.15%)   However you can find:  * the 9070 at 0.20% and 9070 GRE at 0.02% if you sort by WIndows (Vulkan) systems * the 9070 at 0.10% and 9070 GRE at 0.01% if you sort by WIndows 10 (DX12) systems * the 9070 at 0.10% and 9070 GRE at 0.01% if you sort by WIndows (DX11) systems * the 9070/GRE/XT at 1.05% if you sort by Linux systems, where Linux accounts for 2.64% of the steam ecosystem  They simply dont sell enough of them",Neutral
AMD,Where's the 9070 XT tho? Did the 9070 and 9070 GRE outsold it?,Neutral
AMD,Where is the 9060XT though. It is the more bang for buck and should be the popular card.,Neutral
AMD,For months my 9070 XT showed up in the survey as AMD graphics. I'm not so sure the survey is the best indicator of market share,Negative
AMD,4060 Ti and 5060 Ti and higher are not going to glaze and promote itself.,Neutral
AMD,Brand recognition of 60 series 2 strong no matter how good or bad the card is.,Positive
AMD,"Wow regular people are ignorant, what a surprise...",Negative
AMD,The guys were mia after the launch week while nvidia was available everywhere after a short period and often at msrp,Neutral
AMD,"That's just a reddit narrative, nobody buys AMD, even at 599 it still wouldn't be on the list.",Negative
AMD,"I don't think this is the first time AMD has done their fake MSRP, RDNA 2 was notorious for this as well, the only difference is there were actual chip shortages back then, this time It's just AMD refusing to sell their 9070 XT at their promised $600 MSRP, it was originally meant to be a $700 product. And they currently have no reason to sell them for lower price when people are buying them at the current inflated price.  I guess we'll see if they can keep being arrogant enough like this, when the 5070 Ti Super 24GB for allegedly the same price as current 5070 Ti MSRP comes out though.   Same can be said with 9070 Non XT which is more expensive than the 5070 Non Ti, and that makes the 5070 Non Ti as a much better deal, hence it is now the most popular RTX 50 series GPU in the world as of the moment and the 9070 non xt is nearly non existent.",Negative
AMD,"I wonder if there is an issue detecting video cards in VMs. Like it doesn't pick up the GPU that is being passed through, so they are just all thrown in that category as it might be reported as not supporting any DirectX.   Although, that seems a high percentage of for it to be VMs. But maybe I am underestimating the popularity of stuff like NVIDIA Now.",Negative
AMD,I haven't upgraded my computer in like ten years. I can see other people who like and play older games not upgrading as well.,Negative
AMD,People outside of reddits bubble who don't enjoy a first world income.,Negative
AMD,"People plugging their monitor into the mainboard, or simply readout errors on Ryzen 7000/9000 systems also fall into that. And all the people with just a 5700G or something.",Negative
AMD,"It does seem like a good card.  Maybe just not enough of an upgrade for many though? the price is definitely good and it's a great choice for people holding out on Pascal cards etc. It is maybe a little closely priced compared to the 5060Ti but still, it's a £50-70 saving as far as I can tell for similar-ish gaming performance.",Positive
AMD,But my favourite tech-tuber says otherwise. /s,Neutral
AMD,Same never seen the survey with a 9070,Neutral
AMD,Nvida also has the whole laptop market to itself. AMD though has itself to blame for it.,Neutral
AMD,Yeap doesn't include the same gaming chips used for different purposes like their workstation GPUs either.,Neutral
AMD,"They surely include gforce now as well, no?",Neutral
AMD,A lot of Nvidia's gaming GPUs end up in server farms for AI.,Neutral
AMD,"NVIDIA sell their GPU not only to gamers even if they are targeted as gaming. So we can say """"""gaming""""""",Neutral
AMD,just GPU,Neutral
AMD,"That won't compete with a 5070 Super considering the 9070 barely can as is, the GRE is more like a 5060ti 16gb killer.",Negative
AMD,">The 5070 Super is rumoured to have 6% more cores,  6%? Who rumors 6%?  5070>Full GB205 is 4.166% increse, why round it to 6 and not 5? Or just state that it's probably gonna be full GB205 most likely. Yea I know it's pedantic, but if some1:s gonna make a rumor, i'd expect them to at least check what the potential die would be and not just pull random number.",Neutral
AMD,"Which is like 5 months away, so not that far away. Is 9070 really cost 30-50% more than 5070? I'm not talking XT, the plain regular 9070.",Neutral
AMD,If MSRP is 50% more on a 5070 no one will buy it.,Negative
AMD,"Where? I Bought my 9070xt for the price of a 5070, and 5070xt is 40% more expensive.  Never mind the dogshit Linux support.",Negative
AMD,once again we have a stuation where the worst selling Nvidia card sells more than entire AMD generation...,Negative
AMD,"Nvidia Graphics Device could be from virtualized enviroments or something with modded firmware.  No, there has never been such an issue with the 9000 series. If you check Linux-only stats you can see 9070 and 9070 XT together as a whopping 1,05%",Neutral
AMD,Dude the second highest card is a 3060 at 300 clearly not accurate   their best selling just looks like the stock they are trying to get rid of lol,Negative
AMD,Don't get me wrong. I'd love to see real competition between AMD and Nvidia as it is the only way for the consumer to gain an advantage. However I find the brigadism in favor of AMD here on reddit stupid and even counterproductive for that end.,Neutral
AMD,"we basically have a monopoly already and have had it for a while, since AMD just phones it in and matches Nvidia price to performance",Neutral
AMD,"Being impressive is not that 750 TI is still being used, rather than the so-called value king GPU that everyone here praised so much and that everyone said that it sold so well has 1/8 of the market share of the certified e-waste 5070 and is even well below a 11 years old hardware.",Neutral
AMD,>  i never been prompted  Welcome to random sampling.,Neutral
AMD,"The DIY market is a fraction compared to the prebuild and laptop market.  Given both companies' revenues from gaming, it could not be clearer that the DIY market is much less important.",Neutral
AMD,">Too bad AMD sell more on DIY market  Sure buddy, care to share a source that is not just the sales report of a single shop for a single month?  >that is what matter the most  According to who?  >this survey need more time, i never been prompted  That is not how a survey works lol",Negative
AMD,8GB XX60 class GPUs are perfectly fine for 1080p gaming as long as you have a PCIe 4.0 motherboard.,Positive
AMD,"Honestly tired of techtubers saying 8GB video cards are useless, as well as testing top-of-the-line CPUs at only 1080p, which you would never use in real gaming with a part like that, and then gaslighting everyone into saying their testing method is the best.  The fact is that at 4K all of the current CPUs hardly matter and that even a lowly Ryzen 5600X can get [within 10% of a 9800X3D.](https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/20.html) (1% lows are [here](https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/21.html) and are also within 10%.) But that doesn't make for a good content video. You need to go all the way back to Intel 8th Gen/Ryzen 2000, both launched in late 2017/early 2018, for a CPU to be a significant bottleneck at 4K.",Negative
AMD,"IMHO, from personal experience with other people, stuff like that and the RDNA 4 MSRP debacle has inadvertently given Nvidia and the benchmark site-that-shall-not-be-named PR victories and a credibility/mindshare boost amongst the lay public. I've met people under the impression that Nvidia is the ***lesser evil*** right now because they at least are currently providing MSRP cards with superior featuresets and upscaling advances, while AMD fabricated a day one MSRP to hoodwink the public only for 9070 XT prices to remain inflated at a lot of regions today.  We can bemoan this public perception all we want--both Nvidia and the site-that-shall-not-be-named have scummy practices and advertising, after all--and the plural of anecdotes is not evidence, but there's no way around the fact that the 9070 XT pricing fiasco was a terrible look for AMD and arguably also for hardware enthusiast communities to a lesser extent.",Neutral
AMD,">HUB is still waiting for AMD to outsell NV based on their sources and insider info, any day now!  HUB never claimed this ever lol.  Wtf happened to this sub? The most popular tool at Home Depot is probably some brushed drill, but nobody claims a $100 brushed drill is a good and long lasting product. A card selling well doesn't make it good, it makes it popular.",Negative
AMD,"It can easily be doesnt need more. If the person only plays competetive multiplayer games (think LOL, Fortnite) he will NEVER use more than 8 GB of VRAM. and there are millions of people that play ONLY these games.",Neutral
AMD,"If you are playing at 1080p, you will not have much stuttering with a 4060.",Neutral
AMD,"A prebuilt with a 4060 is also extremely unlikely to be shipping with a PCIe 3 mobo, so stutters are non-existent, unless you're trying to run stupid settings like RT on that class of GPU.",Negative
AMD,I havent seen a game dip below 10-11gb memory used in ages.  UE5 games are memory hogs.,Neutral
AMD,"Do you think tech reviews aren’t aware of that?  If you see a review and a card gets 40 fps at ultra, then you can assume it’ll get over 60 at med/high. If you’re going to standardise one quality setting to use for all games, then everything set to ultra is the most sensible option.",Neutral
AMD,Likely because both of those are cheaper than the 9070XT (especially with the fake MSRP)  But steam also groups them together for linux,Neutral
AMD,"It funny how people ignore this completely because it suits their views.  According to Steam Hardware survey AMD sold zero 9070XTs. But there are still plenty of people claiming this survey is perfect, even though we already had proof with RX7000 that it is not.",Negative
AMD,That tier of card is a 3070Ti with a VRAM and upscaling DLC baked in. People should be demanding way better even in the budget sector.,Negative
AMD,It also came out later,Neutral
AMD,The most bang for the buck card this gen is definitely the RTX 5070 and the 5070ti to some extent. That's why these cards are outselling everything else this generation.,Positive
AMD,I doubt that. That card is as fast as a 3070 TI (5 year old card) how many people that can afford a $370 GPU are still on a system where that card would be a meaningful upgrade. Probably not many.  inb4 someone reply me about the 5060 and the 5060ti.  Nvidia numbers make sense when you realize 5060 numbers come mainly from prebuilts and the 5060 TI 16GB is the cheapest way to access 16GB on a cuda enviroment for productivity .,Neutral
AMD,Same here,Neutral
AMD,This is true.   RX 480 was better & cheaper than GTX 1060.,Positive
AMD,"Ah the ""Everyone except me is stupid"" argument lol.",Negative
AMD,"> That's just a reddit narrative  It's why there's nothing more worthless and reality distorting than reddit celebrating videocardz & Co. publishing the quarterly mindfactory.de sales numbers.  * a) Germany is a uniquely strong DIY PC Building market. Probably top3 in the world * b) Germany is a uniquely strong AMD market. * c) mindfactory in particular is AMD's preferred European vendor and was the exclusive distributor of the Ryzen 7600X3D on the continent. Their most attractively priced offers are almost always AMD cards  All of these facts combined distort the picture in favor of AMD. Even during the RX 7000 generation before AMD caught up in terms of features like FSR4, mindfactory had quarters where they sold an even split of AMD/nvidia GPUs.",Neutral
AMD,"They do, AMD has had a record quarter. Its just not as people portray them or nvidia for that matter.   For example, I bet next year we will see hate on forums from people believing as MLID claims for the 4th time that Nvidia is cutting supply due to lack of demand. Just you wait",Negative
AMD,The only way this can ever change is AMD investing billions of dollars into forcing their cards into dell/lenovo/hp prebuilts particularly laptops.,Negative
AMD,no one is buying them for 700 in the US. Only in other countries where the 5070 ti is like 900+.,Negative
AMD,"5070 Super 18GB at 549$ will be far bigger killer than 5070Ti Super. That thing will be ""fairly"" cheap for the VRAM and power it has",Neutral
AMD,"Hey KingStatus2627, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Negative
AMD,That has to be the answer. The survey probes for the latest supported version and anything other than dx9+ goes into the bottom bin even if its null or invalid data.,Neutral
AMD,DX11 came out 16 years ago   “Ten years ago” was 4 years after the first DX12 GPU  DX8 was released ***twenty-five*** years ago. A machine running a video output adapter (yes I will call it that here) has no business being connected to the internet whatsoever.,Neutral
AMD,DX9 is over 20 years old. The last GPUs that dont support it came out in 2002 and ran on AGP BUS. Hard to imagine modern steam running on anything that old.,Negative
AMD,"mate 8.1 shipped in '01, intel integrated got dx9 support in '04, theres first world income and then theres anything made within 20 years, far more likely its just VMs",Neutral
AMD,"Unlikely, you need to opt into the survey. I doubt people do that when they’re debugging some issue.",Neutral
AMD,If you write that I'm wrong provide an argument why. Now I can't respond explaining to you why you are the wrong one.,Negative
AMD,They are so fucking convinced that the RTX 5000 series is a big flop and Radeon was outselling (lmao) just because 9060XT is lableed as best selling on one or two pc part selling sites.  Truth is a lot of people don't trust AMD GPUs because of their repeated historical driver issues and below par driver team. Hopefully it is better now.,Negative
AMD,Yeah I upgraded from a 3060 and it wasn't the world's biggest bump but it was significant at 1080p and I'm gonna be upgrading to 1440 soon which I would not have had the confidence to do with my 3060. I feel like this card could have been this market's big midrange card and then... nobody bought it lol,Neutral
AMD,Sure they were being sarcastic.,Neutral
AMD,"AMD is killing it for mobile CPUs, but that is still a hard sell to the uninformed.",Neutral
AMD,> AMD though has itself to blame for it.  And it's certainly not Nvidia forcing OEMs to exclude Radeon GPUs from their lineups.    Good Guy Nvidia would **never** do something like that.,Negative
AMD,"If this was happening in sufficient numbers to make a substantial difference, then why does the Steam HW survey split paint an *even worse* picture for AMD than the gaming revenue split?",Negative
AMD,"5090s sure.   But 5060/ti, 5070 and 5070ti??",Neutral
AMD,That's because Nvidia GPU are usable and preferable in that situation.,Positive
AMD,"The idiots who are downvoting this probably know zip about AI, but you're right. NVidia is making huge amounts of money from 5090s being used for AI. US sanctions mean much of this trade is being done under the table (see the GN video when it comes back up)  If you're running AI at any kind of scale 5090s are actually excellent value compared to NV's datacentre cards, provided your model can run in 32GB or can scale across multiple cards.  16GB 5060Tis are also popular as a cheap way of getting some AI compute with a reasonable amount of Vram for cheap. There's also a thriving market in taking low VRam cards and upgrading them - I've even seen old 2080Tis upgraded from 11GB to 22GB on sale from Chinese suppliers, and also 4090s with 48GB. Would not be surprised if this is being done with 5070s and 5080s too.",Neutral
AMD,"Knowing AMD too, they will overprice the GRE on release, put it in China only and then months down the line cave on the price and release it to the wider market too late to make a dent in NVIDIA's sales. AMD truly is their own worst enemy.",Negative
AMD,I think I got some numbrs mixed up. I was drawing from my memory after doing osme napkin math from reading the spec sheet for the supposed 5070 Super. It might have been the core count + a small rumoured speed bump combined should yield 6%.,Neutral
AMD,"i wouldnt call ""Slightly less than half the time since last launch"" as closely approaching.  The prices vary here. Let me go check today. 5070: 571 euro, 9070: 666 euro (note prices include 21% tax).  Difference today: 17%.",Neutral
AMD,The 9070 is the more expensive one.,Neutral
AMD,"Eastern europe. Altrough todays price difference seems lower, 17% more only. 5070 non-ti vs 9070 non-xt.  Linux support is better on AMD cards, but i rarely use linux and so does over 97% of steam users according to this survey.  Also you typed 5070xt, probably a typo.",Neutral
AMD,I don’t even think AMD even made as many cards in total for a generation than any one given sku of an nvidia generation.,Neutral
AMD,"I presume the Best Selling GPU list from amazon that has the 3060 ranked #2 is also fake, with +3K sales in the last month? [https://www.amazon.com/gp/bestsellers/pc/284822/](https://www.amazon.com/gp/bestsellers/pc/284822/)  It's not like the vast majority of DIY market is avoiding the 8GB like the plague or something and finding other options in that price range. Or 3060 gained a big jump in market share on Steam last month. No, no, no, everything is a conspiracy and everything is fake. /s",Negative
AMD,"Agreed.  I've seen several cases of people complaining about not being able to buy a 9070 XT for the initial advertised $599, and without fail, some people try to run defense for AMD by yelling how Nvidia is also bad and showing 9070 XT's available at Micro Center for...$699.  I have no idea how that's supposed to win hearts and minds. Not everyone has easy access to a Micro Center, and a $699 9070 XT is still *$100 above the original MSRP*. Worse still, I know my Micro Center at least has plenty of MSRP RTX 50-series cards; this cuts down on RDNA 4's value proposition and makes people justifying the inflated RDNA 4 price look like a bunch of bizarre and out-of-touch hypocrites.",Negative
AMD,Yeah the onus is on AMD to be competitive.  Consumers will not do charity. Offer the best value in the overall package. Nvidia -$50 with an inferior feature set will not work. AMD needs to stop pretend they are even close to Nvidia's value.,Negative
AMD,"I don't think you understand.    The 750 Ti was a super-budget ($100-150) card that was also on-par with the PS4 for performance, meaning it was relevant from 2014-2020.    RX 9060 XT is a $350+ GPU that released 3 months ago.",Neutral
AMD,"Imagine comparing a budget card from 10 years ago with a high one from a few months ago, but do you think about it when you write?",Neutral
AMD,"From a sales perspective, certainly, but you can't ignore what's happening in the DIY market, where people are more informed and make informed choices. These are usually people who carefully consider their purchases and don't just buy what's available. If you sleep too long, ignoring the DIY market, you'll experience a bit like what happened to Intel, where it's slowly being displaced from its dominant position despite having in the past almost total dominance. Generations of good products change things, even if only in the DIY market.",Neutral
AMD,Why are you downvoted. This is absolutely true,Neutral
AMD,This only makes sense if your game has perfect frame time health (no stutters) and is of course not CPU limited at any time. So basically eSports games and indie games.,Neutral
AMD,"The reason they test the differences at 1080p is because they want to compare how cpus fare against each other, if you go to 4k max settings you will be bottleneck by gpu, not the cpu  If you know you are being bottlenecked by your gpu, why upgrade cpu then?",Neutral
AMD,the worst is that they always test graphics intensive games. There's never any sim games or heavy duty multiplayer games that actually strain a cpu,Negative
AMD,"this was actually one of the most interesting things to me. I saw a lot of people hyping up amd cpus, especially the x3ds, and speaking of older cpus as if they were literally obsolete but whenever i saw actual videos of performance or sites that showed what their performance differences would be they were always kind of minor relative to the price differences. a lot of the claims are also just wrong. like i'd see someone say ""this amd cpu is waaaaaaaaaaaay faster than this intel cpu, get this one,"" and i look at performances to see that the two cpus are basically equivalents as far as gaming goes.",Positive
AMD,My favorite is when they put different CPUs up against each other and say one is slow and the games have 100+ FPS. Testing Games on youtube does more realistic reviews by using hardware in the same way the average gamer will by jacking up the settings as high as they can go.,Positive
AMD,"> HUB never claimed this ever lol.  There is a massive hate boner for HUB in this thread in particular.  I recently rewatched some of it and it's clear that HUB was referring to retailers serving the DIY market in Australia in the first launch week. Nothing else.  Looking at mindfactory data, we know that AMD did really well in their launch for the 9070/XT cards.  So any data we have shows that HUB wasn't talking shit.  What most of the ""HUB IS WRONG"" crowd don't get: Steam Surveys actually show perfectly well how tiny the DIY segment is.  It simply doesn't matter at all how DIY does, prebuilts will outsell DIY by two orders of magnitude. Shocking that HUB don't have insight sources inside Dell, HP and Lenovo. How dare they not have that.  So congrats AMD, you had a great launch in 1% of the market. But that won't mean shit for the overall picture.   Especially in a segment none of these companies actually care about because it's neither data center nor AI.",Negative
AMD,DLSS was a huge improvement for lower tier gpus too.,Positive
AMD,"Sometimes I think reddit/youtuber commenters can't fathom that there's a massive demographic of gamers who only play those kinds of games and maybe a few other games that may or may not be graphically heavy. I've given up on getting deep into that discourse because some people are stuck on how they think the world should be and not how the world is, i.e. people think 8gb cards shouldn't exist and get pissed at amd/nvidia for selling them and pissed at people for buying them because it's enough for them or they can't afford more.",Negative
AMD,"You dont need more than 8g to play any modern game too. We cant say about the future, but for now this competitive games talk is unnecessary when any game is playable.",Neutral
AMD,?  Insufficient VRAM is a major cause of stutter.,Negative
AMD,> I havent seen a game dip below 10-11gb memory used in ages.  Allocation is not usage.,Neutral
AMD,"They lower RT in testing to medium because its ""unusable"" or ""will just favor Nvidia GPUs anyways"", but if the VRAM can't handle ultra settings, its ""planned obsolescense"" and ""killing PC gaming"". The quotes I put are real quotes and video titles",Negative
AMD,"These same reviewers are perfectly capable of dialing it down to medium when testing igpus, but somehow that's too hard for discrete cards.",Negative
AMD,It isn't perfect but it's better sampling than techtubers asking a single retailer how many GPUs they sold over a launch period. This also largely tracks alongside AMD and Nvidia's gaming financials in their statements.,Neutral
AMD,I got the hardware survey yesterday so my 9070XT will be recorded in the september one at least.,Neutral
AMD,I am talking about AMD cards here. Come on you can see the context. Also I disagree but whatever.,Neutral
AMD,> how many people that can afford a $370 GPU  This is not a lot of money. Wtf,Negative
AMD,1060 6gb was ridiculously good uplift though. Maybe 1060 3gb was the debait of the day?,Positive
AMD,Popularity=/Quality   4060 8GB literally the McDonald of GPU matket,Neutral
AMD,"I don't have a horse in this race, I buy whatever I can afford/looks nice at the time.  Ended up with an all-AMD system.   Still wouldn't trust Mindfactory numbers for anything beyond their own store.",Neutral
AMD,"Or.. or they could try being better than Nvidia **overall** and not this ""were better at r but Nvidia is better at xyz"".   Good luck.",Positive
AMD,"Yeah it's baffling that they've totally ignored the mobile market, or rather they are ignoring gaming laptops specifically which are always big sellers.  They've got a pretty great, efficient chip that could surely be cut down and segmented for mobile and offer some obvious tangible benefits (12GB+ VRAM at a lower cost) that would make it very attractive in a laptop but nope... apparently not interested.",Negative
AMD,If it launches at that price it'll be Nvidia's next 1080Ti I think. More than fast enough for many for years to come and no concerns about VRAM becoming an issue for about the same amount of time.,Positive
AMD,Is that price confirmed or just wishful thinking?,Neutral
AMD,I got a windows 98 as well. It doesn't have steam but it does connect to the internet.,Neutral
AMD,"I don't use this computer for gaming, but i still have a  windows xp that still runs and all the hardware in it was made before 2002 and steam does run on it still. It's actually great for playing older games.",Positive
AMD,"They are still very popular in net cafes in China and south east Asia, you really do have a first world perspective on this issue.",Neutral
AMD,I'm opted into the survey and my 9070xt came back as integrated. And no it's obviously not inactive lol  There's thousands of people reporting the same about their 9070s being submitted as generic amd GPU as well.,Positive
AMD,"More like AMD is killing mobile APUs by overpricing things like Strix Halo. The fact is you can buy more performance for less than a Strix Halo laptop by getting an Intel CPU and an NVIDIA dGPU laptop, pocket $500 and get better performance in games.",Negative
AMD,"Source it if you want to make that claim, sounds explosive.  The evidence we do have meanwhile says that AMD are just dogshit to work with as far being able to supply chips and have not forged relationships with the ODM's that matter in laptops like Compal, Wistron, Quanta etc. Intel and Nvidia meanwhile have and the effect is plain to see.  Of course AMD haven't even bothered to have a mobile dGPU this time around so it's not like anyone even could work with them on current gen gaming laptops etc.",Negative
AMD,"5090 contributes more to the revenues (since it's more expensive) which is what the argument is about, revenues.  They don't need to sell anything lower tier to the datacenters and the revenues could still be dominated by those sales. Also 5080 is I think most you can get in China through legal channels.  Just search online you can even rent 5070s for AI.",Neutral
AMD,"This has nothing to do with Steam survey, which we are discussing and which closely corroborates the revenue streams as well. Its a factor but nowhere near the factor",Negative
AMD,"eh, I'd definitely call 5 months close since thats within the timeframe that most people can wait before upgrading. (I for one have the money for a gpu right now, but am waiting for the drop because to me it is close) Even if it isnt ""close"" yet it's close enough that its not worth nitpicking",Positive
AMD,I'd expect the 5070to come in at 550-600,Neutral
AMD,and i remember when ATI/Radeon used to be 40% of the market...,Neutral
AMD,"Someone's on the defensive, huh?  I can do the comparison of 5070 and 9070 if it can make you feel better with yourself, but it will still be bad, as for every 9070 there are 15 5070.",Negative
AMD,Goes against the narrative.,Neutral
AMD,"Because they have been called ""planned obsolescence""",Neutral
AMD,"It's not. PCI-e 4.0 doesn't change the fact that many games will emergency evict textures and you'll get early 2000 texture resolution. It also doesn't eliminate VRAM related stutter, just mitigates some of it some of the time.  Unless you know for a fact that the only games you're going to play actually require less than 8GB (e.g. that one competitive game you play, 2D indie titles, etc.) those cards are just bad value.",Negative
AMD,"It's a typical case of what you use them for.  Esports games, games like Tarkov, MMOs like WoW or ARPGs like PoE2? X3D chips have absurd gains. like 15-50% when compared to their counterparts. It's completely insane. They can literally mask terrible optimization really well.  AAA games and overall most singleplayer games? Mostly GPU bound and/or just way less taxing on the CPUs. So there you barely see any gains or 14900k taking the lead, simply do to the single core IPC wins Raptor Cove has vs Zen4/5.  So really, it depends on the games you play.",Neutral
AMD,> Testing Games on youtube  That's also straight up a fake info channel,Negative
AMD,"Yep, and DLSS4 is now good enough to be used even in twitch shooters without artifacts causing gameplay issues.",Positive
AMD,"Well, in a few select games, if you use RT and no upscaling, you do. But that means someone buying the card will need to play those specific games with those specific settings to even notice the issue.",Neutral
AMD,Look at any general GPU review of a 4060 8GB - for e.g. Techpowerup.  Minimum FPS averages to over 70 for the 4060 in the games tested at 1080p.  The idea that 8 GB is insufficient for budget gamers is a notion that is out of touch with reality.,Neutral
AMD,"Yeah because if you do ultra settings on an igpu with modern games it’ll just crash out (due to not having enough vram funnily enough), or it’ll be so slow you’ll be measuring seconds per frame nor frames per second. At that point it’s a useless test.  It’s not like it’s impossible to test at multiple settings, but 1) reviewers have a limited amount of time to test cards, and the time they have is only getting shorter with each review cycle, 2) medium settings aren’t standardised. Leaving out naming oddities like GTA V’s high being the medium setting, what each game decides to reduce when lowering settings down to the medium preset isn’t standard, one game might disable grass textures entirely while another will barely reduce them, and so on. If you want all games tested to be on a level playing field, it makes sense to crank all the settings to max. And 3) the whole point of a test is to push the GPUs to the max. If you drop the settings down to medium, with the high end modern cards you’re going to be limited by the CPU. If you do all your comparisons at 1080p low, then you’ll conclude that a 5080 has about the same performance as a 5090, even if that is definitely not the case.",Negative
AMD,"It's not youtubers problem when redditors conflate launch day numbers from few retailers with overall sales since then, which basically every single person complaining about youtubers does. The assertion then that this makes youtubers wrong is simply disingenuous when it very likely that their exact claim was correct.  Also, I never said Steam survey is bad, it still is most accurate data about sales we have, but there are plenty of people who claim it's perfect.  My previous post is already at -2, yet downvoters can't provide a single reasonable reason for 9070XT to be entirely missing from steam survey.",Negative
AMD,"No, you participated in August survey. On 1st day of every month you can get prompt to participate then day later results are published.",Neutral
AMD,1060 was a good uplift purely because 960 was a poor uplift over 760 + finally being able to move from 28nm -> 16nm process.,Positive
AMD,"So you're saying the cards like the 4070, 4070ti, 4090 are all trash cards because they're popular?",Negative
AMD,"Maybe in an alternative reality  AMD is an enthusiast brand that has no mainstream penetration but also doesn’t have a high end. Their target audience is a subset of the enthusiast crowd that trends young with limited income, while also simultaneously not having an actual low end either.   I’m sure you can tell how well that goes.",Neutral
AMD,"Im sure their interested but why would they spend the time, money, and effort when they dont have enough wafers to go around. So they stay relevant in GPU, lead in CPU, and compete in data centers where the real money is at. That way they can afford more R+D.",Neutral
AMD,Super themselves are not confirmed. All this are rumors and most of the pricing is drawn from a conclusion that 40 supers kept their MSRP as the original 40 series (in case of 4070S and 4070TiS),Neutral
AMD,I would really hope you don’t run something that you put in your credit card info and do online transaction on it.,Negative
AMD,"those are most often equipped with 60 class cards, 10/30/40, you aren't going to be playing any even close to popular game made within the last 15 years on a dx8 product",Negative
AMD,Take a screenshot and send GabeN an email,Neutral
AMD,"> Source it if you want to make that claim, sounds explosive.  No one ever sourced Intel suppressing AMD's CPU sales to OEMs until the EU commission took Intel to court, on AMD's evidence.",Negative
AMD,"5090 contributes more to the revenues (since it's more expensive) which is what the argument is about, revenues.  Dude you know how many 60 cards they sell compare to 90 cards? There is a reason one is called mainstream and another is called enthusiast",Neutral
AMD,That seems a whole lifetime ago now,Negative
AMD,"Someone defensive and someone just spreading bullshit, anyway show me DIY market where 5070 do better",Negative
AMD,"I guess the problem lies within the fact that 1080p is still considered a thing for gaming although it was the standard even in 2010, decent 1440p monitors cost less than $199 and once you upgrade to that, you immediately realize that 1080p should only be reserved for laptop, tablet and smartphone displays.",Negative
AMD,"Not really, my old 8gb card was already hitting pretty high vram usage in 1080p in more recent titles. An 8gb cars now really isn't going to last too long unless you are willing to stick to older titles and skip newer ones.",Neutral
AMD,No 8gb can still play 2025 triple a games not in 2000 texture resolution or settings. Anyone who says otherwise doesn't play games,Negative
AMD,Really? Do you have any evidence?,Neutral
AMD,Do you actually play twitch shooters with framegen or are you just making things up lol,Neutral
AMD,You can have decent FPS and still be stuttering like crazy.,Negative
AMD,"Redditors only believe in ultra settings so they think 8gbs are stuttering all the time. Thats how far out of touch they are. They probably dont even know settings exists  ""If you're going to lower settings, why dont you just buy 2nd hand? lower settings is unplayable, holding industry back not the series s or games are console first, etc etc""",Negative
AMD,I think you made a typo,Negative
AMD,Well Mr tech Jesus tests 70 series GPUs at 1440p medium for RT and ultra anywhere else for reasons he outlines in his videos,Neutral
AMD,"Sure, but I'm just pointing out that their constant whining about ram size has a simple fix that they seldom seem to mention in those rants.",Negative
AMD,"The reasoning is probably because it just didn't sell as well as people think. There's always the classic iGPU excuse, but given how popular AMD CPUs are if it was truly a widespread issue we'd probably see AMD Radeon as the most popular singular block on the survey, and we don't.  The reality is that Nvidia dominates the prebuilt market and the VRAM debacles just aren't as important as people pretend it is. Likewise all those little features that people claim don't matter with Nvidia GPUs end up mattering to some degree. AMD spent years without a good DLSS competitor. We still don't have the AMD equivalent of DLDSR and RTX VSR/HDR etc.",Negative
AMD,"Oh, didn't realise that, weird its conducted on the first of the month and named for the previous month.",Neutral
AMD,"And who was talking about the 4090 and 4070? Anyway, when the 4070 Super came out, it was an excellent card, perhaps the best on the market. If you buy a 12GB 5070 today, you're making a bad purchase... because it will become obsolete before a 9070, which is generally a bit faster and, more importantly, has more memory at a completely comparable price.  McDonald's GPUs are cards with 8GB of memory these days; the 5070, while not a card I'd recommend, isn't at that level.",Positive
AMD,"It's been the same excuse since Zen 2 era and they're on track for well over 30 billion revenue for 2025 and they still can't multi task.  Intel won't even double AMD's revenue this year (it was 10x that of AMD ten years ago) and they have a whole deadweight of a fab to power and paying for even more advanced TSMC nodes. Despite that fact they still somehow continue to be able to ""bribe OEMs to not use AMD"" (with negative cash flow three years in a row) and AMD can't afford to according to reddit.  AMD ten years ago at a fraction of the market cap and 1/6 the yearly revenue did better in getting their graphic cards in system integrators. AMD either doesn't want to, or is extremely incompetent with OEM management, no two ways about it.",Negative
AMD,> when they dont have enough wafers to go around  Honestly I don't know about that. 9070/XT is always in stock - just expensive - but clearly there isn't the demand for it.,Negative
AMD,"Your ignorance is showing again. That's what they use, go look for yourself. When playable means 30fps at 720p there is in fact a large amount of modern titles that you can play. [You might be surprised.](https://www.youtube.com/watch?v=-iuFlmyYzq4)",Negative
AMD,Then maybe AMD should also provide some evidence to some court about nvidia doing the same.,Neutral
AMD,They would need to sell 7 times as much to generate the same revenues.  So if say Nvidia sold:  - 1000 5060 to gamers - 100 5090 to gamers - and 300 5090s to AI datacenters  Datacenter would still be the bigger portion of that revenue.  I should have known better though. This sub has no common sense.,Neutral
AMD,"well, for average age on this subreddit it probably was.",Neutral
AMD,"I have all the proof I need right on the Steam survey, if you are in denial it is not my problem",Neutral
AMD,Why is the existence of a common resolution that serves as a low barrier to entry for PC gaming a 'problem' in the first place?,Negative
AMD,"I disagree, 1080p is completely normal resolution, enthusiast clearly lost the plot thinking that it's unusable.",Negative
AMD,"hey, be lucky we arent regressing like we did in 00s, when resolutions got smaller.",Neutral
AMD,"i dont know why 1080p is considered a problem. It is the best balance for 24 inch  and smaller monitors/displays.   and higher refresh rates are more viable at 1080p, as well as multi monitor setups.",Positive
AMD,">decent 1440p monitors cost less than $199 and once you upgrade to that, you immediately realize that 1080p should only be reserved for laptop, tablet and smartphone displays.  Why? As someone who recently upgraded from 1080p to 1440p i didnt feel that big of a difference. Sure having a bigger screen is nice but 1080p was perfectly usable and needed less hardware to get good framerates on, for budget gamers 1080p is still a good option imo",Neutral
AMD,And then there's me. Bought a 1440p monitor last years after been on 1080p for 12 years. I have been playing dark souls 3 for the past month and only realized 2/3ths into the game that I had the resolution on 1080p instead of 1440p.,Neutral
AMD,"40yo gamer here, my first graphic card was an ATI Radeon 9600 pro, im not upgrading from 1080p just for the sake of FPS, and not having to upgrade my gpu for a long time, i know if switch to 1440p there IS no coming back so i Will delay the change as much as possible",Neutral
AMD,what if I just dont spend $150 on a new monitor when I already enjoy gaming on my $20 1080p monitor I got from facebook marketplace,Neutral
AMD,">An 8gb cars now really isn't going to last too long  Itll last a decent amount of time still. The majority of gamers use 8gb cards and game devs know this, they design games with that in mind",Neutral
AMD,It offends people on this sub to lower settings.,Negative
AMD,"DLSS is not just framegen, upscaling does not impact frame times.",Neutral
AMD,"I siad DLSS4, not framegen.",Neutral
AMD,"Show me where it happens in games that people with a 4060 actually play. Games like GTA 5, PUBG, CS2, Path of Exile 2, Marvel Rivals etc.",Neutral
AMD,"The PC building reddits can get so echo chambery. I remember back when I was first overclocking my i7-6850K build I was looking up various other threads to see what kind of results people were getting to get a baseline idea of what to expect. Some on reddit, some on other forums.  Sometimes I'd come across a thread of someone who was like ""I got it stable at 4.4 GHz with 1.41v"" etc etc, key being > 1.4v. Other places people would basically just be like ""oh nice grats"" etc. Whenever I'd find those no reddit there'd be a bunch of comments calling OP crazy, telling them the processor wouldn't last more than a few weeks etc etc. One particular comment I still remember was ""You better already have another one coming to you in the mail, because that'll be dead by this time next week.""   Well my i7-6850K that ran at 4.3GHz @ 1.425v for the past 10 years says otherwise...  Or another great is in that build I had a secondary GPU, because I run more than 4 monitors, and single GPU can only output to 4. Like oh my god the amount of people being like ""you don't need 2 gpus!"" and I'ld have to be like ""yes, to run 5 you do..."" and they'd just be like ""then only use 4"" and like ""ok but I need 5, so using less than 5 isn't an option..."" - Or you'd have them coming in be like ""You're just hurting the performance of your main GPU because it'll be running at x8 instead of x16"" and me having to explain ""No, i7-6850K has enough lanes so the bifurcation on the X99 Classified keeps both at x16..."" one even argued with me to the point I linked to the user manual stating if slot 1 and slot 4 it is x16/x16 on 40 Lane processors, and they still trying to say I was wrong up until I posted a photo of my BIOS screen showing x16/x16.",Neutral
AMD,They would have to drop 80% of the games they test in their GPU suite if they really want to help gamers buy a budget GPU.  Like who TF (among budget gamers) is interested in Alan Wake 2 but not GTA 5?,Negative
AMD,Fixed it.,Neutral
AMD,"> The reasoning is probably because it just didn't sell as well as people think.  9070 GRE that is not sold in most of the world is in survey, yet 9070XT isn't? How do you even think this could possibly by it.  Same thing that is happening with 9070XT happened with 7800XT and 7900XT previously. I was also ridiculed and heavily downvoted back in middle of January when I said that 7700XT outselling 7800XT made no sense, and few months later 7800XT magically overtook 7700XT the first time it appeared in survey(17 months after release) and continued to gain share rapidly, while 7700XT didn't bulge in same timeframe.  Steam simply fails to report some AMD cards from same series correctly for past two generations.",Negative
AMD,"Ain't nobody is buying a 9070 over a 5070 lil bro. Anyway the 5070 Super 18 GB is coming out and that will send AMD to the coffin, if not already. It's going to be the next 1080ti, if you can buy it at MSRP.",Neutral
AMD,"Your ignorance is showing again, the 750ti is a DX12 card",Negative
AMD,Steam survey with prebuilt and laptop data 🤣🤣🤣,Neutral
AMD,Because all these people can't see past their own hands thus they think their view point is the only viable one.  Hard for them to shake it off when they belong to echo chambers and watch Youtubers that keep parroting the same message.  EDIT: formatting,Negative
AMD,"It isn't unusable, it just isn't very nice..unless it's a small monitor, then it's small. Which isn't nice. Imo of course.",Negative
AMD,the era of 1366x768 still haunts my nightmares,Negative
AMD,"I upgraded from a 1080p to a 1440p monitor at 24"" and the difference was absolutely huge, it is a problem simply because it's ancient at this point, GTX 460 was released in 2010 to basically be the first midrange card to cope with 1080p just fine, that is **15 years ago**.",Negative
AMD,"as someone who rocks 27 in 4k monitors, I really dont think so  for 11-18in laptop monitors, it has its place and even then if you will note that most premium laptops (macbooks, surface laptops, etc.) all use better than that resolution, but by 24 inch really 1440 should be the default.  higher PPI makes things look much nicer, and for even 4k when you sit up close, something beyond 32in I feel isn't as nice and esp people trying to use smaller (40 some inch) TVs as a monitor and sit at monitor distances 4k even enough.  it is there because its cheap and accessible, and that is really all there is to it.",Neutral
AMD,"Definitely not my experience, I got a 24"" 1440p display and the upgrade from 1080p was gigantic especially in text but also in gaming, the 1080p monitor then was as if I didn't put my goggles on, a blurry mess - once I saw the 1440p in action.",Negative
AMD,I cannot get the 'didn't feel (see) the diiference' like..wut.,Negative
AMD,So can 6GB gpus in many cases at minimum settings.,Neutral
AMD,I've never seen someone specify DLSS3 or 4 and not mean framegen.,Neutral
AMD,DLSS upscaling never caused issues in twitch shooters,Neutral
AMD,"If you honestly think people buy new PCs with the intention of avoiding new AAA games, you're smoking shit.",Negative
AMD,"You're not allowed to show the games people are actually buying & playing.  For example: [https://steamdb.info/charts/](https://steamdb.info/charts/)  [https://steamdb.info/stats/globaltopsellers/](https://steamdb.info/stats/globaltopsellers/)  ""You're only suppose look at the most demanding games of 2025 and come to a conclusion about a gpu like a reviewer and not a consumer""  Redditors are shocked when average consumer library is filled with old games & mostly GAS.",Negative
AMD,"> 9070 GRE that is not sold in most of the world is in survey, yet 9070XT isn't? How do you even think this could possibly by it.  Because the Chinese market is huge?   >Steam simply fails to report some AMD cards from same series correctly for past two generations.  That probably does happen but for it to be significant, you'd think the generic categories for ""AMD Radeon(TM) Graphics"" and ""AMD Radeon Graphics"" to be huge but they're not. Even if you discount iGPUs completely and assume those generic categories are for two separate cards, it's a disaster showing for AMD and they need to be better.  They're getting there. FSR4 is a step in the right direction. If they could sustain lower prices next launch without the rebate issues they make actually make a splash.",Negative
AMD,I never said it wasn't. You implied it was.,Neutral
AMD,Do you even know that Nvidia laptop GPUs are different entries in the survey?,Neutral
AMD,"Lots of things, if not most things, follow an S curve. Eventually you hit diminishing returns and ""progress"" slows.   Is what it is.",Negative
AMD,"1080p is the sweet spot for 24"" monitors.",Positive
AMD,It should be a crime against humanity to sell laptops with that screen resolution.,Negative
AMD,being 15 years old doesnt matter when it's working just fine for tons of people still. You're saying its bad because its old without any rationale for why old is bad,Negative
AMD,"i switched from a 24"" 1080p 60hz monitor to a 27"" 1440p 180hz and it does look a bit more sharp but it wasnt some eye-opening incredible experience like ive seen other people describe, and i cant notice the higher refresh rate like at all",Negative
AMD,*Wut?*  The primary use of DLSS is upscaling and has been from the start. DLSS4 introduced the transformer models which look even better than the previous modes. Either you are making stuff up or never actually looked.,Positive
AMD,back when it was artifacting too much it did.,Neutral
AMD,there are millions of people who buy prebuilds and then never play anything other than competetive online games like LoL or Fortnite.,Negative
AMD,"Yes, people buy pre-builts that come with a 4060 to play the games they already play most of the time  - given that they used either a crappy laptop or an older PC that can no longer take any meaningful upgrade for this purpose before buying the new PC.  If you don't get this then you might be smoking shit far more than what you think I smoke.  And if we really are to talk about AAA games - we can come up with better examples than Hellblade 2 that has less than 100 active players or Star Wars Outlaws that has less than 500 active players on Steam.   Funny that you are still unable to show me examples where 8 GB is a real problem on a modern platform where stutters cannot be mitigated with the simple concept called lowering graphical settings.",Negative
AMD,Most reviewers are not reviewing the newest AAA games. They typically keep games that basically no one is playing for a long time. Alan Wake 2 wasn’t even that popular yet is a common game to bench. Reviewers pick games based on how easy they are to bench not based on how representative they are for the average gamer,Negative
AMD,"nobody said avoiding, Fifa is new  AAA and 4060 is great for it.",Positive
AMD,the purpose of review is to stress test the hardware in as many ways as you can afford to (time-wise) and to do that you use the most demanding games.,Neutral
AMD,"Do you think it’s productive for a GPU review to test stardew valley, just because it’s in the top 10 most played games?  Don’t get me wrong it’s an amazing game and I love it to bits, but there is absolutely no point in testing it. The answer is obvious, any GPU made in the past 5 years will easily handle the game.  Esports titles make up a lot of that list, and those are also all designed to be as easy to run as possible. A dota II benchmark would be completely pointless.",Negative
AMD,Actually gaming media should be made exclusively about mobile phone P2W lootbox/gatcha games because that's the actual biggest global earners and the actual most popular thing in the world of gaming.  ALL of PC gaming combined doesn't hold a candle to mobile.,Positive
AMD,"The only group more out of touch with reality than redditors commenting about PC gaming saying that 8GB GPUs are ""dead"" is the Democratic party.",Negative
AMD,"> Because the Chinese market is huge?   You are just making excuses to fit steam numbers. 9070XT does not show up in AMD centric stats like ""VULKAN SYSTEMS"" that are so diluted that cards with 0.00% show up. I can give you more examples since you are keen to deny reality - there are zero 7900XT sold according to steam survey. Regular 7600 - zero cards sold. Instead of trying to come up with reasons how that could possibly be real, just face the fact that 9070XT selling zero units is not possible.  To give you more context, we know that on release day in-stock ratio of 9070XT to 9070 was about 3:1.",Negative
AMD,Upvote there...one single thing where you are right  There are still prebuilt...,Neutral
AMD,Couldn't agree more,Positive
AMD,"How much you notice Refresh rate does depend on what type of games you play along with what level of performance your rig can produce.  That said I'd be genuinely shocked if you really cant tell the difference at all between a 60Hz monitor and a 144Hz+ monitor. I'd be more inclined at that point to believe that something has been configured wrong and either the PC is stuck using the monitor in a locked 60Hz mode, or all the VSync settings for games were enabled and haven't been updated. Gotta check stuff like your GPU control panel for frame limits/power saving settings as well.",Neutral
AMD,"(make sure you actually have the higher refresh rate enabled in Windows and in the driver package. For some reason, that doesn't always enable by default)  (it's also worth remembering that not every panel is built the same and some of those just aren't very good at high refresh rate)",Neutral
AMD,> and i cant notice the higher refresh rate like at all  The difference in desktop usage alone is so big that going back to 60 after years of 120+ feels like it's lagging and I'd think even 144 will look like trash if you've sued a 360hz+ display for a long time.,Negative
AMD,"Yes I am well aware, I've been using DLSS for years. I'm saying people usually just say DLSS2 when referring to the upscaling portion, and DLSS3 when talking about framegen. At least, that's how it used to be.",Neutral
AMD,"Yes. My sister has a 6700 XT and while she does play some AAA titles but her ""go to"" games are Minecraft, Fortnite etc.",Positive
AMD,>Funny that you are still unable to show me examples where 8 GB is a real problem on a modern platform where stutters cannot be mitigated with the simple concept called lowering graphical settings.  [Here ya go.](https://www.youtube.com/watch?v=KNDZ7YF2lzY),Negative
AMD,"the games chosen should not be based on popularity but in their ability to test different aspects of the device being reviewerd. The more varied those aspects are the better. If we only tested by popularity then most  benchmarks would be of team fortress, counter strike and LoL. Not useful at all.",Neutral
AMD,"Ease of benchmarking might be part of it, but the real reason to keep older games in the rotation for awhile is to maintain common points of comparison between hardware.  I looked at the [most recent Hardware Unboxed GPU review](https://www.youtube.com/watch?v=-LAH5vh-Cpg), and it's quite a reasonable mix of graphically demanding, older, and popular.",Neutral
AMD,"Cyberpunk is probably *the* benchmark game, and a lot of people play it.  There simply is no point testing esports games, those are made to get decent performance on a laptop so any competent modern GPU will easily get triple digit FPS in them.",Neutral
AMD,"That’s valid. Whenever someone makes a definitive, absolute statement, I assume they’re exaggerating or lying....I never believe anything ""x gpu is x"" or anything hyperbolic to even 1%.",Negative
AMD,"Yes I 100% believe reviews should encompass a variety of games. Expand the list of games you're testing 25 minimum. Go ahead hardest games to run pt, rt, most popular games, casual games etc. If you're going to only test latest games and proclaim x is x because of (xyz).  Its a review not a benchmark tool. A lot of people are waiting for the 8gbdeadcirclejerk, been boring for the past 2 gens. As long as amd copies Nvidia thats what we'll get & most games still running well with 8gb.   ""Otherwise redditors, youtubers will fall into the same story again. How could xy sell? Are people restarted? Dont they see how bad value that is? I'm so smart I never buy bad value, its tiresome""  Reviewers still havent even caught up with adding upscaling/fg permanently or as a separate slide even though most people are going to enable them by default now since its better than native taa or unoptimized no aa most of the time.",Neutral
AMD,Billions of players + f2p + monetization loops + ads > Peecee gaming revenue.   You can start a youtube channel reviewing mobile games theres a market,Neutral
AMD,"Wow so funny, you're so smart and cool",Positive
AMD,"Excuses? Mate the only one making excuses here is you. As I said (bolded for you):  >**That probably does happen** but for it to be **significant**, you'd think the generic categories for ""AMD Radeon(TM) Graphics"" and ""AMD Radeon Graphics"" to be **huge** but they're **not**.",Negative
AMD,"TLDW:    **16 games average (1080P):**         R7 9700X + RTX 9070XT:    Win11 24H2 (25.8.1): 152.5 FPS (Avg), 104.4 FPS (1%) --> **100%**     CachyOS (Default Drivers): 147.3 FPS (Avg), 104.4 FPS (1%) --> **96.5%**     Nobara 42 (Mesa Git): 147.3 FPS (Avg), 107 FPS (1%) --> **96.5%**     Bazzite 42 (Mesa Git): 143.7 FPS (Avg), 97.1 FPS (1%) --> **94.2%**         R7 9700X + RTX 5080:    Win11 24H2 (25.8.1): 158.4 FPS (Avg), 108.8 FPS (1%) --> **100%**        CachyOS (Default Drivers): 136.1 FPS (Avg), 95.6 FPS (1%) --> **85.9%**        Nobara 42 (Mesa Git): 133.3 FPS (Avg), 90.1 FPS (1%) --> **84.1%**            Bazzite 42 (Mesa Git): 133.8 FPS (Avg), 81.2 FPS (1%) --> **84.4%**             **16 games average (1440P UW 3440x1440):**         R7 9700X + RTX 9070XT:    Win11 24H2 (25.8.1): 97 FPS (Avg), 71.9 FPS (1%) --> **100%**     CachyOS (Default Drivers): 90.5 FPS (Avg), 68.4 FPS (1%) --> **93.2%**     Nobara 42 (Mesa Git): 90 FPS (Avg), 70.1 FPS (1%) --> **92.7%**     Bazzite 42 (Mesa Git): 89.2 FPS (Avg), 69.3 FPS (1%) --> **91.9%**         R7 9700X + RTX 5080:    Win11 24H2 (25.8.1): 108.7 FPS (Avg), 79.7 FPS (1%) --> **100%**        CachyOS (Default Drivers): 93.6 FPS (Avg), 69.6 FPS (1%) --> **86.1%**        Nobara 42 (Mesa Git): 91.2 FPS (Avg), 63.7 FPS (1%) --> **83.9%**            Bazzite 42 (Mesa Git): 91.8 FPS (Avg), 59.3 FPS (1%) --> **84.4%**",Neutral
AMD,I have my hope in Nvidia finally fixing DX12 performance and I'll be happy for now.,Positive
AMD,"That’s amazing taking into account how little effort is put on Linux drivers, plus compatibility layers. A real alternative for those fed up with Windows (or not having a TPM)",Positive
AMD,Also want to point out that the 9070xt seems to lag behind in comparable performance vs the older gens. Last one with a 7900xtx was on parity or better if i remember correctly.,Negative
AMD,I’m actually surprised by the amd results. Thought there would be less performance loss.,Negative
AMD,"Looking at those 9070 numbers, maybe I should just move to Linux. Windows has been shitting me with random problems for years at this point.",Negative
AMD,"Games generally performs worse on Linux than on Windows, outside of very specific GPUs and games.",Negative
AMD,"Unfortunately I have to assume this entire set of tests is compromised, because this video is being sponsored by a Windows 11 key seller. With such an obvious conflict of interest, it's hard to take this video in good faith even if the testing data is accurate.    Using Flatpaks and these particular games is also a bit suspect, but that's getting into the details.",Negative
AMD,"Here is the converted Reddit table formatting:    **Windows vs Linux Performance Comparison**  | **Configuration** | **Win11 24H2 (25.8.1)** | **CachyOS (Default Drivers)** | **Nobara 42 (Mesa Git)** | **Bazzite 42 (Mesa Git)** | |-------------------|------------------------|-------------------------------|---------------------------|------------------------| | **R7 9700X + RTX 9070XT (1080P)** | 152.5 FPS (Avg), 104.4 FPS (1%) | 147.3 FPS (Avg), 104.4 FPS (1%) (96.5%) | 147.3 FPS (Avg), 107 FPS (1%) (96.5%) | 143.7 FPS (Avg), 97.1 FPS (1%) (94.2%) | | **R7 9700X + RTX 5080 (1080P)** | 158.4 FPS (Avg), 108.8 FPS (1%) | 136.1 FPS (Avg), 95.6 FPS (1%) (85.9%) | 133.3 FPS (Avg), 90.1 FPS (1%) (84.1%) | 133.8 FPS (Avg), 81.2 FPS (1%) (84.4%) | | **R7 9700X + RTX 9070XT (1440P UW 3440x1440)** | 97 FPS (Avg), 71.9 FPS (1%) | 90.5 FPS (Avg), 68.4 FPS (1%) (93.2%) | 90 FPS (Avg), 70.1 FPS (1%) (92.7%) | 89.2 FPS (Avg), 69.3 FPS (1%) (91.9%) | | **R7 9700X + RTX 5080 (1440P UW 3440x1440)** | 108.7 FPS (Avg), 79.7 FPS (1%) | 93.6 FPS (Avg), 69.6 FPS (1%) (86.1%) | 91.2 FPS (Avg), 63.7 FPS (1%) (83.9%) | 91.8 FPS (Avg), 59.3 FPS (1%) (84.4%) |",Neutral
AMD,Bigger gap on nvidia linux vs windows than i expected tbh,Neutral
AMD,Can I get a TLDR for this?,Neutral
AMD,"The drivers have quite a lot of effort put into them.  What's missing is the fancy GUI control panel apps, which are either non-existent or offer a fraction of what they do on Windows.",Neutral
AMD,Really? 100% vs 85% is like swapping your RTX 5080 for an RX 9070/RTX 4070 TI. I find it hard to believe that people will willingly gimp their performance like that.,Negative
AMD,RDNA4 on Linux is still a bit rough. The gap should be smaller on RDNA3 (Linux might even win in a few games).,Neutral
AMD,3% and 7% don't seem that bad honestly... And that's including ray tracing into the mix.,Neutral
AMD,"> Windows has been shitting me with random problems for years at this point.  hahahahha, that's peanuts compared to the world of insane shit and random broken stuff on linux.",Negative
AMD,I switched for gaming when I upgraded to a 9070 XT from an Nvidia card - it's been fantastic. The only thing I can't do is play stuff with kernel level anti cheat.,Positive
AMD,I've been on Linux + AMD hardware for years. And love it. But I don't play  competitive games with anti-cheat.,Positive
AMD,"You should.  Unless you have a piece of software or hardware that isn't compatible with Linux (that is the case for me, unfortunately), then there's no reason not to switch.",Neutral
AMD,Linux performs 84.% to 96.5% that of Windows 11 on the same hardware.   Linux slower than Win 11 (in these tests),Negative
AMD,"Despite how it appears, it's all pretty digestible if you take a moment to understand it (perils of trying to present data on a platform with as limited a range of formatting and arrangement options as Reddit, unfortunately), but the short of it is, Windows is consistently fastest across all configurations.",Neutral
AMD,"Yeah IDK where this idea that linux drivers have less effort put into them comes from. AMD rewrote almost the whole driver stack to get it included in the linux kernel and nvidia's main profit driver comes from datacenter now which is pretty much all linux, so their focus is heavily on linux drivers right now. As you said, the GUIs arent there yet, but the underlying drivers are pretty solid on both sides.",Neutral
AMD,"The 15% performance impact is the least of problems IMHO.   Gsync, freesync, idle power consumption, DLSS, Direct Storage are IMHO bigger problems.   On the other hand, Elden Ring and old classics run better on Linux.",Negative
AMD,"I am willing to, because game performance is not the #1 matter for me. That means that, on my system, I can game fairly fine, and don’t have do deal with other Windows shit in order to achieve that.  Of course, it’s not for everyone. But I still consider it amazing.  Now if Nvidia did their jobs and had feature (and performance) parity with Windows drivers… Then those numbers would be better even.",Positive
AMD,"I read it differently: I only need to boot up Windows if I need more than 85% of the max power of my system, otherwise I can stick to Linux. (It's probably not exactly how the math works out, but it's close enough)",Neutral
AMD,15% at most is not actually that high knowing that you could be on whatever os you want.,Neutral
AMD,"Linux still wins in a few games tested, but those RT and the Silent Hill scores really bring the Linux average down.",Negative
AMD,"Unlike Linux though, windows sometimes has issues that simply make no fucking sense and all the advice eventually boils down to “uh just do a fresh reinstall lmao”. Bonus points for some random ass error like 0x74729472883 that leads absolutely fucking nowhere when searched.  For an example I had an issue where the time just refused to sync with a time server, which meant my one time codes were always a few seconds out. I followed every god damn suggestion I could find, running every repair command and manually starting/restarting services, all that stuff. Nothing worked. Inexplicably almost a year later it started working all by itself. Made me insane.  That’s old stuff though. Currently I’m dealing with a windows update that refuses to install despite trying every method including offline, safe mode, all that. Also currently have an issue with random micro hangs in games and while switching tabs/windows. Seemingly no reason, no hardware faults, no viruses. I love windows.  Oh yeah one of my favourite issues I ever came across was an odd game crash in FFXIV, which turned out to be my audio interface randomly getting set to a higher bit/sample than supported in windows settings, causing it to randomly crash. This happened 3 times and I still don’t know why, since it uses bog standard USB audio drivers that are fully integrated into windows settings.",Negative
AMD,EAC works on linux at least. Its kernal level on windows but not linux.,Neutral
AMD,"> then there's no reason not to switch.  That’s nota good argument in favor of switching, though. There’s not really a compelling reason to switch for most people",Negative
AMD,>hardware that isn't compatible with Linux  Something like that exists?? Are you talking about PS3 or Switch 2?,Negative
AMD,Is that native or with Proton?,Neutral
AMD,OP's comment wasnt formatted properly on mobile app for me unfortunately,Negative
AMD,The only thing I won’t agree with is idle power consumption. My system draws more on Windows than Linux…,Negative
AMD,"I've never had a problem with FreeSync on Linux, and I can't think of a game that actually uses Direct Storage to score more than a few percentage points of performance.",Positive
AMD,"Would you pay $450ish for an RTX 5060? Because that's around the threshold, going from a 5060 TI 16GB to a 5060 performance.  Or even worse, going from a 5090 to a 5080 performance, paying $2500 for a 5080.  I'd want Linux to be better, but in its current state of 85% performance from Windows it's just missing out on your hardware's full performance.",Negative
AMD,"Could be fine for people with high tier hardware since they already get excessive amounts of FPS so 15% loss is still playable, but to most people on -60 class cards that turns a 55-60 FPS playable game to 45-50 hitchy gameplay. I'm more accepting of that FPS since I came from 5600g system playing at the lowest settings, but most users definitely won't want to sacrifice that.",Neutral
AMD,"RT basically wasn't a thing just couple of years ago, all 6000 series lifespan was without RT support, only after like a year since 7000 series was launched, RT support was added to mesa. But for whatever reason on my 7900xtx it stresses one or 4 threads of cpu and have 1/2 or in worse case scenario (cyberpunk PT) 1/4 of windows performance.    But hey, at least FSR4 now works decently even on RDNA3, what is infinitely better than windows, where it's simply isn't possible. On other hand, we on Linux don't have catalyst and features that cover with it, like fluid frames.",Neutral
AMD,"The mesa radv RT implementation [0] is a fair bit less complex that the gpurt [1] one used in amdvlk - which is exactly the same code as the windows proprietary driver so should be similar to that in performance.  It should also be possible to use both amdvlk and mesa radv drivers at the same time, switching per app at runtine in user space with no modifications.  Really the entire amdvlk is the same as used on windows, the only possible ""secret sauce"" is some per-app tunables, all the code is the same as published on GitHub.  [0] https://gitlab.freedesktop.org/mesa/mesa/-/tree/main/src/amd/vulkan/bvh  [1] https://github.com/GPUOpen-Drivers/gpurt",Neutral
AMD,Best part is googling that 0x8483737 whatever error and being sent to the MS forums where some nerd will just tell you to run sfc /scannow or a dism health check.,Negative
AMD,"Certain PC accessories don't have proper Linux drivers or software.  For me, it's my Elgato stuff like the Stream Deck and Stream Deck Pedal.  Both of which I use extensively.  And those are one example.",Negative
AMD,A lot of controller equipment for video production and music making has no drivers and isn't universal using USB Human Interface Device.,Negative
AMD,"This is a kind of common challenge. They aren't talking about your basic CPU/GPU/MB/RAM, though I definitely did have some serious problems years ago with an earlier Intel atom for many months after release until a kernel update fixed it.",Negative
AMD,Creative external sound card for example.,Neutral
AMD,"most (if not all) the games are running on a translation layer, hence the lower performance.",Negative
AMD,"Proton probably.  Most folks prefer using proton over native anyway, because native Linux doesn't get the same support.  Results are just better if they focus proton.",Positive
AMD,AMD GPU?   Ohh and I forget RT (because that is currently not relevant for me).   Sometimes gaming on linux feels like Debian stable LTS. Rock solid but a few years behind the curve.,Negative
AMD,"Some people won't have problems with HDR under Linux and say ""works on my machine"". It still is a mess.   I don't think Direct Storage is used that much yet (Ratched & Clank), but I am sure that if newer console ports make use of it, it will take a few years before it runs smoothly on Linus.",Negative
AMD,To get away from windows? Absolutely. That's worth it's weight in gold to me personally. A small price to pay to not deal with Microsoft.,Positive
AMD,"I'd argue that if you're paying much more for a 15% perf increase you're doing something wrong to begin with tbh.  That said, at some point hopefully Nvidia will fix their drivers and get that \~95% of performance that AMD is getting through Proton as well.",Negative
AMD,15% is totally negligible if you go from 400fps to 460fps. Makes no real world difference because it’s not slow to begin with,Negative
AMD,"Yeah, I've gone from Windows to Linux and found it rather painless, but I have a heavily OCed 4090. A 15% drop for me is still very playable in pretty much everything, but I don't know how acceptable that would be for someone on a more reasonable system.  It's one of those things where I just want to be a +1 to encourage adoption and get more focus on ironing out performance from vendors and game developers. I have always had my problems with Microsoft, but I absolutely *hate* where they are going now. I'll take a small hit if it means I'm doing my very small part in growing the user base for Linux.",Negative
AMD,"I don’t know why are you going with the worst case scenario, that only applies to nvidia cards, and fps for Nvidia/AMD are on the same ballpark.  That goes to show however that nvidia as a company could do way better in their driver implementations for Windows…",Negative
AMD,"Yeah, wish Linux had those tools that AMD offers on Windows. I play less demanding games, so it doesn't really affect me, but would still be nice to have.",Positive
AMD,">But hey, at least FSR4 now works decently even on RDNA3  Wait really? How?",Positive
AMD,Meanwhile in Linux land you eventually end up at a 15 year old issue on GitHub with some guy saying it’s intended behaviour and that you’re an idiot for ever wanting to do whatever it is you’re doing… I still love you Linux don’t worry.,Negative
AMD,"> Stream Deck and Stream Deck Pedal  Maybe these projects help you:  https://github.com/StreamController/StreamController  https://github.com/nekename/OpenDeck  The first one is available as a flatpak, so pretty easy to install",Positive
AMD,Ohh okk,Neutral
AMD,Explain to me in simpler terms...... What other things are there?,Neutral
AMD,"AMD iGPU+Nvidia with MUX. I can switch the Nvidia completely off (D3cold) and still get great performance on every day life :)  Yeah, Linux is behind because 6% market share on desktop. Why should nvidia spend as much effort for that 6% as for the rest of Windows?  But on the other hand: if they did it, Linux gaming would definitely be better than windows…",Positive
AMD,HDR in Bazzite is far better implemented compared to the mess in Windows,Positive
AMD,">Some people won't have problems with HDR under Linux and say ""works on my machine"". It still is a mess.  What does this have to do with FreeSync?",Negative
AMD,"I think performance is only important to a point.  If a game runs max settings, and gets good fps, and feels good to play, who cares...?  Losing a small percentage of performance to gain stability, customization, and no big brother recording you is awesome.",Positive
AMD,https://discuss.cachyos.org/t/how-to-use-fsr4-on-rdna4-gpus/9004  this is in the context of CachyOS but it should work on any distro if you use mesa-git for your AMD driver stack,Neutral
AMD,That's basically why they came out with immutable distros in Linux land. Bazzite is a good example of a distro that's really hard to fundamentally break like that.,Neutral
AMD,"I'm using a stream deck on Linux, and both of those projects are much more limited than the Windows version. Stream controller is abandonware and opendeck is in early development and therefore buggy and missing many features.",Negative
AMD,"Audio cards, gaming accessories like wheels, VR headsets, and some mixers.",Neutral
AMD,"Think literally anything else you might plug in.  Video capture,input devices, audio equipment, etc.  People talk about the Linux compatibility of laptops too - I *assume* that's because Laptops tend to be a big pile of parts you're stuck with, including whatever proprietary nonsense the maker created.",Neutral
AMD,"Have you measured the power consumption at the wall with D3Cold? I ask because my system actually uses more power with the nVidia GPU turned off (D3Cold) than not, but that is very likely because it is actually an eGPU over OCulink and not a normal configuration. Still, it would be nice to know if that works properly on a normal configuration.",Neutral
AMD,"I was talking about -60 series cards going from iffy playable 55-60 FPS, to becoming hitchy 45-50 FPS gameplay. Most people have -60 series cards, even older than 4000-series generation even. No way someone with like a 2060 Super or a 3060 would sacrifice 15% of their performance to change to Linux.",Negative
AMD,"No need of mesa-git anymore, mesa-25.2 already released.",Neutral
AMD,"And with Flatpaks and docker/podman, almost everything can be done without touching the core. Fore other cases, a quick VM.",Positive
AMD,Steam deck is quite new hardware too.,Neutral
AMD,"ohh lol, Audio cards are still used? Wheels i can understand but no VR headset support for linux???",Neutral
AMD,"Understood  I assumed things to be mostly plug and play, didn't realise that things are not simple for slightly not so simple devices.",Neutral
AMD,"There's support (generally) but in my experience it's been rocky. Stutters, latency, odd issues that weren't present under Windows. It's been a few years since I've tried though, so maybe things have improved (or maybe they've degraded).",Negative
AMD,"Not strictly a card, so to speak, but I have a Scarlett 2i audio interface which I use for my microphone and guitar inputs and my headphones output. This is all piped to VB Matrix which let's me fiddle with my audio channels on demand.",Neutral
AMD,"Not so much in the traditional sense (an internal card), but there's a huge market for external audio devices.",Neutral
AMD,"Damn, i thought that was a windows thing.",Negative
AMD,Ohhh so advanced stuff,Positive
AMD,"This is the lowest market share AMD/ATI ever, in 2010 AMD almost had 45% of the share",Negative
AMD,"Just goes to show how little influence to tech-tuber sphere actually has on sales. Yes, sentiment matters, but doesn't necessarily translate to sales.  That said, it's obvious AMD isn't allotting a lot of resources to consumer GPUs.",Negative
AMD,HUB and GN in shambles.,Negative
AMD,"AMD's idiotic (Nvidia-$50) strategy is going to work out any day now, hardware enthusiasts told me so.",Negative
AMD,>Nvidia introduced two new Blackwell-series AIBs: the GeForce RTX 5080 Super and the RTX 5070.  ?,Neutral
AMD,"Here it is, here's the reality for the AMD fans. RDNA4 didn't do ANYTHING to increase AMD's market share. I'm so tired of hearing ""this time what AMD's going to do will work!"" or ""Give it another quarter, then you will see the results!"". All the MLID and HWUNBOXED FUD about ""RDNA4 is a hot seller and is destroying NVIDIA"". Yeah... sure at one local retailer.  Get a grip. AMD's stuff is, in the eyes of ordinary gamers, too expensive and not available enough to beat NVIDIA's dominance. With how poorly NVIDIA's drivers were this time, with poor availability for NVIDIA, with tariffs, with them ignoring gamers now, they're flying as high as they ever have! This was AMD's best opportunity in YEARS to make a dent in the NVIDIA mindshare and they failed by not being upfront about their own MSRP and availability. If AMD truly want to gain market share, they HAVE TO LOWER PRICES and take lower margins. AMD also has to compete across the whole stack, from the 6090 all the way down to the 6050. But they just will never shake that mindshare of being seen as the cheap brand and they always will be that, embrace it and use it against NVIDIA.",Negative
AMD,BUT BUT HARDWARE UNBOXED SAY...,Neutral
AMD,"Despite the ""Nvidia has abandoned the gamers"" rhetoric online, IRL Nvidia's mindshare and brand recognition had never been better.   Nvidia has become a household name. People who struggle to attach files to an email are aware of Nvidia. Becoming the world's most valuable company will do that.  So when a parent or someone who isnt tuned into the hardware market decides they want to buy a gaming PC, they're just defaulting to the Nvidia option that's within their budget. Probably don't even remember the name of the specific dGPU they have.   I dont think a demand to run local AI has driven this market share collapse - I think the halo-effect brand impact of Nvidia's dominance in AI and how that pushed the brand name into mainstream lexicon has led to it.",Positive
AMD,"The entire techtuber scene is genuinely embarrassing at how ineffective yet morally righteous/self-aggrandizing they are. A smarter, more humble scene would realise they're falling for audience capture/are out of touch but these people are too stubborn for that.",Negative
AMD,6% cant be right. Hasn't amd gaming revenue increased by like 49% yoy?,Neutral
AMD,"***radeon subreddit in shambles.***  I know people will say competition is good, but it should not be consumer obligation to go to the competition for the sake of it, rather than their product. AMD on paper had good product, in reality it was just 5070Ti -50$, but lacks the strong ML capabilities of it, AI capabilities, ecosystem that can both do gaming and AI.",Negative
AMD,What is the cause of this? Is Nvidia ramping too high? AMD ramping to low? Or AMD diverting to products like strix halo?,Neutral
AMD,AMD might just exit the market at this point. Having 6% or less of the market can't be enough to pay for R&D. They are primarily a CPU company and this is clearly not working for them.,Negative
AMD,Hard to capture market share when they can barely capture mindshare 🤷‍♂️,Negative
AMD,You can't take marketshare if your product is inferior in gaming/productivity/AI. Mark my words. Radeon will be dead by 2030.,Negative
AMD,"I'd been reading that sales of the 9070 and xt sold really well but now read that they have lost half of their market share, something doesn't add up.  I'm sure this is correct but how does and stack up worldwide",Negative
AMD,"And here we thought 92% market share was peak, but it just does not stop.",Negative
AMD,"Everything from AMD, except their 8GB 9060xt, is overpriced right now in most of the world. I don't get why they have such supply issues. Or AIBs are just refusing to build AMD GPUs, and choosing to just pump out more Nvidia instead.",Negative
AMD,>AMD GPUs are flying off the shelves. Never seen anything like it.   Reality: basically only weirdo Linux users are buying them.,Negative
AMD,If you look at the top sellers on newegg its all AMD. And if you look at the top sellers on amazon its a gpu brace support along with old 3060 and 6660. also mindfactory says amd is outselling nvidia by double digit %s. I think nvidias numbers are bullshit and they arent being sold to gamers and gaming gpu sales are probably down.,Neutral
AMD,Can we break up the nvidia monopoly already?,Negative
AMD,"Top tier GPU performance does matter for consumer perception.  If they aren't even competing with 5090 (which are launched weeks/months before 80 and 70), they aren't even discussed in gaming communities, even for other mid and low end stuff.  Plus waiting for Nvidia to launch then -$50 pricing is stupid, AMD could get so much clout and publicity if they launched 6 months before and compared themselves to Nvidia's previous generation performance at lot better pricing.",Neutral
AMD,"RIP ATI, I have I think a 9200 sitting in my back seat. Wife found it and thought of me.  Welps, not like AMD can do any worse than 6%...right?",Negative
AMD,"AI is predominantly Nvidia cards. When there's been a spike in those sales,  it'll dip market share for AMD.",Neutral
AMD,This means that IF Intel has a great generation with Xe3 discrete GPU cards then it COULD make a significant dent in Nvidia's market share IF they have the right amount of supply  The B580 was a success beyond Intel's wildest dreams.  By the time the B580's launch the Xe graphics division was on **life support**  Xe3 discrete cards were likely canceled along with the B770 by the time of the B580 launch.  Conclusion:  **Intel badly misread the market and never thought the B580 would be a massive success. I.e. sell out on launch day**  Intel likely restarted Xe3 DGPU and B770 tape out and production after the massive success of the b580 launch  **That's why we do see leaks for Nova Lake A and AX big iGPU tiles in 2027 instead of any leaks about Xe3P Celestial cards and a 2026 launch for them**,Positive
AMD,">it's obvious AMD isn't allotting a lot of resources to consumer GPUs.  I dont think I will ever buy a new AMD GPU again unless they are offering 50% better price and performance than Nvidia. They going into gutter, at some point even Game developer will stop optimize their game for AMD GPU on desktop/laptop. In that case why bother take a risk to buy AMD GPU?",Negative
AMD,Yeah but their influence on Reddit is insufferable.,Negative
AMD,Its like game reviews. there were studies done where less than 1% of consumers consider reviews any influence on purchasing decisions. It wouldnt surprise me if its the same for hardware.,Negative
AMD,..and AMD still benefits from disproportionately high media coverage. I can’t think of another sector where a company that has \~6% Market cap gets <50% of the media coverage. AMD was outsold 15-1 in Q2. it's unbelievable.,Positive
AMD,"**AMD and Intel's DGPU situation:**  AMD Radeon and Intel Arc are getting **Bulldozed** and **Steamrolled**  right now in overall AIB DGPU market share  AMD and Intel both  need to **Excavate** themselves from this situation with  **RDNA5/Xe3P or Xe4**  Maybe then, both companies will achieve a **Zen** moment  in the DGPU AIB market.",Neutral
AMD,I don't think HUB was ever under the illusion that RDNA4 is a massive commercial success. They've only reviewed the product as it exists - their review of the product doesnt become incorrect because it was a commercial failure.   They've also talked on their podcast about how the current pricing on the 9070XT is just way too high,Negative
AMD,Its actually Nvidia + $100 strategy this year.,Neutral
AMD,Maybe a sign the article was written by AI,Neutral
AMD,Probably just an error. They just mean 5080.,Negative
AMD,"> ""RDNA4 is a hot seller and is destroying NVIDIA""  Can't really give any leeway to HUB for making the grandiose statement RDNA4 is outselling Blackwell.  If that's the case then why is it not reflected in the Steam Survey?",Negative
AMD,"Intel offerings were as cheap as it got, lost them tons of money in the process, and they didn’t make a dent in marketshare.   Money is not the problem.",Negative
AMD,The issue isn’t price/performance it’s availability. Due to most of AMDs wafer capacity going to CPUs/Servers they don’t have enough for AIBs/laptops. AMD could have 100% sales at hardware stores but still lose out in market share if they aren’t in laptops/prebuilt desktops.,Negative
AMD,"> ""this time what AMD's going to do will work!"" or ""Give it another quarter, then you will see the results!"". All the MLID and HWUNBOXED FUD about ""RDNA4 is a hot seller and is destroying NVIDIA"".   Don't worry bro RDNA4 was just a test of the improvements they're working on, RDNA5 is AMDs real come back. They're gonna have 4 chips covering the whole range of gaming GPUs and not just that that, they have a 96CU 512bit bus behemoth with GDDR7 that will compete with the 6090!! AMD is back baby.",Neutral
AMD,People really underestimate the sheer amount of mindshare Nvidia has+their presence in prebuilts.,Neutral
AMD,"Also, if you cannot compete on speed or features, compete on price.  And not ""oh, $50 less than equvalent nvidia"" (for certain cases of equivalent). Make the cards _noticeably_ cheaper, then people will buy them, just like they bought Zen 1 when it was still slower than intel but like half the price.",Neutral
AMD,"nah it's just a conspiracy against amd again like intel, now nvidia paying companies to not use amd, and valve doing dishonest reporting with their ""random"" sampling  good thing reddit sees through the lies and we have real unbiased journalism and hardhitting coverage from hardware unboxed and gamers nexus who drop truth nukes against nvidia",Negative
AMD,"A huge majority of these GPU sales are to AI/industry, not gamers.   Get a grip.",Negative
AMD,Well said!,Positive
AMD,"I don't think you have to tell AMD fans that they are never going to beat Nvidia in GPU market share. DIY is a tiny fraction of overall sales, even if AMD did have Nvidia beat in DIY this generation, Nvidia will sell 10x the inventory in prebuilts and laptops, and that's just for gaming. When you factor in people buying PCs to run LLMs, which is almost always prebuilts from large OEMs like Dell rather than DIY machines, it's no surprise that Nvidia gained market share, and will continue to do so as long as AI is relevant. People think gamers are obsessed with buying Nvidia, try the boomers who are integrating LLMs into their firms who now see AI = Nvidia and won't buy anything else.",Neutral
AMD,the reason nvidia has such a high market share comes down to pre-built systems. Jensen knew all the way back in the 90s how important the OEM market is and Nvidia holds all the contracts nowadays.   go into literally any big electronics store on this planet and ask for a PC. you won't find a Radeon inside.   The same is true for intel and the laptop market.,Neutral
AMD,"Its a -50$ card in some cases it's +50$ more expensive at their counter part , so it's worse than it's ever been. Should've been 450$ msrp 9070 xt",Negative
AMD,"AMD develops graphics technology to win console contracts, and to maintain a sector-leading lineup of APUs. RDNA 4 has strong improvements over RDNA 3, and when successors to those key products come out and take advantage of them, they will benefit greatly. Absolutely not a failure.  And the fact of the matter is that the 9070XT is a good card, and the 9060XT is a pretty great one. I think you’d have to be an uninformed idiot to buy the comparably priced Nvidia card in both cases. The market is fully in fuck around and find out mode at this point, in my opinion. AMD will never go away due to the key products I mentioned, so they’re just going to keep their heads down and do as they do. It literally just is what it is.",Positive
AMD,"It’s not price, it’s capability. Local AI is much easier on Nvidia",Positive
AMD,"It doesn't matter what amd makes, people buy Nvidia.",Neutral
AMD,"Yeah how well has selling at low margins worked for Intel? They've been fire selling CPUs and GPUs for years and it's losing them tons of money to the point they've sacked off half of their workforce and are in dire straits begging for subsidies from the US gov just so they can keep their fabs going.   You are forgetting that Nvidia have a R&D budget far larger than anything AMD or Intel can afford to spend on developing GPUs right now.  If AMD cut RDNA4 prices to bare minimum, how would they afford the even larger budgets for future architectures like UDNA2, UDNA3, etc?",Negative
AMD,AMD unboxed is often wrong. Glad people are finally seeing them for the charlatans they are,Negative
AMD,"This HB?  **Fun** **fact**: If you see 9070 XT's sold out shortly after release, it will mean retailers will have sold more 9070 XT's than all GeForce 50 series GPUs combined.   (this includes RTX 5070 stock)  Is that true?  Yes I was told by retailer  Why dont you share the numbers?  ""We want to protect the source""",Neutral
AMD,Rent free...,Neutral
AMD,"This is like saying that movie buffs are out of touch because they dislike franchise slop and give good reviews to movies that don't sell well at the box office. They are reviewing the products on their merits. If the public make different decisions that doesn't mean the reviewer is out of touch it means marketing works to sell a product, shocker!",Negative
AMD,"They have many things under that, not just Radeon. Console SoCs are much more popular.   On the other side, Nvidia gaming revenue has broken record twice this year. And they pretty much just have only Geforce for that. Switch SoCs are listed under OEMs for Nvidia, not gaming",Positive
AMD,"Revenue can increase, and sales can increase, with a decreasing market share. Just need the competition sales to increase more.",Neutral
AMD,They attribute majority of it to zen 5 sales. Gaming SOCs is actually $1.3 Billion and $2.5Billion from CPUs. (Yes AMD lumped them together),Neutral
AMD,Nvidias gaming revenue also increased a lot.,Positive
AMD,"The irony is that RDNA4 is legitimately the first time ""NV -$50"" in  7 years and the consumers are rioting!  They were fine when it was ""NV -$50....no AI hardware, no ray tracing hardware, no x-features set"" but now that is legitimately ""NV -$50 and a few no X-features set"" riots.  Crazy.",Negative
AMD,AMD too expensive.  If they're wanting to grow market share they need to take customers from Nvidia... and all they're offering is a single product that is priced very similarly to Nvidia.  I don't know how they tackle this without taking a loss.,Negative
AMD,AMD produces a worse product and asks more money for it. Thats just the simple reality we live in.,Negative
AMD,"**It's AI farms.**   You've seen the GN doc. You've seen the pics.    Fact is, while Johnny Gamer debates between 8-16GB VRAM and spending $400 or $900 on his gaming rig, some fucking AI startup is giving a middleman $10'000'000 *(in VC money)* to acquire as many RTX 5090s as they can find.",Negative
AMD,"Its the software support, nvidia has CUDA which all the software especially in AI supports so people buy nvidia. And yes, most of the gaming category sold nvidias, are used for AI I can bet.",Positive
AMD,Nvidia recent quarter had 4.6billion usd in revenue for gaming gpus. That's basically 3 times the amount from 2019 and early 2020. Even in 2021 where crypto miners used loans to buy everything peaked at 3.3bil.   The asnwer is a complex one. It is a bunch of different things. I think one of the main ones is the non western world has gotten rich enough to pay for services. Compared to the early 2000s.  Meaning they have internet acess and become aware of why even play video games. Covid just accerlerated that adoption rate that was already going to happen in 2020s.    Nobody cares about the anti amd circle jerk going on this post. Amd might mot be losing customers. It is just continuing the trend they already had last year. Not growing alongside the entire gaming market. Which lmao.,Neutral
AMD,"Radeon R&D is basically bankrolled by SONY, Valve and Microsoft at this point. I don't think it will ""go away"" anytime soon because that's Radeon's customers, not the average consumer.",Negative
AMD,>AMD might just exit the market at this point.  no they wont for infinite reasons including they are making money still.,Neutral
AMD,You are famous.   https://youtu.be/07v4p5IN85s,Positive
AMD,"Their R&D costs are partially paid by Sony and Microsoft for console APUs. They also use their GPU architecture in all of their CPUs, with it being especially important for their mobile lineup they are aggressively going after Intel's market share with. AMD would also be completely daft to abandon the AI server market by dropping Instinct when the whole market is heavily supply constrained. Like their shareholders would probably sue them for it. dGPUs are such a tiny piece of AMD overall strategy with Radeon, they're basically a method of generating an extra bit of income on R&D they were going to do anyway.",Neutral
AMD,> AMD might just exit the market at this point.   They wont. They need to keep GPU developement alive to keel doing console APUs. All the tech is shared there.,Neutral
AMD,If Amd is doing this terribly I cant imagine to think how Intel will hope to compete. 0% after 3 years is brutal,Negative
AMD,it's them that are not making this work...,Negative
AMD,PS6 will not exist then?   That’s the actual customer of Radeon.,Neutral
AMD,RemindMe! 5 years,Neutral
AMD,"> You can't take marketshare if your product is inferior in gaming/productivity/AI.  ATi/AMD graphics-cards weren't even bought when they were BETTER **and** LESS expensive at the same time than anything nVidia had to offer, solely because of people being just mind-f—ked by Nvidia into worse products with less VRAM etc.  Nvidia only sells through brand and mind-share alone – If nVidia one day stumbles on graphics and AMD beats them, people would stick to Nvidia still, no matter if their products would be considerably worse and way more expensive.  Since in that regard with blind mindless brand-loyalty, Nvidia is pretty much like Intel: *Higher price-tags for less bang for the buck and in many segments the worse offer*.",Negative
AMD,"RDNA 4 is doing *really* well in DIY, but because Radeon may as well be non-existent in pre-builts its overall market share impact is bugger all.",Negative
AMD,"It is probably more complicated than that. For example, its easier to find Nvidia than AMD in my country.",Neutral
AMD,"and people who is looking for deals in specific marketplaces  my friend brought a 7900XTX because it was selling for cheap cheap while nvidia cards were being scalped as 50 series was launching.  the 9070 XT was also being scalped and priced higher than normal, so 7900XTX it was for him",Neutral
AMD,Break up Microshit and Apple first.,Negative
AMD,"AMD had the chance to launch the 9070s about 3 months before NV rolled out their similar products, but the price NV slapped on their products sent AMD into panic mode.  End result was a throwback to Vega launch - Rebates for everyone!  Followed by a proper drought of products because they realized honoring the price they set is losing money which they can just focus more on money printing enterprise!  AMD isn't going to do jack for this generation, its over. Wait for RDNA5/UDNA at this point.",Negative
AMD,I agree with everything but the 5090 and the 5080 were launched on the same day.,Neutral
AMD,Which is why IMV AMD should launch the rumoured AT0 SKU late next year/ASAP that is fully enabled along with Zen 6 X3D. Launch ahead of Nvidia with something they *may* not be able to beat with a 6090 Ti that has a few SMs cut down (if GB202 successor has 288 SMs and 6 of 'em are cut for a 6090 Ti ala RTX A6000 Blackwell). Tricky and risky but AMD needs to make a big splash ASAP.,Neutral
AMD,"I think they could have got away with not competing with the 5090, but they absolutely had to try and trade blows with the 5080 at least to really get into the conversation as anything other than ""it's available and it'll save you a few bucks.""",Neutral
AMD,AI is predominately nvidia cards indeed but so is everything else apart from r/linux users on Reddit,Neutral
AMD,> at some point even Game developer will stop optimize their game for AMD GPU on desktop/laptop.   They already had. Look at for example how many games support DLSS vs FSR.,Neutral
AMD,"I mean, that has been the case for the past decade now. Nvidia, while it still cared about gaming, was far better at supporting developers and providing ""day 1"" drivers than AMD.  That said, with AMD dominating consoles, the gap has shifted. In fact, in the past years, there have seemed to be more issues with Nvidia GPUs than vice versa. Hell, Helldivers 2 recent patch fucked things up for GTX 40-series customers.  But yeah, if you buy games on release, there's a good case for sticking with Nivida still.",Neutral
AMD,"I think it's fine. I mean, this sub is dedicated to consumer hardware.  What else are we going to talk about?",Positive
AMD,Really? The Nvidia sub is literally moderated by employees.,Neutral
AMD,"Possibly,  but reviews as a whole do matter, even if not individually. Streamers also have a lot of....well influence. Some games only became big because of Streamers.  So I wouldn't entirely dismiss ""public perception"".",Neutral
AMD,I'm sure they pay well for that,Positive
AMD,"It comes to the one tweet by HUB claiming that if the 9070XT was sold out at launch, it would be outselling at RTX50 cards at that point (RTX 5090, RTX 5080, RTX 5070Ti and RTX 5070).  Which they kept doubling down on. This was in Q1 2025. When Nvidia shipped 8.5 million dGPUs, compared to AMDs 0.7 million. Thats 11.5 Nvidia GPUs per AMD card.  In Q2, Nvidia shipped 10.9 million dGPUS compared to AMD, which stayed the same at 0.7 million. That's 15.6 Nvidia GPUs per AMD card.",Neutral
AMD,Feels like it's so hard to escape AI articles now,Negative
AMD,"It's a hell of an error, the 5080 was released in Q1.",Negative
AMD,"I assume so, it's just such a weird line. If they meant 5080 that is wrong too though...  It just seems odd for a report about this to get details about the products concerned so wrong.",Negative
AMD,"It’s probably outselling it in DIY, but Radeon is just really rare in prebuilts, which is where the majority of GPUs actually move.",Neutral
AMD,"Intel sold out all they can produce, they just didn't think it would sell that much, GN did a video on this, they make around 10k GPUs per quarter which wouldnt put a dent in nvidia's marketshare",Neutral
AMD,"Not a good comparison when Intel's own management cost them the Battlemage generation. Can't sell what you're not producing, because execs decided to develop yet not launch anything. Only after B580's positive reception did Intel hurriedly resume work and we saw some exotic B580 based offerings, but we never did see a B780.   Never going to win market share with a single budget GPU that wasn't shipped in enough volume to be kept in stock six months post launch. It's in stock today, but it's also against two new GPU generations. Intel really needs to go all in on Celestial, it's not like there isn't a huge potential market just waiting for a good price/performance GPU offering out there across the entire performance range.",Negative
AMD,"Here's why Intel will never make a dent in NVIDIA's marketshare and why their situation is different to AMD/Radeon's.  1. Intel is basically an upstart in GPU, they have zero brand presence or mindshare to build off of. AMD on the other hand has Radeon which has been around for 20+ years. In fact the only thing gamers know about Intel's GPUs is their crappy Intel HD 3000 iGPUs that couldn't run games at playable frame rates. AMD doesn't have this issue.  2. Intel is slow to compete with NVIDIA. Look at Battlemage and how we're STILL waiting on the B770, it might not even release. People are not willing to wait for your product to release, if they want to upgrade, they will upgrade to what is available. AMD also doesn't have this issue, within a month or two, AMD was competing with Blackwell.  3. Intel had only bad press with ARC's initial launch, especially because of the drivers situation. Whilst Intel has tried to improve the drivers significantly and done a great job marketing Battlemage and the product even being solid, first impressions are hard to shake and had Alchemist had a better launch, Battlemage would have sold better. AMD doesn't really have this issue, they have for one or two gens but that was long ago and not anywhere close to the disastrous driver situation Intel's had. AMD drivers for the most part, might have a small issue in a few games on release, but they actually worked and were able to play games. Some games on Alchemist wouldn't even launch or run correctly.  4. Battlemage and Alchmeist doesn't compete across the stack. For what it's worth, only competing with basically the 4060 made Battlemage a sort of pointless generation because if you bought say an RTX 3060 years ago, it's not really an upgrade to buy a B580 or B570. Furthermore, if you have a 3070 or anything else, you literally cannot upgrade to a Battlemage card because it's a downgrade in performance. Competing across the whole stack is essential to getting sales and to convince people that your product is fast. This is probably the closest problem AMD has to Intel, but with RDNA3 they tried to compete across the whole stack, they just got destroyed.",Negative
AMD,"I've never seen any issue with 9070/XT availability though?  It's there, it's in stock, it's expensive.",Neutral
AMD,"> Due to most of AMDs wafer capacity going to CPUs/Servers they don’t have enough for AIBs/laptops. AMD could have 100% sales at hardware stores but still lose out in market share if they aren’t in laptops/prebuilt desktops.  Almost like what we've been trying to tell the AMD fans for years, but we kept getting told by AMD fans that AMD supplies enough chips and that it's just some NVIDIA/Intel cartel keeping them out of laptop and AIB markets for GPUs. Time and time again I kept hearing ""But.. but.. RDNA1/RDNA2/RDNA3/RDNA4 is sold out everywhere! People love it!"". The reality is that AMD doesn't supply enough chips as you said and secondarily that I do think that gamers think Radeon is the 'cheap' brand versus NVIDIA GeForce and they're not willing to spend within $200-$300 of the NVIDIA alternative because DLSS, NVIDIA broadcast, CUDA, the NVENC encoder and RT performance advantage are just too good to convince people to switch for the price that AMD is asking for.",Negative
AMD,"Performance isn't really the concern with the 9xxx series though. It performs well, it just doesn't have the feature parity of Nvidia. It's a great series though. You may not agree with the pricing on it, I'd say that about most things in 2025.",Positive
AMD,"it's all anecdotal internet arguing, but Finance reports should be the one to show the clear picture - Nvidia Gaming in Q2 was bigger than AMD data centers Q2.  I was rocking AMD between 2017-2024, before going to 4070S as DLSS was just really good and people praise, so I decided I want to try that in my gaming. And it was great. As I started using LLM, now I for certain know my next GPU will be Nvidia. Yes I still weight my options on price, but overall for now AMD alternatives are just not that much cheaper in Europe for me (100-120$ difference). So I'm asking myself, would it be worth it to give the gaming goodies in the DLSS suite as well the CUDA I use for LLM and the answer is simple no.",Positive
AMD,"I think it's the other way around. People (online forums and YouTubers) overestimate the amount of mindshare AMD has + their presence in pre-builds. Looking at the online discussions, youd think AMD and Nvidia have a 50-50 Market share split.",Neutral
AMD,"Exactly, it feels like the Radeon team just wants to lose.",Negative
AMD,"Haha, the satire is very well done. People actually comment like this, except the ""dishonest valve"" part, redditors (and AMD people especially) have a huge love-boner for Valve and would never criticize it.",Positive
AMD,"I don't think HUB or GN have said that AMD is going to gain market share this generation, they have just given their opinions about what you *should* buy based on the price/performance of current cards. Them saying you should buy something doesn't mean the majority of gamers are going to follow that advice (and people who build their own PCs and don't just buy prebuilts is already a niche).",Neutral
AMD,"If that were true, Steam would be flooded with RDNA4 because people would need an alternative, but it's not even on the HW survey. That 6% market share looking real.",Neutral
AMD,Nvidia report their data centre and gaming revenues separately.,Neutral
AMD,"> I don't think you have to tell AMD fans that they are never going to beat Nvidia in GPU market share.  Go do me a favor and visit the Radeon (not the AMD subreddit but the Radeon one) and try to convince them of that because they keep making out like AMD can.  > DIY is a tiny fraction of overall sales  I wouldn't say it's a tiny fraction of sales, but sure let's agree it's not the majority. We don't really have the data as to what amount is DIY and what's prebuilt sadly, but let's be conservative and say 1/5th is DIY, that's pretty significant still.  > When you factor in people buying PCs to run LLMs, which is almost always prebuilts from large OEMs like Dell rather than DIY machines, it's no surprise that Nvidia gained market share, and will continue to do so as long as AI is relevant.  Who the hell is buying a prebuilt to run an LLM locally? Almost no one.  Anyone serious about running an LLM is probably renting a server, running a cloud instance or is renting a datacenter. Anyone wanting to try an LLM is probably going to try ChatGPT, or Grok, or DeepSeek out online to ask stupid questions. Or they will go to someone like Lamba or Vast or Linode etc and setup a cloud instance. I would say maybe 0-1% of all people interested in LLMs are going out and buying a prebuilt with an NVIDIA GPU to run one. If you can show me some hard data for this I'd be honestly surprised and happily retract what I said. But it's just not cost effective or smart to go out and buy a prebuilt to run an LLM.  >  People think gamers are obsessed with buying Nvidia, try the boomers who are integrating LLMs into their firms who now see AI = Nvidia and won't buy anything else.  Boomers who are integrating AI into their businesses are most certainly going to some other contractor who does it for them and those contractors likely run cloud instances, not local prebuilts in their clients' offices. Any big customer like a multi-national corpo is also likely looking at cloud or datacenter AI too.  Also Steam HW survey is showing NVIDIA 50 series is buying bought up and absorbed into gaming rigs. Meanwhile the 9070 and 9060 series' aren't even showing on the survey.",Neutral
AMD,"I'm sorry but AMD's best GPU product for laptop is just expensive as heck. The fact a Strix Halo device is like $2200 as a starting MSRP, when you can buy a 5070 Ti Laptop for $1700 or a discounted 5080 laptop for $2500, why the hell would you buy a Strix Halo laptop other than if you needed more VRAM.  Also, AMD constantly fails to make a good dGPU offering in laptop. The RX 7600S was basically in nothing that people could buy because AMD never supplied enough. I think the best device for that dGPU was the Framework Laptop because at least you could remove it later on and upgrade from it. But other than that, AMD was nowhere to be seen because they never supply enough, [GPD complained not too long ago about AMD not meeting their obligations](https://www.reddit.com/r/Amd/comments/1793j94/gpd_accuses_amd_of_breaching_contract_by_not/), so it's why they're not in pre-builts, they piss their partners off.",Negative
AMD,Lmfao 450 sure buddy.  It's also - 150 in most of Europe.,Neutral
AMD,"> RDNA 4 has strong improvements over RDNA 3, and when successors to those key products come out and take advantage of them, they will benefit greatly. Absolutely not a failure.  RDNA4 isn't really in any APUs even Strix Halo which is AMD's best ""APU"" is RDNA 3.5.  Like always with AMD APUs as HUB says they don't make sense economically and you pretty much always get them late in the cycle compared to dGPU architecture.  > And the fact of the matter is that the 9070XT is a good card, and the 9060XT is a pretty great one.  Thats subjective and relative really. Good compared to bad competition.  >I think you’d have to be an uninformed idiot to buy the comparably priced Nvidia card in both cases.  Not really, you get NVENC encoder, CUDA, NVIDIA Broadcast, DLSS, MFG, better RT performance and the ability to flex you bought NVIDIA.  > The market is fully in fuck around and find out mode at this point, in my opinion. AMD will never go away due to the key products I mentioned, so they’re just going to keep their heads down and do as they do. It literally just is what it is.  Key products? Consoles only help console fans, doesn't help PC gamers and AMD's APUs are always expensive like Strix Halo and late to the market compared to their dGPU offerings as I said earlier. The only PC Gamers APUs help is handheld players who are a small part of the PC Gaming market.",Positive
AMD,Dumbest comment ever. You act like gamers run local LLMs on their rtx 5070...,Negative
AMD,"> Yeah how well has selling at low margins worked for Intel?  The reason no one is buying Intel for DIY is because their product is garbage, it's literally worse than last gen (Raptor Lake vs Arrow Lake).  However, they're doing well in pre-builts and laptop because they actually care about their OEM partners unlike AMD and they also have a good product there, like Lunar Lake is actually very solid and is a performance leader in the area that matters in laptop which is battery life.  >They've been fire selling CPUs and GPUs for years and it's losing them tons of money to the point they've sacked off half of their workforce and are in dire straits begging for subsidies from the US gov just so they can keep their fabs going.  That's only because Pat over-invested, the board didn't want to invest more money, but Pat said it was part of his grand vision and sold it to the board, then halfway through his grand plan the board decided to tell him to leave and go in a different direction. Not to mention, Intel was planned Government subsidies that they didn't receive in time that has affected this whole grand plan of Pat's.  It's not as simple as ""hurr durr, Intel has low prices and low margins and now they're broke"". Even then, Arrow Lake isn't exactly going for a fire sale either in terms of global pricing, in most regions it's hovering at MSRP, it's just that no one has a reason to ""upgrade"" to it because it's trash.  > You are forgetting that Nvidia have a R&D budget far larger than anything AMD or Intel can afford to spend on developing GPUs right now.  You AMD fans always say this inane excuse, yet somehow you also say AMD has slain the Intel giant with 1/20th the R&D. R&D money is great and all but only if you use it wisely as AMD has in CPU and they could replicate that same success in GPU, but they ignore GPU division pretty much entirely.  > If AMD cut RDNA4 prices to bare minimum, how would they afford the even larger budgets for future architectures like UDNA2, UDNA3, etc?  Well AMD is in a unique position because they have more than one product segment to fall back on. NVIDIA lives or dies by GPU, if the GPU they produce is utter garbage and GPUs fall out of favor in the wider market, their entire product portfolio is at risk and their future. AMD on the other hand has CPU to fall back on, consoles and handhelds. Considering how well AMD's CPU division is going right now, they can easily take away some (not all), but some resources from CPU and funnel it into GPU to improve it. So yeah you take low margins for a while, but because your whole company is buoyed by your CPU success, you can afford to do it while you improve the second division.",Negative
AMD,I could see it being mind factory. Their GPU sales data is like the opposite of the world,Negative
AMD,yeah what a clown,Negative
AMD,"> They are reviewing the products on their merits.  Not convinced, personally. You see a lot of vendetta.",Negative
AMD,I see so many people on reddit treating reviewers like they're market analysts,Negative
AMD,"Rotten tomatoes ""experts"" are notorious for being completely out of touch with the wider market often being diametrically opposed to the wider reviews even on the same site.  The notoriety often has connotations of pretentiosness and so on. A perfect example",Negative
AMD,"> They are reviewing the products on their merits.  But youtubers disabled features on one product because if they left it on it would embarrass the other side. That isn't about merit.  How about actually talking about the markets to explain why one company might not be selling as well as another, naaaaah ""AMD will outsell NV trust us bro!""  Nah these people aren't doing anything on merit anymore. I'm not accusing them of taking a pay out, but whatever is making them essentially handicap one side to give the other side a boost clearly isn't working and it's only burning their credibility in the process.",Negative
AMD,"A key part of a reviewer's job is to say ""Is this worth the money"" - if they can't actually determine what the average audience feels is 'worth the money' they are fundamentally ill-equipped for the job. To use another relevant example, every fucking youtuber said that the Switch 2 was too expensive and now it's one of the fastest selling consoles of all time. Clearly there is a massive disconnect between reviewer's beliefs in what the market is and what the market actually is.   Trying to compare this with a purely qualitative measurement like 'is a marvel movie good' is laughable.",Negative
AMD,"Movie buff scene is hilariously out of touch though. Youll still find them rambling about citizen kane because it did X Y and Z advancement in movie history 90 years ago when everyone has long forgotten both those techniques and the movie itself. If you want a more modern example look at hilariuosly bad takes on the start war sequels. Its not that they had different opinions, its that they got almost everything factually wrong.",Negative
AMD,Its not though. At least here in eastern europe its NV + $100,Neutral
AMD,"I think by now it's just too late to compete simply on price. The last time ATI had almost half of the market share was having a well priced product with mostly feature parity with its competitor.  Until RDNA4, AMD didn't have feature parity and barely a well priced product.  Now with all the features expected, the cost of production, and AMD still having to use more expensive nodes/process to compete there is no way AMD can compete on price.  You saw this with how they reacted to RTX 50. They probably assumed they had a nice price set to compete only for NV to come in less than just about everyone expect sending AMD back to the drawing board and the end result was a product that they had to rebate to even honor the price they set. Now, they aren't even shipping in abundance to satisfy demand and thus reduce price.",Negative
AMD,It literally doesn't even matter what AMD sells. They could sell a reticle buster 2nm at BoM cost and still lose to NV because DLSS stands for dick lick suck suck or something and AMD cards can't do dick lick suck suck and clearly that's what gamers want.,Negative
AMD,"GN is ...... not entirely emotionally removed from this.  Aside of that, you have also seen the Steam numbers yes? They would be speaking entirely different things if that was all there was to the story you know. There should be more to this than that.",Negative
AMD,Yes Steam is a requirement for all AI farms.,Neutral
AMD,Agreed. I find 8x 5090 servers great for my use case (which isn't AI). They perform well at a reasonably low price. Performance for money they're about 3x better than the likes of H200 or B200 as long as you're okay with the much smaller amount of VRAM. IMO 32GB for the top desktop GPU of this gen makes the 5090 more viable that last gen.,Positive
AMD,I wouldn't be surprised if they're also subsidized by the Canadian government for not moving their Radeon divisions closer to Silicon Valley,Neutral
AMD,"For APUs sure, but not for high end GPUs.",Neutral
AMD,Well that was unexpected.  Thanks for the link,Positive
AMD,"The last truly competitive cards AMD made in terms of sales were the GCN 2.0 cards like the R9 290 and 290X  The R9 290 matched or best Nvidia's GTX 780 in games, and Nvidia was forced to respond with the GTX 780 ti  AMD remained competitive in the high end until Nvidia crushed AMD's R9 Fury X with their Maxwell based GTX 980 ti.  AMD failed to respond to Maxwell and then Nvidia sealed AMD's fate with Pascal (AMD didn't respond to it either)  The Vega 56/64 was a joke compared to the GTX 1080 and especially the GTX 1080 ti   AMD finally released their massive uarch rework called RDNA in 2019, however it was too little, too late as Nvidia introduced RT and DLSS with Turing.  Conclusion:  AMD entered a terminal decline with Bulldozer. it's CPU division recovered due to AMD's great decisions and stagnation from Intel  However, Nvidia didn't stagnate and so AMD hasn't recovered.",Neutral
AMD,"Yeah that's what I suspected, radeon seemingly has sold well in diy but I have no idea who big the pre built market it(apparently it's bigger than you'd think). It reminds me of when Intel was everywhere but it's different because nvidia are competent",Positive
AMD,"bought it to experiment with AI, even if it's slower i can run far larger models than 16gb",Positive
AMD,ALL OF THE ABOVE.,Neutral
AMD,Wait for <next generation> is the Radeon fan battlecry.,Neutral
AMD,">Wait for RDNA5/UDNA at this point.  hahaha, I dont think that will disrupt consumer's choice.  AMD gonna need to sell +50% better price/performance than Nvidia to even move things at this point.   Just like how they did on Ryzen, even with all that Ryzen still have yet to get 51% total CPU market share.",Neutral
AMD,AMD must have had some seriuos issues with either stock or drivers for that delay and the stock seems to have existed in January so probably not that.,Negative
AMD,Even reddit linux users know that Nvidia on Linux isnt that bad anymore on the mainstream distros,Neutral
AMD,Nothing's changed there though. The skew towards NVIDIA is far more likely to be the result of the AI boom than RDNA4 being a failure which seems to be the conclusions being drawn from this article. It's a little disingenuous to refer to 2010 numbers when AI wasn't even a thing for mainstream solutions.,Negative
AMD,"> Nvidia, while it still cared about gaming  But Steve of GN said Nvidia is ruining gaming!  Ignoring that in the last 7 years to me the roles of Nvidia and AMD (or rather legacy ATI) swapped.  NV introduces new features, pushes the PC-focused APIs forwarded, and has a bunch of things in the pipeline.  Meanwhile, AMD was not even matching said features 1:1 and introduced their own inferior versions. They sold their userbase said inferior versions at almost NV prices whenever they could and Youtubers cheered them on!  Now we're at almost feature parity and AMD can't even muster the commitment to their products to ramp up production in a timely matter.  But let's keep praising AMD, without them NV would be a monopoly (or something).  How does it go?  ""EL OH EL, enjoy paying $2500 for your 6060 Ti""  Meanwhile ignoring if that were even remotely true, there would be a faster in raster but slower at everything else $2400 RX X060 XT in AMD's product stack.",Negative
AMD,">I mean, that has been the case for the past decade now. Nvidia, while it still cared about gaming, was far better at supporting developers and providing ""day 1"" drivers than AMD.  Funny thing. somewhere in 2014-2015 something happened at AMD and it completely dropped any support to developers. It was so bad that many developers complained publicly about this and of course Nvidia used the opportunity to ""help"" the developers instead. Sent their own engineers. Guess what the games were optimized for! Watch Dogs is a very public example of this and the shitfest that followed.  AMD has been trying to get the relations with developers back for some time now, but its clear some developers are just burnt from past experiences.",Negative
AMD,"The issue isn't the topic, it's the presentation which leads to the dismissal of proper data because it either contradicts what the Youtubers said or completely ignores their position.  Just use this topic of marketshare, Steam Survey has been tracking this info for years but it is disregarded because ""HUB said AMD will outsell NV's whole line up!"" That didn't manifest in Steam Survey.  When the JPR numbers came out to basically explain why the Steam Survey showed the data that it did, oh now ""JPR is not a reliable source"".  But Mindfactory, hold up it shows AMD in great shape thus this is defacto truth!  Discussion is talking about the merits of a product. Responding to delusional posts about where the market should be based on a rumor mill or what should become evident - clueless Youtubers - is tiring. It's pointless, and in the end just makes a good chunk of the participants look ignorant of a hobby/topic they are likely investing thousands of dollars and worst millions of hours on.",Negative
AMD,"yes, among us only became popular because big streamer picked it up and it snowballed from there. But streamers are not reviewers, streamers arent trying to make an objective measurement of the game.",Neutral
AMD,">which they kept doubling down on  Where? I dont use Twitter, but it's no secret RDNA4 sold very well at launch specifically because they delayed their launch and had a huge stockpile of inventory.   But where has HUB argued that their sales figures have remained competitive post-launch through to today?",Neutral
AMD,if you look at newegg best sellers and mind factorys posted numbers this marketshare data doesnt add up. Nvidias shipping gpus but not to gamers.,Negative
AMD,and guess who's the backbone of ai?  nvidia.,Neutral
AMD,"First they fired editors, then site IT, now the writers too.",Negative
AMD,"> It's a hell of an error, the 5080 was released in Q1.  So was RDNA4 technically, but people dismissed the initial Q1 numbers because they said most of the RDNA4 volume would ship in Q2 and look how that turned out lol.  It's just a misprint by JPR I wouldn't put too much weight into the text, it's probably AI generated. The numbers and charts are all that matters.",Negative
AMD,JPR makes mistakes like this all the time. And data mistakes. They're a pretty patchy outfit.,Negative
AMD,"Your boy MLID has Jon Peddie on all the time, guess he's an NVIDIA fan too right?",Neutral
AMD,"> they make around 10k GPUs per quarter which wouldnt put a dent in nvidia's marketshare  You got the number wrong. It was a single supplier talking about monthly sales going up by 3-5 times from 2,000 for Alchemist.",Neutral
AMD,BMG-G31(B770) is likely to come at some point since we see it in Intel's driver stack   Likely in Q4 2025   **The situation leading up to launch:**  Xe3p DGPU's were likely canceled after Intel's disastrous mid year Q2 2024 earnings call.  **Shareholders demanded layoffs and funding cuts and then CEO Pat Gelsinger cut Xe3 DGPU'S and then planned to relegate the Arc brand to laptop iGPU's**  Intel's leadership then prevented the already complete BMG-G31 from getting taped out and launched.  The B580 likely only survived because Intel already ordered 5nm wafers. Intel likely expected it to flop or at best have lukewarm reception.  B580 launch:   *Intel did not expect every single B580 to sell out on launch day and for the TREMENDOUS demand that followed.*  **Intel badly misread the market**   **Intel then likely hurriedly restarted Xe3P discrete GPU development and begun tape out of BMG-G31 (B770)**  *That's why we're seeing leaks about Nova Lake A and AX big iGPU tiles in 2027 but NOT Celestial DGPU'S in 2026*  *IF we get Xe3P DGPU's they will likely be using the same dies as the big iGPU's and they would likely come in 2027 or 2028*,Neutral
AMD,"Except they're not making any money selling battlemage at those prices are they? It's pointless selling it for a loss or even a tiny profit when you have to compete against Nvidia and their huge profits. That's what you're forgetting.   Nvidia have a R&D and manufacturing budget of tens of billions per GPU arch, AMD simply does not have that luxury, and Intel in their current state certainly does not.",Negative
AMD,"> In fact the only thing gamers know about Intel's GPUs is their crappy Intel HD 3000 iGPUs that couldn't run games at playable frame rates.  This is a really good point. In my mind the immediate association between Intel and graphics is not a positive one at all, they make crappy iGPUs.  I know they make more than that now (and their iGPUs aren't even that bad... kinda) but it's a long association going way back to the early 2000s now.  To this end coming up with a new brand for the dGPU division might be a smart move, there's just no positive association in using the Intel brand name for graphics cards.",Negative
AMD,> Intel is basically an upstart in GPU  Intel is dominant in integrated graphics for a long time. They have some experience.,Positive
AMD,"It's expensive and selling above MSRP because demand is outstrippng supply. Given it's not showing up in the data, be tht JPR or Steam, the only sensible conclusion is that AMD is making very few GPUs.",Negative
AMD,"> DLSS, NVIDIA broadcast, CUDA, the NVENC encoder and RT performance advantage are just too good to convince people to switch for the price that AMD is asking for.  I think you are completely wrong with this AMD is prolly doing great in the PC building space right now, this is more of a reality check of how small the PC building space is compared to prebuilts/notebook.  edit: when i got a 9070xt in germany it was 200€ cheaper than the 5070ti btw.",Neutral
AMD,"RDNA4 was already a huge leap in features though. FSR went from subpar vs DLSS 2 from 3.1 to beating DLSS 3 with FSR 4, RT performance significantly improved and is now half way towards matching GeForce vs where they started and FSR Redstone will come eventually for ray reconstruction and whatever equivalents.  Now yeah it's still sorta behind especially before Redstone comes but it's not the massive disparity they had during RDNA1, 2 or 3 anymore. Or what do they need an entire generation for the casuals to get it into their heads that FSR isn't bad vs DLSS anymore?  But I guess AMD has to push hard with RDNA5/UDNA to CLOSE the gap fully or very close to it, and then ideally really have great MSRPs and stick to those prices with good supply or else there'll be another wasted generation of low marketshare where only the console business and iGPUs really justifies continuing the R&D costs of new architectures. But I guess UDNA is supposed to make it so their server and business stuff contributes to gaming dev anyway by unifing them.   I do think RDNA4 could have still been selling well if AMD just produced lots of GPUs at MSRP, and maybe lower the the 9070's MSRP so it's undercutting the 5070. 9060 XT however isn't selling much it seems even though it's probably the best GPU in its price class, guess they needed to price match the 5060 with the 16GB model? I dunno, that definitely would have been a very compelling GPU at that point.",Positive
AMD,"Cheapest RTX 5070 Ti in Finland is  840€ compared to 680€ RX 9070 XT. That is 160€ or almost fifth off the Nvidia price for same performance. I wonder how much cheaper does AMD offerings have to be before they are considered ""reasonable"" in the eyes of gamers.  > As I started using LLM, now I for certain know my next GPU will be Nvidia.  Nvidia has clear advantage in machine learning thanks to CUDA but one would be fine using AMD cards for simple LLM inference. I get about the same performance using llama.cpp with Vulkan and ROCm backends and we know Vulkan inference doesn't trail far behind [CUDA](https://www.phoronix.net/image.php?id=2025&image=nv_vulkan_llamacpp_2). Simple machine learning projects with Pytorch/Tensorflow (which are probably the vast majority) also work fine with ROCm.  E: Corrected 5070 Ti price.",Neutral
AMD,"If you go online you'll think people all have AMD cards, on Linux, surfing with Firefox. Well reality tho...",Neutral
AMD,more like love hate relationship with valve considering how they criticise steam hardware survey of “not being accurate” or “representative”,Negative
AMD,It's not on the list because the all get reported as integrated graphics lol,Neutral
AMD,"Yes but I think they mark all their gaming GPUs like 5090s as gaming share while a huge amount of them go to AI, they dont know where they are going.",Neutral
AMD,"[All of these RTX 5090s will go on the ""gaming revenue"" line.](https://www.tomshardware.com/pc-components/gpus/rtx-5090s-get-ready-to-blow-with-ai-focused-makeovers-in-china-industrial-production-lines-transplant-gpu-and-memory-from-gaming-cards-onto-server-ready-pcbs-with-blower-style-coolers)",Neutral
AMD,"Businesses who handle proprietary data sets and can't justify the costs of an entire server are using local machines, maybe that's a niche use case, but it's what I have experience with so that's what I drew on. I'll concede on that point, most LLMs are probably running on server hardware, not local.   I won't concede that 1/5 is a conservative estimate for DIY to prebuilt sales though. I'd say 1/10 is realistic if you look outside the US centric reddit hardware bubble. You have to remember that Internet cafes in Asia are extremely popular in densely populated cities where the average apartment doesn't have space for a gaming setup, and they buy prebuilts by the pallet. Maybe in the US it's 1/5, but the rest of the world is far more heavily skewed towards prebuilts.   The main point is I agree with you, AMD is never going to catch up with Nvidia in dGPU market share, anyone who thinks otherwise is delusional, even in the main AMD sub this is the prevailing opinion. I would even go as far as to say AMD could make a 6090 killer next generation for $999, and their market share wouldn't increase by even a single point because there are so many consumers who mindlessly buy Nvidia or buy prebuilts/laptops which might as well be 100% Nvidia at this point.   Lowering prices did nothing for AMD in the past, and it won't change anything now. AMD will just change their wafer allocation to favor CPUs and still sell every GPU they make at current margins. Why bother when the Nvidia fans who cry for lower prices from AMD have shown time and time again that they will just wait for Nvidia to lower their prices before buying Nvidia like they always have? Lowering margin only hurts their R&D fund for the next generation.    The irony is that the real losers of this arrangement are GeForce fans, Nvidia has no incentive to produce outstanding gaming products when they're this far ahead. They can afford to lose a few generations and have their market share fall to a precipitous 90% before they start trying again, hell they can afford to lose the entire mid-range and budget markets permanently as long as they have their $2000 5090 marketing prop that only a fraction of people can afford. I don't feel sorry for them though, at the end of the day they do it to themselves.",Neutral
AMD,They're not in the survey because they get reported as integrated graphics if you're not on Linux.,Neutral
AMD,7700 xt same die size as 9070 xt so hey :) margins matter instead of market share,Positive
AMD,"RDNA 4 isn’t planned to ever go in APUs, as far as I know. Obviously the new RDNA 5 APUs wouldn’t have been possible without RDNA 4 first though, right?  And, both cards that AMD launched this year were good value for what they were. That’s all there is to say. What the market did with that is up to the market, and that’s why I consider this fuck around and find out mode.   Obviously we can each have an opinion on who’s offering the better deal, but I think it’s pretty amusing that you think AMD is out there to “help” anyone. That will never be the point of any product that they release, because “helping” the consumer is inversely proportional to profit, and that’s a relationship that’s **extremely** easy to maintain in a duopoly. That goes for both ends of the spectrum.",Neutral
AMD,I literally run a distilled llm on a water cooled 3090.,Neutral
AMD,"I do run LLMs on 4070S and yes, I do game mostly. But the option is there if I need it.",Neutral
AMD,It's a single e-tailer in Germany. I'd never use a single shop that almost exculisvely ships within Germany as a source for worldwide sales numbers.,Negative
AMD,"There's certainly bias for sure, but they arent paid promoters which is the crux of my point.  People like MLID clearly not an unbiased source. HW unboxed pretty clearly an editorial slant there. But i doubt they pin their ego on AMDs revenue figures.",Negative
AMD,"Worse, they're influencers. I can't imagine how ashamed I would be as an influencer if I - and all my colleagues - spent years babying radeon like this and telling viewers to buy Radeon and yet somehow that entire time saw Radeon get the weakest marketshare in its entire history. It would genuinely be a 'come to jesus' moment on how irrelevant you are.",Negative
AMD,The purpose of a review is to inform the consumer on whether a product is worth buying. If they are failing to do this they are bad reviewers.,Neutral
AMD,RT critic and audience scores started diverging around the time of the culture wars kicking off. Almost like unfiltered online review systems are open to abuse from trolls.  Cinemascore polls real audiences in person and doesn't diverge from critics as much as the RT audience score does.  The fact you bring that up just reinforces my point. Loud online opinions don't have any impact in the real world for better or worse.,Negative
AMD,I really hope you arent talking about MFG.,Negative
AMD,"It’s funny how tech-bloggers never counted DLSS as important feature aswell, done raster tests and claimed that AMD is better value for money… and now things turned around with FSR4 being exclusive to RDNA4 while DLSS4 working on every RTX gpu. Slowly they’re admitting that rx6000-7000 cards aged poorly, but i doubt it helps those who were mislead into buying “great value” gpus, lol",Neutral
AMD,> Clearly there is a massive disconnect between reviewer's beliefs in what the market is and what the market actually is.  Ok...? They are product reviewers not market analysts. They aren't setting pricing for the devices or providing analysis to the brands on how to sell their hardware. They are offering opinion based analysis of the products with some quantitative stuff tacked on for end consumers. No person decides not to buy something because some other person says its too expensive.,Negative
AMD,"Popularity is not strongly correlated with merit or quality, typically. While I agree the righteousness of techtubers of late is hard to stomach, they can be right that a product is crap or good and you should or shouldn't buy it even if the market decides otherwise. Consumers en masse can act against their own interests. And often do. Eventually, when the impact of those actions becomes particularly onerous and painful, there will also typically be much wailing and gnashing of teeth about corporate abuses and so on. And sometimes that's true. But sometimes it's also true that a bunch of turkeys spent years voting for Christmas and then complained when they end up roasted.",Negative
AMD,high quality contribution,Positive
AMD,> subsidized by the Canadian government  wouldn't that be public information?,Neutral
AMD,"Oh, so they still do a bulk of their GPU RnD in Canada?",Neutral
AMD,"I mean the architecture is scalable, so whether it's for an iGPU like the PS6 APU or for a dGPU, the architecture is the same pretty much, they just make a different die/mask depending on the product. It's even rumored that AT2 chiplet die for RDNA5/UDNA is used on both desktop dGPUs and for the new generation Xbox for it's APU.  So yes, basically Microsoft, SONY and Valve are bankrolling Radeon's R&D.",Neutral
AMD,"Agreed. I also think AMD lacks the resources to take on both Nvidia and Intel at the same time. When one part of the business is doing well, the other side struggles. Early 2010's Radeon was doing well and Bulldozer struggled. 2020's Ryzen is doing well while Radeon struggles.",Negative
AMD,"As a retired fanATIc, I'm well aware of that. :(",Negative
AMD,What AMD need is consistency. One good generation does not sway consumers. You need to be exellent 3 generations in the row at least until you can start swing market share. You have to earn the trust of consumers to make them switch.,Neutral
AMD,"My comment was more on AMD ramping up production. I don't see them bothering to increase RDNA4 production outside of the current trickle.  My Crystal Ball isn't good enough to conclude how a properly supplied RDNA5/UDNA product can do to mind share and price, but it can only do better than a limited production RDNA4.",Negative
AMD,That is all data centre cards. This report is only considering desktop cards. Meta/Microsoft/twitter/etc aren't influencing this.,Neutral
AMD,"Failure to capitalize on the AI boom would certainly fall under the umbrella of ""RDNA4 being a failure"".  And yes, the entire software stack / ROCM is a part of that - even if thats out of the hands of the team that specifically designed the hardware.",Negative
AMD,"Its even worse when peope use the videos that are wrong as some kind of source to prove you wrong, when all they are doing is showing themselves to be wrong, but they are not realizing that.",Negative
AMD,"Man, did HUB run over your dog or something? You trying to shit on them all over this post is way more insufferable.",Negative
AMD,what makes jpr's more accurate than every store clearly showing nvidia doesnt have anywhere near 100% marketshare in diy?,Neutral
AMD,"I think the notion that any review is ""objective"" died a decade ago.",Negative
AMD,"> but it's no secret RDNA4 sold very well at launch specifically because they delayed their launch and had a huge stockpile of inventory.  Every time we hear this on an AMD launch. I'm just going to break it down and make it simple to understand for you guys so you stop repeating this nonsense every launch because something sells out on Day 1.  If 'Company A' ships 1 million units and 'Company B' ships 1,000 units and both are sold out after a day, it doesn't mean Company B sold well and had a huge stockpile. It's really evident Company B is NOT shipping enough stock to meet demand.",Neutral
AMD,"I thought Pat said ""AI everywhere"" though?",Neutral
AMD,What does that have to do with anything?,Neutral
AMD,"Yeah, 8% shipment for Q1, ""no they are holding it all back for Q2"" and now Q2 is 6%.  Prepare for just more ""Steam survey isn't accurate"" posts and ""Mindfactory AMD 99% dGPU"" rhetoric!  Shoot even the Youtubers are now citing Mindfactory data!  Edit: typos",Negative
AMD,"The point still stands, from /u/KolkataK Intel doesn't make enough to compete with NVIDIA or even with AMD. NVIDIA ships millions of units a quarter, AMD over 600 thousand units a quarter. Intel might be lucky to do 100K a quarter.",Negative
AMD,"I'm taking the view Intel execs badly misread their own product's competitiveness and tried to save a few bucks by canceling it early. Either that, or they knew the big B770/B780 has outsized drawbacks & CPU overhead problems that simply can't be overcome.   It doesn't make sense to launch a B770 or B780 six months away from a C780, so it really does depend on how much of a time gap there is remaining before we see Celestial. And Celestial has to launch in 2026, Intel can't wait until 2027, or even the end of 2026 really.",Negative
AMD,"I'm not forgetting it, you're just dodging by changing your original point. Intel can either sell at a loss or breakeven in order to gain market share, or they can do nothing while burning R&D funding and time. Whether or not Intel is making anything off Battlemage is a different issue, your post originally focused on market share so profits gained is irrelevant. Most companies initially sell at a loss when forcefully breaking into a mature, well-established market anyway, the rule even applies to restaurants.   Anyway, if it was simply an issue of money, size, and funding the world would be over already, no new startups could exist and nobody could ever break into an established, monopolized market. Which clearly isn't the case, NVIDIA has left plenty of space with its profit margin obsession for a more efficient competitor to exist. Intel just has to have a good enough product it can afford to sell and the right executive decision making to apply it.",Negative
AMD,"> This is a really good point.   Thank you.  > In my mind the immediate association between Intel and graphics is not a positive one at all, they make crappy iGPUs.  Yep, really until Meteor Lake Intel iGPUs have been pretty much just as a display output, not really for any serious graphics tasks. Maybe Tiger Lake started the whole better iGPUs, but Lunar Lake has pretty much made a perfectly useable iGPU for some legitimate gaming.  > To this end coming up with a new brand for the dGPU division might be a smart move, there's just no positive association in using the Intel brand name for graphics cards.  Well I think that's what ARC is, like GeForce and Radeon, it's just going to take some time to get that brand presence. But like I said above, Intel is basically an upstart in dGPU, they have nothing to really build off in the eyes of consumers, so they have to make a really killer product some day to get that attention in the public's eyes of ""oh yeah this brand makes a solid offering"". Going to be a while before that happens as AMD and NVIDIA have 20 years of history to build off of in dGPU.",Neutral
AMD,"Those HD 3000 iGPUs weren't the same architecture as ARC Alchemist, the drivers were always trash for games on those iGPUs and honestly they basically ran games like a potato.  Also just because you do some graphics, doesn't mean you're going to be successful at scaling that up. I mean look at Qualcomm they have probably the best GPU performance on mobile phones and they absolutely bungled the X Elite drivers and performance in graphics on Windows. Just because you have a ""graphics"" product, doesn't necessarily mean you can make a capable gaming dGPU to compete with AMD and NVIDIA.  All those Intel iGPUs were really for was for Quicksync, video decode and desktop use really.",Negative
AMD,Yeah wtf lol. Intel has had several dominant periods across the AMD v Intel cpu wars.,Neutral
AMD,"> It's expensive and selling above MSRP because demand is outstrippng supply.  That is not what retailers have said, they've all confirmed the initial batch of a few hundred units were subsidised directly by AMD. Once that ran out (about 5 minutes after launch) the prices shot up and have stayed there since there was no longer any price rebate.",Negative
AMD,"> edit: when i got a 9070xt in germany it was 200€ cheaper than the 5070ti btw.  I love you guys, always ignoring evidence and numbers and going with your own anecdotal experience as if it's gospel.",Positive
AMD,The post you commented upon literally is about PC GPU sales. We just don't know the pre built/diy split. Even then no doubt Nvidia is selling more than amd.,Neutral
AMD,">  Or what do they need an entire generation for the casuals to get it into their heads that FSR isn't bad vs DLSS anymore?  More support and marketing perhaps. I have a 9070xt and none of the games I play have FSR4 within it. I can add it to some games via Optiscaler, though.  I honestly just think that most people who game don't even know AMD makes GPUs. 3 of my FPS only gaming friends buy prebuilts every 3-5 years and AMD is almost never an option, at least not usually the options out in front.  Even when AMD was killing it with their 2600x, 3600x, 5600x, and then 5800x3d, my friends still said to me ""Why didn't you go Intel?"" not knowing that the 5800x3d was the best gaming CPU at the time.  If it's not out in-front of customers on the shelves, then people don't have any idea to research it.",Neutral
AMD,"The issue is that Nvidia does so many software offerings that a lot of people also get locked in because AMD didn't have an equivalent. I'd argue AMD didn't have an actual competitor to DLSS2 until FSR4, several years later. The existence of opticaler with FSR4 means that I don't really consider upscaling as part of my buying patterns anymore but for the past 5 years? Yeah they did.  It's also been years and there still isn't an AMD equivalent to RTX HDR, RTX VSR and DLDSR for example. RTX HDR alone is so useful to me that I can't move a media PC to AMD and stick with AutoHDR.",Neutral
AMD,The 9070XT competition is 5070 (non-ti) though.  >Simple machine learning projects with Pytorch/Tensorflow (which are probably the vast majority) also work fine with ROCm.  How to tell everyone you havent used ROCm without telling everything.,Positive
AMD,"I don't think it's much about price, I genuinely don't think half the gaming community knows AMD makes gpus. For years a lot of people forgot they made CPUs also.",Negative
AMD,Don’t forget old.reddit.com with extensive custom lists in Reddit Enhancement,Neutral
AMD,But also the steam deck is the greatest thing known to man because using FSR with 360p base resolution is the way,Positive
AMD,>It's not on the list because the all get reported as integrated graphics lol  [Another debunked AMD fan lie.](https://old.reddit.com/r/hardware/comments/1n70kxx/jpr_q225_pc_graphics_addin_board_shipments/ncatylg/),Negative
AMD,Even if you discount every single 5090 and 4090 and 3090 nvidia is outselling amd 10-1,Neutral
AMD,Will be a drop in the ocean compared to the number of 5060s and 70s they sell however.,Neutral
AMD,You are missing the point - those are brough for AI because the cards are capable of AI along gaming as well. Nvidia is creating a product that is desired. AMD is not.,Neutral
AMD,"> Businesses who handle proprietary data sets and can't justify the costs of an entire server are using local machines, maybe that's a niche use case  Niche case I'd say. Hiring a Linode instance or something like that for a couple hours or a week is way more cost effective than going out and buying a whole new set of hardware, especially just for experimenting and seeing if a LLM or AI model is feasible for their business or to prototype one they're making etc.  Or perhaps just outsourcing to another company on a subscription-based model with hundreds or thousands of clients who maybe tailor their LLM or AI model for certain clients and their data. I've had my cousin's law firm go out and get AI assistants and I suggested he setup an instance tailored to his firm. But the firm did some digging and found a local company who just has hundreds of AI assistants that they lease out on a subscription model and who tailor their AI assistants to a specific style of business and they customise it so it knows who the people are at the business, the business name, emails go to specific business accounts etc. It was a few thousand a year to use this AI assistant company, but the firm don't have to troubleshoot or maintain it and you get free included bi-annual performance upgrades for the hardware that's running the model, and obviously they update the model too and make tweaks after they do testing to see if they can deploy the model to their clients. In the end, it's just a secretarial replacement really, so all the model needs to do is basic stuff like note down the name and number, who the person wants to talk to and forward emails. Nothing too crazy or extensive. So maybe this is a basic scenario.  Either way, I don't think it's feasible to go out and buy bare metal hardware, especially for a small business. And any large business it's probably better for them to go to Amazon or Google or Microsoft and setup some huge datacenter to do whatever AI thing they want for some multi-million dollar deal.  > I won't concede that 1/5 is a conservative estimate for DIY to prebuilt sales though. I'd say 1/10 is realistic if you look outside the US centric reddit hardware bubble.  I just spitballed a number. It could be 1/10th as you said or even 1/20th etc. In the end, it's significant still for DIY, certainly millions in revenue that you shouldn't ignore it if you're NVIDIA or AMD. Plus DIY buyers tend to be the most loyal customers, so if you win their heart or mind, they're likely to return. In my experience, prebuilt buyers tend to just go where the value is because quality is usually subpar anyways in that market segment.  > You have to remember that Internet cafes in Asia are extremely popular in densely populated cities where the average apartment doesn't have space for a gaming setup, and they buy prebuilts by the pallet. Maybe in the US it's 1/5, but the rest of the world is far more heavily skewed towards prebuilts.  I mean most netcafes I know of in Asia don't buy pre-builts from an OEM like Dell or HP or something, most of them go to a local builder in one of those huge techmalls and puts in an order for 3-4 machine types/tiers but hundreds of units. i.e basic office, then basic gamer, then moderate gamer and then like high end gamer rigs. They put in an order of 1,000 PCs and they might buy 100 office PCs, 300 basic gamer ones, then 500 moderate gamer ones and then 100 high end gamer ones to have different tiers in their cafe. But they're all DIY rigs really, just from a local techmall shop who cranks them out and services/warranties them. I've never seen an asian netcafe buy an HP or Dell prebuilt in years and things like iBUYPOWER or Origin PC aren't really popular in Asia as the DIY prebuilt market is huge there and it's all local small shops vying for business. At least that's my experience from when I used to live in Taiwan. The last time I saw like a netcafe use prebuilts, like proper HP or Dell OEM ones was the early 2000s and it was usually smaller cafes that didn't really cater to gamers.  > The main point is I agree with you, AMD is never going to catch up with Nvidia in dGPU market share, anyone who thinks otherwise is delusional, even in the main AMD sub this is the prevailing opinion.  I wouldn't say thats the prevailing opinion over there, maybe the slight majority, but a lot of them are still believing that it's 2008 and the only reason AMD is behind NVIDIA is because of some marketing campaign or behind doors deals, rather than AMD's own lack of prioritising dGPU.  > I would even go as far as to say AMD could make a 6090 killer next generation for $999, and their market share wouldn't increase by even a single point because there are so many consumers who mindlessly buy Nvidia or buy prebuilts/laptops which might as well be 100% Nvidia at this point.  I don't think so. I think they could make a 6090 killer for $999, the problem is will they supply enough to make a dent in NVIDIA's marketshare and considering how intent AMD is on using TSMC, I don't think that will happen. It all goes back to AMD insisting on using TSMC, they need to diversify their foundry and if they want to take marketshare it might mean going to Samsung for your dGPU gaming products and getting cheaper but lower performance silicon to undercut NVIDIA. It won't happen because AMD probably doesn't want to ruin their CPU dominance and their relationship with TSMC is too important, so they will continue with TSMC. But also because AMD is moving to UDNA for dGPU, which means they are pretty much forced to having a unified architecture on one node, which now limits their foundry options. If they choose a foundry ALL their graphics products have to use it and I very much doubt AMD wants their professional stuff nor their consoles to use Samsung or Intel foundry.  > The irony is that the real losers of this arrangement are GeForce fans, Nvidia has no incentive to produce outstanding gaming products when they're this far ahead.  Absolutely agree on that. But on the other hand, the NVIDIA fan doesn't have much of a choice anyway because they were always going to buy NVIDIA. The absolute losers in their scenario are the people like myself or maybe even you who move between Radeon and NVIDIA and just pick the best hardware option at the time. In the end, if Radeon's not willing to fight NVIDIA and make the best thing possible, then buying NVIDIA is the only real option consumers have because it's sadly the best product available.",Neutral
AMD,"> They're not in the survey because they get reported as integrated graphics if you're not on Linux.  Total lie. If you filter by Windows Only and then select DX12 they show up there, the 9070 for instance is only 0.10% of the survey, so it doesn't even show up on the main GPU page because it's so underwhelming in terms of sales numbers.  But nice try at giving AMD an excuse. Next.",Negative
AMD,"> RDNA 4 isn’t planned to ever go in APUs, as far as I know. Obviously the new RDNA 5 APUs wouldn’t have been possible without RDNA 4 first though, right?  Of course RDNA4 is planned to go in APUs, like Medusa Point for instance.  > And, both cards that AMD launched this year were good value for what they were. That’s all there is to say. What the market did with that is up to the market, and that’s why I consider this fuck around and find out mode.  Good value compared to poor value. Like I said it's all relative. But honestly the 9070 XT has a fake MSRP anyways.  > Obviously we can each have an opinion on who’s offering the better deal  Objectively AMD would be offering the better deal if they could actually meet their supposed MSRP.  > but I think it’s pretty amusing that you think AMD is out there to “help” anyone.  I never claimed AMD was doing something to ""help"" people, I used the word ""help"" not literally, but rather to explain something does not ""help"" a situation. It's like saying ""If the Government doesn't collect taxes, it doesn't help them provide roads for citizens"". I don't mean it in the literal sense that AMD is there to help consumers directly or something like that. Just merely by pricing stuff so high it doesn't help the situation.  > That will never be the point of any product that they release, because “helping” the consumer is inversely proportional to profit, and that’s a relationship that’s extremely easy to maintain in a duopoly.  Yep exactly.",Neutral
AMD,So? I would argue 99% of nvidia users dont run LLMs. Your anecdote is cool but statistically irrelevant,Negative
AMD,"Also, its not even in top 10 hardware retailers in Germany. Its a small outfit. Last time Mindfactory was posted here someone dug up sales data and fucking ikea sold more hardware than mindfactory.",Negative
AMD,Yeah that is fair,Positive
AMD,"> I can't imagine how ashamed I would be as an influencer if I - and all my colleagues - spent years babying radeon like this and telling viewers to buy Radeon and yet somehow that entire time saw Radeon get the weakest marketshare in its entire history.  This, right here!  They are actively burning all the good reputation they built up - why? If it's a payout at least financially it make sense. If it's just because this is their opinions, woof count me out - they gone!",Negative
AMD,"What do you mean ""buy radeon"", do they mean Radeon GPUs or Radeon Stock, because the latter isn't possible",Neutral
AMD,"They arent the AMD marketing department, their success isnt tied to whether the thing they recommend sells. They do reviews of products from all brands and make money from views. Who cares if they prefer something the general public doesnt? Either way their stuff gets watched and its up to the viewer to make their own purchasing decisions.",Neutral
AMD,"By that logic, every critic who panned Transformers or the original Avatar but millions still saw them is a “failure.” Or Blade Runner which was praised by critics but didn't do well at box office. By your standard, literally every respected reviewer is useless",Negative
AMD,"The job of a market analyst is to predict sales, if they get it wrong, then that's a bad job",Negative
AMD,"Then how come the unfiltered online review system has been more accurate than ""professional"" reviewers? Also when it comes to RT specifically, you need to prove you bought a movie ticket to review, so trolls are not allowed to vote.",Negative
AMD,"HUB used to disable and shit on RT because AMD cards weren't good at it.  As soon as they became good at it he's all for RT.  It's stuff like that which makes people tired of these techtube influencer types, everyone has their biases and preferences but the amplified views of some definitely get boring especially when they don't reflect reality very well. Even worse if they claim to be journalists etc which to be fair not all reviewers do.",Negative
AMD,"Or ray tracing, when both Sony and Microsoft released consoles, clearly marketing ray tracing capabilities, but was ignored as a gimmick. Graphics have evolved a lot since DX9, but techtubers still living in the past rather than learning about game development or graphics. It's one of the reasons why I have migrated to Digital Foundry content, regarding GPU's since they are the only ones asking questions why this game runs slow and doing technical analysis.",Neutral
AMD,> They are offering opinion based analysis   And an important part of that analysis is being able to track onto consumer demands.,Neutral
AMD,"Here's the issue. This isn't an argument about quality. This isn't about merit. It's about ""X dollars is too much for y""; if every reviewer says this about a gpu, but that GPU is sold out continuously until the next release, that's a failure of the reviewer to accurately understand public sentiment around the worth of a GPU. To do so once is understandable, to do so for 5 years should be career ruining.",Negative
AMD,The funniest part is that everything I said is true.,Positive
AMD,AMD has received funding from the Ontario Provincial Government in the form of a grant before. They have likely received funding from the Federal Government too at some point.   https://www.techpowerup.com/114967/amd-slated-to-receive-56-million-cad-grant-from-ontario-government,Neutral
AMD,Where is valve helping bankroll Radeon r&d the steam uses Off the shelf soc apu and it hasn't even sold as much vita or wii u . I sure valve will just use off shelf apu again.,Negative
AMD,"RDNA4 will be DOA after the Super series is released.w  When I was last there, RDNA5 was abysmal until they had to give internal discounts on RDNA4. If not, we would just flock to Nvidia instead. At this point, I have better hope for Intel, just want good competition in this space.",Neutral
AMD,Like it would look even worse if they included datacentre cards 😭,Negative
AMD,"To be fair, i know that most 4090s sold were used for AI, not gaming. I would bet the case is same for 5090s. Stuff like University labs and smaller businesses are full of them.",Neutral
AMD,Fair enough but there's still plenty desktop cards being used for AI just the same. The point stands in general.,Neutral
AMD,They're not really focusing on high end cards though & that's where people with heavy workloads like AI would be looking. You could argue their priorities are wrong but it still doesn't indicate a failure if they're focusing more on providing silicon for cpus & keeping moderately relevant in gaming.,Neutral
AMD,"We have 3 sources of valuable data that all pretty much refute Mindfactory numbers (or any AMD-heavy leaning outlet).  - Steam Survey: Shows volunteer user metrics. Whether you want to plug your ears and say ""AMD is misreported"" doesn't change the abysmal growth of AMD branded products. This correlates with: - JPR Numbers: Show that data collected from distributors reveals the anemic amount of units AMD is shipping. AMD ships 6 units to every 94 NV units. This correlates with the Steam Survey abysmal growth of AMD branded products. - AMD/NV Financial Reports: Show that NV's Gaming section (which only monitors GeForce products, so no Switch/Switch2 inclusion) dwarves AMD's Gaming section (which does include consoles, such as Xbox and it's variants and Playstation and it's variants) to the magnitude that NV's Gaming numbers dwarf AMD's enterprise numbers (that should tell you ALOT).  The end result is 3 data sources that would tell anyone with reading comprehension that there is absolutely no way for AMD to outsell NV outside of niche situations where the vendor is allocating more AMD products than NV, and even that would be a drop in the hat compared to the sheer volume NV floods the market with.  Even if NV only allocates 10% of their shipments to DIY markets, that's still 9 to 6, AMD would have to have 100% sale thru to beat NV only having 66% sale thru, and we all know that isn't happening. EDIT: Also factor in the 6 in this equation would be 100% of AMD's shipment. AMD has to sell 100% of it's shipments to beat 6% of NV's shipments.",Neutral
AMD,"Not at all what I said.   AMD *delayed* the 9070XT launch by 2 months. They had 2 more months worth of inventory on launch day then they were otherwise planning. That specific action for the launch of that specific card led to high availability on that specific day.   We dont have specific numbers to argue which vendor had more nominal product available on day 1. We only know the total, final sales over the whole quarter, and we know that the 9070XT launched with only 3 weeks left in the quarter.  There is no doubt that BW sold more in Q1 than RDNA4 - not only because of launching earlier in the quarter but also because they produced more volume. But 9070XT had high available inventory on launch day. This isnt disputed. Plenty of people (foolishly) factored that in with their forward projections.",Neutral
AMD,"Its not mutually exclusive: RDNA4 could've had good launch inventory and also sold substantially less than Nvidia.   RDNA4 launch was delayed. They built up inventory as a result and availability *on launch day* was known to be very good. AMD failed to capitalize on that afterwards.   Still doesn't change my original point was that I haven't heard HUB mention anything about how RDNA4 sales would've remained competitive with Nvidia since launch.  RDNA4 having good availability on launch day (almost entirely due to the postponement) and HUB arguing that RDNA4 sales have remained competitive with Blackwell are 2 different things. I haven't heard HUB make the latter argument and he's been critical of RDNA4's real world pricing on the podcast. If he's tweeted anything about sales figures, I haven't seen it and will admit I was wrong if linked to a tweet.",Neutral
AMD,"Yes, but did he meant his own AI?",Neutral
AMD,"> ""no they are holding it all back for Q2""  Yeah, those people also said the price would drop to the supposed MSRP in Q2 which never happened.",Negative
AMD,"> Shoot even the Youtubers are now citing Mindfactory data!  I watched Broken Silicon today and HWUnboxed was still saying incorrect lowest prices for 5070 Ti and 9070 XT's in Australia. I dunno I feel like he just pulls up his favorite retailer and presumes that's the lowest price you can find stuff here.   But the only reason I bring that up is because he's alluded to that being the same retailer for his sales numbers to affirm that RDNA4 was some sort of success on launch.  I think they simply don't really care to follow up or their sources aren't very good, but even still, each market is different. In NA, Asia and China, NVIDIA tends to be a big seller. In Europe and South America, AMD tends to fair better in terms of sales compared to other regions. I think there is no best set of data but Steam at least collects data across the whole world and generally their numbers line up with the overall market trends that JPR finds for shipments.",Neutral
AMD,"Since they likely laid off the team working on Xe3P in 2024,   1) they would likely have to get a new team to start familiarizing themselves with the unfinished IP in Q1 2025   2) Then they would need to resume development and doi it quickly to meet the 2027 deadline for Nova Lake A and AX  Since we aren't seeing leaks of Xe3P DGPU's then it's likely not gonna come in 2026.",Neutral
AMD,"That's not how it works. If nobody was buying at the listed price, the price would come down. Eventually even if that means selling at a loss.",Negative
AMD,"Im not ignoring evidence you are, the very few numbers we have suggest that amd is doing great within the pc building space.",Positive
AMD,"Never said AMD sells more than nvidia, just that they are not selling less because of price or features, most of it is mindshare and prebuilts.",Neutral
AMD,"If RES wouldnt work id probably quit reddit. Its a lifesaver in user experience. I dont have long custom lists, but tagging users is great.",Positive
AMD,[at the same time disregarding an actual proper survey for a single retailer.](https://www.reddit.com/r/radeon/s/BbVvkMkdMD) i can’t believe he/she said it unironically,Negative
AMD,Steam deck can be a great handheld while steam as a service can be awful gambling encouraging always online DRM. People can have complex opinions.,Negative
AMD,So we're moving goal posts from steam survey to shipping stats now? Cmon dawg get a grip.  It literally reports as integrated graphics outside of Linux lol,Negative
AMD,"How am I missing the point?   Nvidia's gaming revenue does not directly correlate to their ""gaming"" install base, because a large proportion of their gaming dGPUs end up in AI/industrial/mining operations.",Neutral
AMD,"I bought Nvidia for ten years before switching to AMD for the 9070 XT, and the experience has been great so far, and certainly better than I would have gotten with the 5070 for 30 AUD less at the time. AMD's biggest problem right now isn't the hardware (at least in the mid range and budget categories), it's how far behind they are in software. DLSS has been around a long time and is very well supported, AMD need to be way more bullish with FSR4 integration in games because it looks fantastic, I couldn't really see the difference in motion between DLSS 3 that I was using previously on my 3070, and FSR 4 on my 9070 XT, and I noticed the artifacts from FSR 3.1 pretty easily.   If they can keep pushing FSR closer to DLSS (which I'm hopeful for given it ties directly into their plans to push instinct products for the far more lucrative data centre market) and keep up similar gains in price to performance to what we got with RDNA4 coming from the rather lukewarm RDNA3, I think they will be a pretty attractive option for me personally, even if I don't think it will improve the market share situation much if at all.",Positive
AMD,"1% running local LLM is still more than what is run on AMD total hardware. Think of it this way. 1M GPU's with 94/6% market split means \~9400 GPU's are running LLM on Nvidia, while only 3600GPU's are running LLMs on AMD if all 6% of them are running LLM.          But OP original comment while saying AI, the more broad intent here is professional work and that is far easier, adopted and accessible on CUDA, than whatever AMD is running.",Neutral
AMD,> Who cares if they prefer something the general public doesnt?  It's literally their job to be able to influence public sentiment.,Negative
AMD,This does mean they failed their job as a reviewers to inform the audience. Although a much better study would be to look at the audience of specific reviewer who panned/praised a movie.  They cannot affect people who dont read their reviews after all.,Negative
AMD,Dont think any hardware channels claim to be market analysts.,Neutral
AMD,"> As soon as they became good at it he's all for RT  They started benching RT as soon as AMD had RT support, how could they benchmark RT on the Rx5000 series when it didnt have the hardware for it? And the 6000 series sucked at RT the 7000 series not much better. Only with the current gen is amd actually ""good"" at RT. So they have had RT on their benchmarks for 4+ years before AMD got even close to being competitive in it.  You people really just have a hate boner for HW unboxed that ignores all facts.",Negative
AMD,>  Digital Foundry content  And certain folks despite DF because of their focus on IQ (probably because it shows AMD GPUs struggling in RT workloads/and FSR/2/3 being a mess but I digress).,Negative
AMD,"Oh boy, RT... i remember when there was holywar how ""RT-PT is just a gimmick"" and it's absolutely unplayble on anything below 4090, how everyone was clowning on ""fAkE FrAmEs"" on both reddit and from ""tech reviewers"" and here i was, playing fully path traced Alan Wake 2 on 4070 at 1440p highest settings with Frame Gen at 55 (in the forest) to 90 FPS (everywhere else basically) on a controller and having amazing visual experience, latency was not worse than playing any  30 fps AAA game on mine PS4 Pro at the time. Anyways, it's nice to have features! DLDSR alone is imressive, often overlooked, tool ;)",Positive
AMD,"You're just repeating the same fallacious argument. Public sentiment = popularity. Just because something is popular, or to use your words is backed by public sentiment, doesn't mean it's any good or that the public sentiment is justified. It would be completely wrong for journalists to say a bad product is good just because it is selling well. Consumers make bad choices all the time, sometimes en masse. I'm no fan of the YouTube channels being discussed and agree they all suffer from delusions of righteousness. But they are right about the GPU market.",Negative
AMD,"And by the way, x dollars is too much for y very much does hinge on merit.",Neutral
AMD,"Like it or not, AI upscaling and path tracing is the futher. DLSS is widely supported, FSR4 is not. RTX is also way better with path tracing performance. Maybe if Radeon could git gud at new FSR version adoption and path tracing performance, while drastically undercutting in price (not NV -50) they could sell.",Positive
AMD,50 million 15 years ago to develop APUs. Interesting.,Neutral
AMD,2010 lol,Neutral
AMD,"It's not hard to see how. If Valve is buying APUs from AMD as a large customer, then they're helping bankroll their next set of products.",Neutral
AMD,yeah dude people are buying 5060 to do AI,Neutral
AMD,"5070 was the big volume card this gen.  The 5090 dominating the performance crown is a contributing factor to Nvidia's brand recognition, and that brand recognition helps drive sales of lower cards.   Also doesnt help that 9070XT is just too expenaive. I'd rather just get a 5070ti for $750 rather than a 9070XT for ~$700",Neutral
AMD,the only potentially valid source is steam survey. nvidia can just lie about where the geforce products are going.,Negative
AMD,"> AMD delayed the 9070XT launch by 2 months. They had 2 more months worth of inventory on launch day then they were otherwise planning. That specific action for the launch of that specific card led to high availability on that specific day.  [High availability on that specific day?](https://www.youtube.com/watch?v=SPE95_RnL_Q)  > We dont have specific numbers to argue which vendor had more nominal product available on day 1. We only know the total, final sales over the whole quarter, and we know that the 9070XT launched with only 3 weeks left in the quarter.  But you said there was high availability of RDNA4 on launch day. So what is it, high availability on launch day thanks to a delay or only 3 weeks left in the quarter and so the Q1 figures are wrong because AMD didn't have enough time to sell through? In either case both RDNA4 and Blackwell sold out on Day 1. So... it means AMD didn't sell a lot really if their Q1 numbers were so low.  > There is no doubt that BW sold more in Q1 than RDNA4 - not only because of launching earlier in the quarter but also because they produced more volume. But 9070XT had high available inventory on launch day. This isnt disputed. Plenty of people (foolishly) factored that in with their forward projections.  See the video I linked above, it was all a fugazzi. I wouldn't call 705 cards on launch day at a large retailer in one city like Dallas which has a population of 1.3 million ""high availability on launch day"". Within 10 minutes they all sold.  Granted this is one location of one retailer in one region it doesn't illustrate the picture everywhere, but considering that in the video it's discussed that most of the volume went to NA rather than other regions like Europe. I wouldn't say AMD had high availability on launch and that their delay was a success and allowed for stock to be plentiful because their Q1 numbers were not solid but I guess they were better than Q2 2025's numbers.",Neutral
AMD,"HUB's original tweet was that if RDNA4 sold out on launch day it would have sold more than all of Blackwell combined to that point. Q1 figures from JPR are nvidia selling 8.5M dGPUs and AMD selling 0.7m.  For HUB's tweet to be true, nvidia would have had to sell less than 0.7m GPUs combined in 9 weeks, then sell over 7.8m GPUs in under 4 weeks. Do you think that is plausible?",Neutral
AMD,"I definitely got that vibe from one of their retrospective videos on their take on recommending the RX 5070 XT over the RTX 2060 Super.  I wasn't surprised for them to say, knowing everything up to now, that they were still right in their recommendation.  Tone. Deaf.  I only heard of HUB in the last two years (ironically when I started really using Reddit due to new job and the most downtime I've ever experienced in my professional life, not that I'm complaining), and right off the bat they turned me off.  Their data seems...questionable, their reasoning for their settings are 100% bias/motivated, and then their deflection is absolute art. Then you see Redditors essentially dying on the hill even HUB abandoned and I'm just confused.",Neutral
AMD,"Since Intel apparently axed the original Celestial and are bringing Xe3P forwards in its place, no leaks still makes sense simply because they're still rushing to deliver the thing. If the Xe3P Celestial wasn't going to appear next year I'm pretty sure Lip Bu Tan would've canned the dGPU division already.",Neutral
AMD,"The price can't come down very far, everyone has to make some profit. They will likely slowly sell the entire production run like they have with previous gens, maybe eventually that will involve selling at a loss but not right now. If it's anything like the 7000 series we'll probably just see manufacturers selling at break even to clear stock.  The biggest complaint about RX 9070XT is that the advertised MSRP was completely fake and only existed through rebates supplied directly to retailers for a very small amount of launch stock. It also ensured very positive launch reviews since it was a great value... at a fake price.",Negative
AMD,">Im not ignoring evidence you are, the very few numbers we have suggest that amd is doing great within the pc building space.  I'm literally using the numbers from the article above and from steam. You're using none. You referenced none.",Neutral
AMD,"Don't bother with this guy, I think he's hoping if he just white knights enough for the monopoly billion dollar company that they'll send him a free 5090 or something.  Dude over here arguing for Nvidia as if his life depends on it.",Negative
AMD,Funny talking about DIY market when AMD has literally nothing to compete at the higher end.,Negative
AMD,I use RES too but I honestly find it very user unfriendly and I only use it for a few things,Negative
AMD,https://www.reddit.com/r/Amd/comments/1hklyg9/deleted_by_user/m3hnlxm/  Here’s a great comment I saved from a while ago. Only part I disagree is about Intel gpus because they’re doing even worse particularly in the data centre accelerators.,Positive
AMD,"Holy crap lol, this is how they actually think. Mindfactory is a more reliable source in their eyes than Steam who services billions of gamers across the entire world lol.",Negative
AMD,The strength of the steam deck is not the hardware (which is very lacklustre) but rather people like it for the software,Positive
AMD,"> So we're moving goal posts from steam survey to shipping stats now? Cmon dawg get a grip.  What? I said simply that if it were true that RDNA4 was selling well, it would be on the Steam HW Survey main page. It is so low in terms of sales that you have to go beyond the main page to find it and I showed you how.  Then you made out like it's not there because it's because of some reporting error that it's not on the Steam HW Survey because it in your fantasy world reports as ""Integrated Graphics"". [I literally told you how to see the 9070 series on the Steam HW Survey and it shows up correctly, the numbers are just insanely low.](https://old.reddit.com/r/hardware/comments/1n70kxx/jpr_q225_pc_graphics_addin_board_shipments/ncatylg/)  > It literally reports as integrated graphics outside of Linux lol  [It literally doesn't.](https://i.imgur.com/WO8Z4sD.png) I told you how to show it beyond the main page. You're just in denial lol.",Negative
AMD,"Because their Gaming GPUs are capable of it. Simple as that. Nvidia gaming revenue correlates to products sold as gaming, a.k.a the 50 series. How are they used is up to the user, Nvidia doesn't care about this.   Now if Nvidia is selling their gaming GPUs though channels that are meant for AI and such, that's whole different topic.",Neutral
AMD,"They are independent reviewers they have no job other than to get their videos watched. ""Influencers"" make their bread on brand deals tech reviewers generally don't. If AMD was paying for the content you'd have a point.",Neutral
AMD,"Who are we talking about, specifically? I'm thinking of legit channels like HWUB, Gamers Nexus, Digital Foundry, etc. Their job obviously isn't to influence public sentiment, they get no money for that.",Neutral
AMD,"They did inform the audience but people can still go out to watch if they think they'll like. Reviewers might say ""It wasn't for me, but if you like that, then maybe you like this too"". Not everything has to be black and white.   And of course only the minority of the sales of these gpus watch a review on YouTube. Most Nvidia sales come from prebuilts, laptops, etc",Neutral
AMD,"Exactly. They're not market analysts, so you shouldn't predict market outcomes based on their videos",Negative
AMD,"Even you here are stating your other post was inaccurate:  > They are reviewing the products on their merits.  They were actively picking what to include in the review to slant it in favor of AMD.  That is not merit, now is it?",Negative
AMD,">They started benching RT as soon as AMD had RT support, how could they benchmark RT on the Rx5000 series when it didnt have the hardware for it?  Its very simple. You benchmark it, get a failure to launch, and then report in your review on the fact that this does not work on AMD cards. Dont sweep it under the rug and pretend the feature does not exist.",Negative
AMD,"> on a controller  While i know the game makes it not matter, but with controllers inherent input lag it really is no excuse not to use framegen here.",Negative
AMD,"Supply and demand isn't a fallacious argument, it's the entire backbone of our (and most alternative offered) economic system.  You need an actual argument. You aren't making one, you're just saying ""Supply and Demand is bullshit, my metric on the worth of a GPU is more correct (even though nobody actually seems to agree outside reddit/youtube)"". People saying ""GPUs are too expensive"" is worthless if people clearly do keep buying GPUs. When talking about price, popularity is really all that matters especially when the only complaints about price are generalised ""everything is too expensive"" rather than actually comparing like products on the market at the same time.",Negative
AMD,"Well yes and no. The issue is online reviewers are detached from what people generally see as being a worthwhile amount to pay for a GPU and in that regard, the merit is worthless. The quibble with reviewers isn't over whether they think a certain tier of GPU is too expensive; they argue everything is too expensive (clearly this is not the case).",Negative
AMD,Way too optimistic. The goalposts will continue to move to whatever functionally vendor locked thing Nvidia does next. Radeon can't time travel and make the NV first party thing first not just because they can't time travel because they aren't NV with NV mindshare,Negative
AMD,"I'd also argue if not for Valve the PC handheld market wouldn't be in the current prime it is.  Unfortunately for AMD, NV is about to steal it from them (I'm waiting to see if it's with Valve's help or not, irony would be if it's MSFT's).",Negative
AMD,unironically the clamshell ones are used for AI because VRAM is more important than compute on some models.,Neutral
AMD,"Yeah dude, if you took some effort to read what else I'd posted instead of jumping to kneejerk, reactionary digs, you'd see that's exactly my point.  The cards people use for AI are higher end & AMD have chose not to even produce any of those this gen. That's the 80 & 90 series without competition + numbers from the AI boom. Give me numbers on both brands 60/70 series from this gen & last around this same period of release then we can talk about their market share & success of RDNA4.   It's disingenuous to make out it's a disaster, from this data at least.",Negative
AMD,"A direct comparison between sales of the mid range cards gen on gen would get a more accurate representation of where they are (at this stage of release). AMD are claiming they can't keep up with demand. I'm arguing that AI usage with high end nvidia cards will have boosted their market share in desktop gpu's. It's no secret Nvidia have been directing people towards the 5090 with pricing too so even those with modest ambitions in those areas will be heading to the top of the tree, of which AMD hasn't even a card.",Neutral
AMD,"???  I watched your video and at 18:30 they say that Microcenter had nearly 20x more RDNA4 cards on launch day than they did Blackwell, literally proving my point.  The fake MSRP is easily explained by AMD lying. a 9070XT GPU die manufacturing cost is closer in price to a 5080 than it is a 5070. Aggressive pricing on the 5070 had them delay release and ""cut"" the price to $599. Launch cards then launched for the price they were always planning to launch for.  >  >In either case both RDNA4 and Blackwell sold out on Day 1. So... it means AMD didn't sell a lot really if their Q1 numbers were so low.  **Your own source** claims Microcenter had 11,600 RDNA4 cards on launch day and 500 Blackwell cards on launch day.  >  >See the video I linked above, it was all a fugazzi. I wouldn't call 705 cards on launch day at a large retailer in one city like Dallas which has a population of 1.3 million ""high availability on launch day"". Within 10 minutes they all sold.  And according to your own video, that's more cards at one retailer than Microcenter had 5090s and 5080s combined at every one of their stores combined.",Neutral
AMD,Best case scenior for him is Steve was only & only talking about launch day. He didn't even show any numbers lmao. The first month sales shows 5070 leading...anyone without any data is lying and should never be believed,Negative
AMD,"I have no problem with HUB's data, in fact most of their review content is great and pretty accurate in terms of numbers/margins. It's when it comes to their ancillary content like the podcast and follow up videos on the main channel in Q&As and stuff that they say some stuff that I just don't think marries with reality or they get led astray by a source.  I always try to say, 'don't attribute their opinion to malice, but maybe they're genuinely time poor or don't know about something and maybe they need to be made aware of it'. This seemed to be the case with the AM5 ASRock board situation where Steve from HUB thought the issue was sorted and I believe that's what he thought/had happened to him, until he was made aware of the issue still persisting. Thus, his opinion changed.  Maybe after benchmarking every day, they simply don't have the time or want to touch anything to do with tech. Steve has a family and a life outside of YT, so I can understand how he can miss things, he just wants to spend time with his kids, do regular life stuff and have some down time which I respect.",Neutral
AMD,Its... you either think like the dev does or spend way too much time looking for the options. Like if you want to filter out the political subs good luck finding filteReddit on your first time using RES.,Negative
AMD,my statistics professor will fail me if i say that hahaha,Negative
AMD,"The strength of steam deck is people can play games they already have. And yes, many people like Steam for some reason.",Positive
AMD,> They are independent reviewers they have no job other than to get their videos watched.  And how does a reviewer get their video watched?,Neutral
AMD,If they are reviewers their job is to inform consumers of a product is worth buying. If they are not doing that they are not reviewers.,Neutral
AMD,"I honestly don't understand the sentiment in this thread lol (why are you being downvoted?)   We're talking about the same thing right? Legit reviewers like HWUB, Gamers Nexus, etc. Or are we talking about another group??",Negative
AMD,"The job of a reviewer is obviously to influence public sentiment. If you don't agree, you have a very personal definition/understanding of what a reviewer is.",Neutral
AMD,"They benchmark hardware, you cant benchmark a feature that doesnt exist. They should have released a review of the 5700XT with charts where all the numbers are 0?",Negative
AMD,"I am making an argument, you're just ignoring it and then totally mischaracterising it and straw manning it because you have a massive axe to grind. Markets aren't always efficient, and gaming GPUs are a perfect example of that. Consumers don't always act in their own interests, and gaming GPUs are again a perfect example of that. This is a market with effectively a single monopolistic supplier and the YT channels are correct that the product is heavily over priced. Just because consumers are making bad choices right now doesn't change that.",Negative
AMD,"GPUs are all too expensive. The fact that people are still buying them doesn't contradict that in the way you think it does. Consumers are being price gouged and getting poor value. They're making bad choices. It's a market distortion. Not all markets are efficient, this one isn't, it's totally fucked up right now. It will normalise eventually and people will look back and laugh at how badly consumers were being ripped off and that they went along with it.",Negative
AMD,"AMD's lost marketshare - if caused by their lack of supply - is 100% their fault for failing to meet that supply. Nvidia is outselling AMD 15-1. The 5090 outselling every RDNA4 SKU is a major problem for AMD.   And they can argue that they're struggling with demand, but their sequential sales figures have remained flat quarter-over-quarter.   If AI was the main leading factor than 1) AMD really dropped the ball because AI support for their cards is still horrible. Go see how many hoops people need to jump through to run something on an AMD card that just works on Nvidia, and 2) AI doesnt explain why 5070 outselling 9070. 5060 outselling 9060, etc.",Negative
AMD,"> I watched your video and at 18:30 they say that Microcenter had nearly 20x more RDNA4 cards on launch day than they did Blackwell, literally proving my point.  Firstly, that's one retailer in one region. It doesn't prove ANYTHING about whether more RDNA4 shipped than Blackwell. I mean we can just see from Steam HW survey and JPR numbers that NVIDIA shipped way more stock in both Q1 and Q2 than RDNA4. So whilst Microcenter may not have gotten them or even maybe NA was neglected by NVIDIA, NVIDIA still shipped more cards to people on the whole. Again... your whole point was AMD was somehow masterful by delaying their launch so they could build up stock, yet the numbers do not reflect your theory. In Europe for instance there was barely any stock as evidenced by that same video. So AMD only cared to divert stock to NA and made other regions suffer.  Secondly, I wrote a whole lot and you NEVER answered it. So I'll try again. My whole point was you were trying to say that RDNA4 was smart to delay its launch so it could stockpile units, correct? Except that you also then said that there was only 3 weeks left in the quarter so that's why the Q1 RDNA4 numbers were disappointing.  So what is it?:  1. That RDNA4 had high volume and was smart to delay so they could stockpile lots of stock?  2. Or was it that it was an anemic launch that doesn't reflect in Q1 numbers because there was only three weeks left in Q1?  Choose.  > The fake MSRP is easily explained by AMD lying.  Thanks for admitting AMD lied. Appreciate it.  >  a 9070XT GPU die manufacturing cost is closer in price to a 5080 than it is a 5070. Aggressive pricing on the 5070 had them delay release and ""cut"" the price to $599. Launch cards then launched for the price they were always planning to launch for.  AMD simply made out like the MSRP was $599 by giving AIBs grants/kickbacks/allowances (whatever you want to call it) for selling a cheaper model at that MSRP on launch day for a limited run of units.  Regardless, this is all one big distraction. I still need you to answer the above question. Please answer it.  > Your own source claims Microcenter had 11,600 RDNA4 cards on launch day and 500 Blackwell cards on launch day.  Again, one retailer in one region. In other regions like China there were floods of Blackwell on launch day, entire warehouses stacked. By contrast AMD had little stock in Europe for RDNA4.  > And according to your own video, that's more cards at one retailer than Microcenter had 5090s and 5080s combined at every one of their stores combined.  Yes because NVIDIA focused on Asia and AMD focused on NA. But please answer the questions above. Thanks.",Neutral
AMD,"> Maybe after benchmarking every day, they simply don't have the time or want to touch anything to do with tech. Steve has a family and a life outside of YT, so I can understand how he can miss things, he just wants to spend time with his kids, do regular life stuff and have some down time which I respect.  100% get this, but that doesn't excuse their attitude (think of their approach to the LTT situation where HUB can be thrown in the same bucket as LTT for accuracy).  At the end of the day, we're all human and we make mistakes. But just see their responses to proper criticism.  ""I didn't know!"" sure, but then you double down! Sometimes triple down! This hurts your credibility and ""I didn't know"" eventually should lose all value.  There data is downright questionable due to settings they use. It renders some of their data effectively useless outside of this specific setup that I'd argue 99% of users won't ever find themselves in such as disabling DLSS (this isn't a HUB specific complaint either).  At this point after consuming their content I 100% will attribute their position/opinions to malice. You can't walk way from any of their conclusions or actual commentary during their videos and not take it they have an AMD bias. The irony is 100% recommending AMD to most of their audience while they rock NV hardware (that is likely free for them).  EDIT: a recent example for Pro-AMD. During their blacklisting debacle they stood the line (likely only because it was against NV) and openly declared they will not be a mouth piece for a manufacturer. Fast forward to the 8GB debacle, and they refused to test the 8GB cards at 1080p because ""they said they were 1440p products""  Which is it - are you a mouth piece or not? Because all you come off as are hypocrites.",Negative
AMD,The way I do it is I exclusively browse subs I already subscribed to. I looked at the long option list in RES and I just can’t be bothered anymore. The search function in the options is shit too,Negative
AMD,"outrage farming, its what some of them exclusively do these days",Negative
AMD,Annoying thumbnails mainly,Negative
AMD,">If you don't agree, you ...  How old are you? You sound like a little boy. Not to mention you immediately downvoted my comment, kinda pathetic. But to answer your question a reviewer is **someone who writes articles expressing their opinion of a book, play, film, etc**",Negative
AMD,"And then you think its merit to ignore said features of the competitor?   Do you not see the disconnect here?  If Youtubers were more honest, yes showing those fat zeroes, that might have  A) lit a fire under AMD's ass to get those features into at least RDNA2, but that didn't happen  B) actually show the userbase how far behind AMD is and what they need to catch up.   Instead they picked games with anemic RT features when RDNA2 rolled around and called it ""AMD caught up, in fact they are even better look at these Metro numbers!""  Apply everything we've discussed to things like DLSS and it just gets worse because they had no problems parading FSR around and just happily disabled DLSS.  Merit right?",Negative
AMD,"Yes, they should have. They should have showed a chart pointing out that X product does not have a certain functionality while Y product does and what that means for consumers.  To frame it another way: 8GB GPUs cannot properly play certain games at certain settings that higher VRAM GPUs do because they do not have the hardware for it. Should they stop testing 8GB GPUs at those settings too?",Neutral
AMD,Yup they should have but they didn’t because they are AMD unboxed LMAO,Negative
AMD,"Yes, they should have released a chart where 5700XT has all 0s to show how bad that card was using this feature.",Negative
AMD,"I'm not strawmanning; you're just saying ""ITS OVERPRICED CONSUMERS ARE WRONG"" and throwing in some fluff that appears to justify it but really doesn't (e.g. the monopoly argument, throwing around the phrase 'market distortion'). You need an actual argument on why these are overpriced, not just buzzwords. My argument is more coherent because all I need to argue is ""the right price is whatever the seller and buyer both agree on under supply and demand; since this is a luxury product where people are under no real duress to buy a product they otherwise wouldn't need"". You're just assmad the price is too high for you; you couldn't justify at all why a price of 400 dollars for a 5070 would be ok but 600 wouldn't be.",Negative
AMD,"For 1, they've literally stopped selling their high end stuff to focus on their more affordable cards. So that's the 80 & 90 series cards they've got over AMD anyway. Nvidia always has had the market share in mid range too. So we need to know the numbers for 60 & 70 series cards from this period in the last gen on both those to compare. Then we can then make a judgement on how RDNA4 is.  They don't give a shit about AI because they're knocking it out the park with their CPU's so silicon is already at a premium. They don't need to even bother trying to compete & potentially wasting money in that area. I still think their cards are still too expensive btw. I'm not saying they're completely innocent. I'm just saying that the market share data is skewed by these factors & it's not necessarily indicative of the success/failure of RDNA4.",Neutral
AMD,Yeah it takes a while getting used to and remmebering. but those arent the most important features. user tagging and shortcutes like A/Z and enter to collapse a reply are great.,Negative
AMD,"""XXX ruined gaming"", ""XXXX products are planned obsolescense""",Negative
AMD,"I mean if that's the level of engagement you're going to put up, it makes sense you disagree.",Negative
AMD,"A reviewer is someone who expresses their opinion with the purpose of informing the public on the quality of something. A reviewer who says that a book/film/etc is bad, but who can't actually get a meaningful number of their audience/the public to not engage with that media is failing as a reviewer. This is how 99% of people view reviewers; nobody engages with a reviewer with the understanding that their opinion will not be influenced by the review. There's no point to a review which does not influence the viewer. Instead of thinking up meek insults, try actually thinking about these things please.",Negative
AMD,">reviewer is someone who writes articles expressing their opinion of a book, play, film, etc  No its not. Reviewer is someone who informs the public on whether a product is worth buying. In fact quite often their personal opinion is detrimental to their task which is why good recviewers strive for objectivity and why objective parts (like showing a gameplay or video segment) is so popular nowadays.",Neutral
AMD,"> To frame it another way: 8GB GPUs cannot properly play certain games at certain settings that higher VRAM GPUs do because they do not have the hardware for it. Should they stop testing 8GB GPUs at those settings too?  Once games literally stop working on those cards yes... There's no point including benchmarks for things that dont work. They already didnt feature anything with less than 12GB on their 5090 review because its a ridiculous comparison to put an 8GB card against a 5090 in anything that would stress a 5090. TPU also didn't feature anything with less than 10GB on their 5090 review, how is this a gotcha?",Negative
AMD,They included RT benchmarks for the nvidia cards in the reviews for those cards just without any AMD cards listed because no AMD cards had RT at the time... BUT AMD UNBOXCED LOL  Nothing will satisfy you weirdos short of Steve getting on stage and licking Jensuns toes.,Neutral
AMD,"And all you're doing is parroting some brain dead grade school economics and assuming that if people are buying something, it's priced right. That's not true.",Negative
AMD,Looks like the discoloration is under the IOD.   Also they are using some really cheap and crappy ASUS boards(like the VRM on those is probably hitting well over 100C with a 9950X at 100% load). Though if the motherboards still work I wouldn't necessarily blame them.  EDIT: Just for clarity. The VRM running well over 100C isn't going to hurt the CPU until one of the mosfets dies which will kill the motherboard and might kill the CPU.  EDIT2: if the motherboards are broken I would go and check if CPU 8pin power connector isn't shorted out. If there is a short chances are the motherboard sent 12V to the CPU.,Negative
AMD,Interesting. Would be awesome if AMD could review this.,Positive
AMD,">These are supposedly top-quality motherboard   These are trash tier boards with discrete mosfets for power delivery. Wholly incapable of sustained power for a 9950X. The top 2 sets of MOSFETs have NO cooling. I don't have a diagram for this board, but if any of those are VSOC and they have a highside FET fail short, you will shoot 12v directly into SOC and explode whatever it is. Using these boards for sustained 230W PPT (AKA AMD's 170W TDP) is kinda insane.",Negative
AMD,"1. The problem of burning AM5 CPUs is not limited to ASRock and the community at large is failing by either supporting that myth or actively not dispelling it. 2. Those motherboards will not sufficiently power that CPU. The CPU will simply not boost as high (HUB has demonstrated this in several videos, 1 of which is very recent and a part 2 is expected) or the motherboard VRMs should burn out. This is a bit of ignorance on \*gmplib part. Asus has their loyalist. 3. [The 9950X can suck down 270 watts](https://hwbusters.com/cpu/amd-ryzen-9-9950x3d-cpu-review-the-top-multi-core-gaming-processor/attachment/power_consumption_peak-40/), well past the abilities of the the Noctua NH-U9S to cool, which they admitted to though they went with advertised numbers instead of tested numbers. The CPU should have throttle. That's a double whammy of failures.",Negative
AMD,> 9950X  > Asus Prime B650M-K  > Asus Prime B650M-A WIFI II    https://imgur.com/a/As6wT53  and when that blew up they bought this very different one  https://imgur.com/a/B61teYf  hmm...  It's a mystery  Steve what do you think?  https://youtu.be/ZtHOOyWYiic?t=274,Neutral
AMD,"That reminds me of the burn out we saw earlier, from high SoC voltage, but I thought that was fixed with efi updates",Neutral
AMD,Where's GN and HUB pristine journalism over this hot report ? Steve & Steve must become specialists of Multiple Precision Arithmetic ASAP ! Melting Zen 5 doing math is no joke.,Negative
AMD,"was PBO enabled in this system? It is known that higher PBO settings can cause relatively fast degradation, even back to Zen3.",Neutral
AMD,this needs fast clarification,Negative
AMD,This is why you shouldn't buy AMD for serious work.,Negative
AMD,this needs fast clarification,Negative
AMD,The picture quality isn't giving me confidence in their reporting tbh.,Negative
AMD,"Asus and high voltages? Name a better duo.  The X670E Crosshair Hero that I had before, had a lot higher voltages than the MSI X670E Carbon wifi, to the point that with the same settings, the MSI system was 10-12ºC cooler with the same CPU cooler and CPU. (CPU core temperature)  This was a overpriced 400€ motherboard, now imagine on a crappy sub 100€ motherboard like the ones they used without VRM heatsinks, that's like dumping gasoline into the fire. Surprised pikachu meme.",Negative
AMD,"We can only hope that AMD has a new IOD in the works, hopefully one that allows for a higher FCLK or more bandwidth per cycle to each CCD, and preferably with an IMC capable of utilizing per-bank refresh",Positive
AMD,"The discoloration seems to be off-center. Isn't the IOD more symmetrically placed with respect to the axis that is perpendicular to the line separating the LGA pads?  That would mean the pads below one of CCD is discolored, right?  Looks like they died due to excess heat. They were using a NH-U9s after all, and described it as a gradual death.",Negative
AMD,"They list the specs, and yeah the Asus Prime B650M-A WIFI II isn't some paragon of quality. I'd be looking more at the Noctua NH-U9S being inadeqaute for the task though.",Neutral
AMD,[https://imgflip.com/i/a4drtt](https://imgflip.com/i/a4drtt),Neutral
AMD,"> The top 2 sets of MOSFETs have NO cooling. I don't have a diagram for this board, but if any of those are VSOC and they have a highside FET fail short, you will shoot 12v directly into SOC and explode whatever it is.  The VSOC rail shouldn't have been overwhelmed though, especially on the first setup with DDR5-4800, and especially with Zen5 CPUs which seem to have a quite neat improvement over Zen4 as far as VSOC requirements go for driving memory.  I find this part amusing though: ""We don't overclock or overvolt or play other teen games with our hardware.""  They may not play ""teen games"", but the manufacturers surely do. Even without getting into silly XMP/EXPO matters, a lot of motherboards just can't have safe settings out of the box.",Neutral
AMD,inadequate power shoul dnot result in cpu having problems related to overdelivery of power. I suspoect their noctua based cooling is far insufficent and teh real problem. they also don't seem to hgave any kind of temerature monitoring in place.,Negative
AMD,My CPU died on Gigabyte mobo last week.  My RMA got accepted and I'll receive cash,Negative
AMD,You’re right it isn’t Asrock.  It’s just a total coincidence that they have multiple dead CPUs reported in their subreddit daily while other brands have once monthly.,Negative
AMD,"when 97% of the cases is Asrock, you can assume Asrock is at fault.",Negative
AMD,"But it can't be these ""top-quality motherboard[s]"", and the setup was safe because ""We don't overclock or overvolt or play other teen games"", so the motherboards weren't just the best, they also surely had a safe default configuration!  Shortly after starting to use heavy AVX512 workloads, I started smelling something burning on a motherboard with a better VRM setup than that new motherboard, with a better CPU cooler, and with a CPU frequency limit (that may have not been the limiter though during heavy AVX512 usage).  Using that first motherboard was simply a suicide run.  I don't know though why the hell can't we have safe defaults, configuration of VRM throttling, and let's be fancy, even VRM temperature monitoring. It's great that the VRM may survive 100-110 °C suicide runs for at least a couple of months, but I neither want that, nor expect other components soaking in the heat in the neighborhood to be rated for these temperatures.  Of course the alternative would be good VRM designs with properly sized heatsinks, but that costs more money, so I'm staying realistic. Just give me configuration (and monitoring).",Neutral
AMD,"so what's the issue with GN exactly? last time AMD had cpu's burning he did launch an investigation, but i haven't watched his channel ever since",Negative
AMD,Why would they? Doesn't AMD get a free pass on whatever they do?,Neutral
AMD,"Because they aren't a tech news reporting website, but maintainers of a software library.",Neutral
AMD,I've had MSI motherboards that overvolted too. Asus ain't alone.,Negative
AMD,"CPU's since the Pentium 3 or Athlon XP have thermal protection built into their CPUs. Under no circumstance should a modern CPU burn itself up with under any circumstance. No matter the load applied, as long as the power provided by the board isn't grossly out of spec. IE, huge voltage spikes out of spec.   Excessive heat can still damage other motherboard components, but a modern CPU should never be able to damage itself. The only exception I've seen in modern times is a CPU completely naked being able to damage itself in a micro-second at time of POST. IE, no heatsink touching the IHS.",Neutral
AMD,"> on a part that is rated for  \* on a part that has been measured at 270W  But if the chip is overheating, it should be throttling itself.",Neutral
AMD,That has been the current rumor for a while: Zen 6 will have a new IOD. It's one of several reasons I'm waiting for Zen 6 X3D to arrive before upgrading.,Neutral
AMD,"Interestingly, the Strix Halo processors (AI 385/395) use infinity link vs infinity fabric.    Infinity link is smaller, more efficient, and allows much higher transfer rates.    It's also what AMD used in the 7900 xt and xtx.   It's more expensive to produce than the interconnects used in Infinity fabric.  Edit:  https://chipsandcheese.com/p/amds-strix-halo-under-the-hood  >We use fan out, we're for level fan out in order to connect the two dies. So you get the lower latency, the lower power, it's stateless. So we're able to just connect the data fabric through that connect interface into the CCD. So the first big change between a Granite [Ridge] or a 9950X3D and this Strix Halo is the die-to-die interconnect. Low-power, same high bandwidth, 32 bytes per cycle in both directions, lower latency. So everything - and almost instant on-and-off stateless - because it's just a sea of wires going across.",Neutral
AMD,"Should be very doable, after all Turin got GMI ""Wide"" which is an upgrade over the standard GMI by doubling the CCD/IOD bandwidth, it got left out of consumer Zen 5 for using the same IOD as Zen 4, but a newer consumer IOD would allow for its use.",Positive
AMD,It's bulging in the exact same spot Ryzen 7000 chips started to bulge due to excessive SOC voltage.,Neutral
AMD,"The CPU will just throttle if the cooler can't keep up, it won't be a sudden or catastrophic event like that, it'll just sit at 95 constantly. A VRM redlining for hours/days seems a more likely reason for failure.",Negative
AMD,"The issue is that the VRM will try to deliver the requested power. With woefully inadequate VRM components coupled with nonexistant or extremely weak cooling, those components will get HOT (110C+ tCase). When (not if) those components fail, depending on which ones fail since they are discrete parts, you can totally nuke the load (CPU)",Negative
AMD,modern cpus without a cooler can even boot. they just keep throttling themselves like crazy and takes half an hour to load windows. modern CPUs can also work fine in 100C. The throttling level for many are set at 105/110C.,Negative
AMD,"Asrock admitted fault, but other vendors having the same issue is a deeper problem though and perhaps there is some issue with Zen 5 CPUs.",Negative
AMD,companies dont really give a shit about consumer hardware evidently. its all about cutting corners on cost at every possible opportunity and taking 0 responsibility for whatever happens afterwards. the devs of GMPlib should know better than to run a heavy compute workload on consumer hardware continuously for months on end.,Negative
AMD,"It really is crazy to start a GN video where Steve starts with a almost 10 minute history lesson on some recent examples of AMD's incompetence only for Steve to give three thumbs up in the conclusion!  So long as Nvidia is Nvidia, it seems AMD can for real burn your house down, as the hyperbole loves to reach, but somehow it's still Nvidia's fault even though they have zero to do with CPUs.  Nvidia is ruining gaming! Thanks, Steve!  HUB, I don't even think they know how to get into the BIOS.",Negative
AMD,"If they are reporting damage to the LGA they need to include a non-blurry image of the LGA. Any phone from the last 5 years can do that.  Also curious what the failure is. Can the units still POST or are they crashing during workloads? What does this mean:   ""Neither of the 9950X CPUs died immediately, instead they died the exact same way after a couple of months at high load. This seems to suggest a gradual but predictable degradation.""  If they think they found an issue and are broadcasting it publicly they should have receipts / data to support the degradation claim (esp. if they have 10+ of these systems running the same workloads).   Not saying they shouldn't broadcast this, but basically all they said is ""CPU dead"". Hopefully they follow up with more details.",Negative
AMD,I'm still amazed that a place like r/hardware has posts that just disregard the thermal protections that have existed for years.  You got literally buildzoid throwing the MB under the bus even though he acknowledges it is likely not the cause! Why the misdirect then?,Negative
AMD,A friend and i booted up an i7 2600k without cooler and it worked for a few minutes until it shut down. Worked like a charm on next boot (with cooler this time),Positive
AMD,but other vendors arent having the same issue. they are having many many times lower amount of issues.,Negative
AMD,">Any phone from the last 5 years can do that.  Very minor counterargument - that's actually not true. Some of the newer phones with 'top' sensors have terrible close-focus capabilities as one of the tradeoffs. To the point where an iPhone cannot take a clear photo of an ID with the wide (1x) lens and ~15cm min. focus, relying on the ultrawide+software to do the stitching (fake macro mode). That's the effect that's visible on the image.",Negative
AMD,"When it is a known fact that the failed set ups are using garbage motherboards with overvolted stock settings and an insufficient cooler, it seems stupid to immediately assume the CPU is the problem. Yes the CPU shouldn't damage itself, but also maybe it's not fair to basically set the CPU up for failure in every possible way then blame it when it dies. Imo all 3 components involved here likely share some of the blame",Negative
AMD,Two ASUS motherboards failing in succession in similar ways suggests otherwise,Negative
AMD,"ah, two ASUS motherboards vs 2000 AsRock ones.",Neutral
AMD,First gen ryzen mainly excelled in more heavily multithreaded things lile video editing.   And even on that front it wasnt THAT impressive. The big shock with ryzen 1000 was that it was anywhere near intel performance. That had not happend for like 8 years at that point. So this isnt that much of a suprise.,Negative
AMD,"Back when it was ok to be 30% slower in gaming,  as long as it was cheap.",Neutral
AMD,A casual video that put into perspective that core count is not the only metric for gaming performances.  Ryzen 7 1800X (OC) vs Core i3 12300 in a few older and more modern titles.  Zen1 was never a great performer to begin with but even with 8 cores it's still way behind a modern 4 cores CPU.,Neutral
AMD,I use a 2700x in my workstation. It really works great. For comparison i also use a 5900x in my home server and 9800x3d in my gaming/home office PC.,Positive
AMD,"Shoot i used a 1700 until maybe 1.5 years ago or whatever. IT was a solid CPU, yes it was not the best for gaming, but considering i came from a FX-6300, it might as well have been a Ferrari to my shitty commute car.  I just know that, just like with Nvidia now, i refused to buy Intel because they were so far up their own ass with ego. So yes i took a performance hit, but it was not that bad and i paid a fair bit less than for an i7. NEver regrated it and i built an entirely AMD system 1.5 years ago and its a beast.",Negative
AMD,Still very usable but then I still use my 5820k. It's only games that are very poorly optimised do these older CPU's show actual problems.,Negative
AMD,"Oh, I remember when people claimed that this would age better than a 7700K or a 8700k for gaming because of the cores and game only used like 30% of the total CPU!",Positive
AMD,"Yea, first gen Ryzen was maybe about on par with CPUs in the Sandy Bridge / Ivy Bridge / Haswell era. Way, way better than the bulldozer stuff they were putting out before, but still a little bit slower than the Skylake / Kaby-Lake / Coffee-Lake stuff Intel was putting out at the time.   AMD made up for it by having really generous core counts. The single threaded performance was close enough, and the multithreaded performance was impressive.",Positive
AMD,I used a 4th gen i7 for modern games until last year. I'm sure this thing is fine.,Positive
AMD,"Still use my 1700 overclocked to 3.7Ghz daily. It helps that in the few competitive games I play, it's generally enough for me to get over 100 avg frames (with Marvel Rivals being my first regular game where I've really felt how slow my CPU is), and in the story games I do play, usually my 2060 is the limiting factor before my CPU in reaching a steady 60.",Neutral
AMD,and also the price for it... was a huge seller...,Positive
AMD,Yeah. People don't remember how bad the bulldozer family was.  Zen 1 could keep up in multithreading and was behind in single core. Zen 2 was ahead in multithreading and about even in single core. Zen 3 is when they were convincingly in the lead.,Neutral
AMD,"The R7 line more or less matched the 6800x while running on $80 motherboards instead of $300 boards and also simultaneously being priced as low as $300 for the CPU (vs $1000)  The R7 line did NOT beat kabylake (e.g. i5 i7 7700k) in gaming overall but was still ""fine"" for people who didn't buy the fastest card on the market for 1080p gaming. It was often practically tied with an OCed 7700k at 4K.  The R5 line swept the floor vs the Kably Lake i5s (e.g. i5 7600k) in productivity. It also beat them in newer/more demanding games (lower frame rate scenarios). It did NOT win in older/high-frame rate esports titles.   My expectation is that in the real world (mid range CPU + mid range CPU), the R5 was generally the better gaming chip.  The R3 (4C/8T) vs i3 (2C/4T) comparison was VERY one sided and the clock speed gap was smaller than with the i7s and most i3 models couldn't be OCed if I recall correctly.   \------  This is at/near launch. AMD was generally (not always) ahead on everything that wasn't 1080p gaming with high performance video cards when comparing the top end parts.     About 6 months later intel upped the core count of their desktop line by 50%. This are up most of the reason to go for Zen 1 and in my view Zen + wasn't a big enough jump.",Neutral
AMD,Also the the R5 1600 was a better buy than 7600k with 4 threads.,Positive
AMD,"Man, I remember when AMD was selling the 8350 over and over and it was just so bad compared to Intel offerings. Huge fan of how they turned it around, I just upgraded my Ryzen 1600 to a 5700x last month on a b350 board.",Positive
AMD,> And even on that front it wasnt THAT impressive.   You could get a $399 1700x (but often cheaper) which basically performed as well or better than the 6800k which was like $1100.  Basically more than twice the perf/$$$. Not THAT impressive smh.,Negative
AMD,Yeah people were so shocked with what AMD had achieved with the Ryzen 1000 series because the FX line was so dogshit haha,Negative
AMD,"I just retired my r7 1800x back in April.   that thing was a monster. Loved it for productivity. Obviously it's showing its age now, but it did 6 years.   First Gen ryzen had snowflakey stuff like faster ram timing very affecting performance.",Positive
AMD,"I mean it's still okay to be slower at gaming if the price is right. Budget CPUs are budget CPUs. Zen 1 never competed with the 7700k well, but eh, you could make an argument for the 1600 and below given how anemic intel's offerings really were.",Neutral
AMD,"early zen is in a bit weirder spot with cores compared to others as you have them in multiple CCXs with horrible latency inbetween, combined with the cores themselves underperforming and a bad imc it's not much of a surprise that it falls behind so much",Negative
AMD,It was way behind by my 4670k that was released like 5 years before it... Well at least for things that only scaled up to 4 cores...,Negative
AMD,Oh first Gen Ryzen sucks now according to this subreddit? lmao,Negative
AMD,you hould be comparing it to 7700 though. which also had 4 cores.,Neutral
AMD,>core count is not the only metric for gaming performance   You're unfortunately going to get nowhere with this argument on Reddit    Reddit is obsessed with 8-core CPUs despite evidence that it makes almost no difference vs a 6-core of the same CPU generation for gaming  Overall CPU-performance > Core count,Negative
AMD,"Was riding one of those puppies until end Q1 this year, absolute champ at anything I asked it to do.",Positive
AMD,"Are you me? I went from an FX-6300 to an r7 1700! Still rocking the 1700, tho. Hopefully not for too long",Positive
AMD,"Yea I remember ryzen 1 and 2 got undeservedly high praise on reddit despite both having pretty terrible single core perf. (worse than even cpus from 2013 / 2014). A lot of cores, but slow ones, good for niche tasks or certain multicore optimized ganes but bad for everything else including most games, web browsing, most apps, etc. Reddit just hates intel that much it gaslit people into thinking those early ryzens were much better than they actually were.",Negative
AMD,8700k is a whole different generation when they started actually using more cores,Neutral
AMD,"Its a fools game to plan for longevity except the extremely obvious. I have a slight preference for IPC over cores based on history, but its hard to know what moment of history you're in. In the early days of Intel 4c era everyone said you should get the 4c/4t part for gaming. And that proved correct for a long time. But if you still had that CPU by the time covid and inflation came around and everything was expensive you were pretty miserable compared to the corresponding 4c/8t part which definitely would last you longer.",Negative
AMD,R7 1700 for 330 was a steal,Positive
AMD,Then Intel sold skylake for like 9 generations! I’ve been rocking a x470 through 3 different processor generations over 7 years and it’s still just fine for gaming. Intel would change sockets every few years just out of spite before they had real competition. I hope Intel can come back with a strong one soon to keep the rivalry going cause everyone benefits when they are trading blows,Positive
AMD,"Yeah, there were lots of little adjustments to make on the RAM side to get it to run to it's full potential. That said, those 1st gen x370 motherboards have proven themselves. I went from a launch day 1800X to a 5800X. I kinda wish I had sprung for the X3D but I upgraded before the price cut and it was hard to justify at the time.",Neutral
AMD,Zen got good (for gaming) with Zen 2 onwards.,Positive
AMD,And 4 threads. Hyperthreading in 4770K really helps.,Positive
AMD,"Objectively it was still worse than Intel, but the reason Zen 1 was and is beloved is because of what it represented",Negative
AMD,it always did. It wasnt until third gen - Zen 2 - that AMD CPUs got good.,Neutral
AMD,"Well even if it does not now, when I bought my 8cores in nov 2024, it was to keep it for the next 5-7y  So I'm pretty sure at that point that 8 cores may perform better than 6  Just like 6 now is the norm over 4",Neutral
AMD,Is this the part where everyone tells you how smart you are and congratulates you on your brilliance?,Positive
AMD,"And until consoles get more than 8 cores, developers won't target more either.",Neutral
AMD,"You're right, for most games even say a 3300X will match a 3600, but it was bloody nice for people with real tasks to have options that didn't cost the earth.",Positive
AMD,"That depends, what was your GPU for your FX and then what did you use with the 1700?",Neutral
AMD,"One of the most common thing I started hearing back then was ""What about Chrome, Spotify and Discord running in the background!?"" or ""What about if you want to stream?"" as if suddenly everyone was streaming.",Neutral
AMD,YEP....,Neutral
AMD,"7700K, 1700X and 8700K all released in 2017",Neutral
AMD,"R5 1600 for i5 prices was also another steal from zen 1, people may be used to 6 core being the main stream budget option nowadays, but before zen anything over 4 cores forces you to the hedt platform which cost even more on the mb side, not to mention intel 6 cores like the 5820k and 6800k cost around the 400 mark",Neutral
AMD,I remember everyone buying the 1800x which was $500 when all the benchmarks showed the 1700x (and often the 1700 on many workloads) performing like 95% of the 1800x. Never made any sense to me.,Negative
AMD,It was only a steal if you valued the extra cores. Even those only really mattered if you were hosting VMs or rendering really long videos. They didn't really translate in gaming or everyday multitasking. Pretty sure it launched right before the 8700k too and a ton of people were holding out for that CPU. For good reason.,Neutral
AMD,I just swapped to a 5800XT.   I am very much trying to ride til it dies on the am4 platform.,Neutral
AMD,"Zen 1 was pretty bad. They were like 30-40% behind intel there.  Zen+ reduced this to around 20-25%  Zen 2 reduced this to 10-15%  Zen 3 they were ahead  Then alder lake came out  THen they added X3D to Zen 3 and were on par with the 12900k  Then Zen 4 and raptor lake were about on par outside of X3D, which thrashes everything.  And zen 5 barely improved on zen 4, and arrow lake ended up being like intel's version of zen 1, regressing to alder lake performance in gaming. And Zen 5 X3D once again thrashes everything.",Negative
AMD,"This subreddit and others went from ""first gen Ryzen good"" to ""it was worse than Intel but we like what it represented"" lmao.",Negative
AMD,unlikely. Unless you use it for paralelelized workloads or play strategy sims that actually utilzie the cores its mostly useless because in next 5 years we wont be over cros-gen for next console gen so all games will be aimed at current slow consoles.,Negative
AMD,"People made the same argument in 2019 with the 3600/3700x   The 3700x hasn't aged any better    The consoles have used 8-core CPUs for over a decade, we still aren't seeing apprciable gains from 8-cores on PC",Negative
AMD,They really aren’t wrong about the general notion of this Reddit particularly around the Zen2/3 days though.,Neutral
AMD,no its the part where you failed to take in any context to what you read.,Negative
AMD,But developers aren't targeting a core count in the consoles   They're targeting the consoles level of CPU-performance   The PS4 used an 8-core CPU. And got bodied by quad cores of the day that were far faster,Negative
AMD,"Same gpu, the shite, but still rocking, r9 380. At the time I was a kid that wanted to make and edit videos, so I only upgraded my cpu to the best I could afford",Neutral
AMD,"Yea lol. Also don't forget how suddenly everyone was doing 3D rendering, video encoding, physics sims etc, when in reality 99% of people who bought those's CPU's would rarely, if ever do such tasks. Furthermore, even back then, many of those could be done much quicker using GPU acceleration (i remember using nvidia's hardware video encoder nvenc more than 10y ago already). Nowadays the CPU is becoming even less relevant for those tasks as more and more programs implement GPU acceleration.  And finally the price, the 1800x was around the 500 usd mark wasn't it? A 7700k was 300 or so i think. Yes sure slower in those niche computation tasks, maybe a bit less suited for heavy multitasking, BUT cheaper, faster in almost all games, faster in web browsing and day to day usage. The 1800x was a terrible value, except for those rare cases where you were actually doing physics sims or 3d rendering on the daily.",Neutral
AMD,"I had a friend with a ryzen 1700 who was like ""i can play a game while playing another game!"", I mean that's nice but not particularly helpful.",Negative
AMD,Just like people talk about AI stuff/local LLMs for GPUs these days,Neutral
AMD,Still a 40% multithread improvement gen over gen at the time,Positive
AMD,"Kaby lake (and for a long while beofre that) i5 was 4 core, i7 was 4 core 8 threads. Coffeee lake i5 was 6 cores no ht, i7 was 6 cores 12 threads.      but really, we are seeing the same stagnation in ryzen now. ryzen 7 ought to be coming with more memory channels and cores by now.",Neutral
AMD,yeah I bought the 1700... then later slapped the 3600 on,Neutral
AMD,Because the by far biggest channel at that time Linus Tech Tips pushed Ryzen a LOT. He especially loved the Cinebench performance,Positive
AMD,Yeah my plan was to skip AM5 and hop back in on AM6.,Neutral
AMD,Those two statements are not mutually exclusive. You realize that right?,Neutral
AMD,"It can be good, and still worse than what Intel was offering at the time (in a lot of facets). The price to performance in multi threaded applications was way better though.  Also people were absolutely fed up with Intel by 2017 and absolutely liked what Ryzen represented in terms of price to performance.",Positive
AMD,"It was like intel's current Arrow Lake. Not great performer, but represents a great leap in its architecture. Namely, the usage of chiplets.",Positive
AMD,it helped that it was much cheaper than Intel offerings at the time.,Positive
AMD,You mean vs 3600X or vs 9600X as I have Sen people compare?,Neutral
AMD,It's like I said. You can't get anywhere with this discussion on reddit,Negative
AMD,6 cores being the sweet spot for gaming isn't a revelation.It's well known knowledge and has been for a while.,Neutral
AMD,Bulldozer 8 cores is more similar to 4 cores with smt rather than 8 independent core,Neutral
AMD,"I was rocking a 960 with my FX-6300. Then i wanted a whole new PC, so bought a 1070 first and the bottleneck with the FX-6300 was insane. Of course once i got the rest of the PC with the Ryzen 1700 it ran great. USed that from the launch of Ryzen until the 7000 series came out. So a good long life once i replaced my 1700 with a 7900X and i replaced my GTX 1070 with a AMD 7900XT.",Positive
AMD,"1800x sucked, R7 1700 was the best value 8 core.",Negative
AMD,"Yeah but most people werent buying 1800x, they were buying 1700s, at the same price as the 7700k, and then OCing them to 1800x performance.",Neutral
AMD,> ryzen 7 ought to be coming with more memory channels  This is segmentation to keep Threadripper relevant for more use cases. If you really need memory bandwidth you need threadripper.,Neutral
AMD,">with more memory channels  This is like arguing that cars have been stagnate for a century because we still have 4, 6, and 8 cylinders.  128b memory is fine for a desktop CPU. Improvements come from new generations of RAM.",Neutral
AMD,Yeah Ryzen stagnation is real. Same core count on mainstream for about 8 years now  Just as long as Intel stayed on 4 cores.,Negative
AMD,"More memory channels for consumer hardware are just a no go unless you are soldered in. Mainboard layout, cost factor of needing to populate them all for nominal performance, etc.  On the professional side you can easily get a dozen channels.",Negative
AMD,"> ryzen 7 ought to be coming with more memory channels and cores by now  That's happening next gen but the point stands that more mt is kinda useless for majority of the consumers. 9950x ain't providing meaningful experience difference over a 9700x for the average desktop user, you'd probably not even know that it has double the cores without checking. The highest core count ryzens are some of the worst selling skus even if comparing sales revenue and not unit sales  If anything you should hope for larger cores, more bifurcation, more cache, accelerator blocks for offload, or other features such as lp cores over just more and more classic cores. Any one of those would create a more meaningful experience change than just giving ya another 2 or 4 cores  >""but but value, more cores means more value!""  Yeah but you ain't doing anything useful with those cores, so their existence becomes a ""feels good"" thing with little actual use.",Neutral
AMD,"The 6 & 8-core of the same generation   E.g. a 3600 vs 3700x. 5600 vs 5700x, 7600x vs 7700x etc",Neutral
AMD,"I wish I had some of the gems from back then saved. But here’s a tangentially related and hilariously similar one, about gpus instead:   https://www.reddit.com/r/Amd/comments/lxhvm9/any_news_on_when_ray_tracing_will_work_on_radeon/gpozhnb/?context=3",Neutral
AMD,"Jaguar in the Xbox One and PS4 was not related to Bulldozer, FYI. It was an Intel Atom competitor.",Neutral
AMD,Which goes back to what I said   The overall performance of the CPU is more important than the core count,Neutral
AMD,"sure, but I want more segmentation, not less. so not just either 2 or 8 memory channels, but also a segment with 3/4 in the middle between HEDT and vanilla.",Neutral
AMD,We are getting 12c mainsteam cpus with zen 6 at least.,Neutral
AMD,"> Yeah Ryzen stagnation is real. Same core count on mainstream for about 8 years now >  > Just as long as Intel stayed on 4 cores.  LOL, what?  From Zen 1, Ryzen has doubled the core count of their parts (mainstream is 8 cores, higher end is 12 and 16), massively increased the cache (and then increased it massively _again_ with X3D), massively increased clock speed, drastically improved performance of AVX2 (and implemented AVX-512), and has just all-around improved various aspects of performance.  The performance leap from Bulldozer to Zen 1 was absolutely massive. The difference between a Ryzen 7 1800X and a Ryzen 7 9800X3D is even larger.",Neutral
AMD,"They have something to offer there. If you need more than sixteen, they have threadripper. Intels complete obliteration in HEDT is real.",Neutral
AMD,"this is not a credible issue given the price of X870E motherboards. X870  might just as well been a ""we mandate 3 / 4 memory channels"" and these would still be expensive motherboards but not by much.",Negative
AMD,You should look into what AMD is doing with strix point and the ryzen AI max processors they are doing almost everything you just said lol,Neutral
AMD,"That's a banger, made me laugh   Weirdly enough I didn't even knock 8-core CPUs, just said that you don't see appreciable gains over a 6-core of the same generation",Positive
AMD,We already have that Ryzen is 2 channel threadripper is 4 channel and threadripper pro is 8 channel.,Neutral
AMD,Now only if we also get scheduler updates on windows,Neutral
AMD,"It's also worth remembering that to a vast majority of people, more than eight cores is largely not particularly useful and how improved IPC, frequency, and memory latency are the actually useful parts. Like, back in the day, I bought a 5900X specifically because my boss gets weird with what he wants and sometimes I need to encrypt and zip a .7z archive at maximum compression preset that's dozens of gigabytes at the pace of yesterday (I couldn't afford the 5950X and the X3D parts didn't exist yet). If it wasn't for that, I could have gone for a 5600X and that would have been more than enough for gaming and VR.",Neutral
AMD,"Ryzen 7 1700X 8 cores  Ryzen 5 1600X 6 cores  Ryzen 7 9700X 8 cores  Ryzen 5 9600X 6 cores  Really not that complicated. How do you not understand?   Yeah they did increase performance a lot, obviously. Only the core count is where they stagnated. Even the street price per core stayed largely the same.",Neutral
AMD,"if we get 12 cores on a single ccd than the schduler is not an issue, windows usually just put workloads on a core that is avaible. If they are on the same silicon on the same ""ccx"" then there is no issue.  If we get p/e core u-arch on the same ccd then we have issues.",Neutral
AMD,"Why would that matter? Windows has no problem with 8 cores either. If we now get 12 cores on one CCX, it should not be different",Neutral
AMD,Its not like the 4 core was limiting back then. The hate by this argument was unkustified,Negative
AMD,"> Ryzen 7 1700X 8 cores  > Ryzen 5 1600X 6 cores  > Ryzen 7 9700X 8 cores  > Ryzen 5 9600X 6 cores  > Really not that complicated. How do you not understand?  You listed two arbitrary SKUs in the respective families, while failing to mention that the Ryzen 1000 series SKUs you listed are the mid-range and high-end, while the 9000 series SKUs you listed were the low end.  The 12- and 16-core Ryzen 9 parts are now Ryzen's high end, while the X3D parts offer 600+% more L3 cache than even the best that Ryzen 1000 offered.  Your comparison was against Intel's numerous generations with the same quad-core part with barely any performance increase between successive generations, and that's not the case with Ryzen at all.",Neutral
AMD,agree,Neutral
AMD,"This isn't a simple matter of it being about the number of cores a CPU has, it comes down to the architecture, and how Windows sees/communicates with them and designates those tasks. Ryzen 1000 suffered from performance issues, because Windows didn't know how to correctly work with Ryzen's core setup. While its gotten better over the years, Microsoft is still releasing updates to make sure Windows utilizes AMD correctly.",Neutral
AMD,The price at each level has not changed much. The 12 and 16 core parts are substantially more expensive than the 8 cores were back then.,Negative
AMD,Arbitrary? lol okay,Neutral
AMD,"Ryan Smith.  Now that’s a name I haven’t 'seen' in a long time!  Anyhow,  >As mentioned previously, RDNA 4 has new memory compression/decompression features. This is entirely transparent to software; it is all handled in hardware. AMD has seen a \~25% reduction in fabric bandwidth usage.  That should explain why AMD chose to stick with GDDR6, or perhaps how the 9070XT manages to compete with the 5070 Ti, despite the latter having a massive 40% raw bandwidth advantage.  Of course, the 9070XT also has 33% more SRAM (64MB vs. 48MB), so it’s not exactly a 1:1 comparison, but still… quite impressive.  In any case, I have a feeling things will really heat up on the Radeon side next generation thanks to GDDR7, N3, and the shift to a brand-new architecture (hopefully). The stars certainly seem to be aligned.  That’s not to say Nvidia will be sitting on its hands, of course, but regardless, fingers crossed.",Positive
AMD,A dedicated hardware transfermer. Now that's something even Nvidia don't have!,Positive
AMD,Does Ryan write for STH now? Very cool write up.,Positive
AMD,If AMD also has hands on the 3GB GDDR7 vram modules that would be a dream come true,Positive
AMD,New architecture is a question mark. Dont know its from the ground up or compartmentalization of RDNA4 modular design steps. But Im expecting driver woes at launch. Thats my bar set. Nvidia will be busy with neural server processing rathern than normal graphical improvements.,Negative
AMD,"Nvidia is also supposedly doing a new uarch for the 60 series, so it might be a bigger gap than 40->50",Neutral
AMD,">compete with the 5070 Ti, despite the latter having a massive 40% raw bandwidth advantage  That's because blackwell gaming skus have excessive mem bandwidth due to how aggressively nvidia decided to cut the core counts across the stack (except 5090). The 5080's 12% faster than 4080 but has 34% increase in bandwidth, that's an unintended effect of moving to gddr7 when the decision was to cheap out on all sku core count  The difference would have been larger if nvidia didn't choose to squeeze gamers and give them 10+% generational gains.",Neutral
AMD,"This type of compression is nothing new. It's been done for decades. Nvidia has historically been much better than AMD when it comes to compression so I would be very surprised if AMD has actually surpassed Nvidia in that regard.  I think it's a mistake to think ""AMD does compression so their GDDR6 actually performs like Nvidia's GDDR7"".  Here is a link to an Anandtech article about the improvements to memory compression Nvidia did in 2018: https://web.archive.org/web/20240229212853/https://www.anandtech.com/show/13282/nvidia-turing-architecture-deep-dive/8",Neutral
AMD,This is good news for future consoles. Squeezing more out of less will show benefits.,Positive
AMD,"Yes but the 5070ti is a disabled chip like the base 9070 is, fully enabled AD103 is the 5080 and the 9070xt is certainly behind it in several aspects.",Neutral
AMD,The 9070XT would have been even better if AMD decided to shove 24gbps GDDR6.  A 20% bandwidth bump would have been very helpful.,Positive
AMD,"Right now the claim is that lower end RDNA5/UDNA will use LPDDR5X on discrete GPUs to get around supply constraints on GDDR memory for GPUs that are in like the 60 tier class.   Now that claim makes no sense to me, because I can't imagine GDDR6 has supply issues with mostly only AMD needing it. But maybe LPDDR5X is cheaper, and with AMD's memory bandwidth, and cache changes, that benefits them somehow. Plus they can use the same silicon on their massive laptop APUs with 128gb to 256gb of RAM next generation. But their architecture and the way they are moving does signal this.",Neutral
AMD,"I don’t think amd would use N3 for gaming gpus in a while, i think they’re gonna use it for their gold goose which is data center first",Neutral
AMD,going to new architecture was never a good outcome for AMD. it always too another 2-3 iterations to actually get it right.,Negative
AMD,Nobody has any transfermer as far as I know.,Neutral
AMD,An asic within an asic,Neutral
AMD,There's no RDNA 4 APUs,Neutral
AMD,"Given the newest RDNA3.5 APUs came out after the 9070 XT, I wouldn’t expect it for a while.",Neutral
AMD,"Rdna 4 is a holdover generation, it's only used for a couple products 9070/9060, and won't be used anywhere else.   RDNA5(or whatever they call it)is the next full generation, it will be in next gen APU's.",Neutral
AMD,"RDNA4 was itself a new uArch (RDNA3 is GFX11, RDNA4 id GFX12) with relatively few software bugs. Don't think I would expect driver woes at launch, at least with RDNA4 AMD actually showed that driver stability was a key focus and they held back the launch a bit to ensure it.   RDNA5 also looks to be a new uArch, with GFX1250 looking to be CDNA4 instead. So next should be GFX13-based GPUs for consumer grade GPUs.",Neutral
AMD,"New Arch is UDNA based on the MI400 series AI GPU.  It should be quite interesting.  My guess is AMD becomes a strong contender from here on out.  There's no longer a benefit from new manufacturing technique, which means chiplet can actually spread its wings.",Positive
AMD,"GFX13 is a clean slate µarch. Been confirmed so many times now that's it's a given.  It's nothing like RDNA4. I've read some of the patents, that was shared by Kepler\_L2 3 weeks ago and RDNA 5 looks like it cold be the biggest redesign since GCN. Scheduling is completely overhauled, CU gets a RDNA like rework, massive changes to RT pipeline etc...",Positive
AMD,They better do. They've been coasting since Ampere on a fundamental architectural level. NVIDIA needs a proper redesign and a massive RT increase nextgen if they want to distinguish from RDNA5 and its massive rumoured PT gains.,Neutral
AMD,It'll be a bigger leap regardless because Nvidia is getting a die shrink with the 60 series unlike with Blackwell.,Positive
AMD,"No one said this type of compression is new, just that AMD used a newer method for RDNA 4.",Neutral
AMD,I remember the Maxwell (900 series) reviews and the hype was all about compression to improve memory bandwidth as well.,Neutral
AMD,"No, it's definitely not new!  Delta color compression was how the ""Tonga/Antigua"" (GCN 3) with a 256-bit wide bus was able to compete with ""Tahiti"" with a 384-bit bus on the original GCN 1 architecture.  And like someone else pointed out, memory compression was how the Maxwell 2.0 got as good as it did, and it was further improved with Pascal (GCN 4 on AMD's side).",Neutral
AMD,"Do those even exist? I’m not believing Samsung until the actually product come out, just like GDDR6W.",Negative
AMD,Not true.   OCing the VRAM has close to zero impact on performance.  SRAM adds a lot of flexibility.  I'd rather them use GDDR6 again and stack 32-64GB I stead of jumping to 7.,Neutral
AMD,"It's not a supply constraint or a cost problem, the rumors simply say the smaller RDNA5 dies will be shared with mobile APUs (Medusa Halo and Medusa ""small"", if that will be a thing). By using lpddr they can also add a lot more memory, if needed.",Neutral
AMD,I can only see GDDR6 supply being a issue if AMD was going to go big on production for their cards.    I somehow doubt that will happen.  They seem fine with playing a very distant 2nd to NV.  The more likely reason to go with LPDDR5x for low end cards is cost.  Its a good enough cheap option which make sense for low end cards.,Negative
AMD,They have to move on from N4 at some point. That node is becoming the next 28nm with how slow everyone is moving off it (AMD and NVIDIA).  RDNA5 isn't releasing until prob +1.5 years from now. By then the CDNA chiplets are probably already on N2 or even A16.,Neutral
AMD,Really hope that's changed. They do have unlimited funds now relative to where things were in pre zen era.,Neutral
AMD,Lersa Su is a genius.,Positive
AMD,"xD  IIRC Imagination technologies has had Ray instance transform in HW for a while, but they're pioneers in HW and has been so for decades. Introduced Tiled base rendering 18 years before Maxwell.",Neutral
AMD,"Also AMD did postpone there 9070XT launch by 3 months, leading AIBs to show off coolers and cards on tables, but unable to show any demos. That probably gave AMD some extra time to polish the Driver stack vs Nvidia who's launch was Middling with some driver black screen issues on top.",Neutral
AMD,Neat thx for the info!,Positive
AMD,RDNA4 was a safe iteration in a known direction from RDNA3. dont think its comparable.,Neutral
AMD,New arch also gets a lot of Sony and Microsoft money assuming it’s used for the new consoles,Neutral
AMD,>New Arch is UDNA based on the MI400 series AI GPU.  What are the hints that point to MI400 using the same arch as gaming GPUs?,Neutral
AMD,yet in comparable architecture chiplets still result in worse performance as evident by AMDs attempts to use them.,Negative
AMD,"People have been saying this for well over a decade. I still think Intel being a better contender with Druid is more likely. AMD just manages to disappoint in so many ways time and time again but I do think RDNA5 will be modern AMDs best uarch yet, better than gcn1? Time will tell.",Neutral
AMD,"It seems like RDNA5/UDNA has a dramatically different cache hireachy.  The conventional 2/4/6mb L2 cache +32/64/96mb of L3 infinity cache is replaced by a bigger pool of L2  that's smaller than infinity cache and much larger than the old L2  but likely has lower latency than inf cache.  If I had to guess why they did this, it's probably to save on die area allocated to on-die SRAM since scaling for it has collapsed with 5nm.  Having less levels of cache also likely reduces validation time since it's less complex.  AMD is instead likely going to rely on a faster GDDR7 memory and a wider memory bus to make up for the smaller capacity",Neutral
AMD,"the rumoured massive RT gains for RDNA5 would put it on part with 50 series though, so its not like they are eclipsing them. and AMD rumours never turn out to be as good as promised.",Negative
AMD,Yep node and architectural change. They can't do Ada lovelace ++ or Ampere+++ . They have to make fundamental changes to the GPU if they want to keep up with AMDs scalable AT0 monster.,Neutral
AMD,"I do think the wording implies this is some new AMD trick that makes AMD's GDDR6 equal to Nvidia's GDDR7. That only makes sense if we pretend Nvidia doesn't already have aggressive compression, which they’ve had for years. Historically, Nvidia's compression has been better than AMD's as well.  A ""\~25% fabric traffic reduction"" in gaming tests isn't a free +25% to external bandwidth, and it doesn't erase a \~40% raw GB/s gap. Even if we assume AMD just matched or let's even go as far as to say \~10% better than Nvidia's compression, they'd still be well behind on effective bandwidth.  The simpler explanation for those results is that the tested scenarios aren't that bandwidth-bound. Once bandwidth clears a threshold, other limits (shader/geometry/queues, cache behavior, drivers) dominate. So ""RDNA4 compression makes GDDR6 keep up with GDDR7"" overstates the feature by a lot.",Neutral
AMD,"No G6 24Gbps it's still sampling, not in mass production yet.   Doubt that'll happen before GDDR7 3GB becomes widely available.",Negative
AMD,They're not spending that precious die space on more cache.,Negative
AMD,"It's reuse as APU GCDs. You save cost on the actual memory, but increase die size because of the bigger memory controller, so cost wise it should be a wash.    It's all rumors anyway though.",Neutral
AMD,"true, the funding situation is a lot better, and they did increase their software team significantly. so hopefully better launches from now on. yet still felt a need to lie about zen 5 launch so....",Positive
AMD,That he is.,Neutral
AMD,I remember reading about the kyro evil king when it came out.,Neutral
AMD,"A safe iteration how exactly? Pretty much everything across the WGP and outside of it was tweaked in some way. Out of order memory handling, oriented bounding boxes for RT, dynamic register reallocation, the hugely beefed up WMMA capabilities.  You don't get a ~50% performance bump per WGP - about 35% of which being per-clock - with just small iterations.  No, reality is like an other commenter suggested, AMD just held back RDNA4 a little longer whilst they focused on nailing down drivers for a few months. They left a larger gap between mass production and release intentionally compared to RDNA3.",Neutral
AMD,"Sony has a 30B contract.  I bet Microsoft is similar.  Like you said, the word is that they are using UDNA with a focus on Path Tracing performance.  Hoping for 48-64GB VRAM.",Neutral
AMD,"It is it's being used on everything including handhelds, it's basically the new RDNA2.",Neutral
AMD,https://www.tomshardware.com/pc-components/cpus/amd-announces-unified-udna-gpu-architecture-bringing-rdna-and-cdna-together-to-take-on-nvidias-cuda-ecosystem  Isn't the whole point of UDNA to unify RDNA and CDNA into one arch?,Neutral
AMD,"Due to the shitty MCM implementation in RDNA3. They're laying the groundwork for something better but it's reserved for a post RDNA5 as confirmed by the leaked ATx lineup.  It'll probably be a Accelerated interface die acting as a base die (with PHYs, L2 and CP) connecting directly to mem PHYs. This is the workload preparation stage (work items), then these are distributed and load balanced across autonomous shader engines that handle their own scheduling and dispatch. We can call these shader engine dies (SED). This will probably be clusters as one SE is very small, half the size of Zen chiplet rn. On the side AMD has a Media interface die with encode/decode, display, and IO.  Speculation but it's not far fetched, based on CDNA GPU packaging and AMD patents.",Neutral
AMD,"With how far behind Intel are on PPA even with Battlemage, it's probably going to take those 1/2 generations to get to Druid for them to catch up with a product that doesn't actively cause them to bleed cash whilst selling it.   I wouldn't count of a technically competitive Intel GPU product before 2029 at the earliest. Sure it can be competitive on market value if they're willing to continue to sell at a loss, but that doesn't help Intel in the long run that is already very cash strapped.",Negative
AMD,"Kepler\_L2 said L0 and L1 is merged in CDNA4. Maybe RDNA 5 goes even further and merges all CU level caches (including VRF) into one big shared flexible cache like Apple did with M3 and A17 Pro.   Like you said it's about SRAM scaling being bad and minimizing SRAM investment. N3P = no scaling, N5 poor scaling vs N7.      NVIDIA definitely has the advantage here while AMD's cache system is overly complicated and inflexible. The next logical step is to make caches into one big shared block that can be dynamically allocated for different purposes.   L1 will be much more important with the new WGS and ADC scheduling and dispatch within each SE. No more global scheduling only work item preparation and load balancing. This and other methods could explain why L2 will be less important and less used allowing AMD to use 24MB L2 on AT2 (According to MLID and Kepler\_L2 rumours).   Actually the rumour for AT2 is +25% CUs and -25% PHY width vs 9070XT, so perhaps AMD has some novel and forward looking memory saving technology in UDNA. AT2 on paper could easily be stronger than 4090 in raster and that's wild with 24MB L2 and such a weak memory subsystem.  Not expecting anything at AMD FID 2025 but maybe there's a slim chance that they'll share a glimpse into RDNA5. 2027 can't come soon enough.",Neutral
AMD,"Should've worded that differently. All I can say is look at the patents, including the ones Kepler\_L2 shared 3 weeks ago. AMD is matching and exceeding 50 series functionality + tons of scheduling overhaul. If we assume AMD just caught up to 50 series then AMD still has OBB + Dynamic VGPR allocation and Ray instance node transform in HW, NVIDIA doesn't have these rn. But like I said there's so much more in the patent filings going beyond NVIDIA 50 series implementation.  Don't care about the BS MLID PS6 perf PT >5080 or Kepler's RT 2X perf gain per CU.  What will likely happen is that AMD goes 20-30% ahead of Blackwell at iso-raster but NVIDIA cranks PT HW to eleven and more than doubles PT performance at each tier.",Neutral
AMD,"Compression aside, AMD relies on large caches.",Neutral
AMD,I didn't think of that!  And yeah its rumors but its still interesting to think of what they'll do.,Positive
AMD,"Now that NVIDIA has lowered the bar with 50 series drivers, then perhaps AMD can get away with more xD  We'll see.",Neutral
AMD,"Agree with u/Strazdas1 RDNA4 was about adressing a ton of issues in RDNA3 + AMD GCN baggage and obvious issues that needed to be fixed.  Meanwhile RDNA5 will literally change everything. Complete clean slate moment. You're comparing a Terascale -> GCN moment with a major architectural tweak, albeit still no more than a tweak.  NVIDIA has had OoO memory since Turing. WGPR is really cool but AMD wasn't the first to introduce it. Apple has had it since 2023. OBB is nice and actually an AMD first for once but I suspect this is really Cerny's doing more than anything. WMMA is just catching up to NVIDIA 40 series ML HW 2.5 years later.  RDNA4 had a small relatively IPC increase. I did the math months ago. Think it was around 6-8% but not sure. Only exception is 7600 -> 9060XT but that card used RDNA 2 WGP data stores and not the beefed up RDNA 3 stores.  AMD held it back because they wanted to see what NVIDIA priced the cards at, then they didn't invest enough in the driver team + had absurdly low stock at launch so they had to delay the launch.  AMD has not been serious about competiting against NVIDIA since IDK when. where's the prebuilt SIs and OEM contracts? 8% total market share what a joke, if you look at prebuilt vs DIY then it's not good for AMD. They need to increase market share in ALL markets not just some.",Neutral
AMD,"yes, a tweak everywhere to improve on RDNA3 rather than an attempt to make any real improvements. Going the same paths already taken by others and known to work.",Neutral
AMD,...Except better. Forward looking instead of regressive and reactionary. UDNA or whatever AMD ends up calling it is another GCN moment for AMD.,Positive
AMD,AMD never said MI400 is going to use UDNA architecture. I have been following the scene very closely and there's no proof MI400 and RDNA5 will use the same arch.  I believe the unification won't happen in the next generation and it needs more time. Unifying architectures doesn't happen at the snap of fingers. It takes a long time especially when architectures are very different.,Negative
AMD,How do you think RDNA5/UDNA will compare with Xe3? (Design work finished for Xe3 core ip in dec 2024)   More importantly if Intel wants to keep pace with RDNA5/UDNA assuming the WGS and ADC scheduling from the patents makes it into the final uarch then Intel needs to finish the Xe4 core IP in late 2025 or mid 2026 and release products using Xe4 in 2028 or 2029   **Intel vs AMD big APU's**  Nova Lake-AX will use 512 XVE's or 64 Xe3 cores and a mini version will have 256 XVE's or 32Xe3 cores according to rumors. Both of them will compete with RDNA5 48CU Medusa Halo and Medusa Halo Mini  Nova Lake will use Xe3 graphics and the Xe4 media engine (guess the core IP wasn't finished yet),Neutral
AMD,"if the patents you mean are ones you laid out in a few posts you did about it, i think the conclusion we came to was that it would put it on part with Nvidia, but Nvidia isnt going to sleep either so AMD unlikely to eclipse.  Also patent filing and actual product isnt the same thing. A lot of patents never get fully implemented. with AMD we have a history of overpromising rumous and underdelivering releases.",Neutral
AMD,Yeah but UDNA seems to completely change that. No MALL in ATx lineup and only 24MB L2 in rumoured AT2 SKU despite a possible +25% CUs vs 9070XT.,Neutral
AMD,"For sure, AMD better pray Nvidia doesn't have a Kepler to keep with it like GCN1 did.",Neutral
AMD,"MI400 is the generation after next - CDNA5. CDNA4 is MI350 (gfx1250), CDNA3 is what's currently available on the market.",Neutral
AMD,There won't be a unification. The last thing AMD needs is another HPC Vega disaster situation. DC blackwell is entirely different from consumer Blackwell.  It's about merging the fundamental design (ISA + basic blocks) and how the chip cache hierarchy is configured. The rest will be wildly different.,Negative
AMD,"Kepler\_L2 confirmed it's all in RDNA 5/UDNA. SWC, WGS, ADC, GMD, MID, Local launchers and likely many other yet to be disclosed changes. For RT LSS, DXR 1.2 compliance, DGF and prefiltering nodes, low precision ray/tri intersection, overlap trees and many more HW level changes to RT core in UDNA.   No idea but expect RDNA 5 to leapfrog pretty much everything else. This is another GCN moment for sure. 60 series might hold PT advantage and introduce novel NeRF ASIC block in 3D FF.  But Intel needs to expedite their Xe gen roadmap if they want to stay relevant.     Sounds very interesting. Didn't know Intel will have a mobile GPU targeting high end using LPDDRx as well. Interesting. First Apple, then Qualcomm, AMD, then NVIDIA and now also Intel.",Neutral
AMD,"The new patents changes things quite significantly especially the scheduling changes that align with GPU workgraphs, but that's reserved for games many years into the future (fine wine). Apparently also major cache level changes in RDNA5 so that's another source of potential RT gains.  Conclusion on Spring post was a conservative on par with NVIDIA Blackwell, new patents suggest significantly ahead. Yes indeed and I mentioned that at the bottom of my comment. They can't allow AMD to take the PT crown. Also maybe something insane nextgen like a NeRF ASIC within the geometry pipeline. IIRC NVIDIA had a paper on this a while ago.     That's true but the interesting thing is that if you go even further back IIRC I couldn't find an RT patent that didn't get implemented in RDNA4. So at least for RT patents most of them can reasonably be expected to be implemented in RDNA5. Skeptical about the DMM patents, but everything related to DGF and prefiltering nodes is pretty much a given.  Perhaps this time it's different, we'll see but not fully convinced either. And there is still the possibility that a lot of the patents might be reserved for AMD's nextnextgen.",Neutral
AMD,"Don't think that kind of thing will ever happen again. Designs are already extremely optimized on both ends. Doubt NVIDIA can do another Fermi -> Kepler or Maxwell (not happening) moment, but perhaps they'll surprise us. I could see them pulling a Maxwell moment with PT though. The fundamental core is still Ampere which at is core is still Turing/Volta so a clean slate might happen and that could have massive ramifications similar to AMD's RDNA5/UDNA.  On AMD side let's just hope the architectural changes are massive + AMD's patent derived (unconfirmed) scheduling overhaul is forward looking like GCN Async compute. Prob not relevant for launch and first couple of years. but the other changes can make a massive impact as well.",Neutral
AMD,"There's another patent supporting RDNA5 potentially being amazing if you're interested. Already mentioned it in the post 4 weeks ago, but I understated the impact. Local launchers are very likely to be arriving in each RDNA5 WGP. This will allow them to launch and schedule work and manage ressources on a per WGP basis with a 10x dispatch latency reduction vs the command processor within each shader engine (rn I think).   For workgraphs workloads this will result in insanely high speedups. Ray tracing and procedural code will see massive speedups. Unfortunately think this requires developers to change code like the Workgraphs API. But perhaps AMD can do some magic at the compiler level, but I doubt this. We'll see.  NVIDIA better have something major nextgen or AMD will leap frog them. Repeating myself here but Kepler already called RDNA5 the biggest change since GCN. I don't think that's inaccurate based on the radical patent filings and this probably isn't even exhaustive.",Positive
AMD,>MI400 is the generation after next - CDNA5  Indeed and that was known. MI400/CDNA5 is coming late 2026. RDNA5 in H12027?  >CDNA3 is what's currently available on the market.  AMD announced that they started shipping MI355X/CDNA4 to hyperscalers 1-2 months ago.,Neutral
AMD,"I second this, there is no reason to waste MI400+ series die area on things like RT engines.",Negative
AMD,Intel needs to pour money and manpower into finishing the Xe4 graphics IP and pulling forward it's release date  Nova Lake-A and AX will use the Xe3 graphics IP (both are expected to release sometime in 2027]  Intel needs to have Xe4 versions of both big APU products ready in 2027 or 2028   **What I think happened**  I think Exist50 was right. Xe3P based Celestial products were likely canceled after the disastrous midyear earning call in 2024  After the unexpectedly huge success of the B580/B670 Intel must've then restarted their DGPU/Large iGPU plans  That's because We're not seeing any DGPU Celestial leaks but we are seeing Nova Lake-A and AX that are supposed to ve ready in 2027  Considering Xe3 will be used in Panther Lake (Q4 2025 and Q1 2026 release) it supports my hypothesis.  **Intel's DGPU future:**  If we don't see DGPU Celestial products in 2026 then either we don't see them or Celestial will be like Battlemage and Intel will have to sell bigger dies that compete with smaller AMD and Nvidia counterparts in 2027 (likely sharing GPU silicon dies with Nova Lake-A and AX)   If they do release a Celestial DGPU then it will likely only be 1 or 2 SKU's that compete on price and are ready in 2027  It will likely be very quickly replaced with Xe4 Druid DGPU's in 2028 which should look more like a proper    AMD/Nvidia  GPU lineup.,Neutral
AMD,"MI400 is early 2026, AMD seems to be pulling it up",Neutral
AMD,"Indeed. There will be a unification but not how most people think.. CDNA is GCN++++. RDNA is well RDNA. Now it's time to unify the foundation for each architecture into UDNA.   It's probably more so about unifying how the cache hierarchy, share core ISA and CU layout (partitions, etc...). Everything else will be wildly different between the two, just like NVIDIA Blackwell DC and Consumer.",Neutral
AMD,"Your previous two comments were deleted. Something about cache hierarchy overhaul and another comment about Xe4.  I hope you're right. If Intel wants to compete then it can't be a Vega 64 vs 1080 situation, as was the case with 4060 vs B580. Hope Xe4 is a miracle GPU architecture. PC market needs some competition rn.",Neutral
AMD,"No, they're not! LOL! AMD can't launch 3 new INSTINCT series in just 1.5 years! LOL! AMD pulled MI355X/CDNA3 by half a year because it was a minor redesign of MI300X/CDNA2, but MI400 is still on track for late 2026., because it brings major changes to the architecture, NICs and cabinets designs. When you make such a major design changes you don't pull a launch by a year. That's crazy.  LE: MI400 is not coming in early 2026 because it comes bundled up with Venice, which launches in late 2026, so your speculation got KOed.",Negative
AMD,"MI400 is not early 2026, according to Forrest Norrod, EVP of Data Center Solutions, Advanced Micro Devices: ""we are  anticipating material revenue from the MI450, which **we'll be launching in about a year from now in next year**"", said on 08 September 2025 [Source.](https://www.investing.com/news/transcripts/amd-at-goldman-sachs-conference-ai-and-data-center-insights-93CH-4230321)",Neutral
AMD,"So it sounds like quantum compute is going to have conventional compute fallback, specifically for error correction and/or fault tolerance.  So, basically, quantum-powered compute output, and conventional compute checking the faults in quantum output, and potentially correcting them, so quantum output can continue.   Sounds to me like they're trying to throw conventional AI at quantum computing to see what happens?",Neutral
AMD,"I heard Asrock boards are having issues with 9000 series processors. Isn't it just the X3D chips that are having problems? Even the normal ones are getting fried too? I'm using a B650 Asrock board and am now concerned about my chip dying, should I update my bios?",Negative
AMD,Asus Prime are terrible as always.,Negative
AMD,"Good testing.  But it should be remembered this is **testing with Ryzen 9 9950X** and DDR5-8000.    Your 7800X3D/9800X3D on DDR5-6000 will be fine on pretty much any board, gamers.",Positive
AMD,like the look of the Gigabyte+ B850M Gaming X.,Neutral
AMD,"One of my two new builds this spring is an Asrock B850I Lightening ITX board paired with an Ryzen 8700G APU. Intended for office work, browsing, Netflix, VPN and a little 3D graphics it works very well for its intended purpose. It's quiet and small while drawing little power. This will do for many years.",Positive
AMD,"Ah, the B850 where AMD mandated PCIe5 but forgot they cheaped out on PCIe lanes so we get cursed lane distributions and M.2 slots sharing lanes with anything like it's B450/550 all over again. Like, the B650 boards are actually better unless you *reeeaaaaly* need that PCIe5 (and no, you don't).  Man, Intel boards are so much better featured it's not even funny...",Negative
AMD,"As'ss brands being ass as usual.   Reddit conjecture doesnt undo the fact that asrock can't have a well used usb port last 3 years or asus can't deliver a decent vrm or memory support for under $300. And I don't blame malice on their part for that, whomever is sourcing their capacitors is the biggest culprit imho.",Negative
AMD,I wish they had the Gigabyte B850M Force available to test  It is a motherboard with a lot to like and a lot to hate,Negative
AMD,"Oh look, Asus Prime still being a dogshit board, who would have guessed.",Negative
AMD,30 minute spec sheet readout. Very informative.,Positive
AMD,Not sure whether I'm more afraid of ASUS firmware or ASUS hardware these days.,Negative
AMD,The Micro ATX Gigabyte B850M GAMING X WIFI6E looks to be pretty good.?,Positive
AMD,even tho Asus primes boards are garbage their TUF boards are banger .,Positive
AMD,"Asus as always has the worst cheap boards. They rely on their high to medium priced board quality to get a good reputation, so they can then sell bad and cheap boards at good profits at the low end.",Negative
AMD,"Props to the companies putting a double digit number of USB ports on their boards.  You can easily have too few USB ports, but you can't have too many.",Positive
AMD,Best b 850 msi mag tomahawk max wifi,Positive
AMD,im thinking of getting ASRock B850M Pro-A WiFi Micro ATX AM5 Motherboard for ryzen 5 9600X but i heard theres motherboard failure for 9600 is it fix yet?,Negative
AMD,"Most of the reports seem to be 9000 series X3D, but there’s a lot of buzz, so some reports of other 9000 and 7000 line up are getting reported. Could be normal failure rate, could be not. Asrock subreddit mods promised summary of all reports they’ve collected so far to be available this week",Neutral
AMD,My 7700X died in an ASRock Pro RS X670E,Negative
AMD,">I heard Asrock boards are having issues with 9000 series processors. Isn't it just the X3D chips that are having problems? Even the normal ones are getting fried too? I'm using a B650 Asrock board and am now concerned about my chip dying, should I update my bios?  It's mainly the X3D CPUs that is reported in the Asrock subreddit, but AMD do (going by posts) honor the RMA with no hassle.  Edit: And using 800 series chipset.",Negative
AMD,"There are a handful of reports of 7000 series CPUs as well, and a slightly higher number of 9000 series. A vast majority of the reports are 9800X3D though.  We also don't know if it's still a problem. ASRock released a BIOS to supposedly fix it, but we've also seen failures after that. Was that because the CPUs were on the previous BIOS for some time? Dunno.",Neutral
AMD,"It seems that even other brands have issues, there was a news about it today with AMD giving a comment that ODMs should follow the base settings.",Neutral
AMD,The issue has been addressed via BIOS update. There was a problem with AsRock allowing too high of PBO currents by default which was frying some CPUs (in combination with a Ryzen Master bug which was enabling PBO without user interaction).,Negative
AMD,"I mean, you probably should update BIOS either way. It's free performance and fixes. But it's a little vague right now. So... No way to know for sure.",Neutral
AMD,">I heard Asrock boards are having issues with 9000 series processors.  We are talking maybe 100-200 out of tens of thousands of boards, and other brand also had some problems with the 9000 series.",Negative
AMD,Prime used to be decent. I'm still using my Prime X370-Pro from 2017 with 5700X3D. Then the series got enshittified by Asus really hard.,Neutral
AMD,">as always.  Prime was good, guessing before you were into PCs.",Positive
AMD,ASUS boards should just start at $250. Their low-end stuff is total dogshit and has been for a long time.,Negative
AMD,The gigabyte b850/m gaming x wifi 6e is well priced enough that it should be the default recommendation.,Positive
AMD,"If I recall correctly, the Techpowerup Chart said in their gaming suite the 9800X3D used on average 65W with slightly more in PBO Max mode.",Neutral
AMD,The only thing I miss with these boards is clear information about recommended and not recommended cpus.   I don’t really think anyone pairs lower end boards with high end cpus but it should be in writing what CPUs they are intended for.,Negative
AMD,"It's a torture test across platform life IMO. If it can take that stress test, it will handle the final gen on the socket with aplomb",Negative
AMD,it's a good way to actually stress the components tho,Neutral
AMD,I've used Gigabyte boards for years and they've been a bit of a letdown.  Sure they work but their feature set kinda seem like a bit nickel and diming.  I've been running Asrock for the last few generations and have been really enjoying their bang for the buck boards and love the stability.  Trying to get an old B550 mobo with 2.5GBe was hard to find on any Gigabyte boards but was easy to find on Asrock.,Negative
AMD,"The PCIE 5 main slot doesn't affect lane sharing in any way, If any lane sharing exists it's just the manufacturer giving more options like they did with 600-series boards. b650, b650e, b850 are all the same essentially just with PCIE 5 being needed on some, even X870 confusingly is only single chipset rather than the daisy chained one of X670, X670E and X870E, and the difference to B-boards is just mandatory usb4 maybe better wifi or something as well.  >Man, Intel boards are so much better featured it's not even funny...  Are they? The low end stuff quickly glancing looks similar and ofc no OC on intel B-boards(aside from 12th gen BCLK on some). Yea Z-boards have x8 chipset connection rather than x4 and the cheaper Z boards are seemingly chepear than amd dual chipset ones are(of any gen I think X670(E) wasn't exactly cheap at launch either vs Z690/790) so i guess that way sure.",Neutral
AMD,"> Like, the B650 boards are actually better unless you reeeaaaaly need that PCIe5 (and no, you don't).    OR be like me and luck out with a b650m-e that ASUS accidentally enabled 5.0 for the PCIE slot",Neutral
AMD,"learned this recently myself but most b650 users can enable pcie5  while not officially supported, works just fine",Positive
AMD,They do it deliberately.  They built up a good brand with their ROG Strix line before and then enshittification hit and they drastically lowered quality while keeping prices the same to increase margins.  Why it's important to read reviews and not depend on brand name.,Negative
AMD,"And the crazy part, Asrock was a spin off Asus group, now Asrock is having better motherboards than Asus.",Positive
AMD,"Yeah, they put in zero effort on their low end. There's a couple exceptions though, like the x570 prime is fine.",Neutral
AMD,">Not sure whether I'm more afraid of ASUS firmware or ASUS hardware these days.  Their Windows software is the big security problem, along with loosing private keys for signing their drivers.",Negative
AMD,Its almost 10% failure rate,Negative
AMD,">but AMD do (going by posts) honor the RMA with no hassle.  most people will prefer avoiding RMA by not buying Asrock - troubleshooting, then waiting for days-weeks for RMA is not something that you want to do - it was the exact reason why i purchased Gigabyte motherboard for my 9800X3D and not Asrock, i don't want to go through RMA because Asrock is faulty, i would rather choose a different brand.",Negative
AMD,Heard rumours of a refresh release with  better memory controller.,Neutral
AMD,And what looks an awful lot like a few bad batches of Zen 3ds.,Negative
AMD,If you go to r/asrock it’s a couple posts per day.,Neutral
AMD,"Yep, Prime used to be what TUF is now.  Mostly no bullshit, solid components and a decent price point with almost all reasonable features vs the Crosshair/Maximus lineup.  Classic ASUS, really.",Positive
AMD,"Hell, Asus used to have the equivalent of the Noctua tax, and people didn't mind much.",Neutral
AMD,"They can only afford to be bad because they used to be good. Built the name on affordable quality, slowly scaled back both affordability and quality in favour of profit. Here we are.",Negative
AMD,Price doesn't mean the product will be good.,Negative
AMD,"For gaming workloads, that's probably about right. 50-75W on average, maybe maxing at around 85-90W in heavier games. For all core workloads, it's probably closer to 115-125W depending on your settings. Still low enough to be handled by even mediocre low-end boards, but not nothing.",Neutral
AMD,"> I don’t really think anyone pairs lower end boards with high end cpus  Why not? Consider how ridiculous would it be to say that you shouldn't pair cheap USB devices with your expensive computer.  Limitations should be clear, but we are way beyond the old times of hardware safety depending on the user. Power delivery systems of consumer products destroying themselves without protections kicking in are simply faulty.  > it should be in writing what CPUs they are intended for.  A user friendly list could be nice, but it would be more generic to properly disclose power requirements and capabilities instead of CPUs advertising TDPs not matching power consumption, and bad motherboards attempting to deliver power beyond their safe capabilities.  An intended CPU list wouldn't tell me a whole lot, while I could make a better decision seeing a 90W CPU, and potentially settling for a motherboard delivering only 80W continuously, knowing that I ended up with a budget build limited by the motherboard. This can already happen, it's just not disclosed properly, and some motherboards don't have properly set power delivery limits.",Negative
AMD,">I don’t really think anyone pairs lower end boards with high end cpus  I mean there are low end boards with good vrm that'll have no problem powering higher end cpu:s, so if you just want ""a board"" and don't care about any extra stuff, something like the asrock B650M hdv/m.2 is better than a lot of these boards due to it's very low price at jsut shy over $100 USD. Sure no pcie 5.0 main slot, but that's a non issue unless you run out of vram on an x8 wired card, even a 5090 doesn't really lose anything going pcie 4.  Also shockingly it's in stock currently cause last year it spent most of it's time out of stock or incresed price, though there is the asrock cpu issuebeing talked about and idk if that affects 600-boards as well.",Neutral
AMD,"I think that given there is no Ryzen 3 on AM5, the only possible difference is if they support OCing on 16 C cpus properly with enough power fed to them. And because AM5 is positioned as the premium option over AM4 where the value buyers were pushed for the longest of time (and even now unless you have ali you can't get sub 100 dollar CPUs for AM5), all boards at least support fully loaded 16 C AM5 offerings and most likely is fine with OCing said chips too unless you are going for something like LN2.   there isn't so much really cheap, really low quality boards that only supports ryzen 3 4 core processors and maybe stretch to 6 and 8 cores and without OCing like it was with AM4 (where you can for example pair up cheap and old A320 boards with 5800X3D no problem because you can't OC the thing).  And as a result, not a lot of places do them now, as most people who games just grabs X3D and rarely OCs them since its the cache that mostly make them good and with all the issues with them cant be OCed or possibly burn them out by all that jazz, people stick with stock with them now unlike before when if you want to game on intel you pushed a 5ghz all core OC at the minimum. which meant you had to have good motherboards.",Neutral
AMD,pretty sure most folks are reporting being able to run pcie 5 on most/all b650 boards,Neutral
AMD,How does one enable it? Just throw in a pcie 5.0 nvme or GPU? It's auto enabled right?,Neutral
AMD,"> Why it's important to read reviews and not depend on brand name.  Going with a mix works too, just not like how some people do it.  I avoid ASUS because they've shown they are willing to associate their name with garbage quality. They may make some okay products, but with good competition, it's just easier to skip the riskier companies not caring about their reputation.",Negative
AMD,"My fuckin b450 Strix can't even handle 64GB of ram at 2666mhz. 2400 is the best it can do. Don't even bother asking it to let the CPU run to TDP either. 80w, best it's got. (1000w PSU)",Negative
AMD,That's been the case for a long time,Neutral
AMD,In a way that seriously calls their whole damn software stack into question.,Negative
AMD,"Didn’t word it well enough, I meant non-X3D cpus that are getting reported could be within normal fail rate, but are getting reported as well, while in normal circumstances it’d be just quietly RMA’d",Negative
AMD,"Yeah, I have an asrock for my 5800x3d cpu, but if I upgrade, no brand loyalty here.",Neutral
AMD,"Same boat here, it's just not worth the headache. It's a shame because as proven even in this review Asrock boards offer excellent hardware for the price.",Negative
AMD,">most people will prefer avoiding RMA by not buying Asrock - troubleshooting, then waiting for days-weeks for RMA is not something that you want to do - it was the exact reason why i purchased Gigabyte motherboard for my 9800X3D and not Asrock, i don't want to go through RMA because Asrock is faulty, i would rather choose a different brand.  I absolutely sympathize with that! But all motherboard manufactures have their various issues occurring to some of their motherboards.  Missing PCIe lanes for the GPU slot is a bummer for MSI and Gigabyte, for instance. Various longstanding serious security issues in software and UEFI: Hello Gigabyte and ASUS!  So I went with Asrock. None of my builds use an X3D version CPU, though.",Negative
AMD,Talking 9000 series cpus?,Neutral
AMD,">If you go to [r/asrock](https://www.reddit.com/r/asrock/) it’s a couple posts per day.  In that forum any boot problem is by default attributed to a dead CPU, so there is that. Also a number of trolling posts.",Neutral
AMD,if it's on r/asrock it must be true,Neutral
AMD,"And TUF motherboards used to be relatively high end, no-nonsense boards with a focus on durability (hence the name). At some point they turned it into a ""budget"" gaming brand of sorts.",Neutral
AMD,"Well no, I more meant they should drop their <$200 product lines, because they're complete crap.",Negative
AMD,"check like cpuz or hwinfo to see, it might be already enabled   some board owners are reporting bios updates that explicitly disable it (gigabyte if I recall)   but there should be an option to select it if need be in the bios",Neutral
AMD,"Had X570 ASRock for 3900X, now went with MSI X870E Carbon for 9950X3D after reading about ASRock current issues",Neutral
AMD,Yes both Ryzen and Intel arrow lake refresh rumours.,Neutral
AMD,"Yeah, people don't seem to realize that there is a natural failure rate for *all* hardware. For CPUs it's pretty low at ~0.25-0.5% from what I can tell of past hardware data. That still means hundreds or thousands of CPUs failing in their lifespan.  At /r/asrock they seem dead-set on blaming ASRock for any failure at all. Not saying they don't deserve *some* blame, but failures happen regardless of brand.",Negative
AMD,Why would they its free money? The real quality products sell the asus brand and they can sell their leftovers to the poors who watch tech tubers glazing their top end products.,Neutral
AMD,Good info. Ty.,Positive
AMD,"That'd good. Of course being able to tell if you are buying a fixed CPU before you buy is the important thing. How could you know if they are shipping you a fixed batch cpu? For the record AMD did delay 9000 series release do to qualify assurance issues. So it's a combo of Asrock over juicing, which they admitted, and likely bad batches from AMD. Explains why they both are approving RMAs readily. I'm just skipping both Asrock and 9000 series. 7600 are only slightly slower in games (5 -10% max)and 20 bucks cheaper than 9600x.",Neutral
AMD,"Especially when some of it's gonna be the longstanding ASROCK problem with being fussy about RAM makes... which honestly, why the fuck if you can get it for a not inane price, are you not rocking GSKILLs with that warranty and compat anyway?",Negative
AMD,Yes. Why AMD withhold release for 1-2 weeks is still unknown.  But Even Ryzen 7600 CPUs on Asrock motherboards are showing light burn marks but users are not bothered because it is working fine. I had one which shows slight marks but working fine. Also saw one user commented in Reddit saying his 7600 was running fine on Asrock boards for a year then he changed CPU to 9800x3d. He saw some burn marks on 7600 but was running fine then went and checked 9800x3d that is also developed same marks after 5 days of use. Nobody is going to check CPUs unless it reports some unusual behaviour. Asrock is bigger culprit here.,Neutral
AMD,"AMD did say why. But no exactly. https://www.techpowerup.com/324887/amd-delays-launch-of-ryzen-9000-series-processors#:~:text=We%20appreciate%20the%20excitement%20around,on%2Dsale%20on%20August%2015th   We appreciate the excitement around Ryzen 9000 series processors. During final checks, we found the initial production units that were shipped to our channel partners did not meet our full quality expectations. Out of an abundance of caution and to maintain the highest quality experiences for every Ryzen user, we are working with our channel partners to replace the initial production units with fresh units. As a result, there will be a short delay in retail availability.",Neutral
AMD,"I really wish I could justify the price tag for one of these things, they are super cool, and upgrade-ability is so unique in the laptop space",Positive
AMD,Yup. I love the idea   Not the price,Positive
AMD,"Hopefully once more people start buying them, they'll be able to scale up production and get better volume discounts.",Positive
AMD,"I've been following them for a couple of years, but they still don't ship to Norway. 😐",Negative
AMD,"whats your gpu and cpu usage, and temps when this is happening?   Did you also try updating your bios and chipset drivers, try other games, or see if you have same issue if running 64 GB ram with 75%+ usage to see if issue occurs?",Neutral
AMD,"I mean you say it yourself that the botfarm not running resolves everything. What's it doing exactly? Probably memory swapping a lot, using a bunch of CPU cache, possibly bouncing between CPU CCXs.  First you should ensure you are appropriately pegging your other processes to specific hardware e.g. first 8 core CPU affinity goes to the botfarm, other half is free.  But depending on exactly what that botfarm is / is doing and how its using your resources it may not be possible to avoid.",Neutral
AMD,"doesnt occur with 64gb set at 85% or 90%, gpu cpu usage around 20-30%, cpu temp 65-75, gpu 50-55, i have all latest drivers, happens borderless/full screen thing happens in everygame",Neutral
AMD,"That low GPU usage when gaming seems off. Typically it's like 99% GPU usage.   Do you usually have that low of GPUs usage when gaming, regardless if the issue is happening or not?",Negative
AMD,what are your temps under load?  idle temp usually doesnt matter.,Neutral
AMD,"What's the rooms temperature? Is the fan on the CPU cooler running loud?  If it was like 10C and the fan was running at 100% while idle and still getting 50-65C then there would be a problem, maybe no thermal paste or not enough mounting pressure.   But if you're in a 25C room and the CPU fan is barely moving that'd be normal",Neutral
AMD,"check for voltage, if it is around 1.4-1.5, you might benefit from applying -0.100 offset in bios",Neutral
AMD,"For that series never go above 90 c. Under load between 60 to 80 c. A little bit above 80 but less than  85 is okay. Get ryzen master and undervolt your CPU volt and mhz. I can get my r5 5500 at 2400 mhz and 1 v. Your may vary I would not recommend setting that low to start measure your temps under load like with a videogame. A dvd, video file, of browning the net will not raise your CPU temp much usually one degree above idle the same is for the GPU temp. My idle temp for GPU is 37 and it won't go past 38 while watching a tv show via file streaming or DVD playback. So undervolt use a few programs to monitor your temps while playing a game for CPU and GPU. Make sure they stay below 80 c. I can keep my CPU and GPU at 70 c or below while gaming at 4k. It took a lot of hard work and tweaking to get to that point. It wasn't a one day thing. I am still tweaking for optimal performance and temps. I want my PC to last at least 5 years before I upgrade. Use MSI after burner and do a performance overlay that will display while gaming to get real time data under load. Idle temps are irrelevant for the most part unless your CPU starts up at 93 or 95 degrees. Then you will have to take off your cooler clean it up and reapply your thermal paste as well as undervolt. Best of luck.",Neutral
AMD,"your cpu cooler is trash, included stock cooler would perform better.",Negative
AMD,Thanks for the help!,Positive
AMD,That’s just not true in the slightest,Negative
AMD,no its not.,Neutral
AMD,"It is, i used it on a i5-6600k back in 2015 i belive and my cpu would reach 90C while playing CSGO. Replaced it with a hyper 212 evo and rarely would go past 70C in more demanding games then csgo.",Neutral
AMD,OP is talking about idle temperatures...,Neutral
AMD,my idles were bad too. 65C idle on a 65W cpu is bad.,Negative
AMD,"CAD modeling software relies on higher clock speeds for faster response rates. In this case, you get good single-core performance and multi-core with the i9-285K.",Positive
AMD,"1. I would lean towards 9900x, I don't use Fusion, but I do Solidworks, SW is not very multi-threaded at all, unless for specific case, like flow works, solidCAM etc. If I were to build a tanky workstation right now (with company budget), I would probably go 9950x3d or 9950x (I run my CAD in VM and my workstation also acts as a server backup for my main server). Probably save the x3D premium for other hardware, so 9950x or 9900x.   2. Because of ITX, I would also go for AMD over Intel, it has a slightly (very slightly) lower need in peak thermal capacity. So you can get smaller cooler, smaller case. I think that in the day to day, I would not see too much difference in CAD work, but when you need to generate g code etc, or do (CPU) rendering, then the more cores the better.   3. If you are using company money, go for a Quadro or an equivalent Radeon Pro. There is nothing really wrong going with a gaming GPU, but for Solidworks, the preferred route is a professional graphic card, just for the sake of having no issue and getting support. So A2000 ada or A4000 ada will fit the bill.  This in turn reduce the thermal envelope you need for the GPU. But price is high on them... I know Fusion switch to DirectX... so you can get a gaming GPU and if you do play games on it.. then it's better, but if it's only for work, then quadro is the way I would go.   4. For the case, make sure you get the right thermal envelope need for the CPU, ITX can be very small, so after the CPU choice, you'll know if you need a AIO, a tower or a skinny heatsink.  Any tips for moving workflow, do you have a centralized file management? if yes, then the migration is automatic. If not, build one first, move your work files there.",Neutral
AMD,"r/bapccanada is for Canadians and would be familiar with prices.  overall lgtm, heres my version of price performance am5 build. A little bit cheaper than yours but 9600x  [https://ca.pcpartpicker.com/list/gpQxBq](https://ca.pcpartpicker.com/list/gpQxBq)",Neutral
AMD,Check canada computers bundle deals,Neutral
AMD,"The P3 Plus uses QLC flash, which has low durability and becomes noticeably slower after a few years. Get a better drive with TLC flash like the Kioxia Exceria Plus G3, WD Blue SN580/SN5000, TeamGroup MP44L/G50, Patriot P400/VP4300 Lite, or Klevv CRAS C910.",Neutral
AMD,"tbh not bad for ur first build  9070 is prolly the best price to performance card rn and with 16gb of vram should be fune for upcomming triple a titles that will need more vram in the future. Solid am5 chip and mobo for future upgrades however if u find a 9600x for around the same price i would recommend that. Seen a 9600x for around the same price and if not cheaper than that 7600x  solid so far. Imo lean towards 850 watt, gives u more headroom tbh",Positive
AMD,"i wouldnt go with an amd gpu, but but that would last a while for sure. youre also probably looking at 1650 after taxes and shipping.",Neutral
AMD,"You can play around with memex or canada computers bundles and save roughly 100-200 [https://www.memoryexpress.com/Products/BDL\_MM00004639](https://www.memoryexpress.com/Products/BDL_MM00004639)  The prices listed for individual components in the bundles are upcharged but overall, you still save at least 100",Neutral
AMD,"Just a couple questions. Any idea why the memory and cooler show $0? Also it shows two separate coolers, is that required? I noticed you have a different power supply, Is the one I had just overkill for what is needed? As for the case is that mostly preference? Thanks again!",Neutral
AMD,"Will do, thanks!",Positive
AMD,Sounds good. Ill keep an eye out for some sales. Thanks!,Positive
AMD,"Okay, just not a fan of AMD? Would you recommend looking for maybe a 5060 TI if its close ish in price? Thanks!",Negative
AMD,"Yeah, ive checked memory express but didnt realize about the bundles. I will check them out. Thanks for your input!",Positive
AMD,"I think I had this pcpartpicker list linked to a bundle, but the bundle is no longer available. mobo, ram, and cpu bundles are always available, so just choose the cheapest one. I linked the cheapest one I saw that's atx in my other comment.  The cpu cooler wasnt listed on pcpartpicker, so I just linked the amazon link below. It's the same cooler, just added it there to add the price.  750w is fine, I just did 650w to help get the price down. msi mag is often the cheapest but most reliable psu's out there.  Case is mostly preference, but it can impact thermals and noise. Some cases have really shotty build quality as well. I choose lancool 207 because, per gamer nexus case review, its the best budget pc case for noise-normalized thermals.",Neutral
AMD,"i actually swapped to team red recently for the cpu. as for gpus though, AMD is definitely sub par. they might have decent clock speeds, but they dont have rtx, dlss, or good driver support. You can have all the horsepower in the world, but if your horses dont have a brain youre not going anywhere fast.  so yea, id stick with nvidia for this build if you can. maybe in a couple years AMD will have caught up to nvidia in terms of efficient drivers, but i doubt it. The only thing they have going for them atm is FSR, but its still not as good as DLSS.",Negative
AMD,"Interesting, thanks for the follow up!",Positive
AMD,I should have added the storage. 1tb nvme samsung add. With a 4tb WD external HDD.,Neutral
AMD,"I will say that going for Asus only on a limited budget is going to make it a little less efficient. If you went with a cheaper non-Asus motherboard, you might be able to fit a used 5600 into the build. Likewise for going for Corsair RAM; you generally pay extra for the branding unless it's on a sale, and with DDR4 I'd actually recommend going used since they're winding down production, so new prices go up, but lots of people are starting to switch to DDR5 systems and sell of their old RAM.  To get more in-depth, are you located in the US? If so, enter your stuff into PCPartPicker and it'll be easier to price-compare and suggest cheaper alternatives.",Neutral
AMD,"Get better RAM, 2x 16GB 6000 CL30.  You don't need thermal paste, just use the one that comes with the cooler.  Looks alright otherwise.  What type of games do you play?  For like AAA games on the 1440P monitor, a 9600X with a 9070 would be better.",Positive
AMD,Do you live near a microcenter? If you do get a mobo/cpu/ram bundle and use the extra money for a better gpu,Neutral
AMD,The Peerless Assassin 120 SE already comes with thermal paste. I don't know about it's quality but i don't think it's worth It to spend 10 dollars on another one when you already have It.,Negative
AMD,"I would go for 32gb ram, otherwise you are good to go. CPU will last for a long time so you can in the future just slap a new GPU 2-3 generations  in the future when your games slow down and be happy again :)",Positive
AMD,yeah these parts aren't great imo unless it is mainly for esports  9070 + 9600x for 1200. that leaves you 300 which is enough for a nice mini led 1440p monitor  [https://pcpartpicker.com/list/xFHkMC](https://pcpartpicker.com/list/xFHkMC)  using this combo  [https://www.newegg.com/Product/ComboDealDetails?ItemList=Combo.4818435&cm\_sp=product-combooption](https://www.newegg.com/Product/ComboDealDetails?ItemList=Combo.4818435&cm_sp=product-combooption)  and the gpu here [https://www.newegg.com/sapphire-tech-amd-11349-03-21g-radeon-rx-9070-16gb-graphics-card-double-fans/p/N82E16814202460?Item=N82E16814202460](https://www.newegg.com/sapphire-tech-amd-11349-03-21g-radeon-rx-9070-16gb-graphics-card-double-fans/p/N82E16814202460?Item=N82E16814202460),Negative
AMD,It is a very shitty gaming rig if your cpu costs as much as your gpu...,Negative
AMD,"The RAM is dumb.  That's a pretty weak GPU compared to a great processor, so if you play games that are CPU heavy/not GPU demanding, then that's fine. Especially so with the 1440p monitor.",Negative
AMD,If you're going to spend $1500 get a better GPU. At the very least get a 5070. Downgrade the CPU to a 9600x or something similar.,Neutral
AMD,Get a cheaper b650 mobo and get 2x16 ram with better timings.   People saying weak gpu are kinda right but it's good enough for the near future. So you can just upgrade your gpu in 5 or 6 years and leave everything else the same.,Positive
AMD,"- 16 GB -> Get 32 GB (seriously, it's 2025, not 2018. getting 16 GB is for 500$ budget systems)  - Ram at 5600 -> Get 6000  - Arctic Silver 5 -> Get Arctic MX6 (Arctic Silver 5 is the king from 2009, but MX4 and MX6 are the new value kings)",Neutral
AMD,Hard disk/ssd?,Neutral
AMD,"In a gaming rig, your GPU is usually more important than your CPU.    I’d downgrade the CPU to a 7600 or 7700x (or their newer 9000 series equivalents). And upgrade your GPU to an AMD 9070 or 9070xt or a Nvidia 5070 or 5070ti - get whichever card is cheaper at their respective tier in your market, in mine (UK) it’s AMD.   Also, you want 32gb of 6000 CL30 ram, and the CPU cooler you’ve selected already comes with adequate thermal paste (make sure you remove the peel on the cold plate before mounting).",Neutral
AMD,Man....PC gaming use to be affirdable,Neutral
AMD,"Do you already have a storage drive? Because if not, you'll need one.",Neutral
AMD,"Arctic silver was the best about 15 years ago, these days you should get Noctua nt-h2 for a good long lasting paste    And you need to get 2x16gb 6000mhz cl30 ram. Everything else looks pretty good to be. Thermalright phantom spirit is a newer version of peerless assassin but they both  perform really well so it's not a big deal   Edit: also yea in nearly all cases you should downgrade the CPU if it lets you afford a faster GPU, only exception is if you play competitive games on the lowest settings and demand the highest framerates, then going with a 7800x4d or 9800x3d can make sense. Otherwise you get much higher average fps by going with say a 7600x and a better GPU. Even the lowest end zen4 a 7400f would be good if you're going for budget. You could upgrade all the way to a 9800x3d in a few years on the same system, possibly even up to the unreleased next gen which is likely to stay on am5",Positive
AMD,"Only thing I would change is get a cheaper b850 mobo and get cl30 6000mts ddr5 ram instead of cl40 5200mts and go for a ryzen 7 9700x instead of a 7800x3d. Thts too much cpu for a 9060xt. Now if you were gonna get a 9070xt then I say maybe the 7800x3d is worth it but in most situations I’d say do just that, swap the x3d for a 9700x, get a cheaper b850 or even a b650 motherboard, & most importantly, switch that ram to a 2x16GB kit of 6000mts CL30 ram. 5200 cl40 is way too slow. Oh and you don’t need to buy thermal paste the assassin cooler will come with its own thermal paste.",Neutral
AMD,"Lots of good comments already, just condensing the most important parts: Get 2x16GB DDR5-6000 RAM (I use [G.SKILL Flare](https://pcpartpicker.com/product/cnbTwP/gskill-flare-x5-32-gb-2-x-16-gb-ddr5-6000-cl32-memory-f5-6000j3238f16gx2-fx5 ) in my 7800X3D build) Don’t buy thermal paste (use the tube included with the cooler) Get a better GPU, and if you need more budget for that, get a slightly cheaper CPU like the 7600X3D. You’d almost never notice that “downgrade” in 1440p gaming. This bit is personal preference, take it or leave it: I would go with the RM750x over RM750e.",Positive
AMD,As others have said switch the memory to ddr5 6000 CL30. I would drop the CPU to a 7600x and put the difference to a better gpu.  I also don’t see any storage. A good 2TB nvme will run $100-150,Neutral
AMD,Everything is great but the ram I would upgrade. It’s so fucking  easy to go really deep in the rabbit hole of specs and performance on certain parts but this build is honestly really good a will run every game you wanna play really well. Just upgrade the ram because overtime that will cause blue screens or your pc not to boot and it’s just annoying. But good luck on the build hope it turns out good!,Positive
AMD,"I would downgrade the CPU to maybe a 7700 (non-X or X doesn't really matter) - they're REALLY REALLY cheap on AliExpress from the SZCPU store front, but just be careful that you buy the one from SZCPU.  Downgrade the CPU so you can get 2*16GB RAM and possibly bump the GPU up to a 9070",Neutral
AMD,For the 7800x3d I would recommend a aio cooler instead of a air cooler. Also upgrade the ram! Get. 2x16 6000,Neutral
AMD,https://www.reddit.com/r/buildapc/s/Tbd1Asg6e3  Here you go op.,Neutral
AMD,"You should really look at what games you want to play and inform us. Simulation heavy games tax the CPU hard but generally, your CPU is way overpowered for the GPU, it's not a problem and gives you future proofing, but swapping from an X3D to a 7600X or 7700X should give you the budget for a 9070 XT which will be more beneficial across the board, you may even manage to do a 9600X or 9700X which will match very nicely with your chosen motherboard. Also, while the CPU cooler is a very effective choice, if you care about volume they are a bit on the loud side, especially with an X3D chip, little less so with an X chip. The RAM is the killer though, that's wiiiild. For gaming, it's very slow. Not only is the transfer rate low, the timing is also slow. Ditch the thermal paste (coolers come with it) and the saving should cover getting RAM in 6000MT/s and CL30, that's the sweet spot (often RAM is misadverised in MHz instead of MT/s, as long as it says 6000 you're good) Those changes should give you a nice well rounded build and still stay in budget.",Neutral
AMD,I would go with the Innocn 1440p 240hz for $130 on amazon,Neutral
AMD,"For 1500 you can get a much better GPU CPU combo.  Feel free to swap the 5070 for a 7090 XT if that's your flavor.  [PCPartPicker Part List](https://pcpartpicker.com/list/jrbDWc)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $178.51 @ Amazon  **CPU Cooler** | [Thermalright Peerless Assassin 120 SE 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/hYxRsY/thermalright-peerless-assassin-120-se-6617-cfm-cpu-cooler-pa120-se-d3) | $34.90 @ Amazon  **Motherboard** | [ASRock B650M-H/M.2+ Micro ATX AM5 Motherboard](https://pcpartpicker.com/product/xFmNnQ/asrock-b650m-hm2-micro-atx-am5-motherboard-b650m-hm2) | $99.99 @ Amazon  **Memory** | [Patriot Viper Venom 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/4cCCmG/patriot-viper-venom-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-pvv532g600c30k) | $85.99 @ Newegg  **Storage** | [Crucial P3 Plus 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/yGZ9TW/crucial-p3-plus-2-tb-m2-2280-nvme-solid-state-drive-ct2000p3pssd8) | $119.95 @ iBUYPOWER  **Video Card** | [Asus PRIME GeForce RTX 5070 12 GB Video Card](https://pcpartpicker.com/product/wtJBD3/asus-prime-geforce-rtx-5070-12-gb-video-card-prime-rtx5070-12g) | $549.99 @ B&H  **Case** | [Montech XR ATX Mid Tower Case](https://pcpartpicker.com/product/fc88TW/montech-xr-atx-mid-tower-case-xr-b) | $73.93 @ Amazon  **Power Supply** | [MSI MAG A750GL PCIE5 750 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/dbCZxr/msi-mag-a750gl-pcie5-750-w-80-gold-certified-fully-modular-atx-power-supply-mag-a750gl-pcie5) | $99.99 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$1243.25**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-09-08 19:14 EDT-0400 |",Positive
AMD,"I have bought the `Thermalright Peerless Assassin 120 SE` too, it comes with a thermal paste in the package, no need to buy another one. The `Asus TUF GAMING B850` is good too and it supports ECC RAM, but ultimately I decided to go with the `MSI MAG B850 Tomahawk`.    I personally prefer having a beefier CPU than spending more on a GPU, but that's because I don't play games often, and I only play lightweight games and always on lowest settings anyway. As others have mentioned, if you like to play those fancier and more modern games, it might be a good idea to invest more on a better GPU and downgrade the CPU.    However, I think most people will agree with me that a 2x8GB DDR5 RAM kit is a waste of money. RAM is literally one of the cheapest components in the system given their importance, and having 32GB is a no-brainer.",Positive
AMD,"40-60% of your budget should be GPU last I researched budgets (subtracting peripherals). Your cpu is less important at 1440p and above. Having a top of the line cpu ensures you'll never need to swap it out anytime soon, but you're going to need to replace that gpu sooner rather than later.",Neutral
AMD,Upgrade the ram to 32 for sure,Neutral
AMD,"Cpu is very overpowered compared to the gpu, especially in 1440p. Save some money on the cpu, go for 32gb of ram (make sure the first word latency is 10ns or less) instead of 16gb. If any budget remains, either go for a 9070 or look at keyboard/mouse/audio upgrades seeing as you are upgrading the monitor.",Negative
AMD,Not bad.,Positive
AMD,Definitely ditch that thermal paste for a first time build as getting it anywhere other then cpu or gpu core will fry your shit just so u know and I would get a 7600 cpu and better gpu you can look into solid older cards and pick one up cheap on eBay just upgrade the gpu in the future get like a 7800 XT or 6800 xt if u want amd or a 4070 ti used they are around same price and better,Neutral
AMD,"There's a lot of comments (well, all the comments), about how bad this is assuming it's a gaming PC.  So what is the ""work"" you're going to do on this thing?  That needs to be known to really give you any useful advice.",Negative
AMD,"The first PC that I just built has very similar parts. The biggest red flag to me is the PSU, you want at least an 850W so that you have room to upscale your parts and you're not at any risk of not meeting the power requirement when you do. 750W is really the absolute bare minimum, but I don't think any experienced builder would recommend anything below 850W for these parts, especially if you want to upgrade. I used a [Montech Century II ](https://www.googleadservices.com/pagead/aclk?sa=L&ai=DChsSEwiIlpKi8sqPAxXlKtQBHVsqJToYACICCAEQBBoCb2E&co=1&gclid=CjwKCAjw_fnFBhB0EiwAH_MfZg8tym3XodLJL5T0LEM7nd2B1dj2jypvNMc5NqE2H-ZcCATukVssTRoCS3kQAvD_BwE&ohost=www.google.com&cid=CAESeuD2lWDN_RnBsbsVxN20Qb7orN_E2dCV45coDZf_SmrOQgNeSx8tgdtMlqo9hW-RkC4HLgMYmjqnCfdVX9kEfIXvFeadpYYUhkRYbezva9HEPB4SnoV59g_kCZmzaL4aEv9RBIn73ohzpRgguX368-P2tLbRaRy2B7gF&sig=AOD64_0KyZIUEZL_TVQeNaUPyHsReEJ8Kw&ctype=5&q=&ved=2ahUKEwio14yi8sqPAxVvMtAFHU4fA00Q9aACKAB6BAgpEA4&adurl=)for my build, it retails around $89.  Your ram is also on the low end, you'll want 32GB (2x16) probably, just make sure you're buying the right version for an AMD CPU.  Also make sure you're using a good SSD, a lot of them were reported to have issues when I was doing my research. The only real recommendations I found were for the Samsung SSDs. I got a 990 EVO for my system.  The Phantom Spirit is technically an upgrade over the Peerless Assassin for your cooler.  As a final note, AMD GPUs are giving more performance per dollar compared to NVIDIA, but I would still reccommend you start looking at least at the 5060ti (16gb) before you commit to the AMD card. I paid the extra for the 5070ti for my PC and I am really glad that I did.",Neutral
AMD,"Solid part choices, double your RAM amount though. 16 gigs IS NOT enough for upper-midrange gaming. 16 gigs aptly described as bare mimimum for mainstream gaming. 8 gigs is realistic minimum for basic computing. RAMS so cheap compared to other parts. Go for the gusto. Get 32 or even 48. Anything above 32 isn’t nessecary and anything above 48 is just to show off. For Gaming. 32+ always , nowadays. Just focus on getting RAM that you know you can count on, reviews. After that comes speed.  If I’m not mistaken though, the X3D chips really benefit from extra ram speed. The higher the frequency, the better, and the lower the CAS (CL) latency, the better.  I’ve always used Corsair and Kingston RAM 🐏 , never let me down.",Neutral
AMD,"9600x + 9070 would give you more performance than 7800x3d + 9060 xt   \[PCPartPicker Part List\](https://pcpartpicker.com/list/Pz2qFZ)    Type|Item|Price  :----|:----|:----  \*\*CPU\*\* | \[AMD Ryzen 5 9600X 3.9 GHz 6-Core Processor\](https://pcpartpicker.com/product/4r4Zxr/amd-ryzen-5-9600x-39-ghz-6-core-processor-100-100001405wof) | $194.99 @ Newegg   \*\*CPU Cooler\*\* | \[Thermalright Peerless Assassin 120 SE V3 70.84 CFM CPU Cooler\](https://pcpartpicker.com/product/vZfV3C/thermalright-peerless-assassin-120-se-v3-7084-cfm-cpu-cooler-pa120se-v3) | $34.90 @ Amazon   \*\*Motherboard\*\* | \[Gigabyte B650 AORUS ELITE AX V2 ATX AM5 Motherboard\](https://pcpartpicker.com/product/KrWJ7P/gigabyte-b650-aorus-elite-ax-v2-atx-am5-motherboard-b650-aorus-elite-ax-v2) | $169.99 @ Amazon   \*\*Memory\*\* | \[Patriot Viper Venom 16 GB (1 x 16 GB) DDR5-6000 CL30 Memory\](https://pcpartpicker.com/product/nxFCmG/patriot-viper-venom-16-gb-1-x-16-gb-ddr5-6000-cl30-memory-pvv516g60c30) | $44.99 @ Amazon   \*\*Memory\*\* | \[Patriot Viper Venom 16 GB (1 x 16 GB) DDR5-6000 CL30 Memory\](https://pcpartpicker.com/product/nxFCmG/patriot-viper-venom-16-gb-1-x-16-gb-ddr5-6000-cl30-memory-pvv516g60c30) | $44.99 @ Amazon   \*\*Storage\*\* | \[Silicon Power UD90 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive\](https://pcpartpicker.com/product/f4cG3C/silicon-power-ud90-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-sp02kgbp44ud9005) | $95.99 @ B&H   \*\*Video Card\*\* | \[ASRock Challenger Radeon RX 9070 16 GB Video Card\](https://pcpartpicker.com/product/KXkqqs/asrock-challenger-radeon-rx-9070-16-gb-video-card-rx9070-cl-16g) | $599.99 @ Newegg   \*\*Case\*\* | \[Montech XR ATX Mid Tower Case\](https://pcpartpicker.com/product/fc88TW/montech-xr-atx-mid-tower-case-xr-b) | $73.93 @ Amazon   \*\*Power Supply\*\* | \[MSI MAG A750GL PCIE5 750 W 80+ Gold Certified Fully Modular ATX Power Supply\](https://pcpartpicker.com/product/dbCZxr/msi-mag-a750gl-pcie5-750-w-80-gold-certified-fully-modular-atx-power-supply-mag-a750gl-pcie5) | $99.99 @ Amazon   \*\*Monitor\*\* | \[Acer VG271U M3bmiipx 27.0"" 2560 x 1440 180 Hz Monitor\](https://pcpartpicker.com/product/c6mNnQ/acer-vg271u-m3bmiipx-270-2560-x-1440-180-hz-monitor-vg271u-m3) | $179.99 @ Newegg    | \*Prices include shipping, taxes, rebates, and discounts\* |   | \*\*Total\*\* | \*\*$1539.75\*\*   | Generated by \[PCPartPicker\](https://pcpartpicker.com) 2025-09-09 04:34 EDT-0400 |",Neutral
AMD,"Really depends on what you are doing with it, for generic gaming a 7500F is more than enough, that would save you 200$ to add to the GPU.  Change the ram to 32GB CL30 6000 as well.  A (very rough) rule of thumb is that for a gaming system, the GPU should cost close to half of the total system price.",Neutral
AMD,"[PCPartPicker Part List](https://pcpartpicker.com/list/X74F2x)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 9600X 3.9 GHz 6-Core Processor](https://pcpartpicker.com/product/4r4Zxr/amd-ryzen-5-9600x-39-ghz-6-core-processor-100-100001405wof) | $194.99 @ Newegg  **CPU Cooler** | [Thermalright Phantom Spirit 120 SE ARGB 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/MzMMnQ/thermalright-phantom-spirit-120-se-argb-6617-cfm-cpu-cooler-ps120se-argb) | $37.90 @ Amazon  **Motherboard** | [Gigabyte B850 AORUS ELITE WIFI7 ATX AM5 Motherboard](https://pcpartpicker.com/product/72RnTW/gigabyte-b850-aorus-elite-wifi7-atx-am5-motherboard-b850-aorus-elite-wifi7) | $189.99 @ Best Buy  **Memory** | [Silicon Power Value Gaming 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/cCKscf/silicon-power-value-gaming-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-sp032gxlwu60afdeae) | $85.97 @ Silicon Power  **Storage** | [SK Hynix Platinum P41 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/yGTp99/sk-hynix-platinum-p41-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-shpp41-2000gm-2) | $124.99 @ Newegg  **Video Card** | [MSI SHADOW 3X OC GeForce RTX 5070 12 GB Video Card](https://pcpartpicker.com/product/fxHp99/msi-shadow-3x-oc-geforce-rtx-5070-12-gb-video-card-rtx-5070-12g-shadow-3x-oc) | $545.99 @ Amazon  **Case** | [ENDORFY Ventum 200 ARGB ATX Mid Tower Case](https://pcpartpicker.com/product/6bCZxr/endorfy-ventum-200-argb-atx-mid-tower-case-ey2a014) | $72.00 @ ModMyMods  **Power Supply** | [Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $89.90 @ Amazon  **Monitor** | [ASRock PG27QFT1B 27.0"" 2560 x 1440 180 Hz Monitor](https://pcpartpicker.com/product/LfzXsY/asrock-pg27qft1b-270-2560-x-1440-180-hz-monitor-pg27qft1b) | $157.77 @ Newegg   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$1499.50**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-09-09 07:04 EDT-0400 |",Neutral
AMD,U can get a 5070 and a 9600x for $700 so I just upgraded to that. Seems like the best deal on the market. Built a nice pc for $1200 couple days ago,Positive
AMD,"I say get a cheaper cpu since 7800x3d is overkill for a 9060xt. I recommend a 9600x and use that extra money for much better RAM, if you do it right you could get at least 32gb (2×16) RAM as 8gb will really bottleneck performance and what games you can play.",Neutral
AMD,"Overkill cpu, allocate some of those funds for a better gpu.  Also 32GB Ram is what you should be aiming for.",Neutral
AMD,"Assuming u live in the US, try going to micro center for some kind of bundle. Also, do get 32 gigs of faster ram (6000C30).   I personally would also recommend considering Intel, as some of their cpus are very cheap right now (I got an open box ultra 7 265k at $200). However, if you want to stick with AMD x3d chips I also totally get it. Just try your best to put more budget onto your gpu for a more balanced set up.  I'll say that this gaming rig is not stupid at all, but it can be improved according to what kind of gaming you want.",Positive
AMD,"Generally, yes. The system would be better-balanced for all kinds of gaming if you had a faster gpu and less overkill cpu - go for a 9700x and a 9070 or 9070xt (or 5070/5070ti).   You should also get better ram. 6000mhz cl30 is what you want, in 2x16gb configuration.   Otherwise, you did a very good job for your first time. That’s pretty close to a 10/10 list with the changes I suggested.   If you only play competitive shooter games, then I would actually stick with the cpu and gpu choice you made and just change the ram. If you plan on playing a variety of games, or more graphics-intensive games, I strongly suggest the changes to the cpu and gpu. The changes I suggested will also be great for shooters as well, just more versatile overall.",Positive
AMD,[https://pcpartpicker.com/list/Tj4DLc](https://pcpartpicker.com/list/Tj4DLc),Neutral
AMD,At 1440p you should spend $100-150 less on the CPU and $100-150 more on the GPU.,Neutral
AMD,You need a better power supply. Gor the the RM 850x. Better warranty and parts.,Positive
AMD,Probably ok with stock cooler,Neutral
AMD,would you say this level of cpu for this build is unnecessary?,Negative
AMD,The 6000 speed is important for the 7800x3d because the memory controller talks to the RAM at 6000 so you will have a nice stable 1:1 connection.,Positive
AMD,"I'm considering buying the same CPU/gpu combo, but I've read on a lot of comments that CPU is overkill. What if I plan on using my PC for CAD, and 3d animations too though?",Neutral
AMD,Nothing worse than having to take your cooler off and not have thermal paste to re-apply.   Always have extra thermal paste.,Negative
AMD,This is the only major thing i would change on that build. Nothing wrong with the rest. Maybe even lower CAS modules like cl 28. This would be the single most important thing for performance.,Positive
AMD,"This should be higher, OP is quite budget constrained. Getting the 7600x3d bundle from MC would save enough budget for OP to make a couple other small changes and get a 9070 xt.",Neutral
AMD,I got mine 10 days ago and paste that came with it was thick and had chewing gum texture. Id spend 5 bucks on new paste.,Negative
AMD,"Keep in mind also to look for combo deals! Winter is coming and with that, black Friday and Christmas sales!",Positive
AMD,"Oh I could consider this, I'll fiddle around with this one a little. (Also that case is adorable I love it)",Positive
AMD,Paradox games enjoyers in shambles.,Negative
AMD,Not if he plan to play esports titles only,Neutral
AMD,"Id rather buy it like this and upgrade the gpu later, than have to upgrade the whole system later",Neutral
AMD,What GPU should I go with?,Neutral
AMD,650 is usually pcie 4 right? If he is gunna upgrade gpu in the future might want to make sure it has pcie5,Neutral
AMD,"It still is, people just think they need 60+fps in 4k native 🤣   That 9060xt ""budget"" $400 gpu card runs Witcher 3 at 120fps in 1440 and 70fps in 4k.  The 5070 they mention does 220fps in 144 and 127fps in 4k for $550.  Unless your game is processor bound, which very few are *cough baulders gate cough* if you swap a $150 cheaper processor for a $150 better gpu you will likely get way more frames for the same total $$.",Neutral
AMD,"Oh yeah I forgot to mention, I have 2 M.2 Nvme SSDs leftover from my previous PC. I'll just use those.",Neutral
AMD,"I was going to suggest a higher wattage supply as well, OP may not need it immediately, but if they do other upgrades later, the few extra dollars now will eliminate a complete replacement later.",Neutral
AMD,Utter nonsense...,Negative
AMD,"The total draw of this system is 450w max, what are you on about? Why on earth would he need more than 750w? It's just a waste of money.",Negative
AMD,The 7800X3D doesn't come with a stock cooler.,Neutral
AMD,this depends on the type of game you play and the resolution. with 1440p I agree that in general a 9070 is more appropriate.,Neutral
AMD,"Small correction: It always talks to ram at 1:1 not 6000mhz. If the op fits a 5200MHz ram, the Fclk will match it.  The controller will maintain that 1:1 relationship up to 6000mhz ram (by default, with overclocking it can be forced in bios to keep it even at higher speeds but it can be unstable).   Since the Fclk & memory subsystem speed is a bottleneck on the Ryzen platform it's beneficial to maximise it by running 6000MHz.",Neutral
AMD,The extra cache on the 9800X3D boosts its gaming performance.  It's not really better than the 9700X for anything other than gaming.,Neutral
AMD,I agree but the pa 120 comes with enough for a few repast and it’s decent quality I’ve seated two so far and still have some left.,Positive
AMD,"This, look into CAS latency and how it actually effects your CPU performance compared to a lower CAS latency",Neutral
AMD,"Exactly, 1500 is the threshold where a 9070xt or 5070ti is possible but only with good deals and sales, and microcenter is the FIRST place to look for those",Positive
AMD,"I get what you're saying, but $1500 is quite budget strained? I'd think that money buys a build that is pretty solidly above average.",Neutral
AMD,yeah u can also swap down to the 5070 to save some money it is 524 and also comes with borderlands 4 for free rn,Neutral
AMD,"Bought a 7800X3D just for cities skylines, comes in handy too multitasking with those types of games",Positive
AMD,"Don't think ""gonna do some gaming and work"" means esports player...",Neutral
AMD,"Don't let these guys upsell you with GPUs you don't need for the games you play.  You could definitely go with a 9600x for a CPU and stick with the same GPU and easily upgrade that a few years down the line if you wanted to save some money.  The other advice on RAM is good, everything else is fine.",Neutral
AMD,The difference is not noticeable. Usually just 1%,Neutral
AMD,"i had acquired a i9 14900k for almost free.  piecing the system together would of costed me $1,200 to complete it.   and that's without a gpu too.  kicked in the nuts",Negative
AMD,"Exactly. The more you buy, the more you save.",Neutral
AMD,It wouldn't reach 450W even if you ran Furmark + Prime...,Negative
AMD,because the CPU is good for a long time.  That GPU will be upgraded in 3 years,Positive
AMD,Oh no. Better buy one then,Negative
AMD,"I meant the CPU. Its seems like an overkill, I was asking how we could balance that if it is",Negative
AMD,"I see, thank you.",Positive
AMD,I got mine 2 weeks ago and paste that came with it was thick like chewing gum so I had to toss it away.,Negative
AMD,"You are just assuming though.   Regardless of that you said that is a shity build, you didn't said anything about being good for the OP. Also having a good cpu could be beneficial to work, he didn't specified what kind of work he is doing",Negative
AMD,I don't see the point in arguing over whether OP is playing eSports games or not.  Until the OP specifies whether it is or isn't either a balanced CPU / GPU or GPU focused build are valid recommendations.,Neutral
AMD,"Just because they don't NEED a 9070 over the 9060 doesn't mean they won't benefit from it, and the consensus is that the CPU is way more than needed so shift budget from one to the other to balance performance better. No, the 9070 XT isn't strictly necessary but it's definitely beneficial. Either way the point of the advice is more that the CPU and GPU are far apart enough that you can upgrade one, downgrade the other and still have a nice balance.",Neutral
AMD,"Oh okay yeah, I just need it to last for 2 years or so I can always upgrade if my interests get more demanding. I think it'll be enough. Thanks!",Positive
AMD,Why so expensive? I was building 9800x3d systems for around $1k with no gpu? You in a silly country?,Negative
AMD,What exactly are you trying to say? OP included a cooler in their build.,Neutral
AMD,"game dependent, I would look more at a 7600X3D or a 9600X/9700X and lean into a 9070/9070XT. More GPU will do more good than a top tier CPU.",Positive
AMD,"If not specified differently, we are building a general, all-around gaming rig, and in that case you most definitely don't want your cpu to be as expensive as your gpu. That cpu is terrible for work, you can get faster for half the price...",Negative
AMD,buy a cooler?,Neutral
AMD,>More GPU will do more good than a top tier CPU.  That was my mindset...,Positive
AMD,OP could play 4x games or other CPU intensive games,Neutral
AMD,I'm sure there is lines of work that would benefit with the extra cache,Neutral
AMD,They already included a cooler. Why would you go out of your way to tell them to do something they are already planning on doing.,Negative
AMD,And I'm sure the OP doesn't do that.,Negative
AMD,"I'm not, though.",Neutral
AMD,"Again you are assuming, he never said that",Negative
AMD,You made multiple posts encouraging OP to buy a cooler. They already stated that they were going to buy a cooler. Why did you post that they should buy a cooler?,Neutral
AMD,"If he was doing something very specific and needed a bulid for that, would he not mention what it was?",Neutral
AMD,i've done no such thing,Neutral
AMD,"Not everyone thinks to be that specific when posting here. Even on Blackmagic's forum, we often have to ask the OP to spell out what they have, what version etc so we can better help him.  You may very well be assuming based on what is supposedly not said.",Neutral
AMD,> Oh no. Better buy one then  What was this post about then?,Negative
AMD,"ok, so I like where u are going with this build but at $1100 we might want to go with a lower tier CPU for now. this is what I got  [https://pcpartpicker.com/list/NTLrkf](https://pcpartpicker.com/list/NTLrkf)   .    .Total:	$1129  So right now the Ryzen 5 7600 is on sale and will come with a decent stock cooler so no need to buy one. with it I picked a very nice motherboard that has plenty of USBs and a large VRM so it can handle any CPU ur son might upgrade to later. this is just how I see it but if upgrading is something that is important then getting a good motherboard will help with that down the line.  Now I did change the RAM to a kit of 6000mts CL30 this is a better value kit and most AM5 CPUs dont like running above 6000mts anyways. I also switched the SSD to a more reliable and better quality one for a similar price.   I like the case and that's kinda a personal choice so if u son likes it then Id keep it. I also think that is a really good choice for a GPU. I did swap out the PSU for a decent 750w unit, this will give him just a little more room for upgrades in the future and is a good investment  this list isn't perfect or the best value so feel free to change it to match ur budget or style. hope this helps",Positive
AMD,Are you near a micro center store?,Neutral
AMD,Thermalright phantom spirit is a much better cpu cooler for a similar price.,Positive
AMD,Gosh I really appreciate this. I especially appreciate how you explained your changes which gave me a lot more knowledge generally. Thank you!,Positive
AMD,Do you think it would make any sense to go with the higher spec CPU and lower GPU given his interest in those types of CPU heavy games? Or is it splitting hairs at some point?,Neutral
AMD,No I’m not. Sorry I should have mentioned that.,Neutral
AMD,Thank you!,Positive
AMD,"So CPU heavy really just means the games performance will depends more on the CPU than it will the GPU. So at this price point if u are buying new parts I wouldn't go any lower than a 9060xt 16gb. The 7600 is more than capable of getting reasonable FPS in any game like u mentioned above.   If the budget was 1250 or so then I would say its definitely worth considering but not at 1100.   Plus the way this is built gives u room to upgrade later if the performance becomes an issue, so I wouldn't say u are splitting hairs its definitely something to think about. But I think u will be fine",Neutral
AMD,"Oh also, if your son likes Factorio, tell him to take a look at Shapez 2. I've been having a blast with it.",Positive
AMD,"All good! I'd go with this build instead. It just doesn't make sense to pair a high-end CPU with a low-end GPU unless you have very specific requirements.  You can also use windows for free with a watermark until you have money to buy a licence later.   [PCPartPicker Part List](https://pcpartpicker.com/list/Vc9TLc)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $177.00 @ Amazon  **CPU Cooler** | [ID-COOLING SE-214-XT ARGB 68.2 CFM CPU Cooler](https://pcpartpicker.com/product/Zr8bt6/id-cooling-se-214-xt-argb-682-cfm-cpu-cooler-se-214-xt-argb) | $16.98 @ Amazon  **Motherboard** | [MSI PRO B650-A WIFI ATX AM5 Motherboard](https://pcpartpicker.com/product/V7tLrH/msi-pro-b650-a-wifi-atx-am5-motherboard-pro-b650-a-wifi) | $149.99 @ Newegg  **Memory** | [Patriot Viper Venom 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/4cCCmG/patriot-viper-venom-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-pvv532g600c30k) | $85.99 @ Newegg  **Storage** | [TEAMGROUP MP44L 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/2x4Ycf/teamgroup-mp44l-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-tm8fpk001t0c101) | $54.99 @ Newegg  **Video Card** | [XFX Swift OC Radeon RX 9060 XT 16 GB Video Card](https://pcpartpicker.com/product/cD7MnQ/xfx-swift-oc-radeon-rx-9060-xt-16-gb-video-card-rx-96ts316b7) | $374.99 @ Newegg  **Case** | [Lian Li Lancool 207 ATX Mid Tower Case](https://pcpartpicker.com/product/zysV3C/lian-li-lancool-207-atx-mid-tower-case-lan207rx) | $82.99 @ Amazon  **Power Supply** | [Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $89.90 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$1032.83**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-09-09 19:02 EDT-0400 |",Neutral
AMD,Thank you!,Positive
AMD,Thanks!,Positive
AMD,9070XT.,Neutral
AMD,"I have a 9060 XT and it does well with 1440p games. I would get that plus the 32GB RAM, which is now the recommendation for most games.",Positive
AMD,"9070xt if you’re considering that option. It is on par with the 5070ti in every aspect except dlss is better than FSR, and nvidia cards have more ray tracing cores, so better performance in ray tracing.   The 9070xt is better in every single way than the standard 5070. The 5070ti is closer to the 5080 than the 5070, so take that information as you want regarding the 9070xt.  RAM matters more if you’re installing a dozen mods or running half a dozen background apps while gaming. 16gb is enough if gaming is all you’re doing. Ram usage is adaptive, meaning it will lower performance of background apps while prioritizing memory performance of the app or game you’re using",Positive
AMD,RTX 5070,Neutral
AMD,"Just curious, why not a 9070?",Neutral
AMD,"Out of those the 9070 XT. If that is an option I don’t even know why you’re having 9060 XT as am option. Better is better.   If you could go up to a 5070 Ti than that over the 9070 XT.  You should get 32Gb RAM anyways, RAM is so cheap now, just save up a bit more and get 9070 XT + 32gb",Neutral
AMD,9070xt,Neutral
AMD,"9060XT is basically a 7700XT/6800 with FSR4 and other modern features + more efficient, I would suggest buying the 16GB model.",Positive
AMD,I have a 6750XT paired with a 5600X. It plays pretty well at 1440p but I am really concerned with performance on Bordelands 4 the upcoming UE5 game.  For me that you give me close to 2x uplift on performance (excluding any CPU limitations),Neutral
AMD,I have an rtx 3060 gpu as well as a amd 5900x cpu and i play games at 1440p. What are you playing that makes you need something so over powered?,Neutral
AMD,"Don’t think you can really go wrong here, but the price difference is so wild that it’s tough to give an actual recommendation. A 9060xt 16gb is, to me, the utilitarian choice. If you just want a card that can do 1440p/60+ in new AAA stuff with reasonable settings and smash everything else, that’s your best bet, and it’s the best “budget” card the market has seen in a long time. If you gave me 1000 bucks to build a PC, that’s the graphics card I’d pick, no questions asked. Built my gf a pc recently and did exactly that. The 5060ti 16gb isn’t half bad either if you can find it at MSRP in your region and the Nvidia feature set is worth the extra 60-80 bucks to you.   The other two cards are substantially faster, particularly the 9070xt, but I’m not a huge fan of either right now for different reasons. I wouldn’t pay more than 500 bucks (USD) for a 12gb card like the 5070, and the 9070xt is just too expensive in the US. Although admittedly it’s much more reasonably priced where you’re at, 650 is not too bad.",Neutral
AMD,"For what you want the 9060XT should be fine and its significantly cheaper than the others, is it the 8 or 16GB version, most times it will be fine either way, but hopefully it's the 16GB.",Positive
AMD,Where are you finding a 9060 xt for $410 CAD? cheapest I found was an open box for $465,Neutral
AMD,9070 non xt is also a good option.,Positive
AMD,9070xt. Just monitor yung ram when gaming. Close other apps if needed,Neutral
AMD,5700xt can play decently any game at 1440p,Neutral
AMD,"I only recommend this if you 1: Have a job, 2: Are responsible, and 3: Have the money to outright buy it rn.  Imo I would finance the 9070XT on Newegg with PayPal pay in 4. You're looking at four $189 installments every 2 weeks. There is 0% interest so you dont have to worry about paying more. Now with the money that you save from financing you can also get that 32GB of ram instead of having to sacrifice it.  Now like I said before I would only recommend this if you have all of the three points that I listed.",Positive
AMD,Or 5070 ti if it’s close in price,Neutral
AMD,"Did you, by chance, test it on bf6 on 1440p? I have a 3060Ti and it ran decently but I am afraid it will not hold up to a full conquest server.",Negative
AMD,Fsr4 and dlss4 are about equal some games dlss is better other fsr is better main thing is just that dlss is in more games  Still 9070xt is a good card depending on what country you are in it can be quite alot cheaper then the 5070ti and in most games they match.,Positive
AMD,"Zero reason to buy a 5070 when he has a working card right now. We'll have 5070 Super 18gb in 3-4 months. If he is in dire need of a nvidia card at the 550 USD range then sure buy it, but if he can hold on he should, or just get a 9070 xt",Neutral
AMD,Because the cheapest 9070 XT I'm looking at getting is only 26 CAD (\~19 USD) more expensive than the cheapest 9070,Negative
AMD,"Oh I should have clarified about the 9060 XT, yes it is the 16 GB I'm considering",Neutral
AMD,"Ive never played BF6, but there's been tests done on it, confirming it can reach 60FPS+ on 1440p Ultra with a 7600X CPU.",Neutral
AMD,Nice of you to assume anyone will be able to buy that as MSRP sooner than 6 months after launch,Neutral
AMD,"Oh ok interesting. I’ve been looking at upgrading into pretty much the exact price range you’ve got here and I’ve heavily considered a 9070 (about a $100 USD difference from the XT from what I can find), but if they’re that close in price that makes a lot of sense",Positive
AMD,Oh sweet. I haven’t even considered Ultra settings haha. My GPU runs it on low with DLSS and it still looks great on 1440p. Thanks a bunch!,Positive
AMD,"[PCPartPicker Part List](https://pcpartpicker.com/list/982FVF)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $177.00 @ Amazon  **CPU Cooler** | [Thermalright Phantom Spirit 120 SE 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/CwGhP6/thermalright-phantom-spirit-120-se-6617-cfm-cpu-cooler-ps120se-black) | $34.90 @ Amazon  **Motherboard** | [ASRock B650M-HDV/M.2 Micro ATX AM5 Motherboard](https://pcpartpicker.com/product/Dq4Zxr/asrock-b650m-hdvm2-micro-atx-am5-motherboard-b650m-hdvm2) | $112.99 @ Newegg  **Memory** | [Silicon Power Value Gaming 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/cCKscf/silicon-power-value-gaming-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-sp032gxlwu60afdeae) | $85.97 @ Silicon Power  **Storage** | [TEAMGROUP MP44L 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/2x4Ycf/teamgroup-mp44l-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-tm8fpk001t0c101) | $54.99 @ Newegg  **Video Card** | [XFX Swift OC Radeon RX 9060 XT 16 GB Video Card](https://pcpartpicker.com/product/cD7MnQ/xfx-swift-oc-radeon-rx-9060-xt-16-gb-video-card-rx-96ts316b7) | $374.99 @ Newegg  **Case** | [Montech XR ATX Mid Tower Case](https://pcpartpicker.com/product/fc88TW/montech-xr-atx-mid-tower-case-xr-b) | $73.93 @ Amazon  **Power Supply** | [Lian Li EDGE GOLD 750 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/Jjy8TW/lian-li-edge-gold-750-w-80-gold-certified-fully-modular-atx-power-supply-eg0750g-black) | $81.99 @ Newegg Sellers  **Monitor** | [MSI G274QPF-QD 27.0"" 2560 x 1440 170 Hz Monitor](https://pcpartpicker.com/product/LfvD4D/msi-g274qpf-qd-270-2560-x-1440-170-hz-monitor-g274qpf-qd) | $198.00 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$1194.76**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-09-09 18:02 EDT-0400 |  7600X with better cooling, same GPU, better monitor.",Neutral
AMD,Chat am I an idiot I copy/pasted from PCPartPicker why isn't it formatted as a table,Negative
AMD,"Both lists below are better builds around $1200, get you on AM5 you could upgrade cpu down the road if need to.",Positive
AMD,"Not great for the cost.  [PCPartPicker Part List](https://pcpartpicker.com/list/RXtth7)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $177.00 @ Amazon  **CPU Cooler** | [Thermalright Assassin X 120 Refined SE 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/q6H7YJ/thermalright-assassin-x-120-refined-se-6617-cfm-cpu-cooler-ax120-se-d3) | $17.89 @ Amazon  **Motherboard** | [Gigabyte B650M GAMING PLUS WIFI Micro ATX AM5 Motherboard](https://pcpartpicker.com/product/9HTZxr/gigabyte-b650m-gaming-plus-wifi-micro-atx-am5-motherboard-b650m-gaming-plus-wf) | $133.75 @ Amazon  **Memory** | [Silicon Power Value Gaming 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/cCKscf/silicon-power-value-gaming-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-sp032gxlwu60afdeae) | $85.97 @ Silicon Power  **Storage** | [TEAMGROUP MP44L 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/2x4Ycf/teamgroup-mp44l-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-tm8fpk001t0c101) | $54.99 @ Newegg  **Video Card** | [PowerColor Reaper Radeon RX 9060 XT 16 GB Video Card](https://pcpartpicker.com/product/fzh2FT/powercolor-reaper-radeon-rx-9060-xt-16-gb-video-card-rx9060xt-16g-a) | $369.99 @ Newegg  **Case** | [Okinos Aqua 3 MicroATX Mini Tower Case](https://pcpartpicker.com/product/FfQKHx/okinos-aqua-3-microatx-mini-tower-case-okicc-aqua3-matxb-h3ba) | $64.98 @ Amazon  **Power Supply** | [Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $89.90 @ Amazon  **Monitor** | [Acer Nitro XV1 XV271U M3bmiiprx 27.0"" 2560 x 1440 180 Hz Monitor](https://pcpartpicker.com/product/p8zXsY/acer-nitro-xv1-xv271u-m3bmiiprx-270-2560-x-1440-180-hz-monitor-umhx1aa301) | $199.99 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$1194.46**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-09-09 17:58 EDT-0400 |",Neutral
AMD,I think the 9600X bundle and a 9060 XT 16 GB will be your best bet.,Neutral
AMD,"I would actually save about $150-$200 more to get their 7700x bundle instead for $400. That $299 bundle is only giving you 16gb of ram instead of the optimal 32gb. That combined with a 9060XT 16gb will be much better for you, and is cheaper than buying a 9600x+motherboard+RAM individually. Going for the 16gb Ram kit will screw you in the long term.",Neutral
AMD,"If you could save up just a bit more to do both, that’d be ideal!",Positive
AMD,"The problem with mc 9600x combos is the 16gb of ram, otherwise they are great.",Neutral
AMD,option 3 100%,Neutral
AMD,3. going for the 9800x3d and not upgrading your gpu doesn’t make sense,Negative
AMD,if you chose option 3 you need to think about your motherboard i guess,Neutral
AMD,i think the Ryzen 7 9800 x3d is good bundle with asus tuf mobo and 16 gb ram for 629 ish,Positive
AMD,"i will suggest Upgrade both to a Ryzen 9600X (MC bundle of 299) plus a Radeon 9060XT (350-400).Here’s why:  •   Your current 3600X will bottleneck any serious GPU upgrade.  •   The 9800X3D bundle is a beast, but pairing it with a 5600XT makes zero sense—you’d be sitting on a monster CPU with a mid-tier GPU holding it back.  •   The 9600X + 9060XT option gives you a balanced system right now, both CPU and GPU in the same tier.  Plus, that bundle moves you to AM5 + DDR5, so you’re future-proof. Down the road, you can just drop in a stronger CPU or GPU in 5–10 years when you feel like upgrading again, instead of rebuilding from scratch.  Basically: balanced now, easy upgrade path later. Way better value than overspending on just CPU or just GPU.",Neutral
AMD,"Keep in mind you can always sell your current rig for some extra $, do that, take option 3 and upgrade to a 9070XT (or 5070 Ti depend on pricing)   I'd say you can get like 250-300$ for the old rig.",Neutral
AMD,always gpu first ..,Neutral
AMD,100% option 3. Get the most bang for your buck. Problem is you'll have to upgrade your mobo and most likely ram for the 9600xt.,Neutral
AMD,"Looks good overall -- hard to say though if you're getting a good value without prices.  \* I would consider swapping the 7700X for a 9700X as the prices are pretty comparable (unless maybe this is part of a kit / bundle). Possibly you could also ""downgrade"" to a 9600X which is an excellent value for money (and AMD allows to run at 105W now) and use the money on GPU or elsewhere.  \* I see the RX 7800 XT you noted is listed on PC Part Picker at $839.58. For that price you should just get basically any 9070 XT instead as it will perform better and has RDA4 / FSR4. Also possibly consider a 9070 or even 9060 XT 16 GB (9060 XT 16GB will be somewhat slower but you would also save \~ $400 from the PC Part Picker price on the 7800 XT).  \* Possibly downgrade the T500 unless you're doing video editing and save a little money there; most modern M.2 SSDs are so fast you won't notice the difference except in specific IO-intensive workloads.  \* PSU is solid B tier, should be great for this setup. ([SPL's PSU Tier List - Google Drive](https://docs.google.com/spreadsheets/d/1akCHL7Vhzk_EhrpIGkz8zTEvYfLDcaSpZRB6Xt6JWkc/htmlview?usp=sharing#gid=1973454078)) PSU Tier List doesn't like the fan for some reason but it's probably fine.  \* Not sure of the exact SKU of the RAM as there are many listings under that name; consider at least CL30 6000 which is basically the sweet spot of price/performance these days.  \* ""Pre-build"" your PC on [Pick parts. Build your PC. Compare and share. - PCPartPicker](https://pcpartpicker.com/) to make sure there are no issues with size or PSU connections, although you're going a full E-ATX case and ATX PSU so it's doubtful you will have issues.  \* Peripherals you could min/max all day long... find what you like and check reviews on-line for good value. I have had good luck with Redragon and like their products tbh as they're generally a good value for money.",Positive
AMD,"music production isnt especially thread heavy so this will do plenty, and the difference of the v cache will be good for gaming. This looks like a good build to me EXCEPT THE GPU. if it was 600$ it would be great but at this price point you can get a 5070 ti, which is pretty much better, and better for productivity and any AI audio work you wanna do.",Positive
AMD,"Looks good, just one thing. For $750 get a 5070 ti over the 9070xt. Asus prime is a good MSRP card.",Positive
AMD,"Good build, but I'd make a couple changes. I upgraded the GPU for the same price. You can save some money on the motherboard and I added a case. Obviously go with whatever you like on that front but the one I added is pretty well regarded.  [PCPartPicker Part List](https://pcpartpicker.com/list/rmQpcx)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 7800X3D 4.2 GHz 8-Core Processor](https://pcpartpicker.com/product/3hyH99/amd-ryzen-7-7800x3d-42-ghz-8-core-processor-100-100000910wof) | $359.00 @ Amazon  **CPU Cooler** | [ID-COOLING FROZN A620 PRO SE 58 CFM CPU Cooler](https://pcpartpicker.com/product/MFvD4D/id-cooling-frozn-a620-pro-se-58-cfm-cpu-cooler-frozn-a620-pro-se) | $29.99 @ Amazon  **Motherboard** | [Gigabyte B650 EAGLE AX ATX AM5 Motherboard](https://pcpartpicker.com/product/CvcgXL/gigabyte-b650-eagle-ax-atx-am5-motherboard-b650-eagle-ax) | $159.99 @ Amazon  **Memory** | [Patriot Viper Venom 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/4cCCmG/patriot-viper-venom-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-pvv532g600c30k) | $86.99 @ Amazon  **Storage** | [Samsung 990 EVO Plus 2 TB M.2-2280 PCIe 5.0 X2 NVME Solid State Drive](https://pcpartpicker.com/product/hpqrxr/samsung-990-evo-plus-2-tb-m2-2280-pcie-50-x2-nvme-solid-state-drive-mz-v9s2t0bw) | $119.99 @ Amazon  **Video Card** | [Asus PRIME GeForce RTX 5070 Ti 16 GB Video Card](https://pcpartpicker.com/product/Cpbypg/asus-prime-geforce-rtx-5070-ti-16-gb-video-card-prime-rtx5070ti-16g) | $749.99 @ Amazon  **Case** | [Montech XR ATX Mid Tower Case](https://pcpartpicker.com/product/fc88TW/montech-xr-atx-mid-tower-case-xr-b) | $73.93 @ Amazon  **Power Supply** | [Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $89.90 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$1669.78**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-09-07 22:53 EDT-0400 |",Positive
AMD,"That CPU cooler seems pretty cheap, make sure it has the specs to sufficiently cool.  Also, it's advised to have have a PSU that can supply 200W above your estimate maximum draw. You might want a 800 or 850W for that rig.",Positive
AMD,"Yep, good catch, thank you",Positive
AMD,"Ok, I like what you did there",Positive
AMD,Warranty tbh.   Sure 9070XT is the high end option. But get yours fixed.,Neutral
AMD,Assuming you're bought it new just use warranty,Neutral
AMD,Use. Your. Warranty.,Neutral
AMD,I'd personally suggest the 9060xt.  The 9000 series are spectacular tbh...,Positive
AMD,If it's the 16gb 9060 xt I'd say the upgrade is worth it,Positive
AMD,If its the 16gb 9060xt then absolutely that. Thats the one i just got for my first pc,Neutral
AMD,"It honestly depends on what your needs are... Fps, solo campaign, rpg, streaming, vid rec., etc ... Do u have an interest in A.I. or M.L.? RX is amazing for raw frames n streaming. ARC is amazing for AI n ML so it depends on what your use cases are",Positive
AMD,9060 XT and get the 16GB version.,Neutral
AMD,is the current price still good with it?,Neutral
AMD,the pricing would be my real question as it seems to be too much for both,Neutral
AMD,"they're both overpriced, is this USD?",Negative
AMD,Nah. MSRP is 350. +/-50 is what I'd accept. 100+ more is crazy. That's 30% upcharge.,Neutral
AMD,What country are you in? It would depend on local pricing and what alternatives are priced at. Sometimes countries are just expensive unfortunately,Negative
AMD,usd equivalent of php,Neutral
AMD,where can we get msrp ones im from the philippines.,Neutral
AMD,"if I had to overpay for something, it'd be amd over Intel.  Especially because their new CEO has been threatening to discontinue their GPUs",Negative
AMD,I see. Are the 9060 xt around those price?,Neutral
AMD,thanks for that info ill keep that one in mind,Positive
AMD,on stores here yeah around those price,Neutral
AMD,I see. Then I guess it's fine to buy it. It's a much better card that the b580 by 25%.,Positive
AMD,"5700x3d if you can find one at a reasonable price. 5800x/5700x are both decent upgrades.  id recommend at least a 9060xt 16 gb or b580, those are good value gpu's atm with good raster and vram",Positive
AMD,"B580 you want at least a zen 4 or x3d CPU, the GPU overhead is really bad",Negative
AMD,"is the 9060xt much better than the 7600xt paired to a r7 5800x?   will there be any bottlenecking?   also, is the 5800x significantly better than the 5700x?",Neutral
AMD,"[https://www.techpowerup.com/gpu-specs/radeon-rx-7600-xt.c4190](https://www.techpowerup.com/gpu-specs/radeon-rx-7600-xt.c4190)  35% faster with fsr 4 is a big jump. 9060xt 16gb is a 1440p capable card whereas 7600xt is just a 1080p card.  Bottlenecking is a misused term and smth most dont need to worry about. This is a fine pairing.  5700x is probably good enough for your needs, if i recall, the 5000 series is roughly the same in terms of gaming performance. You just go higher if you need more cores. [https://www.tomshardware.com/reviews/cpu-hierarchy,4312-2.html](https://www.tomshardware.com/reviews/cpu-hierarchy,4312-2.html)",Positive
AMD,"I think I'll go with the 9060xt and the 5800x combo, I've never considered the 9060xt since I thought it's a lot more expensive than the 7600xt, they are almost the same price where I live and also the 5800x is just like 20 USD more than the 5700x, so it's a better pick I guess.",Positive
AMD,bro cable management pls,Negative
AMD,"thats a AM4 rig, great job nonetheless.",Positive
AMD,Thanks!,Positive
AMD,"By supported they mean using the manual override in the driver. So, not a lot changed they just allow the driver override for FSR 3.1 for more titles now.",Neutral
AMD,"Ever since I bought my 9070XT, the card is getting better and better, new drivers improving performance, tons of new Adrenalin features, and now this, you love to see it.",Positive
AMD,I bought a 7900xtx late last year and feel completely left out but somehow have a premium card. I don't get it.,Negative
AMD,Doesn’t work in new world,Neutral
AMD,No list of the games added. And it says it now includes 85 games total. It did not add 85 additional games.,Neutral
AMD,I just upgraded from a 3070 to a 9070xt last week! This is wonderful timing!,Positive
AMD,rx 570 supports fsr 4?,Neutral
AMD,"You love to see it, this is a huge patch for RDNA 4 owners. They seem to be getting fsr4 into more titles at release as well.",Positive
AMD,"Its more than 85, every game with fsr 3.1 will support fsr 4",Neutral
AMD,The haters will have to eat some humble pie. 😂,Negative
AMD,"For all titles with 3.1.  IMHO it's a big step forward, getting away from whitelisting.",Positive
AMD,Its still good to see them improving the list of fsr4 compatible games,Positive
AMD,Its not manually done though unlike nvidia.,Neutral
AMD,"No need for driver overrides anymore, any fsr 3.1 game will use fsr 4 as the driver automatically overrides it and for any older far game or any DLSS game we can use optiscaler mod.",Neutral
AMD,I just wish it didn't take them 6 months to get to this level,Negative
AMD,Upgraded my PC with a 9070xt and now all I get is unity crashes in tarkov.,Negative
AMD,"Serious question, what Adrenaline features do you use? I have nothing enabled and always have great performance so don't even look at them.",Neutral
AMD,i'm glad i spent my money on a 9070 and not a 5070 with less memory,Positive
AMD,"Adrenaline has always been great, much better than GFE/NVapp (*imo of course) and its awesome that hasnt made them complacent, they keep improving it. The only remaining pro for nvidia is DLSS adoption, so I love to see AMD putting some focus on that cuz fsr4 is at a point where its good enough to not be considered a con for AMD gpus anymore",Positive
AMD,"it's always like that, GTX buyers were left out with raytracing, 20/30 series were left out of framegen, RX/Vega users were left out of HIP...",Negative
AMD,"Keep an eye out down the road! AMD is apparently working on an FSR4-lite for RDNA3, if the source code leaks are to be believed",Neutral
AMD,"FSR 4 uses AI to upscale, which is new for AMD GPUs, RDNA3, and below cards can't use FSR 4 for the ""same"" reason GTX cards can't use DLSS, it's a hardware limitation.",Neutral
AMD,"FSR 4 is being worked on (or at least was) for RDNA 3 cards.  FSRs source code leaked recently, and it showed that they were working on an int8 version of FSR 4.  If you're on Linux, there are ways to pretty easily enable FSR 4 via the Mesa driver on RDNA 3.  Though it's clear that int 8 optimizations aren't enough just yet to have FSR 4 be performant on RDNA 3.  If AMD released an int8 version of FSR 4, even RDNA 2 and older cards could run it, but it's unlikely AMD would whitelist anything before RDNA 3, given that performance could degrade, even on balanced/performance presets.",Neutral
AMD,"We knew from public information at the RDNA3 announcement (November 2022) that:  * It wouldn't support this tech for upscaling  * It most likely couldn't achieve good results without it  * Future gens (RDNA4+, UDNA) were almost certain to take it up and leave this harsh divide where future cards were close to parity with Nvidia but RDNA3 and older were left out in the rain.  Unfortunately it got sold to loads of people who weren't tech enthusiasts when they didn't know or understand this at the time.",Negative
AMD,"That sucks, sry to hear but I honestly havent heard the name ""new world"" in quite a while",Negative
AMD,"Nice, thats an awesome card you have there and a solid upgrade from a 3070 (a good 60% increase in raster iirc)",Positive
AMD,"Im afraid not, it relies on hardware acceleration with FP8 support which is exclusive to RDNA 4 (9000 series). Imo, this was a necessary evil for AMD to actually compete with DLSS",Negative
AMD,"No, not all.  >The company has also made it clear that this works only with DirectX 12 titles that have integrated a signed FSR 3.1 DLL as per AMD’s developer guidelines. Any games running on Vulkan, or those that use non-standard methods such as third-party plug-ins, are not compatible with the FSR 4 driver upgrade.",Neutral
AMD,"Their actual wording was ""most"" games with fsr 3.1 will support fsr 4",Neutral
AMD,It’s 85 games if you include the FSR 3.1 games that can use the driver override.  That’s why it’s an AMD announcement because it’s a driver update. All they did is whitelist more games for the override function.,Neutral
AMD,"To be fair they have a long ways to go to be on par with DLSS adoption, but its good to see them going in the right direction and giving it some level of priority. They also need to get new titles implementing native fsr4 so customers dont need to rely on third party programs to use the features of their card imo, which they do seem to be doing a better job of than in the past.",Positive
AMD,"I know, it’s a good thing for everyone who bought a RDNA 4 card.  I just mentioned that because there are many people who only read the headline and assume that it means that FSR 4 adoption is improving which doesn’t really. Not really any new games that didn’t have FSR before and not really any more support for FSR by developers.",Neutral
AMD,Don't even need to toggle anything?,Neutral
AMD,"No, it literally says that in the article and the AMD announcement.",Neutral
AMD,Well usually there would have been an x900 card for at least 3 of those months that most people wouldn't have bought. And by the time the 800 and 700s were around it'd be in good shape,Neutral
AMD,Were you using nvidia before and have you run ddu?,Neutral
AMD,"I use it to apply an undervolt and overclock to core clock speeds + a VRAM overclock. On top of that, I use it for the FSR 4 override in supported games (primarily in AC Shadows so far). Sometimes I use frame gen (Fluid Motion Frames 2.1) to get 100+fps if I have a base of at least 50-60fps.   For things that you can also do in GFE (I think), I also use sharpening in some cases when anti aliasing/TAA/upscaling results in an image that's a bit too blurry, Radeon Chill to limit FPS if there's no option in game or the in game option is buggy, and in some games I'll turn on Radeon Anti-Lag, which is similar to reflex.  To get the best use out of Radeon Enhanced Sync (with Free Sync), you should also be using the driver level frame limiter (assuming the in-game frame limiter doesn't have a slider/custom values) to set your max frame limit to a number under your monitor's refresh rate. A good formula is refresh (refresh*refresh/3600), so for 120hz, you would set it it 4 less than 120, for 240hz, you would set it to 16 less than 224. Per digital foundry though, the Special K frame limiter is apparently the best frame limiter tool in terms of stability, input latency, and frame timing. However, I have not done this and It also isn't a Radeon Adrenalin thing.",Neutral
AMD,I use both and feel like the NV app blows Adrenaline out of the water. RTX HDR and super resolution video are also huge pros of nvidia I use regularly. And I always miss RTX HDR when I'm not on my desktop,Positive
AMD,"The difference is that RT and tensor cores shipped 2 years after the GTX cards, not 4 years before.  RDNA3 literally released over 4 years after the RTX 2000 series.",Neutral
AMD,They have a $1000 GPU you don't think they know that?,Neutral
AMD,AI was new for AMD since RDNA3.,Neutral
AMD,"honestly AMD should've done that long ago, sure have a software upscaling solution for everyone, but also do hardware AI upscale like DLSS, Nvidia created an environment where even if you own a RTX 2000, you can still use DLSS 4 upscaling, because the necessary tech is IN THE CARD, they can just keep updating it. Of course, in future AMD will now do the same, but they pretty much left all previous RDNA cards in the dirt by doing that.",Neutral
AMD,even if games don't use signed fsr3.1 you can replace the DLL files with fsr4 and it will work,Neutral
AMD,That's unnecessarily pedantic. That's by far most such games.,Negative
AMD,"Yeah, the number 85 is used just for the ""officially supported"" games, theres definitely a lot more dx12 games that'll work fine with fsr4 override",Positive
AMD,"Fair point, they have a lot more work to do in terms of adoption specifically. Here's hoping they continue focusing on getting fsr4 on more and more games, especially new releases.",Positive
AMD,">FSR 4 can now be enabled in most DirectX 12 titles that already support FSR 3.1.   And   >Additionally, users need to switch to FSR 3.1 in their supported game settings and then toggle FSR 4 within AMD”s Adrenalin Edition software  I'd assume yes",Neutral
AMD,The article is incorrect then bec the driver changelog says that any fsr 3.1 direct x 12 game will automatically will use FSR 4,Negative
AMD,All new build. Didn’t have to worry about nvidia drivers. Also reinstalled the drivers. Checked pretty much everything. Unity crashes and pc shuts down only on tarkov. Can run cyberpunk on ultra and have no issues.,Positive
AMD,"This guy Adrenaline's 💪  For my pc (4080S/7600x) I need 3 programs to match adrenalines level of customization (MSIAB, RTSS and NV control panel). Not a massive deal i guess but its still nice to have it all in one nice package. On my partners rig (6950xt/7500f) I use adrenaline and its just a great tool, always miss it when back on mine.",Positive
AMD,"Beyond that I find that visually Adrenaline is a bit of a headache, the colour scheme for one but it feels so damn cluttered and visually messy. Plenty of features sure but I find it to be such a pain to use.",Negative
AMD,"Fair enough with HDR, but super resolution? The amount of power it takes to give such minimal visual improvement never felt worth it to me. Even at level 1 quality my 4080S was still chugging power (dont remember specific numbers but it just didn't feel worth it at all)  I myself have used both (partner has a 6950xt) and where I find myself using adrenaline on hers fairly often, Nvidia app has long been deleted on mine as RTSS/MSIAB work far better for undervolting/monitoring and I just download drivers direct from their site.",Negative
AMD,"NV app is so good that I had to remove it to make the drivers not reset my settings. And RTX HDR is overrated, autohdr with gamma fix is better with less bugs",Negative
AMD,"i don't know what you're onto here, it doesn't make sense, why compare release dates to RTX series? AMD never promised AI upscaling before rdna4, sure rdna 3 has ai accelerators, but RTX20/30 also have them yet they can't run all the nvidia ai features (framegen), my point is that you shouldn't buy hardware thinking it will get better features in the future, because it most likely won't, and if it does - well that's a nice bonus      also, there's still a possibility rdna 3 will get a weaker version of fsr4 in the future, there are traces of it in the accidentally leaked repository, you can even enable regular fsr4 on linux, it's just awfully slow",Negative
AMD,Plenty of people on reddit seem to be struggling with this concept,Negative
AMD,"I completely agree, and yeah it sucks for rdna 3 (and older) owners but it will pay off in the long run. As the owner of a 4080S its amazing getting free performance in the form of these DLSS upgrades  Also just to point out, in my experience the people who went with those older AMD cards went in knowing the deal with FSR, and most seem content with that fact.",Positive
AMD,"Did you try running other Unity games? If it's just Tarkov, you should report this to their dev team.",Negative
AMD,"On top of what the other guy said,  I also use Adrenalin to record clips, Nvidia also has that feature, but I found AMD's solution less buggy and more robust, also their metrics overlay is really nice, and prevents me from needing riva tuner.",Positive
AMD,"I love the new Nvidia App but man, I do wish they'd copy some features from adrenalin. I wish they'd move away from the automatic tuning and just allow manual OCs thru it.  Would like to see the ability to record gifs as well.",Positive
AMD,">  And RTX HDR is overrated, autohdr with gamma fix is better with less bugs  Delusional",Negative
AMD,"Im not a fan of NV app at all, evidenced by my replies in this thread but we gotta be honest, RTX hdr is great",Negative
AMD,"For sure. Recently ive actually switched to steams built-in recording feature, its been very reliable for me but adrenaline is great for things like emulators or epic exclusives. I just use OBS for those cuz I hate gamebar and have gutted it from windows completely through the registry and powershell 🤣",Positive
AMD,">I wish they'd move away from the automatic tuning and just allow manual OCs thru it.  Im right there with ya, and i also think I come up with some great gifs/memes in my head and am just too lazy to create them 🤣",Negative
AMD,"I tested Steam's version on Beta and was really bad, how is it nowadays? I rarely play games outside of steam so I might change it.",Negative
AMD,I heard the steam recording tool takes more resources than shadow play and relive. Is that true?,Neutral
AMD,"Yeah thats true, for me there were issues with the audio especially which is why I didn't completely switch over at first. I kept testing to see if it'd improve and in the last ≈2 months I've not had a single problem. I have heard that some ppl still have issues when ""only game audio"" is selected, so use ""all system audio"" instead, which isnt ideal if you're in discord vc or listening to music, but i haven't experienced that issue for months.  I'd say maybe try it out but still have adrenaline running at first just to make sure you wont lose any recordings.",Negative
AMD,"I've never run benchmarks to check for any performance loss, I'd just assumed that a built in tool would be pretty efficient. I've never heard that before so maybe I should look into it, but I will say I haven't noticed any like stuttering or noticeable changes in framerate since using it",Neutral
AMD,"The 7600X could be cooled with some folded up tinfoil, harsh words, and a stiff breeze.  A 125 watt AMD Wraith Prism is much more than that.",Negative
AMD,"It will cool more then enough, but it's pretty loud.",Neutral
AMD,"i have one on mine R5 5600, and another on mine r5 5600X, never past 55/60°C while gaming, full load, i have a good airflow case too, with good fans.  recently i built a R7 5700X and the owner asked for a prism on it too, still testing, but first results, at full load, in a  few games too, was near 70°C sometimes, the prism selector is still on the ""low"" position, for mine two r5 setups too.  i think it´ll cool enough the cpu, but if you see that it is overheating at 80/90°C frequently, than a powerfull cooler would be advised",Positive
AMD,"Honestly, the wraith prism is probably one of the great OEM coolers that COU manufacturers have come out with  A handsome and very competent cooler on its own with some tasteful RGB   My first Ryzen (and regular 5600) had this puppy and I never once had an issue (my 9900x now has a peerless assassin but I figured it's worth the upgrade)",Positive
AMD,Without OC? Yes.,Neutral
AMD,I have one with a 3700X that seems to do fine. I mostly liked the look of it being AMD branded. It should do fine on a 65 watt CPU such as the 7600X imo considering the fan is rated for up to 140 watt iirc.,Positive
AMD,"It will cool it enough but all i can say: the cooler, the better",Positive
AMD,"Came with my 7700, did just fine but swapped to a 240 AIO for more cooling and less noise. Can run fans at quieter rpm now",Positive
AMD,I had one with my 3700x. It served me good for a stock cooler.,Positive
AMD,I have this cooler on 5700x3D it’s fine and works well. Only that it’s a bit loud if I want the CPU to stay under 70C. Other than that no complaints. Best 10€ spent. But with custom fan curve you can make it great and not too loud,Positive
AMD,"I buy extra of these coolers. They're good enough to cool a 5950X under normal circumstances.   I use fanspeed to alter their hysteresis curve though as they get rampy with the load.  Plus I love that fan, it's mesmerizing. Still haven't found case fans that has that effect.",Positive
AMD,Its the stock cooler that came with an older AMD FX CPU I bought a good few years ago.  I'm still using it now with a Ryzen 7 5800X. Only given it a good dusting once and its working fine so far.,Positive
AMD,Get a peerless assassin and be done with it.,Negative
AMD,I couldn't give a toss about the RGB but I got one as it was the best cooler that would fit the space I had for it. It does a good enough job and I'm running UE5 games and Cyberpunk among others.,Positive
AMD,"It's good, as long as you're not overclocking of course. To give you the rough idea of its performance, it's supplied with R7 7700 and R9 7900.",Positive
AMD,"Depends on the CPU, I used one to cool a 3300X for years as I put an AIO on my old 3800X it came with. Temps were nice and cool on such a low power CPU... But I wouldn't use it on an 8 core if you want best performance even if it's rated for one... I wouldn't even use it on a 6 core... quiet and cool are more important for me.",Neutral
AMD,"Got one on a 3900X. Not rendering or anything on that machine, but I do have a few containers running and occasionally game in a windows VM on that machine and the 2080’s fans kick up more than the wraith’s.   So the answer to the question you wrote is yes. The wraith can cool. The answer to the question I think you meant to ask depends on what CPU in what configuration in what conditions.",Neutral
AMD,"Good enough, yes. But not great.   Better tower air coolers don't cost that much, btw.",Negative
AMD,I bought one used for 10-12 euros. It works great for my ryzen 5 3600.,Positive
AMD,Works well depending on the case. worked supremely well with my old case that had a mesh side panel and even better with a fanduct. but in normal systems with relatively good airflow and a side panel that leaves room to breathe.,Positive
AMD,"Should be alright, they’re still decent coolers, just not the quietest.",Positive
AMD,"""Good enough""",Neutral
AMD,"It's decent depending on the CPU but whatever you do, do not use pre-applied thermal paste. Overtime It will become glue and you will rip out the CPU if you try and take the cooler off.",Negative
AMD,More than good enough for a 7600x. If you were to go 7700x or above in the future then I’d probably recommend changing it,Positive
AMD,Yes,Positive
AMD,"The 65/45w chips yes, if you didnt fuck up your airflow completely, eg one case fan and not putting your PC into a closed box. the 100w tdp chips probably if you have four fans. going to the heavier  chips I wouldnt and as soon as you go above slight Clock/volt tuning",Neutral
AMD,Temps should be fine. But that noise 🫩  Had one on a 3700x for three years. Upgrading was the best decision ever  Even silent mode is very noticeable,Positive
AMD,"If cooling is all that matters for you, then yes, it will do its job more than fine. It is a very loud cooler, however, so if that would bother you, I suggest looking elsewhere.",Neutral
AMD,"Of all the box coolers ever made, it is the best. It can easily handle any CPU under 120W at stock settings.  If you intend to overclock or use very intensive cpu loads then I'd still suggest an upgrade to a decent tower cooler but for most users it's more than sufficient.",Positive
AMD,It's rated at a TDP of 105W so it can definitely cool down low power Ryzens.,Positive
AMD,"Yeah, it's pretty good. It's not going to be ice cold, but will keep you cool enough for most applications.",Positive
AMD,"I used to cool a 5800x with the Wraith Prism.  Perfectly adequate, even if the temps are a bit on the hotter side.  If you want a few degrees cooler and a bit more performance, you can give the 7600x a bit of an undervolt with PBO.",Positive
AMD,It’s loud but one of the best included stock coolers,Positive
AMD,"I think that the thing with air coolers is that it is very dependent on the case and GPU (e.g., are you using a FE that will push hot air directly onto the CPU heat sink?). When you get your prebuilt, game on it and check the CPU temps. If it never throttles then you are good.",Neutral
AMD,"Performance are good, but it can get noisy even at stock settings, if you enable the turbo mode it goes flying.",Neutral
AMD,Its good for the AMD 65 watt CPUs. its not so good for their 105 watt CPUs,Neutral
AMD,"Sure, if the noise doesn't bother you.",Neutral
AMD,I don't have one but the reviews I've seen have all talked about how its a decent cooler for the processors it comes with. Its not going to handle a higher powered or overclocked processor.,Neutral
AMD,I have it and is very good  Although it you are aiming for a Ryzen 9.... Maybe not,Positive
AMD,Wraith Prism is actually one of best stock coolers and it easily cools 7600x,Positive
AMD,"It cooled my Ryzen 7 7700 well enough, but it was loud as hell. So I bought a Be Quiet! cooler, same (slightly better) temps but WAY less noise  Keep in mind that the Wraith Prism was the stock cooler I got with my 7700, and that it's only a 65W CPU, so it probably didn't need any more than the Wraith Prism for cooling. I just didn't like the noise it made.",Negative
AMD,"Perfectly adequate. Is it the world’s best cooler? No, but it’s perfectly adequate for that chip.",Positive
AMD,Yeah it's fine,Positive
AMD,"When I used it on my 2700X it apparently wasn't enough, CPU was at 80°C but never boosting past 3.8GHz (over a 3.7GHz base) despite advertising 4.3GHz on the box.  I then went for a noctua NH12 for my 5800X and it is boosting at 4.9 GHz while keeping it at 75°C.  I'm not drawing conclusions too fast though because it seems modern ryzens became even more power efficient through the generations. Maybe the prism is enough for a 7600X today.",Neutral
AMD,I use the cooler that came with my Ryzen 5 5600 and never had an issue with temps.,Positive
AMD,"It's pretty good , nice small formfactor, but loud.  Good RGB functionality (can actually control colors according to fan speed / cpu temp) .  A Thermaright Assassin X 120 SE V2 for example should be available <$20 and will do better with a lot less noise.",Positive
AMD,"for my sisters system it was good enough, for a small case and a 5600",Positive
AMD,"It was just barely NOT enough for a r7 5800xt, but that chips is a spicy 140w. I was hitting 89 and just a touch of thermal throttling at heavy workload. Workloads greater than what most games will call for. The wraith prism is “supposedly” rated at 125w which is plenty more than the 7600x 104w TDP.  It has been fine on my brother’s r5 5500.",Negative
AMD,Yep,Neutral
AMD,Why not test by yourself? Just run prime95 or cinebench and check if you cpu reaches 80 or even 90c,Neutral
AMD,"When I was on a budget I ran mine with a wraith prism and it was totally fine, sure its not as good as a good aftermarket cooler but if an additional 40 bucks hurt you, stick with the stock cooler",Neutral
AMD,"I don't know if it still applies, but be careful if you ever take that thing off - the stock thermal compound is like glue and in the past has caused cpu's to get ripped out of sockets (this was older sockets though, I don't know how much it matters now.)",Negative
AMD,I used a wraith prism with a phase change pad (indium) and a 5600x for 4 years.   It was perfectly fine and hit all performance requirements,Positive
AMD,Most likely. Just apply your own thermal paste as the one provided has tendency to FUCKING STICK TO THE PROCESSOR LIKE A FUCKING GLUE.,Negative
AMD,Yeah its good enough. Tweak the loudness via fan curve software,Neutral
AMD,If you pay $20 (around the going price in my region) for one? Go with peerless assasin.,Negative
AMD,I'm still using the wraith prism that came with my 3700x on my 5900x. Running it overclocked and never had a problem with temperatures. (Not above 80°C ever),Positive
AMD,Question is not whether it'll be cool but wether it'll be quiet...,Neutral
AMD,It was enough for my 3900x which has the same TDP before I upgraded. Kinda noisy when it ramps up but it kept thermals in check.,Neutral
AMD,not really .. basic gaming but do not rely on heavy duty coiling,Neutral
AMD,Yes,Positive
AMD,Just get a thermalright assassin 120 man just $20 and so good these stock coolers are shit and the orientation is bad,Negative
AMD,"I got one with the stock thermal paste still intact purely because of the looks and it went around 73C on a regular 5600 while in low mode and being audibly loud during gaming. I wouldn't buy one if you don't already have it, a cheap tower cooler will do the same and with less sound.",Neutral
AMD,It is truly “good enough”.  It’s not great… but it is “good enough.”,Positive
AMD,It should be ok but I have to say they are pretty loud when running something demanding,Negative
AMD,I used this cooler a few years ago as it came with my old 3700X. At the time I played on 1080p and my cpu would average in the mid 70’s. Take that as you will. I’m not sure how hot the 7600X runs but judging by other comments here it seems like you should be fine?,Neutral
AMD,"this is a legendary stock cooler, its suitable for most ryzen cpus",Positive
AMD,"Should be good enough. Currently using it on mine. 3700x. No reason to invest on a bigger cooler. I had space issues with the Deepcool Gamaxx 400 where the cooler was already pressing against the glass of my O11D, hence, decided to return to the Wraith Prism.  I did unlock the ARGB by plugging in the USB header.",Neutral
AMD,"I recently built a system for an acquaintance with a 7500F. I managed to set PBO to 200 MHz, and a negative curve of -40 all core. With only an ID-Cooling SE-214-XT BLACK CPU cooler, the temps didn't even reach 80C in Cinebench or OCCT.  So if the Wraith Prism isn't something you have lying around, a cheap mono tower air cooler, like the one I used, will do the job just fine.",Neutral
AMD,It should be enough with a good TIM until you can get a better cooler installed but make sure to toggle the RPM switch on it from its default of Low to High.,Neutral
AMD,From my experience it was ok but I personally like my so better more noticeable difference in cooling and the fan you could feel the hot air,Positive
AMD,"I had it cooling my 3900x, it was pretty well regarded from what I recall. I think coolermaster made it?   I know some folks slagged it off for it being a stock cooler but it was legit for my CPU then and it kept it plenty cool",Positive
AMD,"Was the wraith prism supposed to just always run at max speed? I had to take mine out because it just ran full speed all the time, little thing gets loud after a few minutes.",Negative
AMD,"Yes, the stock cooler is designed to be suitable for the processor it is paired with.   You only need an aftermarket cooler if you run into trouble, which is usually when people run their machines hot (with a massive GPU or insufficient airflow) or in a warmer than normal environment.",Neutral
AMD,maybe test it yourself ?,Neutral
AMD,My go to saying is it can be cooled with an icy glare lol,Neutral
AMD,and the 7600 non x can be cooled with my nutsack,Neutral
AMD,Tdp isn't reliable but I agree its more than enough. Coolers have become really efficient compared to those spiky baskets with flimsy fans of the past.,Positive
AMD,"Yep - this was my experience. Temps were fine, but it was a bit loud. As with any (reasonable) fan noise, you'll probably be able to tune it out pretty quickly, but there are quieter options if you have a choice in the matter.",Neutral
AMD,Mine is super quiet. I don't hear it at all.,Neutral
AMD,The CPU isn't overheating at 80/90° that's their comfort zone they will boost until they hit that or higher particularly on modern Ryzen chips,Neutral
AMD,My two years old R5 5600x was around 89°C when gaming. Decided to upgrade the cooler from wraith stealth to a better one.,Neutral
AMD,What about AMD precision boost overdrive? You think I can turn it on?,Neutral
AMD,7600x is actually 105w tdp and 142w ppt limit so it will push this cooler close to the limit,Neutral
AMD,I too have it with 5700X3D mine is silent after setting -20 offset on CPU. Rarely see above 70C,Neutral
AMD,I still use mine for 7800x3d and no issues at all,Positive
AMD,This CPU cooler was never bundled with FX cpus. First time it came out was 2018 with 2700X,Neutral
AMD,"The pre-built its not arrived yet, so if I manage to know if I need a new cooler before the pc comes I dont have to wait to use it and mount it right away",Neutral
AMD,And my axe!,Positive
AMD,"yes, i know; in a technical/specifcations context, yes, to me tho, 80/90 is too much already on my builds, i aim for way lower temps and /stability/longevity.",Neutral
AMD,How is the temps after upgrading to a better cooler? my 5600 with stock fan goes up to 90C in cpu heavy games.   I had to undervolt to keep the temps between 70-80..,Neutral
AMD,Wraith prism with good thermal paste sits at 60c,Neutral
AMD,"i never saw mine reach such temps, my r5 5600X being on duty for around 13/15 months now, and the 5600, 8 months, both always with prism cooler and good case airflow",Neutral
AMD,"these cpus have automatic throttling with temps, so it would just not run as fast as it might otherwise. But you're talking about extremely small differences unless the cooling is truly inadequate.  i don't know if all the wraith prisms are the same or not, but i tried one on my 5800x3d when i first got it and it wasn't enough. (but that cpu generates a lot of heat bc of the 3d cache). The 5800xt cpu comes with a wraith prism so presumably it will cool it enough for it to run at spec.",Negative
AMD,"Wait till you actually have your PC. Then play around with it.   Simple answer: try it, if it gets too hot, stop and turn it off. If you want it on, spend more money for a bigger cooler.",Neutral
AMD,"Using 7600 with stock cooler, with PBO temp reaches max 80-85C while gaming   When turned off it barely exceeds 75C max",Neutral
AMD,"I mean hard limiting to 105w could work within temp limits.   I used to use wraith on 2600x, it worked pretty well. It you get it with thr cpu, give it a shot, if you want to buy it, buy sth else.",Neutral
AMD,"When you cut the tdp to 65w, it loses very little performance",Negative
AMD,Love it,Positive
AMD,"That's not how it works, in a practical sense modern amd CPUs will boost until they hit that regardless of how good your cooler is. Look at any of the posts on this subreddit about this subject. Also lower temps do nothing for longevity",Negative
AMD,"After upgrading, the temps hasn't gone over 75 at all. I highly recommend upgrading from stock cooler if the cpu temps are above 85°C.  I upgraded to ""be quiet! Pure Rock pro 3""",Positive
AMD,"i have a zalman, that when i need to clean the wraith prisms i use it for a few days, it´s a CNPS9700A LED, and it make a few noises already, due to it´s age, but when i use it i have a little better cooling action, it takes my temps a few degrees lower then the prisms, i just don´t use it continuosly because of the noise.",Neutral
AMD,"Yeah, I guess your situation is different. I experienced my stock cooler had a delay. So, every time the cpu usage increased, the temps would spike high really fast, and the cooler was slow to catch up and spin faster. That was one of the reasons I upgraded the cooler, and I don't experience that anymore.",Negative
AMD,I know I have a 7500f it is literally the 7600x with a little less clock and 65w tdp and compared to my friends 7600x it is literally same performance I can overclock my CPU and get that 300mhz but very not worth it,Negative
AMD,"are you telling a ex; 4.6Ghz boost cpu will boost to 5.5Ghz for example only because it didn´t hit the temperature limits? rsrsrs  as soon the cpu tries to hit higher frequencies it will exponentially and innefectivelly consume a lot more energy, thus heating a lot more than necessary just for a few hundered Mhz, i don´t think it´s a clever way of building durable things.  i never saw my ryzen 5600/5600X above 4.8ghz nativelly, without overclocking it, i don´t do overclocks by the way, just don´t care about OC.  lower temps, less wear, less dilation and contraction on components and materials, stable lower voltages, less wear too, you are simply forcing less your hardware, how´s that not give you more life? rsrsrs",Negative
AMD,We can't put links in this subreddit but just go onto AMDhelp and look at any of the hundreds of posts on this subject,Neutral
AMD,"PBO, RAM OC (because normally expo has pretty bad timings), on the GPU, maybe OC or UV.",Negative
AMD,Yikes..,Negative
AMD,Maybe they should address the motherboard gap first? LGA1851 is literally rotting on shelves and taking up inventory for many retailers,Negative
AMD,I assume it will use the same socket at least.  Probably won’t be new core designs like RPL R was right? Just clockspeed bump and price cut. Hopefully it doesn’t self destruct either.,Neutral
AMD,It was reported before that the new cpus coming out are on a new socket  [https://www.xda-developers.com/intels-next-gen-nova-lake-s-cpus-lga-1954/](https://www.xda-developers.com/intels-next-gen-nova-lake-s-cpus-lga-1954/),Neutral
AMD,Arrow Lake Refresh is not Nova Lake,Neutral
AMD,Download GPUz and see for yourself.  https://www.techpowerup.com/gpuz/,Neutral
AMD,Go to the manufacturers website and download and install a proper driver. If you cannot determine which GPU you have let the software identify it itself.,Neutral
AMD,5070 Ti outperforms the 9070 XT by a 5% average. 9800X3D will outperform the 7800X3D by about 5% as well so it might just even it out. Honestly it comes down to DLSS (Nvidia) vs FSR (AMD). DLSS is the better upscaling tech and has wider game support but FSR 4 has improved a lot over the years. Id probably go with the 7800X3D + 5070Ti  Hardware unboxed did a very good video where they tested both these GPU’s in 55 games;  https://youtu.be/tHI2LyNX3ls?si=0ofDu294Dw3EpLkP,Positive
AMD,"Both GPUs perform basically the same (at some games 5070 Ti is better, at some it's 9070XT). RT performance is a bit better on the 5070 Ti but the 9070XT is often very close in some games. The 5070 Ti has one big advantage: DLSS. AMDs good upscaling (FSR 4) isn't supported on many games and you often have to use the older FSR 3 or 2 that look way worse. DLSS on the other hand has more games supporting it's newest version and older versions of it still look good.  The 9800X3D is noticeably better than the 7800X3D at 1080p (up to 30fps more) but the difference becomes less noticeable at higher resolutions (especially 4K). At 1440p the difference is gonna be like 20fps at max (probably less) and at 4K you won't notice it at all except for some games  So if upscaling matters to you get the 5070 Ti build. If you wanna play at 1080p (which you most likely don't with this system) the 9800X3D could be a big advantage. At 1440p it depends on if better upscaling is worth these ~15fps you're loosing",Positive
AMD,"2, the 9070XT and 5070TI perform about the same where as the 9800X3D does have some performance gain on the 7800X3D, but with how close these builds are it might come down to ram/psu/case/cooler etc etc or just save hundreds or get a better build by doing it yourself.",Neutral
AMD,After the driver update the 9070XT performs basically identical to the 5070 Ti actually,Neutral
AMD,Im just wondering if the difference between the RT from dlss and fsr is obvious to the average eye or is it just performance metrics. But i guess thet difference is still that dlss is more supported on more games compared to fsr,Neutral
AMD,FSR 4 and DLSS have basically no difference but DLSS is just much more supported. RT is sometimes the same on both GPUs but some games still perform better with Nvidea,Positive
AMD,"Yeah, would be difficult to find something better at $200",Negative
AMD,Yes,Positive
AMD,Good deal,Positive
AMD,For parts great deal      I say parts because cyberpower has crap AIO's and power supplies i can not see nor say those are in good quality here. This could also be air cooled so look into that.      But yes parts wise $500 is a good deal,Negative
AMD,"iGPU is ""okay"", however paring it with something like a used 5700XT would be a much better experience for you.",Positive
AMD,igpu is fine for your intended usage.,Positive
AMD,"The 7600X is the same or stronger than a GTX 660 in my testing, with better frame times and support  (Helped someone upgrade from a FX 8350 and GTX 660 system, but GPU hadnt come in yet)",Positive
AMD,what is a good used price for this gpu? I dont want to get scammed with a fake or overpriced,Negative
AMD,"will the 6500xt be better? If I dont get a dedicated gpu, should I get a better cpu with igpu?",Neutral
AMD,"130 - 170 buckaroos, lower than this price is risky since they are usually broken.",Negative
AMD,"Sure. 9600X is probably a good idea.   You may wish to also consider Intel core ultra or the on-sale 14600K as well. Better price/performance, includes an iGPU.",Positive
AMD,tysm,Positive
AMD,"I don’t know, but i think i would get some headache!",Negative
AMD,"That's a no to 5060ti and RAM. Move the budget towards the 5070 (Recommend the Asus Prime which is at or below MSRP now), which will also then qualify for free copy of Borderlands 4 to eat up some of that extra you pay for 5070. MUCH faster card, even with 12gb VRAM....that extra 4gb on the 5060ti isn't gonna do diddly squat for you on that tier of card. In the end it's gonna cost you $50 more since you won't be spending $70 on Borderlands 4.  I would not drop any more money into that system besides GPU. 16gb RAM is fine.  Then you can start saving for a MOBO/CPU/RAM upgrade...but what you have should hold over fine for now (ASSUMING your PSU is decent).",Neutral
AMD,"Hey so this game is gonna run like shit on every system, I hate to break it to you. I breaks my heart",Negative
AMD,"Already have my free copy of BL4 from the laptop promotion best buy was doing, but thanks for letting me know!  Alright, thanks! Do you have a link for the 5070 you're talking about?  My PSU is 400W.. :/",Positive
AMD,People keep saying this and then I play the game they are talking about and it plays perfectly .,Positive
AMD,"That changes things.  5070 will need PSU upgrade, which if you didn't already have BL4, is what I'd suggest and moving that RAM budget to PSU budget.  If still want to upgrade you can, but if budget is restricting and you don't want to spend that much now but still want to upgrade GPU then other option might be considering 9060XT 16gb at $350 (but only if you can get it at $350, any more and 5060ti is better buy). Otherwise I would move to 5070.  Not sure if I can post links on this sub (they always remove my post) but it's on BestBuy search SKU: 6614787  Keep in mind if you do get the 5070 and the BL4 code, it can only be redeemed by those running 5070 or above cards since that's what the promo applies to.",Neutral
AMD,I see the GeForce RTX 5070 12GB at $550. Is the 5060ti in my post the better buy? And do I actually need more ram or is that just what steam recommends for maxed out settings,Neutral
AMD,"What resolution are playing at?  As for RAM, it's recommended but isn't priority for you. You have to manage everything else first, especially GPU... if you have a weak graphics card, it doesn't matter if you have 32gb or 64gb of RAM... you'll be limited by the GPU before you run in RAM issues.  Depending on your monitor resolution, and resolution you play at.... 'maxed out settings' is going to be all relative. For example if you have 1440P monitor, if you get a 5060ti/9060XT tier card.... you're not going to be playing maxed out settings without help of upscaler (DLSS or  FSR4).. 16 or 32gb RAM becomes irrelevant. I wouldn't worry about that right now.  As it stands... your choices are 9060XT @ $350, 5060ti @$430 and 5070 @$550. The 5070 will requires PSU upgrade, but others may not. 5070 comes w/ game code but limited to who you can sell to if you don't need it. Might be able recoup some money though as plenty of people who bought before promotion. w/ that in mind.. if you can get say $40ish for the game code.. the 5070 is better buy over the 4060ti. If you don't want to spend that much then it's a tossup between 9060XT and 5060ti. The 5060ti is about 5% better raw performance wise but does have a more widely supported DLSS software stack which at that caliber of card you are more likely to use.....so to you it may or may not be worth the extra $70.... if not and you really don't want to spend as much then 9060XT @ $350 is better value card in that budget.  If you prefer Nvidia card, then 5070 is overall better value card as it is 30-40% faster than 5060ti and why I'd pick it over the 5060ti if one can budget for it. Difference in performance is too big.  I'm not even bringing up (since someone else will) the AMD 9070 and 9070XT because they're well over MSRP and out of your budget and not worth it at the prices they sell at.... and at those inflated prices, you might as well move to 5070 ti (which is well over your budget as well).",Neutral
AMD,"2560 x 1440. Doesn't the game code need to be activated with the graphics card through the nvidia app?  If not, then it looks like I'll be picking up the 5070!",Neutral
AMD,"at that resolution, you need minimum 5070 (and 550W PSU) if you want any enjoyable experience at the target 'maxed out settings' and even that is going to be a tough ask for this Unreal Engine 5 game without DLSS.  Yes, the game code does have to be activated via nvidia app...but doesn't have to be the 5070 you buy... it could be any 5070, 5070ti, 5080 etc. So if your buddy has a 5070 or above, they can redeem it etc.",Neutral
AMD,"Alright, lets say I don't max out my settings, maybe medium? High even? Would that be an enjoyable experience? (if i don't upgrade my psu) And that's interesting, I didn't know that. Thanks!",Positive
AMD,"Yes. Should still be able to do mix of Ultra/high/medium settings with 5070 and DLSS upscaling @ 1440p resolution. To me, enjoyable means it never dips below 60FPS... and target around 90FPS+.... to me that is enjoyable.  Unfortunately, People often have this misconception that because they spent X amount of money on a card, they can crank everything to ultra setting and the GPU should just be pushing out 120FPS constant. It's not how it works... you always have to tune your settings to what the card/hardware is capable of. I still have to tune settings for my 5090 in lot of AAA games. At least you are being reasonable about adjusting your settings lol.  Same thing applies to amount of VRAM a card has.... same misconception exists, ESPECIALLY on this sub, that you need LOTS of VRAM and anything below 16gb VRAM on GPU is useless... again, that's wrong. Just because 5060ti/9060XT have 16gb VRAM doesn't mean you can crank up settings and  it's gonna be faster.... it could have 96gb VRAM, it's still going to be 30-40% slower than a 5070 w/ 12GB VRAM at it's intended resolution (1080p/1440p).  Yah would have been nice if 5070 had more VRAM but in no world is a 16gb XX60 class GPU ever going to be a faster gaming card than xx70 class. Period. Putting 16GB on weaker entry level cards was an FU homage from Nvidia and AMD to everyone raging about VRAM and people ate it right up.",Positive
AMD,"Lol yeah I used to play maxed out, but then over time I had to swap to high and now I play on medium, so I've been through it 😂. Thank you for all the quick responses! I appreciate it",Positive
AMD,"Just checked on google and it says the recommended psu for the 5070 is 650w, would I not be able to use the 5070 until I upgrade my psu then?",Neutral
AMD,"The cooler is one of the least effective at its price point, but it isn't a problem.  Is there a problem?",Negative
AMD,"There is absolutely nothing wrong with this build. If anything, cooling is almost overkill. I'd be very happy with this build.  If you want me to nitpick something: The B650 board only has PCIe 4.0 - you can get a B650E or B850 with PCIe 5.0 for slightly more money. And a 4TB SSD is much better value than a 1TB+a2TB SSD. But practically, none of this will make a very relevant difference to your experience.",Positive
AMD,list seems fine,Positive
AMD,Why do you think there's something wrong with it?,Negative
AMD,I would maybe look into getting a 5700x3d if you can. I had a 2080 super and a 3600x and upgraded to a 4070ti super and 5700x3d and it was a massive upgrade.,Positive
AMD,That's a perfectly fine combo. Enjoy your new setup!,Positive
AMD,"Not the best, not the worst, you could sell the 3070 and fund a Ryzen 5xxx CPU, or if your really deal hunting maybe a am5 lga 1700 combo",Negative
AMD,"You'll probably be cpu bottlenecked for some games, but it shouldn't be horrible at 1440p.   I'd start saving up for a ryzen 5700x3d or 5800xt, that'd be a good match for the 5070.",Neutral
AMD,"It will be CPU bottlenecked, best to go for a 9070 instead, it's better anyway and AMD has MUCH lower CPU overhead so will do way better in CPU limited situations.",Positive
AMD,You need the a really good overclock+undervolt on the 3600x so that it doesnt struggle to keep up with the 5070,Neutral
AMD,"Definitely looking into the 3Ds, im liking what I see in terms of gaming performance",Positive
AMD,Oh man cant wait to finish downloading games and finally enjoy VR to the fullest!,Positive
AMD,"5600X seems decent, was looking into it",Positive
AMD,"yea if you can swing it, it's an awesome upgrade and you don't need a new motherboard or ram you can just drop that cpu in and you're good to go. I'm not sure that you can buy them new any more though so you would have to look on the used market.",Positive
AMD,"Hurry up if you are. They have stopped making them.   After the 5800X3D and 5700X3D, AM5 is probably the next best cost option  Don't forget to update your mobo bios in the right order if you do find one",Neutral
AMD,"Good bump from 3xxx, and if your just gaming a 5600(x) or 5700(x) will do fine and get you 90% of your frames",Positive
AMD,"Sudden increases like that could be poor mounting pressure on the CPU Cooler, bad thermal paste application, plastic still attached to the copper base, or the motherboard BIOS is shoving voltage like there is no tomorrow.  Is it sitting at95ºC all the time? or only for a few short periods of time?",Negative
AMD,You shouldnt be paying 1600 USD for an AM4 build. For only 70$ more you could jump onto AM5 avoiding CPU bottleneck and having a future upgrade path:   [https://www.newegg.com/skytech-gaming-desktop-pcs-amd-radeon-rx-9070-xt-amd-ryzen-7-9700x-32gb-ddr5-1tb-nvme-ssd-st-shadow4-2101-b-al-black/p/3D5-000Z-002X2](https://www.newegg.com/skytech-gaming-desktop-pcs-amd-radeon-rx-9070-xt-amd-ryzen-7-9700x-32gb-ddr5-1tb-nvme-ssd-st-shadow4-2101-b-al-black/p/3D5-000Z-002X2),Neutral
AMD,I vouch for this u/Robokop3,Positive
AMD,All wild rumours.,Negative
AMD,- A product by AMD - A consistent naming scheme easy to understand  Choose one.,Positive
AMD,They make the name confusing on purpose obviously,Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"I've not seen the XL Radeons since about 20 years ago, they used to be made for OEMs, cut down versions of the XT models i.e. 9800 XT and 9800 XL (one of my old Medion desktops had one)",Neutral
AMD,"XL, for the larger gentleman.",Neutral
AMD,"Ah, for a second there I thought the XL was for extra VRAM or something, and was hoping that AMD was about to release an LLM/AI card for us poors. But alas...",Neutral
AMD,Hopefully it will come soon,Positive
AMD,a chunky 9060?,Neutral
AMD,"This seems to regularly come up - ""xl"" is a suffix used internally for a bin of dies, e.g. the ""navi31 xl"" was marketed as the 7900GRE, ""navi21 xl"" was the 6800 etc.  I assume this is a wires crossed between internal chip name and external marketing name, or even the marketing name for that particular bin hasn't been decided.",Neutral
AMD,Would make a lot of sense if this is also an oem model,Neutral
AMD,you mean a small north american... XXXXXXL in chinesism?,Neutral
AMD,But isn't the 9060 non-XT OEM-only already?,Neutral
AMD,likely regional exclusive too,Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,Magnus is Xbox. Orion is PS6 and Trion is PC  GPUs,Neutral
AMD,"they need to make a high end competitior to Nvidia ""Megatronus Prime""",Neutral
AMD,The custom APU they made for the Steam Deck is called Aerith. The one they made for the OLED model is called Sephiroth. Take that how you will,Neutral
AMD,Can they PLEASE just have it support PyTorch without any weird shit?   That’s it. That’s all that we need to happen.,Neutral
AMD,I guess I’m old because these are not the Transformers I’d start with.,Negative
AMD,"When rocm 7 hits and capacity is more available they will crush it. No one likes NVIDIAs near monopoly, as soon as a better option is available many will jump ship.",Negative
AMD,"Just give me ""Metroplex heeds the call of the last prime."" A 64 Gb card so we can run decent local models",Neutral
AMD,So MLID was right about that. AT0 = Alpha Trion.  https://i.redd.it/eh5jsgh7u0nf1.gif,Neutral
AMD,"Does any of this even matter? I got an RX 9070 non-XT because after vouchers and platform discounts it was cheaper than even the cheapest RTX 5070 in my region. Undevolted really well but it performs like a drunken master. Some games like 10-15% faster than something like a RTX3080, in some games nearly twice as fast. It's all over the place depending on title all white consumption was around 230W a good 100-110W lower than most OC RTX 3080s yet people are still buying Nvidia, JPDR now reports 94% market share. AMD will be forced to answer to investor pressure as they are not doing anything remotely useful to push their cards into the hands of gamers. The fake MSRPs are so bad in some places that after vouchers and platform discounts it is cheaper to get Nvidia.  My region and Mindfactory are the exceptions not the norm.  RDNA2 was a better showing but we never got to see how that would play out had the crypto cancer not happened.",Neutral
AMD,If UDNA/RDNA5 does not catch up in a meaningful way to NV or be heavily discounted compared to the last few gens I think AMD's discrete GPU division is dead.,Negative
AMD,Nvidia are going to be fucked when VOLTRON is formed. Flaming sword in their ass.,Negative
AMD,AMD naming scheme strikes again.,Neutral
AMD,Hurry up! I am waiting to upgrade from my current 6900XT,Neutral
AMD,"Can they please just have it avaliable for the advertised price lol....  Wanted a 7 series...but ray tracing performance fell short of claims....so no 7900xtx.  Wanted a 9 series...but not in stock, and 600 was all but a lie...and i really didnt want to be stuck with a 16gb gpu anyway....so no 9070xt...  I still want a new gpu.....amd... Can you please offer me something at the performance you claim, for the price you claim, that is actually available with minimum 24g and ideally 32g+...my wallets open....but i want value....",Negative
AMD,Why isn't it Unicron?  I want Unicron that destroys all GPUs.,Negative
AMD,do lotr next!,Neutral
AMD,"these names,, love it",Positive
AMD,Missing Rodimus 🧐,Neutral
AMD,Likely due to the Lego 3D design putting things into transformation depending on market segment and needs,Neutral
AMD,Shouldn't there one be called Optim... oh right 😅 Nvidia would probably sue them.,Negative
AMD,"If they name one after Megatron, I'll be interested.",Positive
AMD,Till all are one,Neutral
AMD,hoping for more frames at native resolution while using less wattage...,Positive
AMD,When is AMD beating NVIDIA?,Neutral
AMD,"Given their piss-poor track record of competition against Nvidia, maybe they should have started with Go-bot names.  They can upgrade to Transformer names when they decide to either beat Nvidia significantly at performance or undercut them in price by a significant amount instead of just coasting along at slightly under what Nvidia charges for the same performance...",Negative
AMD,Where is my beefy 200w total board power APU? That's what I want. A big boy I can stuff anywhere.,Positive
AMD,"Considering that the next architecture is supposed to be UDNA, not RDNA5, I don't have high hopes for the accuracy of the rest of the information.  If RDNA5 was going to be a thing, AMD would have disclosed that sometime in the last 3 years, but they instead listed UDNA as coming after RDNA4.",Negative
AMD,ok but did they fixed amd reset bug yet,Neutral
AMD,"So, they're now naming their GPUs after Japanese anime characters.",Neutral
AMD,"Xbox should be named after a Decepticon, given the way Microsoft has ruined it in the past 10-15 years.",Negative
AMD,Either way  ![gif](giphy|sDcfxFDozb3bO),Neutral
AMD,They need Primus himself,Neutral
AMD,"If they finally have a top tier card that beats nvidia's xx90, they just should name it bigus dickus.",Neutral
AMD,so we will see asus megatronus prime prime GPU,Neutral
AMD,Unicron,Neutral
AMD,And have a collaboration of some kind with the Detroit Lions,Neutral
AMD,What? Why not MegaUnicronPrime?,Negative
AMD,the halo product that sell poorly?,Negative
AMD,steam deck 2  codename squall when?,Neutral
AMD,they keep dropping older GPUs that have pytorch support that builds and runs just fine as well... very annoying.,Negative
AMD,You can already run PyTorch today using TheRock https://github.com/ROCm/TheRock/blob/main/RELEASES.md,Positive
AMD,"""Ultra Magnus"" is at least a name I know.",Neutral
AMD,"That’s me, I don’t need 5090 performance, I need 5080 performance with more vram! That’s really it for my 3440x1440 set up. I was on the fence between the 7900XTX and the 4080 I got. Honestly I should have got the 7900xtx",Neutral
AMD,"I feel like thats been said for generations now and 9070XT launch, availability and price was a joke.",Negative
AMD,gonna be 9 years soon since ROCm has been waffling around.  for reference you could already use CUDA for enterprise and consumer software around 3 years in (Used it for avc encoding in editors back when SpursEngine was considered an actual viable choice).  AMD finally got video encoding to comparable quality only like two years ago. This is with 100x the money nvidia had when developing cuda. They are turning into Intel where they are slow as balls and push all the development work onto users.,Neutral
AMD,Easy there buddy... you'll anger the MLID reddit hate squad lol.,Negative
AMD,"I really hope AT2 XT is more than 18GB of VRAM, bc RDNA5/UDNA is supposedly coming out in 2027 and 18GB just won't cut it for the high end imo.",Negative
AMD,"The problem is not gamers in the DIY market. Figures from resellers in Europe show that the 9070 XT sells more than all RTX 5000 combined. In the US, it's less favorable due to higher prices for AMD which indicates that there is demand anyway.  I think the 94% come from China which favors Nvidia,  from OEMs which is dominated by Nvidia in prebuilts and laptops.  If AMD wants GPU market share, they need to attack those markets more aggressively.",Negative
AMD,"I'm gonna get myself a 9070 (non-XT) this December, because it's the cheapest card of all",Positive
AMD,"they were ""dead"" between 2014 up to 2020 (when RDNA2 launched) and even that wast 1:1 parity with Nvidia feature wise  the GPU divison isnt going anywhere and they are at much better position now, even if they arent making high end GPUs, they pretty much closed all feature gaps (RT in RDNA4 is very decent, FSR4 quality is good, Perf/W and Perf/Area is very close) and they very likely going to keep designing console's GPUs and mid-range cards like the 9070xt",Neutral
AMD,Any time now.,Neutral
AMD,Get ready for a long wait and dissapointment,Negative
AMD,"its not a grim as they put it....   Amd GPUs are still in consoles, all of the recent handhelds, steam decks, and a shitton of laptops have radeon graphics onboard.  amd graphics arent going anywhere. imo",Neutral
AMD,"For AMD, its never late as long as they sell enough cards to make a good profit. In recent years the RADEON side has been mostly production constrained. AMD sells mostly every card they produce. The issue with market share is that the production is small because CPUs are much more profitable.   For you, does It matter? ir you but what is best for you, does It matter if the market share is higher or lower? Games ""use"" RDNA. They won't run slower because the market share is lower.",Neutral
AMD,how can someone believe this,Negative
AMD,"I bought a 9070XT for a few reasons. I got a reasonable price on one of the better reviewed cards, AMD has vastly improved their drivers over the last few years including the excellent FOSS Linux drivers, and the performance target was right where I wanted.   For what I paid, the card is excellent. It's reliable, it's not too bulky or loud, I use it on Linux. I can even play demanding games like *Monster Hunter Wilds* at 4K60 with ray tracing (some upscaling but no frame generation).  Even though nVidia might have market dominance, AMD's current offerings absolutely stand on their own.",Positive
AMD,I'd wait for rdna5 :) if amd won't fuck it ip,Positive
AMD,lmao,Neutral
AMD,little redditors trying to support a 260 billion dollar company  LMAO,Negative
AMD,"Properly done games use API not tied to specific hardware brand, like DirectX 11/12, Vulcan etc  So there's no problem as long as features required by games are supported by the GPU driver",Positive
AMD,amd has like 100% of console gpus market  That 94% is silly numbers  only userbenchmark and hired nvidia shills believe such nonesense,Negative
AMD,"amd has overall 17% market share, even excluding igpus its still well over 10%, sales numbers don't mean that much, amd volume always stays on the shelves for long but once the price drops they are the best deal and everyone buys them, these stats only reflect the current gen gpus but most popular right now are still 7xxx and even some 6xxx,   back in 2018/2019 amd was selling so many rx 570/580 (because they were already old and very cheap) that they had like 50% share in sales",Neutral
AMD,"You fundamentally misunderstanding how game development works doesn't mean your new card will suddenly be unusable. Developers don't optimise for specific hardware (except in the case of consoles which, funnily enough, use all AMD hardware!) They use engines with features that both companies develop to utilise, and some products are better at that than others. You buy the underdog because it's (often) cheaper and you like a bargain. If you don't want that, don't buy it. Nvidia cards have better features and more widespread and robust support for those features.",Negative
AMD,"""wtf did I buy the underdog for beyond better morality?""   Well, avoiding the potential risk of fire for one",Neutral
AMD,its the best for your pricepoint at the time you buy. thats the end of it.,Positive
AMD,posts as of late are using udna and rdna5 synonymously under the auspice that engineering internally is doing the same,Neutral
AMD,Even Mark Cerny mentioned RDNA5 as the future of AMD. People interchange RDNA5 and UDNA because it's the same thing.,Neutral
AMD,Even Mark Cerny referred to RDNA5,Neutral
AMD,UDNA only means unified tbh. The micro arch is going to the original gcn days (gcn being the micro arch and not the isa) and that means the cdna and rdna is gonna merge. You may as well call it rdna and I won't be surprised. NGL rdna sounds so much cool,Positive
AMD,As someone that isn't much of a console guy what has Microsoft done to ruin it? All I know is they've never been as big as Playstation and thanks to them releasing console exclusives to PC we now have Sony doing the same. Seems like a huge win for PC gamers.,Positive
AMD,Primus sucks though,Negative
AMD,And lower end should be called **Incontinentia Buttocks**.,Neutral
AMD,Any reasons they cannot make a halo product that sells wellly are purely technical in nature lol.,Negative
AMD,I'd like to remind you that on Steam alone there are more 5090s than any AMD GPU of the current gen,Neutral
AMD,"*""Whatever....""*",Neutral
AMD,tidus_laugh.gif,Neutral
AMD,They are focused on implementing support for all GPUs from Vega onwards.,Positive
AMD,"Not on windows without wsl, which also comes at a big performance loss",Negative
AMD,"> should have got the 7900xtx  It sure seems like I am going to have mine from Jan 2023 until whenever they release RDNA5 so realistically ~4 years, and wow, just look at me cry a river because I am so sad about not having Nvidia that whole time, my tears are flooding the towns and drowning the townspeople, Noah's Ark is rising",Negative
AMD,"Same for me, my 2080Ti was a bit long in the tooth and 3080/90 was sheer unobtanium at that time. Got the 7900XTX. For 1440 ultrawide it is perfect and only real upgrade path is a 5090. That is not going to happen unless I win the lottery.",Neutral
AMD,"I will probably get the R9700 when it’s available. I use a Mac for training for the memory and MPS and ROCm are both 2nd class citizens for now, but I think that will change in the next 6-12 months.",Neutral
AMD,"lol there was maybe a bad two months before those GPUs were fairly available, with some models at MSRP. 9060XT stock is already completely stable, and that’s been out for less time as well.",Neutral
AMD,How was the pricing a joke if they couldn’t keep it in stock? NVIDIA and Apple have a massive part of the production of all of the latest nodes.  And I don’t think you know what rocm 7 has.,Negative
AMD,"You should tell the AMD customers that are spending billions on their datacenter GPUs, this is great, well researched info and they need to know.",Positive
AMD,High-end is 36GB.   AT2 XT = 9070 XT sucessor = mid-end.,Neutral
AMD,"Figures from Europe? What are you talking about. What's your sources? Steam survey, JPR, all disagree with what you're saying. And don't even try to quite Mindfactory here. There's no way the numbers will be true if a single card variant 9070 XT outsells an entire product stack.",Negative
AMD,"Given  that in most titles at 1440p the gap between the XT and non-XT is not that impressive, it will come down to price in the particular region. For me I saw the XT prices were like 31% higher, not worth it. I can just undevolt non-XT keeping the same power target to close the gap in most cases. If you can get the XT for like 10% more that'd be the way to go and still undevolt it to get efficiency gains. If non- XT is the best value in your place just go for it, it's a very good card.   Surprised me in a lot of titles that nobody tests any more. A widely reported outlier and current AAA title is AC Shadows, non-XT is significantly faster than 3080 but I have seen this sort of outliers in older games, my example is ME Andromeda. I used a specific point in the game to test the most brutal frame rate drops.   Undevolt is a lottery, I was expecting to not be able to go lower than -40, maybe -50 but I've been good at -70 and -100, keeping it at -70 for the moment. Will try playing around a bit more to see how much further I can get it to undevolt. In any case without any undervolting the non-XT is a very good card at stock.",Negative
AMD,"I'm sure if they could switch to Nvidia they would. I'm actually wondering why they don't. The first Xbox used Nvidia and currently Nintendo is the only manufacturer using them.  They've been behind on a lot of stuff we have on PC because they had to wait for AMD to catch up, and that hasn't fully happened yet",Neutral
AMD,1mo old account with mainly AMD hating and a few top poster badges. Userbenchmarks bot,Negative
AMD,"We've heard ""wait for [next Gen, when Nvidia will also be next Gen]"" for literally a decade now. If we're lucky, amd will catch up to current Gen Nvidia... Next Gen.",Neutral
AMD,Yea i chose a 7900XTX over 4090 because of the potential IED in my tower. And it was half the fucking price of a 4090,Negative
AMD,"The fella that works at Sony?  > Big chunks of RDNA 5, or whatever AMD ends up calling it, are coming out of engineering I am doing on the project  That was the quote I could find, it doesn't sound like he had access to its proper name.",Neutral
AMD,"From what little I have read from actual AMD employees, it seems it is called unified not because they are merging the two archs, but because they are unifying the software stack between corporate and consumer hardware with the new arch, and providing only a single arch between them.",Neutral
AMD,"There are more reasons, but it started with the Xbox One. Just look at their *gaming* console reveal. The main message was not gaming: https://www.youtube.com/watch?v=nULp0pGKCS8  In addition to that they wanted always online DRM and to fuck over customers that buy physical games in regards to reselling [0] and sharing them with friends. Sony was clever and used that fuck up in their PS4 reveal to great success: https://www.youtube.com/watch?v=kWSIFh8ICaA  [0] https://www.cnet.com/tech/tech-industry/xbox-one-restricts-disc-resales-requires-daily-gaming-check-in/",Negative
AMD,Unicron?,Neutral
AMD,"He couldn't multitask so he transformed into Cybertron, started producing Energon being the core after making the Thirteen Primes of Creation to handle his chores. Yeah that tracks.  Meanwhile Unicron still out there waking up every while or so and wreaking havoc",Neutral
AMD,yeah and most of those are used in workstation pc and not gaming pc....  what else you got??,Neutral
AMD,Lets not make excuses for them.,Negative
AMD,They have native windows binaries available. See the link above,Neutral
AMD,I should have waited for few hours and got 7900XTX for less than 470€ instead of buying GRE for 430€ (was shopping for a used card few months ago and this still makes me annoyed at myself) What makes it worse is the fact that my GRE sucks at at any kind of OC so it's basically just a more expensive 7800XT,Negative
AMD,"I doubt rdna will launch in 2029, it will be more likely late 2026/early 2027",Negative
AMD,How much did they produce to begin with?,Neutral
AMD,"That has nothing to do with broad rocm support. no shit a company spending billions has engineers to write their own software. you fanboys are the reason they get nowhere with their software and I wouldn't be surprised if they went down the same route as intel in 5 years (straight into the bean counter toilet thanks to hubris).  and for comparison, nvidia has 7x the sales and higher profit margin. they are doing better than AMD and Intel combined. good job to AMD for losing leadership and tens of billions of dollars every quarter to a younger company I guess?",Negative
AMD,"They can call it mid-end, but the price I still consider high end. But still, only 18GB in 2027 even for mid-end is not good imo",Negative
AMD,"Steam survey is the least reliable source as it includes every PC on the planet that installed Steam. For recent GPUs it's ok but the percentages are not representative of actual sales. Like 5% on the Steam survey is actually a massive number.   I don't know where you live but if you go talk to any computer shop it's closer to a 60-40 split in favor of Nvidia. So nowhere near the 96-4 repartition from JPR. But it makes sense. Europe DIY is a very small market compared to global DIY, prebuilts, laptops etc...",Negative
AMD,https://en.overclocking.com/mindfactory-more-amd-gpus-than-nvidia-sold/,Neutral
AMD,"simple....amd offers custom solutions. nvidia doesnt.  ""Semi custom solutions"" is what they call it  making your own Cpus AND gpus has its own advantages.   and its powered eveything from Xbox and play station for over a decade, the steam deck, Haydes canyon, chinese clients, etc  a lot of these are custom APUs, easier to design around and manufature when its all just one chip.   the switch has the tegra...which is a whole ARM SOC that while tweaked and made for nintendo.... its not on the same caliber to whats in current consoles.",Neutral
AMD,"Because Playstation, XBox, Steam Deck and practically all PC gaming handhelds use AMD GPUs?",Neutral
AMD,"First because dGPU is not even the main market for games. The console market is the main market and AMD dominates it. Game devs will very often plan their performance and graphics quality based on the console hardware.   Then, for the most part, devs don't even optimize that much for specific brands. The brand-specific optimization comes from the GPU-maker sending their own engineers to do the fine tuning. This is usually something proactively done by Nvidia/AMD, not something requested by the devs. As long as AMD is willing to invest in doing it, they will keep doing it.",Neutral
AMD,Every console game is made for rdna?,Neutral
AMD,"I mean rdna4 isn't big deal against rtx50 with greens better features like DLSS od Ray Traycing. Let AMD cook and see what happens next. Maybe their FSR, Fluid Motion Frames and ray traycing can get much better? And maybe not? Rtx5070 could be no brainer for the most of us, but this 12gb vram is so disappointing and price gap between 5070 and 5070ti is simply too big.",Neutral
AMD,Probably closer to 50%. Nintendo has sold nearly as many units as Sony and Microsoft put together over the last five years.,Neutral
AMD,How the hell would he not have the internal codename of the product that he is engineering?,Negative
AMD,"What don't you understand in ""Steam hardware survey""? You know, from the premiere PC gaming platform",Neutral
AMD,https://i.redd.it/6yhjscmv01nf1.gif  [TheRock/ROADMAP.md at main · ROCm/TheRock · GitHub](https://github.com/ROCm/TheRock/blob/main/ROADMAP.md),Neutral
AMD,As a 7900 XTX owner I can attest that you made the incorrect decision. That thing rips,Negative
AMD,4 years from the time he bought it (in January of 2023) would be 2027.,Neutral
AMD,"I appreciate the passion, over a computing library...  rocm 7 (which it still seems like you do not have any info on) is a huge step forward, and for the first time AMD has committed to getting it to run on their consumer GPUs.  Logically somewhere between ""it sucks"" and ""it's awesome"" there must be ""hey this is pretty decent now"". I think people who actually use these tools are definitely seeing AMD getting to the third one now. One way they are doing this is to make the API much closer to CUDA so some ports are trivial. The R9700 has the same memory as an RTX 5000 and costs 1/3. For many people like me who do ML and don't need 4/8 bit or use transformers, it's a steal.  It takes 3-4 years to build a new architecture - you can bet in 2022 AMD shifted to making something more competitive with the tensor cores. Look what they did to intel with Zen. So I would expect this time next year a very different landscape.",Positive
AMD,Steam survey is way more reliable than mind factory sales.  Its results are also close to what JPR is showing. Radeons aren’t selling more than Nvidia and anyone saying otherwise is coping hard.,Neutral
AMD,*Nintendo raises eyebrow quizzically*,Neutral
AMD,Yeah and how well has that worked out? Remind me what games are using FSR 3.1 or FSR 4?  Less than 100?,Neutral
AMD,"He is part of Sony, not AMD, and they have a partnership, and the codename for the overall project is apparently ""Project Amethyst"". His statements indicate he has no idea what AMD is / will call the upcoming architecture.  Afaik, he isn't engineering the architecture, he is engineering the software that will be running on it (on the ps6), AMD has been in part engineering the architecture based on the requirements for that software.",Neutral
AMD,why is your picture covered in mold,Negative
AMD,This is exactly my point there is no reason for even gfx803 to the disabled... yes somethings will not work but it the code still functions for what it can even in the lastest ROCm... they just disabled it and shot their customers in the foot for no reason.  If it were unsupported and they only accept patches for fixes and dont' provide them themselves we'd be much better off than them entirely disabling working code.,Negative
AMD,Well it was just bad luck as i had been looking for a card for few weeks with nothing good coming up. So instead of paying 400€ for a used 7800XT or 3080 i went with the GRE. Then obviously few hours after i bought it that XTX came available.,Negative
AMD,"As a 7900 XTX owner, it's now 2025 and a random FlashAttention Github branch from 2023 is still faster at SDPA than the official Pytorch impl. Also it took two years before it even got borderline acceptable, and still isn't on some tensor shapes. The Pytorch story on the 7900 XTX sucks, and when the 7900 leaves service it will still suck. I should have gotten an NVidia.",Negative
AMD,"Yeah I can easily wait another 18 months. If the 4090 could have pumped the G9 57"" at 240Hz it would have been an option, too. But that only became possible with Blackwell and the upgrade to 5090 from XTX modOC is maybe 40% which would uhhh, not improve things much in practice with driver frame gen at 120fps cap. It would literally just be a $2500 Nvidia 7900 XTX for me. And then I'd block it and try overclocking and melt the goddamn 12 pin, nah, bro. I can wait.",Neutral
AMD,"> rocm 7 (which it still seems like you do not have any info on) is a huge step forward  I'll believe it when I see it. AMD doesn't exactly have a brilliant track record with ROCm thus far and there's no evidence to suggest that anything's going to change. ROCm isn't going to get any better without more money getting thrown into it.  > It takes 3-4 years to build a new architecture  And much longer to come up with an actually useful, well documented and well supported software stack that rivals CUDA.  >  Look what they did to intel with Zen  Zen 1 was a minimum viable product that did its job to keep the company afloat. It really wasn't until Zen 3 and the X3D series where they actually started consistently beating the shit out of Intel at their own game, with Intel obviously doing itself no favors by basically sitting on their laurels and getting hardstuck at 14nm++++++ for the longest time. It's a great comeback for sure, but context matters and it's not something that can be easily replicated if any of the conditions didn't line up (like TSMC incidentally also being in the business of producing 3D Vcache). Nvidia isn't Intel.",Positive
AMD,"Look, when rocm 7 advertises itself with stuff like idk 3x faster in vllm or whatever it was, that doesn't mean that AMD have put heroic efforts into optimization, it means that their code had horrible, gaping flaws in it until now. I'd almost be embarrassed to advertise that.  quick guide:  > ""we improved performance by 20%"" = ""our engineers got really down into the weeds and squeezed out some more cycles with black magic"" > > ""we improved performance by 300%"" = ""yeah we were calculating everything twice, oops""",Negative
AMD,"It absolutely does lmao thats cope, Every modern high end game is built AND designed around the next gen consoles, aka an amd chip. Switch is not only a minority as most games on pc are NOT switch ports (rather the opposite), but also the switch is an arm, so even if it was the case it wouldnt even give an advantage to nvidia gpu.  desktop pc are a complete oversight for game developping, most devs do not design nor build and not even sometimes optimize their game them",Neutral
AMD,"Switch 2 is irrelevant for the conversation, it didn't catch up to new consoles. Switch 2 AAA games(outside of Nintendo's own) will remain either as ports of old games or handcrafted Switch 2 versions in order to adjust to its shit specs.  New games still focus on proper consoles and PCs first in development, then a Switch specific port is made (if it is even considered). And all proper consoles run on AMD.",Negative
AMD,"There's around 80 million PS5s, ~75 million active PS4s, and 30 million Xbox Series consoles. Yes, there's more Switch 1 consoles than any one is those, but that's still a major chunk of the market, especially for a major game release to target. Plus most developers just use a major game engine anyway, and support for AMD in those isn't going away.",Neutral
AMD,"The PS5 has shipped more than 80 million units, and sold more than 75 million to customers.",Positive
AMD,The only people that make Nintendo games are Nintendo anyway,Neutral
AMD,since when are games developped for switch then transferred to consoles / to pc ports ? oh yeah its the opposite,Neutral
AMD,fsr 3.1 is a hot piece of garbage and is NEVER an option to choose.  fsr 4 is another story but it just came out,Negative
AMD,What is Pytorch,Neutral
AMD,Zen 1 was also massively delayed.,Negative
AMD,"Huh, that's interesting given that the big controversy on the dev side for the Switch 2 is all about how hard it is to get dev kits....  Also:[Roughly half of software sales on Switch has been third party software ](https://www.nintendo.co.jp/ir/pdf/2024/241106e.pdf)",Neutral
AMD,![gif](giphy|pfHBWCCOySoRq),Neutral
AMD,"Point made  It’s been how many years and amd has been incapable of producing a competing technology to Nvidia  And people think they can magically produce a gpu competitive with whatever they put out? They can’t even compete on the software realm, there’s no foreseeable future where they are capable of competing on the high end",Negative
AMD,tl;dr AI,Neutral
AMD,Cope,Neutral
AMD,Would you be so kind to share the link to that data then?,Neutral
AMD,How many new AAA games are being developed for Switch vs PS5/Series X?,Neutral
AMD,"Which is, by definition, comparable...  Swítch 1&2 are some of the few consoles that use Nvidia chips. add 6 million for the Switch 2,  Add the various gaming handhelds, most notably the steam deck, the xbox series whatever they're calling the current ones, and you have at minimum 110 million units.  The Switch 2 will keep selling, Nintendo products always do, but so will the PS6, and Xbox Series-whateverthefuck. Couple that with the rest of the APU market, in which Nvidia hasn't started selling yet. Granted, those are dominated by Qualcomm and Apple.",Neutral
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,"His point is perfectly relevant. The only devs that develop games for switch are Nintendo, everyone else makes games to run on ps and Xbox.",Neutral
AMD,"Wasn't the same said about CPUs just a few years back? Look at Intel now, technology can move quite fast",Neutral
AMD,"dont mistake amd taking the ""wrong"" route for fsr with them being ""incapable"" of producing a competing technology to nvidia. Nor only this is absolutely not true, but also... kindly reminder the ceo of nvidia is the UNCLE of the ceo of AMD. They litterally have family gatherings together. you dont find that suspicious how they always seem to take different roads tech wise, then end up adopting what the other one did when ut works ?  people need to stop that nvidia vs amd bs. They are in the same camp and playing you guys. Both gpu are capable",Neutral
AMD,"Oh, well yeah buying an AMD GPU for AI workloads that's your first mistake, even as a huge AMD guy you gotta buy Nvidia for that",Negative
AMD,he deleted his post lmao,Negative
AMD,>Also:[Roughly half of software sales on Switch has been third party software ](https://www.nintendo.co.jp/ir/pdf/2024/241106e.pdf),Neutral
AMD,"I’m well aware  Gpus have been shown to not provide the same benefit with chiplets as cpus. And AMD has tried multiple times to rearchitect things but technology has consistently outpaced them  They were behind the curve on ray tracing, on dlss equivalent, on path tracing, on their software suite (smooth motion, fsr3, 3.1)  I understand this is the AMD subreddit. But at some point I would’ve thought even people here would see how AMD has completely ceded the high end because they are incapable of producing a card within cost to even decently compete with Nvidias high end. This isn’t like ryzen",Negative
AMD,"that is in no way contradicting neither my or his point. 90% of third partybgames sold on switch are not ""developped"" for the switch, but ports. the copium omg",Neutral
AMD,Certified Aliexpress classic,Neutral
AMD,So 9600x with no igpu,Neutral
AMD,ok but how about an international release,Neutral
AMD,Ali express is gonna have field day with this one,Negative
AMD,Are 6 cores enough for todays games ?,Neutral
AMD,So AMD 6 core era is now the new Intel 4 core era. They're gonna milk consumers with this bullshit for years to come until Intel steps up with competition.,Negative
AMD,"Why is the 9500F 7–24% faster than the 7500F, while the 9600X is only 1–5% faster than the 7600X? Is the RDNA3 iGPU really that power-hungry?",Negative
AMD,Borderlands 4 in minimum specs demands any 8 core cpu. So 6 core cpus are obselete.,Neutral
AMD,You got it,Positive
AMD,And bit lower clocks,Neutral
AMD,"For the most part, yes.",Neutral
AMD,"Yes. 8 is optimal, any more is a special case scenario.",Neutral
AMD,https://youtu.be/AR9V8RTvVcM,Neutral
AMD,"I mean the 600x could definitely move up to 8 cores, but a cheap 500f being 6 cores is fine with me as long as they stay cheap",Neutral
AMD,"Granted zen 4 to zen 5 was pretty underwhelming BUT intel didn't just stall in core count but also in core performance. If we continue to get 6 cores but with (significant) ipc increases, that is progress.  Rumor has it we will be getting core count increases with zen 6.",Neutral
AMD,">8 is optimal,    8 cores are typically <5% faster than a 6-core of the same generation   For gaming, it's not the optimal CPU in the stack",Neutral
AMD,This test is nonsense. Testing only one game is stupid and doesn't tell anything.,Negative
AMD,"Just make the Ryzen 5 a Ryzen 3, the Ryzen 7 a Ryzen 5 and the x900 Ryzen 9 a Ryzen 7. That would simplify everything.   So, Ryzen 3 would be 6 cores, Ryzen 5 8 cores, Ryzen 7 12 cores and the Ryzen 9 a 16 core CPU",Neutral
AMD,"In benchmarks, yeah.  I don't know a single gamer that runs nothing but the game itself, so improvements from core count can become significant.",Neutral
AMD,And that’s only because the 8-core chips are better silicon and clock higher,Neutral
AMD,"Oh, sure, Steve is keeping the real data from his years of experience a total secret!",Neutral
AMD,Would be easier to match the Ryzen with the number of core and then a number for perf index of the core / cpu,Neutral
AMD,">In benchmarks, yeah.    So objective measures of performance vs you saying 8 is optimal?   >I don't know a single gamer that runs nothing but the game itself   Every gamer I know does. Our small groups aren't representative   Other tasks also don't apreciably change the margin:   https://youtu.be/Nd9-OtzzFxs",Neutral
AMD,What? Don't act stupid.,Negative
AMD,"Like Ryzen 6, Ryzen 8, Ryzen 12 and Ryzen 16?",Neutral
AMD,That's 3 years old now...,Neutral
AMD,>Every gamer I know does. Our small groups aren't representative  They don't have Discord open for voice comms? They don't have a browser in the background?  I don't believe you to have any level of expertise in these matters.,Negative
AMD,Browser doesn't matter whatsoever outside of RAM performance which will bring down performance anyway. That applies to all programs you run btw so everything you have open will cost CPU performance. They all add to overall RAM latency.  From what I've seen of Windows scheduling if you run Discord there's a high chance it's just running on the same core as the game's main thread so it'll also bring down performance even on a 32 core CPU.,Negative
AMD,future steam console apparently is also going to be using silicon with the iGPU removed. I guess they may have plenty of die salvage stock where the iGPU's don't make the cuttoff mark for their normal SoC (APU) models.,Neutral
AMD,Nothing beats the goold ol' Aliexpress 7500F for $120,Positive
AMD,"I’d like to buy a “G”, please. Preferably with RDNA 3.5 or later.",Neutral
AMD,"Retailer leaks are usually on point, maybe its coming soon",Positive
AMD,"there are still no f versions of the x3d cpus, what do they do with x3d cpus that have a broken igpu",Negative
AMD,That one doesn't make sense to use in a Deck. Having a dedicated GPU is too inefficient for a handheld.,Negative
AMD,"But prices are going up (like always xd). I got mine 7700 for 140$ half a year ago, now the cheapest I can find is 205 bucks on AliExpress",Neutral
AMD,I'm super happy with my 7500F,Positive
AMD,hopefully these will be cheap too with better distribution,Positive
AMD,"I got my 7500F locally while I was up in the PNW this summer for $100. Pretty good price I'd say, it was $142 for one on aliexpress at the time and the one I got was a same-day pickup... which is good because I got it literally the day before I flew back home haha.",Positive
AMD,"RDNA 4 ""G"" with FSR 4 support would be amazing!",Positive
AMD,me too,Neutral
AMD,"It’s for the future steam console, so should be good to use",Positive
AMD,Because it's not for a handheld. Fremont is an iteration on the stam box concept,Neutral
AMD,"yeah i don't know how reliable [the rumor](https://tech.yahoo.com/gaming/articles/valves-secretive-fremont-gaming-device-182649536.html) is. Perhaps it's just a prototype using this discrete gpu setup as interim until the real silicon is ready. But perhaps the rumor could be accurate too. 4 to 6nm is incredibly efficient, so even with suboptimal setup they may well get decent power efficiency.",Negative
AMD,Now that was an amazing deal! I'd take a 7700 over my 7500F for that price tbh,Positive
AMD,You gotta ask for deals on the 7 7700. Here in Australia we still get the ryzen 7 7700 on AliExpress for around $140 USD when some guy gets exclusive coupons to drop the price for us Aussies.,Neutral
AMD,"Chinese sellers adjust prices frequently based on demand and availability,so either demand is way higher now or amd is slowing down production of 7000f series to make way for 9000f series.",Neutral
AMD,I’m surprised I got a 5700X for $100 AUD a few weeks ago honestly,Negative
AMD,pricing in the US is so bad for these CPU's that the X-SKU's are consistently less expensive than their vanilla counterparts domestic. and as for aliexpress? you're only beating the domestic market if you get the crapass 8400F with the paste IHS.,Negative
AMD,"I'm you from the future, the 9070XT pairs exceptionally well with the 7500F don't let the comments about it getting ""bottlenecked"" bother you",Positive
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,That's in 2028-2029,Neutral
AMD,I got a 7700 for 170€ in France on Ali Express,Neutral
AMD,tarriffs,Neutral
AMD,Thats a nice cpu for a b450 or b550 mobo. Its runs nice and cool at 65w,Positive
AMD,"I'm not too concerned about the potential CPU bottleneck in this scenario because it's not like I'm running 1080p where the GPU doesn't get much action. that, and my 7500F was originally intended to be a stopgap CPU between getting started on AM5 and getting an R7 11700/AI 470/whatever they call the Zen6 generation when I start to demand more performance what the 7500F can provide.  oh, just realized this is not my GPU for 3440x1440 post. I should clarify I run 3440x1440@180, so that's plenty of load to be put on the GPU.",Neutral
AMD,"If MSRP for 9070 non XT is $549, then the GRE has to drop below $500 to be appealing. So 3600RMB is still not enough, drop more.",Negative
AMD,"Good.  That card, and really all the RDNA4 cards, need a solid price drop.  I realize AMD needs a profit to keep doing stuff but they got too money hungry here.  Especially vs the advertised launch MSRP's.",Positive
AMD,"AMD has a branding problem, outside of reddit AMD cards barely register when it comes to sales",Negative
AMD,"AMD back at launching products too expensive, then dropping the price just a few weeks after when it doesn't sell. Maybe look into the market next time? The RX 7800 XT is selling great at the same price as the 5060 TI (16G). So why would anyone pay more for a model with less ram? Drop the 12G GRE to 5060 TI - 50€, and add a 16G version at 5060 TI pricing and it's going to sell just fine.",Negative
AMD,this was much needed price drop,Positive
AMD,"I would buy a 9070xt, but I am not willing to spend $100 over MSRP for it when I have only one game that necessitates the upgrade right now... Besides I'm just playing all my backlog old shit anyway so I guess I'll just go lurk r/patientgamers",Negative
AMD,What is GRE meaning? 9070 xt > 9070 gre > 9070 ?,Neutral
AMD,"it should be 2999 at most, isn't it 12GB?",Neutral
AMD,"They don't, Americans need to stop blindly buying what they're recommend and Watch the price tag.  That problem is mostly in the US, in the EU you can even find some of those below MSRP.",Negative
AMD,"> That card, and really all the RDNA4 cards, need a solid price drop.  :: Check MicroCenter ::  :: Half a dozen cards that had 25+ in stock a week ago are all down to less than 10, some less than 5 ::  Yeah, they really don't.  :: Check Amazon ::  :: Literally all are $100+ over retail ::  :: Rounding out the bottom of the top 20 in sales ::  :: GeForce 5070 Ti is actually going for MSRP ::  Okay maybe they do.",Negative
AMD,"I was wondering, are they earning a profit considering the AiB's are the ones determining the final off the shelf price right?",Neutral
AMD,Prices for the other models are good and around or below MSRP. If americans want that too they need to stop blaming AMD and stop the thing that increases prices.,Neutral
AMD,> AMD back at launching products too expensive  It's crazy how they almost priced the 9070 xt at 700+ dollars. Hub talked about this before but apparently amd was gonna price that card at 700 dollars until they saw their video + gn's video about pricing and then they immediately dropped it to 600. Crazy to think that there is a timeline where amd keeps the price of the 9070 xt at 700 bucks.,Negative
AMD,"Why would anyone pick 7800XT over 5060ti at the same price? Only reason if you exclusively play COD. It’s like 10% raster difference, however in real world with DLSS rtx card will be faster and will have better image quality",Neutral
AMD,"No, GRE is the bottom tier of a class. The 70 gre is a cut down 9070.",Neutral
AMD,"Personally, I will consider it if MSRP drops to 3299 and I can purchase it at MSRP.  It is 12 GB, so it should not be price equal to 9070 non XT when at launch.",Neutral
AMD,saying that as if the MSRP is a great price,Neutral
AMD,Nearly every tech youtuber has been recommending RDNA4 and yet AMD has lost even more ground to Nvidia in this generation of GPUs.   IMO that and the 5070 being the best selling card of this gen indicates that people are looking at the actual price to performance charts and not outright rejecting 12GB cards just because youtubers want to have 16GB.,Neutral
AMD,They need to get it down now that 5070 TI id readily available at MSRP.   When you’re already spending $700…… $50 extra is feasible for lot of people and most will just opt for the Nvidia card.   You say 25 to 10 in week while Nvidia is selling out much much larger quantities of 5070ti.,Neutral
AMD,They are earning a profit but no one besides AMD or the AIB's know how much since they keep the price they sell to AIB's for the chips secret.  AMD was making increased margins off of RDNA2 back during the crypto boom because of this.  So was NV.  They constantly adjust what they sell the actual chip for to the AIB's in response to market changes and supply sitting on the shelves.,Neutral
AMD,Where are you finding a 9070 xt for below 600 dollars? Im not american but in my country the cheapest 9070 xt is well above msrp.,Neutral
AMD,"I mean, most of the time was sold for 700 bucks anyway, the MSRP was pretty much a lie outside a couple of lucky guys.",Negative
AMD,"Yes i know. When you have to ask reviewers to price your product you have an issue. Leadership at the Radeon Group seems to lack confidence in their product as well as understanding of the market. If you look at the rumored 8900 XT(X) specs, AMD canned a competitive high end chip just because they were afraid of Nvidia.",Negative
AMD,"10% faster and equal in RT is a no brainer for people that know the 5070 is not as fast as a 4090. Not everyone looks at smeary MFG DLSS artifacts and thinks ""Oh yeah, that was totally worth 450€"".",Neutral
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,"It is a great price though... better than the 5070 for only $50 more, or a few % worse than the 5070 ti for $150 less could have been a market changer for anyone who doesn't care about ray tracing that much, and even then the 9070 xt isn't a slouch",Positive
AMD,"Stop looking at shipments numbers, those are inflated on Nvidia's side because of all the AI stuff that runs on consumer hardware.  If you go look at the most bought GPUs on Amazon and Newegg you'll see that AMD is doing way better than past gens, other distributors will say the same and Radeon is reporting record sales.  I don't see why people are surprised that the 5070 is doing good, stuff doesn't change in a single generation, especially when one of the competitors holds 85% of the market share, AMD didn't even book the capacity to sell more than the 5070.  Still, RDNA 4 is doing well and people like the product, in the US people want those GPUs enough to keep them over MSRP while the 5070 and 5070ti fell at MSRP.",Neutral
AMD,I feel like you're missing the 2nd part of my post where AMD's 9000 series is getting slaughtered on Amazon compared to Nvidia's 5000 series XD,Negative
AMD,"notebooksbilliger.de has 9070 XT's for 549€ without VAT.   At the time of 9070 XT launch, 549€ was ~592$.     If people weren't snapping up the 9070 XT's at current prices, AMD would probably lower prices, due to the weakening of the dollar vs the euro, like NVIDIA did.",Neutral
AMD,"You should remember that american MSRP is without taxes, but in other countries usually includes taxes. [Here](https://www.computerbase.de/preisvergleich/xfx-swift-radeon-rx-9070-xt-triple-fan-gaming-edition-rx-97tswf3b9-a3431083.html) is the currently cheapest card i could find at 629€ (at Mindfactory), well below the 689€ MSRP. You can find similar offers in most of the EU, adjusted for local VAT.",Neutral
AMD,Now imagine if this thing was 700 msrp and then it was also inflated like the nvidia gpus lol. That would be even worse.,Negative
AMD,"Thats simply not true) but noone stopping you enjoying TAA Blur, have fun with it!",Positive
AMD,"just because nVidia's pricing is horrid, doesn't mean AMD's is great",Negative
AMD,But the August Steam Hardware Survey corroborates the JPR data. If all those consumer GPUs were being exclusively used for AI they wouldn't be showing up on Steam. At least some of the folk who bought 50 series GPUs for AI are also gaming on them occasionally.,Neutral
AMD,"My apologies, I think I misread your comparison.   I've been saying for the last few years now that I think AMD is just content with selling their cards to the sub10% base they have that will buy AMD cards. Really doesn't look like they have much intention on gaining any marketshare....they definitely do not act like it. If they cared, they'd try to ramp up supply and get pricing closer to MSRP before Nvidia got there............but they didn't.",Negative
AMD,"If DLSS is so good, why waste the money on a 5060 TI? Just get a 5050 and let MFG take over :D",Positive
AMD,The msrp is fine. It performs like a 4080 basically. Great deal at $600 and FSR 4 is decent too.  The issue is that's not the price you can find it at.,Positive
AMD,"The Steam hardware survey is after different data, it wants to know what most people pay on, not what hardware people are buying. This means that Internet Cafès in Asia are reported multiple times because of all the accounts that log in those.",Neutral
AMD,"For what it's worth, it does look like they're selling through in some regions. While they're not great on Amazon, they have a lot of entries in the top sellers on Newegg and apparently have bumped their market share from sub-10% to like, 25% this year.  That said, I hope they have an answer to what appears to be an incoming late 2025/early 2026 refresh on NVidia's part.",Neutral
AMD,"And the 4080 is/was priced even worse.  It's a 7 series card, 7 series is historically in the $400 range, $500 max.  The 5700 XT was $399 at launch. The 6700 XT was $479. The 7700 XT was $449.  $600 for a 7 series card is a joke, especially without nVidia's semi-excuse of using expensive GDDR7.  I swear people buying GPUs all have Stockholm syndrome at this point.",Negative
AMD,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Negative
AMD,"Still didn't answer the question? Why buy a 5060 TI? Buy a 5050, enable DLSS and save 200€. if you don't care about MFG, you can even get a used 3050 and go with that. I never even said something about the VRAM, afterall we know that there is absolutely no issues with DLSS and VRAM.",Negative
AMD,The name of the cards is irrelevant. It's a gpu that performs like the previous high end at mid range prices. That is fine. What would you have priced the 4080? It's 50% faster than the 3080 which was $700.  $1200 was clearly too much but $900 would have been fine. AMD offering something similar for $600 the next gen is pretty good.,Neutral
AMD,"You're making the argument that DLSS is better than having more performance, not me. If that was true there would be no reason to buy either card.  With the 9060 XT (16G) at the 369€ MSRP and most custom models around 400€, paying 450 for a 16% faster RX 7800 XT does make sense. Paying that for a 10% slower 5060 TI only makes sense to Nvidia fanboys. If you have a 500€ budget, the 7800 XT is the fastest card you can get right now as the 9070 costs \~600€",Neutral
AMD,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Negative
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,7800XT (all RX7000 lineup) is outdated card that noone should buy in 2025 ;),Negative
AMD,"Don't buy them. Bought a ONEXGPU2 late last year. He had issues with the wall power supply (tongs were loose). It was causing arcing and would power off randomly.  After weeks of sending info then claimed that it was an ""accessory"" and didn't fall under their limited warranty. I paid for it with tarrif charge, but the GPU still doesn't work. Probably due to so much ""testing"" to reproduce the bug on video.  Don't buy as their support is shady at best. Warranty non existent.",Negative
AMD,"Many years ago, there were touch screen music & video player devices branded as ""MP5 Player.""   This is probably similar branding.",Neutral
AMD,"USB 5.0 is essentially USB4 v2, though on a capable device it might get 80 gbps throughput.  That would theoretically surpass oculink, which caps at just over 60 gbps, but still, it's confusing because the market is waiting for pcie 5.0 to ramp up with more solutions outside of nvmes.  But this is not USBc implemented over pcie 5.0, which should cap out at 80gbps as well, it's just doubled up usb4.",Neutral
AMD,"Words don't mean anything. You can buy an ""AI powered"" milkshake leave alone the blender.",Negative
AMD,This naming conventions getting out of hand. Usb4.0 2.0 Yeah I don’t think usb 5 even exists yet? The Wikipedia page doesn’t mention 5 existing  currently and says 80gbps is usb4 2.0,Negative
AMD,Were to buy this?,Neutral
AMD,"this would be nice, but they don't provide any BIOS updates (critical security fixes provided by AMD as part of the AGESA updateS)",Negative
AMD,"Hopefully it will be world-wide available. I don't like the red accent they have for their Sapphire Pulse. Spec wise it looks similar, but with a bigger/longer heatsink and 3 fans instead of two.",Negative
AMD,Looks Deus Ex-ish. The new ones,Neutral
AMD,...I'm sure some people will appreciate this aesthetic.,Positive
AMD,"""I never asked for this...""",Negative
AMD,Well i would call it Cupra edition. For those who don’t know it is a car brand which uses copper design elements,Neutral
AMD,buttwhy.gif,Negative
AMD,26cm length and 3 fans of the same diameter?,Neutral
AMD,You could always get the Nitro+ That card look fairly clean and runs very quiet,Positive
AMD,I never asked for this.  Would've been better to do this with the 9070 XT imo.,Negative
AMD,Gorgeous if you ask me,Positive
AMD,Has anyone noticed that like 90% of the posts on this sub are all from videocardz.com,Neutral
AMD,"In my experience, Sapphire cards look much better in person, although I agree there might be some art direction changes in recent years.",Positive
AMD,the triangles does reminds me of cupra. I think the designer is a car enthusiast.,Neutral
AMD,"True, but it seems like this model is similarly priced as the Pulse version.",Neutral
AMD,That's what I was thinking. If it wasn't out of stock I'd already have one.,Neutral
AMD,I hope so I'm waiting for my 9070xt pulse i hope it looks better than on the website,Positive
AMD,"I 've got the pulse version and the card is flawless. Runs very cool and it is noiseless, probably the best card I've ever had in years.",Positive
AMD,"I really don’t like this trend of AMD taking up the majority of their keynotes to have CPU partners come up and talk about how great AMD is.   I guess wall street wants to see it, but it’s just a bit dull unless they are launching a new lineup of AMD business or consumer laptops.   2026 isn’t even getting many products besides a 2026 APU refresh and of course Instinct MI400.   Maybe they talk about FSR Redstone but that was promised for end of 2025.   No GPUs likely coming in 2026 since we just got new ones this year.",Negative
AMD,Lenovo spokesperson?   AMD high end Thinkpad (P16 without suffix and P1-equivalent) or he can shove it.,Neutral
AMD,Lenovo VS Dell? What’s better,Neutral
AMD,"Not a whole lot else for them to do. GPU generations are getting longer, so new releases in 2025 means nothing on that front. The CPUs might showcase, but we'll see when Zen 6 is ready.",Neutral
AMD,AMD are really turning into a 1 trick pony with the AI processors.,Negative
AMD,"Dell is a Intel bitch.   Lenovo is pretty nice, but if you want avoid a chinese company, look for Schenker notebooks (XMG is their gaming series)",Negative
AMD,"Lenovo has better build quality and cooling. Lenovo works with the Open Source community to improve support for their hardware on Linux if you're ready to move beyond Windows including BIOS firmware updates. My Lenovo hardware has been consistently reliable, and outlasted almost everything else. Also, they have a huge R&D center as well as a distribution and repair center near me in North Carolina. After they took over the laptop business from IBM, they expanded their presence here. Meanwhile IBM keeps laying people off, and Dell closed their customer support center here as soon as the tax break ended. So I'm a little biased towards Lenovo for multiple reasons.",Positive
AMD,"if its pure product announcement, it will only be 15 mins long LOL  the whole corporate circle jerk is basically padding for time and also stroke the ego of both sides.",Negative
AMD,"It’s where all the marketing buzz is right now.  CPU speed and core count on even entry level options has already met the needs of the masses.   Efficiency keeps going up, but is otherwise not that exciting. (Though idle efficiency is the more important number though we always report load efficiency)  NPU performance is at the point where it’s gaining high double digit every generation. It’s the next means to create an a new inflection point in personal computing advancement.",Neutral
AMD,Dell can be an Intel bitch but I will only accept Lenovo isn’t ALSO one when they make a non-Intel high end Thinkpad,Negative
AMD,">FSR 4 can be enabled for most games that support FSR 3.1 with DirectX® 12.  Pretty happy with this. I just tested Titan Quest 2 and FSR4 seems to work, even though the game only supports FSR 3.1 atm",Positive
AMD,"[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-9-1.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-9-1.html)  # Highlights  * **New Game Support**    * Borderlands® 4    * Hell Is Us * **New Game Support for AMD FidelityFX™ Super Resolution 4 (FSR 4)**    * FSR 4 can be enabled for most games that support FSR 3.1 with DirectX® 12.       * Learn more about this update to FSR 4 in our blog [here](https://gpuopen.com/news/amd-fsr4-over-85-games/) * **Fixed Issues and Improvements**    * Corruption may appear while playing Mafia: The Old Country on Radeon™ RX 6600 series graphics products.     * Intermittent application crash or [amd.comOpen](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-9-1)driver timeout may be observed while playing WUCHANG: Fallen Feathers with FSR 4 enabled on Radeon™ RX 9000 series graphics products.      * Playstation® VR controllers are not detected while playing SteamVR™.     * Intermittent application crash or driver timeout may be observed while playing Monster Hunter Wilds with Ray Tracing enabled and recording with AMD Software on Radeon™ RX 7600, 7700, and 7800 series graphics products.  # Known Issues  * Intermittent application crash may be observed while playing The Last of Us Part II on Radeon™ RX 7900 series graphics products.  * Intermittent application crash may be observed while playing Call of Duty®: Black Ops 6 on Radeon™ RX 9000 series graphics products.  * Intermittent application crash may be observed while playing NBA 2K25 in MyCareer mode on Radeon™ RX 9070 series graphics products. AMD is actively working on a resolution to be released as soon as possible.  * Intermittent application crash may be observed while playing FBC: Firebreak on some AMD Ryzen™ processors such as the Ryzen™ AI 300 series and the Ryzen™ 7000 series.  * Stutter may be observed while playing games with some VR headsets at 80Hz or 90Hz refresh rate on some AMD Radeon™ Graphics Products such as the Radeon™ RX 7000 series. Users experiencing this issue are recommended to change the refresh rate as a temporary workaround.  * Corruption (missing scan travel lines) may be observed while playing GTFO™ on Radeon™ RX 7000 series graphics products.  * Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled.    # Package Contents  * AMD Software: Adrenalin Edition 25.9.1 Driver Version [25.10.25.10](http://25.10.25.10) for Windows® 10 and Windows® 11 (Windows Driver Store Version 32.0.21025.10016). * Ryzen™ AI NPU MCDM Driver version 32.00.0203.297 (Date: 2025-08-25)",Neutral
AMD,you're free to roast me for linking 24.9.1 under this comment tree,Negative
AMD,Working link:   https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-9-1.html,Neutral
AMD,"Crazy that the VR issue is still a thing, it's been like 6 months at this point",Negative
AMD,Do you still need to be on Rdna 4 to use fsr 4?,Neutral
AMD,Getting rid of the whitelist this patch? Good news!,Positive
AMD,"So FSR4 is now what, driver enabled for most fsr 3.1 games??",Neutral
AMD,Seems like stutters on Warzone for 7000 series is just a lost cause now,Negative
AMD,Just installed good so far.,Positive
AMD,Come on AMD get it together with the vr situation and while your at it get compatible with the oasis vr software,Neutral
AMD,2k25 Makes the list days after 26 comes out 🥀,Neutral
AMD,Anyone interested volunteer to create full list with FSR 4 titles (driver - compatible)?,Neutral
AMD,What’s the deal with fsr4 on vulkan been 6 months now,Negative
AMD,u/AMD_Vik is there any chance of porting Adrenalin over to Steam OS and Linux Mint at some point in the future or is it unlikely to ever happen? Cheers!,Neutral
AMD,Does it show up in the adrenaline software? Or just in thier website?,Neutral
AMD,Is this the update that enables fsr 4 in cyberpunk 2077 or is that still coming in a future driver update?,Neutral
AMD,Has anyody experienced crash to desktop in BF2042 during the kill screen? Is that fixed in this update? I'm still on 25.3.1 because of that.,Negative
AMD,"My first/last Radeon was a AGP 9800 pro way back in 2004 world. I just retired my evga 1080ti hybrid last month with hellhound 9070xt.  I've done the nvidia driver update/install dance for 20+ years, but what is the best way for Radeons?",Neutral
AMD,"Verdnask sutters still not fixed after how long? Its not even mentioned in the known issues anymore, either cod has figured out a fix and will deploy it soon or they think they fixed it with the last update which is not the case",Negative
AMD,Just tried this in Cronos New Dawn and it says ‘Inactive’ despite being toggled on and FSR 3.1 being enabled in game.,Neutral
AMD,Is this using 4.0.2?,Neutral
AMD,"6 month and one driver not working properly.Only problem with any version driver.25.9.1 FSR4 logo show  and after 5 seconds hide, and fans when is full speed stoped for 2 seconds and back on full speed again (9700XT), and droped  watts from 363w down to 143 watts and lose FPS.I play Roadcraft latest version",Negative
AMD,"Unfortunately, I can't update higher than 25.4.1 as starting from 25.6.1 Zero RPM option turns on (despite originally it was turned off) and can't be turned off is it auto turns on every time I try disabling it. Only PC restart helps. Also 25.4.1 has some problems - sometimes CPU metrics doesn't work showing zeros. I had to come to terms with this problems.",Negative
AMD,"Mine auto-updated to 25.9.1 but it says the date is from 25/08/2025, is this the correct version of it?",Neutral
AMD,"Awesome. Can we get rid of the awefull AMD Install Manager though? I don't need that bloatware on my PC and for some stupid reason it's still being installed despite unchecking automatic updates and whatnot. Give me the choice here, AMD!",Negative
AMD,"Hopefully, this'll fix the issues I've had with my GPU via driver crashouts  Otherwise, I'm just gonna keep it manually clocked to 2600",Neutral
AMD,"I've been having clock speed related driver timeout crashes for weeks, since 25.6.1 or so on my RX 6800, and this driver has fixed it! Can return to stock clocks and get 30% of my performance back.",Neutral
AMD,"Is hardware acceleration still broken? Has anyone tried it? I mean the pink artifacts when hardware acceleration is on. I’m stuck on version 25.4.1 because of that.     update : Never mind, I already did a clean install of the new drivers and the issue still exists. Going back to 25.4.1 again.",Negative
AMD,u/AMD_Vik When is the VR 80/90Hz driver bug going to be fixed? I bought this card for my Valve Index on SteamVR as VR is my main form of gaming.  A card that can't game at lower refresh rates meaning some games are less playable. In addition Beat Saber has broken shaders on the walls on recent drivers after 24.12.1.  As a result I'm still stuck on a driver from last year and and we are 9 months into this year.,Negative
AMD,can rdna 3 have fsr4 now?,Neutral
AMD,"Still no fix for gtfo scan travel lines. Come on guys, its been 5 months since you broke it  Ps. Can you make it so that the install manager is OPTIONAL. Its annoying to have to go uninstall it every time",Negative
AMD,"Not sure if it's an issue with the game itself or an AMD driver issue, but No Man's Sky crashes every time I spend too much time in the pause menu, specifically in the log section.  Edit: Should also add, this is pre this update",Negative
AMD,Monthly update… Excellent AMD!,Positive
AMD,i haven't update windows since ssd issues is that safe to install that new driver without windows update ?,Negative
AMD,"Anyone still getting crashes when using Microsoft Media Player(Built in Windows Media Player), when watching a clip for a long duration or switching to another clip or clicking on timestamp within that clip",Negative
AMD,"Oooooo, FSR 4 on AC Shadows about a week before the new expansion drops!  Niccceeeee!  Can't wait to run a benchmark when I get off work to see the improvements.  Already runs pretty good on a 9070XT, but still don't quite get 60 FPS on Ultra settings in 4K, and whatever the setting is below Native on FSR 3.1 (Ultra, Extra Ultra, Super Ultra?! lol) without FG.  I hope this pushes me at a stable 60 FPS, or FG now doesn't have ghosting on points of interest on your HUD anymore.",Positive
AMD,"Nice, took Friday off to play Borderlands 4.",Positive
AMD,Fix Beyond All Reason crashes on 7000 cards please!,Negative
AMD,How about adrenaline opening up every time i right click on desktop,Neutral
AMD,Is there a new Vulcan version that isn't broken on this driver?,Neutral
AMD,"Last driver update had me crashing constantly, hopefully this one is better!  6750xt  Edit: Played Borderlands 1 for about an hour and was able to close it without issue so that’s good!  Edit 2: just hopped on Borderlands again, will also play some Overwatch to continue testing. Will report back soon!  Edit 3: Played Borderlands GOTY Enhanced Edition for about a half hour, no issues! Playing some Overwatch right now, no issues so far. Also updated Windows as well so idk if that's gonna help or hurt. Will continue to report back!",Positive
AMD,"Playing Cronos New Dawn on Epic settings, FSR 3.1 in Quality Mode in game with Super Resolution enabled in Adrenalin; HDR mode with 60 fps locked with no issues and no stutters. Beautiful game and 9070 XT sings. Average power usage at around 85%.",Positive
AMD,New world doesn’t seem to work with the update to try and force fsr4,Negative
AMD,All I want is a fix for Call of Duty Black Ops Cold War… Every driver version since 24 just has crashes all the time.,Negative
AMD,"Hello everyone Recently, we have AMD install manager which is just horrible Whereas before we could download directly from the drivers by refreshing if a new update had appeared Am I the idiot or is install manager completely useless and unusable? I have to download from the site and reinstall manually every time now   THANKS",Negative
AMD,I'm just kind of amazed they've been releasing updates for my RX 570 again,Neutral
AMD,Any plan of fixing the issue with UE4 ray tracing? No updates for that YET!!,Negative
AMD,i have 25.9. installed and auto updater pushed 25.8. onto me?   What is going on,Negative
AMD,"Every time I wait for an update for VALORANT with shader cache, I say hello to everyone. And for AMD it is special. For years, to fight this problem, you need to be able to do it.",Positive
AMD,Any plan to add fsr 4 support for vulkan titles like enshrouded?,Neutral
AMD,Anybody else's Youtube video buttons/icons got bigger after this update?,Neutral
AMD,did this by any chance fix wow dx12 driver crashing?,Neutral
AMD,is it safe to install this update now! no issues? i'm still on 25.5.1 -- RX6800 btw,Positive
AMD,Linky no worky.,Neutral
AMD,"Stalker 2 not supported, just fsr 3.1, or im missing smth?",Negative
AMD,Has anyone with these previous issues:  https://www.reddit.com/r/AMDHelp/comments/1khzck4/version_2551_update_rx6800_crashes_often/   https://www.reddit.com/r/Amd/comments/1k60kzh/amd_ryzen_chipset_driver_release_notes_70409545/   https://www.reddit.com/r/AMDHelp/comments/1k2a66p/blue_screen_with_the_new_2531/   Tried this driver yet? Especially RX 6700XT? The last driver I am able to use without issue is 24.12.1,Neutral
AMD,"I had a problem since 25.8.1, where if I play almost any game and then opened a friend stream on Discord, it locks up the whole PC.  Happened almost inmediately every single time with 25.8.1 and the Battlefield 6 Open Beta.  This is still happening for me with this driver. Rolled back to 25.4.1. RX 6700XT.",Negative
AMD,"7900 XTX with my usual report, as much as possible:  1. On 25.8.1, Cyberpunk and Alan Wake 2 would crash with RT and PT. I no longer have them installed to see if that's still the case. It probably is :(  2. Single monitor: VRAM clocks go to idle, dual monitor with high refresh rate still go to max VRAM. Locking both to 60 Hz brings VRAM back to idle.  3. Forcing Aniso: probably still iffy in DX 10-11-12 and nonexistent in Vulkan. DX 7-8-9 works, OpenGL works.  4. No issues with video hardware acceleration or MPO  5. AMD really, REALLY needs to clean up technology supported games. What games support BOOST? What is happening to the technology?  6. Anti-Lag 2 support is expanding, albeit at a slow rate. This needs encouraged somehow  7. FSR4 for RDNA3 is still officially needed. Unofficially, you know, Linux and also [redacted].  8. FSR4 for Vulkan is still M.I.A.  9. No RIS2 for anything other than RDNA4?  10. We need some more magical driver boosts that help RT / PT. Not realistically possible until DXR 1.2 support and games.  11. AMD should REALLY do an FSR 3.1 / 4 / AL2 plug-in for Unreal Engine 4.25 - 4.26 - 4.27. It might be a tad late and strange now, but games somehow are still releasing with them. Or getting updated: see Hogwarts Legacy.  12. Vulkan GPL might be broken for the past few drivers. DXVK isn't as smooth as DXVK GPLAsync. Needs investigated.",Negative
AMD,Hogwarts Legacy performs worse on my 9060xt 16GB after this driver update. I am getting higher frame times and overall less FPS than before,Negative
AMD,u/AMD_Vik with the consumer branch racing ahead to 25.9.1 and the v620 branch stuck on 25.1.1 are old datacenter gpu out of support?,Neutral
AMD,Assasin creed valhalla already crashed on newer drives been stable before,Negative
AMD,This verison of adrenalin stopped showing CPU temp with overlay(I have amd cpu). 25.8.1 did show cpu temp.  Actually cpu is gone from the performance - tuning section as a whole.,Negative
AMD,I went from 110fps with fsr4 on silent hill 2 to 140 fps fsr4 post driver,Neutral
AMD,Whoever released that driver deserved to be fired from their job.,Negative
AMD,Anybody getting real sick of amd updater ?,Negative
AMD,"No rush, u/AMD_Vik , but by any chance, will you guys ever bring back the System Latency readings that was apart of the beta Adrenalin driver for Anti-Lag 2?   I know you can technically use the Ctrl + Shift + L command whenever this feature is enabled in-game, but it would be nice to have it integrated into the Adrenalin overlay. (Along with it working across all games; similar to the Frame Generation Lag reading that came with AFMF 1 and beyond.)",Neutral
AMD,"Does anyone get this weird issue in Hell is Us: with these drivers the image quality has bumped a lot (probably FSR4 being triggered), but after a certain play time the game start to have consistent time paced frame deeps?! No other game has presented similar issue, even Satisfactory now running in FSR4 shows consistent frametime. Restarting the game fix the issue for about 30-40min. Wasn’t happening in previous version",Negative
AMD,"Do I need full adrenalin suite for this, or will a driver only install automatically upgrade fsr3.1 games to 4?",Neutral
AMD,"u/AMD_Vik I noticed that on the AMD website, Gray Zone Warfare is now listed as an FSR4 title with the new 25.9.1 drivers. When I went to test it, it shows in adrenalin that the FSR4 toggle is on (meaning it is reading the game can have it on) but no matter what I do in game, adrenalin still says ""inactive"". Is this a known issue? And if it is, what kind of issue is it? Is it like a UI bug in adrenalin, an issue with it not reading or overwriting the dll's correctly, I'm genuinely just curious because I'm interested in learning this stuff. Also, how long is it estimated to take to fix this issue? Thanks for any help you can provide!",Neutral
AMD,"I faced to crash issue like 25.8.1 on new 25.9.1, i have to roll back older version",Negative
AMD,"I can't change the memory optimizer setting on the performance tab for my laptop's integrated GPU, whenever I switch from productivity to gaming the device just freezes and I have to manually restart my laptop by pressing the power button, I have tried multiple versions of drivers but the problem persists, any information regarding this issue?       My device:   HP Victus 15 FB2777ax   AMD Radeon 760M Integrated GPU   Windows 11 23H2",Negative
AMD,Does it fix the DXGI_ERROR_DEVICE_HUNG crash? I'm experiencing it in every game since I installed driver 25.8.1 (RX 6700 XT),Negative
AMD,performs worse than the previous version. no questions,Negative
AMD,"Isn't it entertaining when the ""known issues"" list is longer than the ""what we fixed list""?   I'm so thrilled with my XFX Mercury MagAir 9070xt that I'm pondering trading it for a 5070ti.",Positive
AMD,World of Warcraft GPU timeouts still happening...,Neutral
AMD,Delta Force still stutters like crazy unless I disable SAM/re-sizable bar :(,Negative
AMD,How is adrenaline still this bad holy shit,Negative
AMD,In Dune awakening not work,Neutral
AMD,"It's been over 6 months and they still haven't fixed the ""general experience"" bug.",Negative
AMD,Will this let fsr4 work good in black ops yet? I get double frames when using fsr1 🤷,Neutral
AMD,"Estoy teniendo problemas con el nuevo driver, desde que lo instale mi Rx 7900XTX se corrompe el driver, apaga la pantalla y se craseha el vídeo, deja de reconocer las frecuencias y temo que esté driver haya dañado mi GPU, ya que se crashea, ya borre drivers, los reinstalé y solo me duró 8 horas antes de volver a corromperse, sinceramente estoy preocupado ya que la Rx 7900XTX no fue barata y en caso de haber sido dañada por el driver no tengo para comprar otra :/",Neutral
AMD,So will AMD finally pay attention to the OASIS VR driver for Windows Mixed Reality HMDs?,Neutral
AMD,"installed and space marine 2 is blurry textures, going to roll back. i switched from team green and now im not sure why",Negative
AMD,"I still have 24.10.1, i had no problems with this version so far. 0 crashes in 1 year of use, should i update or no?",Positive
AMD,"Still crashes with 2077 path tracing hihihi hehehe, good job!!!!!!!!!11!1!!",Positive
AMD,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1lomnxt/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
AMD,Took you long enough,Neutral
AMD,"That’s what the driver now enables, so all games with 3.1 will get FSR4",Neutral
AMD,That's pretty huge IMO. One of the biggest downsides to FSR4 that I've seen is the lack of support (heavily because devs need to build for it and nVidia has such a massive market share many only care about DLSS). Adding backwards compatibility with being able to run 4 on games with only 3.1 implemented expands the pool quite a bit.,Negative
AMD,"I was using Optiscaler with TQ2, guess I'll have to remove it and see how it's working now.",Neutral
AMD,"I've been trying int in Cronos and it says "" Inactive "" for some reason",Negative
AMD,Good thing I never bought PSVR as the controls wouldn't have worked until now.,Negative
AMD,"Glad the scan lines issue in GTFO is finally acknowledged in the release notes. Shame it's not fixed yet though, it's been months...",Negative
AMD,No fix for the terrible performance of RDNA 4 in CS2 with MSAA. Boo.,Negative
AMD,">* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled.    That's...not entirely true. I mean...it might be, but it doesn't just happen when loading saves. It crashes randomly, usually within around 5 minutes.",Negative
AMD,"it ok, we enjoyed a lil time travel",Positive
AMD,Hmm looks like this post links to a 404 without the .html in the URL,Negative
AMD,Yeah. The whip will crack.,Neutral
AMD,It's the thought that counts,Positive
AMD,Since I rolledback to 24.5.1 Im super happy and amazing performance than 24.12.1. 6750Xt OC,Positive
AMD,I just switched to Linux because of this and that. Wireless Quest 3 is nice.,Positive
AMD,"Haven't been able to use anything but 24.12.1 due to the drivers crashing(black screen) the whole system after about 3-4 hours of continuous use, and every driver since has had the same issue. Rare stutters are fine, maybe this one is different?",Negative
AMD,ofc. fsr 4 (lite) \*might\* (big might) get shown at the fsr redstone event thing.,Neutral
AMD,I pretty sure you have to,Neutral
AMD,FSR 4 will never come out on older AMD cards,Neutral
AMD,Yes,Positive
AMD,"Literally says that, it’s just a swap of DLLs",Neutral
AMD,"I only have Death Stranding installed that has FSR 3.1, and that now has a FSR toggle  Edit: Death Stranding does not support FSR 3.1,the toggle must be showing because I use FSR on Optiscaler",Neutral
AMD,We're working with Activision to resolve this. The last release (25.8.1) included a driver-side change to try and mitigate the issue.  I've asked to have the driver release notes updated to reflect this situation once again.,Neutral
AMD,Yup. Still not fixed but not listed on the release notes.,Negative
AMD,I'm getting stutters in HD2 with my 7900XTX,Negative
AMD,It’s already fixed,Neutral
AMD,https://www.pcgamingwiki.com/wiki/List_of_games_that_support_high-fidelity_upscaling  This website here has a pretty good list just search FSR 3.1 and you should have most of the games,Positive
AMD,https://www.amd.com/en/products/graphics/technologies/fidelityfx/supported-games.html Is there a current list on this page?,Neutral
AMD,there is a thread in r/radeon  181 games so far,Neutral
AMD,"It's no easy task - much of adrenalin's functionality on windows is contingent on the win-gfx kmd driver. amdgpu on linux distros provides a vastly different environment.  nevertheless, I appreciate you registering your interest. I would also like to see some form of advanced gfx control panel on linux desktops (IHV agnostic or otherwise).",Neutral
AMD,"Doesn't show for me yet, but i got it on the site",Neutral
AMD,That update already came. 25.8.1,Neutral
AMD,Think that was fixed last update!,Positive
AMD,"On the last driver, I could only play for 10 minutes, but after the update to 25.9.1, I played for 1h30m and had no issues.",Positive
AMD,this might be a bug i test also on forbideen west its fsr 3.1 also say inactive on adrenaline xd,Negative
AMD,yes,Positive
AMD,You need to run a Dism health check/repair. Had this on a few machines and that fixed zero rpm and metrics overlay issues. Only one machine I had to clean reinstall windows and i think it's because it had armory crate installed which buggered everything.,Negative
AMD,yes,Positive
AMD,Did you know you can just uninstall it and it just reverts to the style of checking for updates.,Neutral
AMD,Can you lmk how it goes? I haven’t had a stable driver since 24.12.1,Negative
AMD,"Artifacts in what? System, browser, games? What GPU? What system?",Neutral
AMD,It works fine for me,Positive
AMD,Thanks for testing it. Was wondering the same thing.,Positive
AMD,"Have you tried disabling MPO ? Nvidia shared registry files that still works today, it solved all my issues with those chromium weird behaviors (screen tearing, freeze, etc). The post is still up if you want to try it : [https://nvidia.custhelp.com/app/answers/detail/a\_id/5157/\~/after-updating-to-nvidia-game-ready-driver-461.09-or-newer%2C-some-desktop-apps](https://nvidia.custhelp.com/app/answers/detail/a_id/5157/~/after-updating-to-nvidia-game-ready-driver-461.09-or-newer%2C-some-desktop-apps)",Positive
AMD,"I'm sorry for the wait. The fix for this is an extremely risky change, and appears to have been pushed out for proper test coverage.",Negative
AMD,"far as I'm aware, the workaround for this issue is a risky change, and I think release management want to thoroughly test this before this gets into a public driver.",Negative
AMD,I think that's the game. That last update did some weird things.,Negative
AMD,NMS is broken.  It used to run silky smooth at between 60 and 90 watts for me before the last game update. In hundreds of hours of gameplay I don't remember it ever dipping below 60fps. Now it uses 75 to 160 watts and there are drops to 20fps all over the place.  This newest driver has made no changes to that behaviour.,Negative
AMD,"I mean, it should be roughly the same performance, no??  FSR 4 simply looks better at much the same performance cost, unless I'm mistaken.",Neutral
AMD,"I don't know how people still do this. To me it seems so much more fun to go to work and rush back home to play the game, rather than sit at home all day with just the game. Maybe im just getting old",Positive
AMD,"Did you get the ""DXGI_ERROR_DEVICE_REMOVED with Reason: DXGI_ERROR_DEVICE_HUNG"" error? I had to rollback to 24.12.1",Negative
AMD,"I'm in the same boat. I have a 6900 XT and also set it to ""download"" or ""update drivers as soon as possible"" and it doesn't do it for some reason. If I go to AMD's page, I can see the new drivers available for download. I guess it's a matter of giving AMD feedback that it's not working how they thought it would work.",Negative
AMD,turned on computer and left it on for a bit and when I came back and started adrenaline it was already on 25.9.1 so auto update does work for some/most? people,Neutral
AMD,"I dont think it was the driver update, noticed they got bigger a few days ago for me.",Neutral
AMD,Same if you get an answer lmk please,Neutral
AMD,"You have to play with the custom resolutions on both monitors to address the vram spike. Experiment with your second monitor at 60 and set a custom resolution in adrenaline where you only change the refresh rate. If your main monitor is 170 hz for example. Set custom resolutions in incriminates of 1 from 170 to 160 with the 2nd monitor at 60hz. Keep changing your custom resolution going down from your max refresh rate until the vram stops spiking.  This is been a problem with AMD since I've been using them. I have yet to find a computer where I couldn't fix this problem on.  For what it's worth, Nvidia has the same multi monitor issues with idle power draw. The vram spike on AMD is what causes the idle draw to go up. I can confirm that my 9070xt has lower idle temps on multi monitor than my 5080.",Neutral
AMD,ddu uninstall reinstall fix?,Neutral
AMD,Reinstall your drivers this has been a bug for almost a year now I think,Negative
AMD,Care to elaborate?,Neutral
AMD,That error usually means unstable OC/UV - with every driver update your OC/UV profile may become unstable.,Negative
AMD,What games? I also got 6700 XT never seen this. I mostly play Battlefield 5 and PUBG though.,Neutral
AMD,"Thats interesting because I have been having the same issue both in WoW, as well as other games like PoE2.  I use a 7800x3d as well as a 7900xtx and narrowed it down from everything ( I just bought a new PSU in the case that it was one of the issues) to what I'm believing is either a horrible driver issue, or Windows.  I have tried everything under the sun to fix it: undervolting and lowering my GPU's max frequency, Disabling HAGS, MPO, TDRdelay regedits, and I know for certain it isn't any hardware fault on the GPU, CPU, or RAM from testing with OCCT and Memtest86 because they held completely stable under extreme loads (Memtest ran for a full day).",Neutral
AMD,"Have you tried using dx11? I know it blows to downgrade but I haven't had any issues since switching to it. Friend who uses Nvidia has the same issue and hasn't had any since the switch either, so I'm thinking this might be a wow issue with their dx12 implementation",Neutral
AMD,I use 25.4.1 and is very stable for Delta Force,Positive
AMD,[see here](https://old.reddit.com/r/Amd/comments/1n40oz8/amd_being_difficult_with_wmr_3rd_party_driver/nbw7t4t/?context=3),Neutral
AMD,"Buddy had the same problem with a slow SSD. He used the 4K texture pack and his SSD could not load the textures fast enough into VRAM. It worked better, but still not great without the 4K textures. After upgrading to a faster NVMe SSD the problem went away. It happened in a few games, but in SM2 it was the worst.",Negative
AMD,"Bro, I have the same 24.10))) And in October it will be a year))) I also didn't have any problems with it for a year)",Positive
AMD,YO do u also get pink shadows when pt is on? I found a fix for the crashing on pt but I still get pink hue during the day I just can’t play it with low fps and with frame gen it’s still has so much stuttering. Thinking of trading my 9070xt for a 5070ti smh,Negative
AMD,If u can’t start the game after the crashes from turning on fsr+pt first go to the folder where the shaders are located and delete all of it then run it again (it won’t start) after verify the game files wich will put u in window borderless and put it back to fullscreen and u should be good . Just don’t turn on fsr with pt it will crash it again.,Neutral
AMD,All games supporting fsr3.1 and dx12,Neutral
AMD,I read it as my fsr3 gpu enables fsr4 as well. Was happy for a bit and i read your comment.,Positive
AMD,I thought this was the case from the get go... was it *not?*,Neutral
AMD,On 6800?,Neutral
AMD,devs support fsr because of consoles. many many games have fsr 2x or 3x. fsr4 had to restart from scratch for likely the final time and therefore the game library is going to be smaller.,Neutral
AMD,"If the game has FSR 3.1,it should show a FSR toggle in Adrenaline",Neutral
AMD,this might be a bug i test also on forbideen west its fsr 3.1 also say inactive on adrenaline xd,Negative
AMD,I spent my whole weekend trying to work out why my controllers weren't working!,Negative
AMD,Weird. I got mine yesterday and they are working just fine and I haven't updated the driver yet.,Positive
AMD,They've always worked fine for me. 9070 XT.,Positive
AMD,"CS2 performance has improved A LOT since RDNA 4 came out.   Like the 9070 was getting beaten by the 5060 Ti when the 5060 Ti launched. Now the 9070 is trading blows with the 5070 Ti, while the 9070 XT is beating the 5080 from what I can tell.  Anecdotally I was getting around 300 FPS on maps like Mirage on high settings 1440p when I got my 9070, while I get more like 380-450 these days, with my current performance on Train being about where my performance on Mirage was when I got it.",Positive
AMD,"Hey u/AMD_Vik  , sorry to bother you , but can amd please implement a background fps limit  ( Limit tabbed out game to fps X ) which nvidia got already quite a few years ? i need rivatuner on amd for it :/  i suggested this feature to amd now for a few years ( and i think even another amd employee on reddit ) a few times and nothing :/  ( for anyone searching... Add this in the global profile for Rivatuner till amd adds it )  `[Framerate]`  `Limit=XXX ( your max fps limit )`  `IdleLimitTime=50000`  50000 = 20 fps if you minimise games.  ( Also mentioning u/AMD_RetroB  )  **Not related for you Vik / RetroB** but people will suggest it :  No chill doesnt solve that , if you set 10 fps as min , and move your mouse on desktop while tabbed out the fps of the tabbed out game actually increases and 10 fps is anyway a terrible min fps with how chill works.",Negative
AMD,Vik. Do you think this small fix can be introduced on next drivers?   Using a 780m and a discrete GPU. On older drivers 780m gets recognized by Windows like power saving card and discrete like high performance one. But on newer drivers from some months ago power saving and high performance only show the discrete one. I have to specifically select the 780m if I want to use it.   I know this is just a registry change the driver installation does. But there is no manual info on how to fix it or change it. Thanks.,Neutral
AMD,"Hi Vik, are you guys aware that the last two releases brought back the issue of videos played on a second screen get all choppy if not forced into full screen?",Neutral
AMD,"thanks, posting here for posterity if users want the link if they don't see the existing comments with it: https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-9-1.html",Neutral
AMD,"Any recent guide? Last time i checked ALVR was recommended and getting that setup was a big pain, so i just didn't bother at some point.",Negative
AMD,"This is what I figured, it makes use of newer hardware advancements",Neutral
AMD,Yeah ... about that ... 👀,Neutral
AMD,"Strictly speaking it doesn't actually swap the DLLs, it replaces it in memory at runtime.",Neutral
AMD,I'm having driver timeouts and crashes with UE5 games using 25.8.1 that I didn't when using 25.6.3.    Mainly affected 2 games: Nobody Wants To Die and Karma: The Dark World with RX 7800 XT.   I have sent at least 3 bug reports to AMD in the month of August. I hope your team is on it.,Negative
AMD,"I mean this as respectfully as possible, but why is this taking 6 months to fix? Honestly if I had known I would have just bought a 5080 in April.",Negative
AMD,You've been working on this for 5 months now lmao,Neutral
AMD,You can play HD2?!     I literally crash as soon as I hit the planet's surface. AH needs to get their heads out of their asses and release those optimization patches ASAP!     I uninstalled it and won't be playing until then. Tired of constant crashing...,Negative
AMD,"no, it isn't.",Neutral
AMD,"Some kind, yeah. Doesn't have to even be called Adrenalin, but just somewhere to cap/uncap framerates, turn cards features on and off and manage driver versions per game if needed.  One more question. Would you get a 9070 xt or a 7900 xtx for 4k gaming, or is there a cheaper card that is capable of handling most current titles on 4k at 60Hz?  I was looking at the clarity difference between FSR3 on the 7900 xtx and FSR4 on the 9070 xt earlier and FSR4 is much cleaner and clearer, but the 7900 xtx is still considered the flagship and has apparently has more punch despite being three years older.",Neutral
AMD,"Would it be possible to maintain the CoreCtrl project and work from there? It would be great having a Linux version of adrenalin, even if it doesn't have all the features. CoreCtrl has been a great application that has now entered maintenance mode.",Positive
AMD,Oh word? I missed that then. Thanks for the heads up! Been waiting for a diver update to play and clearing out backlog first. Sounds like a week or 2 from now is a go for a new playthrough!,Positive
AMD,They had an issue like that?   Played 2042 with every iteration and never encountered a similar problem.,Neutral
AMD,"Thank you, I'll try it.  At what moment I should perform this DISM check/repair? Should I delete my current Adrenaline version before it or I can do it while it's still installed? And after that upgrade it to a newer version? And what do you mean by ""armory crate""?",Neutral
AMD,"Yes, which is what i'm doing every time. Still a bit annoying having to do that. Thanks for pointing it out though.",Negative
AMD,"Gotcha, rn I just kind of put it back to 2600, but I'll try and test it if I can/remember too",Neutral
AMD,"rx 7900 xt  windows 11  Artifacts appear in Discord and Chromium-based browsers when using hardware acceleration. A lot of people have this issue on almost every newer driver, except 25.4.1.    [https://www.reddit.com/r/AMDHelp/comments/1kiqjtr/strange\_pink\_and\_purple\_visual\_glitches/](https://www.reddit.com/r/AMDHelp/comments/1kiqjtr/strange_pink_and_purple_visual_glitches/)",Neutral
AMD,sorry what GPU?did you had that issue before?,Negative
AMD,"If I'm understanding this correctly, there is an existing and finished patch in the pipeline that hasn't been released as it needs major testing due to how extensive the changes are.  Would it be possible to get a pre-release version of this build? Acknowledging that it may cause bugs elsewhere.  If not, It's nice to know that the patch as least exists and that this isn't some unfix-able silicon level hardware bug. But please do test and release it soon.  Thanks for replying.",Neutral
AMD,Thanks for the response. I also just looked back at the known issues and noticed the GTFO issue only mentions RX 7000 when it also includes RX 9000   Wasnt sure if you guys knew about that or not,Neutral
AMD,"What GPU are you using? I'm on a Pulse 9070 XT and haven't had many issue with performance in general. Mostly high frames from what I can tell after turning off overlays (they weren't hurting performance, but I was suspicious these were the culprit to the crashes, but apparently they aren't for me).",Neutral
AMD,"Marvel Rivals did actually run better for me from 3.1 or 4. I was able to bump up to the next quality level and still get 60 FPS with no dips, where before the same quality level jumped between 50-60 FPS. It's the only game I've personally tested going from FSR 3.1 to 4, but imagine it's going to vary game to game.",Positive
AMD,I have a lot of PTO and nothing to use it on. *shrug*,Negative
AMD,"I did not. But tbf I didn’t even get an error message, my whole PC would lock up!",Negative
AMD,"Ah cool, thanks for letting me know. It literally happened after installing the driver so it was a funny coincidence.",Positive
AMD,It was with ddu uninstall i just went back to 25.8.1 been stabel since release btw i have rx9070xt,Neutral
AMD,"I have no OC/UV. I even tried reinstalling 25.8.1 with DDU, but nothing changed. I solved rolling back to 24.12.1",Negative
AMD,"Stellar Blade, Street Fighter 6, The Alters and REMATCH. I rolled back to 24.12.1 and it seems the error disappeared",Neutral
AMD,I was able to completely stop my timeouts on a 9800X3D/7800XT system by disabling Anti-Lag in wow AND in Adrenalin.,Neutral
AMD,Rollback to drivers 24.12.1,Neutral
AMD,Buddy of mine had WoW crashes in both Retail and Classic for the longest. He has the same setup 7800x3D and a 7900xtx. Eventually one day we updated his motherboard bios and forgot to re-enable XMP (his memory doesn't have EXPO) and his crashes went away. Eventually we RE-enabled his XMP and his crashes came back. Anyways he now plays at stock jdec speeds with his Corsair RAM and everything has been perfect. For the record he just has two 16 Gig sticks of Corsair Vengence 6000 CL36.,Neutral
AMD,MEMTEST86 is no longer the way to test RAM stability. Look at overclocking forums. Use TM5 with ANTA777 extreme profile or karhus which is a paid one,Negative
AMD,I don't want to roll back the drivers because the new ones are needed for BF6 and because of driver compatibility for other new games like Cronos: The New Dawn.  I know it's also good on 24.8.1.,Positive
AMD,"Thanks for the following up, reddit is a cesspool that makes difficult to search for specific topics, i read that thread once and i missed you response there, buried beneath all the other comments.",Negative
AMD,i have the game installed on nvme and i feel like this wasn’t a problem before,Neutral
AMD,"Yea that’s what also the driver says, when hovering over the FSR4 option",Neutral
AMD,"Still hoping one they we get it on RDNA3 in some form, even downgraded version",Neutral
AMD,"Until now it was only possible to tweak supported games, now all games are supported which have 3.1. the toggle is there in any game but only enables when it finds FSR 3.1",Neutral
AMD,"No, RDNA4 only.",Neutral
AMD,It will come to rdna3 but don't see them making it work for rdna2 as it's Ai are next to 0,Neutral
AMD,AMD has never advertised RDNA2 support and it's not likely to come. RDNA 3 having more AI cores on some models may get it eventually. It was a fundamental change in upscaling approach with 4.,Neutral
AMD,I did too and gave up.,Negative
AMD,"Cs2 depends on the cpu you have, even with a 3060 + ryzen r9800x3d",Neutral
AMD,"Not with MSAA on, I suppose.",Neutral
AMD,"funny you mention it, we've been discussing this functionality recently.  Totally agree that this should be implemented",Positive
AMD,Cant find `IdleLimitTime=50000` anywhere in riva,Negative
AMD,"huh, that's odd. can you give us a list of drivers tested?",Neutral
AMD,I mean this https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Fi-have-multiple-gpus-and-graphics-preference-has-not-shown-v0-uppyccerdb9e1.png%3Fwidth%3D404%26format%3Dpng%26auto%3Dwebp%26s%3Db94d216495c9de8e758317b6fc2cd4973e24dd99,Neutral
AMD,Specifically on chrome? I'm not aware of this regressing,Neutral
AMD,"I had issues with ALVR on Bazzite and Fedora But I got it working on Nobara. The WiVRn is actually really easy to setup otherwise, it just doesn't have the SteamVR overlay.",Neutral
AMD,"Yea true, some people just don’t understand such things",Negative
AMD,That's odd. I haven't seen anything on internal test coverage that'd suggest an uptick on failures with UE5 apps on NV3X. I'll see if we can acquire these games and take a look,Negative
AMD,"if we could have fixed it on the driver side, we would have",Neutral
AMD,not sure what more to add - it's not something that was caused by (or can be fixed by) the gfx driver software.,Negative
AMD,That's a bummer,Negative
AMD,"I think the best route for advanced gfx control in an environment like the linux desktop would be something that's IHV agnostic (equally usable by other gfx vendors like intel, nvidia, qualcomm and so on); the general aim is to have controls implemented at the DE / WM level.  Perf at 2160p may vary between those two choices by the title, also bear in mind that FSR 4 isn't presently available to RDNA4 outside of Windows (outside of the use of mods, which I've no experience with anyway).",Neutral
AMD,"Corectrl is indeed neat, I absolutely loved that it retained the older Crimson UI, though if we *were* to maintain a first party GUI for Linux (and this isn't commitment to such work by any means), it'd likely be of our own design.  I've been using [LACT](https://flathub.org/en-GB/apps/io.github.ilya_zlobintsev.LACT) for tuning control on Fedora 42, it works very well in my experience.  For DVR, I've been using [GPU Screen Recorder](https://flathub.org/apps/com.dec05eba.gpu_screen_recorder) - this works incredibly well, supports audio source config, HDR (even in an environment which doesn't comprehensively support HDR), global hotkeys (even within Wayland), and much more.  We've been working towards other display improvements for the linux desktop; the ['Display Next Hackfest'](https://blog.sebastianwick.net/posts/display-next-hackfest-2025/) was hosted at one of our Markham sites a few months ago; hopefully this will make colour space control more approachable for DE's in the near future.",Positive
AMD,Maybe the issue was with the 6000 cards. Mine is a 6900xt. This driver seems ok so far though.,Neutral
AMD,I did it while old Adrenaline 25.4.1 was installed. I'll check if DISM helped for a couple of days...,Neutral
AMD,"Armory crate is an ASUS ""software"" but it's really troublesome for many users.",Negative
AMD,Appreciate you man take your time with it,Positive
AMD,"It sounds like chromium issue. On Nvidia it was checkerboard, on AMD... Purple things. Try this: https://www.reddit.com/r/AMDHelp/comments/1kiqjtr/comment/n9le1y4/?utm_source=share&utm_medium=mweb3x&utm_name=mweb3xcss&utm_term=1&utm_content=share_button",Negative
AMD,Universe is telling you to switch to Firefox.,Neutral
AMD,Do you have a CPU with an iGPU?,Neutral
AMD,"This will be aligned to an upcoming major version of the driver, I don't think a pre-release in this situation will be feasible. please bear with us",Negative
AMD,"No prob. the workaround for the affected domain (d3d11 on PAL user mode driver) should broadly apply to both generations, but I'll clarify internally.  To actually address the root cause of this issue, we'll need to work with the developer of GTFO.",Neutral
AMD,It's an RX7600.   If I had to guess I'd say it's related to the memory management they did recently. It used to be the case that 8GB cards using high settings and FSR would run into a VRAM limit and get a Vulkan error. Then they fixed that. But now the same error is re-occurring for me.,Neutral
AMD,"Yeah apparently its more noticeable in theater mode. They rolled it out around the start of September and it's called ""big mode"" or something. If it bothers you and you use uBlock Origin I seen there are filters you can add to disable it, though haven't tried any yet.",Neutral
AMD,Are you using MSI center of any similar software?,Neutral
AMD,It's been almost a day since this comment. How are things now? Did the crash actually stop with 24.12.1?,Neutral
AMD,"I'll give this a try, thanks!",Positive
AMD,Radeon driver is actually low-key MemTest. This is not common knowledge.,Neutral
AMD,They need to make it work with Vulkan now,Neutral
AMD,"It's coming for RDNA 3 GPUs, leaks said they were focused on the SDK and now working on support on all RDNA 3+ GPUs (7000, strix halo, PS5 Pro)",Positive
AMD,"Yes it does to some extent, but the GPU can be influential even if the game is CPU limited due to GPU driver overhead. AMD GPUs have much lower overhead than Nvidia GPUs, meaning they tend to get better performance in CPU limited scenarios.  I'm not CPU limited on most maps though, except maybe Mirage.  Personally I have a pretty balanced system with a 9070 and 5700x3d, but I tend to play the game on settings taxing enough that I end up GPU limited (1440p, high almost everything and 4x MSAA). Because of the settings I use, I can clearly tell that performance has improved, since I play it GPU limited.",Positive
AMD,"I play with MSAA 4x. I'm pretty sure most benchmarks I've seen do the same as well, since the consensus in pro circles is that MSAA 4x is the best AA setting in terms of performance cost and visual clarity.",Positive
AMD,"1440P 4xmsaa on a 9070xt here, runs fine",Positive
AMD,"What do you mean, i have 9060xt and playing with 4x msaa the performance is great, im cpu bottlenecked with 7500f",Positive
AMD,"Whenever this gets added, please let me know the address you'd like my firstborn to go to.",Neutral
AMD,"Just chipping in to add this feedback,   A friend and I were playing Mechwarrior 5 (clans and the original), and on the menu's the game would uncap it's frame rate sending her 7900 rocketing up in temps. It seems to be a bug with the engine used, so I don't know if anything can be done on a driver level but hopefully someone can suggest something.",Negative
AMD,"thanks ! it would be really awesome as nvidia got this feature now like... idk 10 or 15 years , could even have a nice name like amd energy saver or smth.",Positive
AMD,"You don't find it , you add it below the max fps entry.",Neutral
AMD,"Hi Vik. Thanks for your answer. 24.5.1 from last year does this correctly. I can even install it now and will be correct. So it's not new builds of Windows messing with that.   Later, Windows added an extra setting there to force a GPU, so it's a workaround. Not ideal but it's something.  About this year drivers I been using them from adrenaline June releases and problem is there on all of them. Didn't test previous ones because starting June the ogl blackscreen drama was fixed :D  Thanks.",Neutral
AMD,I'm using the newest version of Brave browser. So chromium based. Is there any way I can share data on this to help?,Neutral
AMD,"Nice, thanks for letting me know.  Interesting it works under Nobara (which is what i was running at the time) which in turn has Fedora at it's core :D",Positive
AMD,I’ve also had issues with ue5 games as well. Not sure if there’s some underlying issue with the engine and for it interacts with amd stuff or not,Negative
AMD,Related discussion:   https://www.reddit.com/r/DarkTide/comments/1mjl9gb/comment/nclmdjd/?context=3,Neutral
AMD,Ah. So activision is just being activision? I do find that plausible. Thanks for the information.,Positive
AMD,Will the 7000 Series and RDNA3 get anything in the future or did we essentially choose the worst time to buy into AMD?,Negative
AMD,A huge one at that!  I wait a year and a half for my ODST crossover and I don’t even get two weeks with it before I’m forced off the game!  It’s time for my yearly Borderlands 1/2 playthroughs anyway so no biggie!,Positive
AMD,"This method actually helped me in the browser, but the issue still exists in Discord and Steam.",Neutral
AMD,I have the same issue with Firefox.,Negative
AMD,what about  discord? steam? any other app that uses hardware acc?,Neutral
AMD,14700k,Neutral
AMD,"Yeah I've noticed it go up to 14 gigs of VRAM usage and that seems like a LOT for a game that came out in 2014, even with engine upgrades.",Neutral
AMD,Nope,Neutral
AMD,"Unfortunately I did not have time to play :/ Yesterday I just tested The Alters for about 40 minutes and I got no crashes, but sometimes Stellar Blades and SF6 crashed after 2/3 hours, so I cannot say the problem has been solved yet",Negative
AMD,If it doesn't work feel free to hit me up. I did a lot of work to fix my problem and discovered it's not the same cause for everyone,Negative
AMD,"i dont think they will , rdr2 i think is the biggest vulkan game and it has only fsr 2 , if they would add it which would cost them to implement it like 50k usd just for coding just for several games to get fsr 4 if they even have fsr 3.1 . not only it isnt needed right now for vulkan , there are alot of games which have vulkan has the ability to switch to dx 12",Negative
AMD,Interesting to read! Do you do it for the visual clarity mainly?,Positive
AMD,"keep yer chillens, dang it!",Positive
AMD,"Use rivatuner as FPS limit ( MAX ) , as both chill , and the FPS limit in the amd driver both can fail during load screens or boot ups of games.",Neutral
AMD,I think a GPU trace but let me check in with internal testing first.,Neutral
AMD,Might have been because of the extra stuff installed on Nobara. I had more problems getting permissions on Fedora in order to get it working then on Nobara.,Negative
AMD,"To their credit, it's a complex issue, I don't mean to trivialise this by any means.",Neutral
AMD,"Did anyone checked windows 10?   Edit: temporary you can disable hardware acceleration in steam and discord, but after digging I suggest to try different cable in different output on GPU/input in display. Are you in HDMI?",Neutral
AMD,"So yes, a UHD Graphics 770.   Are you sure these artifacting apps aren't using that instead of your GPU?   Whether you are cabled out of your GPU or mobo doesn't 100% dictate this.   If unsure, try disabling it in the BIOS/UEFI.",Neutral
AMD,"Yep, I'm running a 9070xt and after the latest update, VRAM quickly skyrockets to 14-15GB.  On ultra settings, the game will crash and resets my display driver within a few minutes.  The lower the settings, the longer it takes to happen, but the game eventually crashes.  More upsetting, I've had two hard crashes from the game that rebooted my entire sytem.  I dual-boot with linux for fun, and so far, NMS runs stable for me under linux.  VRAM still gets to 13-14GB, but I've been able to play for an hour-plus and close the game without any performance issues or crashing.  But yeah, there's some kind of memory leak or major underlying issue going on.  There are a lot of threads popping up on steam with the game crashing, giving a vulkan swap chain (out of memory) error, since the most recent updates.",Negative
AMD,Try disabling SAM in driver.,Neutral
AMD,"Got it. If it's not a problem could you please report back once you've got some time to play? Been having this issue too (different GPU though, RX 5700) and happened to find your comments just now so I'm now wondering whether I should downgrade to 24.12.1 too.",Neutral
AMD,"Doom Dark Age has Vulkan and FSR 3.1 too, so it‘s up to the devs",Neutral
AMD,Alternatively you can have my babys if its finally getting added ( iam a dude btw  ill find some babys to give away ),Neutral
AMD,Awesome. I really appreciate your checking in on it. I'm running a 6950XT.,Positive
AMD,Youre good. It’s just extremely annoying.,Negative
AMD,if im not mistaken yes some people had this issue with win 10 as well.  there is  no need to enable vulkan in brave://flags because i only set : Choose ANGLE graphics backend to OpenGL and i see no artifacts while browsing...also AMD\_matt from amd forums confirmed that they insvestigating  this issue   [https://pcforum.amd.com/s/feed/0D5KZ000011WpJJ0A0](https://pcforum.amd.com/s/feed/0D5KZ000011WpJJ0A0),Negative
AMD,well disabling igpu from bios deosn't help at all      thank you anways.,Negative
AMD,"I bought it last October. Back then the game would show about 7300MB VRAM used at startup and it would climb to 8100MB and then the game would became unstable and show Vulkan memory and swapchain errors.  Then a few months ago they announced some new memory management feature and the game would start with 7300MB used and never go above 7800MB, even after hours of running.  With the latest update the game starts with 8100MB used and never drops below that.  So I'm assuming the fps slowdowns I'm seeing are caused by having to swap things in and out of VRAM.",Neutral
AMD,Would like an answer if possible too please,Neutral
AMD,"awlrighty, let's try to capture an ETL with GPU tracing enabled.  The easiest way to get this done is via [UI for ETW](https://github.com/randomascii/UIforETW), this will automate the installation of the Windows ADK so there's no annoying set up process.  When you launch the app (~/etwpackage1.60/etwpackage/bin/UIforETW.exe), you'll see a checkbox for GPU tracing, please check that and keep it enabled.  With UI for ETW open, please reproduce the issue with video stuttering when not maximised on Brave, click on 'start tracing', let it capture about 5 seconds of stuttery video playback, then hit stop and save (alternatively you can use the keyboard shortcut Ctrl+Shift+R to start and stop).  You can navigate to the trace file from the GUI by right clicking and opening via explorer. These files can get large pretty quickly but compress well. Please compress this to .zip and DM me a link using your method of choice (I personally like to use https://send.vis.ee for ephemeral large file transfer).  hit me up if you need a hand with anything",Neutral
AMD,"That's good they are aware of it. Have no idea how it will end, but for checkerboard pattern there was needed not only GPU driver update, but also system and browser.   This is pretty strange to me, but I don't use HDMI connection - maybe that's the case? If there is better way to wait for the fix than changing those settings in every hardware accelerated app based on chromium, like trying display port, it's worth to try.",Neutral
AMD,"u/cabrerahector u/RusFoo I've played REMATCH for almost 3 hours. In the first 10 minutes the game froze and my PC rebooted 2 times (no DXGI errors) as soon as I scored a goal, but I searched online for a fix, capped FPS to 60 and then I was able to play without any crash (BTW I don't know if reboots and DXGI crashes are related). The I played The Alters for about 45 minutes and I got no errors",Negative
AMD,"Well, I’m using DisplayPort, so it has nothing to do with HDMI. Someone said it wasn’t an AMD problem but a Windows issue related to MPO. I tried that solution, but the issue still exists. :) It seems that fix was for a different visual glitch.",Neutral
AMD,"That's good news, thanks for the update. Might try downgrading as well sometime this weekend and see what happens. I'll share my results here too once I do. Thanks again!",Positive
AMD,Appreciate you brother thank you much for taking the time,Positive
AMD,"That's changes much. If it's present on HDMI and display port (one guy fixed this by changing cable however and installing different driver), with mpo and without, then it will be same as checkerboard issue. There was no other fix than changing this angle thing in browser and disabling hw acceleration else where. But what's totally boggling here is why some doesn't have it, and some does. If that could be config-settings related, its gonna be close to impossible to fix. Checkerboard was present in all machines equipped with Nvidia GPU - this was common, but here.... It may be some driver-firmware of other hardware compatibility that appeared with system update. It may be SAM, ULPS, simply everything.    And with rDNA 2 (6800XT) i had one issue that appeared with later drivers and was never fixed - it was ULPS, but appeared only with display port and only on windows. My display was waking up just to get sleep in regular time periods. To fix this I had to disable ""enableu ULPS"" in registry after every driver update. Every single time. Problem is... Issue was present after swapping to RTX 3080. And there was no ULPS in registry.   After weeks of searching in free time, I found out, this was question of... Compatibility of monitor(display) firmware-system-driver and there is nothing I can do. I got experience with this kind of issues on clients systems, where display firmware is the culprit (usually it's displays from 2017-2020) with no new drivers since 2019 - 2021. After swapping back to 6800XT, seeing this again (but be able to workaround it), I was sure it will happen with my next GPU - 9070XT. But for some unknown reason it's not happening (!?).   So it started on 6800XT, ~ 3 months after purchase, I had no previous GPU (GTX1080ti - sold it), but borrowed old GTX 1060 from my brother setup (pascal gen required firmware update for display port - I applied it) - same here. RTX 3080 - too, next 6800XT from different AiB - same. 9070XT - no issue. I just hope AMD is not renewing some certificates (just guessing) and this will happen again after some time with 9070XT.",Neutral
AMD,"7000 series could use some fine wine, not this time I guess, crazy considering the ""flagship"" by some measures is technically an older gen",Negative
AMD,Please AMD give me FSR4 on my 7900XT,Neutral
AMD,This is great I just tried fsr4 on stalker 1 remake and worked like a charm,Positive
AMD,Would this work for Metal Gear Solid Delta? I’m not sure what version of FSR that’s got.,Neutral
AMD,"I got the 9070 XT and updated the driver, but still no FSR 4 shown for Hogwarts Legacy and Star Wars Outlaws for example (only fsr 3). Am I doing something wrong?   Got version 25.9.1 installed.  Edit: Aaah I just notice the ""FSR 4"" enable option in Adrenalin, that actives FSR 4 (while choosing FSR 3 in game)",Negative
AMD,This is joyous news,Positive
AMD,Good to see AMD making strides on the software front. If only they could sell the 9070XT at their advertised $600 or was that another jebait?,Positive
AMD,Now Linux. Tired of patching games manually with optiscaler.,Negative
AMD,"Okay, once again I have to manually install the new driver because for whatever reason AMD's software claims my driver is current even though it's not.  Is this how it is with AMD?   9070 since the end of May, I have had to manually install every update.",Negative
AMD,"funny how everyone with a 7000 card was against upscaling in general, now everyone is begging for fsr4 lol.",Negative
AMD,Still doesn‘t work with Microsoft Flight Simulator 2024,Negative
AMD,Can they enable freesync MST on Linux ?,Neutral
AMD,"ITT: people who got memed by Nvidia proprietary tech, 23rd edition",Neutral
AMD,"I tried it in throne and libtery, the frames are so unstable like lows of 60-70 and highs in the 300s(pre update i sat at 90-120 fps all the time)  There's also this really diabolical frame lag occasionally where it feels like playing with 200ms input lag.",Negative
AMD,It’s still coming but probably not till Redstone comes out,Neutral
AMD,Yea i am quite annoyed at the lack of FSR4 on my 7900 XTX. I feel quite forgotten.  I was Nvidia buyer from Geforce 2 GTS until 1080TI. This is my first AMD.  Not sure i will get AMD again. simply due to the lack of FSR that does not look like shit.   I am more and more regretting i did not get the 4080.,Negative
AMD,"According to leaks FSR 4 is coming to RDNA 3, just give it time.",Neutral
AMD,just use xess2.,Neutral
AMD,For that reason not buying amd ever again .,Negative
AMD,its impossible,Negative
AMD,"MGS Delta uses FSR 3.0 apparently, so I don't think this would work",Negative
AMD,"Use OptiScaler, it works amazingly, at least on Linux.",Positive
AMD,"Did you notice worse perfomance? In Silent Hill 2 atleast, It plays worse",Negative
AMD,Same,Neutral
AMD,"Its normal, it is staggered updates so if there is a big nasty bug, only few people are affected before it can be resolved.   This is industry standard, it can take a few hours to a few days.",Negative
AMD,"To be fair, I had to do that with NVIDIA drivers too for a while. Downloading via the app freezed the PC, fortunately they fixed that issue.",Neutral
AMD,It will be there eventually. Probably best to push automatic prompts for update after a month when the driver has seen some use.,Neutral
AMD,"Uninstall the install manager, I was wonder why it wouldn't update yesterday too.",Neutral
AMD,"The current state of game ""optimization"" dictates that you need upscaling, unfortunately. And it doesn't look like it's going to improve any time soon since you have so many AAA game studios choosing to use UE5 for their games for at least the next 3-5 years.",Negative
AMD,"Asobo's TAA-U is better than FSR4 anyway. (one of, if not the only, TAA solution i can say that about)",Positive
AMD,"I know it's not official, but it's been working great on Linux for months. Better than XeSS in terms of quality and about the same in terms of performance impact",Positive
AMD,"Well same can be said for Nvidia as well.  You bought the GTX 1000 and miss on all the RT, DLSS of the RTX series. Or you bought the RTX3000 and miss the FG feature.",Neutral
AMD,"Doesn't the 7900XTX lack the hardware instructions for FP8 data types to run the FSR4 model?  It's pretty hard to backport software that requires hardware that doesn't exist in older cards...  And FP4/FP16 could be made however it will perform differently, and likely to a lower standard, and or not grant the performance gains at the same ratio making it functionally useless (i.e. resolution go up, fps stay same or go down)",Negative
AMD,"Same. Got tired of reading headlines and just sold my 7900xt and got a 5070ti for MSRP and free borderlands 4. Not going to lie Dlss upscaling is amazing. I'm noticing details and objects in movement I didn't see before. It's hard to explain but it's a different depth.   I personally had issues with adrenaline not saving undervolt settings, getting stuck, calling random games Elder scrolls. So far with my new setup I havent had to do anything extra after setup. Adrenaline looks more modern tho.",Positive
AMD,Its being developed but likely it will be worse because pure emulating via fp16 is slow.,Negative
AMD,"feels even more annoying and shitty because even if i were to update from the XTX to make use of FSR4, theres NOTHING on AMD to upgrade to, moving to the 9070XT is stepping down on VRAM and raster performance. it doesnt feel right.",Negative
AMD,"You have a card that devours any game that's put out and can run huge AI models. Chill out, bestie.",Negative
AMD,"I only had ATI/AMD cards in the past. FidelityFX and later on FSR1 was quite bad, OpenGL was ass if you wanted to run console emulators.  After suffering for a few years, I bit the bullet and bought nvidia instead. No more FF14 DX11 crashes.",Negative
AMD,Do more research next time. Nvidia has always done better with DLSS,Neutral
AMD,"Nvidia forgets about you every generation.   Buyers of a 3090 can't have framegen, Buyers from a 4090 can't have multi frame gen",Negative
AMD,I'm with you on that. When I get the funds I'm switching to Nvidia. Being left in the dust is getting old.,Neutral
AMD,I chose 4080 over 7900xtx. Before i always use AMD cards.   I've never been so happy. DLSS4 is just magic. Yesterday i started play RE4 remake second time and install DLSS mod. Its just miles better than any ingame AA.,Positive
AMD,Its funny that nvidia did enable dlss 4 for older cards... but amd was usually the one doing fine wine. Oh well....,Neutral
AMD,"I'm in the same boat, I think my 7900xtx will be my last AMD GPU. Their software support is a disaster. I can't stand NVIDIA's business practices, hopefully intel can get competitive before my next upgrade cycle.",Negative
AMD,"Its not impossible, fsr4 is coming on 7000series this year or at Q1 2026. But i bet it will come when fsr redstone releases",Positive
AMD,you need optiscalar then,Neutral
AMD,"I have tried fsr 4 for Hogwarts Legacy (not that I really needed it, but just to try) and got a lot of lagging and the game looked really blurry.",Negative
AMD,"The update is not available immediately it may take a few hours to show up on the app, chill out lol",Neutral
AMD,Has it gotten simpler to enable? Last time I looked it up my eyes kinda glazed over at all the steps.,Neutral
AMD,Talk about bad luck 😭 like switching lanes at the supermarket or on the highway just to get stuck more,Negative
AMD,It can translate Fp8 instructions to fp16 like on linux .  The image quality is on par with fsr4 on 9000 series but the performance improvements are not that big ( you get on average 50% more FPS  on 7900xt/xtx using FSR4 on performance mode compared to native  with the latest kernel & drivers ),Neutral
AMD,This reads like an Nvidia sponsored comment,Negative
AMD,"I read some people gotten it to work in some games by taking the file from the 9070XT drivers.  But only works for a few games and is artifact prone.  My 7900xtx does some fp8/fp16 when i run AI. And i think emulating some fp16 is easier than showing native resolution in very demanding games. Ofc depending on the upscale you are doing.  Like Cyberpunk i think would be easier for my card to run 1440p and upscale it with fp16. Than actually running the game in 4k. But what do i know :P  Now i am forced to run the game at 4k, and i have to chop quality to get the 100 fps i want.   Cause the FSR 3.1 or what i have, looks like donkey shit.",Neutral
AMD,"Yea, 9070XT is more a side grade than an upgrade. In some use cases a outright downgrade.   It is a odd place to be at for sure.",Negative
AMD,"At this point most console emulators offer Vulkan support and it usually runs great, the main emulator I'm aware of that still runs bad on AMD is original Xbox emulation which I believe is still only OpenGL/Direct3D on the main emulator for it I'm aware of, but virtually everything else offers Vulkan renderers now.  I know that doesn't address any of the other points, but I used to basically have a dedicated emulator PC running Nvidia, at this point now my main gaming rig with the 6900XT has basically zero issues emulating everything outside Xbox.",Neutral
AMD,"My main reason for picking AMD was simply the VRAM. Without it i could not run Yi-34b models etc.  Budget could not fit a 4090, so it stood between 4080 and 7900 XTX.  The 7900 XTX won cause of VRAM. Im not really regretting my decision, just annoyed over the deplayed FSR4 and feeling the need to vent. 24gb VRAM over 16gb VRAM anyday",Neutral
AMD,"Nvidia didn't implement ML Frame-Gen on Ampere for the same reason AMD haven't implemented FSR4 on RDNA3, lack of hardware support. At least it does look like AMD are working on a fallback method for FSR4 on RDNA3 where as Nvidia completely abandoned Ampere (though it can run FSR3 frame gen, so ironically AMD provided the fine wine for older GeForce cards).",Neutral
AMD,if it comes to 7k series it will come to 6k series too? neither of them have IA cores...,Neutral
AMD,source?,Neutral
AMD,"It's one or two environment variables, if you're on Mesa 25.2",Neutral
AMD,Yeah FP16 take twice the overhead though and requires casting every FP8 to FP16 that adds up :'(  That's a big performance hit!,Negative
AMD,"I doubt it.  I swapped my 9070xt for a 5070ti. Dlss4 (specifically the transformer model) is just better than even fsr4. Adoption is much more widespread. At native dlss4 looks better than TAA in basically any game, and dlss4 quality looks better than a lot of native TAA implementations (blah blah lazy devs)  I ended up having to pay about $100 to go from the 9070xt to the 5070ti (though I also got free borderlands 4 for what it's worth) and would do it again in a heartbeat.",Positive
AMD,"Nah I believe him. Had a 7900xt also last generation. Month after month was waiting for FSR3 to be adopted into games after they announced it. Of course we know how that went. By the time FSR 3 had decent adoption, we start hearing rumors of FSR 4 exclusive for the new gen cards. Raster performance was amazing of course but frame gen and upscaling are also amazing these days, on Nvidia. That's the Nvidia premium plain and simple. Got a 5080, drivers sucked in the first few months, things are ironed out now and I'm not placing my trust in AMD to deliver on their software side anymore.",Neutral
AMD,Because it is. Adrenaline had saved just getting uv setting for years.,Neutral
AMD,"Want images of both cards and purchases or something 😅. don't know what else to tell you. Just putting my input. Every FSR4 or Redstone post/article I kept feeling Fomo.  And I am putting this for any random new PC users. 7900xt was my first card. I kind of got swayed from the ""Raster"" conversations here. But now I have enough experience to know raster is not enough for me.",Neutral
AMD,There is no fp8 on 7900 the fsr 4 files won't work . What people do is on Linux they use fp8 emulation fond via fp16  Yes you gain a bit on 7900 on fp16 emulating fp8 but amd wants it to be usable at 1080p and bigger gains.,Negative
AMD,The new Nvidia frame gen dropped the use of the dedicated HW and it should now work on Ampere (on paper),Neutral
AMD,"They are working on int8 version of fsr4 for 7000series. They have ai cores,not so strong as 9000series. It will not come to 6000series",Negative
AMD,"Their leaked source code of fsr4 that got deleted shortly after, search about it",Negative
AMD,That is too much for a typical user.,Negative
AMD,There is indeed a perf hit but 7900xtx has so much raw power it can still actually get like +10% perf with fsr4 quality on emulated fp8 on linux. This is using the full fsr4 model which AMD hasn't optimized for rdna3,Neutral
AMD,It should not be.. Usually you can use larger precision fatser than smaller one. The issue here is that there are way more small fp8  on rdna4 that there are fp16 on the xtx. It s little more complicated than just a casting overhead.,Negative
AMD,"I don't like upscaling nor frame gen and I think both look like Vaseline was smeared on my screen, just in different ways. I went AMD because I couldn't give a damn about either tech, or ray tracing. Give me big 'real' FPS numbers and a big VRAM pool, or give me death!",Negative
AMD,"Aha, so thats how they do it.   Sad it takes so long for them to implement it. Cannot be that hard making the pipeline.    Guess their focus is somewhere else than an ""old"" card.",Negative
AMD,"They still locked that generation out, which was the point. Even if it now technically would work.",Neutral
AMD,"10% eep, that's really not good at all :S",Negative
AMD,"Even as someone who has both nvidia and AMD, I stuck to native only until DLSS4, it legit surpass native TAA in a lot of the games that are not properly tuned.",Positive
AMD,"To each their own, the beauty of the free world! Glad you are happy with your setup!",Positive
AMD,I thought that about upscaling until I had DLSS 4. It is incredible.,Positive
AMD,"If fsr4 is using a model, they might be reverse quantizing it? for fp16",Neutral
AMD,"It is better than nothing at least. Given the lack of fp8 acceleration on rdna3 which is needed to run fsr4.0.2 at full perf, 10% is pretty generous",Neutral
AMD,"> games that are not properly tuned   Well, there's your problem, isn't it? For the last couple years devs have been using upscaling and particularly DLSS as a cop out for not optimizing games correctly (both performance optimizations and visual ones). And so we get games with that same Vaseline effect even at native resolution. It's disgusting and I will not give money to devs that don't know how to optimize their shit",Negative
AMD,TAA is not native.,Neutral
AMD,"Sadly, They will keep getting worse. It is UNREAL (not a pun) how many UE games have terrible TAA which can be tuned easily using a few cvars to make it look decent without too much blur. The values that developers put in are ridiculous. Other engines suffer even more because you can't tune the TAA and have to completely disable taa and inject another form of AA with reshade",Negative
AMD,What I meant was the game running at native resolution with TAA as the anti aliasing method if that wasn't obvious to you btw.,Neutral
AMD,"Yep. If the game is poorly optimized, the studio ain't getting my money.    Which probably means a lot of 🏴‍☠️ is in my future.",Negative
AMD,Have a question which gonna sound weird but i dont understand well. Is this gonna improve rt performance of rx9070xt???,Negative
AMD,Does anyone know if FSR affect input latency? If it does how noticeable is it?,Neutral
AMD,Man it sucks that a 5 year old game is still the go to for all legendary graphics examples and 1) they discontinued the engine and 2) no one else has even come close with an urban open world and 3) honestly gpus aren’t _that_ much better than the 3090 that came out when cp2077 launched.,Negative
AMD,Has AMD completely forgot the AMD 7000 series exists lmfo,Neutral
AMD,Rx 9070 xt keeps getting faster. How bout that?,Neutral
AMD,Im hoping that ray reconstruction just works in the existing 3.1 DLL implementation.,Neutral
AMD,"Good to see that AMD is finally able to implement all these nice features now that they've added their equivalent of Tensor cores to RDNA4.  It really was a bit of a chicken vs egg problem that the technology to make use of specialized compute units wouldn't get developed if GPU manufacturers didn't ""waste"" die area adding them in the first place.",Positive
AMD,should i buy or wait then ? have been planning and waiting a long time to upgrade from RX 570. 4GB,Neutral
AMD,It will work on chips like the z2?,Neutral
AMD,Odds that it will come with some iteration of FSR4 for RDNA3?,Neutral
AMD,"Please, I'm waiting for this so I can play Indiana Jones path traced.",Positive
AMD,"Yet another tech skipping previous gens. Bought a 7900xtx, it will be my last AMD GPU.",Neutral
AMD,Kind of. The technologies involved are broad and lean toward improving image quality for the same given inputs rather than optimizing existing ray tracing algorithms.  Neural radiance caching has the potential to improve performance by reducing the number of rays needed for a given scene. Although benefits diminish as the scene becomes more dynamic.  Ray regeneration is more about image quality (noise reduction) but could allow for fewer rays meaning higher performance.  Improved FSR4 is again about image quality but if that lets you run at lower input resolutions then it increases performance.   AI frame generation could mean frame generation (which already exists) just with fewer artifacts. And I expect this is where AMD will allow for greater than 2x frame generation.,Neutral
AMD,"I believe that is the case but I am still unsure how much work the devs would have to do to support it. So if we look on how long it takes for devs to use FSR 3.1 (or 4), it will take sometime to take off.   I hope they are successful, not because I have an AMD card but more about competition.",Neutral
AMD,"The ""Ray Regeneration"" part will improve image quality but hit performance a bit.  ML FG will improve image quality but hit performance a bit.  The neural caching will improve performance.",Neutral
AMD,"In games that supports it, they will have technology similar to the ray reconstruction in DLSS (FSR ray regeneration) and something called Neural Radiance Caching that improves global illumination.",Neutral
AMD,"It's ML solution for ray tracing, so when using ray tracing only some images would need some enhancements so using ML it can make the image look better.  So if you use RT now it would make the image even better (with some cost)",Positive
AMD,"The current software stack used by devs for path tracing is made by nvidia, do you expect nvidia to optimize their software to run better on AMD?  This one will be made by AMD and optimized for AMD hardware.   But the quality of it remains to be seen, the samples showed so far weren't that great.",Neutral
AMD,On Nvidia you see RR’s true benefit when you turn on pathtracing.,Positive
AMD,"FSR doesn’t affect input latency, but frame generation does.",Neutral
AMD,"all of the resolution scaling techniques technically have a hit to input latency, but the framerate growth gains which lowers the latency is generally greater than the latency that the feature adds, making it a net positive technique when in context to latency.  all except DLSS/FSR Native res, then I guess the point is you added latency for super sampling reasons.",Neutral
AMD,It’s still Nvidia’s playground for most of their new tech (with good reason imo),Positive
AMD,"To be fair...  Cyberpunk 2077 became a testbed for future features or benchmarking in the same veins Crysis 3 continues to be used as a testbed for benchmarking.   at this point; I'm expecting CDPR and Nvidia to add RTX Hair in the next update, instead of reserving that for le Witchardo 4.",Neutral
AMD,Honestly this doesn't look good for AMD... You're showing off you can FINALLY have clear reflections in a game that is 5 years old... Nvidia has had this already for half a decade now... AMD acting like its something new lmao.  Like people were running this exact software feature set on Nvidia 2 GPU generations ago lol.,Negative
AMD,Could very well be announced together with Redstone. the leaked SDK did show that AMD has worked on FSR4 or an alternative for RX 7000 series,Positive
AMD,7000 lacks the needed hardware (I guess).,Negative
AMD,They are working on bringing FSR4 to RDNA3. Leakers are saying end of Q4 or sometime in Q1. And leaked SDK also confirms they have been working on it.,Neutral
AMD,"Ray Regeneration is their equivalent of Ray Reconstruction, and knowing what inputs RR requires, that’s practically impossible without dev work.  On the other hand, games with FSR 3.1.4 implemented should be an autoupgrade to the Redstone ML FG.",Negative
AMD,"Tbh, even nvidia cannot get RR in many games. I doubt AMD can do that easily as well.",Negative
AMD,"No, AMD does not have tensor cores named or structured exactly like NVIDIA's in the RTX 9000 series GPUs. NVIDIA's tensor cores are dedicated, specialized hardware units within each streaming multiprocessor optimized for matrix multiply-accumulate (MMA) operations in AI and machine learning workloads, such as DLSS upscaling and neural rendering. These have evolved across generations (e.g., 5th-gen in Blackwell) with support for formats like FP8, sparsity, and high-throughput INT8/FP16 computations.  Instead, AMD's RDNA 4 architecture (powering the Radeon RX 9000 series, launched in early 2025) uses {AI Accelerators} (second-generation in RDNA 4) as its equivalent for AI acceleration. These are not fully standalone ""cores"" like NVIDIA's but integrated into the compute units (CUs) via specialized tensor ALUs and instructions like Wave Matrix Multiply-Accumulate (WMMA). This enables similar matrix operations for tasks like FidelityFX Super Resolution 4 (FSR 4), an ML-based upscaling tech that leverages hardware-accelerated FP8 WMMA for improved quality and performance over prior FSR versions.  AMD still does not have dedicated Tensor cores like Nvidia does. They're just AI cores mixed in their Compute Units.. Nvidia actually has Real Dedicated Tensor cores separate from the CU's. 9000 series does not have that.",Neutral
AMD,"Do you want to upgrade now? Then do it. If you are still fine with your GPU then wait until you are not. Nothing more to it, there will always be better stuff coming out.",Positive
AMD,"I don’t think Redstone is a reason to wait, unless you’re currently unhappy with the functionality offered by the 9000 series and want to see if Redstone tips it over for you. It *may* help close the gap in heavy RT/PT games which is the major gaming advantage of Nvidia right now, but it also likely depends on devs implementing it which could take a while.",Neutral
AMD,"Bye then, you won't be missed.",Neutral
AMD,"""another""?",Neutral
AMD,I mean is it much different on Nvidia side?   You bought GTX1000 and miss the whole DLSS stuffs.  You bought RTX3000 and miss the Frame Generation.,Neutral
AMD,"Implementing FSR 3.1 isn't a lot of work. It really boils down to priority. It doesn't help any that Nvidia heavily incentivizes their own upscaler by virtue of being the market leader. Nvidia can ignore sharing their implementation while others make it easily available for all platforms.  Basically, it's on AMD to work harder than Nvidia to get their tech out there as conveniently as possible to consumers.",Negative
AMD,I hope so. Playing 4K with my card and having no issue. But when i turn on RT...,Neutral
AMD,"I dont think it will be that hard, I already seen people DLL swap fsr 3.1 with 4 in some games utilizing the files from github",Neutral
AMD,"Performance hit from denoiser will depend on the title. In cyberpunk cards that can’t use ray reconstruction, use nvidia real time denoiser. NRD has a higher fps cost compared to ray reconstruction. So when used with path tracing, ray reconstruction gives both better quality and performance",Neutral
AMD,"FSR has a tiny latency cost. However in almost all correct usage scenarios you will have a higher frame rate and thus lower frame times, so net positive.",Positive
AMD,"Agreed that it's incredible, but I'm more frustrated with the state of the gaming industry that there are only two companies who seem to be able to execute on this level (CDPR and R*) and they only do it every 10-15 years.",Negative
AMD,I don’t think they’ll do any more red engine updates. The hair is in UE5.,Neutral
AMD,AMD adds a feature they didn't have.  And you manage to act like they ran over someones cat.,Negative
AMD,Maybe Optiscaler can hook up Regen to Reconstruction like they do with DLSS>FSR currently…?,Neutral
AMD,It all depends on how it's implemented. If SDK 2.0.0 is already primed for Redstone then hopefully any games implementing the latest SDK should hopefully just work,Positive
AMD,"I don't think its AMDs ball to convince devs, its Sony's. Once Sony has all their 1st party devs on it, basically the entire Sony stack will probably use it for next gen hardware. At the bare minimum, Sony's 1st party devs will likely use it. AMD is taking advantage of their mutual reliance on each other with Project Amethyst.",Neutral
AMD,thats true but the pricing. every time it breaks my budget limit  radeon 6000 series was great with price  but i did not had the need. now my current GPU; its unacceptable but the 9070xt pricing is unaffordable for me. its almost 80K bangalore,Negative
AMD,"You say that but AMD are in a strong position, and yet they are still experiencing record low market share. It's probably not a good idea for them to lose appeal to consumers that aren't brand loyal to Nvidia.  I'm hopeful they will turn things around with or post redstone but I don't have much confidence in it happening.",Negative
AMD,Thankfully I’m not impressed by RT so I turn mine down to low settings.,Negative
AMD,RR gives same/better performance on RTX 40-50. Older gens have performance affected substantially,Neutral
AMD,"If AMD uses identical input data, then that might be possible.  I would imagine they would, it would mirror their AI/DC strategy of keeping as much identical to Nvidia as possible for faster swapping back and forth, but it's all guesswork for now.",Neutral
AMD,"Quite possible, albeit it will all depend on the final product and seeing what inputs they both require. Opti devs are certainly interested in it.",Positive
AMD,I  am not very hopeful. Nvidia also relies on the fact that it can easily replace DLL or use nvidia app override. Many games still come with outdated dlls for DLSS too. A game that I was playing uses NVIDIA DLSS DLL 310.1 and FSR3.1.3 for an update that released last week while FSR3.1.4 and 3.1.5 is available. It is on unreal engine which means updating DLL was as easy as updating the plugin version they have and they didn't bother the 10 minutes of extra work to do that.,Negative
AMD,"Well, if you can’t afford the gpu you want then don’t buy it and save money for it. If by the time you have the money a new model gets announced then maybe wait for it so prices of previous models drop (also the exact opposite could happen you never know).",Neutral
AMD,I wouldn't rule out lower class cards like the 9060 XT.  I'm using one in my living room PC and it's a great card for that since I tend to play games on a TV there and it's plenty powerful for that with FSR4.  There won't be RDNA5 cards until early 2027 according to latest leaks so really no reason to wait for that.,Positive
AMD,why not 9060 XT? it should be 3.5x faster than your card or so,Neutral
AMD,"AMD is making billions.  Intels lunch money is all theirs now, consumers and businesses can't stop giving AMD money for their cpus.  Hell, I was just reading they won't make any competitor to Nvidia's Geforce Now for the simple reason that it's running on their CPUs. They're getting paid by Nvidia for the stuff they are good at. Nvidia will openly say they test their gpus using AMD cpus.  It doesn't need to do anything with the gpu side besides keep it warm and keep the drivers up to date. Anyone wants some custom chips, AMD has the gpu tech to go with it. And when AMD puts out a gaming card you know they can make a decent one if they want to.   But as long as their GPU designs require more TSMC silicon than Nvidia's, they cost AMD more to make and that sucks when people demand AMD cards to also cost less.  Gpus are low priority until they can get better designs.",Neutral
AMD,"RT in many implementations is still used often as a hybrid tech and not a full on implementation. It's part of the reason why I unironically think some of the best implementaitons of raytracing are Nvidia's RTX Remix projects on 2 decade+ old games. You have an old game whose game engine is soo lightweight that any modern device will run laps around it for performance, so all of the performance goes straight to raytracing. Even with minimal model and texture modifications, the lighting changes absolutely evolve the game.  I do think its the method that many 360/PS3 era, or late PS2/GC/Xbox era games can be remastered without ruining the original gameplay, without having to fully do a ""remake"" of a game (e.g Oblivion remastered is a minor example).",Neutral
AMD,"I still think of RT as a meme technology in its current iteration. If you're staring at a scene after all the rays have populated, it's gorgeous, but reflections blur to a distracting degree the instant you start moving. Turning the camera and watching the lighting change (or reflections fill in) over a half-second as the rays populate is also distracting in a bad way. Until that issue is somehow solved, I specifically do **not** want RT effects in any game I play. If I turn to look at something, I want to see what that something looks like straight away, not watch it morph before my eyes as I wait for the lighting to resolve.",Negative
AMD,Algorithms of RR and DLSS just get heavier with newer versions. I remember that first iteration of RR was improving performance on 30-series cards.  Real question is how capable is RDNA4 at computing whenever AMD releases?,Neutral
AMD,Fingers crossed!,Positive
AMD,As long as its 3.1 and has the Dll it can be upgraded to the latest FSR 4 now,Neutral
AMD,i usually upgrade in a 5-7 years horizon due to high costs in india. hence i push max my budget and try to buy something which will last somewhat longer. my rx 570 4GB is purchased i think april 2019 or before.  the only issue which i see is 4GB limit else i would not have upgraded i think since i play at 1080p. that 4GB VRAM is causing limitations.  but thank you for helping next card release dates.,Neutral
AMD,i usually upgrade in a 5-7 years horizon due to high costs in india. i am trying to push max my budget and try to buy something which will last somewhat longer. my rx 570 4GB is purchased i think april 2019 or before. the only issue which i see is 4GB limit else i would not have upgraded i think ; since i play at 1080p. that 4GB VRAM is causing limitations.  i sae the 9060xt and its a solid card. but i dont think it will last long. maybe i am wrong.,Neutral
AMD,"Yeah, this is definitely true, I'm happy with their CPU's, and APU's if you include consoles/steam deck. I've just been bitten too many times by their driver and software issues relating to their dedicated GPU products. I'm also very aware I'm effectively shouting I to the void but what can you do?",Positive
AMD,Of course but I just couldn't understand what is deterring them from including the latest version.,Neutral
AMD,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Neutral
AMD,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Negative
AMD,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Negative
AMD,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Neutral
AMD,Just hodl until you get the biscuits,Neutral
AMD,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Negative
AMD,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Neutral
AMD,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Neutral
AMD,"why even panther laker when it was only for mobile, cancelled it and just released nova lake next year",Negative
AMD,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Neutral
AMD,I thought Arrow Lake refresh was in the cards for 2025.,Neutral
AMD,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Neutral
AMD,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Negative
AMD,This entire thing is a mobile roadmap so why are you here?,Negative
AMD,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Neutral
AMD,"Yeah if it's Surface roadmap, it's a nothing burger.",Negative
AMD,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Neutral
AMD,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Neutral
AMD,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Neutral
AMD,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Neutral
AMD,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Neutral
AMD,"I like how they just throw random words around to pad their ""article"".",Neutral
AMD,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Neutral
AMD,"""leaks""",Neutral
AMD,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Negative
AMD,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Negative
AMD,Scared of their rumor?  Lets release our rumor!,Neutral
AMD,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Negative
AMD,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Neutral
AMD,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Negative
AMD,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Neutral
AMD,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Negative
AMD,bLLC is not stacked cache,Neutral
AMD,What do you consider random? The article was perfectly clear.,Neutral
AMD,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Neutral
AMD,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Neutral
AMD,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Negative
AMD,Yeah. Definitely just you,Neutral
AMD,You could literally make that claim with any CPU performance increase.,Neutral
AMD,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Neutral
AMD,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Neutral
AMD,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Neutral
AMD,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Negative
AMD,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Neutral
AMD,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Negative
AMD,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Neutral
AMD,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Negative
AMD,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Neutral
AMD,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Neutral
AMD,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Negative
AMD,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Negative
AMD,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Negative
AMD,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Negative
AMD,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Neutral
AMD,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Positive
AMD,It's not sorcery. Its just Intel doing the game developers work.,Neutral
AMD,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Negative
AMD,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Negative
AMD,Never count out Intel. They have some very talented people over there.,Positive
AMD,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Neutral
AMD,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Negative
AMD,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Neutral
AMD,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Neutral
AMD,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Negative
AMD,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Positive
AMD,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Neutral
AMD,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Positive
AMD,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Negative
AMD,How is this doing the game developers work?,Neutral
AMD,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Negative
AMD,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Negative
AMD,The intel software team is pure black magic when they allowed to work on crack.,Negative
AMD,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Neutral
AMD,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Negative
AMD,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Positive
AMD,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Neutral
AMD,Balanced in full load will just do the same thing as High Performance.,Neutral
AMD,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Neutral
AMD,Or... Just process lasso.,Neutral
AMD,Because it’s optimizations on how it can efficiently use the cpu.,Neutral
AMD,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Neutral
AMD,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Positive
AMD,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Neutral
AMD,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Neutral
AMD,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Neutral
AMD,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Positive
AMD,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Positive
AMD,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Neutral
AMD,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Negative
AMD,You need to download the Intel Application Optimization app from the Windows store,Neutral
AMD,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Positive
AMD,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Positive
AMD,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Positive
AMD,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Neutral
AMD,Where to download this APO,Neutral
AMD,Except its THE game devs job to optimize games for multiple cpus and gpus.,Neutral
AMD,It’s literally their job to do so? wtf you talking about?,Negative
AMD,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Neutral
AMD,"Will do, I got a 265K. Performance is already great tbh.",Positive
AMD,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Neutral
AMD,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Neutral
AMD,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Neutral
AMD,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Negative
AMD,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Negative
AMD,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Negative
AMD,That's not simply what APO does.,Neutral
AMD,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Neutral
AMD,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Positive
AMD,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Positive
AMD,i am not talking about older games. i am talking about newer games.... really dude?,Neutral
AMD,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Neutral
AMD,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Neutral
AMD,Thanks a lot.,Positive
AMD,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Negative
AMD,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Neutral
AMD,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Negative
AMD,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Negative
AMD,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Negative
AMD,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Neutral
AMD,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Positive
AMD,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Positive
AMD,Did the DP4A version also improve from 1.3 to 2.0?,Neutral
AMD,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Neutral
AMD,Okay but why would I want to use that instead of NVIDIA DLSS?,Neutral
AMD,It’s the least you should get after not getting FSR4.,Negative
AMD,You'd use it over FSR if that's available too?,Neutral
AMD,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Neutral
AMD,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Negative
AMD,And they expect people who bought previous RDNA to buy more RDNA,Neutral
AMD,Not by a significant amount.,Neutral
AMD,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Neutral
AMD,for the games that dont support DLSS,Negative
AMD,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Neutral
AMD,10 series cards will benefit from this,Positive
AMD,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Negative
AMD,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Positive
AMD,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Positive
AMD,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Neutral
AMD,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Negative
AMD,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Positive
AMD,DP4a is cross vendor.   XMX is Arc only.,Neutral
AMD,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Neutral
AMD,1080ti heard no bell,Neutral
AMD,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Neutral
AMD,where did amd touch you bud?,Neutral
AMD,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Neutral
AMD,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Neutral
AMD,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Neutral
AMD,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Positive
AMD,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Negative
AMD,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Negative
AMD,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Neutral
AMD,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Negative
AMD,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Negative
AMD,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Negative
AMD,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Neutral
AMD,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Neutral
AMD,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Negative
AMD,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Neutral
AMD,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Neutral
AMD,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Positive
AMD,Thank you :),Positive
AMD,Wouldn't the 300 series actually be Arrow Lake Refresh?,Neutral
AMD,"Videocardz is wrong      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Neutral
AMD,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Positive
AMD,This is pat's work,Neutral
AMD,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Neutral
AMD,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Positive
AMD,Large L3 cache without reducing latency will be fun to watch.,Positive
AMD,Is Intel finally making a comeback with their cpus? I hope so,Positive
AMD,the specs sure do shift a lot..,Negative
AMD,it's just a bunch of cores glued together - intel circa 2016 probably,Neutral
AMD,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Negative
AMD,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Positive
AMD,Bout time,Neutral
AMD,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Negative
AMD,Will it be available in fall of this year or 26Q1?,Neutral
AMD,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Negative
AMD,Noob question:  Is this their 16th gen chips?,Neutral
AMD,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Neutral
AMD,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Neutral
AMD,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Neutral
AMD,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Positive
AMD,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Positive
AMD,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Negative
AMD,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Negative
AMD,"So alder lake on cache steroids ?  If the heavier core count chips don’t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Neutral
AMD,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Neutral
AMD,"""E-Cores"",  Ewww, Gross.",Negative
AMD,"**Hi,**  I’m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPU’s clearly starting to bottleneck a bit, but it’s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Neutral
AMD,NVL will definitely be the 400 series. PTL is 300.,Neutral
AMD,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Neutral
AMD,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Neutral
AMD,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Neutral
AMD,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Neutral
AMD,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Neutral
AMD,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Positive
AMD,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Negative
AMD,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Neutral
AMD,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Negative
AMD,More like *despite* him.,Neutral
AMD,All the SKUs rumored so far are BLLC,Neutral
AMD,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Neutral
AMD,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Negative
AMD,Why not 8 P cores with HT + even more cache and no e cores at all,Neutral
AMD,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Positive
AMD,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Neutral
AMD,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Neutral
AMD,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Negative
AMD,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Neutral
AMD,Nova lake? More like 26Q4.,Neutral
AMD,Only Pantherlake for mobile,Neutral
AMD,"It doesn’t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Neutral
AMD,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Neutral
AMD,It is really happening. The only question is when? Or can they release it on the next year without delay?,Neutral
AMD,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Negative
AMD,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Neutral
AMD,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Negative
AMD,"E cores are the future, P cores days are numbered.",Neutral
AMD,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Negative
AMD,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Neutral
AMD,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.     You can definitely wait a bit.",Negative
AMD,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Neutral
AMD,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Neutral
AMD,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Negative
AMD,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Neutral
AMD,The former ceo,Neutral
AMD,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Negative
AMD,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Positive
AMD,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Positive
AMD,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. You’re using dated info.,Negative
AMD,No different to 12-14th gen then.,Neutral
AMD,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Negative
AMD,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Neutral
AMD,E core is 30% faster than hyper threading,Positive
AMD,HT is worse than E cores,Negative
AMD,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Neutral
AMD,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Neutral
AMD,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Neutral
AMD,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Negative
AMD,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Neutral
AMD,ARM chips regularly do this sometimes on a yearly basis.,Neutral
AMD,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Neutral
AMD,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Neutral
AMD,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Negative
AMD,"> They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Neutral
AMD,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Neutral
AMD,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Neutral
AMD,"Lmao, some of my games still runs on e cores",Neutral
AMD,People are still disabling e cores for more performance.,Neutral
AMD,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Neutral
AMD,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Neutral
AMD,fym no different they're 50% faster,Positive
AMD,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Negative
AMD,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Neutral
AMD,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Neutral
AMD,"As a advice, Nova Lake will further increase memory latency.",Negative
AMD,In which gen iteration?,Neutral
AMD,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Neutral
AMD,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Negative
AMD,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Neutral
AMD,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Neutral
AMD,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Neutral
AMD,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Neutral
AMD,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Neutral
AMD,They don't pay attention,Negative
AMD,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Neutral
AMD,That is more so bc the 9950x3d isn’t really a 16 core cpu it is two 8 cores,Neutral
AMD,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Neutral
AMD,How do you think they're doing 2 compute tiles if the memory controller is on one?,Neutral
AMD,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Neutral
AMD,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Positive
AMD,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Neutral
AMD,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Neutral
AMD,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Neutral
AMD,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Positive
AMD,That didn't stop Intel with N3B,Neutral
AMD,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Neutral
AMD,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Neutral
AMD,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Positive
AMD,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Neutral
AMD,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix the problem. It doesn't happen very often but when it does it's very annoying.",Negative
AMD,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Neutral
AMD,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Neutral
AMD,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Neutral
AMD,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Neutral
AMD,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Negative
AMD,Will it also introduce Lunar Lake successor?,Neutral
AMD,I wonder if this could become avaliable on LGA 1954 because that'd be a good alternative for budget builds potentially if the iGPU is potent enough.,Positive
AMD,Is anyone left?,Neutral
AMD,I would love to see a Nova Lake-A (strix halo like) APU with an 8+16 tile + 20Xe3 cores which is equal to 2560 FP32 lanes or 40 AMD CU's with 16-32mb of memory side cache to handle igpu bandwidth demands,Positive
AMD,"Wonder what node such a product, if it does exist, would use for the iGPU tile.   AFAIK, rumor is that the iGPU tile for the standard parts would be on 18A or an 18A variant. However, if this is a flagship part, I would assume they would be willing to pay the extra cost to use N2...   However that would also presumably involve porting the architecture over to another node, which I don't think Intel has the resources, or wants to go through the effort, to do so.   Also wonder if such a product will finally utilize the ADM cache that has been rumored since like, forever.   And this is a bit of a tangent, but I swear at this point in the leak cycle for a new Intel product, there's always usually a ton of different skus that are leaked that may or may not launch - right now we have NVL bLLC, NVL standard, and now this, and a bunch of them end up not launching (whether they were cut internally, or the leaks were just made up). So it would be pretty interesting to see if any of the more specialized rumored NVL skus end up actually launching.",Neutral
AMD,These designs are going to be the go-to for creator and mid-range gaming at some point in the future. Having it all integrated together should provide opportunities for optimization and form factors that are difficult with discrete cards,Positive
AMD,How long do I have to wait for something that is more powerful than an AMD 7840HS and still cheaper?,Neutral
AMD,Good to hear! I have a strix halo asus tablet the 390 one and its only got 32GB ram but its fast!  Would love to see an intel version…,Positive
AMD,"This is great move by Intel since they aren't making dGPU for laptop so they can get some marketshare by making strong Intel ecosystem. If Nova Lake AX released then people don't need to buy overpriced laptop with Nvidia dGPU anymore, not to mention with Nvidia shitty small vram.",Positive
AMD,"I'm still using Alder Lake, I suppose all these chips will be released in 2026 ?",Neutral
AMD,That's Panther Lake in a few months,Neutral
AMD,Lunar lake was always confirmed even by Intel as a 1 off design with soldered memory and many of its design choices.,Neutral
AMD,"No, the memory config wouldn't work.",Negative
AMD,No not really.  It's pretty f'n bleak atm.,Negative
AMD,"That isn't that different from the AI MAX 395+ already released, which has 16 Zen 5 cores and 40CUs and some MALL cache (I think 20MB) to help with bandwidth.  For this to be a meaningful upgrade, you'd want more than 20 Xe3 cores, which means you need more bandwidth, which means you need probably LPDDR6 memory to get it high enough as otherwise the cache demands would be prohibitive and LPDDR6 memory isn't coming to client until probably 2027.",Neutral
AMD,It's N2.  Sadly Intel Inside has almost entirely become TSMC Inside,Negative
AMD,"Nova Lake-AX will not be released, and DLLC will not be released, and nova Lake will not be released.",Negative
AMD,"Unless they can find some way to bring down the cost.....these aren't gonna get much traction, the same applies to Strix Point of course. While more efficient than your typical 2000$ laptop with a decent GPU.....the traditional laptop is still gonna outperform them based on pure specs.  You get more battery hour and less heat but people who use these kind of laptop are used to having it plugged 24/7. Feels more like experimental models but they have a long way to go before they see proper adoption. Particularly the cost.",Negative
AMD,"> mid-range gaming  Strix halo is only in $2000 systems. high end price, mid-range performance.  Good for stock margins i guess.",Positive
AMD,I'm sure that Xe2 in Lunar Lake is faster than the 7840HS iGPU on average if you run it with the max performance mode in the OEM software that comes with your laptop.,Positive
AMD,The 7850HS can't compete with the Xe2 igpu because it's bandwidth starved.   The 8mb of memory side cache insulating the 4mb of L2 from main memory allows the Xe2 Arc 140V igpu in LL clocked at 1950mhz to beat the RDNA 3.5 890m clocked at 2950mhz as it only has 4mb of L2 as a last level of cache before hitting memory.   TLDR: RDNA 3.5 is bandwidth starved on strix point due to lacking memory side cache,Neutral
AMD,PTL's not really a LNL successor.,Neutral
AMD,"Yeah, it was just to prove a point that ARM is overrated.",Negative
AMD,"I hear you, questioning my decision to return under Pat’s hire-back spending spree. Dodged this one… but this is hitting differently.",Negative
AMD,What about an 8 + 16 big LLC tile and 32Xe3 cores (4096 FP32 lanes or ~60 AMD CU's) + 64mb of memory side cache?   Or even better a mid range part with an 8+16 tile and 16-20Xe3 cores + 16-32mb of memory side cache,Neutral
AMD,Just read about the JEDEC spec for lpddr6 of 14400mt/s.  That is wild!,Neutral
AMD,Who said it's It's entirely TSMC ? they have been pretty open about majority of tiles Intel the CPU Tile will be shared with N2 ofc though,Neutral
AMD,"It should be Intel here, TSMC there.  Or a little bit of Intc, a little bit of TSMC.",Neutral
AMD,IDM 2.0: Where we proudly declare our fabs are world-class—while quietly handing the crown jewels to TSMC.,Positive
AMD,"The BOM is lower, so the question is where the markup is coming from.",Neutral
AMD,Consoles have big APUs and they have better value than anything. I know they're subsidized however.,Positive
AMD,A mid range Nova Lake-A SKU with a 6+12 tile and 16/20Xe3 cores with 16-32mb of memory side cache would be sick,Neutral
AMD,Thats the problem with AMD sticking an additional 8 cores instead of infinity cache to fix the bandwidth bottleneck.,Negative
AMD,>still cheaper,Neutral
AMD,"It's the closest that we will get to a Lunar Lake successor   Really, panther lake is a combined successor to both Arrow Lake-H and Lunar Lake",Neutral
AMD,"Low volume and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference.  Combine that with not moving a lot of them compared to the rest of the lineup, and each one of those units has to make up more of the costs from things like bad dies or general spin up costs for a new chip.",Negative
AMD,It already has 32MB infinity cache.,Neutral
AMD,The 7840HS is cheaper because it is older.,Neutral
AMD,"It competes in the U/H lanes, so what is today ""ARL""-U and ARL-H. But yes, in practice it should bring *most* of the LNL goodness over, but PTL-U is still not a true successor in the original ~10W envelope LNL was designed for.",Neutral
AMD,"> and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference   It's no bigger than the equivalent dGPU, and you save on memory, package, and platform costs. Half the point of these things is to use the integrated cost advantage to better compete with Nvidia dGPUs.",Negative
AMD,The new Ryzen AI 250/260 with 780M is still cheaper because it doesn't have copilot+ .,Neutral
AMD,I will see the performance envelope of PTL U and decide should dump my LNL or not,Neutral
AMD,"The low volume part of that quote is important. If you're making heaps of them, each can take up a little bit of the startup costs for that design. If you only make a few, those costs get concentrated on what you do make.  Low volume also means a single defect is promotionally more of your possible units, and hits the margins harder. You do get more total defects in a larger run, but at sufficiently high quantities of those defects, you start making lower bins to recover some of the defects. If you hardly make the volume to cover one widespread SKU, you aren't likely to have enough to make a cheaper one with any degree of availability. At some point you hit a threshold where it's not worth selling what would be a lower bin because there's not enough of them.",Neutral
AMD,Because putting a 40 TOPs NPU for the Copilot+ certification is costly.,Negative
AMD,"It doesn't have a 40 TOPS NPU, it's the same 16TOPS one in the 8840HS it's literally just another rebrand. It's cheap because it's from 2023.",Neutral
AMD,Exactly. Hence your Xe2 lunar lake being faster is irrelevant to the question because it won't be cheaper regardless of age when they have a NPU that takes close to 1/3 of die space.,Negative
AMD,I think I didn't say anything that deviates from what you just said.,Neutral
AMD,"Maybe I misread or misunderstood, I thought you said the 260/250 had a 40TOPS NPU",Neutral
AMD,"“Yeah just push it to production nothing’s gonna happen, no need to QA/QC this”",Negative
AMD,Haha wow that is hilariously bad.   And here I thought ASUS newer bios update was bad due to some higher temps,Negative
AMD,"The new Gigabyte BIOS also has booting issues for me, and memory is even more unstable.  At least the UI is entirely in English now.",Negative
AMD,"who needs quality control, what can go wrong?",Negative
AMD,"This is why you don't rush to update software, let others do the testing for two weeks",Neutral
AMD,Gigabyte is trash,Negative
AMD,The Elon Musk method,Neutral
AMD,"> “Yeah just push it to production nothing’s gonna happen, no need to QA/QC this”  Seems to be the motto of a lot of the tech world",Neutral
AMD,"Yup indeed.   I would even go as far as one full month   Hence, why I learned to avoid installing new Windows Update automatically",Neutral
AMD,Intel really needs to be able to compete with X3D or they're going to continue getting dominated in the enthusiast consumer market. I like Intel CPUs and was happy with my 12600K for awhile but X3D finally swayed me to switch over.,Positive
AMD,"Intel has had plans for big ass L4 cache for almost a decade now, just that it never made it past the design board.  Supposed to be marketed as Adamantium. But it got ZBB’d every time I suppose due to cost.  For Intel to implement Adamantium, regular manufacturing yield has to be good enough I.e cost is low so they can splurge on L4.  Of course now they are forced to go this way irrespective of cost. I’d love 16p + L4 CPU.",Negative
AMD,"Honestly, good. I've been using AMD for a while now but we need healthy competition in the CPU space for gaming otherwise AMD will see a clear opportunity to bring prices up",Positive
AMD,"Something interesting is that the extra cache isn't rumored to be on a base tile (like it is with Zen 5X3D), but rather directly in the regular compute tile itself.   On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.   I think Intel atp desperately needs a X3D competitor. Their market share and especially revenue share in the desktop segment as a whole has been ""cratering"" (compared to how they are doing vs AMD in their other segments) for a while now...",Neutral
AMD,Hasn’t this been on their roadmap for a while now? I’m pretty sure they said 2027 is when they’ll have their version of x3D on the market,Neutral
AMD,"These core count increases could be a godsend at the low end and in the midrange. If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming, Intel will have a massive advantage on price.  That being said, if leaked Zen 6 clocks (albeit they’re from MLID, so should be taken with a grain of salt) are accurate, Nova Lake could lose to vanilla Zen 6 in gaming by a solid 5-10% anyway.",Positive
AMD,"Funny how non of this news posted on reddit hardware sub or even allowed to be posted. Guest what? R amdhardware will always be amdhardware! It's painfully obvious that unbearable toxic landfills sub is extremely biased to Amd. Meanwhile all Intel ""bad rumors"" got posted there freely which is really BS!  I still remember i got banned from that trash sub for saying ""People need to touch grass and stop pretending like AMD is still underdog because they aren't"" and the Amd mods sure really mad after seeing my comment got 100+ upvotes for saying the truth, but that doesn't matter anymore because i also ban those trash sub!",Negative
AMD,Intel should be ahead of the curve on things not looking to compete on previously created tech,Neutral
AMD,"Very fine-tuned ARL-S almost reach 9800X3D performance. Extra cache could help to close the gap   Given people are willing to overpay for price-inflated 9800X3D, I wonder if it could work given buyers need an entirely new platform. 9800X3D users are fine for a pretty long time like 5800X3D users did",Positive
AMD,"Lol, requires a new socket. Intel is such trash.",Negative
AMD,Intel will simply always be better than amd,Positive
AMD,"AMD gains tremendously from X3D/v$ because the L3 cache runs at core speeds and thus is fairly low latency, Intel hasn't seen such low latency L3 caches since skylake, which also has much smaller sizes, so the benefits of this could be much less than what AMD sees.   Only one way to find out, but I advise some heavy skepticism on the topic of ""30% more gaming perf from 'intel's v$'""",Positive
AMD,"Either more cache or resurrecting the HEDT X-series... Doesn't matter, as long as there is an affordable high-end product line.",Neutral
AMD,"The 12600k was a fine chip, but AMD had the ace up Its sleeve. I upgraded from a 12600k to a 7950x3d and it was one of the best PC upgrades I ever made.",Positive
AMD,I mean 9800x3D and 14900K offers basically the same performance in the enthusiast segment. Going forward though it would be nice to have more cache so normal users doesn't have to do any sort of memory overclocking just to match 9000x3D in gaming.,Neutral
AMD,4070 ti won’t cut it man - upgrade!,Negative
AMD,Broadwell could have been so interesting had it planned out.,Positive
AMD,"I want a 32 Core/64 Thread 3.40 GHz Core i9-like CPU. Not Xeon like with Quad-Channel and stuff, just 40 PCIe 5.0 lanes and 32 Power-Cores instead of little.big design. 😬",Neutral
AMD,">Otherwise AMD will see a clear opportunity to bring prices up  AMD already did, as you can see zen 5 x3d is overpriced as hell especially the 8 core CPU. Zen 5 is overpriced compared to zen 4 which is already more expensive than zen 3. Not to mention they did shady business like keep doing rebranding old chip as the new series to fools people into thinking it was new architecture when it wasn't and sell it with higher price compared to chip on the same architecture in old gen.  Intel surely needed to kick Amd ass because Amd keep milking people with the same 6 and 8 cores CPU over and over with price increases too! Not to mention radeon is the same by following nvidia greedy strategy.  Edit: Some mad Amd crowd going to my history just to downvote every of my comments because they are salty as hell, i won't be surprised if there are from trash sub r/hardware. But truth to be told, your downvote won't change anything!!",Negative
AMD,"Even though it's not stacked, I believe it's still going to fix the last level cache latency issue MTL and ARL have.   Ryzen CPUs have lower L3 latency than Intel because each CCX gets their own independent L3, unlike Intel's shared L3. Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3, so possibly giving the existing cores/tiles their own independent L3, improving latency and bandwidth over shared L3.  But one thing intrigues me. If this cache level has lower latency than shared L3, wouldn't this more properly be called L2.5 or something below L3 rather than last level cache? Will NVL even still have shared L3 like the previous Intel CPUs? I know the rumor that it will have shared L2 per two cores, but we know nothing of the L3 configuration.",Neutral
AMD,"> On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.  It is already a non-issue since AMD moved the 3D V-Cache to underneath the compute tile.",Neutral
AMD,"Adamantaium was on the interposer, did they change plans?",Neutral
AMD,"Don't remember them saying anything like that, but by around that time their 18A packaging is supposed to be ready for 3D stacking.",Neutral
AMD,"Nova lake= skip of it's just as good as zen, you would be looking at 2 gens after that and then swap from AM5 to intel.",Positive
AMD,"> If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming  Doubt that since it'll probably lack hyperthreading and the E-Cores are slower, even 6C12T CPUs are starting to hit their limits in games in the last few years, faster cores won't help if there's much less resources to go around, it kinda feels like intel went backwards when they removed hyperthreading without increasing the P-Core count.",Negative
AMD,Intel managed to run Sandy Bridge's ring bus clock speeds at core clocks which resulted in 30 cycles of L3 latency.   Haswell disaggreated core and ring clocks allowing for additional power savings.   Arrow Lake's L3 latency is 80 cycles with a ring speed of 3.8ghz,Neutral
AMD,"I'd like to see the HEDT X-series come back too, but Intel would have to come up with something that would be competitive in that area.  It's not hard to see why Intel dropped the series when you take a look at the Sapphire Rapids Xeon-W lineup they would have likely been based off of.  I think AMD would also do well to offer something that's a step above the Ryzen lineup, rather than a leap above it like the current Threadrippers.",Neutral
AMD,Well it was a downgrade on system snappiness as intel have way higher random reads than amd.,Negative
AMD,> I mean 9800x3D and 14900K offers basically the same performance  LMAO,Neutral
AMD,"Huh? 9800x3d is universally known to be like 20-25 percent faster, even in 1 percent lows.   https://www.techspot.com/review/2931-amd-ryzen-9800x3d-vs-intel-core-14900k/",Neutral
AMD,Maybe that is your experience.  Neverthelesss if you compare most gamers who switched to 9800x3D they report a significantly noticeable uplift in fps and 0.1 fps. Maybe a negligible few reported a decrease. And this has very likely nothing to do with the x3D CPU but other causes.,Neutral
AMD,"Ah you’re missing the final piece. As far as i’m aware this pretty much requires controlling the OS as well (or at least solid OS support). Consoles get their own custom operating system, Apple built a new version of MacOS for M chips. Intel and AMD though don’t control windows.",Neutral
AMD,UMA is such a hassle That's why I don't see it much except for calculation purposes (HPC/AI)...,Negative
AMD,"Application developers are supposed to try to avoid copies from GPU memory to CPU memory, instead letting it stay in the GPU memory as much as possible",Neutral
AMD,">so there is still the cost of useless copies between system RAM vs allocated GPU ram.    There is none, AMDGPU drivers have supported GTT memory since forever, so static allocation part is just to reduce burden for app developers but if you use GTT memory you can do zero-copy CPU+GPU hybrid processing.",Negative
AMD,"Intel needs something decent because AMD has taken a page out of intel (up to gen7) playbook, same cores no changes. Intel now provides more cores but it's the 100% core increase Vs AMD 50% and bLLC that should shake things up, hopefully they keep the temperature down as I don't want to have to replace case and get a 360mm rad just to not throttle, and not ever again do a 13th and 14th gen degradation show.   If all goes well going back to intel for a few years then AMD, brand loyalty is for suckers, buy what's best for performance and value. Hopefully intel i5 has 12P cores and i7 18-20P cores that would be nice to have",Neutral
AMD,"bLLC is just a big-ass L3$ and since Intel does equal L3 slices per coherent ring stop, it'll be 6\*12 or 12\*12 with each slice doubling or quadrupling. The rumor is 144MB so quadrupled per slice, probably 2x ways and 2x sets to keep L3 latency under control.",Neutral
AMD,"Intel and AMD have effectively the same client L3 strategy. It's only allocated local to one compute die. Intel just doesn't have any multi-compute die parts till NVL.   > Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3  8+16 is one tile, in regardless of how much cache they attach to it",Neutral
AMD,It is a massive issue for amd. You're voltage limited like crazy as electron migration kills the 3D cache really fucking fast. 1.3V is already dangerous voltage for the cache.,Negative
AMD,"I still think there's a slight impact (the 9800x3d only boosts up to 5.2GHz vs the 5.5GHz of the 9700x), but compared to Zen 4, the issue does seem to have been lessened, yes.   And even with Zen 4, the Fmax benefit from not using 3D V-cache using comparable skus was also only single digits anyways.",Neutral
AMD,"Adamantium was always rumored to be an additional L4 cache IIRC, and what Intel appears to be doing with NVL is just adding more L3 (even though ig Intel is calling their old L3 the new L4 cache? lol).   I don't think Intel can also build out Foveros-Direct at scale just yet, considering they are having problems launching it for just CLF too.",Neutral
AMD,"I'm an e-core hater but arrow lake e-cores are really performant and make up for the loss of HT. arl/nvl 4+8 would wildly beat 6c12t adl/rpl.  HT was always a fallacy anyway. If you load up every thread, your best possible performance is ~60% of a core for a games main-thread.  I would much rather pin main-thread to best p-core in a dedicated fashion and let the other cores handle sub threads. Much better 1% lows if we optimize for arrow lake properly (still doesn't hold a candle to 9800X3D with HT disabled though).",Negative
AMD,"Yeah, I somewhat agree with this. I suppose it depends if Intel’s latency problem with their P+E core design is at all a fixable one - 4c/8t is still shockingly serviceable for gaming, but 4c/4t absolutely is not.",Neutral
AMD,It's the same ratio as 285K 8P+16E vs AMD 16P and we know that 285K is competitive despite no hyperthreading,Neutral
AMD,"Sooo a few months ago, I helped a buddy of mine troubleshoot a black screen issue on his newly built 9800X3D and RTX 5090 rig, a fairly common issue with Nvidia’s latest GPUs.  While working on his PC, I'd notice a series of odd and random hiccups. For example, double clicking a window to maximize it would cause micro freezes. His monitor runs at 240Hz, and the cursor moves very smoothly, but dragging a window around felt like it was refreshing at 60Hz. Launching League of Legends would take upwards of 10+ seconds, and loading the actual game would briefly drop his FPS to the low 20s before going back to normal. Waking the system from sleep had a noticeable 2-3 seconds delay before the (wired) keyboard would respond, which is strange, considering the keyboard input was what wake the system up in the first place.  Apparently, some of these things also happen to him on his old 5800X3D system, and he thoughts that these little quirks were normal.  I did my due diligence on his AMD setup: updated the BIOS and chipset drivers, enabled EXPO profile, made sure Game Bar was enabled, set the power mode to Balanced. Basically, all the little things you need to do to get the X3D chip to play nice and left.  But man... I do not want to ever be on an AMD system.",Neutral
AMD,did they measure responsiveness and timed the click to action? and was it significantly different? how much difference are we talking about?,Neutral
AMD,"can you explain exactly what you're talking about here? are you talking about a situation where the system needs to do random reads from an ssd? aka: boot time, initial game load time?",Neutral
AMD,"How was ""system snappiness"" measured?",Neutral
AMD,No.,Neutral
AMD,"Now both AMD and Intel chips are “disaggregated “ which means between cpu and other agents like memory controllers, pcie, and storage there is higher latency than the 12/13/14th gen parts. AMD has higher latency due to the larger distances involved on the package.  Also Intel is not really improving the CPU core much. There won’t be a compelling reason to upgrade from a 14700 until DDR6 comes out. At least not in desktop. Nova lake high cache parts will cost $600 or more so value/dollar will be low.",Negative
AMD,So? Major upgrade for everything else,Neutral
AMD,"I had an 12600 not k. I had the opposite experience, I upgraded to a 7800x3d and the snappiness was a night and day upgrade. I can recommend a x3d to anyone. Pair that cpu with Windows 11 IoT LTSC and you have a winner <3",Positive
AMD,"Meant to say ""Gaming Performance""  >Higher avg on X3D  >similar or same 1% lows on both platforms >Higher .1% lows on Intel.",Neutral
AMD,"any comment that starts with ""I mean..""  I never go any further, its like some weird reddit think where everyone with ignorant comments seems to start out with this,  at least often anyway.",Negative
AMD,"""Enthusiast Segment"" my good sir. All the benches you see are poorly configured or stock 14900K. With tuning it's a different story. Intel craptorlake scales with fast ram.",Negative
AMD,"As a long time AMD user I know that Intel needs to be tuned to perform best. So when you tune the 14900K or even 285K you get like 20% performance uplift vs stock. X3D just performs great out of the box because of the huge L3 Cache. At the very least if you do not like microstutters or frame drops and want consistent gaming performance Intel 14th gen is superior vs current AMD's offering. Anyone with a specific board like Apex, Lightning, Tachyon, or even Gigabyte Refresh boards + i7/i9 13-14th gen with decent memory controller can achieve similar gaming experience. I'm speaking from experience since I also have a fully tuned 9950x3D/5090 on my testbench. For productivity task Intel feels much better to use as well. I feel like Intel is just better optimized for Windows and Productivity too.",Positive
AMD,"Actually Intel thermal is already better than Amd ever since Arrow Lake and Lunar Lake released. Even Core Ultra 7 258V is arround 10c cooler than Amd Z2E and Strix Point on the same watt.   On MSI Claw 8 AI+, Lunar Lake temp at 20w is just arround 62c while the Amd version is arround 70c. I wouldn't have a doubt Nova Lake and Panther Lake will also have good thermal because it will have 18A node with BPD and RibbonFET GAA which is more advance than traditional silicon when it comes to power delivery and efficiency.",Positive
AMD,Ah so bLLC on both tiles is a possible configuration? Any chance Intel actually goes for this?,Neutral
AMD,You can very simply get 9800x3D to 5.4 with little effort,Neutral
AMD,I haven't seen any of those issues on AMD where the underlying cause wouldn't also cause those issues on Intel.,Negative
AMD,"7800x3d here and never had these issues, came from intel",Neutral
AMD,Difference between 85MBps and 140MBps in q1t1 random reads and writes.,Neutral
AMD,Lets just ignore the whitepaper WD and Intel did about this.,Negative
AMD,So what configuration (tuning and ram settings) can a 14900k match a 9800x3d?,Neutral
AMD,That (Apple Silicon is good) and UMA are different stories I already know that Apple Silicon is good,Positive
AMD,On which benchmark(s) / metrics?,Neutral
AMD,"Haven't kept up with mobile since AMD 5000 and intel 10th gen, all I remember is intel needing XTU undervolting and then intel blocking XTU so using third party undervolt programs, AMD like I said 5000 never needed undervolting.   Desktop side, AMD 7000 is a hot mess, like seriously ridiculous. Seems that was sorted on 9000,    Let's wait and see on nova lake and zen6, like I said, brand loyalty is stupid, bought into AM5, so it's cheaper for me to go for zen6 with 244Mb of L3, but no am6-7 that will be intel.",Negative
AMD,"In theory, yes. For packaging reasons and market segmentation, probably not.",Neutral
AMD,"what's that on in terms of percentage, or seconds to person? was it noticeable?  i'm not techy enough, but is random reads and writes for clicking things and accessing data, and less so on copying and pasting a file?",Neutral
AMD,where is this whitepaper,Neutral
AMD,"You can send white papers all day but if most people buy these for gaming or productivity, AMD is winning in both categories.",Positive
AMD,Stock clocks 5.7/4.4/5.0 HT Off With DDR5 7600-8000 on Windows 10 22H2 or Windows 11 23H2 is enough to match 9800x3D at 5.4ghz with 6000 c28/6200 c28,Neutral
AMD,So MLID leaked in his most recent video about this (and he ranks it with a blue color code - 'very high confidence')  What do you think about this?,Neutral
AMD,[https://youtu.be/0dOjvdOOq04?t=283](https://youtu.be/0dOjvdOOq04?t=283) This explains it.  Gonna find the whitepapers link again.,Neutral
AMD,Based on what evidence?   I looked online for a few moments and found:  (1) Buildzoid from actually hardcore  overlocking did a 12 hour live stream where they couldn't even get a 8000 mhz overclock stable on the 14900k. No wonder why people haven't benched this lol  https://www.youtube.com/live/bCis9x_2IL0?si=ht3obVoBLcRFCyXI  (2) Plenty of benchmarks where an overclocked 9800x3d is about 10 percent faster than an overclocked 14900k with 7600 ddr5,Neutral
AMD,"Haven't you been listening? The conversation is strange (Confused) You first brought up the story of UMA, right?",Neutral
AMD,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Negative
AMD,"That channel has a lot of videos and even on this specific video it would help if you point the specific time you're referring to.  Now regarding AI, I assume you are talking about token generation speed and not prompt processing or training (for which Macs are lagging due to weak GPU compute).  I happen to have expertise in optimizing AI algorithms (see https://www.reddit.com/user/Karyo_Ten/comments/1jin6g5/memorybound_vs_computebound_deep_learning_llms/ )  The short answer is that consumers' PCs have been stuck with dual-channel RAM for very very long and with DDR5 the memory bandwidth is just 80GB/s to 100GB/s with overclocked memory.  Weakest M4 starts at 250GB/s or so and M4 Pro 400GB/s and M4 Max 540GB/s.  Slowest GPUs have 250GB/s, midrange about 800GB/s and 3090~4090 have 1000~1100GB/s with 5090 having 1800GB/s bandwidth. Laptop GPUs probably have 500~800GB/s bandwidth  LLMs token generation scales linearly with memory bandwidth, compute doesn't matter on any CPU/GPU from the past 5 years.  So by virtue of their fast memory Macs are easily 3x to 8x faster than PC on LLMs.  The rest is quite different though which is why what benchmark is important.",Neutral
AMD,"I'm highly suspicious, for packaging reasons if nothing else. I'm not going to call it impossible, but that would put enormous strain on the X dimension. Think about it, it's something on the order of 3-4x what the socket was originally for.  And obviously goes without saying, but MLID ""very high confidence"" doesn't mean shit.",Negative
AMD,"Based on my own testing , the games I play and benchmark results. Bullzoid is not relevant to this conversation. You can keep downvoting me for speaking the truth it's okay. I can't blame you because youtube and mainstream techspot are 99% misinformation. If you believe 9800x3D is 30% faster then go buy it, nobody is stopping you. I have investments with AMD and you're doing me a favor by supporting them.",Neutral
AMD,Just to be clear compute and hardware support for things like tensor cores have a massive impact. HBM is king but on older cards like the mi100 (released five years ago) can be out paced by a mid range card like the 4070.  All I wanted to convey is llm and token generation is a complex topic with limitations and struggles beyond memory bandwidth.,Neutral
AMD,"I haven't downvoted you at all, what are you even talking about.   You want people to believe in some conspiracy theory where any other information is a lie, and only you provide the ""real truth"".",Negative
AMD,"Besides, UMA wasn't first developed by Apple. Even if Intel introduces it, the software side or the software framework… Moreover, the OS side has to deal with it, so it is necessary to consider it a little. That's what you said earlier",Neutral
AMD,"I take no side there. I'm a dev, I want my code to be the fastest on all platforms  I have: - M4 Max so I can optimize on ARM and MacOS - Ryzen 9950X so I can optimize with AVX512 - in the process of buying an Intel 265K so I can tune multithreaded code to heterogeneous architecture.  The problem of Intel and AMD is segmentation between consumer and pro.  If Intel and AMD want to be competitive on AI they need 8-channel DDR5 (for 350~400GB/s), except that it's either professional realm (Threadripper's are 8-channel and EPYC's are 12-channel) with $800~1000 motherboards and $1500 CPUs and $1000 of RAM.  Or they make custom designsvwith soldered LPDDR5 like the current Ryzen AI Max 395, but it's still a paltry 256GB/s.  Now consumer _need_ fast memory. Those NPUs are worthless if the data doesn't get fetched fast enough. So I expect the next-gen CPUs (Zen 6 and Nova Lake) to be quad-channel by default (~200GB/s with DDR5) so they are at least in the same ballpark as M4 chip (but still 2x slower than M4 Pro and Max).  I also expect more soldered LPDDR5 builds in the coming year.",Neutral
AMD,"Have you not seen HardwareUnboxed 9070 XT Finewine video? Tell me why should I trust someone like that? I'm just sayin if you want real information test it yourself. I just don't trust product influencers like GamersNexus, HUB , Jayz2cents and other mainstream channels. Id rather buy the product and test it myself. Im not forcing you to believe what I said, its for people that actually knows what Raptorlake is capable off. If you want to see a properly tuned 14900k check out facegamefps on youtube. This is a very capable platform.",Neutral
AMD,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
AMD,Is it safe to install an Arrow Lake CPU without a third-party contact frame over the long run?,Neutral
AMD,"Edit: solved. Windows is capable of disabling e-cores on boot. The setting to turn them back on in MSCONFIG, boot tab, cores dropdown. Whatever feature causes this, I do not know but would love to find out, and furthermore their is no legitimate purpose for this feature, and Intel should prevent Microsoft from doing this, in my opinion, if there is any legal means available. Unless it interferes with Intel’s revenue.  Poor performance, incredibly low benchmarks. Beginning to suspect bad CPU.  I have tried bone stock with cleared CMOS settings. I have loaded multiple overclocking profiles, clearing CMOS in between. I have tried undervolting, and adjusting Load Line. I have changed Lite Load settings. I have tried with and without XMP enabled. Changed windows power plans. Nothing gets my cinebench r23 score above 11k. CPU-Z benchmarks it as 43% as fast as the previous generation CPU. I never overclocked this cpu, and I changed to the updated bios with new microcode the day it was available. CPU Purchased from Intel, retail box.  Windows 11 Build 26100.4484  CPU: Intel I9 14900K, stock clock, 360 AIO liquid cooling  RAM: Patriot Viper Venom DDR5 32GB (2x16) 7200MT/S CL34  GPU: MSI Ventus NVIDIA RTX 4080S  Motherboard: MSI Z790-S Pro Wifi with most recent bios  Storage: WD Black 2TB NVME on CPU; Toshiba 20GB X300 Pro  PSU: NZXT C1000 PSU (2022)  Display: Samsung Odyssey G93SC  I am at my wits end with this thing. I first noticed games crashing to desktop a few months ago. I thought it was just poorly coded (Helldivers 2). I checked my FPS in the game, and while previously I had to frame limit it 10 144FPS, I was now getting close to 60-80FPS.  Is my CPU a toaster or is there something I'm missing?  CPU-Z output, and HWiNFO64 sensor readings:  [https://imgur.com/a/ROuZOKS](https://imgur.com/a/ROuZOKS)",Neutral
AMD,"I bought a 265kf CPU from Amazon. On the checkout page it clearly stated that my purchase qualified for the Intel Spring Gaming Bundle and that I would receive an email with a Master Key. Well, I didn't receive anything and I spent all day being transferred from one Amazon support staff to another to no avail.  The promotion is literally still active and if I try to buy the CPU again it shows the same offer.  But somehow no one on Amazon or Intel support can tell me why I didn't get the email.",Neutral
AMD,Does anyone know if the upcoming Bartlett Lake-S 12 p-core no e-core CPU will suffer from the same stability issues as Intel 13th gen and 14th gen CPUs?,Negative
AMD,Any news to share regarding this link? https://www.reddit.com/r/intel/s/Bg4QnVzIdD,Neutral
AMD,"Bug report: Latest ARC driver 32.0.101.6972 causes crashing using Speed Sync  I get crashing in ALL games when attempting to use the new Speed Sync option on this driver.  Specs are:  Ryzen 9700x, Arc B580, 32GB DDR5 RAM, 2tb SSD.  Monitor is non VRR compatible (older gsync monitor ASUS PG348Q)  Hope this gets resolved as it sounds like a great feature.",Negative
AMD,"Hello,  I own a i9-14900K, mobo MSI MAG Z790 Tomahawk. I am wondering if it’s possible a specific program can cause my cores to be power limit exceeded, and is there any fix to that? Currently it drops the cpu speed to 0.8 GHz. This one program LeagueClient.exe for a game, League of Legends, has recently started to cause this problem, the only fix is to restart the computer. I have updated my bios, changed power limits within the bios, disabled c state, disabled EIST, disabled e-cores, tried to go into safe boot but the program won’t launch, reinstalled multiple times.",Negative
AMD,Has the degradation of the 13th and 14th gen CPU’s been fixed yet?,Neutral
AMD,"Been getting the following error A LOT while playing Expedition 33, and now after 18 hours I can't start the game without it crashing:  `LowLevelFatalError [File:C:\Hw5\Engine\Source\Runtime\RenderCore\Private\ShaderCodeArchive.cpp] [Line: 413] DecompressShaderWithOodleAndExtraLogging(): Could not decompress shader group with Oodle. Group Index: 760 Group IoStoreHash:52bcbf8ac813e7ee35697309 Group NumShaders: 29 Shader Index: 9301 Shader In-Group Index: 760 Shader Hash: 3BF30C4C9852D0D23B2DF59B4396FCC76BC3A80. The CPU (13th Gen Intel(R) Core(TM) i7-13700KF) may be unstable; for details see` [`http://www.radgametools.com/oodleintel.htm`](http://www.radgametools.com/oodleintel.htm)     Am I just screwed because I bought the wrong generation of Intel CPUs 2 years ago?",Negative
AMD,"I'm not sure if this matters, but when running `garuda-inxi` on my laptop, it tells me that my i3-8130U is Coffee Lake Gen 8, not Kaby Lake Gen 9.5. I've seen similar problems reported with users using CPU-Z, is this just a known bug or is there something else going on?  I'm going to be tweaking my CPU performance soon, so I'd like to make sure of the CPU capabilities/options first.",Negative
AMD,"I've been planing to build a PC from scratch for video editing and Ultra 7 265k and Arc B580 are good in my price bracket. Now, I'm worried with all the talk of Intel potentially going bankrupt or having layoffs, is it a good idea to still buy their products? Will we lose support with drivers and stuff like that?",Neutral
AMD,My gaming laptop Intel core i7-12700h runs at a constant temperature 95 . Doesn't matter if I'm playing a high end game like cyberpunk or some indie game like hollow knight. Is this thing suppose to always run at this temp?,Negative
AMD,"con el nuevo microcodigo de intel , los juegos de ubisoft (ASSASINS CRREED ODYSSE) tienen tiempos  de carga excecivamente altos, diria que al menos 10 veces mas de lo normal, Al devolverr la bios al microcodigo anterior todo funciona bien, cuando lo van a parchar?",Neutral
AMD,is it worth the upgrade for ai preformance cus my 5070 ti wont work for some reason so now my cpu is my main accelertor and user so should i upgrade to ultra 9? and i  will remove 5070 ti from my flair soon so yeah .,Negative
AMD,"ok now this is something. my Intel HD Graphics Control Panel is no longer there? no idea how long its been gone but i clearly remember it being there at a point.  using a Lenovo G510(i7-4700MQ, HD Graphics 4600)   Windows 10 Home 22H2 (Build 19045.6216)  windows apps in settings shows the intel driver but not the control panel   drivers from [lenovo's website](https://pcsupport.lenovo.com/in/en/products/laptops-and-netbooks/lenovo-g-series-laptops/lenovo-g510-notebook/20238/downloads/ds103802-intel-vga-driver-for-windows-10-64-bit-lenovo-g410-g510?category=Display%20and%20Video%20Graphics) are dated 16jul 2015. version seems to be 10.18.15.4240   drivers from [intel's website](https://www.intel.com/content/www/us/en/download/18388/intel-graphics-driver-for-windows-10-15-40-4th-gen.html?wapkw=intel%20hd%20graphics%204600) are dated 9jan 2015. version is 15.40.7.64.4279  now whats funny is that my device manager shows that my driver is dated 8mar 2017 which is version 20.19.15.4624   i also have another driver that i can see in the update drivers menu(drivers already present on my device) along with this one which is dated 29sep 2016 version 20.19.15.4531  i have tried reinstalling the driver from the update driver menu using the 8mar 2017 version, no change at all.  my drivers dont seem to be DCH drivers.   intel also says that they [discontinued the ms store version of the control panel](https://www.intel.com/content/www/us/en/support/articles/000058733/graphics.html) anyway.  this is all that i could think of writing here. any other details required just ask.   any help would be good lol",Neutral
AMD,"my i5-14600KF is being throttled at low temps and refuses to go past 0.8ghz of clock speed. Nothing I do seems to get it to stop throttling. According to throttlestop i have a red EDP OTHER ongoing throttle under CORE and RING. My average CPU temp is 31 degrees C across all cores and im getting 0.69 Voltage to my CPU  CPU: Intel i5-14600KF stock settings no overclock   GPU: Intel Arc B580 ONIX Odyssey BAR resizing enabled   Motherboard Gigabyte Ultra Durable Z790 S WIFI DDR4   RAM: Corsair Vengeance DDR4 16 GB x 2 (32GB)   Storage: 1TB Corsair MP600 CORE XT SSD + 2 TB WD Black SN770   PSU: Cooler Master MWE Gold 850 V2  EDIT: NEW INFO ACQUIRED   When running in safe mode and when booting into BIOS settings my CPU acts normally and receives typical voltage. Something running on my computer is throttling my CPU as i boot into windows. If i open Task Manager quickly after booting, I see system interrupts consume a mild amount of CPU before quickly going away, rather than sticking around when their is an ongoing hardware issue. I have reason to believe a program, either maliciously or due to error, is fucking with my CPU. Also worth noting is that Intel Graphics Software reports my CPU utilization as far higher than task manager, anywhere between 2-50% higher.",Negative
AMD,"Currently in the planning/purchasing phase of a small NVR/Steam Cache server. Information on VROC on X299 is pretty limited. so far I've seen mixed information on the drives supported. Before I purchase x4 Intel P4510 drives, I was hoping someone on here has a similar configuration that works.  The mobo manual states that only Intel based drives are supported but doesn't clarify which intel drives. Also saw on the intel forum that X299 CPU raid is further limited to only Optane based NVME drives. This drive will not be booted to, and I dont want to do a windows based raid.  My planned specs are:  CPU: 10900X  Mobo: X299 Taichi CLX - One of the few that seems to support bifurcation & VROC  Drive: Intel P4510 1TB  VROC Key: VRoc Standard  Are the Intel P4510 supported for Vroc on the x299 platform?",Neutral
AMD,"My Intel I210 ethernet device has device id 1531 meaning unprogrammed. The freebsd ethernet driver does not work with 1531. It needs device id 1533 meaning programmed. (Can I use a different driver? No, it's an embedded system that only supports this driver.)  I was linked this:  https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html  but 1) have no idea how to do it 2) cannot access the .bin file and tool it requires  Does anyone have ELI5 steps for getting the device to show devid 1533? Where can I get the .bin?",Negative
AMD,"The RMA process for Intel is absolutely atrocious. I don't know if  anyone can give advice on this but here is what is happening:  Based in Germany, for geolocation info. I was one of the early adopters of the 13th gen processors, but as I don't follow tech news too strongly I didn't find out about the issues with these chips until autumn 2024 when the issues I was having with my PC escalated to a point I couldn't ignore any further. Identified the CPU as the likely culprit and started the RMA process.  Firstly, Intel would not offer any solution where I could continue to use my PC whilst they analysed my CPU. As I use my PC for work, having it out of action for weeks/months was simply not an option, so I was forced to pause the RMA ticket whilst I saved up for a new cpu way ahead of my expected timeline.  With that aside, I reopened my ticket and the requirements they lay out are near impossible to meet - they wanted a clear photo of the matrix on the front of the chip and the matrix on the pcb itself.  The front of the chip was simple enough but on this series Intel printed the matrix in dark grey on a dark green pcb. It's barely visible just with the naked eye and I've tried so many ways to take a picture of the matrix with my smartphone and nothing I do is getting a clear picture.  I don't understand how your average consumer can possibly meet this requirement - solutions online apparently suggest purchasing a special type of expensive scanner or a macro lens for your smart phone? Which is ridiculous to me.  There is no way they cannot verify my chip with the rest of the information I have been able to give them, as well as far as I am aware, the matrix on the side of the chip on the 13th gen is literally the same as the one on the front of the chip.  The whole experience is proving to be awful, time consuming, feels like it should be illegal and has completely put me off ever using Intel again, or recommending it to anyone I help spec builds for.",Negative
AMD,"Hi, I have a Lenovo Ideapad with an Intel CPU and Intel GPU.  I find Windows font rendering unreadably faint. I have an astigmatism, and a light sensitivity, so even with prescription sunglasses I can't use dark mode, or bright screens, respectively.  I've tried finding Intel Graphics Software app settings which might help. The contrast settings quickly get too bright for my eyes, so they don't fix this. The right gamma settings might help though, one profile to make medium shades darker to make text bolder, and one to make them lighter to make images bolder.  I tried the support site here, but it has buggy scrolling which triggers my migraines, and it doesn't work with Firefox's reader view:  https://www.intel.com/content/www/us/en/support/products/80939/graphics.html#211011  How can I find or create these profiles? The Intel Graphics Software app doesn't seem to have an option to create profiles.  P.S. I can use the Windows Display Calibrator; if I ignore the instructions, and turn it as dark as possible during gamme, I get readable text at the end; if I turn it as light as possible, I get clearer images. But I can't see  a way to save those and switch without going through the whole rigamarole again.",Negative
AMD,"Can someone explain to me the differences between all the different names of the Intel CPUs, I’m new to laptops and am trying to learn. What’s the difference between 155H, 255H, and Meteor/Lunar/Arrow Lake and which one is “better”? All the different names get so confusing to me",Neutral
AMD,"I've been chasing down a crashing issue in a Unity program. Memtest86+ ran for 24 hours and cleared 18 passes so I was confident it's not RAM, and I started setting core affinity in an elimination pattern to see if it might be CPU related. I have discovered it crashes within minutes if I set affinity to Core 8, but it's rock solid on Cores 0-7 (haven't individually tested any of the cores past 8 yet but I have it on 9 now and it seems fine so far).  I have a 13900KF (not Overclocked) which is part of the batch that had the microcode issue. My BIOS was updated, but I am now suspicious of this core. Is it likely I should RMA this? Is there something else I should try first?  So far the system itself has been stable but this one Unity program crashes consistently, and I was having tab crashes in both Firefox and Chrome that also resolved when I removed Core 8 from their affinity...  The CPU passed the Intel CPU Diagnostic tool but because the crashes are so specific to core 8 it makes me suspicious.",Neutral
AMD,"u/Progenitor3  yes, it is generally safe to install an Arrow Lake CPU without a third-party contact frame over the long run. CPUs are designed to function properly with the standard mounting mechanisms provided by the manufacturer. Third-party contact frames are optional and may offer additional stability or cooling benefits, but they are not necessary for the safe operation of the CPU. Always ensure proper installation according to the manufacturer's guidelines to maintain optimal performance and safety.",Neutral
AMD,"u/TerminalCancerMan  Intel cannot comment or interpret results from third party benchmark tools. Run [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) to confirm if there are any issues with the CPU. You may try this, If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor need a replacement.",Neutral
AMD,"u/Progenitor3  Just fill in your info, and it’ll automatically create a ticket for you. Our team that handles those items will get in touch within 3–5 business days.   [Software Advantage Program](https://softwareoffer.intel.com/Support)",Neutral
AMD,Gotta wait for real-world tests to know for sure tho.,Neutral
AMD,"u/Special_Ad_7146 To stay on top of Intel news, **visit** our [Newsroom](https://newsroom.intel.com/).",Neutral
AMD,"u/Hippieman100  just checking can you confirm which software you're using? From what I’ve seen in the latest [ReleaseNotes\_101.6972.pdf](https://downloadmirror.intel.com/861295/ReleaseNotes_101.6972.pdf), there doesn’t seem to be a “Speed Sync” feature in the current version that comes with this driver. It does support V-Sync and Adaptive Sync though. Just wanted to clarify are you referring to the older Intel Arc Control software or the Intel Graphics Command Center? Just making sure we’re on the same page!",Neutral
AMD,"u/TheCupaCupa to better understand and isolate the issue, I kindly ask for some additional information. Please find the details requested below:   1. When the CPU drops to 0.8 GHz, do you notice any **error messages or warnings** in Windows or BIOS? 2. Does the issue happen **only with LeagueClient.exe**, or have you seen it with other programs too? 3. Have you checked **CPU temperatures and power draw** when the issue occurs? 4. Can you check **Event Viewer** for any critical errors or warnings around the time of the slowdown? 5. Are you using any **custom power plans** in Windows, or is it set to Balanced/High Performance? 6. Is **Intel Turbo Boost** enabled in BIOS? 7. **When did this issue first start happening?** Has it occurred before? 8. Have you made any software or hardware changes to the system recently?   Once I receive this information, I will be able to properly assess the situation and provide further assistance.",Neutral
AMD,"u/Alloyd11 Not all 13th and 14th generation processors show instability issues. Just to better assist you are you planning to buy or use one of these processors, or do you need help with your current system?  Let me know how I can support you!",Neutral
AMD,"u/amitsly “crashes” is a pretty broad term, and not every system issue points directly to the CPU. There are quite a few steps we go through to fully isolate the problem before concluding it’s a processor fault.  To help us assist you better, could you please share a bit more info about the crashes?  * When did the issue first start happening? * Have you made any recent changes to the system either hardware or software? * Is there any visible physical damage to the system? * What troubleshooting steps have you already tried? * Have you noticed any signs of overheating? * Have you tested the processor in another working system, or tried swapping it out to see if the issue follows the CPU?  The more details you can provide, the quicker we can get to the bottom of it!",Neutral
AMD,"u/Jay_377 its  likely comes from Intel’s naming convention. The i3-8130U is part of the 8th gen, but it falls under “Kaby Lake Refresh” (for mobile chips), not “Coffee Lake” (which is for desktops). Tools like `garuda-inxi` or `CPU-Z` might label it differently based on how they categorize architectures, not a bug, just naming differences.  [Intel® Core™ i3-8130U Processor](https://www.intel.com/content/www/us/en/products/sku/137977/intel-core-i38130u-processor-4m-cache-up-to-3-40-ghz/specifications.html)",Neutral
AMD,It is because Kaby Lake CPU's are 7th gen processors for laptops and 8th gen to 9th are Coffee Lake check rhis link for better understanding:https://www.intel.com/content/www/us/en/ark/products/codename/97787/products-formerly-coffee-lake.html,Neutral
AMD,"u/Reality_Bends33 Intel has been around for a long time and is known for making solid, reliable products, so you can feel confident about choosing them for your PC build. The Ultra 7 265k and Arc B580 are great picks for video editing, offering the performance you need without breaking the bank. While there’s been some discussion about Intel facing challenges, remember that big companies like Intel usually keep up with driver updates and support, even if they’re going through changes. The tech world is always evolving, and Intel is investing in new technologies to stay ahead. So, you’re likely to get the support you need for your products.",Positive
AMD,"u/unknownboy101 It’s normal for your processor to heat up during heavy tasks like gaming. Intel CPUs are built to manage heat by adjusting power and speed, so they stay safe and avoid damage. However, running at a constant **95°C** on your Intel Core i7-12700H even during light gaming is **not ideal** and could indicate a cooling issue. While Intel CPUs are designed to handle high temperatures and will throttle performance to avoid damage, consistently running near the thermal limit can shorten the lifespan of your components and affect performance. **Feel free to check out this article for more info or steps to try. Just a heads-up this is specifically meant for boxed-type processors. You can still take a look, but I strongly recommend reaching out to your laptop’s manufacturer to get help with the overall system configuration.**  [Overheating Symptoms and Troubleshooting for Intel® Boxed Processors](https://www.intel.com/content/www/us/en/support/articles/000005791/processors/intel-core-processors.html)  [Is It Bad If My Intel® Processor Frequently Approaches or Reaches Its...](https://www.intel.com/content/www/us/en/support/articles/000058679/processors.html)",Neutral
AMD,"u/Frost-sama96 Tenga en cuenta que solo puedo apoyarlo en inglés. He utilizado una herramienta de traducción web para traducir esta respuesta, por lo tanto, puede haber alguna traducción inexacta     **To help me dig a little deeper into the issue, could you share a few details?**  * What’s the **make and model** of your system? Is it a **laptop or desktop**? * Do you remember **when the issue first started** happening? * Which **BIOS version** are you referring to, the one that works fine? If you can share the exact version , that’d be super helpful.",Neutral
AMD,"u/Fluid-Analysis-2354 If your 5070 Ti isn't functioning and your CPU is now your main accelerator, upgrading to the Ultra 9 could be beneficial. The Ultra 9 offers enhanced AI performance, which can significantly improve your computing tasks. If AI capabilities are a priority for you, the upgrade is worth considering.",Positive
AMD,"u/BestSpaceBot , As with all good things, your product has reached the end of its interactive technical support life. However, you can find [Intel® Core™ i7-4700MQ Processor](https://www.intel.com/content/www/us/en/products/sku/75117/intel-core-i74700mq-processor-6m-cache-up-to-3-40-ghz/specifications.html) recommendations at [Intel Community forums](https://community.intel.com/) and additional information at the [Discontinued Products](https://www.intel.com/content/www/us/en/support/articles/000005733/graphics.html) other community members may still offer helpful insights or suggestions.. It is our pleasure to continue to serve you with the next generation of Intel innovation at [Intel.com](http://www.intel.com/). You may also visit this article for more details [Changes in Customer Support and Servicing Updates for Select Intel®...](https://www.intel.com/content/www/us/en/support/articles/000022396/processors.html)",Positive
AMD,"u/ken10wil   To help figure out what’s going on with your CPU throttling issue, I’d like to ask a few quick questions that’ll help me dig deeper:  * When did you first start noticing the problem? * Have you made any changes to your system recently like installing new software, updating drivers, or swapping hardware? * Is there any visible damage to your PC or loose connections? * Have you updated your BIOS to the latest version for your Gigabyte Z790 board? * Did you try resetting the BIOS to default settings to see if that helps? * Have you tried reapplying thermal paste to the CPU? Just to rule out any cooling contact issues. * Are there any startup programs or services that might be messing with your CPU? * What background processes pop up right after boot? You can check Task Manager or use Reliability Monitor to trace anything unusual. * Can you run the [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) and let me know if it passes or fails?  Let me know what you find happy to help you troubleshoot further once we have a bit more info!",Neutral
AMD,u/PM_pics_of_your_roof   Interesting thanks for pointing that out! Let me check this on my end and I’ll post an update here once I have accurate information.,Positive
AMD,u/UC20175 Let me check this on my end and I’ll post an update here once I have accurate information.,Neutral
AMD,"I don't think that there it is awful, I mean in the past where the issue blew out and many consumers are reporting. People at Intel can barely accommodate them but they still give the replacement needed for the consumer.   If i am right, that image is necessary for them to get your serial number and other stuff. Unless you have the box of the processor, or you did not purchase it as a tray then I think you'll be fine.",Neutral
AMD,"Hi, I think you'll need to connect with your laptop manufacturer or Microsoft to help you customize an ideal setting for your display. But could you share your full laptop model?",Neutral
AMD,I think this will help you https://www.intel.com/content/www/us/en/processors/processor-numbers.html,Positive
AMD,I fixed the problem. It was windows msconfig disabling e-cores on boot. You should forbid them from doing this.,Negative
AMD,"I can't remember which (not at my pc), I think it's intel graphics command center. I get a list of v-sync options, off, on, smooth, smart and speed (from memory). Speed was not available in previous versions of the software/driver. Underneath there is an FPS limiter and a control for latency improvement (I can't remember the name) with off, on, and on + boost options available.",Neutral
AMD,"Just got back to my PC, it's actually neither Arc Control or Intel Graphics Command Center. I'm using Intel® Graphics Software (25.26.1602.2), should I be using something else?",Neutral
AMD,"Hello, thank you and I'll try my best.  1. No error messages or warnings in Windows, unsure how to check BIOS. 2. Yes, currently I have only found this happens when LeagueClient.exe is started. 3. Using HWInfo64, right when I open LeagueClient.exe, all P and E cores have ""Yes"" in the Current column for ""Power Limit Exceeded"". Core temperatures are: Current- 33C Minimum- 31C Max- 72C Average- 42C. CPU Package Power, minimum is 65.699 W, maximum is 129.230 W. Upon starting the program, the maximum value doesn't change. 4. For Event Viewer, no warnings come up when LeagueClient.exe is started. 5. I have tried setting it to Balanced, High Performance, but I'm mainly on Ultimate Performance. 6. Yes, Intel Turbo Boost is enabled. 7. The issue first started happening 2 days ago, 07/25/25. No it has not occurred before. 8. I have not made any new hardware changes to the system. I did have a windows update, 2025-07 Cumulative Update Preview for Windows 11 Version 24H2 for x64 based Systems (KB5062660) (261.000.4770) installed on 07/25/25, however I installed this update later on during the day after the problem had already started.",Neutral
AMD,"Well, I didn't immediately blame the CPU. The crash message specifically mentions the issue that point the finger to the CPU, plus the provided link a saying that's the root cause.  Anyhow, here are more details:  1. The issue first started happening about 2 hours into my Expedition 33 playthrough and has happened at least 20 times since (in about 18 hours of gameplay) 2. I have not made any changes whatsoever to software or hardware before it started happening. Last night, after the 20th crash, I did run an update for various drivers & the BIOS (including the 0x12F update) using Gigabyte's CC software. 3. No physical damage that I could observe through the PC's glass window 4. I have tried everything the internet had to offer about this specific issue regarding Expedition 33. This includes verifying game files, changing config and settings, reinstalling, lowering the CPU clock speed and more.  5. I didn't notice the temperature when the crashes happened but I'll be on the look out. 6. I don't have a way to swap out the CPU nor do I have a replacement CPU",Neutral
AMD,"Weird, mine isn't in that list.",Negative
AMD,"I5 14600k desktop  When i try Odysse ACC, \*game\* I notice the start up and loading in to the game was so slow, like 8 or 10 minutes to Load.    \--Versión 182011.06 MB2025/05/21SHA-256 ：493D40A2351EED41FCF60E51346B9065880E87F986C1FD1FB1A3008E8C68DA26  ""Update the Intel microcode to version 0x12F to further improve system conditions that may contribute to Vmin Shift instability in Intel 13th and 14th Gen desktop-powered systems.   Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later. ""-- With this BIOS frrom ASUS page, The game do that   But, when i rollback to>   Versión 181211.04 MB2025/03/28SHA-256 ：98528167115E0B51B83304212FB0C7F7DD2DBB86F1C21833454E856D885C7EA0  ""Improve system performance and stability      Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later.""  Intel microcode 0x12B t  The game Load A lot faster and run well.  I dont know what this happens, but is a error caused by New microcode",Negative
AMD,"\- I started noticing the problem between august the 19th and august the 20th.  \- I updated the system around the time I \*noticed\* the problem, but I am not sure if that is when the issue started. I reverted the machine to a state prior to the update and the problem still occurs. Any other driver updates or software installs happened after I had already noticed the problem, and were initiated trying to solve the problem  \- There is no visible damage to my pc or wires  \- my Bios is up to date, and aside from turning on resizable BAR months ago for my GPU, my bios settings are default. I reset to default and turned resizable BAR on again just to be sure, and the issue still occurred.  \- I have not, mostly because it would be a hassle and because the CPU has remained very cool. I have manually overclocked it as a temporary solution to the problem and even under these conditions it is averaging 35-40 degrees Celsius when idle and hovers around 50 degrees when under stress  \- While the issue did not occur in Safe Mode, I am not sure which program is causing the throttling. I have disabled all installed startup programs and still get throttled.  \- No unusual processes pop up right after boot, though the sum of all processes hits 15% CPU utilization, stays there for a while and then the CPU throttling begins (all happens in less than 30 seconds after boot). I could see what happens after disabling even the security related startup programs and see if there is any difference  \- I pass the PDT tests but I have abysmal performance on various benchmark tests, and the PDT takes a long time to complete. Overclocking leads to more expected results.",Neutral
AMD,"Thank you. I spent some more time looking and it appears VROC on x299 was discontinued sometime in the past two years? Seems intel pushed people towards RST. I guess the question still stands since VROC on X299 has moved into sustaining mode, so drives that maybe came out during that time should still work.      [https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html](https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html)",Neutral
AMD,What I've gathered from support so far is documents needed by section 2.14/2.15 of https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html are behind a premier account registration. I'll try registering an account.,Neutral
AMD,"I'm mostly referring to the fact they've designed the matrix to be printed near invisibly with it being absolutely miniscule and basically the same colour as the pcb, then made it a mandatory requirement for the RMA process.   It reeks of a company trying to dodge responsibility for faulty products by making the hoops so difficult to jump through theres no reasonable expectation the average consumer will be capable of complying.   Why do I need to go out and purchase a macro lens for my smart phone? Its putting me out of even more money than I already am.",Negative
AMD,Ideapad 1 15IAU7.,Neutral
AMD,"u/Hippieman100 **Intel Graphics Command Center** and **Intel Arc Control** have been the go-to software for many users, but they’re being phased out soon, and support will be limited moving forward.  Since system (not your PC) includes an **Intel Arc B580**, I highly recommend switching to **Intel Graphics Software** instead. This newer software is bundled with the graphics driver package, which you can find at the link I’ll provide-[Intel® Arc™ & Iris® Xe Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html). Before making the switch, please take a moment to read through all the details and driver descriptions on that page.  Also, based on my checks, it looks like you're currently using **driver version 25.26.1602.2 on your PC**, which is outdated.  Let me know if you have any questions.",Neutral
AMD,"u/TheCupaCupa If the motherboard BIOS allows, disable Turbo and run the system to see if the issues continues.  If the instability ceases with Turbo disabled, please let me know.",Neutral
AMD,"u/amitsly For further analysis, please provide the crash dump or log files generated by the game. You can follow the guide and scroll down to the section titled **""Crashing/Freezing Issues/BSOD""-**[Need help? Reporting a bug or issue? - PLEASE READ THIS FIRST! - Intel Community](https://community.intel.com/t5/Intel-ARC-Graphics/Need-help-Reporting-a-bug-or-issue-with-Arc-GPU-PLEASE-READ-THIS/m-p/1494429#M5057) for instructions.  Once you’ve obtained the files, kindly notify me so I can send you a private message to collect the logs.  For isolation purposes, please try the following step and let me know the outcome:   **If your motherboard BIOS allows it, disable Turbo Boost and observe whether the system crashes continues.**",Neutral
AMD,Yeah that's weird try checking qith laptop manufacturer about it,Negative
AMD,"u/ken10wil  Thanks for the detailed info - this is really helpful for narrowing things down. What I'm seeing here points to a software issue rather than hardware failure. The sudden onset timeline, passing PDT tests, and the fact that safe mode works fine all suggest you're dealing with a software or driver problem.   Your CPU temps are totally normal too, so I can rule out thermal throttling. That 15% CPU usage spike right before throttling kicks in is actually a big clue - something's definitely triggering this behavior.     [Information about Temperature for Intel® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [How to Know the Idle Temperature of Intel® Processor](https://www.intel.com/content/www/us/en/support/articles/000090343/processors.html)  [What Is Throttling and How Can It Be Resolved?](https://www.intel.com/content/www/us/en/support/articles/000088048/processors.html)  For overclocking: Note that the motherboard does have an impact on the ability to overclock. Some are better quality and more capable than others. Furthermore, the way the motherboard is set up also impacts the overclocking ability of a particular system (such as liquid cooler vs fan). Please note that if the system was overclocked, including voltage/frequency beyond the processor supported specifications, your processor voids warranty.   **Next steps to try:** • **Check Windows power settings** \- sometimes updates mess with power profiles. Set to ""High Performance"" and see if that helps •   **Look at that 15% CPU spike** \- open Task Manager right after boot and sort by CPU usage to catch what's eating those cycles •   **Try disabling Windows security temporarily** \- sometimes antivirus can cause weird throttling behavior •   **Check Event Viewer** \- look for any error messages around the time throttling starts • **Check Reliability Monitor** \- go to Control Panel > Security and Maintenance > Reliability Monitor to spot any anomaly issues or critical events around August 19-20thThe fact that overclocking fixes it temporarily suggests your CPU is being artificially limited by software, not hardware.   Since you've already tried the obvious stuff like BIOS reset and driver rollbacks, you're probably looking at a Windows service or background process gone rogue. Keep me posted on what you find with that CPU usage spike and reliability monitor - those are likely our smoking guns!",Neutral
AMD,"u/PM_pics_of_your_roof  You're absolutely correct about VROC on X299 being discontinued. Intel moved VROC for X299 platforms into sustaining mode within the past two years and has shifted focus toward Intel RST (Rapid Storage Technology) for consumer applications. While VROC is no longer actively developed, it remains supported in sustaining mode for existing users, which means NVMe drives that were released during VROC's active development period (roughly 2017-2022) should still function properly. However, newer drives may work but won't receive official validation or certification. For anyone building new systems, Intel recommends using RST instead of VROC, but existing X299 users can continue using VROC with supported drives from the qualified vendor list. ***Support is now limited to sustaining mode with no new features or drive certifications planned, as confirmed in the support article you referenced.***",Neutral
AMD,"u/UC20175  Per your inquiry:  The necessary tools and firmware files, but they are only accessible through Intel’s Resource and Design Center (RDC), which requires a Premier account.  1. Register for an Intel RDC Premier Account 2. Visit: Intel RDC Registration Guide [https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html](https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html) 3. Use a corporate email address for faster approval. 4. From the RDC, search following Content IDs:  EEPROM Access Tool (EAT) – Content ID: 572162  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162)  Production NVM Images for I210/I211 – Content ID: 513655  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655)",Neutral
AMD,I guess you are the only one who is having trouble on that part. I mean I haven't seen anyone get mad about it. So I don't really seem to find any fault I mean it is their preventive measure for fake processors that are out there. Just ask for some help to take a picture it's not that hard,Negative
AMD,"I think you should revert the Gpu setting to default. Then start changing some settings in the Microsoft setting. There are settings for fonts, brightness, and such. There is also some setting for the smoothness of the animation in the Windows",Neutral
AMD,"You have misunderstood, I am using intel graphics software already as I said, 25.26.1602.2 is the version number of that software. My driver's are up to date according to intel graphics software.",Neutral
AMD,"Hello,    great news, it solved itself. I opened the program this morning and everything was fine. If this happens again, I will try disabling Turbo and see what happens. Thank you for the help!",Positive
AMD,So what is the QVL list for x299 VROC? I can’t find that information. I can find QVL for C621,Neutral
AMD,"So I was able to download the files from the RDC. Currently when I run        eeupdate64e.efi /NIC=2 /DATA Dev_Start_I210_Copper_NOMNG_4Mb_A2.bin  It says ""Only /INVM* and /MAC commands are valid for this adapter. Unsupported."" This is for three 8086-1531 Intel(R) I210 Blank NVM Device. Presumably in order to program them with device id 1533, I need to run a different command or use a different tool?  Thank you for your help, I feel like it is very close to working.  Edit: I believe it works, using /INVMUPDATE",Neutral
AMD,"[This is literally what I am dealing with.](https://imgur.com/a/WBowyjO)  I'm not even worried about posting this image because please tell me how to get a better photo than this with a regular smart phone, and you can barely see there's anything there.",Negative
AMD,"Yes, I've turned off the system animations, and the blinking cursors.  There aren't settings to switch fonts, to replace thin/faint fonts with bold ones. The folks developing Windows like Segoe Ui, maybe they can read it. There are regedit hacks, but I'm not sure how to do them. There is also Winaero Tweaker, which can switch some fonts, but doesn't work on some older apps with unreadable text. There is MacType, which can bolden text without switching fonts, but doesn't work everywhere either.  There are settings to scale fonts, or scale everything, or reduce resolution. I've tried every combination of these. I've reduced resolution, since it does work everywhere. But I can't go below 1280x720, and I end up breaking some interfaces anyway. There is also the magnifier, but it gives me a migraine.  Here's the thing. If text is too thin/faint, making it 2x as bold will take less than 2x the screen space; making it 2x as big will take 4x the screen space.  I'm surprised how well tweaking gamma has worked. So far it has worked everywhere with dark text on light images, and it's worked very well.",Negative
AMD,"u/Hippieman100 Ah, I see I overlooked that part! Looks like this is the installer version, my apologies for the confusion since we were discussing three different software options earlier.  Alrighty, since you're using the latest version now, how can I help? Are you running into any issues with the new application, or is there a specific feature that's not working as expected?",Neutral
AMD,"u/TheCupaCupa Great to hear it fixed itself! If it happens again, trying Turbo off sounds like a good plan.",Positive
AMD,"u/PM_pics_of_your_roof  You're encountering a common issue, Intel doesn't maintain a centralized QVL (Qualified Vendor List) specifically for X299 VROC at the platform level. Since X299 is a consumer chipset, the VROC compatibility lists were typically maintained by individual motherboard manufacturers rather than Intel directly. The C621 QVL you found is for Intel's server/workstation chipset, which has more formal validation processes. For X299 VROC compatibility, you'll need to check with your specific motherboard manufacturer (ASUS, MSI, Gigabyte, etc.) as they would have maintained their own compatibility lists during VROC's active period. However, since X299 VROC is now in sustaining mode, many manufacturers may no longer actively update these lists. Your best bet is to search for your specific motherboard model's support page or contact the manufacturer directly, though given the discontinued status, this information may be limited or archived.",Neutral
AMD,Relax 😅 and follow this:https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html,Neutral
AMD,"Yeah switching fonts, I haven't tried it. But in windows settings you can change its size and such. Try looking at the Microsoft community or support changing it. But I am glad that adjusting the gamma helps.",Neutral
AMD,"Thank you for the reply. Sadly asrock doesn’t have a QVL for nvme drives in U.2 form factor or enterprise grade drives, just m.2. Let alone a QVL for VROC support. Looks like I’ll be the test subject for this.",Negative
AMD,"That is what I'm following, I provided support everything on that list, including decoding the matrix on the front of the chip but they don't proceed with the ticket until they get a picture of the code on the pcb which is nigh on impossible.  I wound up taking it into the office to use a high resolution scanner thats used to digitise paintings and just about got something readable. I remain firm in my stance that whilst wanting to verify a serial number isn't outrageous, making the process in which you verify said information virtually impossible is.   I simply don't understand how regular people can take such a detailed photo using smartphone technology. My phone is 4 years old and was absolutely not capable (Samsung S20+)",Negative
AMD,u/PM_pics_of_your_roof  **Best of luck with your configuration!** Your pioneering work might just pave the way for others looking to implement similar setups.,Positive
AMD,"u/Danderlyon  **just sent you a message, check your inbox when you get a chance!**",Neutral
AMD,"LPCAMM2 and CAMM2 are not the same thing, CAMM2 utilizes the same DDR modules as conventional DIMMs, LPCAMM2 it for laptops and it utilizes LPDDR modules, you don't want LPDDR memory on your desktop, it has awful timings/latency.",Negative
AMD,">Faster DDR5 DRAM chips are coming, even at high capacities, but existing DIMM standard is a bottleneck due to signal degradation.   Intel is already running MRDIMMs at 8800 MT/s with 12800 MT/s planned on a conventional DIMM socket. The DIMM format can handle the frequency, but bottleneck is the DRAM itself, so MRDIMMs basically RAID two 4400 MT/s memories (and in future 6400 MT/s) on a single DIMM to get around the the limited speed of the DRAM. Signal integrity is evidently not a problem past 10 GT/s, so don't expect CAMM to suddenly unlock more performance from existing DDR5 DRAMs or to make the performance hit from multiple modules dramatically smaller (although it may help somewhat).   >Does that mean it would be prudent to wait a bit with new MoBos purchase ?   IMO no, at least not with DDR5. We will see if DDR6 at >12 GT/s runs into issues with DIMM, but that is still a ways off.    >LP/CAMM2 apparently brings many other benefits, not just frequency bump. It shoudl be compatible with LP/DDR6, allow line-load-reducing, clk regen ( like CUDIMM), MRDIMM data rate doubling (friggin cool!), registering, ECC etcetc   The format allows those memory types, but putting a CAMM with LPDDR into a system that doesn't support LPDDR is not going to work. Similarly to DIMM, the CPU still has to support a feature, so don't expect things like MRDIMMs to suddenly work on consumer hardware.",Neutral
AMD,"We need more speed, further, faster, farther. Where is the easy button for all this?",Negative
AMD,"I'm at Computex right now talking with both motherboard makers and memory makers. At this point, they're not seeing a ton of demand for CAMM2 modules in the enthusiast space, so I don't think this year will be the year for mainstream adoption. I could see adoption picking up for laptops and for full systems (see the ASUS TUF T500 that launched a couple days ago).   I don't have a crystal ball, but I'd almost think the major transition will happen with DDR6 - at this point, if you have DDR5 in a DIMM already and are upgrading your rig while reusing the same DDR5... why would you re-buy it as CAMM2?",Neutral
AMD,">you don't want LPDDR memory on your desktop, it has awful timings/latency.  Ryzen AI Max uses LPDDR5 and achieves 256GB/s memory bandwidth vs 100GB/s at most for 2-channel DDR5 on regular PC.  Apple M4 is the same. M4 Pro is 400GB/s and M4 Max is 540GB/s. Ultra is 800GB/s reaching recent GPU speed.",Negative
AMD,"Yeah, this is unfortunately the big problem.. Convincing mobo makers to switch.  I think it will have to start with Laptops right? And it'll have to come from consumers not wanting normal SODIMMs or soldered RAM.. So maybe a few years yet :( .",Negative
AMD,"Yeah and they still have awful memory timings, you are confusing memory channels and bandwidth with memory latency.",Negative
AMD,"Latency kills any graphics application that need to display at 60fps, 144fps or more, one blip and frames drop.",Negative
AMD,"Guess, I'll stay on bios v1701 on my Strix Z890-H then. I have already set my cache ratio to 40 and the NGU and D2D to 32...  My E-cores are set to 48 and my RAM is running at 8200MT/s in gear 2 with the XMP Tweaked profile.  Haven't touched any voltage settings and my system is stable. I have no idea if this can be improved but I'm quite happy with the performance at the moment.",Neutral
AMD,How to retain my previous undervolting settings along with this new S200 Boost mode? I have an ASRock Z890 Riptide WiFi board and a Core Ultra 9 285K.,Neutral
AMD,I'm having issues since I enabled S200 boost mode. I can't seem to get my RAM to run stable without xmp. If I run both RAM settings and boost mode I hard crash at random. What RAM settings do you guys use when using boost mode?,Negative
AMD,"Yes for xtu if 200s is off, but once 200s boost is active, xtu doesn't let you do anything. No undervolting, no multiplier. Even if vmd and undervolt protection is off, xtu wom't let you. 200s boost is useless, just use performance intel bios setting, xpm profile and undervolt in xtu amd get a bigger boost than 200s boost.",Negative
AMD,"Yeah, 200s boost is trash. I had better performance with an undervolting 0 OC, than no possible undervolting with XTU. The 265k sees no benifit from this. I disabled it, undervolted, and added 53x and 47x and have better perf.",Negative
AMD,"Sigh, I reverted to old bios only allow me to use xtu.",Negative
AMD,"200s boost is basically Intel sanctioned overclocking that does not void your warranty. Ive enabled the feature, and I'm pretty happy with how much my ram latency dropped (9.6ns) and how I'm also getting improved 1% lows as a result.   You are free to disable it, over clock and tune until your heart's content. But calling it useless is a bit of a stretch.",Positive
AMD,Are you able to undervolt using xtu even 200s boost is disabled?,Neutral
AMD,"Rumors and alleged hardware id captures suggest that yes, more discrete GPU models are coming.  It appears that Intel has cancelled the higher end Battlemage Xe2 GPUs though, which considering the performance of the B580, it's a shame if we don't get to see big Battlemage.      But Celestial is allegedly part of the tile set for the upcoming Panther Lake chips as Xe3 cores, and Druid (Xe4) is supposedly pretty far along in production if not close to being finished.     But all of that is just rumors at this point.  Who the heck knows what will happen?  The company got a new CEO and it looks like he's trying to make Intel slim down a bit.",Neutral
AMD,I'm not sure Intel itself knows that all things considered,Neutral
AMD,"dgpus/igpus are a marketable side effect of producing server farm gpus, so probably.",Neutral
AMD,Even most people at Intel probably don’t know at this point.  Intel’s new CEO did say that Intel will get rid of its “non-core business” to focus on its “core business” and “expand that using AI and Software 2.0”.,Neutral
AMD,"Intel's [got job listings for the Arc division](https://www.pcguide.com/news/intel-isnt-giving-up-on-discrete-gpus-as-new-job-listings-reveal-the-company-is-shooting-for-much-much-higher/) and have stated they're ""shooting for much much higher"" so presumably we're at least going to get Celestial and Druid. If they both flop then maybe Arc'll get canned but otherwise I'm fairly certain Arc's here to stay.",Neutral
AMD,* I see similarities between the engineering philosophy of Intel GPU's to AMD  Bro can share me some of your weed ?,Neutral
AMD,Given that Tan has said that he believes in you need to ship products to learn from them.  I don't see him canceling many chip programs that have future product development needs.  Intel needs ship products to get people into their eco system and GPU development is needed for AI chip development.  Either way I would not expect much rumors until closer to Thursday and Intel announces earnings.,Neutral
AMD,"In the land of HPC, AI workloads, GPU is king. If they stop making GPU then they stop being relevant. They already goofed up big time by not making GPU last two decades. If they deprioritise dGPU now then they will never be able to catch up.",Negative
AMD,"I do hope Intel continues.  Intel HAS to develop shaders and tensors for iGPU and NPUs anyway. Might as well get more out of it by releasing dGPU cards as well.  Nvidia completely abandoned the low end of the market. It's likely a low margin product, but with high volumes. The market used to be mostly sub 300 € GPUs.  As for mid tier and high tier card, Nvidia sells their VRAM like it's made of money, and AMD is hopelessly behind in software. Words can't describe how bad AMD is at accelerating anything.   Give how quickly Intel figured out driver, I am more hopeful Intel can figure out an acceleration stack to make use of their NPU units.",Neutral
AMD,"My best is they'll slim down the division and not release discrete cards. They need to keep the r and d up to make their igpu competitive.   Intel need to save money first and foremost, so I would think layoffs are coming fairly soon",Neutral
AMD,"I hope they keep trying. While not perfect (of course)  , they had good showing with ""only"" 2 gens of products. But they really need to fix gpu usage, which seems kind of low right now. They would need to fix this before launching high end gpus, because right now going larger doesn't seem to scale very well for their arch.",Positive
AMD,"Do they even produce B570/580?   Not even available, or for a stupid price.",Negative
AMD,"> But Celestial is allegedly part of the tile set for the upcoming Panther Lake chips as Xe3 cores  PTL has an Xe3 iGPU. Celestial is the name for a (once) future dGPU line. Xe3-derived dGPUs are dead, however. The last shred of hope would be something Xe4-based years down the line.",Neutral
AMD,And it has also allowed intel to make proper integrated graphics on the laptop side,Positive
AMD,"Clearly,  As we know, network administrators pass their time by playing video games on server farms they manage.",Neutral
AMD,"He mostly means, Mobileye etc. Not GPU's... Though that might not also make the cut.  I think new CEO is more likely to split the foundry than to cut GPU's given their importance in AI. I don't think either of these is going to happen.",Negative
AMD,"Yeah, but GPUs and WiFi chips are both part of the intel package, even if they dropped discrete gpus they still need to develop igpus and ai accelerators, so it's probably not actually that big of a cost",Neutral
AMD,"Yep, GPUs were Pat’s project, most likely gone with the new guy’s focus on essentials.",Neutral
AMD,"> AMD is hopelessly behind in software  If Nvidia drivers are anything to go by, they may be trying to catch up to AMD's Glory days.",Negative
AMD,"How can they save money if they have to develop the software parts all by themselves ? They can't, and that's why they need as much help from the open-source community as possible. That means putting GPUs into their hands.",Negative
AMD,“Hardware Unboxed said the same.” 🤣,Neutral
AMD,Here’s to hoping that the overhead issue gets fixed soon so that we could have better Intel gpu’s,Positive
AMD,"Or much simpler explanation. It didn't scale well to make economic sense?  If anything overhead issue is less of an issue on a premium higher performance cards, as people are more likely to overpay for their cpu in higher price ranges.",Negative
AMD,"Overhead issue is when we use older cpus like 9400 or 3600  If intel is to make high end GPU , it would be obviously bought buy those having good enough cpu as well like 5700x or 12600k  It would be stupid imo to buy a high end GPU (whether it's amd .intel or nvidia ) with older CPUs  The real reason could be potential b770 wasn't giving satisfactory performance uplift to be considered as mid-high end GPU that's why they cancelled b770 and now looking forward c770 i.e celestial",Negative
AMD,"I have not seen anything that indicates that it's cancelled except -more often wrong than right- Moore's law is dead.  In fact Intel product head, said that they would continue with discrete in this January. Though didn't specify if they will ever go for a higher end card.",Negative
AMD,"I can't find any good info saying Celestial cards with Xe3/Xe3p are cancelled or dead.  Peterson himself said the work for Celestial dGPUs is finished, and they have been spotted in data pulls on Linux.  I don't believe either one of those things mean there will never be a Xe3/Xe3p dGPU.",Negative
AMD,stop watching Moore's law is dead and stating his videos as facts,Negative
AMD,"He is referring to any money-losing side project.  To survive, Intel needs to tighten its belt to conserve money, just as AMD did between 2011 and 2017.  There is no way that Intel would abandon GPU development, as ML/AI is a very lucrative market, but most users here are talking specifically about gaming GPUs.",Neutral
AMD,"There will still be Intel GPUs for ML/Al (because that’s where the money is), but I don’t think that that is what most users here are talking about.",Neutral
AMD,"> Or much simpler explanation. It didn't scale well to make economic sense?  That's pretty much the entire Arc lineup  >If anything overhead issue is less of an issue on a premium higher performance cards, as people are more likely to overpay for their cpu in higher price ranges.  Imagine that it makes twice as many draw calls.  Even the Ryzen 7 9800X3D is going to be overloaded.",Negative
AMD,Imagine that it makes twice as many draw calls.  Even the Ryzen 7 9800X3D is going to be overloaded.,Negative
AMD,"> I have not seen anything that indicates that it's cancelled except -more often wrong than right- Moore's law is dead.  That would be a broken clock right twice a day kind of situation. Gelsinger cancelled Celestial a few months before he was fired.   > In fact Intel product head, said that they would continue with discrete in this January  The wording was ""continued investment"", which means precisely nothing at all. A single future driver update for BMG would count.",Negative
AMD,"> Peterson himself said the work for Celestial dGPUs is finished  No, he said nothing at all about dGPUs. He didn't even name Celestial at all. His comments were that Xe3 was essentially finished, because that's the PTL iGPU, and we know that's more or less done from a hardware standpoint.",Neutral
AMD,Who ever said I got that from him?,Negative
AMD,"Not to mention, nVidia is also coming to eat Intel's lunch, with an APU of its own (and surely there'll be consumer versions too).   It would be absolutely suicidal for Intel to get rid of its GPU business unit.",Negative
AMD,"You don't know what you're talking about, you never did, I always read your posts and you act as if you know what you're talking about.  Intel's job hiring proves you're just a clueless redditor making things up https://www.pcguide.com/news/intel-isnt-giving-up-on-discrete-gpus-as-new-job-listings-reveal-the-company-is-shooting-for-much-much-higher/",Negative
AMD,"Fair enough, but that still does not make the point that there will be no Celestial dGPU release.  If xe3 is all the way done and allegedly popping up on high performance test strings on Linux, it would follow that sometime soon we will see Celestial in the graphics card market.     Again, there seems to be no evidence that Celestial is cancelled on dGPUs.  The only evidence found is that the uArch is developed and will, in tile form, be an igpu.",Neutral
AMD,"Lmao, people continue to be in denial about the things everyone else everyone knows. Tell me, is it still ""FUD"" that Intel's using N3 for ARL? Or that BMG is 4060-tier?  Btw, you can't even find that job link.",Negative
AMD,"No *public* evidence, at least. Intel has been cagey on this for a reason. Would simplify things if they just admit what their roadmap (or lack thereof) is.",Negative
AMD,"Notice how you started to move the goal posts, you can literally find the job postings yourself with a simple Google search.  Also, maybe you should do something else in life than sitting on reddit for over a decade spewing BS.",Negative
AMD,"Yeah if they layed out a real honest roadmap and stuck with it, it would be better for everyone.",Positive
AMD,I’m running 1703 with the Z890 EXTREME. Zero issues.,Positive
AMD,Release date for 200s boost bios?,Neutral
AMD,"I don't usually get the board list/release notes until Friday. However, roll out starts during that week, so you can start checking Tuesday - Thursday, but I don't have a better date for you at this time.",Neutral
AMD,it's available if you have an apex:  https://dlcdnets.asus.com/pub/ASUS/mb/BIOS/ROG-MAXIMUS-Z890-APEX-ASUS-1801.ZIP  probably soon otherwise tho,Neutral
AMD,Thank you but actually i am looking for Z890 TUF-PLUS.,Neutral
AMD,Intel 200S boost available on version 2001  [https://dlcdnets.asus.com/pub/ASUS/mb/BIOS/TUF-GAMING-Z890-PLUS-WIFI-ASUS-2001.ZIP?model=TUF%20GAMING%20Z890-PLUS%20WIFI](https://dlcdnets.asus.com/pub/ASUS/mb/BIOS/TUF-GAMING-Z890-PLUS-WIFI-ASUS-2001.ZIP?model=TUF%20GAMING%20Z890-PLUS%20WIFI),Neutral
AMD,"One thing also to note is that because of the way our website syncs, sometimes you might find a newer BIOS on a different regional website (although this doesn't happen often). So, if you see someone say they see it, and you don't, try to find out which regional website they saw the BIOS update listed.",Neutral
AMD,b860 tiene soporte para 200s boost?,Neutral
