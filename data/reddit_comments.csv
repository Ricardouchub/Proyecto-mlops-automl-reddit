brand,comment_id,text,subreddit,created_utc,score
Intel,ntyjuf7,Why does this need an article? It's a tweet by an official account praising their own product.,hardware,2025-12-14 10:28:41,114
Intel,ntynem1,"The B580 has 200W TDP, in a perfect world and TDP scales linearly, the B770 would be 50% faster, that would put it around the 5060Ti/9060XT.  If the price also scales linearly, that would be around 375€, seeing that the 9060XT is going for 350€ now, it's gonna be tough competition.",hardware,2025-12-14 11:02:45,45
Intel,ntypvez,Im really looking forward to panther lake X. 4-4-4 core configuration and Xe3 iGPU with sr-iov is perfect for running a Linux-Windows mixed vm environment without having to get a gaming laptop with a dedicated GPU for virtualisation.,hardware,2025-12-14 11:26:24,7
Intel,ntyixk6,I hope the Linux driver support and performance is good in these,hardware,2025-12-14 10:19:48,19
Intel,nu8l0vn,"Intel ARC needs to maintain their momentum. They have an excellent pricing strategy and genuinely compelling features, it's time they released a card that competes in the midrange. And no, I don't count the A770. As a B580 owner, increased ARC adoption rates will be sure to benefit all cards in the range, so I really hope that intel is committed for the long-haul here. They are not in the position to be burning consumers anymore",hardware,2025-12-15 22:51:09,4
Intel,nu1xwz6,"Releasing a GPU more than 1 year after the B580 came out seems weird to me. Unless this is a new architecture, or is using Intel's own process, and fabs.",hardware,2025-12-14 22:08:28,7
Intel,ntyjaqa,"4070 performance for $350-400, I'm calling it now.",hardware,2025-12-14 10:23:22,9
Intel,ntyxo3a,Hopefully they've seen Nvidia and AMD fuck things up by having two VRAM configurations and know not to do that.,hardware,2025-12-14 12:35:17,6
Intel,ntz74a1,"300W? Sounds like they're chasing the big boys. Hope the performance justifies the power draw, leaks can be misleading.",hardware,2025-12-14 13:43:36,2
Intel,ntyfbh6,"Hello Revolutionary_Pain56! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-14 09:44:11,1
Intel,nu28x8z,They wouldn’t need a B770 or mystery GPU if they actually released more than just a B50 to the masses.,hardware,2025-12-14 23:06:54,1
Intel,ntzwyfc,"I don't know what the driver situation is like a year later, but B580 was anywhere between a 4060ti and a 3060 (or less if the driver really choked), so comparing B770 to a single Nvidia point of reference probably isn't the whole story.  Intel has been selling a big chip with a lot of hardware relative to what they charge, so when the drivers work Battlemage can punch way above its price class. I expect the same this time.",hardware,2025-12-14 16:10:14,1
Intel,nu3z9nt,"It's been deleted, so it might even be inaccurate.",hardware,2025-12-15 05:31:41,10
Intel,nu1v2ky,Ad revenue.,hardware,2025-12-14 21:53:55,7
Intel,ntzk2x5,Trying to apply logic or rules to the internet is a waste of time.,hardware,2025-12-14 15:02:28,18
Intel,ntyxt68,"Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds. That however will mean a bigger die and viability might be questionable (considering they're already massive for the performance).",hardware,2025-12-14 12:36:26,26
Intel,ntzjh8i,"Price doesn't scale linearly because die sizes make defects scale quadratically. so pricing is the same, 2 50mm\^2 dies are cheaper than 1 100mm\^2 die     However in GPUs there is a fixed cost for every GPU so there is a sweet spot",hardware,2025-12-14 14:59:01,12
Intel,ntz8bta,"As always, TDP is a semi-arbitrary figure and has little to do with what the GPU requires.  Most GPU's of today have heavily inflated TDP's simply to try and juice benchmarks on review day as much as possible.",hardware,2025-12-14 13:51:23,5
Intel,nu40836,"The BMG-G31 is supposed to have 32 Xe cores in 8 render slices on a 256-bit memory bus, compared to the 20 Xe cores and 5 render slices on a 192-bit memory bus for the BMG-G21. Unless Battlemage is seriously memory bandwidth-limited, it should be almost 50% more performant.  The only question is die size. If it's 50% larger than the 270 mm^2 BMG-G21, that would exceed 400 mm^2. The GB203 in the RTX 5080 is 378 mm^2 for context.",hardware,2025-12-15 05:39:14,2
Intel,nu49a7b,"That ""perfect world"" of yours seems to violate basic physics though...",hardware,2025-12-15 06:56:09,1
Intel,ntywdzf,With tdp of 300 w it better be RTx 5070 or 9070 territory for much low price,hardware,2025-12-14 12:24:39,-1
Intel,nvdrawd,Intel never confirmed SR-IOV on Panther Lake - did they?,hardware,2025-12-22 15:23:42,1
Intel,nu14qjw,"You can choose between high performance and crashes (xe) or low performance and stable (i915), and with Intel firing linux devs left and right I wouldn't expect much improvement any time soon.",hardware,2025-12-14 19:45:02,10
Intel,ntz5xy4,That would be an amazing value proposition.,hardware,2025-12-14 13:35:49,5
Intel,ntypyrq,Rtx 5070 16gb for 380$,hardware,2025-12-14 11:27:18,-1
Intel,nu2970r,I’d be happy if they didn’t gate the Arc Pro B60 behind bad distributors.,hardware,2025-12-14 23:08:26,3
Intel,ntz0y7e,"So banking on the hope, that *everyone* ***else*** *somehow falls behind by accident*, only for Intel to succeed?  If that's their business-plan (looking at their foundry-woes, it seems it is), that's an awfully idiotic business-model.  ---- Last thing I heard, was redditors moaning about en masse that monopolies are bad. *Which one is it?!*",hardware,2025-12-14 13:00:48,-21
Intel,nu8khh7,"The B50 is not a gaming GPU and actually underperforms in gaming tasks compared the the B580. They need to have an actual range of cards, not just a budget option, and even more budget option, and a server/workstation GPU. The B770 is essential to compete in the midrange",hardware,2025-12-15 22:48:12,1
Intel,nu8kn1x,"A year later the drivers are fantastic, seriously not even a single hiccup. Been playing Hogwarts legacy at 4k 60fps with Xess Quality upscaling, and no frame gen.",hardware,2025-12-15 22:49:02,1
Intel,ntyyf0i,"> Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds.   The problem with this idea, is that this would cost them far more money, as you need more die space, which they already use relatively inefficiently compared to nVidia.  They can't really afford not to use every bit of die space they have for all that its worth.",hardware,2025-12-14 12:41:19,20
Intel,nu40pti,"Battlemage doesn't have the ability to add more Xe cores per render slice, this is something Intel has changed for Xe3. The BMG-G31 will have 128 ROPs, the same as an RX 9070 XT, or more than an RTX 5080.",hardware,2025-12-15 05:43:07,2
Intel,nu49ukp,"FWIW that is not how pricing structures work. It's a bit more complex than that.   E.g. defect rates scale with die size, that is true. But larger dies also have more budget for DFM structures, that can lead to fewer overall functional faults than smaller dies and more variability and binning opportunities. So although smaller dies tend to be on average cheaper, it is not necessarily that 2 dies half the size will be cheaper than the twice as large single die.",hardware,2025-12-15 07:01:13,1
Intel,nvdwqwo,"afaik it works on every iGPU since skylake, but the driver is not in the mainline kernel",hardware,2025-12-22 15:51:22,1
Intel,nu2hk6w,I'm using an Arc A770 right now in Linux.  With i915 performance was unusably (for me) low.  With xe it's been fine.,hardware,2025-12-14 23:54:28,6
Intel,nu2tqb3,"the driver is already open source right? i think it will get better over time on virtue of being open source, but relying on intel to fix it now probably isnt gonna pan out.",hardware,2025-12-15 01:01:50,0
Intel,ntz7z9x,"Well it kinda has to be, the 4070 came out nearly three years ago.",hardware,2025-12-14 13:49:08,24
Intel,nu096mb,5060 performance for twice the price isn't a good deal.,hardware,2025-12-14 17:11:40,-2
Intel,ntz10sx,"I could see that. Nvidia really bailed out Intel by making the 5070 not much faster than the 4070 without using MFG to cheat lol   Edit: for all the Nvidiots downvoting, [the truth hurts](https://www.techspot.com/review/2960-nvidia-geforce-rtx-5070/#RT-1440p-png)",hardware,2025-12-14 13:01:20,6
Intel,ntz7opy,This seems an absurd overreaction. All I'm saying is they don't do a 5060ti or 9060xt situation where there's a 8 gig model and a 16 gig model.,hardware,2025-12-14 13:47:16,19
Intel,nubvjrs,"It’s not but the B50 is the only Arc Pro that isn’t gated behind a bad vendor like Hydratech.    If they can’t properly launch the B60, why should I trust Intel or it’s partners with the B770 or some mystery GPU?",hardware,2025-12-16 13:27:34,1
Intel,nu1491m,5060 is not nearly as performant as the 4070,hardware,2025-12-14 19:42:34,19
Intel,nt8w4et,"Interesting results. If this is representative for consumer laptops, Panther Lake is a much bigger upgrade than most here, including me, expected. But it almost seems too good to be true somehow.",hardware,2025-12-10 06:01:38,26
Intel,nt7pdwj,is Geekbench a CPU or a GPU benchmark?,hardware,2025-12-10 01:17:56,12
Intel,nt7pbu4,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-10 01:17:35,1
Intel,ntafnry,How does this compare to the Snapdragon X2 Elite?,hardware,2025-12-10 13:56:04,1
Intel,nuahcx5,4 pcores  8ecores 4 lpcores..,hardware,2025-12-16 06:05:52,1
Intel,ntcj3az,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:16:10,1
Intel,ntbju2x,"Is Intel just ""squeezing the toothpaste"" again ? Even a low-frequency single-core 288V gets 2,700+ on Geekbench, while the 285H gets 2,600+ in single-core and 14,785 on multi-core. Therefore, TL;DR: I don't see Panther Lake being a huge improvement over the current Alder/Arrow Lake pairing. We will have to wait and see the power consumption, though.",hardware,2025-12-10 17:23:27,1
Intel,nte6d5x,"I’m sorry, but that’s awful? Only 9% better single core when it has a better node and a newer architecture? Compared to what Apple and Qualcomm achieve every year, that’s pathetic",hardware,2025-12-11 01:36:38,0
Intel,nt8wwpg,"Probably because GeekBench 6 only scales to a certain point, where more cores won’t help with improving performance compared to improving core IPC",hardware,2025-12-10 06:08:21,8
Intel,nt9ghvf,"Fr, I really need to get a new light laptop (bc my old one's hinge is broken), but starting to feel  like I'd be better off waiting for Panther Lake than compromising with a bulky gaming laptop....",hardware,2025-12-10 09:13:58,5
Intel,ntcj6hz,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:16:37,-2
Intel,nt7pvq6,cpu,hardware,2025-12-10 01:20:55,17
Intel,nt7t1it,"Probably one of the worst benchmarks out there for multicore tbh, I’d be more curious about the cb r24 scores",hardware,2025-12-10 01:40:02,14
Intel,ntdv8jz,"Panther Lake doesn't bring any major changes to the cores. It's mainly about bringing node shrink, redesigned SoC, and new iGPU.",hardware,2025-12-11 00:29:31,1
Intel,ntcpemm,"It’s obvious that this is the viewpoint of an outsider. Professionals would never look at it this way. Professionals first evaluate a processor based on its specifications, features, and process technology. Lunar Lake and Arrow Lake are *not* using some outdated process — they use TSMC’s then-most-advanced N3B node, Intel’s first time adopting it. Meanwhile, Panther Lake uses Intel’s own **18A** process.  Based on the current benchmark results, Intel’s 18A appears to outperform TSMC’s N3B by at least the same margin that **Intel 4** trailed behind N3B — which is an astonishing result.  Every day you hear people saying how much TSMC has advanced, how far ahead its processes are, how “outdated” Intel’s nodes are, how AMD’s processors using TSMC have excellent efficiency. These kinds of statements have been repeated endlessly over the past decade.  Yet today, Intel is using its newest process node to **clearly surpass** TSMC’s top process from just one year ago.",hardware,2025-12-10 20:47:39,-1
Intel,ntep84w,"the real test will be how many watts the X9 388H needs to achieve its scores, because the 285HX needed like 90 watts to achieve its scores  so if the X9 could hit its scores while on its base TDP (65 watts) then thats a \~40% increase in efficiency, not bad",hardware,2025-12-11 03:32:35,4
Intel,ntfpawi,"But it does that while clocked almost 6% lower, so the IPC gain is actually decent. Especially considering most people expected Panther Lake to be a side grade because of the small architectural changes on the cores.",hardware,2025-12-11 08:27:12,2
Intel,nthte2d,"For the use cases of PTL, Geekbench (which is mostly consumer focused) is a good indicator.  It doesn't assume that its workloads are perfectly parallel, it assumes some threads are used more heavily than others, so its value in nT is influenced by its 1T.    If someone is using this for rendering or other highly parallelizable workloads they might want to look into a subtest or into an alternative benchmark, but for typical consumers it seems like Geekbench is a good approximation of their experience.",hardware,2025-12-11 17:03:33,2
Intel,ntckb4u,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:22:19,-2
Intel,ntgubsm,"Is macOS not an option? Because IMO, MacBook Air is *the* thin and light laptop to get, hands down.",hardware,2025-12-11 14:03:57,1
Intel,ntfhexr,"Couple of subtests leverage some new arm vector instructions and get huge scores, but those have limited influence to the overall score. Apple is better across the board, though the difference isn’t as big as the overall score suggests.   One difference is that since geekbench is distributed as binary it’s compiled more directly for apple architectures specifically while others use more generic targets. But that has very limited effect.",hardware,2025-12-11 07:10:24,2
Intel,nt7rc6b,But they have a gpu compute test too,hardware,2025-12-10 01:29:45,13
Intel,nt8452s,"Geekbench claims it's much more realistic than those multicore tests that scale nearly perfectly with tons of cores, and I think that's a fair take. It's not as if they didn't know how to create a benchmark that scales like other nT tests do, geekbench 5 nT does that.   I wouldn't call it worse, just different.",hardware,2025-12-10 02:45:40,28
Intel,ntfm9jb,Geekbench runs common workloads as they are commonly implemented. It gives you a score on how well a multi core implementation of that workload would actually run in that CPU.   I think that is far more useful than some perfectly parallel workload measuring max power and core count.,hardware,2025-12-11 07:57:01,5
Intel,ntco5ms,"I have carefully compared the various models across Geekbench, PassMark, and the differences between Meteor Lake, Arrow Lake, and Lunar Lake. If my judgment is correct, the theoretical peak performance of the 484 in Cinebench R23 should reach around **24,500**; the 285H scores **22,500**. Compared with the 285H, it should be easier for the 484 to achieve high scores because its power requirements are significantly lower than the previous generation built on TSMC N3B.  Its peak performance will not be extremely strong because the frequency is not high. IPC is likely improved by around **10–11%**, but clock speeds drop by about **6%**. Overall, that means single-core performance should only rise by **4–5%**.  The improvement will be most noticeable in Geekbench. Since PassMark single-core also shows gains, the IPC uplift and resulting single-core increase should be quite certain. If Geekbench were the only source, it would still be questionable, but PassMark is more solid and has higher reference value.  Overall, in terms of peak performance, the uplift is average—around **10%**, close to that figure.  However, the real key is the **efficiency gains**. I believe they will be excellent. Compared with the 285H, which requires **65 W** to reach **20,000** points in Cinebench R23, I estimate that the **388H** may only need **40–45 W**.  I also estimate that the Cinebench R24 score should fall around **1300–1400**. Compared with Qualcomm’s X Elite 2 at **1950**, there is still a significant gap—but the two products differ drastically in scale.  Overall, Panther Lake’s greatest achievements lie in several aspects:  1. **Energy efficiency** — likely the best among all x86 products. 2. **Performance per mm²** — excellent. For example, the 484: if you look at its die shot, the total area of the CPU (including the CPU tile’s 4P and 4 LPE cores and all caches) is essentially equal to the die area of a traditional monolithic 8-core design. That means the 484 uses the same silicon resources as past 8-core chips, yet **no AMD mobile 8-core processor surpasses it**, either in raw performance or efficiency. 3. It also offers better performance-per-area than Qualcomm’s processors. The X Elite 2 has **18 cores**, including **12 “very large” cores**—similar in size to Intel P-cores—and **6 large cores**, each larger than Intel’s E-cores. The die area of this chip is **2.5× larger** than Panther Lake 484’s.",hardware,2025-12-10 20:41:27,-4
Intel,nthu15b,"I mean, it does bring SOME changes to the cores, both are a next generation, it just isn't a more radical change like will be happening with NVL.  A mid single digit improvement is still pretty decent.",hardware,2025-12-11 17:06:47,2
Intel,ntd3lso,">The benchmark is very friendly to ARM and least favorable to AMD.   How so?   >The only valid reference is **same-generation, same-architecture comparisons**,  Geekbench is nice because it explicitly allows cross ISA comparisons. You don't have to take my word on it either, Intel and AMD themselves have used geekbench before to compare themselves to the ARM competition.   Same thing applies to spec and cinebench 2024.",hardware,2025-12-10 21:57:00,10
Intel,ntsl9xc,My old 14900hx gets 35k multi core in cinebench r23,hardware,2025-12-13 10:15:38,1
Intel,ntfgs1k,"What's with the Cinebench fascination? At any rate. Geekbench 6 runs a raytracing test, and the 388H leak shows it at 29700 points compared to a 285H scoring 25300 points. That would place the Cinebench R23 scores at about 20% higher for the 388H  https://browser.geekbench.com/v6/cpu/15500755  https://browser.geekbench.com/v6/cpu/15474224  At any rate, the reason Geekbench doesn't scale perfectly with more threads is because a lot of workloads hit scaling limits due to Amdahl's Law, or memory bandwidth limitations. This applies to SPECint and SPECfp results for multiple threads as well.",hardware,2025-12-11 07:04:32,1
Intel,ntgxa5r,"lemme know once UTAU and Fighter Maker 2002 works on Arm macOS    (my point is that I work with a lot of old abandonware apps that barely even run on x86, so there's no chance in hell they gonna work on macOS)",hardware,2025-12-11 14:20:52,1
Intel,nt9ouc7,"Thus useless to compare high CPU core counts.  If you actually need more than 8 cores you also have workloads that scale much better than Geekbench 6. It's especially dumb to claim this CPU is close to a 16 core, 32 thread zen 5 cpu based on Geekbench...",hardware,2025-12-10 10:37:37,2
Intel,nvdnrp5,I have workloads that scale fine with 16 threads and would scale fine with 32. People who actually buy high end multicore CPUs have a use for them.,hardware,2025-12-22 15:05:12,1
Intel,nt8fz34,It runs for far too short a time to reflect accurate multi-core performance.  People don't get a multitude of cores to run a task for a few seconds.  They do it for tasks that take minutes or hours to complete.  I'd argue it spends too little time on single-core tests as well.  I don't trust it to provide any useful information about anything other than transient performance.,hardware,2025-12-10 04:00:09,-2
Intel,nt8esdi,I agree but the problem is it's being mindlessly used to compare MT scores as in this article.,hardware,2025-12-10 03:52:12,-1
Intel,ntd4133,Bro there's no need to spam this same comment like 4x in the same post's comment section T-T,hardware,2025-12-10 21:59:01,7
Intel,ntduxlv,"Why's the scaling ""problematic""? Its nT scaling is by design because GB6 is trying to replicate common consumer workloads which are rarely embarrassingly parallel. If you wanna see how well nT scaling for rendering is, there's cinebench for that.",hardware,2025-12-11 00:27:43,6
Intel,ntwk3p9,"was your 35,000 score achieved with power consumption above 100W? Can you try it now at 80W and see how many points are left? Also, limit it to 40W and check if it can reach 20,000 points. Because I estimate that the 388H has a chance to hit 20,000 points at 40W.",hardware,2025-12-14 00:54:40,2
Intel,ntmv22s,"Ok, a simple no would have been fine.   Seems like those extremely old apps would run on any old POS x86 machine, if anything harder to run on modern hardware but hey what do I know. Best of luck.",hardware,2025-12-12 12:29:41,1
Intel,nt8x38v,"I mean, shortness is more of a problem if a device can cool itself properly or not rather than a problem of the CPU itself, unless said CPU in question is impossible to cool in that form factor",hardware,2025-12-10 06:09:56,14
Intel,nt9ufxf,Geekbench correlates with SPEC really well while taking a fraction of the time to run. Making it run for more minutes changes nothing,hardware,2025-12-10 11:28:32,6
Intel,nt8wp7g,"It provides useful information about the chip itself to real computer architecture enjoyers. Idk if gb6 changed it but geekbench has historically correlated with spec scores. Longer running programs like cinebench test the whole system including the thermal solution but geekbench gives a much better view into the pure performance of the cpu itself (and the associated memory system :/). Besides, you can always slap on a bigger cooler if thermals are that limiting.",hardware,2025-12-10 06:06:35,8
Intel,nt8mbai,"It's being used for comparison because that's what we have. AFAIK, this is the *only* 388H benchmark we have",hardware,2025-12-10 04:44:37,9
Intel,ntgnaao,That looks like an AI post to me,hardware,2025-12-11 13:22:24,2
Intel,nt9re78,It has been going hayway since SME just like GB5 had issues with AES Skewing results,hardware,2025-12-10 11:01:14,1
Intel,nt92449,Fair enough but I'd rather they kept something similar to GB5 multicore test in addition to their new 'more realistic' one.,hardware,2025-12-10 06:54:12,-2
Intel,nvfvk61,Incredible hardware news. Thanks for the share.,hardware,2025-12-22 21:50:44,20
Intel,nvl1r29,"This was sarcasm, by the way. A video from Usagi Electric on how computers count isn't hardware related but this is? OK mods.",hardware,2025-12-23 18:20:13,12
Intel,nu6hk69,"Frankly, I wouldn't buy one for gaming, though I must admit Battlemage is pretty sweet for video editors thanks to 10-bit AV1 and 4:4:4 chroma on the HEVC side + you also get two codec engines (at least on the B580 with the same G21 core).  For perspective, you'll have to move up to Nvidia GB203 (RTX5070Ti), or better, to get your hands on two or more NVENC engines for the same 10-bit AV1 + 4:4:4 H.265.  If I was a serious video editor, this is *the* graphics card I would get.",hardware,2025-12-15 16:37:33,46
Intel,nu6rlfl,The main bit that intrigues me about these ARC GPUs is their Linux gaming performance & how they compare to their windows performance.,hardware,2025-12-15 17:26:31,12
Intel,nu7ksue,Not in the same system the 1080 ti was in.,hardware,2025-12-15 19:47:27,7
Intel,nue205x,"One interesting data point is he's testing with 7500f. We have no comparison with contemporaries or higher end CPU to examine CPU bottleneck, but it's a realistic scenario and system for the card.  Interesting how that 1gig made all the difference in TLOU2",hardware,2025-12-16 20:01:26,3
Intel,nu6oprg,"idk why, intel gpu is so expensive in my country like bruh that gpu perform worse than cheaper nvidia/amd. those sucker trying to scam buyer just cause ""intel"" name in it.",hardware,2025-12-15 17:12:19,1
Intel,nu6jvho,Yeah intel's quick sync is very good at video editing and streaming as well. Even preferred over nvenc in streaming (no idea about video editing),hardware,2025-12-15 16:48:43,15
Intel,nuacl0f,"I would and I did (Intel B50 gpu).  So far, zero regrets and zero issues on linux.  Edit: Fedora for those that are curious.",hardware,2025-12-16 05:26:38,14
Intel,nugufgj,Rumour has it Linus torvalds uses an Intel card because he wanted something on a budget that could drive dual 6k screens.,hardware,2025-12-17 05:53:53,2
Intel,nu8m2qq,"For desktop use Intel on Linux is great, but gaming performance and compatibility is horrifically bad.",hardware,2025-12-15 22:57:03,3
Intel,nwpbxq3,Iceberg's intention always been to show realistic performance and average user would get with prices imaginable lol. Otherwise for highest possible performance people would prefer gn or hub.,hardware,2025-12-30 09:19:38,1
Intel,nu75g90,"it's the retailers, they don't sell well and need higher margins",hardware,2025-12-15 18:32:51,12
Intel,nwpc21t,"Yeah retailer margin and taxes made arc GPUs very less desirable, in my country b580 is close to rtx 5060 in price",hardware,2025-12-30 09:20:46,2
Intel,nuitvat,Its not a rumor lol he did a video with Linus tech tips and specifically requested they put a b580 in the PC they built him  [link to video](https://youtu.be/mfv0V1SxbNA?si=jT_3dFy1H40vrVjk),hardware,2025-12-17 15:13:42,5
Intel,nuc491o,"Performance is a little worse than on windows, but compatibility is not horrifically bad. It's pretty much the same as on windows.",hardware,2025-12-16 14:18:10,9
Intel,nuj4t5k,"I believe Linus Torvalds wanted an ARC Pro B50, but settled for the B580 because that's what LMG could get their hands on",hardware,2025-12-17 16:07:30,5
Intel,nuzrua9,can you imagine a bunch of nerds whispering about which graphics card an old man uses?,hardware,2025-12-20 06:16:26,1
Intel,nuf0jlu,"I'm curious as to how much worse. I've considered an upgrade to a B570 due to them being seen for £150 new, putting it into used RX 6600/6600 XT territory, but if the Linux performance of say a B580 on Linux falls closer to either or, then it's probably not a worthwhile choice over the used AMD options for me.",hardware,2025-12-16 22:58:15,3
Intel,nskpbhm,"TLDW:    GPU Models Tested: MSI Shadow 2X RTX 5050, Intel Arc B580 FE      16 games average:    1080P, High-Ultra Settings:     Native TAA: Arc B580 is 14% faster, 23% faster at 1% lows due to higher VRAM        DLSS 4 Quality vs XeSS Ultra Quality: Arc B580 is ~11% faster     DLSS 4 Quality XeSS Quality: Arc B580 is ~20% faster     DLSS 4 Balanced XeSS Balanced: Arc B580 is ~15% faster     DLSS 4 Performance vs XeSS Performance: Arc B580 is ~14% faster",hardware,2025-12-06 10:33:27,74
Intel,nskzrhs,"""There was a time, about a decade ago when the $250 price tag offered solid products, but the world has changed""  Yep, inflation. $250 in 2015 money is $342 in todays money. And you can get a very solid product at that price tier, the RX 9060 XT is $369 on Newegg.  GPU prices haven't gone up, you money is just worth way less.",hardware,2025-12-06 12:12:49,135
Intel,nsl4bqs,"5050 really has no right to exist at the price it does. B580 is obviously being sold at near cost or even a loss however, it's not exactly a fair comparison but that doesn't matter to consumers.  If you just want to game then I can't see any reason to consider anything else at this price point.",hardware,2025-12-06 12:50:37,46
Intel,nspebmq,I'd still probably go nvidia here as I don't trust intel's compatibility with older titles and the like.   Still it would probably be better to spend $20 more on a 9060 xt 8 gb or $50 more on a 5060 than either of these.,hardware,2025-12-07 03:40:56,8
Intel,nsm5bxh,"If UE5 games generally run this poor on Intel GPUs, there might be trouble ahead as there are lots of those games in the pipeline.  You still couldn't get ~~more~~ me to buy an Intel GPU, even if I was desperate for a cheap GPU right now. I'd just adjust my settings.",hardware,2025-12-06 16:32:46,8
Intel,nszc9eq,"The B580 is decent enough, but it might be better to just save a bit more and get a 16GB 9060 XT for $350 or something. That card is likely to last 10 years flat at this point, and it will definitely last at least 5.  And yes the 5050 is not good. Getting something with a half-decent iGPU would be a better use of your money at that point.",hardware,2025-12-08 18:55:08,1
Intel,nt0zpd3,The biggest issue is that he did not test PCIE 3.0 vs 4.0 vs 5.0. Those GPUs are very likely to go into budget builds or as upgrades to older motherboards like the B450.,hardware,2025-12-09 00:04:12,1
Intel,nsnsg7n,Imagine spending $250 on a GPU when you could literally just save $100 more for like a 100% percent more performance.,hardware,2025-12-06 21:49:06,-6
Intel,nskuu1l,Wait isn't xess a lower resolution per quality setting?,hardware,2025-12-06 11:27:34,37
Intel,nsuy6fa,"yeah there is a reason why there are 5050s for 210  the thing is a sub 200 dollar GPU, which matches it capability and vram well, its more or less the I want to step up from igpu deal",hardware,2025-12-08 00:59:57,3
Intel,nslxyj3,Maybe for the low end but high end I can’t even buy a card at msrp outside of America.,hardware,2025-12-06 15:53:05,31
Intel,nspes5s,"*ignores that this is a 50 tier product and should be compared with the 950 and 1050*  This kinda of ""but but inflation"" virtue signalling I'd very unhelpful to these kinds of discussions. It's as if you're saying people should stop complaining gpus are several times more expensive than they used to be with the actual low end market completely destroyed.",hardware,2025-12-07 03:43:50,7
Intel,nsl2hua,Tech is supposed to beat inflation. Look at monitors or TVs or SSDs (before now) or CPUs or ....,hardware,2025-12-06 12:36:00,12
Intel,nt09q01,All the tech tubers are just turning into old men shouting at clouds. They will probably all be replaced by younger people living in the now soon enough.,hardware,2025-12-08 21:40:56,1
Intel,nslaf1q,"First of all 250 euros bought way more gpu in 2015 than 360 does today. And the lower end and midrange gpus were much less cut down vs the high end chipa today.  A 5050 sits where the 750ti did when the 980ti was out. Now you get entry level performance for mid end prices  Have wages actually increased that much? Because that is the only useful measure of ""inflation"". Everything else is just corporate profits   If prices for everything go up but wages don't then that leaves less money for frivolous shit like ram and storage and laptops and consoles, not more.  Even in my country where our wages are automatically indexed to match inflation, our purchasing power has dropped because the actual cost of living isn't properly represented in whichever calculation is used for the inflation number.  Houses have gone up by 100+ percent since 2015, rents have gone up by over 60 percent, grocery prices have more than doubled, utility prices have risen sharply, public transport has more than tripled in cost.  Minor expenses like clothing or a tv you buy every ten years have stayed flat, but that isnt what people are spending 80 percent of their income on.",hardware,2025-12-06 13:33:32,-9
Intel,nsl9m9r,"Shh, everyone knows that prices only go up on luxury goods due to evil corporations, after all how will people live without their computer not being 800% faster than last year?",hardware,2025-12-06 13:28:13,-7
Intel,nsmb10n,"> Yep, inflation. $250 in 2015 money is $342 in todays money.   People really need to stop using CPI. I can bet you that GPUs don't make it to the market basket. Yes, your money's value has fallen but not by that much.",hardware,2025-12-06 17:02:45,-6
Intel,nslqts2,You're getting downvoted for speaking the truth.  The RTX 5050 should be a $150-180 GPU for the price and value it offers but unfortunately people are gonna defend the price tag that the card was set for by Nvidia,hardware,2025-12-06 15:13:39,26
Intel,nspdyuf,"For a while you could get them for $229, which would be more acceptable vs a 5060 for $299, making it the same FPS/$. But the 5060 is actually the one on sale right now for only $30-$35 more. 30% faster for like 12% more money.",hardware,2025-12-07 03:38:38,1
Intel,nsn70s5,"I'd say that the cheapest new GPU that I'd blanket recommend with no ifs, buts and caveats is the 9060XT 16GB, everything below that either struggles with outright performance, VRAM or software issues like Arc.",hardware,2025-12-06 19:49:40,12
Intel,nsmn51w,"Intel checks all the right boxes on paper (generous VRAM, decent pricing compared to competitors, an alternative to the duopoly) but the recent CPU overhead stuff coupled with the crapshoot that is trying to play older games and it just isn't worth it",hardware,2025-12-06 18:06:54,7
Intel,nsl1d2d,"Yeah they cover this at the start of the video https://youtu.be/lLe5AP6igjw?t=229   XeSS 1.3 shifts everything down a tier, so their quality scaling ratio is everyone elses balanced ratio.  Older versions of XeSS match DLSS/FSR scaling ratios.",hardware,2025-12-06 12:26:39,37
Intel,nskx602,"I know quality is, not 100% sure about others. dlss quality preset uses higher resolution than XeSS and FSR quality presets",hardware,2025-12-06 11:49:24,2
Intel,nsxjyy3,Its fine for people who need a dGPU but not a beast for work. think stuff like CAD or Photoshop. It will also be fine for people who only play competitive multiplayer games.,hardware,2025-12-08 13:22:59,1
Intel,nsxk5xm,Nvidia cards are bellow MSRP here in eastern europe. AMD cards slightly above MSRP.,hardware,2025-12-08 13:24:15,2
Intel,nt0aiwr,"They are all selling below MSRP in the UK. £979 is MSRP for a 5080 and I can buy 3 in stock models for less than that price without much searching, at scan.co.uk.  If you are in South America its probably your countries insane import taxes, protecting their home grown GPU market lol.  29 upvotes from children who have not bothered to check or do any kind of reasoning.",hardware,2025-12-08 21:44:54,1
Intel,nsl44tr,"> Tech is supposed to beat inflation.  And it does, wtf are you trying to claim?  $100 CPUs these days run circles around 6700K which was the flagship in 2015. A B580 is faster than a GTX 980 Ti, which was the flagship card of 2015.",hardware,2025-12-06 12:49:07,77
Intel,nslgt6n,"It does. For the price of a 1993 CRT TV, you can get a flat-screen LED thrice the size and with 10 times the resolution.  SSDs? A 2 TB nvme is a fraction today than a 128 GB Sata one was a little over a decade ago.   What actually changed is inflation, and that the buying power of today's middle class person decreased significantly relative even to the 2000s.",hardware,2025-12-06 14:14:46,17
Intel,nssphno,Gpus are way more expensive to produce,hardware,2025-12-07 18:04:53,1
Intel,nsxk9lk,"> Tech is supposed to beat inflation.   It does, despite wafer prices increasing the last 10 years.",hardware,2025-12-08 13:24:53,1
Intel,nslrykc,Wrote a fucking who? The fact that TVs are cheaper in nominal terms than they were 15 years ago does not mean it has to be the same thing with every other tech product. TVs are not products manufactured necessarily on cutting-edge expensive nodes.,hardware,2025-12-06 15:20:04,0
Intel,nsridpx,"It does. For the same amount of money, you get way better GPU(unless your braindead thinks the gtx970 has same performance of 9060xt)",hardware,2025-12-07 14:17:24,0
Intel,nsmyw1o,"It does, but also, inflation has been extremely bad for 5 years.",hardware,2025-12-06 19:06:53,-1
Intel,nslef0x,"You are just making shit up at this point. NVIDIA GeForce GTX 760 (2013) release price: $250.   That was a shit card, arguably a worse product than the 9060 XT is today, when you compare it to contemporary rivals. How do i know it was shit? I had it.",hardware,2025-12-06 13:59:32,33
Intel,nsmzdz9,TSMC inflation is FAR higher than CPI. You are half right,hardware,2025-12-06 19:09:24,13
Intel,nsxnqe7,"according to US bureau of labour staticstics that measures the CPI it includes  all personal computers (desktops, laptops, tablets) and related equipment (printers, monitors, smartwatches, smartphones). It does not look at GPUs specifically, but the effect of that will be visible.",hardware,2025-12-08 13:46:23,2
Intel,nslsb0f,I doubt ppl are gonna defend the 5050 considering a 5060 or an 8GB 9060XT is not much more and a fair bit faster.,hardware,2025-12-06 15:22:04,12
Intel,nsuys01,"i mean, the 5050 off of amazon rn is 210, so it is getting there as a sub 200 dollar GPU for improving over iGPU right",hardware,2025-12-08 01:03:40,1
Intel,nsmjt4j,I pretty much but there's nothing else people *trust* in the category because apparently Arc cards are for professional nerds or something whereas I haven't had a real bad driver issue in over 2 years with my A770  People are also conditioned to fear older gen GPUs so 6xxx and 7xxx parts are sitting on shelves waiting for blowout discounts. People would still rather spend more on a basic nvidia from the 5000 series.,hardware,2025-12-06 17:50:02,0
Intel,nsokkai,B580 user here. I coupled it with a R5 5500 and as of the recent updates the card just seemed to run much better vs when I got it last July.  There was a video before which also revealed that the CPU overhead is now being addressed in subsequent updates.  https://youtu.be/gfqGqj2bFj8?si=PyAfB2NhqZKWWVXY  I’d say it’s getting better and that I’d recommend it over a 5050 since the overhead is now fixed/negligible.,hardware,2025-12-07 00:31:34,9
Intel,nsxo5ua,Intel is in their second GPU generation. Its going to take a lot longer to catch up with the institutional knowledge and practical application in videogames that the others were developing for over 20 years. The CPU overheard was not an issue in Intel iGPUs and Alchemist because GPUs never got fast enough to matter. It is only now that they noticed that issue since the GPU is far enough to create it.,hardware,2025-12-08 13:48:58,1
Intel,nsq49gv,And that only in terms of native resolution and does not mean equal final image quality.,hardware,2025-12-07 06:54:10,6
Intel,nsljs69,\>dlss quality preset uses higher resolution than XeSS and FSR quality presets  Not exactly. FSR and DLSS are evenly matched in internals at all quality presets.,hardware,2025-12-06 14:32:33,21
Intel,nt3tjxg,I’m in Australia lol. The msrp for the 5090 is 1999 USD which translates to 3011 AUD The cheapest 5090 is 4800 AUD. That’s not even close at all to the msrp…,hardware,2025-12-09 13:05:09,1
Intel,nsq2h5v,"It sure is funny every time all those 6700k/8700k era CPU's pop up on used parts sites or FB Marketplace and still expecting close to initial prices.  Who even buys them anymore? At least a Q6600 has retro value ,but those are just obsolete.",hardware,2025-12-07 06:37:50,9
Intel,nslou4e,"Not if you consider how the workloads being run on them have also changed. A GTX 970 ($450 inflation adjusted) would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  In other words, demand for performance has outstripped performance improvements, and those improvements are not felt as much.",hardware,2025-12-06 15:02:28,-11
Intel,nsm6adc,"A 100$ CPU in 2015 would easily run 2015 made software. A 100$ CPU in 2025 would barely run the electron JS slop. This includes Windows.  In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Yes yes yes it is very good on benchmarks but I don't stare at benchmarks all day. I use my computer for things you do at computer. Don't force my CPU to crunch how much digits of Pi it can compute.  A 980 Ti can easily run top 2015 games. Now? My laptop barely runs modern AAA games without looking a blurry mess. I simply can't fucking understand how you people look at the glorified motion blur and call ""yup it is the pinnacle of computer graphics"". How the fuck majority of modern AAA games look any better than RDR2 can anyone fucking tell me?",hardware,2025-12-06 16:37:49,-11
Intel,nstfa7l,"Why aren't you comparing relative buying power of 2014/2015 vs now then?  $650 got you what in 2014, a GTX 980TI?  $330 got you what in 2014? How close to the top end are both these things?    That's $900/$455 today, thereabouts.  What does $900 get you today?  Does that buy you anywhere near the top end?  And how does that product compare in relation to others above and below it?  Because the $330 product in question ($455 today) got you about ~75% to top end performance for ~half~ MSRP of the 980TI.  How does a $455 product of today square up relative to the top end?   Why don't we throw in a GTX 980TI vs a GTX 280 comparison while we're at it.  Make things really interesting.  I'll let you fill in those blanks (along with the $330 card in question) hoping you actually learn something in the process here.  The bar is very low, try not to trip.    The underlying point that user was making was pretty obvious if you read the comment they responded to.",hardware,2025-12-07 20:08:33,-4
Intel,nsm3fs2,"cmon man, give him some slack, he just made shit up cause it's convenience for his argument.",hardware,2025-12-06 16:22:46,15
Intel,nslu7ja,How is it comparable? The 9060 XT is a very good card for 250. Ideally it'd be around 200 or below but for 250 you get a card that's a bit overkill for even 1080 P gaming.,hardware,2025-12-06 15:32:39,-9
Intel,nsluf98,In quite a few countries the 9060 XT is at or below the RTX 5050s MSRP.,hardware,2025-12-06 15:33:51,18
Intel,nslugvd,Then why did the comment above mine get multiple downvotes? It's Reddit and that's how it goes unfortunately,hardware,2025-12-06 15:34:05,2
Intel,nsnx0y4,6x and 7x are priced far too high for old stock and are poor value compared to nvidias 50 series. They really haven't had a good price/performance low-mid end card since the 6700XT which are extinct at retail.,hardware,2025-12-06 22:14:23,4
Intel,nspdbpk,"I think they must have to go into every game, and adjust that t fix it, because it's not a universal fix it seems. Maybe per-game optimizations .",hardware,2025-12-07 03:34:19,4
Intel,nsxjt6i,yep. XeSS 1.3 is closer to DLSS 3 rather than DLSS 4 in terms of image quality. Its good enough to game on in my opinion.,hardware,2025-12-08 13:21:58,6
Intel,nsqehxx,"> Who even buys them anymore?  The best SKUs on sockets have always demanded a premium in the used market. Since that's where people upgrading old machines will go.  And many machines from OEMs are not readily upgradable with just new boards/CPUs combos. Since they use custom form factors etc. So it's either a in socket upgrade or replace the whole machine. The socket 6700k is on is also especially affected by the ""premium"" factor. Since there's no lower end SKU with 4C/8T. You either get the 6700/7700 variants or are stuck with lower thread count.",hardware,2025-12-07 08:33:16,11
Intel,nsm3au4,"> would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  I think your memory is impacted by the expectations at the time. And the problem of reviews often using older titles inflating numbers, ffs some are still benching with GTA V to this day.   The [970](https://tpucdn.com/review/nvidia-geforce-gtx-1060/images/witcher3_1920_1080.png) couldn't even get 60 fps in witcher 3. Which was released in 2015.  And the performance it got in Witcher 3. Was not much better than what the 5060 Ti got in [Black Myth Wukon](https://tpucdn.com/review/msi-geforce-rtx-5060-ti-gaming-16-gb/images/black-myth-wukong-1920-1080.png)  Which even including 2025 titles. Is one of the hardest/heaviest titles with the worst performance. You can expect much better performance in almost every title. Just like the 970 was doing better than it did in Witchers 3.   But to argue that we got a lot better performance back then in the games releasing at the time, that is just false.",hardware,2025-12-06 16:22:03,36
Intel,nsp8t2z,Yes 3.5GB of memory in 2015 was soooo much better than 12GB today /s,hardware,2025-12-07 03:04:36,3
Intel,nsm6g3n,"All the GPU makers are betting on you using DLSS/FSR/XeSS as part of your usage to play games. Maybe even frame generation along with Relex, and all the other tech they ship GPUs with. They used to only rely on you using regular AA techniques.   If you ignore all those options you have today, and pay like it's 2015, it might be worse a lot of the time. If you use those options, you're generally way ahead of where a GTX 970 would fall. So it depends if you're willing to adopt new rendering tech, or rejecting it.",hardware,2025-12-06 16:38:39,3
Intel,nsmzr6o,"No, not really, in 2015, you happily accepted 45 FPS on not the highest settings at 1080p",hardware,2025-12-06 19:11:17,4
Intel,nsn5d71,"This don't sounds like CPU problems at all, more like either Win11 is a vibe coded pile of bugs, or ACPI problems.",hardware,2025-12-06 19:40:49,8
Intel,nsmgtnv,"> Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Something is wrong. I keep reading people's experiences of stuff like this and I haven't experienced it, I'm not doubting it but I'm so curious as to what is wrong.  In particular I read a lot of people saying Windows Explorer takes forever to open etc",hardware,2025-12-06 17:34:05,7
Intel,nsmsf6b,"> In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  That's Windows for you.",hardware,2025-12-06 18:33:48,1
Intel,nsxkhp8,it does not matter how close to the top GPU is. its a completely useless comparison.,hardware,2025-12-08 13:26:19,2
Intel,nsm74h5,I'm not sure what you're saying. The 9060xt is $250 only in 2013 money. They are arguing it's better value than a GTX 760.,hardware,2025-12-06 16:42:14,1
Intel,nslzd1j,"Ppl up/downvote kinda randomly, doesn't really mean much post can go from +/-20 to the opposite real quick sometimes.  Anyways It's at +8 currently was at +something(2 maybe?) when i commented so who cares.",hardware,2025-12-06 16:00:40,4
Intel,nsxodfi,"Nvidia and AMD do a lot of per-game optimization in the driver as well. In some cases very brutally, for example Nvidia is known for grabbing all games DX12 drawcalls and rearranging them in driver because the way game handles it is inefficient.",hardware,2025-12-08 13:50:12,1
Intel,nsm6ry8,I remember upgrading to a 970 in 2016 and still being unable to max Witcher 3 at 1080p60 but got close enough,hardware,2025-12-06 16:40:22,4
Intel,nsxkob8,"funnily enough, GTA 5 Enhanced Edition can be quite a benchmark for ray tracing nowadays. But it took to this year for it to be released. I think we can consider it a testbed for whats going to be implemented in GTA 6.",hardware,2025-12-08 13:27:28,1
Intel,nsmnevw,"Bringing up 1080p no RT Wukong benchmarks sort of makes the point for me: the only way these cards look comparable is if we pretend features and standards are the exact same they were a decade ago.  High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen. RT was not a thing in 2015, now it is and Nvidia marketing really wants you to use it. It's like you're comparing Witcher 3 on Ultra settings to Wukong on Medium or High settings, and acting like it's apples to apples.  The moment you take modern displays and features (including DLSS to be fair) into account, it paints a picture where technology has moved on, developers and players would love to move on, and GPUs are struggling to make that jump.",hardware,2025-12-06 18:08:19,-11
Intel,nsxl3k9,"if you run out of memory today the game swaps textures and continues running, it just looks uglier.   If you run out of memory in 2015 it starts using the superslow 0.5 GB and everything breaks.",hardware,2025-12-08 13:30:09,2
Intel,nsxl8jt,Nvidia is certainly expecting DLSS+FG to be the typical use case. The vast majority of their benchmark and marketing material is with those two.,hardware,2025-12-08 13:31:02,1
Intel,nsmwsat,"I've got nothing against DLSS, I use it whenever I can, but sometimes it's just not enough to bridge that gap.  Another user brought up Witcher 3 and Wukong as an example of a graphically advanced 2015 game vs a graphically advanced 2025 game. The 970 would get 50+ fps on Ultra settings Witcher 3. Max out Wukong on a 5060 Ti and no amount of DLSS will make that card stop crying and screaming.",hardware,2025-12-06 18:56:07,-5
Intel,nsoze8r,"Like, Debian has no issues running on a n150 with multiple docker containers without instantly spiking the cpu to 100%.",hardware,2025-12-07 02:04:41,3
Intel,nsxmt3k,"I found a way to sort of kinda make file explorer slow. But its really a perfect storm thing. Have multiple screens, one of which is running in HDR and another in SDR. Have the file explorer tree open. Have a HDD, slower the better.  When you browse folders it refreshes the tree. When it refreshes the tree it asks connected devices if they are online, including the HDDs. Now move the window back and forth between your screens. When the explorer moves into HDR screen, it gets redrawn. Same when it moves to SDR screen. I suspect but cannot confirm there is a bug where the old instance is not cleaned correctly. So now when you browse it asks all devices if they are online 10 times. 100 times. At some point youll start noticing actual delays in opening folders.  Works even better if you havent restarted for a month.",hardware,2025-12-08 13:40:46,1
Intel,nsxn5gg,"the opening notepad thing, if you use taskbar it has a bad habit of not actually opening notepad until it finishes the online search for apps called notepad or whatever you typed. Disabling online search in start makes it fly really fast.",hardware,2025-12-08 13:42:53,1
Intel,nsm8g0h,The 9060 XT 8GB is currently retailing for 250$ in many areas. I'm saying that the 760 isn't an ARGUABLY worse product. It is a worse product for it's time straight up.,hardware,2025-12-06 16:49:09,6
Intel,nslzlxy,"Fair point, but isn't lower and competitive prices good for us?",hardware,2025-12-06 16:02:01,1
Intel,nsmqlcg,">  and acting like it's apples to apples.  Apples to apples would be comparing W3 performance for both cards.  Wukong even without RT is a CONSIDERABLY more advanced game graphically than original W3.   >High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen.   And? Better monitors showing up doesn't change the laws of physics and basic economics. It doesn't make scaling with die shrinks suddenly increase. With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.   And before you start harping on about die sizes. The die in the 5060 Ti is actually more expensive than the die used on the 970. Wafer price increases more than compensates for the size difference.",hardware,2025-12-06 18:24:26,17
Intel,nspgewd,"Looked it up. Wither 3 got 52 FPS at Ultra settings, no Nvidia Hairworks turned on, for a GTX 970. Wukong gets 42 FPS at the cinematic preset native resolution, which is actually intended for cinematics, but developers allow people to enable anyways. As Digital Foundry has said, they maybe shouldn't.  Gets over 70 FPS if you turn the preset down 1 notch to high. No upscaling, or frame generation, or hardware RT, which is like what Nvidia Hairworks was for Witcher 3. It's really not hard to get Wukong to run at 90 FPS on a 5060ti with some minor tweaks.",hardware,2025-12-07 03:54:39,6
Intel,nsxlsss,According to TPU review maxed out Wukong with DLSS got 42.3 fps. Not exactly the 50 fps you remmeberr for witcher but close. Heres a link to the review: https://www.techpowerup.com/review/black-myth-wukong-fps-performance-benchmark/5.html,hardware,2025-12-08 13:34:33,1
Intel,nsxop63,"That's very interesting thank you, I can definitely see why I haven't experienced it.  You genuinely wonder how Microsoft are testing these days.",hardware,2025-12-08 13:52:10,1
Intel,nsmvfxi,"> With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.  Except in practice from 2005 to 2015 you got considerably more advanced graphics *and* higher resolutions *and* generally higher framerates too. Now either you pay up or you gotta pick one.  As for the rest of your post, it's more of a digression. All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.",hardware,2025-12-06 18:49:17,-3
Intel,nsxm00p,"> As Digital Foundry has said, they maybe shouldn't.  hard disagree. As someone who does not have a lot of time for videogames and often end up playing older games with newer cards, those beyond high settings are great as it allows me to make use of my newer card and make the old game look better.",hardware,2025-12-08 13:35:48,1
Intel,nsrkwkw,"I disagree with comparing RT to Hairworks, when the visual impact as well as the emphasis put on it by Nvidia is so much bigger. I also disagree with using 1080p as a reference for Wukong, when high res and high refresh rate monitors are as cheap and plentiful as 1080p was back then.  Imagine you went back to 2015 and told the GTX 970 guy he's supposed to play his games at 2005 resolution and turn off antialiasing, how do you think he'd react?",hardware,2025-12-07 14:32:28,1
Intel,nsncxlc,">All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.  Why complain about something that there are valid reasons for lol",hardware,2025-12-06 20:22:05,5
Intel,nsyfiin,"It just makes such a small difference in UE5, it's really not worth losing 30% performance over for this engine. They would agree with you for a lot of other games, and Avatar Pandora kind of has a hidden setting, they'll maybe make available in menu at some point. Right now you need to modify a config file to enable it. Maybe they just need to wait until the final patch of a game to show those settings, years after launch, or just name them ""next gen"" or ""experimental"" with the setting below called ""ultra"".",hardware,2025-12-08 16:15:19,1
Intel,nss9lt6,"You can use DLSS and frame generation to play at higher resolutions. That's their intent. Especially UE5 games, because TSR was developed by Epic for a reason. The games on UE5 are really never intended to be run at a native resolution. I don't tell people to run UE5 at a 2013 resolution, but I also don't tell them to have the 2013 mindset that everything has to be run at native resolution, and that's the only way to play it. 1440p Balanced DLSS should give you around 50-60 fps without frame generation.",hardware,2025-12-07 16:44:57,2
Intel,nt9j82j,"The performance loss does not mater for future (re)plays.  The config settings are usually hidden because during testing there were instabilities found that they didnt think was worth fixing. There were some games that had settings beyond ultra with names like ""Extreme,"" ""Nightmare,"" or ""Insane"".",hardware,2025-12-10 09:42:20,1
Intel,nosu5u7,"3050 performance at half the power, can't argue with that.  I do wonder how long it will be before the low-end dGPU disappears entirely from laptops, suspect these new chips probably aint it due to price but it's presumably coming.",hardware,2025-11-14 12:51:04,54
Intel,noshwzt,"While Geekbench's OpenCL isnt super popular for GPU testing;  I still cant believe a mainstream integrated GPU is actually competing with laptop 3050Ti, and these results still aren't with final optimizations!  by the time Panther Lake comes out, Intel claiming that its as fast as laptop RTX 4050 might actually be true lol",hardware,2025-11-14 11:16:17,68
Intel,notzu22,Are these better than Ryzen igpu? How do they compare to RTX 4050 AND 5050 in non demanding game?,hardware,2025-11-14 16:35:58,8
Intel,notsbf4,"I'm excited sure but if the claims are true how can anyone here expected these to be reasonably priced and sell well? If it competes with a laptop dedicated GPU it will be priced 2-3 times that laptop. The 285H isn't even that popular and its expensive, hope the 385H is affordable AND the claims are true.",hardware,2025-11-14 15:58:55,5
Intel,noudeop,"The score is slightly above a GTX 1650 Super which in turn is slightly above a GTX 1060 which is barely matched by current most performing 128 bits iGPU (Lunar Lake / Strix Point)  Even if we expect 50% improvement compared to 890M, that's still half the performance level of a full power mobile RTX 3060. (roughly half a PS5 too)",hardware,2025-11-14 17:44:26,4
Intel,npgcjdt,This would still no where be close to M4/M5 in single core and GPU,hardware,2025-11-18 06:45:37,1
Intel,nvtf2fd,I sincerely hope Samsung will work on their camera and make it better in this version.,hardware,2025-12-25 03:06:03,1
Intel,nosfy6e,"Hello 6950! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-14 10:58:08,1
Intel,nouc8xh,"The problem with buying a device relying on Intel graphics drivers, is that you're getting a device relying on Intel graphics drivers.  I'd rather not.",hardware,2025-11-14 17:38:32,-6
Intel,nosxwa8,"iGPUs have already killed off the MX series, suppose it’s a real possibility other low end dGPUs also get killed off",hardware,2025-11-14 13:14:59,43
Intel,note1cz,"More likely what is considered a low end dGPU will shift, just as a current low end dGPU would have been an high end model a decade ago.",hardware,2025-11-14 14:47:09,12
Intel,np3ssro,"iGPUs is why low end desktop parts dont exist anymore, its going to do the same for mobile parts.",hardware,2025-11-16 06:06:05,1
Intel,npclgdx,"It'll happen eventually but they have a few issues.  First is memory bandwidth, they'd need to put a wider bus on it to address which is costly.  Second is the memory itself, they need something with higher bandwidth (but that might be addressed with LPDDR6).  To address the bandwidth issue they often need to put on extra cache, the 12Xe3 core is basically maxing out the bandwidth of the 'standard' chip, and if they went to say 20Xe3 cores (which would be 4060+ level) then they would need a bigger memory bus and more cache (Strix Halo put 20mb of MALL cache to try and address, and a 256 bit bus) and that is more design work.  They'd also have an issue with the socket - the APU would be too big to fit into a standard socket and would need a custom one which means a custom motherboard which increases costs.  Another is heat - while more efficient overall, all the heat is in one spot so the computer needs to be thoughtful about moving that out.  A think that in 2028 big APU solutions can compete well against XX50 series and most XX60 cards.  It isn't happening before then however.",hardware,2025-11-17 17:36:51,1
Intel,nosr91b,Apple's M5 already beats the 3050Ti though.,hardware,2025-11-14 12:30:46,33
Intel,notfij9,I mean the early Iris pro onboard GPU’s traded blows with gtx 650m at a lower power draw. It’s been done before. Still nice to see but nothing ground breaking,hardware,2025-11-14 14:54:51,11
Intel,nou9dky,We will have to wait for the actual laptops to release & see how they do in real games; performance in synthetic benchmarks does not always line up in real tests.,hardware,2025-11-14 17:23:51,10
Intel,nov4acg,PTL should have comparable or possibly even lower production costs than ARL. There's some room for optimism.,hardware,2025-11-14 19:59:19,8
Intel,noydico,this GPU is also coming into the 355H btw (more specifically there's a Ultra X7 version of the 355H with the full 12-core GPU)     and iirc last time the 255H was pretty popular,hardware,2025-11-15 09:37:36,2
Intel,npcp3di,"Way we should think about it in my mind is that it's got 12Xe3 cores, and 1 Xe2 core = 2 RDNA 3.5 cores.  Strix Halo has 40 RDNA 3.5 cores (though with a bigger bus and enhanced cache) which is kind of like 20 Xe2 cores (maybe a bit more), and Strix Halo basically ties with a 4060.  If you assume a 5-10% improvement from the architecture going Xe2 to Xe3 to offset the bus and cache (which isn't *as* limiting with only 60% of the hardware), then you're probably at about 60% of a 4060 power, which is more or less where this looks to fall.  4060 mobile passmark = \~17,400  4050 mobile passmark = \~14,300 (81% 4060)  60% 4060 mobile passmark = \~10,500  3050 mobile passmark = \~10,100  That puts it pretty much at a 3050.  With a little sprinkle of cutting edge upscaling and a bit of frame gen it'll be just fine for light titles, but won't be a dedicated monster.  It's just a good solid chip for light-weight long-battery general purpose use that can play some titles on the side (or play older titles with confidence).  I suspect it'll review pretty well, since the reviewers tend to be excessively games-focused and this will appeal, but it's not a games machine - it's a surprisingly zippy long-life chip that's a good fit for all-day laptops.  EDIT: 8060S (Strix Halo) is \~18,000 for reference.  EDIT 2: 890M (strix point's top end) about 8,100, so the PTL 12Xe chip will smoke that.  I'm excited about it, it looks like a great chip!",hardware,2025-11-17 17:54:36,0
Intel,nvy72ph,"I swear, the only issue i have with the book 5 is the camera, this shit doesn't deserve the book 5 hardware",hardware,2025-12-26 00:34:46,1
Intel,noui1dy,"It is not 2023 anymore. Intel Arc drivers work quite well, and Lunar Lake graphics are already pretty solid in the low power category. No need to keep repeating this nonsense when you haven't even used the devices.",hardware,2025-11-14 18:07:17,17
Intel,not3o9j,"Only sticking point seems to be cost I suppose, implementing a truly competitive iGPU looks like it will be expensive if Strix Halo is anything to go by.  I think the Intel/Nvidia partnership might at least come close to making it a reality to have a 50/60 tier integrated GPU in a laptop that isn't much more expensive than one with a dGPU currently... in theory it could/should be cheaper.",hardware,2025-11-14 13:49:22,11
Intel,nou51fd,Not really? Even optimistically speaking this won't be close to a 5050 laptop when accounting for bandwidth limitations in real world tasks that use the cpu and gpu at the same time.,hardware,2025-11-14 17:01:51,3
Intel,nou5hpq,"It's a good point. There really isn't any truly low end dGPU options out there now as far as I know, Nvidia abandoned the GT line and someone up there mentioned MX is dead etc.",hardware,2025-11-14 17:04:08,9
Intel,nouozrl,I’m assuming the caveat to posts like this is “running a version of windows/linux”,hardware,2025-11-14 18:41:25,12
Intel,nossn02,"and the 780M when fed enough power also comes within range, and the 890M also beats it.",hardware,2025-11-14 12:40:39,23
Intel,np3szy6,"It could beat a 5090, it would still be useless until the bootloader is open.",hardware,2025-11-16 06:07:46,2
Intel,nougqlr,Wasn't the previous Intel igpu really good for games and efficient?,hardware,2025-11-14 18:00:43,8
Intel,nouo5af,"Even if what you said was true, I know enough about Intel to know that even if it worked great, they'd find a way to fuck it up eventually. That's what they do.",hardware,2025-11-14 18:37:14,-6
Intel,noteu83,">if Strix Halo is anything to go by.  I think Strix Halo skews how expensive a competitive iGPU could be. Strix Halo was very low volume and was also, imo, overbuilt. I think it's definitely possible to have a more cost competitive big-iGPU SoC",hardware,2025-11-14 14:51:22,14
Intel,np2vxz8,Prices for Strix Halo are rumored to fall in the new year; I'm curious to see if it ever reaches 1000-1500 dollar laptops like I've seen suggested it could. Seems like a ridiculous idea right now.,hardware,2025-11-16 02:14:32,1
Intel,nox0o6z,And in gaming.,hardware,2025-11-15 02:39:40,6
Intel,noxmkdx,I mean...technically Asahi Linux exists for Macs. Though not the M5,hardware,2025-11-15 05:15:54,3
Intel,nost857,Then there's the 8050S & 8060S,hardware,2025-11-14 12:44:41,11
Intel,np2p16a,"How so? It is not even beating, at best barely even with 3050 35W (non-Ti).",hardware,2025-11-16 01:33:18,1
Intel,npfpm2g,I'm sure thats the first thing that comes to mind when people purchase a thin and light.,hardware,2025-11-18 03:44:31,1
Intel,novhbul,"The Arc 140V like in Lunar Lake 268V? it's about 3050 Mobile (35W) sort of performance in real tests. There's a 140T in some chips too, I think it's less powerful than the V.",hardware,2025-11-14 21:08:26,6
Intel,noto02z,Strix point is the better comparison,hardware,2025-11-14 15:37:54,3
Intel,nou57r7,"I hope so. I was a little surprised by the price but as you say, they clearly have a market for that thing and it isn't particularly price sensitive.",hardware,2025-11-14 17:02:45,2
Intel,nosxd97,they are not really comparable to the traditional APUs,hardware,2025-11-14 13:11:43,21
Intel,not3ot0,These are 256bit bus devices and have even fatter GPUs .....,hardware,2025-11-14 13:49:27,16
Intel,npbumwg,"Wait, is the 780/880m close to the 3050? I had heard that it was close to the 1050 at some point... As an 780m owner, it sounds like I can suddenly play way more games than I imagined?",hardware,2025-11-17 15:24:12,1
Intel,novka84,yes that one,hardware,2025-11-14 21:23:45,2
Intel,notxkw1,"Right, but Strix Point suffers from the same problems that PTL-X will: 128b bus chocking the memory bandwidth.  If iGPUs want to truly replace entry level dGPU, there needs to be wider bus variants",hardware,2025-11-14 16:24:49,10
Intel,npcclrj,"Might seem so, but mobile 3050 is way slower than let's say 1660 laptop.  Still, you can play many games, just at lower res and settings. You can look up benchmarks/tests for handhelds with this gpu (or ryzen z1/z1 extreme)",hardware,2025-11-17 16:53:04,1
Intel,nour2eo,"No doubt , I meant in terms of availability",hardware,2025-11-14 18:51:36,1
Intel,nngoz2c,Why can't they have a low core count CPU but still keep the full iGPU?,hardware,2025-11-06 18:43:01,72
Intel,nngf3u0,"While this has pretty much been known for a while, you really do get the feeling when looking at these frequency and power stats that Intel hoped for more from 18A.  It's a good node and a good chipset, I think it's targeted and designed well and will do well in the market, but it doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.",hardware,2025-11-06 17:56:37,46
Intel,nnk8kqz,"6 different top models with just 100mhz between them is so stupid, that should be 1 or MAYBE 2 models for that core configuration  But just hope the 356H is cheap then",hardware,2025-11-07 07:43:58,9
Intel,nnku3ds,Is this desktop or laptop?,hardware,2025-11-07 11:19:09,3
Intel,nnglnmb,">16C (4P + 8E + 4LP)  I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  Two clusters make sense. What am I supposed to do with three?!  Unless I'm mistaken, AMD seems to be sticking with full fat Zen 5 cores in both Strix Halo and ""Fire Range,"" and frankly, it sounds like Intel is just trying to keep up in the core count race with its plethora of 'baby' cores.  Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.   I use my laptop for convenience and my desktop for compute heavy work.",hardware,2025-11-06 18:27:30,-5
Intel,nnhbq11,"16 cores, but only 4 of them will be fast. The rest will be slow cores.",hardware,2025-11-06 20:34:10,-3
Intel,nngan3d,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-06 17:35:20,1
Intel,nniwn7f,"Nice. My a770 still slaying 4k ultra gaming in 2025. No stutters no freezing just responsive beautiful gaming. Ofcourse it’s paired with 14900k so my igpu technically has identical specs but I keep temps down this way. Also have an RTX 50 series. I love arc graphics. Bang for buck AT HIGHER RESOLUTIONS — that’s where it’s at. And I’ve had 4090 (sold), amd 9070(sold). Never would I pay what they are asking for a 5090. Just because I wasn’t 100 percent thrilled with price per FPS with the 4090. Once you have had all of them you will see Intel is the way to go with 4k gaming(unless you like paying thousands of dollars). Arc doesn’t have shadows and artifacts . RTX does. Bad. Blurs my picture even on best settings possible.",hardware,2025-11-07 01:45:56,-6
Intel,nngrua7,Upselling,hardware,2025-11-06 18:56:23,48
Intel,nnh35ny,IMO it's probably not economical for them to do it.   Gaming laptops/handhelds are already niche enough as it is -- doesn't make sense for them to package both and for OEMs to carry both for just for a ~$1-200 delta between the units. Those who want the full GPU will just pay for it.,hardware,2025-11-06 19:51:36,14
Intel,nnntnit,"You gotta remember these CPU’s don’t have multi-threading. AMD gets 16 threads out of their 8 core z1 and z2 extreme but with these core ultra’s you get one core and that’s it. A full on physical core is better than a virtual core, but the same amount of cores with multi-threading is much better than just a same core-count CPU without it. Intel are offering up to 16 cores which is 16 cores/16 threads, but generally should offer better multi-core performance vs an 8-core 16-thread. If anything it’s nice Intel have the ability to improve core count offerings without significantly raising the power requirements.   I for one veered away from the MSI Claw 8 AI+ as a standalone PC solution because it lacked the multi-core performance I needed with it only having 8 physical cores and no multi-threading. A 16-core offering is exactly what I need.",hardware,2025-11-07 21:03:16,3
Intel,nnhhojx,Because nobody would buy it.,hardware,2025-11-06 21:03:48,5
Intel,no4wbjw,"yeah, sucks cause they said handhelds was a priority for them",hardware,2025-11-10 17:11:40,1
Intel,nngiuzv,"Not great, not terrible. In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.",hardware,2025-11-06 18:14:25,20
Intel,nngq1l8,According to jaykihn0 it's a heat issue: BSPD and transistor density is flattening the VF curve on the high-end. The ST power curve is substantially improved at lower frequencies over Lunar Lake and Arrow Lake.,hardware,2025-11-06 18:48:00,8
Intel,nngpld3,"If anything, seems strictly worse than N3, even compared to older iterations. That's not great for a node that was supposed to go toe to toe with N2. Especially if the production cost is more in line with the latter.",hardware,2025-11-06 18:45:52,10
Intel,nounnja,"18a is initially launching with its lower power variant, which is why the only nova cpus that will use it for the compute tile are the laptop ones. The higher power variant comes later",hardware,2025-11-14 18:34:48,1
Intel,nnitrfm,"Jaykihn, whose tweets these leaks are based on, says that the design characteristics cannot be extrapolated to the node itself.   So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.",hardware,2025-11-07 01:28:24,1
Intel,nngofbt,"> […] It doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.  Well, we already had rumors of Intel struggling to hit actual intended frequency-goals, no?  Though especially the actual performance-metrics of the node will be quite sobering I think.  The power-draw of these SKUs is supposed to be a TDP of *officially* 25W – In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4) and 65/80W (4+4+4 and 4+8+4) respectively.  So it must be seen, if these parts are any more efficient in daily usage, or just a side-grade to Lunar Lake.",hardware,2025-11-06 18:40:28,-2
Intel,nngjg8l,"Panther Lake is pretty much a pipe cleaner, so it should still improve a bit.  Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD. If that is the case, it will only really be allowed to shine in high powered desktop and server chips, while power limited chips won't be able to reap as many of the benefits of 18A, since laptops tend to target the efficiency sweet spot, rather than clocks.  My understanding is that 18A will scale much better with extra power than N3 and N2. Once you pass the sweet spot, 10% extra performance on N2 might take 30% extra power, while 18A should continue to scale much more linearly for a good while longer.  So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones, especially in use cases where SRAM density is less of a concern. AFAIK Intel CPUs aren't as cache-dependent as AMD, so SRAM density might not be the highest priority, whereas slugging it out with AMD and trying to beat them on frequency might?  From what I've heard, Intel is still just about matching AMD in terms of IPC, while Arrow lake is very efficient, just not great for gaming. If Intel wants the CPU crown back, simply pushing more power could go a long way in clawing back the deficit, assuming 18A does indeed scale well with more power. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.",hardware,2025-11-06 18:17:12,-5
Intel,nnpnb6l,> 6 different top models with just 100mhz between them is so stupid  They've always done that because of the OEMs who like a million segments.,hardware,2025-11-08 03:41:25,3
Intel,nnkuisr,Laptop.,hardware,2025-11-07 11:22:50,5
Intel,nngwxp8,Since when is any user determining what cores they want to use and when?,hardware,2025-11-06 19:21:09,22
Intel,nngrg35,It makes sense to have three as the low-power cores prioritize keeping the power consumption low for battery life but to achieve this they aren't attached to the ring bus which hurts performance so 8 regular E cores are also used,hardware,2025-11-06 18:54:32,11
Intel,nnhqy1t,">I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  From a user's perspective, CPU's are a blackbox and the inner-working details are irrelevant. What matters is results. They want, typically, a balance of performance, battery life, heat / fan noise, and cost, placing greater emphasis on one of these categories over the others.  The idea is that P cores = performance optizied  E cores = Area optimized  LP-E Cores = power optimized.  The theory being to have different core types focus on different parts of PPA.  E cores and LP-E cores are the same microarchitecture. Just LP-E cores are off-ring so the ring (and rest of the cores) can be powered down in light-load scenarios.  Intel's biggest problem is that looking at each of the cores, the P cores perform their designated role the worst. 200% larger than E cores for \~10% more IPC and \~15% more clockspeed is a pretty rough trade off - and by the time you've loaded all P cores and need to load E cores too, that clockspeed difference diminishes hard.  Intel's better off (and all rumors point this way) to growing the E cores slightly and making that architecture the basis of the new P and E cores (like Zen vs Zen-C)",hardware,2025-11-06 21:48:50,11
Intel,nngrdc7,"> Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.  You are the niche use case. The average/median user is using H-class laptops.   These are meant to beat Strix Point and its refresh, which it will do and at better economics for Intel than Arrow Lake H.",hardware,2025-11-06 18:54:11,6
Intel,nnhcb0r,"And with all the issues Windows already has with the scheduler, I fully expect it to never be able to properly utilize the different cores. I expect a similar situation as with previous versions, where it's sometimes necessary to disable the slow cores to eliminate stuttering in games, because they sometimes offload tasks to them which slows the whole game down to a crawl.",hardware,2025-11-06 20:37:09,1
Intel,nngp9jv,"> I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  > Two clusters make sense. What am I supposed to do with three?!  Ain't these Low-Power Efficiency-Cores aren't even usable by the user anyway in the first place (and only through and for Windows' scheduler to maintain)? As I understand it, LPE-cores are essentially placebo-cores for marketing.",hardware,2025-11-06 18:44:22,-8
Intel,nnhcsuh,Darkmont should really not be that far off Cougar Cove. Certainly not *slow*.,hardware,2025-11-06 20:39:41,25
Intel,nnhjsmd,The E-Cores are plenty fast for most use-cases by now and there are 8 of them in there. Aren‘t they even ahead of the skylake IPC by now? 8 will be doing a great deal with a good clock frequency.  Only the 4 LP-Cores are kind of weak. But they aren‘t attached to the ring-bus and really only used for offloading low req Backgroundtasks and an idling machine. So that you can turn off the faster cores when they aren‘t necessary.  But for performant gaming and productivity it is basically a 12 core machine.,hardware,2025-11-06 21:14:06,6
Intel,nngt3ry,I hope there's a 4+0+4+12 for handhelds but they might not consider adding a whole new tile configuration packaging line worth it over just giving the OEM a discount.,hardware,2025-11-06 19:02:26,12
Intel,nnpqhax,I find that weird. Who buys a 20 core CPU for GPU performance equal to like an RX 580?,hardware,2025-11-08 04:04:39,2
Intel,nnj0g84,"Yeah as of now only MSI looks committed to using Intel for their handheld, and not only are they not that successful but they had also branched out to AMD for another line of their gaming handheld.",hardware,2025-11-07 02:08:54,6
Intel,nnpmmi9,Gaming handhelds are niche but not gaming laptops (relative to gaming desktops). Steam hardware survey reveals a large number of mobile gpus very high up in the charts.,hardware,2025-11-08 03:36:31,1
Intel,nnhnhmd,Both Apple's M series and LNL use decent iGPUs for their 4+4 parts.,hardware,2025-11-06 21:31:54,13
Intel,nnnppbh,"This is a minority use case, but is a very valid one. I run a home server (/r/homelab) and could use some extra GPU power for video encoding and some basic AI stuff. Though this is just a home server so I don’t want to put in a dedicated GPU due to the desire to not spend all of my discretionary income on my power bill. Right now I have a 9th gen i7 that is good enough for the basics of what I am doing, but I’ve been keeping my eye out for a replacement. I’m not alone in that need, but I do get this is a minority group. I’m sure there are other use cases though. Saying no one would buy it is wrong.",hardware,2025-11-07 20:42:57,1
Intel,nngm2dr,"IIRC Arrow Lake's compute tile is fabbed on N3B, which is pretty much TSMCs earliest N3 node for mass fabrication.  Its other tiles are on a mix of mature 5nm- and 7nm-class nodes.",hardware,2025-11-06 18:29:25,17
Intel,nnh1yr8,"> In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.  Arrow Lake was the pipe cleaner for 20A which in turn was deemed unnecessary to move into production because everything was going so well that they could use that groundwork to get a head start and move straight to 18A. That's right from Intel's mouth. Now the goalposts shift once again.",hardware,2025-11-06 19:45:51,20
Intel,nngr2a1,"> Not great, not terrible.  For what it's worth, I'm glad and somewhat relieved, that the never-ending charade of 18A (and ultimate sh!t-show it eventually again amounted to, after their blatant Vapor-ware 20A-stunt) finally is about to come to some lousy end, after years of constant shady re-schedules, even more bi-weekly Intel-lies and basically +2 Years of delays again.  So it will be a bit of … 'consolation' I guess, not having to constantly read that 18A-sh!t in every darn Intel-news.",hardware,2025-11-06 18:52:46,-9
Intel,nnh0tuv,"I'm curious to see the core's power curve itself. Or the core+ring, I forget what exactly Intel measures here for their software power counters.   Intel claims PTL has outright lower SoC power than LNL and ARL, so if the power savings are coming from better uncore design and not the node gains themselves....",hardware,2025-11-06 19:40:21,9
Intel,nnh18hs,"> BSPD and transistor density is flattening the VF curve on the high-end   He doesn't make this claim. BSPD in particular was supposed to help the most at high voltage, and the density does not appear to meaningfully differ from N3, which it still regresses relative to.    Best case scenario, the node, for other reasons, can't hit as high voltages as N3, and the top of the curve is just capped. But you wouldn't expect thermals to be such an issue then.    Doesn't seem to be any evidence of the core-level VF curve benefiting from the node either.",hardware,2025-11-06 19:42:19,10
Intel,nniukba,"Jaykihn attributes only density to the supposed heat issues, not BSPD. Which makes sense because the process guy and Stephen Robinson both talked about having to space out the signal and power wires when working with BSPD.  If they prioritized density, then it is possible that frequency could not reach the theoretical maximum due to crosstalk or heat issues.",hardware,2025-11-07 01:33:21,2
Intel,nngraag,"18A has always been low density, but expected to compensate for it with very efficient frequency scaling past peak perf/W. It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Wait for the desktop/server chips before you call it. With fewer thermal constraints and much higher power budgets, they should be able to push well past the perf/W peak, where it should continue to scale well and for a long time before it starts hitting severely diminishing returns.  I'm not talking 300W monster CPUs, but the scaling between a 65W and 105W 18A CPU should be more significant than on current nodes, where you might gain like 10-15% additional performance from that 50% power bump. I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.",hardware,2025-11-06 18:53:47,6
Intel,nnklc8p,> So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.  Why would they suddenly decide to underclock it to such a degree?,hardware,2025-11-07 09:55:39,2
Intel,nngrcwg,"Tbh, LNL battery life (note: not the same thing as loaded efficiency) with -U/-P/-H perf levels and market reach would still be a very good thing. It's just a shame to see Intel finally straighten out their SoC architecture only to be hamstrung by a subpar node.",hardware,2025-11-06 18:54:08,11
Intel,nnhns9s,">In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4)      Intel's existing 2+8 parts have a PL2 of 57W, so seems to me OEMs are permitted to target the same PL2 they've been targeting on U series.  >and 65/80W (4+4+4 and 4+8+4) respectively.  Which is a fairly large drop compared to existing H series. ARL-H PL2 is up to 115W, PTL-H PL2 will be up to 65/80W",hardware,2025-11-06 21:33:20,6
Intel,nnh0cvv,">So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones,  NVL-S being external adds serious question marks to this.   I wonder if post NVL-S they go back to using further iterations of 18A though for even desktop, if they feel confident enough that they can hit high enough Fmax (even throwing away power and density) on those compute tiles.   Maybe for RZL or Titan Lake in 28' (going off memory for codenames lol).   >especially in use cases where SRAM density is less of a concern.  On paper, Intel has caught up to TSMC (even N2) in SRAM density.   >AFAIK Intel CPUs aren't as cache-dependent as AMD,  Generally speaking Intel uses way higher capacity caches than AMD, AMD uses smaller but faster caches.   >. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.  This seems *extreme*. Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?",hardware,2025-11-06 19:38:01,6
Intel,nnh6cvf,"I actually suspect otherwise - speculation on my part, but I suspect that they were running into unexpected process issues when they pumped the frequency and/or voltage higher so they had to scale back performance.  It almost feels like a process bug that they discovered when making the actual chips.  They've also talked about kind of weak yields on 18A on their latest earnings call.  That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  If they have a 'double upgrade' opportunity in both fixing the discovered 18A issues and implementing the originally planned 18AP improvements then it could end up being a somewhat bigger jump.  Don't think it'll be an N2 beater though.  18AP *might* beat the best of N3 in some applications, we'll see.  For that you'd want to look ahead to 14A where the CFO mentioned on the earnings call (and he was somewhat realistic in other areas so it's more believable here) that 14A has been a positive surprise and they're actually pretty excited about it.  It's possible that if Intel gets more experience on High-NA versus TSMC that they might start having an edge later on.  Maybe.",hardware,2025-11-06 20:07:18,8
Intel,nngqma7,"> Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD   And yet we see the exact opposite here. The leaker claims these go to to 65W, even 80W TDP. There's plenty of power to hit any ST boost the silicon can handle, yet it *regresses* vs N3B ARL. And that's with a year to refine the core as well.    > If that is the case, it will only really be allowed to shine in high powered desktop and server chips   Server chips are low voltage. Even lower typically than mobile chips. High-V only matters for client.    > My understanding is that 18A will scale much better with extra power than N3 and N2   There is no reason to believe that at this time. Notice that Intel themselves are using N2 for their next flagship silicon, including desktop.   Edit: Also, for the ""pipe cleaner"" rhetoric, remember how they cancelled 20A claiming 18A was doing so well and ahead of schedule? Even more obviously a lie now.",hardware,2025-11-06 18:50:42,15
Intel,nnj8gs7,Having to roll the dice on the scheduler doesn't make things better.,hardware,2025-11-07 02:56:58,4
Intel,no3fsuj,"Since release of Windows 7, which allowed users to easily set which cores are used on per application basis in first party software (task manager).",hardware,2025-11-10 12:23:22,1
Intel,nnh9r6a,"It's been a thing for a while as a means to circumvent the Windows kernel's shitty scheduling, especially on processors with asymmetric core or chiplet arrangements. Tbf, Windows has slowly begun to catch up and run cores a little more efficiently, but some people still swear by apps like Project Lasso for manually controlling their process affinity.",hardware,2025-11-06 20:24:17,-2
Intel,nnjjmma,"Users aren't ""determining"" anything because they don't have to.   On smartphones, they've virtually zero control over their own hardware.   And on the x86 side, I don't think people are buying big.LITTLE hardware in droves, and even if they are, I doubt the multiple core clusters are their deciding factor.   And if I had a big.BABY CPU, I’d definitely be tempted to play around with it, and yes, I don’t see the big idea behind having three clusters, aside from marketing gimmicks and artificially inflated MT benchmark scores.  From what I've heard, Windows Scheduler has trouble dealing with just two clusters as it is. On Linux, the experience is, at best, tolerable.  At the very least, I've certain issues with Intel marketing these CPUs has having ""16 cores."" I know a lot of 'normies,' first hand, who have fallen prey to this deceptive marketing tactic.",hardware,2025-11-07 04:09:16,-4
Intel,nni9yge,"> E cores and LP-E cores are the same microarchitecture   In this case, they're even the same physical implementation. 100% identical to the compute cores. Also, for MTL/ARL they were neither power nor area optimized.",hardware,2025-11-06 23:31:35,6
Intel,nnhpwee,The average user is using U series,hardware,2025-11-06 21:43:43,8
Intel,nnh2hm0,">, which it will do and at better economics for Intel than Arrow Lake H.  Doesn't seem like this will be the case till the end of 26' though. At least from the earnings call a few weeks ago.",hardware,2025-11-06 19:48:23,3
Intel,nnlj5r4,"The average user runs apps like Teams, entirely off of the LPE core of Lunarlake and Pantherlake",hardware,2025-11-07 14:06:45,2
Intel,nngrbmr,"LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  4 Darkmont LPE, in therory, should be a significant improvement.",hardware,2025-11-06 18:53:58,13
Intel,nngrskq,"They are definitely more than usable if it’s anything like LNL, which basically defaults to them.  If it’s more like ARL or MTL, it’s placebo except for S0 sleep.",hardware,2025-11-06 18:56:10,14
Intel,nngrybw,"The LP cores in Meteor Lake/Arrow Lake were too weak to be usable as multithreaded boosters, this isn't the case with Lunar Lake and Panther Lake will be no different",hardware,2025-11-06 18:56:56,10
Intel,nngqpfh,"Windows are supposed to be using these LPE cores for lighter task such as web browsing or Word documentation or idling so that the they won't have to activate the rest of the cluster -> Reduced power draw and therefore good battery life. Meteor Lake originated with these but they were so slow they were pretty useless outside of idling. Lunar Lake also had 4 LPE cores functionally and we see how good those cores are, and these Panther Lake are supposed to be built on that but with additional 8 E cores for people that really value multi-core performance and to also solve the one major weakness of Lunar Lake, if it is even that major in the first place.",hardware,2025-11-06 18:51:06,8
Intel,nnhryps,>Aren‘t they even ahead of the skylake IPC by now  They're definitely ahead of Skylake now. Cougar Cove IPC vs Darkmont is gonna be like 10% better,hardware,2025-11-06 21:53:47,15
Intel,nni8z4w,> Only the 4 LP-Cores are kind of weak   Not for LNL or PTL. They're not crippled like the MTL/ARL cores. Should be full speed Darkmont. That's like 12th or even 13th gen big core.,hardware,2025-11-06 23:25:55,13
Intel,nnj1ufe,"The LPE cores being weak is probably the point anyway, or at least partially. It doesn't need to usurp an insane amount of wattage to get to full speed because it doesn't scale well, but it stays fast enough at lower wattage to default to that power profile -> Improved battery life.",hardware,2025-11-07 02:17:05,3
Intel,nnlhwmo,They are ahead of Raptor lake P cores and Zen 4 in IPC under 40W,hardware,2025-11-07 13:59:45,1
Intel,nngwvfw,Honestly I wonder if the 4 e-core cluster on the compute tile is outright more power efficient than the 4 p-cores.    I wish some reviewers would do power efficiency testing with different core count configurations enabled on LNL and ARL. Just for curiosities sake.,hardware,2025-11-06 19:20:53,12
Intel,nnihgw0,"At this point, I just hope they have a handheld chip with just 8 e cores (ensuring the CPU tile is as efficient as it gets) and a decent GPU. Maybe in time for an Intel + Nvidia chip, which could be a dream handheld chip.",hardware,2025-11-07 00:15:32,6
Intel,no19s52,Gaming laptop with iGPU should be niche. Most people just get Nvidia in their gaming laptops.,hardware,2025-11-10 01:40:12,1
Intel,nnin9fr,Apple gets their margins in upselling memory and storage they don't need to worry about the added complexity of more SKUs for the sake of price laddering,hardware,2025-11-07 00:48:54,14
Intel,nngr3we,"Perhaps fully mature was an overstatement, but it wasn’t the first product on N3B, the node was already in HVM for over a year at that point IIRC.",hardware,2025-11-06 18:52:58,11
Intel,nnhhz5t,"We all know that's a lie and 20A was a disaster. Why repeat it now? 18A may not be perfect, but it exists and these chips are comming soon.",hardware,2025-11-06 21:05:14,14
Intel,nnkdpn4,What delays? 2025 node in 2025?,hardware,2025-11-07 08:35:46,0
Intel,nnhmjz8,">BSPD in particular was supposed to help the most at high voltage  Says who?  BSPD improves signal integrity, lowers IR Drop, and helps improve density, at the expense of heat dissipation.   Idk how a technology that is known to increase heat density was supposed to *improve* fMax",hardware,2025-11-06 21:27:24,2
Intel,nngweib,"> but expected to compensate for it with very efficient frequency scaling past peak perf/W   Quite frankly, the only people ""expecting"" that seem to be people on the internet unwilling to admit Intel underdelivered with 18A. There's been no objective reason to believe that was even a target for the node. If anything, the exact opposite. This was supposed to be the node where Intel focused more on low voltage for data center and AI.    > where you rarely push past the best perf/W due to thermal and battery constraints   That is not the case. ST turbo in -H series laptops has always been high, nearly on par with the desktop chips. At the power budgets being discussed, there's plenty available to hit whatever the silicon is capable of, and Intel's never been shy about pushing the limits of thermals.    And again, when you adjust for the power/thermal envelope, you *still* see a clock speed regression vs even ARL-H. The best possible outcome for Intel is actually that they can't hit the same peak voltage but look ok at low to mid.    > Wait for the desktop/server chips before you call it   Server is typically low voltage, or mid voltage at most. It's lower down than even laptop chips. And what desktop? Intel's using N2 for their flagship chips for NVL, which should really have been an obvious indicator for how 18A fairs.    > I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.   Intel had numbers in their white paper. BSPD, all else equal, was maybe a couple percent at high V. It's not going to produce dramatically different scaling, and again, we have direct evidence to the contrary here.    In general, Intel's had a lot of process features over the years that look a lot better on slides than they end up doing in silicon.",hardware,2025-11-06 19:18:34,17
Intel,nnguuaj,"> 18A has always been *Xyz* …  There's always some excuse for Intel's stuff the last years, to accidentally NOT perform as expected, no?  > It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Hear, hear. Isn't that all to convenient …  > Wait for the desktop/server chips before you call it.  … and when is that supposed to happen? Nova Lake will be TSMC's N2.  What *Desktop* CPU-line will be on Intel 18A anyway then?",hardware,2025-11-06 19:10:51,9
Intel,nnkoi67,Give reasons why you claim this is an 'underclock' that was done 'suddenly'.,hardware,2025-11-07 10:27:15,1
Intel,nngu3yg,"Of course, Lunar was quite competitive, yet it was mainly so on performance due to Intel basically sugar-coating the living benchmark-bar out of it via the on-package LPDDR5X-8533 RAM. That OpM for sure *majorly* polished its efficiency-metrics by a mile …  Though looking back the recent years, that's how Intel always masked rather underwhelming progress on their process-technology – Hiding the actual architectural inefficiencies and shortcomings behind a invisible *wall of obfuscation* by only ever deliberately bundling it with other stuff like newer PCi-Express versions of 4.0/5.0 or newer, faster, crazy high-clocking RAM-generations.  So Lunar Lake while very strong, was mainly so due to being propped by OpM, and TSMC's processes of course.",hardware,2025-11-06 19:07:17,-7
Intel,nnhp8md,"The article claims it's ""TDP"", which would typically refer to PL1. Might be leakers getting their terminology mixed up, but I wouldn't necessarily assume a large drop in power limits.",hardware,2025-11-06 21:40:24,4
Intel,nnkfy09,AFAIK Intel was at 165W in mobile back then …,hardware,2025-11-07 08:59:02,0
Intel,nnh5e4g,"> Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?  [6%](https://web.archive.org/web/20240901235614/https://www.anandtech.com/show/18894/intel-details-powervia-tech-backside-power-on-schedule-for-2024/2)",hardware,2025-11-06 20:02:30,6
Intel,nnhouzs,"I wonder if the issues Intel's facing with 18A partially explain TSMC's very conservative approach to adopting ~~it~~ BSPD.  I image in reality, 18A (and AP's) fMax limits are almost entirely the reason for N2 in NVL-S",hardware,2025-11-06 21:38:35,3
Intel,nnh96c0,"> That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  There's no doubt that given enough resources they can fix the node to the point where it's good enough for HVM in high performance chips. It's what they did with 10nm after all but it took them several years to go from poorly yielding Cannon Lake/Ice Lake to okay Tiger Lake and good Alder Lake.  If we take the talk about 18A ""margins"" from the recent analysts call to be a euphemism for something like node yields and a stand in for when it will be ""fixed"" then we could be talking 2027 before 18A gets to that point. A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.",hardware,2025-11-06 20:21:25,8
Intel,nnh7vd8,"Yeah I know they talked about yield issues, but everything indicated that it was not a case of manufacturing defect density, but rather something else.   That would imply issues with hitting target frequencies, which as you say, they should be able to iron out in due time. That is as long as it isn't a fundamental issues, which I have a hard time believing, given how unconcerned they seemed about it.   I'm fairly confident it will be ironed out before Clearwater Forest hits mass production.",hardware,2025-11-06 20:14:54,0
Intel,nngu5ix,"There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery. It offers better voltage control and less current leakage, leading to higher efficiency at any given voltage, but especially past the point where current leakage starts becoming more of an issue, i.e. at very high frequency.  A pipe cleaner running at low frequencies is to be expected. Wait for the desktop chips and you will see P cores hitting well over 6+ GHz advertised all core boost, possibly 6.5 GHz. I wouldn't be surprised to see some users hitting 7 GHz stable.",hardware,2025-11-06 19:07:30,-2
Intel,nnlize0,The modern CPU cores schedule all cores equally with E core priority because nowadays the difference between E core and P core is less than between AMD X3D CCD and higher clocking CCD,hardware,2025-11-07 14:05:46,-1
Intel,nnhabba,"Windows Scheduler has come a long way but I mean come on, this is pretty pedantic. The commenter above is clearly not using a tool to assign cores to specific tasks they are just being annoying.",hardware,2025-11-06 20:27:06,8
Intel,nnh1new,4 skymont already seem pretty good in LNL,hardware,2025-11-06 19:44:19,10
Intel,nnh5cdj,> LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  Makes you think why Intel even went all the way to integrate those and waste precious die-space doing so …,hardware,2025-11-06 20:02:16,2
Intel,nniocav,They could be useful if they could check your email and stuff while the laptop is effectively sleeping.,hardware,2025-11-07 00:55:19,1
Intel,nnhrk78,PLT's LP-E cores take after LNL. ARL/MTL's LP-E cores design is dead,hardware,2025-11-06 21:51:48,3
Intel,nnh5ttp,I hope those are powerful enough to eventually act as the low-power booster these were once supposed to.,hardware,2025-11-06 20:04:40,1
Intel,nnh666b,> Meteor Lake originated with these but they were so slow they were pretty useless outside of idling  The last info I had on the back of my mind was that these couldn't even be associated by the user and were basically reserved for Windows itself. Is that still the case now?,hardware,2025-11-06 20:06:23,1
Intel,nnj4lq1,"Yeah after being reminded that the LPE cores are also Darkmont like the E-Cores I looked up the layout in detail and the differences is simply the P and E cores are on a large Cluster with L3 Cache and a fast ring-bus, but the LPE cores are seperated from that Ring-bus and don't have any L3 Cache (just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster).     They are for sure also much more limited in power-budget and frequency, but they can be very fast in general. The main usage is being able to disable the large cluster in Idle or low cpu usage to heavily reduce power draw.     The communication in between the clusters is much slower. So anything that relies on synchronisation and communication between cores is only really fast (latency-wise) if it is kept within the same cluster.      The LPE cores therefore can be very fast actually, but only if the task stays within the cluster. Modern demanding games will therefore 99% soley run on the large cluster, because they would run slower if they spread across all of them. The small cluster could be used to offload background tasks to themselves (if the powerlimits aren't hit), which could at least improve 1% lows and fps stability.",hardware,2025-11-07 02:33:41,3
Intel,nngxyok,"It's not just more power efficient, but also much faster at low power which you kinda want since you'd be mostly GPU bottlenecked. The good thing is that they've made that part of their thread director tech.",hardware,2025-11-06 19:26:15,11
Intel,nnlikjx,Yes it is. Memory uses power and IPC of LPE should be past Zen 3,hardware,2025-11-07 14:03:29,2
Intel,nnkc1a7,Intel makes e-core only CPUs (N100 etc) but at this point they're based on several gen-old e-core design. They're good enough for what they are so they might not update them for a while still though.,hardware,2025-11-07 08:18:15,1
Intel,nnjcalp,"even 4 e-cores is enough, dont forget we use to pair up i7-7700K with GTX1080/1080Ti.  4 e cores + even stronger iGPU probably a better combo for handheld.",hardware,2025-11-07 03:21:21,9
Intel,nnoc1fy,Apple absolutely does price ladder their SoCs.,hardware,2025-11-07 22:41:38,1
Intel,nngy7u4,"True. But the node itself may be kinda mid. It seemed much, much more complex than N3E, and it's not as if the node was any sort of highlight in the products Apple used it in.   Going back to A17 reviews and such, there were serious questions presented about N3B vs N4P.",hardware,2025-11-06 19:27:29,12
Intel,nnhibo4,I'm fairly sure that's what u/ProfessionalPrincipa was also implying.,hardware,2025-11-06 21:06:56,6
Intel,nnhovl3,> We all know that's a lie and 20A was a disaster. Why repeat it now?   Probably in response to all the people assuming there must be some kind of upside.,hardware,2025-11-06 21:38:39,4
Intel,nnkl8kv,"It was supposed to be a 2024 node. And given the perf downgrade and admitted yield problems, seems more like they're only delivering the originally promised metrics in '26 or even '27.",hardware,2025-11-07 09:54:38,5
Intel,nnl32z1,"> What delays? 2025 node in 2025?  What del— *Seriously now!?* Where you living under a rock the last years by any chance? o.O  18A is neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: *'cause Yields again!*) will be 1H26 and most likely even end of first half of 2026.  So it's a 2H24-node, which Intel is only able to offer actual volume basically +1–2 years later. That's called *»delay«*.  **Edit:** And not to mention the actual massive performance- and metric-regression u/Exist50 pointed out already. 18A is basically 20A in disguise. Since 20A wasn't actually knifed, but just relabeled as ""18A"" instead.",hardware,2025-11-07 12:29:34,0
Intel,nnhohos,> Says who?   Intel. It was part of their own published results about PowerVia on the Intel 4 test chip. The gains range from ~negligible at low-V to around 6% at high-V.    Also explains why TSMC is not adopting it in the same way.,hardware,2025-11-06 21:36:46,11
Intel,nnkdym7,Nova Lake is full on N2?,hardware,2025-11-07 08:38:24,0
Intel,nnocn3l,"It's literally a regression from the prior gen, for a node that was supposed to be ""leadership"", mind you.    I don't know why it's hard to acknowledge that 18A is simply underperforming.",hardware,2025-11-07 22:45:03,1
Intel,nngzzny,"Nah, credit where credit is due. LNL made a ton of fundamental design changes that PTL should also benefit from. Yes, they also benefitted a lot from both having an actually decent node and on-package memory, and no, they're not on par with the likes of Apple or Qualcomm, but merely making ARL monolithic on N3 would not have delivered these gains.",hardware,2025-11-06 19:36:12,11
Intel,nnhs780,They definitely have their terminology mixed up. There's no chance PTL-H has an 65W/80W PL1,hardware,2025-11-06 21:54:56,7
Intel,nnpn3zz,Which was never the PL1 but rather the PL2.,hardware,2025-11-08 03:40:00,3
Intel,nnhxmr5,"Didn't TSMC delay their BSPD node too? Ik they changed it from appearing in N2P vs A16, but unsure if there was a timeline shift in that as well.   Honestly, what's going on with N2P/N2/A16 seems to be kinda weird, timeline wise.",hardware,2025-11-06 22:22:47,3
Intel,nnhappm,2027 just seems like hitting baseline parametric yields. Being able to get to an actual improvement in terms of performance is a whole other thing. Not an easy road.,hardware,2025-11-06 20:29:05,6
Intel,nnkqty9,">A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.  How it can be 24 product in 27 when it was 25 node in 25 with products on shelfs in january 26, rofl?",hardware,2025-11-07 10:49:31,0
Intel,nnhg56o,Intel themselves claim they won't be hitting industry acceptable yield till 2027 for 18A.,hardware,2025-11-06 20:56:18,3
Intel,nngyuum,"> There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery.    No, it's not. The main long term advantage of BSPD is to reduce the pressure on the metal layers as they do not scale like logic does. Better PnP is another advantage, but secondary. No one cares much about high-V these days.    > A pipe cleaner running at low frequencies is to be expected   As a reminder, ARL-20A was supposed to be the pipe cleaner, and they claimed they cancelled it because 18A was doing so well! PTL was supposed to be the volume product, a year after 18A was nominally supposed to be ready.    > Wait for the desktop chips   What desktop chips? PTL-S was cancelled long ago, and for NVL, they're moving the good compute dies (including desktop) back to TSMC on N2. A decision which should demonstrate the node gap quite clearly...   Also, ST boost for -H chips is in the same ballpark as desktop ones. They're already at the high end of the curve.",hardware,2025-11-06 19:30:37,12
Intel,nnoxxwo,"They aren't the same though, hence the roll of the dice to see if you actually get the performance you paid for.",hardware,2025-11-08 00:53:15,2
Intel,nnh7b83,Because they still improve battery life under very light loads.,hardware,2025-11-06 20:12:05,8
Intel,nnj1km0,Yeah I think you had to utilize tool such as Project Lasso. I tried to assess the LPE cores to HWINFO64 and it was rather painful to use so I understand why these cores are never used otherwise lol.,hardware,2025-11-07 02:15:28,1
Intel,nnj555b,"Yeah. The efficiency and overall battery life will probably comes down to how Windows manages threads, which seems to be pretty decent now with Lunar Lake seeing as they got pretty good battery life with the same design philosophy.",hardware,2025-11-07 02:36:58,2
Intel,nnkn219,"> just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster  It's also accessible by the large cluster, fyi.",hardware,2025-11-07 10:12:52,2
Intel,nnkd975,"For those games back in the day, yeah. But modern games can and do use more than 4 cores.   Depends how low end/power you want to go, of course. 4 cores + eGPU cuold be enough too depending on your games: [https://youtu.be/XCUKJ-AgGmY?t=278](https://youtu.be/XCUKJ-AgGmY?t=278)",hardware,2025-11-07 08:30:54,8
Intel,nnn8cyi,"The e-cores are unlikely to be hyperthteaded though. 8 e-cores gives you 8 threads just like a 7700K did, though none of them share cores. It feels like a sweet spot for a handheld in 2025 imho.   This is while AMD competitors use 8c/16t of full Zen. An 8 e-core solution would remain weaker but more than sufficient on the CPU side, while allowing enough power for what truly matters (a capable GPU). If paired with an Nvidia iGPU, that could be the perfect handheld chip, if their partnership produces one like that.",hardware,2025-11-07 19:12:43,3
Intel,nnhq813,"Those ""people"" are either bots or trolls. No sane person thinks Intel is crushing it with 18A.",hardware,2025-11-06 21:45:18,7
Intel,nnkrne6,"Whats the name of the close the gap initiative?  5 nodes in 4 years... announced in 2021  2021+4=2025  You can even read 3rd party articles from 2021 talking about 18A in 2025.  https://www.tomshardware.com/news/intel-process-packaging-roadmap-2025   >Intel didn't include it in the roadmap, but it already has its next-gen angstrom-class process in development. 'Intel 18A' is already planned for ""early 2025"" with enhancements to the RibbonFET transistors.",hardware,2025-11-07 10:57:07,1
Intel,nno64t2,">neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: 'cause Yields again!) will be 1H26 and most likely even end of first half of 2026.  How it aint 25 node when there will be products on shelves in january 26?",hardware,2025-11-07 22:08:46,2
Intel,nnljpf8,FMax at same power consumption is higher true. But the temperature is just too high. Both temperature and power consumption are important considerations,hardware,2025-11-07 14:09:47,2
Intel,nnklaca,"High end NVL is N2, low end is 18A. At least for compute dies.",hardware,2025-11-07 09:55:07,3
Intel,nnl268a,"That's what we know so far, yes. At least the performance-parts. The lower end is supposed to be 18A, I guess?",hardware,2025-11-07 12:23:00,0
Intel,nnhcrle,"> Nah, credit where credit is due.  I said verbatim, that Lunar Lake was quite competitive, and I meant it, unironically. It took long enough.  > LNL made a ton of fundamental design changes that PTL should also benefit from.  I haven't disputed that —  The modular concept was surely ground-breaking for Intel, and urgently needed!  Though, it's kind of ironic, how Intel all by itself proved themselves liars, when it took them basically +6 years since 2017, for eventually coming up with only a mere chiplet-copycat and their first true disintegrated chiplet-esque design …  For a design, which *in their world-view*, Intel was working on their tiles-approach already since a decade, which is at least what they basically claimed when revealed by 2018 – I called that bullsh!t the moment I first heard it. Surprise, surprise, they again straight-up *lied* about it …  They most definitely did NOT have had worked on anything chiplets/tiles before, if it took them *this* long.  Just goes to show how arrogant Intel was back then, letting AMD cooking their Zen in complete silence since 2012/2013, for their later Ryzen. *Still boggles my mind, how Intel could let that happen* …  > Yes, they also benefitted a lot from both having an actually decent node and on-package memory […]  In any case, we can't really deny the fact, that Intel basically cheated on Lunar Lake using OpM, eventually creating a halo-product, which ironically was quite sought after, but yet expensive asf to manufacture.  If AMD would've been to cheat like that (using OpM) in a mobile SKU, while dealing some marked cards in such a underhanded manner, it would've been ROFL-stomped Lunar Lake …",hardware,2025-11-06 20:39:31,-2
Intel,nni6v1y,"Willing to believe that, though then I have to wonder why it would be so hard to cool at such a substantially reduced PL2. Also if/how they could pitch this as an HX replacement.",hardware,2025-11-06 23:13:41,3
Intel,nnq97pr,"Yes, I think it was 45- or 65W-parts PL1-wise. The spread was still obscene and outright insane.",hardware,2025-11-08 06:42:53,0
Intel,nnhgp5t,I'm excited for 18A-P. Intel subnode improvements have always seemed to bring decent uplifts (Intel 10SF being a notable example).,hardware,2025-11-06 20:58:58,2
Intel,noaeih3,"The difference is no longer stark. As I said, its now smaller than between 2 CCDs in the most popular CPU line ever made in actual raw performance. ie it's not something entirely unique at the scale the differences are",hardware,2025-11-11 14:51:59,1
Intel,nnheh7k,"How even, if these weren't even used with MTL!?",hardware,2025-11-06 20:48:06,-4
Intel,nnkfcmn,"Yup, pretty much paper-cores for marketing-reasons alone basically.",hardware,2025-11-07 08:52:54,1
Intel,nnlies1,We already have games tested on Skymont E cores. They are very fast,hardware,2025-11-07 14:02:35,4
Intel,nnoujai,"Yes we know modern games can use more cores, but having smaller amount of cores means saving power budget. Those power consumption saved are better off use to juice iGPU.   Lets not forget the E-Cores here are skymont, not skylake. Skymont is definitely more powerful cores. The exact same reason why i5-7600K is more power than i7-2600K.  given how iGPU for handheld are no way near GTX1080 yet. I think it is better to pump more juice for better iGPU.",hardware,2025-11-08 00:32:08,1
Intel,nnhveh1,Go to the intel stock subreddit lol,hardware,2025-11-06 22:11:05,9
Intel,nni71qc,"You'd be surprised how many people are willing to take Intel at their word on it.  Just look at previous threads here. Every time Intel posts nonsense slides, lot of people come crawling out of the woodwork.",hardware,2025-11-06 23:14:46,2
Intel,no3ffkc,"They are doing about as much as the ""sane"" people expected from 18A.",hardware,2025-11-10 12:20:33,0
Intel,nnku399,"> 5 nodes in 4 years... announced in 2021  It was supposed to start with Intel 7 in '21 and end with 18A readiness in H2'24.   https://img.trendforce.com/blog/wp-content/uploads/2023/10/17144859/intel-4-years-5-nodes.jpg  And now in the way end of '25, we're getting something more like what they originally promised for 20A, *if that*, and now they're saying yields won't be ""industry standard"" until as late as 2027. It's a disaster by any objective standards. Their failure with 18A literally got the CEO fired...",hardware,2025-11-07 11:19:07,5
Intel,nnpmsrt,"> How it aint 25 node when there will be products on shelves in january 26?  It seems, you don't actually grasp the concept of the term »difference«. *2025 and 2026 is actually NOT the same!*",hardware,2025-11-08 03:37:46,1
Intel,nnkm46i,so the igpu is propably tsmc too?  do we know if thats because of poor performence or poor manufacturing capazity?,hardware,2025-11-07 10:03:27,1
Intel,nni73m8,I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   I think they'll probably have ARL-R for HX,hardware,2025-11-06 23:15:04,3
Intel,nnqeasg,"Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage. Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.",hardware,2025-11-08 07:33:02,1
Intel,nnhrg1s,"The LP-E cores were a failure in MTL/ARL's design.  They're very useful in LNL and that trend follows with PTL/NVL.  They're definitely not just ""placebo cores"" - LNL uses them more often then the P cores and they can be activated and used in all core workloads as well.",hardware,2025-11-06 21:51:15,4
Intel,nnhp1ul,"They were, just not as often as Intel would have liked.",hardware,2025-11-06 21:39:29,3
Intel,nnkr81u,"For MTL and ARL, yes. For LNL and PTL though, I haven't test them out personally but seeing the battery results of LNL chips made me think that the LPE cores of those things seem to be legit.",hardware,2025-11-07 10:53:10,1
Intel,nnm0kgv,"Oh year, right, I think I missed (or forgot) that. Darkmont should be even better but I'm guessing not by that much.",hardware,2025-11-07 15:37:40,2
Intel,nnhvpi9,Kinda making my point.,hardware,2025-11-06 22:12:40,9
Intel,nnsiatm,"Bro xd  Node readiness and product readiness and product on shelves are 3 different date  Node must be ready before product, product must be ready before being on shelves.  Officially 18A was ready around Q2 25",hardware,2025-11-08 16:59:43,5
Intel,nnkmum2,"> so the igpu is propably tsmc too?  18AP, iirc. Good *enough* (or at least, assumed to be when originally planned) not to be worth the extra cost of TSMC.  > do we know if thats because of poor performence or poor manufacturing capazity?  Poor performance. Intel can't afford to be constantly a node behind. It's just too big of a gap.",hardware,2025-11-07 10:10:47,3
Intel,nni82fk,"> I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   But that's exactly it. They said 65W vs 80W was in relation to cooling challenges, but either is more than sufficient for max ST boost. Idk, maybe reading too much into too little information.    > I think they'll probably have ARL-R for HX   At one point it sounded like they wanted to pitch PTL as a partial replacement. Guess it's not good enough though.",hardware,2025-11-06 23:20:38,3
Intel,nnqhka1,"> Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage.  Of course, I already knew that. Binned Desktop.  > Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.  Well, up until 2018/2019/2020 Intel actually didn't really uncapped any of them and those parts were hard-limited to only pull their max 45–65W TDP.  Only later on when they were trying to hold pace with AMD, Intel insanely increased the TDP to pull to insane numbers (90W, 135W, 165W) … Other than that and prior, you had to resort to modded BIOS.  So until even Apple burned their thick skin on some i9 (**Cough* i9-9980HK in the 16"" MacBook Pro 2019), you didn't had such insane TDPs to begin with anyway (except for BIOS-mods).",hardware,2025-11-08 08:06:08,0
Intel,nnkfnwq,"As said, I owned a MTL-machine back then, and back then you couldn't even associated these cores after all, since you couldn't pin anything on them – So yes, *back then*, these were placebo-cores and they were in fact pretty much 'on paper-cores for marketing-reason' and basically useless.  Seems Intel managed to improve their performance a lot and make them actually useful, which is good!",hardware,2025-11-07 08:56:07,0
Intel,nnl1o21,"Yes, MTL's LPE-cores were basically a dud, LNL was fairly workable and PTL will be hopefully potent.",hardware,2025-11-07 12:19:17,1
Intel,nnhvsow,haha,hardware,2025-11-06 22:13:07,6
Intel,nnxlj8j,> Officially 18A was ready around Q2 25  Which is still at least 4 quarters *past* its due-date actually …,hardware,2025-11-09 14:16:26,1
Intel,nnqk8rl,"That's not strictly true, the (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  https://www.ultrabookreview.com/27215-asus-rog-g703gx-review/#a5  >That comes with a few drawbacks, though, and one of the most important is the noise development. Asus provides three different power modes for this laptop, which you can use to juggle between performance, thermals, and noise:  Silent – CPU limited at 45 W and 4.2x multiplier, GPU limited at 140 W, fans only ramp up to about 40-43 dB in games; Balanced – CPU limited at 90 W and 4.2x multiplier, GPU limited at 140 W; Turbo – CPU limited at 200 W and 4.7x multiplier, GPU limited at 200 W, fans ramp up to 55-56 dB in games.  It was always up to the laptop OEM to configure the power limit and there was no hard cap enforced by Intel.",hardware,2025-11-08 08:33:24,1
Intel,nnndfa5,"The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, negating the entire point of them.  In addition, MTL/ARL had the LP-E cores on the SoC tile which made them functionally useless for full nT load.  LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks *can* stay entirely within the LP island, *and* they're on the same compute tile, so they're also used in full nT workloads.",hardware,2025-11-07 19:38:34,3
Intel,nnqndqo,"> That's not strictly true …  Yes it actually is.   > The (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  Dude, you're basically confirming exactly what I wrote, by literally picking a **i9 9980HK**-equipped notebook of 2019, which is even exactly the very infamous SKU I was talking about …  I was talking exactly on (among else) that very CPU, even Apple couldn't tame and had to undervolt (still without being able to prevent the later sh!t-storm) until eventually abandoning Intel altogether.  ---- All I'm saying is, all the years prior with only quad-core, Intel \*never\* (nor any OEMs for that matter) went above and beyond the official TDP – The only lone exception from this, were those super-bulky Schenker Desktop-replacements.  That only suddenly changed by 2017–2019, when Intel suddenly increased core-count in mobile swiftly from Quad- to Hexa- and ultimately Octa-cores, while pushing the TDP in quick succession  into insane territory.  *That was the time, Asus took about a year to manage applying liquid-metal en masse, for Intel-notebooks!*  You are right with your sample here, yet you only confirm what I said before. Only past quad-cores it was.",hardware,2025-11-08 09:05:43,0
Intel,nnprgvn,"> The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  Do you happen to know, if that (ring-bus related) was the same principle on MTL?  > The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, **negating the entire point of them**.  Yeah, talking about throwing some completely untested sh!t onto the market, irregardless of the fact, if the product makes sense or not — A true classic, I'd say. Just Intel being Intel.  Since dropping those LPE-cores on MTL, would've actually made the mask and thus needed die-space *smaller*, which in turn would've actually *increased* their yields \*and\* in return profit-margins already …  But muh, benchmark bars and ""AMD has moar corez!!"", I guess.  > LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks can stay entirely within the LP island, and they're on the same compute tile, so they're also used in full nT workloads.  Is there any greater penalty (latency or cache-flush-wise) for moving threads off the LP-island to P-cores?  *Thanks for the insides so far though!* I can't really see through anymore to all these constant arch-changes.",hardware,2025-11-08 04:12:06,1
Intel,nmflglf,"I recently picked up an MSI Claw 7 AI+, and have been pleasantly surprised with the current state of their drivers. I would absolutely be interested in a Celestial GPU if they have anything targeting 9070~ or higher performance level. Here's hoping!",hardware,2025-10-31 20:39:13,31
Intel,nmdl0rr,I'm glad they are doing these followup videos. Gotta give Intel credit where due - their cards are getting so much better over time.,hardware,2025-10-31 14:32:50,58
Intel,nmm72sv,The Lunar Lake handhelds would be really popular if they were just a little cheaper.,hardware,2025-11-01 23:31:51,2
Intel,nmf7n07,They said their high idle power is an architecture issue so they can't fix that,hardware,2025-10-31 19:25:22,16
Intel,nme3l5e,God forbid Intel supports Day 1 GPU drivers longer than 5 years,hardware,2025-10-31 16:03:57,-22
Intel,nmfdha7,"true but thats not really that big of a deal, and can be fixed with some tweaks [https://www.reddit.com/r/IntelArc/comments/161w1z0/managed\_to\_bring\_idle\_power\_draw\_down\_to\_1w\_on/](https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/)",hardware,2025-10-31 19:56:31,12
Intel,nmvlitt,So just like AMD then.,hardware,2025-11-03 13:41:12,2
Intel,nmfexoy,"They usually do tho? We dont know about their dGPUs yet because it hasn't been 5 years.  Their Xe Laptop gpus are 5years+ and are supported as of this month.  No idea why you're hating on intel GPUs mate, i'm used to everyone cheering the underdog not the otherway around.",hardware,2025-10-31 20:04:15,25
Intel,nmhjwz4,"I think that guy's a misinformed user that is saying that AMD 5000/6000 series won't get driver updates anymore lol, when AMD just said that those gens aren't slated for game optimizations that use their ""latest technologies"" (FSR4 at the time of writing) anymore. Those gens still will get driver updates but AMD gave up on trying to implement FSR4 on them, like the leaked scripts earlier this year had performance reduction from 20% to up to 30% FPS to use FSR4 for 6000 series cards since they don't have the hardware and just using brute force software for FSR4.   I'd agree that my 6700 XT never getting FSR4 sucks, but the misinfo circling around is that we'd never get anymore driver updates lol.",hardware,2025-11-01 04:34:17,8
Intel,niqvrj6,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? … but Xe3 plus will be enough to call it C series.",hardware,2025-10-10 09:51:49,30
Intel,nis4lzk,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",hardware,2025-10-10 14:45:18,13
Intel,nisn7g8,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,hardware,2025-10-10 16:15:59,4
Intel,niz80tx,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,hardware,2025-10-11 18:17:30,1
Intel,njgcnqq,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",hardware,2025-10-14 14:31:48,1
Intel,njxoogl,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",hardware,2025-10-17 07:35:47,1
Intel,nir35ti,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",hardware,2025-10-10 10:58:36,21
Intel,nir6blp,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",hardware,2025-10-10 11:23:30,15
Intel,nis0as0,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,hardware,2025-10-10 14:23:44,4
Intel,nisagox,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes Xe³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   Xe³ is a Xe³ based architecture like Xe³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. Xe³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",hardware,2025-10-10 15:13:44,4
Intel,njr6343,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",hardware,2025-10-16 06:29:51,1
Intel,nit88y1,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",hardware,2025-10-10 17:58:19,7
Intel,njxozxf,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,hardware,2025-10-17 07:39:00,1
Intel,niruidx,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,hardware,2025-10-10 13:53:56,5
Intel,nircpja,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,hardware,2025-10-10 12:08:58,28
Intel,nisb3yn,That's not what Peterson was talking about context wise when he addressed this in the video.,hardware,2025-10-10 15:16:53,2
Intel,nito8rq,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entry‑level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",hardware,2025-10-10 19:19:56,-4
Intel,nis0ezd,[AMD RDNA 3.5’s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),hardware,2025-10-10 14:24:18,13
Intel,nirfv6f,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",hardware,2025-10-10 12:29:32,16
Intel,niw1ezc,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",hardware,2025-10-11 04:09:31,5
Intel,njr69nx,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",hardware,2025-10-16 06:31:38,1
Intel,nisec0d,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",hardware,2025-10-10 15:32:43,1
Intel,nizcijr,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,hardware,2025-10-11 18:41:45,1
Intel,nitxcqr,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",hardware,2025-10-10 20:07:36,1
Intel,njgdemn,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,hardware,2025-10-14 14:35:38,1
Intel,nitzoza,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",hardware,2025-10-10 20:19:35,1
Intel,nj6xna9,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,hardware,2025-10-13 00:16:37,2
Intel,nj7rqtc,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",hardware,2025-10-13 03:27:42,1
Intel,nifgt95,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",hardware,2025-10-08 14:41:36,38
Intel,nighktm,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",hardware,2025-10-08 17:39:48,30
Intel,nihvtct,MLID must have an aneurysm seeing the guy still employed at Intel,hardware,2025-10-08 21:50:09,14
Intel,nifmhnd,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",hardware,2025-10-08 15:09:23,30
Intel,nihklk5,Igpus not discrete gpus,hardware,2025-10-08 20:51:31,8
Intel,nigirli,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",hardware,2025-10-08 17:45:26,10
Intel,nii5519,Hahaha I can hear Tom saying “fucking Tom Peterson has been *lying* to you” as you say that,hardware,2025-10-08 22:44:00,9
Intel,nifuaux,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,hardware,2025-10-08 15:47:29,11
Intel,niis0eh,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,hardware,2025-10-09 01:01:50,4
Intel,nigp77c,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,hardware,2025-10-08 18:16:46,13
Intel,nigl085,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",hardware,2025-10-08 17:56:04,0
Intel,nilqzn1,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,hardware,2025-10-09 14:41:05,3
Intel,nig3u1g,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,hardware,2025-10-08 16:33:52,18
Intel,nijp5fy,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,hardware,2025-10-09 04:46:48,5
Intel,nigpqtx,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",hardware,2025-10-08 18:19:29,8
Intel,nihx213,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",hardware,2025-10-08 21:56:59,3
Intel,nigh9jb,You stole what I was going to say... take my upvote.,hardware,2025-10-08 17:38:17,6
Intel,nijrll5,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",hardware,2025-10-09 05:07:37,9
Intel,niixhey,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  One thing seems clear, this solution will be cheaper than standard AI data center products.  Hopefully, Intel can get and maintain footholds in medium-sized AI research market",hardware,2025-10-09 01:35:25,72
Intel,nij1t7l,No way that power usage is at idle. Even if there was only one performance state like Tesla GPUs they wouldn't consume that much power.,hardware,2025-10-09 02:02:14,32
Intel,nijk1dw,that's insane vram density,hardware,2025-10-09 04:05:01,12
Intel,nijb3xq,"This thing only has a 1Gb Ethernet port? I don't know much about the use case for this thing, but that seems surprisingly low. Simply from the use case of uploading training data to this I feel like something faster is necessary.",hardware,2025-10-09 02:59:53,16
Intel,niiwt1u,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-09 01:31:12,2
Intel,nimo4dq,Is there a fork of chrome that runs on gpus,hardware,2025-10-09 17:24:22,2
Intel,nindfe4,"Now do it with pro dual version of b770s, 64gb each. Could be a far more economical inference solution than what AMD is providing",hardware,2025-10-09 19:31:05,2
Intel,nikln6u,but is that faster than a single 5090?,hardware,2025-10-09 10:10:09,2
Intel,nik40r8,Is this enough VRAM for modern gaming?,hardware,2025-10-09 07:06:00,-3
Intel,nil0agd,Nvidia: ill commit s------e,hardware,2025-10-09 12:07:51,-7
Intel,nil8gj0,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  If they rely on PCI-E for interconnect, bandwidth will be anemic between cards for training purposes. Might be ok for inferencing, but for training, you need the bandwidth.  That's not unclear at all.  This thing is probably not useful for anything, but serving many users with small models for inferencing.",hardware,2025-10-09 12:59:34,17
Intel,no4mb2m,"I doubt we'll see any UALink this year, but perhaps  on the 160GB  Crescent Island card next year.  Intel hasn't announced anything, but it seems obvious.",hardware,2025-11-10 16:22:54,1
Intel,nikh8oe,"You think Intel is going to maintain these footholds any longer than they would need to?  This push of their dedicated GPUs into the professional space, is just a mere attempt to get rid of the unsold inventory of ARC-GPUs at the highest possible price-tag right from the beginning – Dumping those into the enterprise- and datacenter-space, in noble hope that someone clueless might bite upon the Intel-brand, and then dip into it at high costs *at non-existing support software-support* Intel pretends to deliver some time in the future.  Since as soon as the bulk of it was sold, Intel will cut the whole division and call it a day, leave anyone hanging dry.  That's what's planned with the whole professional-line of them – Buying this, is a lost cause as a business.",hardware,2025-10-09 09:25:55,-14
Intel,nij5zhc,"I think they've just taken the 400W per card TDP and multiplied it by 16 to get the 6400W figure, so I think they're trying to say minimum power draw at full load, not at idle. ""Minimum power draw"" is definitely really odd phrasing for that number. Surely you'd think they'd say ""Total GPU TDP"" or something less confusing",hardware,2025-10-09 02:27:37,39
Intel,niml0xn,I don't think servers are supposed to stay idle for long.,hardware,2025-10-09 17:09:18,2
Intel,niqkqmc,"> the 768 GB model uses five 2,700 W units (totaling 10,800 W), while the 384 GB version includes four 2,400 W units (7,200 W)    And an insane power consumption.",hardware,2025-10-10 07:57:46,2
Intel,nijeto3,You basically always put in dedicated NICs in those kind of servers. The onboard ethernet is rarely used.,hardware,2025-10-09 03:25:25,43
Intel,nilc2tl,Could that make it very cost effective for any particular use cases?,hardware,2025-10-09 13:21:04,6
Intel,nim2kjx,"Even the fancy NVIDIA interconnects are a bottleneck for training performance, as are the memory bandwidth and even L2 bandwidth. One architects the algorithm around that. A tighter bottleneck is obviously not ideal, but it might still make sense depending on the price of the hardware.  As long as a copy of the full model and optimization state fits on the 24GB bank the PCI-E interconnect isn't really too limiting for training. Small models, like a 1B parameters voice model, will naturally fit. The larger the model you want to train, the more complex the architecture might be, but there are ways to train even models over 100B parameters on such hardware, like routing distributed fine grained experts. Not that I think it's a good idea at this point.",hardware,2025-10-09 15:38:17,4
Intel,nij8lcp,It's VideoCardZ so they probably got information from a tweet or Reddit post and copy/pasted without doing any thinking whatsoever.,hardware,2025-10-09 02:43:33,31
Intel,nijhbrl,"I assumed that would be the case, I just didn't see any room for expansion. It's pretty crowded with 16 full sized cards.",hardware,2025-10-09 03:44:18,4
Intel,nilfpel,"I don't really see any, unless you can find a workload that will fit the card and multiply that workload for 32 users, but as each chip is performance wise less than a 3090, it has to be a fairly light workload.",hardware,2025-10-09 13:41:48,5
Intel,nin9mev,"1B might work for a voice model, but useful LLM models ARE 100GB+. Smaller models are fun little toys.",hardware,2025-10-09 19:11:33,3
Intel,nijt662,At least its a human hallucination and not AI hallucination.,hardware,2025-10-09 05:21:24,18
Intel,nik407y,Can also be bad translation.,hardware,2025-10-09 07:05:52,2
Intel,nijj2tk,"according to the datasheet theres two free pcie 4.0 x16 slots on one version and two pcie 5.0 x8 slots on the other one. Both say ""support high speed NIC"" so I assume there is at least enough room for that",hardware,2025-10-09 03:57:40,18
Intel,nilgnxj,"I wonder if these might make good controllers for ai powered robots. Something highly specialised, like a fruit sorter or something. I don’t know. I’m just hoping Intel can find a buyer because it’s good for the market to have more competition",hardware,2025-10-09 13:47:09,2
Intel,nili8e7,"It's a thing where the next generation of this card would be a viable competitor for the generative AI crowd, but not this one, where it can't compete with a 5 year old 3090.  For robotics, you just want inference. There are much better options focused on low power, small form factor NPUs custom made exactly for that, and they can be paired with small SBCs like a Raspberry Pi.",hardware,2025-10-09 13:55:46,5
Intel,niokcbz,No word on OMM so I guess that's reserved for Xe3-P. Wonder what other changes that architecture XE3-P will have.  The new Asynchronous RT Unit functionality in Xe3 is very cool and now Intel also has Dynamic VGPR. Neat.,hardware,2025-10-09 23:22:33,12
Intel,nilzeoj,"It's interesting they lump it in with B-series, given the architecture is pretty distinctly different from Xe2 used in Battlemage. Note that BMG/Celestial names have been explicitly used for dGPUs, however. They're not synonymous with Xe2/Xe3.",hardware,2025-10-09 15:22:43,26
Intel,nime2oj,"It would be nice if Intel released a small, budget friendly, passive cooled, PCIe powered dGPU with multiple QuickSync units, made specifically for AV1 encode and H.265 4:2:2 chroma with 10-bit depth.   It's unfortunate that one has to cough up $2,000+ (at least around here) for an RTX5090 to have three NVENC units at their disposal, and the 5090 still can't compete with a passive-cooled Apple Macbook that draws a couple of watts at peak.   You know things are crazy when Apple is the only company that caters to your specific needs!",hardware,2025-10-09 16:35:12,19
Intel,nilf97b,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-09 13:39:16,1
Intel,nim7yeq,Very bleak chance that Intel will continue with discrete gpu line up. Pretty shit decision tho,hardware,2025-10-09 16:04:46,-4
Intel,nilnsw1,"So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In&nbsp;card under this precisely defined term.  Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – *In favour of Nvidia-GPUs?*",hardware,2025-10-09 14:24:56,-25
Intel,nionxww,Xe3P needs that feature parity with RDNA 5 and RTX 60 overall since its a 2027 product.,hardware,2025-10-09 23:43:45,4
Intel,nimlaz8,"hopefully that means NVL-AX is not getting axed. at least, the 24 core die. heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch. super weird, not believing too much into it. curious about Druid, people seem optimistic, idk why lol.",hardware,2025-10-09 17:10:41,4
Intel,nimhv2z,yeah super weird -- but that does bode well for the next dgpu being based on Xe3,hardware,2025-10-09 16:53:51,5
Intel,nimkx2o,"Arc Pro B50 was pretty interesting to me, it was a modern 75W card that isn’t gimped.",hardware,2025-10-09 17:08:47,21
Intel,nin877l,With a board with lots of PCIe x16 slots you can stack A310's or A380s and get along fine. I do 3 transcodes of av1 great on a single one right now.,hardware,2025-10-09 19:04:21,9
Intel,nir42up,The reason they'll never make this is the same reason they don't put SR-IOV on consumer cards.,hardware,2025-10-10 11:05:57,5
Intel,nirvt3d,Can you do this work on a $600 Mac mini? Or do you need something more expensive.,hardware,2025-10-10 14:00:42,2
Intel,nixqbsp,"Yeah prob, but by how much. Looking forward to the post CES reviews and Xe3 better be a major leap over Xe2.",hardware,2025-10-11 13:25:20,1
Intel,nimkw5p,They will if they have xe cores that gen. They'll only fully kill it if they abandon that core development entirely to exclusively use Nvidia graphics tiles,hardware,2025-10-09 17:08:39,6
Intel,nilrz1f,">Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  What in the world are you talking about. This roadmap says nothing of the sort.",hardware,2025-10-09 14:45:58,23
Intel,nilwkhh,"No, they are saying the GPU in Panther Lake is Xe3, but instead of classifying it as a new-gen architecture, they are saying it's just an extension of Xe2, so they've put it under the B series product family.",hardware,2025-10-09 15:08:45,10
Intel,nioq01t,100% but I fear this is another botched gen so it will at best be on par with current gen offerings.,hardware,2025-10-09 23:56:01,4
Intel,nirvexf,"Intel buying chips from Nvidia won't be as cost efficient as them developing their own chips. Intel can't abandon GPU development in the short term or it'll be left behind in the long term (Nvidia can easily sell their Intel shares and pivot away from Intel leaving Intel with nothing).    If Intel will stick to making dGPUs and expand to competing in gaming with said GPUs they benefit greatly from the experience and talent that dGPU development yields, this makes dGPU development a no brainer. If AMD can afford to make dGPUs Intel definitely can as well.",hardware,2025-10-10 13:58:40,3
Intel,nimlpyk,"> heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch  Would be very interesting if so, though I'd be surprised if projects get resurrected in this spending climate. Competitiveness might also be tight in '27.",hardware,2025-10-09 17:12:42,4
Intel,nimm20x,Celestial was based on Xe3p.,hardware,2025-10-09 17:14:19,8
Intel,niokiba,"Then that can't just be a minor tweak, but perhaps it's just market segmentation.",hardware,2025-10-09 23:23:33,1
Intel,niy2dcz,I don't think Mac minis support hardware AV1 _encoding_. Apple only recently supported hardware AV1 _decoding_ so their devices aren't the best suited to AV1 media,hardware,2025-10-11 14:38:03,3
Intel,nimmzhl,> They will if they have xe cores that gen  Why do you believe so? They made iGPUs without dGPUs before.,hardware,2025-10-09 17:18:48,14
Intel,nirvxip,"They won't Nvidia tiles are for specific market segments, they aren't a one size fits all solution.",hardware,2025-10-10 14:01:21,3
Intel,nin3vrr,That's not what my colleague's say,hardware,2025-10-09 18:42:31,2
Intel,nilz2fy,It wouldn't be a Helpdesk_Guy comment if it didn't include insane made up bullshit to disparage Intel.,hardware,2025-10-09 15:21:03,24
Intel,nim3npl,"Exactly. *Good Lord are y'all shortsighted!* Are you falling for their shenanigans and typical tricks again?  Crucial is, what Intel does NOT mention – That is anything future dedicated graphics, as in *Add-In&nbsp;cards*.  They're purposefully declare PTL as de-facto dedicated GPU, yet any thing of future **d**GPUs is ABSENT.",hardware,2025-10-09 15:43:35,-13
Intel,ninrarq,"Congrats, you looked at the pictures. We all did. Now look at it *again* …  Now explain to me, why ARC alchemist is separated (since it's a line of dedicated graphics-cards, duh?!), while Battlemage gets PTL's X^e Graphics iGPU-tiles put alongside, despite such tiles classically does NOT count as a graphics-card but rather a iGPU (no matter how large).  The actual answer is, that this on the slide was surely NOT done slide by accident, but it figuratively puts PTL's X^e Graphics tiles of PTL's iGPU *on par* with their dedicated Battlemage-cards, which (at least in Intel's eyes) shall from now on signal a ""continuation"" of  Battlemage itself. Thus, there won't be any further dedicated graphics-cards like Celestial as classical Add-In cards, which still is way overdue anyway …  Since if not so, then WHY are PTL's iGPU-tiles labeled as ""Intel ARC B-Series""?! *You see the virtue signalling?*    You're also aware, that Intel recently dropped the Intel Iris&nbsp;Graphics theme and now calls all iGPUs ""ARC""?",hardware,2025-10-09 20:40:49,-5
Intel,niy1tvt,">Intel can't abandon GPU development in the short term  Well Intel now uses the same architecture across both their iGPU and dGPU, so even if their dGPU products get killed, iGPU demand(indirectly, through Core Ultra processor demand) can fund continued GPU architecture dev. Intel still has the majority market share in the iGPU market, so they have the demand to justify continued R&D into GPU dev for iGPU alone.",hardware,2025-10-11 14:35:03,1
Intel,nj09dos,Xe3p is a significant architectural advancement says Tom Petersen.,hardware,2025-10-11 21:46:07,2
Intel,nim48wq,"Care to elaborate where I was blatantly wrong and it didn't actually turned out mostly or even exactly as I predicted prior when popping the prospect of Xyz, often months to years in advance?  A single example would be sufficient here, mind you.",hardware,2025-10-09 15:46:27,-13
Intel,nim6va1,"They did not declare PTL as a de-facto dedicated GPU. By definition it's *not* a dedicated GPU and they never said it is.  They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs. You're trying to fill in missing info with a nonsense interpretation.   What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".",hardware,2025-10-09 15:59:21,9
Intel,nimks6h,"So much buzzwords, yet it sounds like a stroke.  You need help.",hardware,2025-10-09 17:08:07,6
Intel,nip4uap,"Alchemist had both desktop discrete and mobile discrete. Intel's roadmap isn't a 'graphics card only' chart, it's an IP roadmap. That's why Panther Lake's Xe3 iGPU shows up under Arc B-series, they are saying Xe3 is an extension of Xe2, where as Xe3P (Celestial) is the next Arc family.",hardware,2025-10-10 01:23:47,5
Intel,nim4t3o,"> A single example would be sufficient here, mind you.  Sure, here you go:  > So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In card under this precisely defined term.  >Their non-show is basically code for: “We're canceling everything dedicated, and our eGPUs we consider now dGPUs.”  >So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – In favour of Nvidia-GPUs?",hardware,2025-10-09 15:49:13,13
Intel,nino52j,"> They did not declare PTL as a de-facto dedicated GPU.  Except that's exactly what they did – look at the damn pictures my friend, Intel purposefully group it into the same category as Battlemage's dedicated GPUs and Add-In graphics-cards and thus, iGPU-tiles as on par as dGPU cards.  > By definition [e.g. Panther Lake's iGPU-tile] it's not a dedicated GPU and they never said it is.  \*Sigh I *know* this, well all do! Yet Intel wants to pretend, that PTL's on-die eGPU X^e Graphics tile is now basically declared, what *Intel* now considers a **d**GPU. The implication is strikingly obvious here.  > They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs.  Ex-act-ly! You really don't get it, don't you? They deliberately leave everything what *classically* would count as a dGPU open and without ANY path forward, meanwhile their X^e 3-tiles are now considered dGPUs by Intel.  > You're trying to fill in missing info with a nonsense interpretation.  No, I don't. I'm just one of those being able to decypher Intel's typically backhanded tricks and try to explain that they're virtue signaling the worst: ARC as a line of dedicated graphics-cards is dead and won't get a follow-up, instead, Intel now considers it replaced by iGPU X^e Graphics-tiles (like the one in Panther Lake).  > What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".  Their trickery here, that they basically announce the discontinuation of ARC graphics-cards through the back-door.",hardware,2025-10-09 20:24:55,-2
Intel,nirwlsr,I wonder why they're using such naming. If Xe3 is just Xe2 enhanced then why not call it Xe2.5 or Xe2+. And if Xe3P is a new architecture why not call it the real Xe3?,hardware,2025-10-10 14:04:53,1
Intel,ninor5p,"Just wait and see. I sadly have a horrific on point streak to remain in the right with anything Intel, even if I call things months to years in advance.",hardware,2025-10-09 20:27:56,-8
Intel,ninre0y,"The implication here is incredibly obvious:  ""Intel Arc B-Series consistes of Battlemage discrete graphics *and* PTL iGPUs"".  That's the implication here. It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL",hardware,2025-10-09 20:41:15,5
Intel,nixpud1,Because it's most likely still the same foundation. Xe3 just have additional optimizations and HW blocks along the lines of OMM (confirmed) and whatever new tech Intel decides is worth pushing for higher end parts.    But perhaps it's more of a RDNA 3.5 situation. Some early Druid tech backported to XE3P.    Yeah I have zero clue just guessing xD,hardware,2025-10-11 13:22:13,1
Intel,ninu5gl,"> It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL  Of course not, since it would instantly tank their reputation and will kill all further ARC-sales?!  Do you actually think, that Intel is that stupid to plain announce the death of ARC-series graphics-cards? Do you think they'd even sell a hundreds cards after such a announcement? Who would by a dead-end line of graphics-cards, which won't even get any further driver-support and still have disastrous drivers like Intel's ARC? NO-ONE!  Of course they virtue signal it through the backdoor here, to preserve the sell-off the of the rest of pile of shame of their B-series graphics-cards as long as possible (leaving users with none support afterwards), and *only then* pull the plug afterwards when they went through their inventory …  Since when are you following Intel? Intel has always done so, and only announced a sudden discontinuation once the inventory was sold (leaving customers in the open with no further support overnight), despite virtue signalling a continuation all the time (to preserve a proper sell-off).  ---- Wasn't this exactly the same story with **Optane** back then? Each and every question upon a future support was answered in the affirmative, only to pull the plug overnight again as usual …  I'm not another piracy of cons here, it's Intel pulling the same Optane-stunt with ARC – Happily selling off their cards, knowing darn well, that they will pull the plug, yet refuse to actually play with open cards again.",hardware,2025-10-09 20:55:03,-1
Intel,nio14ia,"> > It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL >  > Of course not  Great, we're all agreed that you're making shit up when you said  > Panther Lake now Intel classifies as a dGPU",hardware,2025-10-09 21:31:45,8
Intel,nfxvgt2,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,hardware,2025-09-24 13:00:51,31
Intel,nfxs3mr,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",hardware,2025-09-24 12:41:08,36
Intel,nfxrqaw,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,hardware,2025-09-24 12:38:53,13
Intel,nfz9psy,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",hardware,2025-09-24 17:11:03,9
Intel,nfyjkgh,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",hardware,2025-09-24 15:06:03,5
Intel,nfxz0x9,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,hardware,2025-09-24 13:20:54,2
Intel,nfz9315,we are witnessing the downfall of pc gaming in real time,hardware,2025-09-24 17:08:01,3
Intel,ng021ru,Why should they? They are going to buy NVidia GPUs for everything now.,hardware,2025-09-24 19:27:35,2
Intel,nfxotxk,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-24 12:20:56,1
Intel,nfxyif8,This could be awesome more completion The better,hardware,2025-09-24 13:18:01,1
Intel,ng09fp7,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,hardware,2025-09-24 20:03:42,0
Intel,nfxyqy3,"Multi-Post News Generation, with three articles interpolated per source.",hardware,2025-09-24 13:19:21,41
Intel,nfxswgf,I'm excited for Intels new GPU,hardware,2025-09-24 12:45:57,6
Intel,ng28c6g,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",hardware,2025-09-25 02:43:17,10
Intel,ng09kyr,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",hardware,2025-09-24 20:04:24,4
Intel,nfy71ww,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,hardware,2025-09-24 14:03:45,-7
Intel,nfxzd6m,Competition *,hardware,2025-09-24 13:22:47,1
Intel,nfydv2b,Obligatory article quoting reddit post quoting another article quoting original reddit post.,hardware,2025-09-24 14:38:06,11
Intel,nfy8e3s,"Fake frames, fake articles! /s",hardware,2025-09-24 14:10:35,7
Intel,ng9nabn,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,hardware,2025-09-26 07:24:34,4
Intel,nfy9ky6,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",hardware,2025-09-24 14:16:39,6
Intel,ngt9vbz,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",hardware,2025-09-29 11:53:54,1
Intel,ngt9z2u,"patents expire after 15 years, they will have to share it then.",hardware,2025-09-29 11:54:37,1
Intel,nfykmyd,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,hardware,2025-09-24 15:11:13,11
Intel,nfyiwdp,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",hardware,2025-09-24 15:02:46,9
Intel,ng9ndqw,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,hardware,2025-09-26 07:25:31,3
Intel,ngta2hh,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,hardware,2025-09-29 11:55:17,1
Intel,ngtlhim,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",hardware,2025-09-29 13:07:31,1
Intel,ngtvb8k,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",hardware,2025-09-29 14:02:39,1
Intel,ng9nh90,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,hardware,2025-09-26 07:26:29,3
Intel,nf42stl,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,hardware,2025-09-19 18:12:40,34
Intel,nf2t8pz,"Okay then, what's the Xe GPU roadmap looking like then?",hardware,2025-09-19 14:34:57,75
Intel,nf43nvh,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",hardware,2025-09-19 18:16:54,13
Intel,nf4q7tq,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,hardware,2025-09-19 20:09:01,11
Intel,nf31f8h,They barely lived on before the deal.,hardware,2025-09-19 15:13:57,35
Intel,nf53r9t,"It'll live on our hearts, yes.",hardware,2025-09-19 21:17:43,11
Intel,nf3f8y9,Lol if you believe that I have a bridge to sell you in Brooklyn,hardware,2025-09-19 16:20:00,16
Intel,nf7xz74,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",hardware,2025-09-20 09:42:21,3
Intel,nf8zcs6,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",hardware,2025-09-20 14:09:11,1
Intel,nfacpei,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,hardware,2025-09-20 18:18:08,1
Intel,nf8psx7,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,hardware,2025-09-20 13:16:21,0
Intel,nf605ki,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",hardware,2025-09-20 00:24:16,8
Intel,nfcc5gm,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",hardware,2025-09-21 00:57:40,3
Intel,nf2xqty,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",hardware,2025-09-19 14:56:28,28
Intel,nf53uu0,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,hardware,2025-09-19 21:18:14,5
Intel,nf69muk,"Always selling out actually, they just can't produce that much",hardware,2025-09-20 01:23:31,11
Intel,nf3gd7v,And I have another if you think nVidia is capable of keeping this deal running for that long...,hardware,2025-09-19 16:25:26,5
Intel,nfho1ld,Intel will hopefully split their fab business from the rest of the company either way.,hardware,2025-09-21 21:10:12,1
Intel,nf3itwc,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",hardware,2025-09-19 16:37:23,-21
Intel,nfe3v2w,> they just can't produce that much  because they are losing money on them,hardware,2025-09-21 09:31:45,5
Intel,nfkj39o,Trade bridges.,hardware,2025-09-22 09:21:12,1
Intel,nf6x83x,Intel's arc is dead with or without nvidia deal.,hardware,2025-09-20 04:05:06,1
Intel,nf3naf4,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",hardware,2025-09-19 16:58:38,22
Intel,nf41fhg,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,hardware,2025-09-19 18:05:57,14
Intel,nf5ky00,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",hardware,2025-09-19 22:54:47,9
Intel,nfjjjlm,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",hardware,2025-09-22 03:50:01,1
Intel,nf72gvm,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",hardware,2025-09-20 04:47:19,1
Intel,nf5bseq,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",hardware,2025-09-19 22:01:34,4
Intel,nf5z77h,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",hardware,2025-09-20 00:18:20,4
Intel,nfjkc77,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,hardware,2025-09-22 03:55:54,2
Intel,nf73hs7,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",hardware,2025-09-20 04:55:37,0
Intel,nf6x2qk,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",hardware,2025-09-20 04:03:55,1
Intel,nflcaft,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",hardware,2025-09-22 13:04:14,2
Intel,nf729ff,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",hardware,2025-09-20 04:45:38,3
Intel,nf74cdn,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",hardware,2025-09-20 05:02:36,4
Intel,nfbo7bs,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,hardware,2025-09-20 22:33:22,143
Intel,nfb2xr8,Far more than I expected them to come out at. Damn.,hardware,2025-09-20 20:35:14,17
Intel,nfarr9a,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",hardware,2025-09-20 19:36:31,59
Intel,nfbz1kb,I’m getting one when it releases in Australia,hardware,2025-09-20 23:37:15,6
Intel,nfj2y23,Thats great and all but when will there be stock? (Canada),hardware,2025-09-22 02:00:50,2
Intel,ng8k9a2,Pytorch libs are getting better for intel and amd support but there are still a lot of 3090 cards on the used market and those don’t need hoops to jump through to get compute working.  If I was handed a budget this low by a boss I’d just ask them to buy a lot of 3090 cards.,hardware,2025-09-26 02:15:59,2
Intel,ngid0f0,I'm disappointed.  My order was canceled,hardware,2025-09-27 17:12:34,2
Intel,nfaolmx,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-20 19:19:41,1
Intel,nfbxe5d,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",hardware,2025-09-20 23:27:31,134
Intel,nfc739o,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,hardware,2025-09-21 00:26:10,17
Intel,nfateyp,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",hardware,2025-09-20 19:45:24,207
Intel,nfb479l,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",hardware,2025-09-20 20:41:48,78
Intel,nfb166b,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",hardware,2025-09-20 20:26:03,38
Intel,nfb3pdb,The 3090 does not have ECC on it's VRAM nor certified drivers,hardware,2025-09-20 20:39:13,18
Intel,nfc4ak9,Totally not worth it.,hardware,2025-09-21 00:08:54,-1
Intel,nfb6hdv,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,hardware,2025-09-20 20:53:44,-8
Intel,nfd2k0s,"I'm more looking forward to the B50, but obviously local pricing is everything.",hardware,2025-09-21 03:53:22,7
Intel,njp8t1o,"Yeah, the website now says: ""This GPU is only available as part of a whole system. Contact us for a system quote.""",hardware,2025-10-15 22:34:30,2
Intel,nfbgzuw,"It has SR-IOV, certified drivers and other professional features...",hardware,2025-09-20 21:51:30,22
Intel,nfc15d7,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",hardware,2025-09-20 23:49:42,9
Intel,nfdv26x,">The gaming cards require significantly less robust driver and application support  Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  It was like that since A770 release. When some games straight up don't work or have unplayable bugs, you are literally paying for own suffering.  That said, B50/B60 are good, though i wish B50 worked with ""Project Battlematrix"" that is Intels NVLink. B60 was also expected to be for 500$, not 600$, but it is what it is.",hardware,2025-09-21 08:04:03,15
Intel,nfbzlhb,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 1 year!?,hardware,2025-09-20 23:40:29,-13
Intel,nfeqn5s,"...Do integrated graphics support SR-IOV? Do any of Intel's iGPUs support the ""pro"" version of their drivers,   Like, obviously iGPUs have less compute power and less bandwidth than anything dedicated, but. Part of me feels like iGPUs should actually have the features of the ""pro"" cards, given they aren't replaceable, with the actual pro cards providing performance.",hardware,2025-09-21 12:41:54,6
Intel,nfauwnn,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",hardware,2025-09-20 19:53:22,25
Intel,nfau9ib,Do not underestimate the lack of CUDA.,hardware,2025-09-20 19:49:57,35
Intel,nfdmqe2,"Bought my 3090 for MSRP 4.5 years ago and honestly would never have imagined that it would hold 50% of its value.  I want to get another one when they drop to $200 or so, so that I can finally be disappointed by SLI and make 14 year old me happy by finally clicking the bridge onto two GPUs and firing up a benchmark, but who knows if/when that day will come.",hardware,2025-09-21 06:45:03,3
Intel,nfbbfys,A used 3090 is only $100 more.,hardware,2025-09-20 21:20:29,2
Intel,nfauhs7,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",hardware,2025-09-20 19:51:10,-17
Intel,nfc1012,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,hardware,2025-09-20 23:48:50,-3
Intel,nfb5jdp,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,hardware,2025-09-20 20:48:47,30
Intel,nfc0fwm,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",hardware,2025-09-20 23:45:29,16
Intel,nfbovre,Atlas 300i duo,hardware,2025-09-20 22:37:31,-4
Intel,nfb510h,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",hardware,2025-09-20 20:46:06,31
Intel,nfb3ro9,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",hardware,2025-09-20 20:39:33,8
Intel,nfc234i,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,hardware,2025-09-20 23:55:21,4
Intel,njpmmsp,Yeah it was very scummy imo,hardware,2025-10-15 23:55:13,2
Intel,nfcbz1s,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",hardware,2025-09-21 00:56:35,1
Intel,nfc72xn,It was meant to be a joke. Not so funny I guess.,hardware,2025-09-21 00:26:06,5
Intel,nfek88y,"B580 needed to get out very early to beat RTX 5060 and RX 9060 to the market. Nvidia's and AMD's cards outperform it in gaming raster performance thanks to having been in the market for much longer. They have the more efficient architecture and software for gaming. It wouldn't be great for marketing if Arc came out at the same time with similar pricing, yet got beaten in the FPS benchmarks.  But productivity cards could wait. Arc's always been very competitive in productivity tasks. Maybe because Intel has more experience in this department. So reliable driver is the focus here.",hardware,2025-09-21 11:57:29,10
Intel,nfsgdep,">Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  Good driver and application support is not possible to deliver without a significant install base of your product in the wild. PC configurations vary wildly and usually driver issues are about some bad luck combination of hardware, driver versions, and application versions.  The incumbency advantage in this market is immense. Devs will validate that their stuff works on all manner of configurations including an Nvidia GPU, so they get the perception of having bulletproof drivers.",hardware,2025-09-23 16:15:20,2
Intel,nfd2naj,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,hardware,2025-09-21 03:54:03,12
Intel,nfc0nds,Why does this matter?,hardware,2025-09-20 23:46:45,20
Intel,nfv9260,"Some iGPU do support SR-IOV, I think 12th generation and newer. Motherboard, driver, and OS dependent.",hardware,2025-09-24 00:45:10,2
Intel,nfb63fj,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,hardware,2025-09-20 20:51:42,39
Intel,nfbbr35,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",hardware,2025-09-20 21:22:13,10
Intel,nfbedl2,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",hardware,2025-09-20 21:36:51,9
Intel,nfbifxk,Used market is freaking insane. It is better to grab new or open box.,hardware,2025-09-20 21:59:41,6
Intel,nfcx3qi,Over here used 3090s are sold for 500-600€.,hardware,2025-09-21 03:13:43,2
Intel,nfbbgrb,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",hardware,2025-09-20 21:20:36,27
Intel,nfhd7ci,Dude that feeling is so worth it though. I did that same with 2 Titan Xps a couple months ago and the build just looks awesome. Great performance on pre 2018 / SLI supported games as well.,hardware,2025-09-21 20:19:33,2
Intel,nfb5o6f,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,hardware,2025-09-20 20:49:29,22
Intel,nfayuhx,the super gpus are not expected to release soon?,hardware,2025-09-20 20:14:00,9
Intel,nfcl0od,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",hardware,2025-09-21 01:53:39,13
Intel,nfcm0px,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",hardware,2025-09-21 02:00:02,10
Intel,nfc9w3x,Used 4070 Supers dont sell for nearly $700+ on the low dude.,hardware,2025-09-21 00:43:40,6
Intel,nfco0nb,Yes running business of used cards is how its done.....,hardware,2025-09-21 02:12:43,3
Intel,nfdimrg,no fee SR-IOV makes this a substantially different offering then AMD or Nvidia.,hardware,2025-09-21 06:07:29,11
Intel,nfbr125,Typically run at 250W though to be fair.,hardware,2025-09-20 22:50:29,7
Intel,nfc1wom,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",hardware,2025-09-20 23:54:16,-7
Intel,nfbgadn,not on the Vram like professional cards,hardware,2025-09-20 21:47:32,7
Intel,nfdjdro,Q4.,hardware,2025-09-21 06:14:16,2
Intel,nfdm9xu,I thought it was funny ¯\_(ツ)_/¯,hardware,2025-09-21 06:40:53,2
Intel,nfjxn7s,The b580 is still good at $250 and it’s only recently in the past month the 5060 and 9060xt 8gb dropped near or at $250 in some instances.  It’s a good card at 1440p aswell and it has 12gb of the other 2 8gb cards. Only problem I have it is the windows 10 drivers being abysmal dog shit if you’d want to play any current game with stuttering and crashes.,hardware,2025-09-22 05:47:02,3
Intel,nfddcxx,"Yeah, apparently a year’s worth of it",hardware,2025-09-21 05:20:13,-5
Intel,nfcew77,Because they're super late to the party.,hardware,2025-09-21 01:14:55,-8
Intel,nfb6fud,"Wow, 800-900USD after 5 years is more than I would have expected.",hardware,2025-09-20 20:53:31,19
Intel,nfdc18j,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",hardware,2025-09-21 05:08:46,6
Intel,nfc1e0n,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,hardware,2025-09-20 23:51:10,-8
Intel,nfc1j2x,For what exactly do they need vram without cuda ?,hardware,2025-09-20 23:52:00,-4
Intel,nfbq925,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",hardware,2025-09-20 22:45:52,-11
Intel,nfksagq,Gamers are not niche. They are 20 billion a year business.,hardware,2025-09-22 10:46:43,-1
Intel,nfbohl3,"False, they’re releasing in december or jan. So about 3 months from now",hardware,2025-09-20 22:35:06,7
Intel,nfbxm75,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",hardware,2025-09-20 23:28:51,12
Intel,nfbksrd,"The 3090 Ti did, but the standard 3090 did not.",hardware,2025-09-20 22:13:21,9
Intel,nfjz0lo,"It's a really good general purpose card. Perhaps not the best card if you want to squeeze out the most fps out of games, but it performs really well in a wide range of tasks including gaming. And yes 12GB VRAM really comes in handy.  It can easily be found for under $250. B580 is still cheaper than 5060 and 9060 XT and even RTX 5050 and RX 7600, especially in the UK where you can find a B570 for only 20 pounds more than RTX 3050. That's way cheaper than RX 6600, it's a no brainer. But the prices could only go down thanks to economy of scale mostly. It's been produced for a while, that's why.  Also one of the big reasons it's still selling now is thanks to good publicity. Imagine if it came out as the same time as RX 9060 XT. HWU would tear a new hole regardless of how much of an improvement it is over the Alchemist cards and the impressive productivity performance. Those guys literally don't care about anything other than pushing a dozen more frames in a dozen personally chosen games. Gamer Nexus and Igor too to some extent. It would be bad publicity.",hardware,2025-09-22 05:59:38,3
Intel,nfdem0c,"If intel felt they could have released this safely earlier, they would have",hardware,2025-09-21 05:31:12,6
Intel,nfb6vit,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,hardware,2025-09-20 20:55:46,22
Intel,nfc975k,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",hardware,2025-09-21 00:39:26,2
Intel,nfg0b8g,3090s will be much less attractive when the 5080 super and 5070ti super launches.  If the 5070ti super is 750- 800 with 24gb for a new faster GPU with a warranty it hel used market will have to shift.  The 3090s have nvlink but I'm unsure how much that matters for most people.,hardware,2025-09-21 16:40:24,4
Intel,nfcmei6,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,hardware,2025-09-21 02:02:24,10
Intel,nfc3jog,"rendering, working on big BIM / CAD models, medical imaging,...",hardware,2025-09-21 00:04:18,19
Intel,nfc3y1b,CUDA isnt the only backend used by AI frameworks,hardware,2025-09-21 00:06:45,23
Intel,nfde0a9,"I use opencl for doing gpgpu simulations, this card would be great for it",hardware,2025-09-21 05:25:54,3
Intel,nfbqi8f,You mean your 0 profile history because you have it private?,hardware,2025-09-20 22:47:22,9
Intel,nfd62l7,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",hardware,2025-09-21 04:20:02,0
Intel,nfbqms5,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,hardware,2025-09-20 22:48:07,-1
Intel,nfksqka,Spoken like a true gamer.,hardware,2025-09-22 10:50:25,2
Intel,nfbt6h6,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,hardware,2025-09-20 23:03:03,3
Intel,nfcz630,You still get about 85% performance compared to stock settings.,hardware,2025-09-21 03:28:33,3
Intel,nfdr09z,"Of course, but that’s the point. It took them an excessive amount of time to “feel” like they could have released this",hardware,2025-09-21 07:24:57,-3
Intel,nfbvai4,What are they doing that's making them money? Or are theu selling the compute somehow?,hardware,2025-09-20 23:15:17,18
Intel,nfc1jx5,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",hardware,2025-09-20 23:52:08,-9
Intel,nfc9f6k,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",hardware,2025-09-21 00:40:47,-3
Intel,nfkntvr,">rendering  Rendering what? Because if you're talking from a 3D art perspective, all our software is leaps and bounds better with CUDA or outright requires it. You can't even use Intel GPUs with Arnold or Redshift, so good luck selling your cards to a medium scale VFX house that heavily use those render engines",hardware,2025-09-22 10:07:27,5
Intel,nfdll3m,>CUDA isnt the only backend used by AI frameworks    It is not the only backend for all AI frameworks; it is the only backend for the majority of AI frameworks.,hardware,2025-09-21 06:34:32,2
Intel,nfks66x,It is the only functional one.,hardware,2025-09-22 10:45:44,2
Intel,nfcsv0e,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",hardware,2025-09-21 02:44:26,6
Intel,nfbqlfx,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",hardware,2025-09-20 22:47:54,-8
Intel,nfd6ez2,0 people are using autodesk with an intel arc gpu lmao,hardware,2025-09-21 04:22:47,0
Intel,nfbriv2,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",hardware,2025-09-20 22:53:25,1
Intel,nfkvz03,"Oh no, im part of 60% of world populatioin. how terrible.",hardware,2025-09-22 11:15:48,1
Intel,nfd2pwi,Which is 36% higher performance per watt.       ... to be fair.,hardware,2025-09-21 03:54:35,4
Intel,nfdsvbs,The reason they didn't is because it would have been worse to release it earlier,hardware,2025-09-21 07:43:03,0
Intel,nfbw0b9,Software Development,hardware,2025-09-20 23:19:26,-4
Intel,nfc44n9,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,hardware,2025-09-21 00:07:53,9
Intel,nfcbku1,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",hardware,2025-09-21 00:54:09,0
Intel,nfcbqrf,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",hardware,2025-09-21 00:55:09,-3
Intel,nfkwldk,"Depends on what software you use, but your right that NVIDIA with Cuda is safe bet when is comes to rendering. I use Enscape and that works great in my AMD gpu",hardware,2025-09-22 11:20:33,1
Intel,nfeidh5,Which ones?,hardware,2025-09-21 11:43:23,1
Intel,nfcv5fj,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",hardware,2025-09-21 03:00:04,8
Intel,nfcjfv3,Private profile = Complete troll.  You're not an exception to this rule.,hardware,2025-09-21 01:43:42,13
Intel,nfbs5c3,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,hardware,2025-09-20 22:57:03,-2
Intel,nfdu2bk,"Right man, my point is that it shouldn’t have been",hardware,2025-09-21 07:54:31,-1
Intel,nfbxo4k,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,hardware,2025-09-20 23:29:11,11
Intel,nfcdfer,"Hell, one of them was just the cooler without the actual GPU.",hardware,2025-09-21 01:05:34,9
Intel,nfcde1y,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",hardware,2025-09-21 01:05:20,0
Intel,nfgcxg6,for example every local image and video generation system I've seen.,hardware,2025-09-21 17:38:32,3
Intel,nfbsbxa,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",hardware,2025-09-20 22:58:06,3
Intel,nfc0g5t,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",hardware,2025-09-20 23:45:32,0
Intel,nfducqd,According to whom? With what evidence?   Intel spends more on r&d per year than nvidia and amd combined   Do you think they are just being lazy?,hardware,2025-09-21 07:57:16,3
Intel,nfh4t9j,Getting good LLMs crammed onto GPUs for coding is a popular topic on /r/locallama.,hardware,2025-09-21 19:41:40,3
Intel,nfd4nt4,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",hardware,2025-09-21 04:09:11,5
Intel,nfd0war,And the one directly under that was the actual PCB...,hardware,2025-09-21 03:41:08,6
Intel,nfgecr2,Im running Comfy UI + Flux on my Strix Halo right now without issue 🤷‍♀️,hardware,2025-09-21 17:44:40,0
Intel,nfksgbg,Propaganda is 90% of chinas economic output though.,hardware,2025-09-22 10:48:04,0
Intel,nx5hhax,It's a good gpu for its price. The driver issues that ruined A-series seem to be gone. Just be sure to enable ReBAR: BattleMage needs to to perform best,buildapc,2026-01-01 22:20:13,5
Intel,nx5gj70,[https://www.youtube.com/watch?v=00GmwHIJuJY](https://www.youtube.com/watch?v=00GmwHIJuJY)   [https://www.youtube.com/watch?v=npIpWFSfmv4](https://www.youtube.com/watch?v=npIpWFSfmv4)  Not sure what kind of options you have where you are. 3060 is pretty long in the tooth unless your getting a smoking deal. No low priced 9060s or or anything around where you are?,buildapc,2026-01-01 22:15:11,3
Intel,nx6wr9i,At 250 its a crazy good deal. Its not top of the line but its winning competition is the fact that it beats out every other card in its weight class at its price point.,buildapc,2026-01-02 03:18:01,2
Intel,nx7n3le,It’s great,buildapc,2026-01-02 06:21:14,2
Intel,nxbtc77,"In comparison to other GPUs within its range, what would it be comparable to in terms of performance?",buildapc,2026-01-02 21:45:50,1
Intel,nxbtinv,"In comparison to other GPUs within its range, what would it be comparable to in terms of performance? Especially if the driver issues are no longer present with the more recent updates",buildapc,2026-01-02 21:46:42,1
Intel,nxbt4xl,"A bit out of my price range, even for a used one. I’m only 19 and I’m building this by myself with my own money as my first ever build. From where I’m from, the 3060 12 gb is a bit manageable price-wise, I just wanted to check out other options.",buildapc,2026-01-02 21:44:51,1
Intel,nxo75v2,"i remember hearing that the overhead issues are less of/not an issue now, due to driver updates",buildapc,2026-01-04 18:49:23,1
Intel,nxbtrn7,[similar to a 3060ti](https://www.techpowerup.com/review/intel-arc-b580/32.html) but with newer features,buildapc,2026-01-02 21:47:56,2
Intel,nxbzpsr,Fair. If your on a tight budget. Consider some of the RDNA3 AMD cards as well. I mean if your looking at a 3 generation old 3060 anyway. Something like the 7600 XT 16gb might be on the market where you are at a good price. Its probably 20% or so faster then a 3060.,buildapc,2026-01-02 22:17:35,1
Intel,nvursni,r/homelab,buildapc,2025-12-25 10:48:55,1
Intel,nwmhbvr,"The 9060 xt should be 30-35% faster than the b580 depending on games and resolution. To me, that's definitely worth a 14% increase in cost.  8 GB does mean the card won't age as well, but it doesn't mean the card is suddenly worthless. There are very few games where 1440p simply won't run on 8 GB of RAM. With the RAM crisis only going to get worse, my hope is that developers will put a little bit of effort into optimizing vRAM usage.  (edit: and based off your pricing, the 16 GB should only be about $50 USD more. That is probably worth it because not only will it perform better now, but you'll get a couple years longer out of it, and it will have higher resale later.)",buildapc,2025-12-29 22:17:02,11
Intel,nwmjhz4,"VRAM usage is based on your resolution, mostly, in conjunction with game settings at that resolution.  If you plan on gaming at all at 1440p, do **not** get a card with 8GB of VRAM. This isn't a future issue, this is a now issue. It currently does not have enough vram for 1440p gaming in nearly any new heavy titles, including battlefield 6 (just as a recent example).   Hell, at 1080p my 8gb card was over capacity on battlefield 6. There's an argument to be made that it isn't enough for future 1080p gaming as well.  Can you do plenty of gaming on an 8gb card? Yes. Can you run every game at max settings? No, not even on 1080p. You'll have to turn settings down. Which frankly I'm in the boat of ""not usually a big deal"", but it's not good when you pay 300+ bucks for a new piece of hardware that can't run your games at full tilt.   It is more powerful than the B580, yes. The B580 will last longer because of VRAM, also yes.   My suggestion is really think about how long it will be until your next upgrade and make your decision based upon that. Longevity, go with intel. More power now for just a couple of years (2 or 3), consider the 9060XT/5060 (at 1080p only, skip entirely for 1440p). Or look at their 16gb variants.   You could also look at used options. You can find a 6750XT 12GB for 200-250 USD typically, as an example.",buildapc,2025-12-29 22:28:10,2
Intel,nwmnkd8,"in that price range I think you can find good used deals. also maybe take a look at the rx 6800 xt (or generally gpus from 1 or 2 generations ago). I have one since 3 years or so and works like a charm (you will lose fsr 4 compared to the 9060 xt, but it has 16gb vram and better performance) in my country (in central europe) I saw some sold for about $250",buildapc,2025-12-29 22:49:17,2
Intel,nwmiajn,"It depends on what games you play and refresh rate not just resolution.  As you mentioned, it's not just a matter of the amount of VRAM, otherwise a 1070 or 1080 Ti would still be used -I bring it up because I just upgraded from a 1070 :)  I would spend the extra $40. I think the RX 9060 XT 8GB would also have more resale value.",buildapc,2025-12-29 22:21:58,1
Intel,nwmjwp3,16gb,buildapc,2025-12-29 22:30:14,1
Intel,nwmu4vj,"9060xt 16gb, is the one you should get if you are planning for 1440p in the future. Get that one that future you will thank and appreciate in the long run.",buildapc,2025-12-29 23:24:40,1
Intel,nwmv68o,"definitley wait and save up for the 16gb of the 9060xt, it will be worth it",buildapc,2025-12-29 23:30:20,1
Intel,nwn4giv,If you're on PCIe 3 the 9060 is the only choice here.,buildapc,2025-12-30 00:20:57,1
Intel,nwn5ndl,"So recently I bought a new prebuilt that came with a 5060 TI 8GB. For my old system, shortly before, I'd purchased a 9060 XT 16GB.  I haven't had an AMD card in many a year, and I was inclined to be generous to NVIDIA, thinking ""Hey, they've got DLSS, they've got multi-frame gen, yadda yadda yadda...""  So I'd been playing Borderlands 4 on my old rig with the 9060 XT when the 5060 TI system came in a few days ago. I tried playing Borderlands 4 on the new PC with DDR5 RAM and a much better processor, and for every game I tried (Witchfire, Borderlands 4), it was a much worse experience.  DLSS frame gen treated me well on my old 4060 so I thought  multi-frame gen would be rad and close the 8GB-16GB gap, but holy shit the lag was like improperly configured Lossless Scaling. It was awful, and I am NOT generally sensitive to latency. I really learned that without enough native frames to feed frame gen, it's not even a feature worth using.  I suspect had my new PC had the 16gb version of the 5060 TI it probably would have run everything as well as, if not better than, the 9060 XT.  As an example, on my new PC with the 5060 TI, Borderlands 4 was running like less than between 35-50 fps native on medium settings. With the same processor, same RAM, but a 9060 XT 16GB, it runs natively in the 70s and 80s on high settings.  Now, I do only game at 1080p, mind you, so that factors in as well, I suppose.  I was a true believer that 8GB was still enough in late 2025, especially with AI assistance, until this past weekend. I would urge you to go with more VRAM. I know, personally, I'll never buy another GPU with 8GB VRAM again after I see how impactful it is.",buildapc,2025-12-30 00:27:21,1
Intel,nwn9c6w,"The whole vram issue is valid. But, ultimately, all you can do is look at the games you want to play. For the vast majority of current games, the 9060xt performs better and not by a tiny amount. I don't get why you'd take 30% worse performance now for the hope that in 3 years' time you might get better performance.",buildapc,2025-12-30 00:47:14,1
Intel,nwntzzr,Especially if 1080p is still your target then I'd go with the 9060 no doubt about it.,buildapc,2025-12-30 02:41:24,1
Intel,nwmhqtw,"Idk about these specific cards but a few years ago I got a 1650 4gb and my buddy got a 1060 3gb  When he played games the fidelity was great but had some stutters here and there, just cruising at 70ish percent  Mine would be screaming at 99% usage with lower fidelity but boy oh boy was it significantly smoother  I'd go with more vram",buildapc,2025-12-29 22:19:09,1
Intel,nwn3tz7,"Neither, get the 9060xt 16gb version. Stay away from 8gb.",buildapc,2025-12-30 00:17:36,0
Intel,nwmr1mk,Tell OP they can get the 9060Xt in 16gb,buildapc,2025-12-29 23:07:52,3
Intel,nxf1w8v,1080p yes on medium settings. 1440p probably if you turn down all the settings to low. 4K if you like slideshow.,buildapc,2026-01-03 10:37:04,2
Intel,nxf3qbn,"of course it will, runs comparable to a 3060. You can expect 70fps on AAA titles at 1080p mid-high settings.",buildapc,2026-01-03 10:52:35,1
Intel,nxf92eu,Why a 14600k with ddr5? At that point you might as well just get onto am5 with a 7600x,buildapc,2026-01-03 11:37:26,1
Intel,nxfiey9,It will be a great setup for 1080p,buildapc,2026-01-03 12:49:52,1
Intel,nxfxfre,Listen if I can run new games with a i5 4690k with ddr3 this is a non starter. Lol,buildapc,2026-01-03 14:22:12,1
Intel,nxezd8p,50/50 chance,buildapc,2026-01-03 10:15:34,0
Intel,nxf63rb,just give you a reference point. i'm using 225f (a cpu even weaker than yours) + b580. i play games at 4k medium quality. [here](https://www.reddit.com/r/IntelArc/comments/1q0diio/effects_of_driver_and_firmware_update/)'s one of them,buildapc,2026-01-03 11:12:36,0
Intel,nxf4fn1,Great. thank you :3,buildapc,2026-01-03 10:58:27,1
Intel,nxflrgz,Most comparisons seem to put 14600K on top though?,buildapc,2026-01-03 13:12:28,1
Intel,nxf0hlk,what does that mean xD,buildapc,2026-01-03 10:24:59,0
Intel,nxkv2gf,"40fps, worse than a console.",buildapc,2026-01-04 05:47:14,1
Intel,nxg25bc,"they trade blows, mainly depends on the game being played",buildapc,2026-01-03 14:48:19,1
Intel,nxf1hve,Hope and pray they optimize their game 😆,buildapc,2026-01-03 10:33:38,1
Intel,nxlrk9m,which console,buildapc,2026-01-04 10:29:33,1
Intel,nxm9lvg,"Series X, PS5 pro.",buildapc,2026-01-04 12:59:14,1
Intel,nxmzk7n,"how many fps do they get at 4k medium quality for acs?  you have to compare them with same resolution, same image setting for the same game",buildapc,2026-01-04 15:30:08,1
Intel,nxnnp61,"60fps, they have 60fps modes with equal to or better settings than that.",buildapc,2026-01-04 17:22:41,1
Intel,nxntih2,prove it,buildapc,2026-01-04 17:49:43,1
Intel,nxnu9h3,Digital foundry video on it,buildapc,2026-01-04 17:53:08,1
Intel,nxnw0p3,"as said, you have to compare them with same resolution, same image setting for the same game  a lazy google search [result](https://www.google.com/search?q=ps5+pro+assassin%27s+creed+shadows+4k+medium+quality+how+many+fps)  The game offers several graphics modes on the PS5 Pro:   * Performance Mode: Targets 60 FPS at an upscaled 2160p (4K) resolution, and includes standard ray-traced global illumination throughout the world. * Balanced Mode: Targets 40 FPS at an upscaled 2160p (4K) resolution (requires a 120Hz display with HDMI 2.1 support) and includes extended ray tracing features, such as ray-traced reflections. * Fidelity Mode: Targets 30 FPS at an upscaled 2160p (4K) resolution with the full suite of extended ray tracing features for the highest visual quality.  it looks like you're (mistakenly) read the ""Performance Mode""  here's my game screen. my upscaling mode is ""quality""  https://i.ibb.co/7xCzKnF3/acs.jpg",buildapc,2026-01-04 18:01:04,1
Intel,nxnw9i9,And the performance mode is higher fidelity than you're playing at.,buildapc,2026-01-04 18:02:11,1
Intel,nx7mwmw,$2k pc is a very budget pc now?   Broke ass me can't even afford that,buildapc,2026-01-02 06:19:38,6
Intel,nx7t182,\->Budget minimal rig   \->9800x3d,buildapc,2026-01-02 07:12:15,6
Intel,nx7jdb2,I did have a lot of stability issues with amd gpus but I hear gamersnexus saying nvidia have drivers issues on the 50s cards. So keep that in mind.,buildapc,2026-01-02 05:51:11,1
Intel,nx7ky17,"I would really look into Intel Arc GPU's, I'm running a b580 with many games at 4k60 max settings (although with AI upscaling in some more recent cases e.g. Cyberpunk with RT and ultra perf XeSS = 55-60+ fps at 4k, no FG needed) it has great support for comfy ui (I'm on Linux for my genAI) and can create 1024*1024 SDXL images in around 2 seconds each and Flux Q4ms (iirc) in around 5-10 sec. I'm not so into LLM's and just use ChatGPT for free, but as far as I can tell it's pretty fast with LLM's as well.  It supports XPU without any issue out of the box if you install the drivers from Intel and I've not seen any issues myself.  So, by only looking at Nvidia you're severely limiting yourself.  Also, as a Lenovo server tech support, I see a lot of companies adding Intel compute units (which are based on Xe cores) to their AI-server clusters as they are way more cost effective and aren't that much slower compared to the Nvidia compute units. All of these servers have no issues running XPU based ML/AI.",buildapc,2026-01-02 06:03:40,1
Intel,nxd91c9,"Sorry but with this component list, it's not a budget pc. If you want a latest generation amd cpu and an nvidia gpu, that's going to drive up the cost, more so with the price hikes coming up soon. You're also going from a 1050 Ti to a 5060 Ti, but you want to reuse your PSU... Are you sure that's actually feasible? What is the wattage of your old PSU? What condition is it in? Unless you upgraded it recently, I doubt it'll work with those components you listed. It's not an irrelevant detail at all. Please find out before you commit to your build.  That CPU will also likely need better cooling than what you have right now, especially if you'll be doing LLM work on it. Also on that note, your RAM. 16 GB is not going to be enough for this kind of work. It feels like you've gone all out on the other components but bottlenecked yourself there.  Look, a 7800 would be around 100 euros cheaper and still do what you want it to. And all Nvidia gpus are way overpriced right now, but perhaps a 3060 or a 3080 (if you can find a cheap one, just go with the 3060 tbh) would still be within your budget while also being decent for what you want it for.  The motherboard is way, way overpriced as well. You can find excellent options at half the price. That ""minimal"" rig is not minimal at all and you've overlooked some pretty critical points.",buildapc,2026-01-03 02:28:50,1
Intel,nx86rp9,"Well it's not a Threadripper PRO with 32 cores. So, what would you call a rig with that? Compared with one with 64 cores or 96 cores?  I'm not sure what I've set can be considered as mid though. The diff between 8 core CPU and 32 is kind of...day and night.",buildapc,2026-01-02 09:21:53,0
Intel,nx86kzb,"Hey, thanks for the info about comfy UI! Have you tried training your own LoRA?   I am really interested now. If I might ask, if you have time, for science purposes.  Could you try:   1. Training your own LoRA on a 50 custom images dataset and write here how much it took?   2. Run ollama with \`qwen2.5-coder:14b\` model and record a short video how fast it outputs tokens? Ask it to generate let's say a \`golang project to check available hostnames from yml config\` or smth.  [https://techtactician.com/how-to-train-stable-diffusion-lora-models/](https://techtactician.com/how-to-train-stable-diffusion-lora-models/)  If it's working well I'll consider [ASROCK Intel Arc A770 Phantom Gaming 16GB OC Grafikkarte 16GB GDDR6 HDMI, 3x DP](https://www.computeruniverse.net/en/p/2E33-00V) for \~300 EUR.  Thanks for the comment!",buildapc,2026-01-02 09:20:06,1
Intel,nxg0gv2,Would you recommend me to go with:  A: [SPARKLE Intel Arc B580 Titan OC 12GB GDDR6](https://www.computeruniverse.net/en/p/2E22-14J)   B: [SPARKLE Intel Arc A770 Titan OC 16GB GDDR6](https://www.computeruniverse.net/en/p/2E22-004),buildapc,2026-01-03 14:39:09,1
Intel,nxdat8h,"EDIT: I've put together a starting list of components you can take a look at. Not super-researched as it is quite late, but hopefully enough to point you in the right direction.  [https://pcpartpicker.com/list/6zg6v4](https://pcpartpicker.com/list/6zg6v4)  Total with cpu, gpu, ram and motherboard at 1092 euros, if all bought at the cheapest option. That particular ram is what I have on the build I just completed for myself. Good price and no issues so far.  As for how to get the parts to Moldova... That is going to be tricky, but a third-party courier or forwarder may be of help, though I would be careful with those.",buildapc,2026-01-03 02:39:13,1
Intel,nxgs4nn,I can't speak for the A770 but the B580 works fine. If you need more vram you could also look into the Arc Pro B60,buildapc,2026-01-03 16:55:47,1
Intel,nxfw88d,"Thanks for the help!      1. I have 800 watts capacity right now, I thought it would be enough. (I bought it with reserve at the time)   2. A cheap \~60 EUR CPU cooler I thought I would just omit, it's like a small detail.   3. I honestly thought it's budget build, but I guess I just don't know the brackets, or maybe I think there isn't an universal gradation for it. I thought my current one is budget, better said ""outdated"".   4. I think motherboard is kinda ""non-negotiable"", I didn't want to cheap out on it, it's for many years to come, I might downgrade CPU/buy cheaper GPU or go with less RAM, but motherboard is there to stay for future upgrades, plus I kinda need a few features from there, USB-C, local network 10 Gb/s Ports (Marvell AQtion)     I don't know if I can change the title now to simply ""I need advice for my PC build"", but maybe I should just get a few more salaries and get something better.   I thought maybe I can run qwen3-coder:30b which in 19gb requirement for VRAM as [mstreurman](https://www.reddit.com/user/mstreurman/) said it works on XPUs, I think it should run on DirectML too.  I was thinking maybe something like:   [XFX SPEEDSTER MERC 310 AMD Radeon™ RX 7900 XTX Black Edition, 24GB GDDR6, 384-bit](https://www.emag.ro/placa-video-xfx-speedster-merc-310-amd-radeontm-rx-7900-xtx-black-edition-24gb-gddr6-384-bit-rx-79xmercb9/pd/D0NJ1PYBM/)  But maybe I genuinely need to just go for a cheaper option for now and after a few years stash some money and update again.",buildapc,2026-01-03 14:15:18,1
Intel,nxg5ogs,"It's crazy that the ram is already doubled in price. I guess it's being picked up fast.   The power supply should be good as long as it's in good condition. If you go with the rx7900 xtx, be aware that for optimal use you'll need 3 8-pin connectors. You may be able to get away with 2.   Definitely look into motherboards a bit more. As long as you have DDR5 and pcie 5 support, you're future-proof for current standards. Current graphics cards don't even saturate 16 lanes of pcie4, nevermind pcie5.    Honestly, I would say to pick up the price-critical components (ram, gpu, SSD if you want one) now and save up for the rest.",buildapc,2026-01-03 15:07:09,1
Intel,num2lc1,"Having same issues, having 40-60% decrease in fps across games. Along with stutters, and fps drops. Hard to run even easy to run games like roblox and LoL. Insane that a windows update caused this somehow.",buildapc,2025-12-18 01:14:38,1
Intel,numncz8,"I think I have an restoration point, but damn it sucks",buildapc,2025-12-18 03:26:55,1
Intel,numqe9o,It's working perfectly fine after I rolled back the latest security update.,buildapc,2025-12-18 03:47:40,1
Intel,nxl5man,what's the CPU,buildapc,2026-01-04 07:11:50,1
Intel,nxl5nbi,What CPU are you running in the build?,buildapc,2026-01-04 07:12:05,1
Intel,nxl5p31,Ryzen 7 5800x,buildapc,2026-01-04 07:12:31,1
Intel,nxl5swk,ryzen 7 5800x,buildapc,2026-01-04 07:13:26,1
Intel,nxl60iz,are you using an NVME?,buildapc,2026-01-04 07:15:15,1
Intel,nxl648w,SSD,buildapc,2026-01-04 07:16:08,1
Intel,nxl6n8n,"did you reinstall the drivers completely, like you DDU'd and everything?",buildapc,2026-01-04 07:20:41,1
Intel,nxl6tdr,"im not super knowledgable so im not sure what DDU is, through the app i did CLEAN install which said it'd remove the old drivers and install the new",buildapc,2026-01-04 07:22:12,1
Intel,nxl7g4e,use DDU to uninstall the drivers then install fresh drivers from the Nvidia website,buildapc,2026-01-04 07:27:48,1
Intel,nxls0kq,"DDU = display driver uninstaller.  Download the latest drivers from ngreedia site, then download DDU and use to uninstall all display drivers in your PC, then reboot and install drivers again.",buildapc,2026-01-04 10:33:40,1
Intel,nwjwdn2,Your list is private,buildapc,2025-12-29 14:50:36,2
Intel,nwjvzrr,"It looks like you may have posted an incorrect PCPartPicker link. Consider changing it to one of the following:  * [Use the Permalink](https://i.imgur.com/IW0iaOm.png). note: to generate an anonymous permalink, first click [Edit this Part List](https://i.imgur.com/uqDIcdt.png).  Or, make a table :    * [new.reddit table guide](https://imgur.com/a/1vo0GHH)   * [old.reddit table guide](https://imgur.com/C86vdxB)         *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",buildapc,2025-12-29 14:48:34,1
Intel,nwjwfd7,Your list is Private,buildapc,2025-12-29 14:50:52,1
Intel,nwjwm63,Ur list is private,buildapc,2025-12-29 14:51:53,1
Intel,nwjxjpz,"Your list is set as private, so we can’t see it.",buildapc,2025-12-29 14:56:46,1
Intel,nwjxqdy,"No use getting a 9800X3D (high end) CPU with a B580 (low end) GPU.   If this is for gaming, I would move some money from the CPU and the motherboard to the GPU.   What is your total budget? Do you have a Micro Center nearby?",buildapc,2025-12-29 14:57:45,1
Intel,nwkrlth,"Everything but the gpu is fine.  I saw in a comment you are close to a micro center.  Consider dropping to the 7800x3d bundle or even the 7600x3d bundle and bumping the gpu up to the powercolor 9070xt IF you are hard stuck at $1500 budget   If you have a little wiggle room, you could go with all micro center parts:  7800x3d bundle is 580 (580) -7800x3d -ASUS Tuf gaming b650e -gskill flare 32gb ddr5-6000  Powercolor rx 9070XT is 580 (1160)  Inland QN450 1TB nvme ssd is 100 (1260)  Corsair RM850 psu is 110 (1370)  Peerless assassin is 50 (1420)  Leaves you 80 for a case.  I like the Montech XR. Just used it for my kids build (almost identical to this one) and it’s got good cable management and was easy to work in.  Only 70 bucks too.   Just make sure whatever case you get fits an ATX psu, an ATX board, and the longish GPU.",buildapc,2025-12-29 17:22:03,1
Intel,nwjxmiw,I fixed it,buildapc,2025-12-29 14:57:11,2
Intel,nwjxn95,fixed it,buildapc,2025-12-29 14:57:17,1
Intel,nwjxnsv,fixed it,buildapc,2025-12-29 14:57:22,1
Intel,nwk5f6j,I have a budget of 1.5k and yes I do have a micro center nearby. what gpu and cpu should I get instead then,buildapc,2025-12-29 15:36:29,1
Intel,nwk029g,"I think this is pretty unbalanced, in that you have a budget GPU but a really high end CPU. You should either spend less on the CPU and keep the B580 for a lower cost overall, or you should spend money from saving on the CPU to upgrade the GPU. Also the RAM you selected, it's nice that it is cheaper, but CL46 is quite slow, imo worth to get more expensive 6000CL36 (lower CL is better).  I'd perhaps do it like this, maybe switching out the GPU to a 5060 Ti 16GB: https://pcpartpicker.com/list/PpRqYd",buildapc,2025-12-29 15:09:50,3
Intel,nwkcnq6,"I would get a bundle from Micro Center (https://www.microcenter.com/site/content/bundle-and-save.aspx). Get the 7600X3D one for $400, which comes with a 16GB RAM stick, and you can get a ***matching*** stick on top of that if you want. That will be much cheaper. That CPU is very, very good for gaming thanks to its extra (X3D) cache, which also mitigates any RAM speed/latency concerns one could raise.   With the savings from that bundle, you can beef up your GPU ***significantly***: RTX 5070 Ti > RX 9070 XT > RX 9070 > RTX 5070 > RTX 5060 Ti 16GB > RX 9060 XT 16GB... You can use the relative performance table on this webpage to get a feel for which GPU is faster: https://www.techpowerup.com/gpu-specs/geforce-rtx-5070-ti.c4243  PS: Make sure your PSU supplies enough power considering your changes to the build. Cybenetics rates PSU quality on its website. You should be able to find a solid PSU for $100 or less.",buildapc,2025-12-29 16:11:21,1
Intel,nwk1l5x,"With the current RAM market, there’s no need to be picky about RAM speed and latency. Especially if sticking with an X3D CPU, whose extra cache will negate the speed/latency gap in gaming.",buildapc,2025-12-29 15:17:32,1
Intel,nwk4wzo,how good of a build is that?,buildapc,2025-12-29 15:34:01,0
Intel,nwkfdmd,ty for thr advice,buildapc,2025-12-29 16:24:18,1
Intel,nwkc05v,"If you're talking 6000CL30 to CL36, or like 5600CL36, sure. But 5600CL46 vs 6000CL36 is a very big difference. And in my comment I do also say not to spend for a X3D, because a 250 dollars cheaper regular is a much better combination with the GPU.",buildapc,2025-12-29 16:08:13,1
Intel,nwkecy4,"I get you. I just wanted to specify that, if OP sticks to X3D (e.g., a 7600X3D from Micro Center), the RAM speed and latency gap will have ~~zero~~ minimal (3–4%) impact on gaming performance on a high-end GPU. ~~Zero~~.   *Edit: Corrected my assertions.*",buildapc,2025-12-29 16:19:27,1
Intel,nwkj404,"Yes and no, there is a performance difference still of a few percent, but with a low end GPU probably not very noticeable.",buildapc,2025-12-29 16:41:55,1
Intel,nwkjz8o,Fair enough... about 3–4% on average when using a high-end GPU... I will correct my comments accordingly.,buildapc,2025-12-29 16:46:01,1
Intel,nxhjftz,Rx 9060 xt 16gb,buildapc,2026-01-03 19:00:23,38
Intel,nxixgo8,1080p? Honestly anything 3060 12GB and up.   People overestimate their needs when it comes to GPUs.,buildapc,2026-01-03 23:05:42,58
Intel,nxh8n6m,9060XT 16GB,buildapc,2026-01-03 18:11:36,239
Intel,nxj10o1,"RTX 5070, they dip to 489$ a lot",buildapc,2026-01-03 23:24:27,18
Intel,nxi2zdo,"You can get lucky and get a 9070xt for like 550 on sale. If you’re patient and can hunt sales, I would suggest that.  If you don’t want to wait you can get a 9060xt 16GB for 400 and it’ll crush everything at 1080p anyway. Just make sure it’s the 16GB model.",buildapc,2026-01-03 20:34:10,102
Intel,nximug1,A GPU for 1080p gaming for around 500? Hmm checking around  The easy answer   Rx 9060 xt 16 gig  Rtx 5060 ti 16 gig  Something used from a previous gen or two that was high end.   With some dexterity  Rtx 5070,buildapc,2026-01-03 22:11:46,25
Intel,nxhek10,You build it completely new or did you already have some components? I think a7800x3d and save money for gpu with tha game u plan to play make no sense! You should go for a 9600X and push all your money in the gpu. 9070XT or 5070ti. Wich Country and what is your max budget?,buildapc,2026-01-03 18:38:22,14
Intel,nxit06u,I'd honestly look on the secondhand market and pick up a 2070 Super and you'll be fine with those games. Usually can find them for <$200.,buildapc,2026-01-03 22:42:43,4
Intel,nxj9gns,"5070 is just a bit above this budget, but quite a bit better than lower options like 5060ti or 9060XT.",buildapc,2026-01-04 00:08:33,5
Intel,nxj6zwr,5070 and use DLDSR,buildapc,2026-01-03 23:55:35,6
Intel,nxjvdmt,I’m gonna echo the 9060 XT 16GB. I just got a prebuilt with it and it’s great for 1080p gaming at high settings and 120 FPS. It was $1200 for the whole machine and it came with 32GB DDR5-6000 RAM and 1TB SSD. I imagine prebuilt prices are gonna skyrocket soon too.,buildapc,2026-01-04 02:07:21,4
Intel,nxj4p0h,"RX 9070XT/non XT + RX 9060 16GB + RTX 5060 Ti 16GB are the best options, sadly the 9070 and non XT are pricier than they should be right now.",buildapc,2026-01-03 23:43:32,3
Intel,nxjfwmn,"Walmart has 5070s for $489, beats the 5060ti and 9060xt by a good margin",buildapc,2026-01-04 00:42:26,3
Intel,nxjrfu5,I just got 5060ti 16gb and its been bad ass at 1440.,buildapc,2026-01-04 01:45:39,3
Intel,nxjxvt1,9060xt is 389 on Best Buy rn,buildapc,2026-01-04 02:21:15,3
Intel,nxi9tlx,It's insane that in 2025 a GPU for freakin 1080p is 500 bucks,buildapc,2026-01-03 21:08:09,16
Intel,nxhby43,Go to the used market if you want the best performance for $500. You can get a 7900xt for around $400-$450. If you want brand new you can probably get a 5070 for $500 if you wait for when it goes on sale,buildapc,2026-01-03 18:26:31,6
Intel,nxk3r0c,"A lot of people are suggesting to downgrade the CPU and spend the money on a better GPU which is totally a valid option.   Personally, I’d go a different direction though. Keep the 7800X3D as it’s an excellent CPU. Go with a 9060XT (specifically the 16GB, not the cheaper 8GB) as your GPU. This combo will future proof you at 1080p for some time and allow you to jump to 1440p if you want. The 9060XT can utilise FSR4 and frame gen as well if you do decide to go to 1440p, and the extra VRAM will help future proof you somewhat.   I have 2x rigs at the moment. My main one is a 7900X3D (I wanted the extra cores for work stuff) and a 7900XTX which I couple with a 4k monitor.   My other machine is on my TV which has a Ryzen 7 3700X and a 9060XT. I can play newer games at 4k medium settings with FSR4, but do most of my gaming at 1080p and it runs brilliantly.",buildapc,2026-01-04 02:54:07,2
Intel,nxkb5q7,I have a 3060 and a HP omen monitor. It's 1440 and 165hz. I love it.,buildapc,2026-01-04 03:36:26,2
Intel,nxkwh4h,Maybe RTX 3080 10GB.,buildapc,2026-01-04 05:57:41,2
Intel,nxitf13,Used 1080ti if you can find it for less than 150 bucks  New arc b580 if you can find it for less than 350,buildapc,2026-01-03 22:44:47,2
Intel,nxjul5q,why get a 7800x3d for 1080p?? very odd choice,buildapc,2026-01-04 02:02:58,1
Intel,nxjz3bp,"9060xt is pretty good but if you want a little more expensive for certain slightly higher performances, 5060ti is excellent for budget as well",buildapc,2026-01-04 02:28:00,1
Intel,nxk1hqm,The 5070 MSI Shadow is $489 at Walmart if you have one close by. I Just upgraded from a 2060 super and  it is a huge improvement.,buildapc,2026-01-04 02:41:26,1
Intel,nxkc7on,"I game at 3440x1440 100hz, and my i7 7700 and 2070 were doing everything great, especially with DLSS at performance or ultra performance. I’ve upgraded since (285k and 5080), but I played through cyberpunk 2077 at max graphics (no RTX) at a good enough frame rate that I was just playing the game and having fun.  So, 2070 is ok used, not the best.  3060 ti would be better used.  5070 is sub $500 a lot now.  Even 5060 ti 16gb would work.  I’m not an AMD guy, but everyone is saying they have great options too, I think one with 16gb of vram?  A 5070 would blow your mind probably and is better than you might think. A 5060 ti 16gb would probably definitely serve you 1080p60 excellently.",buildapc,2026-01-04 03:42:29,1
Intel,nxko463,Arc B580,buildapc,2026-01-04 04:58:00,1
Intel,nxl713m,9060XT 16GB,buildapc,2026-01-04 07:24:06,1
Intel,nxl829m,1080p? 6650xt.. it can run 1440p too.   otherwise rx 9070 you should be able to find for around $550 and its brand new architecture,buildapc,2026-01-04 07:33:13,1
Intel,nxl829s,"Honestly for 1080p I would look at an older card but top tier for that gen. A 2080 super would run everything easily at 1080p for example, doesn't have to be the latest tech if you don't care for 1440 or 4k",buildapc,2026-01-04 07:33:14,1
Intel,nxlil9o,5070 would be perfect if you can get it for $500 .,buildapc,2026-01-04 09:08:11,1
Intel,nxlr7qi,"cheapest 5070 at micro center is exactly at $500, costing the same as the cheapest rx 9070.",buildapc,2026-01-04 10:26:24,1
Intel,nxn98t7,i got the 5070 for 1080p and its doin me great . get the best u can find bcs its a long investement,buildapc,2026-01-04 16:16:14,1
Intel,nxnjvt9,"Used 3080,3080ti",buildapc,2026-01-04 17:05:20,1
Intel,nxnnbr7,A 4070 or 4070 super of eBay,buildapc,2026-01-04 17:20:57,1
Intel,nxi7ets,"If you have a Microcenter near you, check their refurbished and open box section frequently. They may have really good deals.",buildapc,2026-01-03 20:56:14,1
Intel,nxiw4ns,Do you plan on staying at 1080p or moving to 1440p? Cause 1440p monitors have gone down in price as 4k becomes more popular. Is $500 your top top budget cause if you depending on your location a better future proofed 9070xt may be available. Or just not spend that much on a gpu for 1080p.,buildapc,2026-01-03 22:58:42,1
Intel,nxjhd36,Xbox series X,buildapc,2026-01-04 00:50:13,0
Intel,nxj0yhw,"Don't go this way.   Drop the 7800x3D for a 7700x or 9700x, which will allow you a bigger budget for a GPU, like the 9070 XT to play easily at 1440p.",buildapc,2026-01-03 23:24:08,-1
Intel,nxkrg3d,You need 1440p,buildapc,2026-01-04 05:20:53,0
Intel,nxjjdcb,"Shit man i still have a 1080ti i have overclocked to the tits that runs everything i throw at it in 2k, Newer stuff that relies on dlss and frame gen i may have to settle for 60 fps med-hig, but everything from the last few years with a 5700x3d is still chugging 100+ fps mostly on max",buildapc,2026-01-04 01:01:10,19
Intel,nxk4yey,What CPU around $200-$250 would you recommend to pair with it?,buildapc,2026-01-04 03:00:51,20
Intel,nxk81ug,Just bought it today. Loving it already. Big upgrade from my ancient RX 580 lol,buildapc,2026-01-04 03:18:30,5
Intel,nxrf6l2,"My friend, do you recommend that gpu for 1440p too?",buildapc,2026-01-05 04:26:04,1
Intel,nxhhdvc,Makes no sense literally 0 performance differences between the 7600x and 7800x3d with a 9060xt https://youtu.be/rM0B8sidI0Q,buildapc,2026-01-03 18:51:04,-160
Intel,nxj93t5,"I haven't seen a 9070XT below $600 in at least a month, and those were MC in-store exclusive. Normal price seems to be $630-660 now, and that's likely to keep climbing.  Even the 9070 is above $580 now.",buildapc,2026-01-04 00:06:43,28
Intel,nxkpsru,My PC Hellhound 9070xt was like $600 on Black Friday.  What were the ones that were going for $550?,buildapc,2026-01-04 05:09:27,4
Intel,nxlzve8,"From what i saw on early benchmarks, the 5060 8gb can run every new game pretty much maxed at 60fps 1080p native. 16gb is mostly overkill for 1080p",buildapc,2026-01-04 11:42:23,2
Intel,nxiuyip,Just get a 5070 they’re on sale for 449 to $500 everywhere,buildapc,2026-01-03 22:52:39,17
Intel,nxi3d22,This! The difference you’ll see going for a 9070xt over a 9060xt is farrrr greater than going from 9600X to 7800x3d,buildapc,2026-01-03 20:36:05,14
Intel,nxj5ki6,"It's not, you're just making it sound like it is. A €200 3060 Ti with DLSS turned on will run everything nicely at 1080p.",buildapc,2026-01-03 23:48:06,10
Intel,nxihbyr,"It's not. A sub 400 dollar 9060xt 16 gb is more than plenty for 1080p and can push 1440p. The Arc B580 is a 250 dollar card that will be great for 1080p. You can even find cheaper for solid 1080p 60 gameplay. 500 dollars is overkill for 1080p, OP is just giving us his budget for a gpu.",buildapc,2026-01-03 21:44:46,37
Intel,nxja384,The GTX 970 launched in 2014 for $349. That's $448 today.  People just don't seem to realize how much inflation has happened in recent years.,buildapc,2026-01-04 00:11:56,8
Intel,nxjfcz1,"Its not, but op is asking for 500, what's the best. A $100 gpu can run 1080p, but so can a $2500 gpu. Op limit is 500",buildapc,2026-01-04 00:39:35,2
Intel,nxkczk4,5090 can do 1080 as well buddy. Crazy that you gotta pay 3k for 1080 gaming in 2026,buildapc,2026-01-04 03:46:54,2
Intel,nxip55j,Maybe I’m crazy but I think a 1080p SYSTEM should be around 500-600,buildapc,2026-01-03 22:23:14,2
Intel,nxknfpx,Good choice actually. Higher the resolution the more GPU dependent you become.,buildapc,2026-01-04 04:53:29,2
Intel,nxkpmfb,this is the way. I just checked and they had a 5070ti for $675...  Also 5070 for $480,buildapc,2026-01-04 05:08:13,1
Intel,nxkkvfn,It's not 2016 bro,buildapc,2026-01-04 04:36:25,7
Intel,nxlredz,are you a time traveler that both come from 2010 and is a boomer by any chance?,buildapc,2026-01-04 10:28:06,1
Intel,nxjtsau,My 1080ti still going and I'm guessing clinging on for dear life. I pray to the gamer gods it lasts until I can afford an upgrade.,buildapc,2026-01-04 01:58:31,8
Intel,nxkaibp,I know I’m not the original guy but the 7600x or 9600x seem pretty solid for their prices.,buildapc,2026-01-04 03:32:41,30
Intel,nxlw261,As other guys said BUT if u dont have ddr5 ram just get 5600x since they use ddr4   Obviously its a downgrade but u will save more money maybe 100-200$ ( 40$ for AM4 motherboard 70$-90$ for ram if 16gb probably a 20-30$ for cpu ),buildapc,2026-01-04 11:09:20,2
Intel,nxhir15,"I don't disagree, but they're not asking about their CPU, they're asking about their GPU. I think the 9060XT is the best 1080p card you can buy right now, regardless of their CPU choice.",buildapc,2026-01-03 18:57:16,99
Intel,nxicpk4,Written downvote for you sir,buildapc,2026-01-03 21:22:19,38
Intel,nxj8td1,Don't generalize performance like this. Different games have different hardware usage. Obviously AAA games at max settings will be GPU bound with a lower-end GPU.  You also linked a video running CS2 and Valorant at Very High settings... so the person obviously doesn't know what they're doing.,buildapc,2026-01-04 00:05:13,10
Intel,nxjhmwx,Here let me proofread this before you post it.   “Would it make more sense to go with less CPU and upgrade the GPU for better performance now and into the next upgrade?”  Copy/paste if you like.,buildapc,2026-01-04 00:51:42,5
Intel,nxjmtbp,Cards already went up $50-70 since two weeks ago.,buildapc,2026-01-04 01:20:11,9
Intel,nxkk00n,Just got the 9070 XT 16gb for $580 at micro center in Fairfax,buildapc,2026-01-04 04:30:40,4
Intel,nxl3i63,There was a Power Color 9070xt for 550 at Newegg a couple days ago. But it was a one day deal.,buildapc,2026-01-04 06:53:48,2
Intel,nxmp4se,"Until you enable RT and then it’s not. Unless you’re budget constrained I would definitely avoid an 8 GB card. Even then, I would suggest getting an arc b580 with 12 GB.",buildapc,2026-01-04 14:34:39,1
Intel,nxke67n,At 1080p? Not a chance.,buildapc,2026-01-04 03:53:53,6
Intel,nxjn0ji,It will do just fine with dlss turned off lol 3060ti is pretty strong and 1080p is easy drive. My Nephew is still running my 1060 6gb lol,buildapc,2026-01-04 01:21:13,3
Intel,nxmnris,"*5k, they raised prices. Wanted to buy a 4090 when the 6090 comes out but it's gonna cost me an eye and a lung",buildapc,2026-01-04 14:26:54,1
Intel,nxl5szo,"the extra money spent on such a strong cpu would be much better put to use in a stronger gpu. the 7600x is cheaper and plenty unless he gets a very expensive gpu, which he's not doing. he is guaranteed to have a better computer if he balances this curve right; gpu is supposed to be the strongest component",buildapc,2026-01-04 07:13:27,0
Intel,nxk0yp1,"Same here,  except I'm still rocking a gtx 960.",buildapc,2026-01-04 02:38:27,4
Intel,nxkdmh2,"As someone with an R5 7600 and an RX 9060 XT 16GB, I can confirm it runs pretty much any game on 1080p max settings at very high FPS (most competitive games over 200/250), unless it's something with wonky performance like Tarkov/Arma or Expedition 33.",buildapc,2026-01-04 03:50:39,15
Intel,nxhjj88,Since op is building a complete new system I would invest less in a cpu and more in a gpu. For 1440p proof. But that's me.,buildapc,2026-01-03 19:00:48,-113
Intel,nxlmnew,Fuck.. I wanna upgrade but idk if should wait to find a deal or just bite the bullet? I hear AMD gpus will increase even more by the end of the month..,buildapc,2026-01-04 09:45:28,1
Intel,nxm9so3,Meanwhile the 5070Ti went up about 500$ already lol,buildapc,2026-01-04 13:00:34,1
Intel,nxl86nk,"Agreed, it's all overkill for 1080p",buildapc,2026-01-04 07:34:18,1
Intel,nxjvhzm,Yeah lol a 2070 super will do the business too at 1080p. Op definitely has a lot of flexibility.,buildapc,2026-01-04 02:08:01,2
Intel,nxl9tjk,"Wrong. This is completely subjective and depends on use case.  1080p doesn’t take much to drive. You’re going to hit your monitors refresh rate limit in most games with even a 3070… Aside from the AAA titles, of course. But 1080p im guessing OP plays a lot of FPS titles. A 7800x3D will give OP better 1% lows and more consistent frames.",buildapc,2026-01-04 07:48:57,0
Intel,nxkwtnf,You'd have no problem with 1440p as well.,buildapc,2026-01-04 06:00:21,6
Intel,nxhle66,"I would agree, but they're not asking that question. I am simply answering what they had asked and I'm not providing unsolicited advice.",buildapc,2026-01-03 19:09:18,101
Intel,nxo42ju,Prob wont find a deal. Demand just skyrocketed and stocks are going dry.,buildapc,2026-01-04 18:36:06,1
Intel,nxo3l58,Not seeing that yet. I am seeing base model stock going out on this panic buy though. The OC'd and special editions are still left but those are the +$150 versions.,buildapc,2026-01-04 18:34:01,1
Intel,nxmrwzi,"Except for Tarkov, where you’d notice a HUGE difference if you use an X3D chip.",buildapc,2026-01-04 14:50:11,1
Intel,nxmrniy,"Yeah, I don’t have a 1440p monitor but I’ve seen some benchmarks and it should run well.",buildapc,2026-01-04 14:48:45,1
Intel,nxhncbh,"Op is indeed asking for GPU advice to match with the CPU. However, if you're more experienced than op your advice could also be to downgrade the CPU and upgrade the GPU for better overall performance.  It's still reddit and not everyone is experienced enough",buildapc,2026-01-03 19:18:17,-110
Intel,nxo4gaf,"Damn.. that’s what I was thinking. I know MicroCenter has a deal if you sign up for their credit card, you’ll get 5% off which is a little over $30 for the card I want but idk if getting a card is worth it just for that.",buildapc,2026-01-04 18:37:44,1
Intel,nxrbpw7,"on newegg the ASUS Prime 5070Ti I bought was 1100$ CAD, 2 days later they refunded me for it being ""out of stock"" and then i check back the cards were never out of stock, instead the price jumped to 1900-2000$ CAD.  I decided to grab an MSI card out of spite. and it was 100$ cheaper than the ASUS one.",buildapc,2026-01-05 04:05:33,1
Intel,nxn2fr2,9060xt 16gb and Ryzen 7 7700x just got a new 1440p monitor and it handles it well.,buildapc,2026-01-04 15:44:08,2
Intel,nxi27c7,I assume he stopped responding because he realized it was pointless with you. You are more than welcome to go have your own convo with the OP about your CPU suggestions. But currently you are attacking someone who directly answered the OPs question with info outside of the scope of the original post.  It's not a good look!,buildapc,2026-01-03 20:30:12,67
Intel,nxj12n3,you clearly have 0 clue whay youre talking about lmfao,buildapc,2026-01-03 23:24:44,18
Intel,nxi69tp,I didn't try to attack him if it came across like that I'm sorry.   I just try to help Op getting the best price to performance.  Like more people in this thread funny how I get down voted for giving alternative suggestions.  While I'm not the only one suggesting downgrading the cpu and upgrading the gpu.,buildapc,2026-01-03 20:50:35,-31
Intel,nxjv8ge,Lol check the video and see it yourself,buildapc,2026-01-04 02:06:32,-1
Intel,nxjgtc4,"Hmm, so you acknowledge you aren’t the only one suggesting downgrading the cpu, and yet you’re the one getting downvoted. So maybe, that in itself is not the reason why people are downvoting you.",buildapc,2026-01-04 00:47:17,15
Intel,nxjw32k,"Dont you just love it when you've got great info to provide, but if you speak in a slightly rude or mean tone of voice/writing style. You get downvoted to oblivion by cowards, for being a big meaniehead..... The mob sure gets easily uncomfortable on this website.",buildapc,2026-01-04 02:11:11,-6
Intel,nxk2a8m,Bro the autism in this sub I swear,buildapc,2026-01-04 02:45:52,3
Intel,nxjwobl,I'm not even trying to be rude or mean. People probably not used to Dutch directness,buildapc,2026-01-04 02:14:27,1
Intel,nxll2qx,1. Depends on the situation. In a situation where 16gb of ram is insufficient: having 8+8+16 would be faster. In a situatuon where 16gb is plenty: having 8+8+16 would be slower.  2. How is your gpu and cpu usage when playing the game.,buildapc,2026-01-04 09:30:53,2
Intel,nxlmzdv,"Google is wrong about arc raiders. I think the 5070TI should see about 100-120ish FPS at 1440p high without raytracing, but it can vary based on your settings. Your CPU and RAM speeds do play a part as well.  With the 5070Ti, you can enable the DLSS Transformer model and set it to quality or balanced to b get extra frames at basically no visual cost. You can also use frame generation if you want, but I would personally not recommend it.  You can check things like making sure your RAM OC is on and that ray tracing is set to what you want.",buildapc,2026-01-04 09:48:28,2
Intel,nxlmboe,"Don't do it, get another kit of 2x 8gb.",buildapc,2026-01-04 09:42:26,1
Intel,nxo79fm,"So when it comes to not reaching performance goals, you mentioned that your GPU usage was 60-80%. That is not good. That means that your processor is holding you back from achieving a higher frame rate. If you turn down the graphics settings, or decrease resolution and the frame rate doesn't improve: you are CPU limited. This happens often in esports/open world titles when trying to achieve high frame rates. Make sure that PBO is turned on for your processor in the bios, get a proper cooler for said processor before you do that though (stock coolers suck), go with a thermalright peerless assassin if you want to save money. In your bios, turn on XMP or expo if your ram and motherboard support it. The default profile or configuration for XMP is fine, tuning it could give a little more performance but can be unstable and time consuming.   Ideally you want close to 99% GPU utilization for a smooth gameplay with consistent frame times.  You might find turning up some gfx settings actually helps in a CPU bound scenario.   Edit: YouTube is a very good source for benchmarks. I wouldn't trust Google ai. Look up an arc raiders benchmark with your specific specifications: 5600x +5070ti and compare settings and fps. If your within 5-10 fps it's likely variables with card model (OC or non OC versions), overclock or undervolt performance on CPU, or ram kit speed.   Also another thing I remembered, in certain ryzen x model CPUs they will boost to what's called TJ Max temperature (sometimes 85-95C) automatically with PBO turned on so that they can get the maximum clock speeds. Improving the cooling solution will therefore increase the clock speeds because it allows the CPU to run at a higher voltage.",buildapc,2026-01-04 18:49:49,1
Intel,nxllg3f,"Right, so basically most games it’s better to just stick with 2x8 then, because I usually use within the limit I guess.  To answer the question, my GPU usage is usually around 60-80% and CPU depends on the game, but usually quite high, I have gotten CCleaner to clear up background usage too, I can give you exact numbers within 10 minutes, I will go an boot my PC up now. Also my CPU runs at around 70-85°C which I don’t know if it is too high or not??",buildapc,2026-01-04 09:34:21,1
Intel,nxlms8a,To answer the question my CPU right now whilst in menu of ARC Raiders is between 30-40%,buildapc,2026-01-04 09:46:41,1
Intel,nxloqnl,"How do I check my RAM OC is on?? Also, I have an MSI 5070TI Gaming trio OC, would it hurt to overclock it or Undervolt it? Would that help a bit?",buildapc,2026-01-04 10:04:11,1
Intel,nxlomkc,"Okay, maybe I will sell this 1x16 as it’s worth £100 rn and get 2x8",buildapc,2026-01-04 10:03:12,1
Intel,nxlqptm,"For a 5600x, 70~85°C sounds little too hot. Do you happen to be using stock cooler?",buildapc,2026-01-04 10:21:54,1
Intel,nxlp1f1,Usually it will show in Task Manager on the RAM page. Do you know what speed your RAM is supposed to be?  I would personally not worry much about overclocking or undervolting the card. Performance is unlikely to change much.,buildapc,2026-01-04 10:06:50,1
Intel,nxlqwq2,"I am using the stock cooler, yes. The next investment needs to be a good cooler that looks good also, just hard to find an affordable one since I would ideally prefer liquid cooling.",buildapc,2026-01-04 10:23:38,0
Intel,nxlpind,Just went to my Amazon order history and the ram I bought was: Corsair VENGEANCE RGB PRO DDR4 RAM 16GB (2x8GB) 3200MHz,buildapc,2026-01-04 10:11:06,1
Intel,nxlpngg,"So, 3200MHz to answer your question, on Task Manager the RAM states to be running at 2133 MT/s (I don’t exactly know what this means, or whether it’s up to speed or not as it’s a different measurement to MHz)",buildapc,2026-01-04 10:12:17,1
Intel,nxmb88n,"If space allows it, grab a Phantom Spirit 120, not too expensive and I've found it does a great job cooling my 5700x3d",buildapc,2026-01-04 13:10:18,1
Intel,nxlq5ve,"2133 is the base speed of DDR4, which means that your RAM is not currently overclocked. I would suspect this is what is causing your performance issues.  If you go into your BIOS, there should be a setting somewhere for loading up a 'XMP' profile that says 3200 on it. Enable that and then boot your PC and check task manager again.",buildapc,2026-01-04 10:16:54,1
Intel,nxlqa97,"Okay, I shall do just this and get back to you ! Thank you for the time being!",buildapc,2026-01-04 10:18:00,1
Intel,nxlqfjk,If you need help you can probably find a YouTube video showing you where the setting is. Basically all Asus BIOS should look the same. I think it's on the left side of the main page in the 'EZ Mode'.,buildapc,2026-01-04 10:19:19,1
Intel,nxlssvm,"Okay, I have OCd my RAM and it’s at 3200 MT/s now. Thank you very much, let me test my performance now!",buildapc,2026-01-04 10:40:44,2
Intel,nw7r87c,B580.  VRAM matters.,buildapc,2025-12-27 16:51:43,9
Intel,nw7rzuo,"easily an rtx 3070. the 3070 destroys b580 in pure rasterisation to the point where vram doesn't make a difference.i would instead look at the 9060xt 8gb instead of the b580, which is newer than the 3070, performs slightly better, uses less power and has more features like fsr 4",buildapc,2025-12-27 16:55:36,5
Intel,nw7tqjq,B580 is hard to beat at its current discounted price.,buildapc,2025-12-27 17:04:19,2
Intel,nw85uqm,A 3070 would slightly outperform i believe  But b580 being newer and more vram may be useful in the future,buildapc,2025-12-27 18:05:53,2
Intel,nw7urnc,"B580 and if you’re into one of the 4 free games, that’s an added bonus",buildapc,2025-12-27 17:09:35,1
Intel,nw8bavu,If you plan to use a linux distro then B580.,buildapc,2025-12-27 18:33:00,1
Intel,nw7sbd8,B580. Do not support NVidia for their predatory market manipulation.....,buildapc,2025-12-27 16:57:10,-1
Intel,nw8es7m,"Get an NVIDIA GPU, probably the RTX 4060. The GeForce cards before the 4000 series don't support Frame Generation, while the RTX 4060 will support it.   NVIDIA DLSS + Frame Generation together is awesome.",buildapc,2025-12-27 18:50:23,0
Intel,nw7ufdn,"Where are you located (i only deal with locals)  If youre in canada, im going to be selling my rtx3070ti for $250 cad.  Never overclocked, was used in my gaming rig.",buildapc,2025-12-27 17:07:50,-2
Intel,nw85p01,He is buying used it literally doesn't effect nvidia,buildapc,2025-12-27 18:05:05,5
Intel,nw88ryo,"True, true",buildapc,2025-12-27 18:20:29,1
Intel,nvqd2qk,">my budget is 300 and the Rtx 3060 is getting expensive and out of stock, what should i do?  RTX 3060 is out of production for a long long long time, prices of new pieces might be high. What is the currency and how much does B580, RTX 5060, RX 9060xt 8GB and 9060xt 16GB cost?",buildapc,2025-12-24 15:45:07,2
Intel,nvqd37j,"With the 5500 you're better off getting an RX 9060xt   The 8GB version is $287, good enough for 1080p gaming  The B580 is great if zen 4/5",buildapc,2025-12-24 15:45:12,1
Intel,nvqf6kt,"Unfortunately, the 5500 is not a very good zen 3 cpu. The L3 cache is neutered, and it has reduced PCIE lanes.",buildapc,2025-12-24 15:56:18,1
Intel,nvqfq9g,You need the latest BIOS update but you should be good with 5000 series. This also depends on the chipset... if you have the newest one for AM4 then you are going to have an easier time than older ones.. Some of the really early ones dont work at all.,buildapc,2025-12-24 15:59:09,1
Intel,nvqpmwl,Idk about 5500 but I know it works very well with 5600 and people are saying don't go lower than 5600 for it,buildapc,2025-12-24 16:51:38,1
Intel,nwueeyb,"It looks like you may have posted an incorrect PCPartPicker link. Consider changing it to one of the following:  * [Use the Permalink](https://i.imgur.com/IW0iaOm.png). note: to generate an anonymous permalink, first click [Edit this Part List](https://i.imgur.com/uqDIcdt.png).  Or, make a table :    * [new.reddit table guide](https://imgur.com/a/1vo0GHH)   * [old.reddit table guide](https://imgur.com/C86vdxB)         *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",buildapc,2025-12-31 02:16:01,1
Intel,nwoc6ez,"They're basically the same CPU, Intel makes 3 chips:  * H0 - 6 fast cores, 12th-14th Gen  * C0 - 8 fast + 8 slow cores, 12th-14th Gen * B0 - 8 fast + 16 slow cores, 13th-14th Gen faster DDR5 support   Both are B0 so it's the same silicon, they just disable some cores and tweak default settings. But it's a K CPU, so tweaking settings isn't blocked unless you have a cheap motherboard.   It's not unreasonable with a 5070 Ti, especially with fast DDR5 which is now unreasonably priced. I did a build for a friend with a 5070 Ti and 14600K(F) with one where we used a 48GB 2x24GB DDR5-7000CL32 kit with Hynix dies from Patriot for about $135. Unfortunately today, that same RAM is about $550.   At higher resolutions like 1440P-4K, the difference between CPUs/RAM will become more negligible.",buildapc,2025-12-30 04:26:50,13
Intel,nwofq16,"14600K is sufficient, especially considering the huge jump in price to the 14700K. Unless the prices are similar, the 14600K is the better pick.",buildapc,2025-12-30 04:50:00,10
Intel,nwov0um,If you plan to play 1080p low get the i7,buildapc,2025-12-30 06:45:22,1
Intel,nwp1i8b,"I'm using the 14600k on my server, works well.",buildapc,2025-12-30 07:42:38,1
Intel,nwpqabx,"To be honest brother i have i5 14600kf and a 5070 but i play mainly cs2 and there i have good performance but sometimes i have dips in fps idk about other games but im thinking about the same think as you .. should i get and 14700kf, my ram is 3600mhz cl17 2x8gb but i have tuned it a bit to cl16  , but even if u have ddr4 the i7 should be the better choice , check amazon i saw the 14700kf for 330€ on the german amazin tho",buildapc,2025-12-30 11:30:25,1
Intel,nwpttk1,"14600k prob the best overall and will keep the 5070 Ti happy. If planning on staying on socket 1700 for a long while, grab the 14700K if have the cash, and also a really good CPU cooler. Update the board BIOS as well.",buildapc,2025-12-30 11:59:41,1
Intel,nwofua6,"My bad, I should have provided more specs. I'm not planning to overclock. I have an MSI B660M MAG Mortar DDR4 motherboard. So your take is that the 14600K will be enough?",buildapc,2025-12-30 04:50:47,1
Intel,nwohxe6,Problem is that my platform is ddr4. So my thinking was that maybe i7 can compensate ram weakness somehow 😅  In my place i5 is about 270$ and i7 is about 420$ so yeah it's bit pricey,buildapc,2025-12-30 05:04:48,0
Intel,nwoh4q8,"Let's put it this way.   Over-simplified the 14600K(F) is like a 150 FPS class CPU while the 9800X3D is like a 250 FPS class CPU.   A 5070 Ti isn't typically good for 150+ FPS in modem games unless you're doing 1080P and/or low-medium settings.   And if you have a 120-144 Hz monitor, then it doesn't even matter. If you have a 240 Hz 1080P monitor, then your may have a bottleneck.   But more than the 14600KF, the DDR4 may also be a bottleneck. However it's not DDR5 > DDR4.   It's Good DDR5 > Good DDR4 > Bad DDR5 > Bad DDR4.   So if you have decent DDR4 RAM, there's a small performance impact. If you have bad RAM, there can be a bigger performance impact.  You can use a RAM latency calculator:  https://notkyon.moe/ram-latency2.htm  Generally:  * Below 9ns is top tier  * Below 10 is excellent quality * 10 is decent * 12 is ok * 13-15 is low quality * 16+ is garbage tier",buildapc,2025-12-30 04:59:21,6
Intel,nwoiycc,Hmm only to a certain extent. Better to put that price difference into actual ram instead haha. The future proof choice is actually to just go ddr5 right now.,buildapc,2025-12-30 05:11:54,1
Intel,nwpailt,Reply to me tomorrow I will tell you,buildapc,2025-12-30 09:06:14,1
Intel,nwoj8hx,"Thank you for such a great, meaningful answer. I'm going to dive deeper into this. Happy holidays!",buildapc,2025-12-30 05:13:51,3
Intel,nwp0rik,"Do you have an opinion on running 4 sticks of <9ns dual channel DDR4, versus running 2 sticks of ~10-11ns dual channel DDR4, for the same amount of RAM?  Both scenarios are in a setup similar to the one OP is considering, with a 5070ti, a 14600k but on a Z690 board.",buildapc,2025-12-30 07:35:52,1
Intel,nwpaujy,"Yea, but lga 1700 is a dead end. It is not future proof.",buildapc,2025-12-30 09:09:20,1
Intel,nwpdc3d,"Ddr5 is, as compared to ddr4. Op already said not to mention am5 or anything else, and that his choice is between those two cpu only. The more future proof choice is ddr5 ram.",buildapc,2025-12-30 09:32:56,1
Intel,nwz1bn8,"I mean not future-proof in terms of upgrades, but squeezing the maximum for reasonable money from a dead platform to be able to use it for 3-5 years.",buildapc,2025-12-31 20:29:34,1
Intel,nwpfprx,"It doesnt matter if the latency of ddr4 is low. Besides, new mobo + ram + cpu would cost a fortune for a little bit perfromance boost on a dead end platform. There is no future proofing in  this scenario. cl15-16 ddr4 3600 mhz would smash cl 36 6000 mhz.",buildapc,2025-12-30 09:55:01,1
Intel,nwqneqb,"For those games, DDR4/AM4 would be just fine. With current prices sitting at what they are, DDR5's improvement over DDR4 is simply not worth the price jump.   Granted, you will not be as ""future-proofed"" with DDR4/AM4, but you will still have a good, perfectly adequate PC. Especially considering the use it'll go thru.   DDR4 is still a very viable option, and you could eventually go to 4 sticks of RAM (something that is iffy on DDR5)  with minimal issues in the future.",buildapc,2025-12-30 15:03:54,5
Intel,nwqn6vp,"Ddr4 is def viable, as long as you dont get scammed by some gonk overpricing them to the max",buildapc,2025-12-30 15:02:45,3
Intel,nwqp5t1,I would just go with DDR4.  My PC right now still has DDR4 (64GB) with a 11700K and I have no issues.,buildapc,2025-12-30 15:12:58,2
Intel,nwrvsdq,"DDR4 is fine. I've been building PCs for 20 years and a little trick a professional taught me that has served me well is this: buy just behind the curve. Whatever is the newest thing or when something new is about to become the norm, buy the generation right before that. That's where you get the best bang for the buck. This stuff will last way more into the future than people let on, especially for a casual user.",buildapc,2025-12-30 18:33:12,2
Intel,nwqp15c,"Check out Wish, Temu or a similar site for the parts you would be surprised at the price of everything.",buildapc,2025-12-30 15:12:19,1
Intel,nwqvrxt,"Compare the prices of both and see what they are. If the difference between DDR4 and DDR5 is only 100$ and you can pay that, go DDR5. If not, go DDR4.",buildapc,2025-12-30 15:45:32,1
Intel,nwr5km8,You might even find a cheap second hand box with enough ram in leaving only gpu to consider.,buildapc,2025-12-30 16:31:39,1
Intel,nwrf10m,"If you're just building a machine for casual gaming, DDR4 is perfect. The additional bandwidth of DDR5 isn't needed by 95% of computer users. There are 600 and 700 series motherboards for Intel that take DDR4 and support Intel 12th, 13th, and 14th gen CPUs. An i5-13600K or 14600K will punch higher than its weight class, each having 14 cores/20 threads. The 14600K for example is same price as AMD Ryzen 7 5800XT and 27% faster on aggregate.",buildapc,2025-12-30 17:15:47,1
Intel,nwsd812,"Ddr4 is still decent, particularly using am4/5700x or 5800x3d for gaming",buildapc,2025-12-30 19:55:15,1
Intel,nwqsh7t,"If she's not really into gaming maybe the enthusiast's option (building a PC) isn't really the way to go? Why not get her a Steam Deck and a copy of Stardew Valley, get a dock to hook it up to your TV, and play some co-op games together?",buildapc,2025-12-30 15:29:31,0
Intel,nwqngjb,"Honestly if all your wife is gonna do is game light games lig minecraft id prob recommend going with maybe a r5 5600 16gb of 3000mhz ram and maybe a 1080 or 2060, would be imo the best choices depending on how the used market is where you live",buildapc,2025-12-30 15:04:09,2
Intel,nx0v4hv,"CPUs don't support resizable BAR, devices do. Arc cards don't require resizable BAR it's just expected to be used.",pcmasterrace,2026-01-01 03:06:37,2
Intel,nx0ze2r,"Rebar isn't needed for Arc gpus to function, but it is needed for them to function as intended. I believe Arc does need a motherboard with a UEFI bios, but could be wrong on that.",pcmasterrace,2026-01-01 03:35:19,2
Intel,nwywe0i,You can look up compatibility on the PC builder site.   I got a Z790 for 125$   I got the I5-14600Kf for 150$   It holds up really nice.  BE REALLY CAREFUL the Intel GPUs need REBAR so keep your eyes peeled for that.,pcmasterrace,2025-12-31 20:02:48,2
Intel,nwyvx1g,"Must be nice to have a MC, my closest is a few thousand miles away.",pcmasterrace,2025-12-31 20:00:16,1
Intel,nwz9s45,intel 13th and 14th gen have cpu failures so only buy if you are willing to risk that,pcmasterrace,2025-12-31 21:15:39,0
Intel,nwyxczb,Totally forgot about the rebar. Thanks for the reminder. My current cpu and mobo support rebar and I have it activated. Definitely something I’ll have to consider when I upgrade to ddr5,pcmasterrace,2025-12-31 20:08:03,1
Intel,nwyxm93,Would 12600kf for 100 be good?,pcmasterrace,2025-12-31 20:09:27,1
Intel,nwyxsvs,A few thousand miles? Sorry that sucks. Their bundles/combos on parts are better than sex sometimes.,pcmasterrace,2025-12-31 20:10:26,2
Intel,nwyxsdq,I mean.. CPUs are pretty far ahead of GPUs at least in terms of gaming.   Unless you planned on running 1440 with everything on ultra the B580 is great on most games. You’ll never feel like you are forced to run a game on low graphics.,pcmasterrace,2025-12-31 20:10:22,1
Intel,nwyy97j,If you plan on investing in Intel GPUs I would press you to get the newest gen.   Intel ARC B580 which is an entry Intel GPU didn’t play nice with older Intel Chips   12600 would hold up just fine.,pcmasterrace,2025-12-31 20:12:54,1
Intel,nwyyd72,Thanks there’s also the brand new core ultra 5 225f with bf6 for 150 on amazon. Would ultra core be a better fit?,pcmasterrace,2025-12-31 20:13:30,1
Intel,nwyyobw,If you don’t plan on upgrading it’s fine but if Intel releases a newer card that more powerful it might get bottlenecked.,pcmasterrace,2025-12-31 20:15:12,1
Intel,nwyyr3l,Dang maybe am5 cpu is better,pcmasterrace,2025-12-31 20:15:37,1
Intel,nwz119d,I’d say to go AMD.   Intel royally fucked up in that race but since they did have a major price drop because of it I scooped one up cheap.,pcmasterrace,2025-12-31 20:27:59,1
Intel,nv5otsy,Going to be a downgrade obviously. Intel has Been getting to the point those cards are actually good but it’s still going from high mid tier to low tier,pcmasterrace,2025-12-21 06:00:51,1
Intel,nv6e0b8,you got a free game from intel,pcmasterrace,2025-12-21 10:05:37,1
Intel,nv6e403,as a low tier card it's quite decent. i use it to play games at 4k,pcmasterrace,2025-12-21 10:06:36,1
Intel,nv6hh4w,Yep went in expecting a downgrade but I don't game too much lately haven't found the latest games all that good and mainly playing older games at the moment so the trade off for now isn't huge for me.,pcmasterrace,2025-12-21 10:39:59,1
Intel,nv6hlv9,Yep Battlefield 6 key.,pcmasterrace,2025-12-21 10:41:18,1
Intel,nw0ptay,How?,pcmasterrace,2025-12-26 13:17:50,1
Intel,nvaa10t,Very cool Exxazy,pcmasterrace,2025-12-22 00:00:40,1
Intel,nvacuz3,"![gif](giphy|Idg2rAVGS3xMZtBdhu|downsized)  It’s fine, plastic doesn’t conduct electricity 🫣",pcmasterrace,2025-12-22 00:16:49,1
Intel,nvag1q5,![gif](giphy|xT5LMtbEtZnbbCE08g),pcmasterrace,2025-12-22 00:35:00,1
Intel,nvavrl6,It lost its sparkle  ![gif](giphy|63VH5uoslatcA),pcmasterrace,2025-12-22 02:08:58,1
Intel,nvb01k1,[https://www.enostech.com/sparkle-intel-arc-b-series-graphics-cards/](https://www.enostech.com/sparkle-intel-arc-b-series-graphics-cards/)  Is this your card? So it's like a design / badge of the brand,pcmasterrace,2025-12-22 02:35:26,1
Intel,nvbewrp,Remove it before it falls in the fan and fucks it up.,pcmasterrace,2025-12-22 04:11:20,1
Intel,nxmjc0s,What exactly is the point of putting 2 different screenshots with different in game locations?,pcmasterrace,2026-01-04 14:00:52,2418
Intel,nxmf1kv,But can't you just adjust the monitor/ TV brightness etc,pcmasterrace,2026-01-04 13:35:00,768
Intel,nxmmzmk,"This literally changes nothing. It just disables the in game overlay and the ability to have preset filters. You can still go the Nvidia panel and change contrast, gamma, and digital vibrance to achieve the same effect. It applies it to the monitor rather than a specific program",pcmasterrace,2026-01-04 14:22:31,173
Intel,nxmdz77,"Always sad when the filters are disabled, people who abuse them just do it the other way and other people who just use them to make the game look lil better witthout really boosting the brightness up suffer.",pcmasterrace,2026-01-04 13:28:14,121
Intel,nxmd6jv,"Filters gave PC players a clear advantage, this evens things out.",pcmasterrace,2026-01-04 13:23:06,469
Intel,nxmgavr,Well I'm screwed my monitor sucks in the blue part of the color spectrum,pcmasterrace,2026-01-04 13:42:50,29
Intel,nxmhr6n,Oof this better not disable RTX HDR  Edit: I would rather have the intended picture but i wonder if you could get a similar effect with windows HDR then deliberately mis-calibrating your display to have a higher max nits.,pcmasterrace,2026-01-04 13:51:31,20
Intel,nxmgr7f,Yeah cause monitors don't have gamma settings xD,pcmasterrace,2026-01-04 13:45:36,13
Intel,nxmhaz4,Ok now disable aim assist on consoles and NOW we're fair,pcmasterrace,2026-01-04 13:48:55,211
Intel,nxmiwc3,Let's ignore monitors can do same thing...mine has some AI Night vision...,pcmasterrace,2026-01-04 13:58:14,17
Intel,nxmhw1o,Mom said it was my turn to post this now,pcmasterrace,2026-01-04 13:52:18,21
Intel,nxmp5rj,Do they get how folks don't have the same exact eyesight they want?   I know I don't with my thick glasses.,pcmasterrace,2026-01-04 14:34:48,3
Intel,nxmjci3,Should do it in escape from tarkov as well...,pcmasterrace,2026-01-04 14:00:56,3
Intel,nxmp5w1,Damn I guess that's some ugly graphics these sweaty players must suffer through to get an advantage? All the best to them I guess. Imagine if they still get killed 🤣,pcmasterrace,2026-01-04 14:34:49,3
Intel,nxmke0u,"I play games for fun. You will never see me lowering the settings in a competitive games, unless I get low frame rate. I play for fun.",pcmasterrace,2026-01-04 14:07:10,8
Intel,nxmhf8e,Alienware monitors have alien vision and it’s completely broken. The night mode or sniper mode is so broken it’s like the original arc filters but on steroids.,pcmasterrace,2026-01-04 13:49:37,7
Intel,nxoxmih,One of the biggest reasons I dumped PvP titles all together. Min/Max culture has absolutely destroyed gaming as a whole.,pcmasterrace,2026-01-04 20:48:44,7
Intel,nxmh8mp,People who turn down their res and graphics to get advantages are so sad.   Way to sweaty.,pcmasterrace,2026-01-04 13:48:31,21
Intel,nxmnouw,They also did similar for The Finals and it's caused issues with higher end cards running the game. I saw 3 or 4 posts about it on their sub the other day.,pcmasterrace,2026-01-04 14:26:29,2
Intel,nxqv7su,That will not fix it at all. Ppl changing gamma in control panel... Embarc surprises me weekly with how little they do in terms of balancing. Even their filters change does fuck all...,pcmasterrace,2026-01-05 02:33:25,2
Intel,nxr799l,Not sure how I feel about this one,pcmasterrace,2026-01-05 03:39:39,2
Intel,nxmjpsp,What after this?! Are people going to complain that setting the graphics to low gives people advantage?,pcmasterrace,2026-01-04 14:03:06,4
Intel,nxpymgn,"Who cares? You can increase gamma and raise black level on any modern monitor or tv and make the night raids look like day raids without any ""filters""",pcmasterrace,2026-01-04 23:45:09,2
Intel,nxmkljj,Let's just cut cross-play.,pcmasterrace,2026-01-04 14:08:24,3
Intel,nxmhutn,"I don't play arc raiders, but I hate it when games clamp down on stuff like this if they aren't actually taking concealment and camouflage seriously as a game mechanic. It's even worse when the maps aren't designed well around balancing hiding and exposure. It just feels so random.",pcmasterrace,2026-01-04 13:52:07,2
Intel,nxmtavx,so whiny console peasants got something nerfed on pc,pcmasterrace,2026-01-04 14:57:44,2
Intel,nxowmzf,"Cool, now please disable auto aim on consoles.",pcmasterrace,2026-01-04 20:44:10,2
Intel,nxovmoq,Let it be known only PC Gamers with micropenises (micropeni?) use Nvidia Filters to get an advantage. Real men play games all natural.,pcmasterrace,2026-01-04 20:39:26,3
Intel,nxpl486,"Pc player here, why can’t all of you pussies just leave your brightness alone and deal with it. The game is supposed to get dark, stop trying to adjust your settings to get an unfair advantage",pcmasterrace,2026-01-04 22:38:11,1
Intel,nxmj80g,I mean I just don’t get why PC players like their shit looking terrible. Flex a 2k+ build but it looks like doo doo intentionally just for a way to win..,pcmasterrace,2026-01-04 14:00:11,2
Intel,nxmk8gv,I think my monitor has a function that does the same as the filter did.,pcmasterrace,2026-01-04 14:06:15,1
Intel,nxmn3tj,Idk what to compare anything too since each image is a different location with presumably its own lighting,pcmasterrace,2026-01-04 14:23:12,1
Intel,nxmn7ym,It’s always the double braided hair.,pcmasterrace,2026-01-04 14:23:51,1
Intel,nxmnr4z,Sooooo… Tarkov next?,pcmasterrace,2026-01-04 14:26:51,1
Intel,nxmo76y,What,pcmasterrace,2026-01-04 14:29:22,1
Intel,nxmormb,"DayZ the game that dealt with this problem and no night vision besides the in-game night vision googles will let you see at night.  If you will try to fix this problem, you will see that game actually insanely grainy at night. You practically can barely see anything outside of things that you were already able to see. See someone in the bushes? Good luck with that. See someone 10 meters away? Yeah, good luck.",pcmasterrace,2026-01-04 14:32:35,1
Intel,nxmqo7r,"Isn't every game like this? Turn down the settings, insane frames, you're basically just looking at pixels.",pcmasterrace,2026-01-04 14:43:18,1
Intel,nxmr9pg,Reminds me of how back in the day you could turn the graphics down to the minimum to not load bushes in War Thunder,pcmasterrace,2026-01-04 14:46:37,1
Intel,nxmrd7n,Common cross play L,pcmasterrace,2026-01-04 14:47:09,1
Intel,nxmrdak,Oh no! Game is r/literallyunplayable now,pcmasterrace,2026-01-04 14:47:09,1
Intel,nxms2a7,Good thing I use Dolby vision and I can still make my game look like this at night. They fixed nothing,pcmasterrace,2026-01-04 14:51:00,1
Intel,nxmske4,Changes nothing you can still do it with other software and Arc raiders and Nvidia cant do anything about it,pcmasterrace,2026-01-04 14:53:44,1
Intel,nxmslsx,Can someone tell me why Nvidia has control over customer hardware?,pcmasterrace,2026-01-04 14:53:57,1
Intel,nxmtny1,This is literally Harrison Bergeron. /s,pcmasterrace,2026-01-04 14:59:40,1
Intel,nxmtxwu,Lmao what excactly the first filter actually do to gain that of an avantage? Remove shadows?,pcmasterrace,2026-01-04 15:01:09,1
Intel,nxmug8a,I have an oled. I don’t even have to use a flashlight in the dark areas.,pcmasterrace,2026-01-04 15:03:53,1
Intel,nxmvxua,God forbid they actually optimize anything.,pcmasterrace,2026-01-04 15:11:43,1
Intel,nxow0rr,reshade  https://preview.redd.it/6svd04w79ebg1.jpeg?width=588&format=pjpg&auto=webp&s=7b040ea81f23edc551b0f22927e4dec1a22f1490,pcmasterrace,2026-01-04 20:41:17,1
Intel,nxowbnn,"Same thing happened in Dayz, there's always someone using the filters or in DayZ's case - Reshade, to cheat. Sucks too, it's hard to look at dayz without Reshade.",pcmasterrace,2026-01-04 20:42:41,1
Intel,nxox6es,You can increase the brightness of your monitor in the nvidia control panel or in your monitors settings.,pcmasterrace,2026-01-04 20:46:38,1
Intel,nxp1c97,huh...didnt even know there was such a thing like filters...  how is nvidia enforcing this? through game client or driver update?,pcmasterrace,2026-01-04 21:06:07,1
Intel,nxp3wp9,Okay anyways. Darken the ingame settings and then go to Nvidia control panel and increase your settings there. Boombadabing.,pcmasterrace,2026-01-04 21:18:03,1
Intel,nxp5usc,Literally who cares in a game that's riddled with wallhackers lmao,pcmasterrace,2026-01-04 21:27:05,1
Intel,nxp5vt7,You can still easily do this with third party programs.,pcmasterrace,2026-01-04 21:27:13,1
Intel,nxp6sfl,"Im not sure which aspect of the lighting specifically causes it but Arc is definitely hard on the eyes for me, to the point where its literally the only game Ive ever used nvidia filters to alleviate it. Now only players with a monitor that has shadow settings can do what the filters were doing which isn't limited to pc players. Messing with nvidia control panel is not a good alternative.",pcmasterrace,2026-01-04 21:31:25,1
Intel,nxp9cio,"So funny thing: disabling filters seemed to make things more washed out, like the left image, while the right image was closer to what I remembered the game looked like before this happened.  Fun Fact: you can still apply monitor-wide filters via the Nvidia Control Panel and the game doesn’t stop it, it was how I got the game to look normal again after everything looked washed out. (This is the only game so far that has looked washed out)",pcmasterrace,2026-01-04 21:43:13,1
Intel,nxpgg4k,You know the only thing the filters ever get used for is the unfair advantage in pvp games idk why most even bother having it enabled at first only to disable it later because they realized “ah people are just sad” the filters are great but more so for singleplayer games,pcmasterrace,2026-01-04 22:16:11,1
Intel,nxpkoty,Me when i make my game look worse for no reason,pcmasterrace,2026-01-04 22:36:12,1
Intel,nxpx9fw,I mean I can get this.  This is the same as using modded skins in old counterstrike to make players light up like leaking gaspipes.,pcmasterrace,2026-01-04 23:38:19,1
Intel,nxqg6kg,I have a genuine question. Regardless of whether or not you think it’s ok for people to utilise this feature.   Why would Nvidia go through the effort of disabling this for arc raiders? Just because Embark asked? Is it typical for them to work that closely with game studios?,pcmasterrace,2026-01-05 01:13:24,1
Intel,nxqi30b,"me monitor has night vision, gg",pcmasterrace,2026-01-05 01:23:40,1
Intel,nxqwxz3,This was a convoluted way to do it anyways that made your game look better than doing it the old way.  If you just type “calibrate” in ur search calibrate display will come up. Press next 3x and turn up ur gamma. And then just turn up saturation/digital vibrancy in control panel..or you can do it all thru control panel but I like the slider on the calibrate display windows app..once you exit the window ur gamma goes back to normal.. You can then see in the dark any game (I used this for Roblox rust like 6 years ago)  Everyone’s monitor can do the same anyways. I have an oled so I literally can barely see in the dark regardless of settings unless I do the black boost thing but that makes black looks gray and it’s ugly,pcmasterrace,2026-01-05 02:42:38,1
Intel,nxra75a,"I like playing in the dark, feels more immersive",pcmasterrace,2026-01-05 03:56:45,1
Intel,nxrfych,I dont use filters but i want to cause i legit cant see shit sometimes on my HDR monitor and it fucking sucks,pcmasterrace,2026-01-05 04:30:22,1
Intel,nxrg0bg,this fixes absolutely nothing you just move from ingame filter to nvidea settings and or monitor settings and guess what guys your tv settings can do the same,pcmasterrace,2026-01-05 04:30:43,1
Intel,nxmmrzk,I play on pc and I hate ppl who do this. Even when on console never upped the brightness for dark places. It's for the weak,pcmasterrace,2026-01-04 14:21:18,1
Intel,nxmpqbb,Lets lock fps to so console players have fair chance.,pcmasterrace,2026-01-04 14:38:04,1
Intel,nxmtygg,"Can still adjust the monitor and windows color management.  The only solution is a locked system with only one approved monitor as if you were on some irl tournament.  As long as we have the ability to manage the monitors and windowsthen this type of ""cheating"" will always exist.",pcmasterrace,2026-01-04 15:01:14,1
Intel,nxnd13f,Good. If they want to cheat by having better vision and/or esp at least make them work for it.,pcmasterrace,2026-01-04 16:33:55,1
Intel,nxoxa74,\*Laughs in AMD\*,pcmasterrace,2026-01-04 20:47:08,1
Intel,nxp3ihc,Good. Look at Tarkov. Streamers were playing that with their shit looking like fuckin Fortnite...,pcmasterrace,2026-01-04 21:16:13,1
Intel,nxq4tt0,Laughs in AMD,pcmasterrace,2026-01-05 00:15:39,1
Intel,nxmh6g8,At this point nvidia just hates us,pcmasterrace,2026-01-04 13:48:08,-1
Intel,nxou3f0,I'd give a damn if these developers gave a damn about Voice Actors,pcmasterrace,2026-01-04 20:32:23,0
Intel,nxme4if,"I was sat in a Night Raid last week wondering if there was a way to brighten up the game because I couldn't see shit. Lo and behold when I found out about Nvidia Filters and gave it a go. It was a completely different game and I was loving it!  To my surprise today, I was struggling once again only to find out shortly after that they disabled this feature a couple of days back. I hope they brighten up the game at least a little bit...",pcmasterrace,2026-01-04 13:29:10,-12
Intel,nxmo5e1,I mean playing on consoles is dog shit anyway with a controller. You're competing with someone who can b-hop around you and point-click headshots vs someone in a tank: LEFT!!!!........ RIIIIGHT!!!....... UUUUP!!!,pcmasterrace,2026-01-04 14:29:05,0
Intel,nxp3juy,Good. Look at Tarkov. Streamers were playing that with their shit looking like Fortnite...,pcmasterrace,2026-01-04 21:16:24,0
Intel,nxqyl2l,"So we remove PC filters for “fairness”, but console aim assist stays borderline aimbot, Cronus is everywhere, and somehow it’s still the PC players’ fault. Crossplay didn’t expose PC advantages, it exposed hypocrisy. If you really care about fair play, start by nerfing consoles.",pcmasterrace,2026-01-05 02:51:19,0
Intel,nxrdjou,Lets keep it a buck arc raiders is garbage and more garbage then that is being restricted to make the console plebs happy,pcmasterrace,2026-01-05 04:16:27,0
Intel,nxms7hb,This post is so lazy it should just be removed.,pcmasterrace,2026-01-04 14:51:47,-1
Intel,nxmedpk,"I used to use filters to play CSGO, I could see through smokes and never got banned, people raged in every game",pcmasterrace,2026-01-04 13:30:48,-44
Intel,nxov8lw,"Because whoever threw that image together is lazy as shit.  The left is clearly screenshotted from a YouTube video, you can see the cursor in the bottom left. And the other is probably just some random screenshot they found.",pcmasterrace,2026-01-04 20:37:37,498
Intel,nxmojpr,Probably because it doesn't do them justice to do the same scene.,pcmasterrace,2026-01-04 14:31:20,478
Intel,nxmtwws,Different locations wouldn't be too bad. Different maps is stupid.,pcmasterrace,2026-01-04 15:01:01,41
Intel,nxovesw,Also the screenshot depicting NVIDIA filters isn’t using NVIDIA filters at all. It’s using gamma correction from NVIDIA control panel.,pcmasterrace,2026-01-04 20:38:25,26
Intel,nxpumhj,"Illustrative purposes. It isn't showing apples to apples, but someone who hasn't played the game wouldn't know that. They *would* know, just by looking at the two photos and seeing one is brighter than the other, roughly what the problem and solution were.",pcmasterrace,2026-01-04 23:25:07,-5
Intel,nxmgo5c,That would be fair as you can do that with a console too.,pcmasterrace,2026-01-04 13:45:06,397
Intel,nxmo0pa,My monitor has a mode that lightens shadows for fps games,pcmasterrace,2026-01-04 14:28:20,46
Intel,nxmrbmk,"Yeah in nvcp put gamma 1.20, saturation 75%, contrast 52% and you’re good. Thats my Tarkovs night raid settings because im on oled (all black) , but this game doesnt need that lmao.",pcmasterrace,2026-01-04 14:46:54,5
Intel,nxp0c36,"I have an LG, Gamer 1 is my regular preset, Gamer 2 is my *I need to actually see shit* preset, Gamma 1, full black stabilizer. In EFT, notorious for super shitty color grading that melds people with objects borders, it's a godsend. I also activate it in ArmA Reforger during night if I don't have NVGs.",pcmasterrace,2026-01-04 21:01:23,1
Intel,nxp93w2,Yeah filters can let you do all sorts of shit. But all you need is gamma/contrast. Pretty much the same shit.,pcmasterrace,2026-01-04 21:42:06,1
Intel,nxpud8z,"The Nvidia filters are really easy to swap between, there used to be a filter setup you could do in Counter Strike that would severely limit the effect of smokes, so you can flip between 2 filters when a smoke lands in front of you for example.   I don’t think it was what most people were using the filters for, but definitely would have an impact",pcmasterrace,2026-01-04 23:23:51,1
Intel,nxmp4jj,It’s not the same thing at all. It will still have the same ratio of light to dark between surfaces,pcmasterrace,2026-01-04 14:34:36,-13
Intel,nxrl1ql,If you say its the same you didnt use filters,pcmasterrace,2026-01-05 05:00:41,1
Intel,nxmk0dl,"Did they only disable the color Filters or also the HDR one? With the ingame/normal Windows Auto HDR the Game Looks Like Shit one my end,even though i calibrated my Monitor. Would be a shame.",pcmasterrace,2026-01-04 14:04:52,17
Intel,nxmsc8t,Yeah this really only hurts more casual players. Hardcore players will just use their monitor settings but more casual players might not want to change their monitor settings solely for one game.,pcmasterrace,2026-01-04 14:52:30,9
Intel,nxov92n,It's pretty funny because Nvidia control panel can change brightness and gamma anyway and isn't blockable 😂,pcmasterrace,2026-01-04 20:37:40,7
Intel,nxmeb3z,Pc players will just do it the old fashioned way to adjust thier monitor settings,pcmasterrace,2026-01-04 13:30:21,326
Intel,nxmiscc,And here I am with an OLED seeing absolutely nothing the moment a corner doesn't have a light,pcmasterrace,2026-01-04 13:57:33,33
Intel,nxmjobv,"my monitor has a cross hair display. Can't take that away, can you, there Jensen Huang?",pcmasterrace,2026-01-04 14:02:52,12
Intel,nxmeqnl,Did you forget to switch accounts😭,pcmasterrace,2026-01-04 13:33:06,20
Intel,nxmf2ob,"Not really, everyone can still use their Nvidia CP to turn up gamma and brightness, AMD users can easily use windows gamma and adrenalin filters.   Still a good step in the right direction, the filters should never have been allowed.",pcmasterrace,2026-01-04 13:35:12,40
Intel,nxmryt8,"Damn We got Twitter-style OF bots now. At least, this is my first time spotting one esp. bc of that NSFW pfp.",pcmasterrace,2026-01-04 14:50:27,5
Intel,nxml5iu,PC players still have the other 9/10 advantages,pcmasterrace,2026-01-04 14:11:43,3
Intel,nxmqw9c,> this evens things out.  ...and also shows that your HW isn't yours anymore.,pcmasterrace,2026-01-04 14:44:33,2
Intel,nxmnazm,Mouse and keyboard is already an advantage,pcmasterrace,2026-01-04 14:24:20,3
Intel,nxmhzoc,It's called masterrace for a reason ;-),pcmasterrace,2026-01-04 13:52:53,4
Intel,nxmti8j,Mouse/kb already gives PC a clear advantage.,pcmasterrace,2026-01-04 14:58:50,1
Intel,nxox9rk,"No its not even, i bet someone plays on a toaster, lets cap fps at 7. Also remove aim assist",pcmasterrace,2026-01-04 20:47:05,1
Intel,nxozs5k,I don't play arc but I've never seen anyone I know use any filter in any game despite a potential advantage. Some people surely do but I don't think that's a majority. Wanting your game to look like shit only to gain a little advantage isn't worth it in my opinion.,pcmasterrace,2026-01-04 20:58:47,1
Intel,nxq0elv,But console players have Aim Assist.,pcmasterrace,2026-01-04 23:54:07,1
Intel,nxmj0ls,"""Clear advantage"" like TVs don't have brightness/gamma settings too 😭  I promise you nothing has changed except that it's more inconvenient now, which is good, but it's 100% on Embark to solve this. I haven't played Rust in like a decade, but I remember at one point at least they would sample the darkest areas on the screen and make them pitch black no matter the brightness in order to fight this tactic.",pcmasterrace,2026-01-04 13:58:57,-5
Intel,nxmwapn,"PC players have been using 3rd party software like reshade for a long time now, this change hardly matters.",pcmasterrace,2026-01-04 15:13:35,0
Intel,nxmguoo,Players on SCUM using filters have extreme night base raiding advantage. Made that game completely unplayable for me.,pcmasterrace,2026-01-04 13:46:11,-5
Intel,nxmfmcs,They should cap PC players to 120FPS in Fortnite so they don’t get unfair advantage.,pcmasterrace,2026-01-04 13:38:39,-30
Intel,nxp8dyy,This did seem to disable RTX HDR. Everything looked extremely washed out after this update. I wasn’t sure what was going on.,pcmasterrace,2026-01-04 21:38:48,2
Intel,nxmi3af,"I don't really play shooters, but I was under the impression that Mouse/Keyboard is usually ""better"" than controllers with aim assist.    Is that not the case here?",pcmasterrace,2026-01-04 13:53:27,59
Intel,nxmidkv,"I get where you're coming from with this but let's be real, even ~~id~~ if you're not that good at shooters it's still way easier to aim with the mouse than even with aim assisted controller. And I'm not saying that in a ""PC players are playing an easier game"" way, on the contrary believe it raises the skill ceiling and it's why all of the serious shooter esports are on PC.",pcmasterrace,2026-01-04 13:55:09,0
Intel,nxmkxg9,![gif](giphy|JjiieDMHZ6pEI),pcmasterrace,2026-01-04 14:10:21,1
Intel,nxouh6q,I don't think the aim assist in arc raiders is that strong compared to games like apex or cod.,pcmasterrace,2026-01-04 20:34:07,1
Intel,nxmnnzt,If you can't win against controller players without a night vision shader filter you might as well stop playing pvp games lmao,pcmasterrace,2026-01-04 14:26:21,1
Intel,nxmm2ln,"If you want to absolutely kill a console game then yeah, disable aim assist.",pcmasterrace,2026-01-04 14:17:10,0
Intel,nxmtrbz,Aim assist is no where near as bad as you all keep making it out to be. I'm on pc btw,pcmasterrace,2026-01-04 15:00:11,-3
Intel,nxmo3xh,Same...it actually works pretty well in arma reforger too...,pcmasterrace,2026-01-04 14:28:51,1
Intel,nxmlj18,"That shit actually works?! I just never touched those settings cause they always looked too stupid to use. Definitely won't be using them now though, I'd rather have a skill issue and lose than be cheating and still lose.",pcmasterrace,2026-01-04 14:13:57,1
Intel,nxqvquv,"You could do this on consoles too, hdcp has been broken for years and they can't ensure your monitor gets what the console wants you to see.",pcmasterrace,2026-01-05 02:36:18,1
Intel,nxp1ha7,"Blame devs, realistic lighting/fog/etc does not make a shooter fun.",pcmasterrace,2026-01-04 21:06:45,-8
Intel,nxmlpgh,\>Buy a 5090 for the best performance possible   \> Turn the graphics to low,pcmasterrace,2026-01-04 14:15:01,10
Intel,nxmi76x,"I know people who are in the lowest Counter-Strike leagues that do that. They want the FPS for faster reaction and I'm like... dude, the weakest link in your setup is you.",pcmasterrace,2026-01-04 13:54:05,16
Intel,nxpp393,Or why tf they care about how people play Arc Raiders?   Stay in your fucking lane.,pcmasterrace,2026-01-04 22:57:35,2
Intel,nxmi356,At this point? Nvidia has been steadily fucking everyone over atleast for the last decade. Only thing they don't hate is their wallet,pcmasterrace,2026-01-04 13:53:25,8
Intel,nxmkyik,For once they fucked with people who deserve it.,pcmasterrace,2026-01-04 14:10:32,-2
Intel,nxmelk8,"That's because they were adding filters that improved visibility at night, not the one to make the game ""prettier""",pcmasterrace,2026-01-04 13:32:11,33
Intel,nxmfbul,"Filters are used to degrade graphics quality for getting advantage in competitive games. Oldeest example, i guess, in quake 2/3 people were making location textures just a white material to make enemy model seen more easily",pcmasterrace,2026-01-04 13:36:49,5
Intel,nxmdqx0,They can also ban reshade. Reshade was super popular in Ark Survival Evolved as it pretty much gave you wallhacks until it eventually got banned,pcmasterrace,2026-01-04 13:26:45,3
Intel,nxme0x8,Isnt reshade already banned?,pcmasterrace,2026-01-04 13:28:33,1
Intel,nxmf0vc,Thats why we can’t have nice things.,pcmasterrace,2026-01-04 13:34:52,30
Intel,nxmhoit,Except that you can't. There is no setting to do that.,pcmasterrace,2026-01-04 13:51:06,9
Intel,nxmpj22,Either this or they’re just lazy,pcmasterrace,2026-01-04 14:36:54,104
Intel,nxr2xey,Which can't be stopped can it?,pcmasterrace,2026-01-05 03:15:20,5
Intel,nxrmbv9,"It is using the filters, you can't take a screenshot with gamma correction from control panel since that only changes visually on your monitor and it's not really a ""filter"" just for that game.   Also because you can't really achieve that much color, visibility and sharpened image with just the NVIDIA control panel settings, game still looks kinda blurry after all.  https://preview.redd.it/5bd2uk8mrgbg1.png?width=2128&format=png&auto=webp&s=a06ff7a46677fe725f48a77d58be141b63dc1e43",pcmasterrace,2026-01-05 05:09:11,2
Intel,nxprsd0,It just reduces contrast,pcmasterrace,2026-01-04 23:11:05,14
Intel,nxqsj5i,Yup i use that for arc its a game changer for dark areas forsure,pcmasterrace,2026-01-05 02:18:58,1
Intel,nxoxzqy,"Im on an oled playing tarkov as well. Thanks for this. You got good settings for day time? I was using 1.5 gamma and 60% contrast. It hurt my eyes how bright it was with the snow, hell the whole game was to bright. Now playing witj 1.3 gamma and 55% contrast but I wonder what you are using.",pcmasterrace,2026-01-04 20:50:26,6
Intel,nxpj6k5,do you play Battlefield 6? wondering how those settings impact the game. with my OLED some enemies can be right in front of me and I cant see em,pcmasterrace,2026-01-04 22:29:08,1
Intel,nxrmics,"true lol, it's not even close",pcmasterrace,2026-01-05 05:10:25,1
Intel,nxpz14g,"Nah, making exploits less convenient is better than just letting shit happen.",pcmasterrace,2026-01-04 23:47:14,0
Intel,nxqadan,If they were adjusting shaders then they aren't casuals. That's sweaty.,pcmasterrace,2026-01-05 00:42:44,0
Intel,nxmj4nk,"Removing bloom, light shafts and turning down shadows, turn off foliage ,running lower graphics. We had that discussion in 2013 (DayZ), Example:  https://preview.redd.it/1lw3zxej9cbg1.jpeg?width=1910&format=pjpg&auto=webp&s=d5d41c0ae2f0792a7fe576849c230c441576d8ab",pcmasterrace,2026-01-04 13:59:38,332
Intel,nxmk02r,Monitor settings can't do nearly as much as those nvidia filters,pcmasterrace,2026-01-04 14:04:49,14
Intel,nxms1jh,"It creates a barrier of entry and an extra obstacle instead of hotkey enable. The filters also allow way more granular adjustments than you could ever get from changing monitor settings. That is before we even get into the ability to blow out certain colors so they are super noticeable, something you can't really replicate with monitor settings at all.",pcmasterrace,2026-01-04 14:50:53,2
Intel,nxn4zjz,"Check out BurntPeanuts brightness, its literally daylight for him when he is streaming.",pcmasterrace,2026-01-04 15:56:14,1
Intel,nxmkdtr,"my monitor's HDR keeps messing with me, so i have to constantly switch on the flashlight and off again for it to recover...",pcmasterrace,2026-01-04 14:07:08,6
Intel,nxmo3y4,Mine has an AI night mode that I wont use 👀,pcmasterrace,2026-01-04 14:28:51,-1
Intel,nxmkind,same... he he he he....,pcmasterrace,2026-01-04 14:07:55,1
Intel,nxmstb3,This account appears to be a bot from just reading their comment history lol.,pcmasterrace,2026-01-04 14:55:05,8
Intel,nxmhc6s,"HAAAAANK, don't abbreviate ~~Cyberpunk~~ Control Panel",pcmasterrace,2026-01-04 13:49:07,100
Intel,nxmfoob,Nvidia has CP?! damn they are really trying to expand into some weird industriee,pcmasterrace,2026-01-04 13:39:04,58
Intel,nxmorss,Nvidia what now?,pcmasterrace,2026-01-04 14:32:36,3
Intel,nxmlmr0,Adrenaline isn't nearly as good as NVIDIA filters. Not by a longshot.,pcmasterrace,2026-01-04 14:14:34,3
Intel,nxmi1r0,A step in the right direction? This is like dumping water out of a sinking boat with a cup. You either fix the hole or you abandon the ship. This fix doesn't do a damn thing,pcmasterrace,2026-01-04 13:53:12,-3
Intel,nxmteac,Thats fine then because filters are software,pcmasterrace,2026-01-04 14:58:15,0
Intel,nxp3qen,Not if you're like me and are trash,pcmasterrace,2026-01-04 21:17:14,5
Intel,nxmsbiw,"Higher fps and faster input too, and invtenrory management.",pcmasterrace,2026-01-04 14:52:24,2
Intel,nxmkfhb,stoopid plebs...,pcmasterrace,2026-01-04 14:07:24,1
Intel,nxmhmfr,Anyone playing at a level where FPS matters has a PC or prefers console anyways.,pcmasterrace,2026-01-04 13:50:46,5
Intel,nxqu2hj,Still working for me,pcmasterrace,2026-01-05 02:27:13,1
Intel,nxmiqou,"Depends on the game, some games has insane aim assist like cod and apex, but in arc raiders the aim assist isn't as strong as those two titles. So MnK is better.",pcmasterrace,2026-01-04 13:57:17,111
Intel,nxmjvzf,"In competitive games with controller support and significant aim assist like COD, pros play on PC to take advantage of more indepth game settings but use controllers to take advantage of aim assist.   Then again there are many implementations of controller aim assist but COD aim assist is the most egregious of the lot.",pcmasterrace,2026-01-04 14:04:08,28
Intel,nxn2qm1,"It depends on how strong the aim assist is.  There are plenty of videos online of games like COD where the overturned aim assist is accurately tracking targets with zero input from the controller. That really pisses off MKB players, because if we take our hand off the mouse the game doesn't magically aim for us. It's especially aggregious in close quarters situations where the controller player can't flick to peripheral targets, so the aim assist just flicks for them with or without input. As a MKB player you can understand that they can't flick, which isn't fair, but it still feels like you're getting beaten by a borderline aimbot.  There's also a big question here about the intentionality of devs in tuning how strong their aim assist is going to be. If 2/3 of your playerbase is using controllers, wouldn't you want to make aim assist stronger to make them happy?  Even outside of the context of cross-play, aim assist has become so ubiquitous in console shooters that I genuinely think that some console players have never played without it. So, they may not fully appreciate how much the aim assist is helping them. This definitely has an effect of flattening the aiming skill curve in a way that isn't present in PC shooters that don't have aim assist.  But at the end of the day it becomes yet another reason why I don't even know why I still bother playing competitive shooters. I can play co-op shooters and friend-slop and single-player games and never have to worry about hackers or normalized toxicity or over-tuned aim assist.",pcmasterrace,2026-01-04 15:45:35,8
Intel,nxmindf,I think it also depends on the aggressiveness of the aim assist. But gamers will always find a way to attribute their own incompetence to an advantage on the enemies side in some way.,pcmasterrace,2026-01-04 13:56:45,23
Intel,nxml8zz,"Every time this comes up I like to remind people that if the game offers aim assist on controller and has a pro scene, nearly every pro competing is using a controller.  Modern high TTK shooters involve a lot of player movement tracking which mouse isn’t good at.",pcmasterrace,2026-01-04 14:12:18,6
Intel,nxmj144,It used to be until devs started putting weird mouse acceleration and curves in to “make it fair”. That plus making aim assist damn near magnetic for controllers makes playing on a gamepad at an advantage.,pcmasterrace,2026-01-04 13:59:02,5
Intel,nxmlyxh,"I'm going to add another opinion on top of what others already told you. While mouse and keyboard is undoubtedly the better control scheme, the aim assist that comes with controllers does a lot to smooth the curve. It obviously depends on how aggressive it is on a per game basis but a mediocre controller gamer (plus aim assist) is going to have a better time than a mediocre mouse and keyboard player, because the latter will have to (typically) play against players with an higher skill ceiling.",pcmasterrace,2026-01-04 14:16:34,3
Intel,nxmldu4,"Not really the case these days, depending on game. Devs crank that shit to 11 on mass appeal games.",pcmasterrace,2026-01-04 14:13:06,2
Intel,nxp3i6i,"Not having to spend so much brainpower aiming is a massive advantage.  If I didn't have to spend as much effort predicting where a player was going to be and moving my mouse there, I could instead focus more on movement, positioning, etc.  Plus aim assist has a massive advantage in regards to response time and when visual clarity is low.  It's why rapid ad movement doesn't work against controller players but doesn't again PC players.  It's why many pros in certain games like Apex have moved to controller where they can lock onto players behind smoke.",pcmasterrace,2026-01-04 21:16:11,1
Intel,nxq9rbc,I would say its more precise than controller with aim assist but in my experience you can get just as fast and precise with a controller and no aim assist.,pcmasterrace,2026-01-05 00:39:40,1
Intel,nxmji10,"some games have stupidly op aim assist, for example in fortnite you can lock onto someone thats behind a wall and you can't see yet and basically can't miss with automatic weapons",pcmasterrace,2026-01-04 14:01:51,1
Intel,nxmj0gs,"I was half kidding. I know that MBK has a huge advantage vs no aim assist.  But as for which one is better, it depends on the game and/or situation. Some games crank up the aim assist higher than others.  It even varies by season, when there is an exodus of console players leaving the game because they are getting clappped they will crank up aim assist, sometimes going too far and causing PC players to quit in frustration.",pcmasterrace,2026-01-04 13:58:55,0
Intel,nxovu7c,"It is, people in this sub just love to bitch about this topic.  Nobody on console with aim assist is running train on competent pc players.",pcmasterrace,2026-01-04 20:40:25,0
Intel,nxmt1aj,"Aim assist is an advantage given only to console players. Even if it was small, it is not, it's still unfair. If they talk about fair then they should remove it as well.  Or do something like a game taking the competitive aspect seriously like valorant and stop cross play. Counter strike is not even on consoles. I'm sure there are more examples those I just know.   To your question. There are variables. Not all pc gamers are good with aim and movement. Not all games have the same aim assist. The impact is different...etc But it is still enough to at least try to level the playing field. That alone tells you how much it is unfair.",pcmasterrace,2026-01-04 14:56:17,0
Intel,nxmizx0,"Simply look at any tournament shooter game where mouse has to compete with auto aim controllers, it's dominated by controller players, I understand cod and arc are different but this has been widely established  for normal people to be happy playing shooter games on controllers, it just works out to an unfair amount of autoaim",pcmasterrace,2026-01-04 13:58:50,16
Intel,nxmv3rk,"Just no, AA on Apex/CoD is straight up equivalent to some soft aimlock on pc a cheaterd could use. I mean, when theres is more videos on yt on how to set up your aim assist than there is of actual gameplay.. Maybe there is a problem   And i'm champ on R6 PC/Iri since MW2 9PC KBM too)",pcmasterrace,2026-01-04 15:07:22,5
Intel,nxp5oea,">if you're not that good at shooters it's still way easier to aim with the mouse than even with aim assisted controller.  This is beyond false, I have to believe it's a joke.  No, it's very widely recognized that M&K has a lower floor and higher ceiling.  All the serious shooters are on PC sure but all the pros play with controller.  Why?  Having the aiming part done for you frees up your brain to do other things, like focusing on positioning, movement, etc.  Plus there are certain advantages to auto-aim a M&K player can never compete with.  Being able to auto-aim through visual clutter is insane, for a M&K player it increasing the processing time to identify and move over a target.  Auto-aim is much better at tracking as well.  An algorithm's reaction time is much better than a humans, it's simply unfair.",pcmasterrace,2026-01-04 21:26:16,2
Intel,nxp4dk8,"Oh, like Siege?",pcmasterrace,2026-01-04 21:20:13,3
Intel,nxn3b8i,"It's actually MUCH worse than you think. Some of us used to game back when aim assist didn't exist and with zero aim assist, controllers were not viable AT ALL. The difference was a joke.",pcmasterrace,2026-01-04 15:48:18,9
Intel,nxmlu8i,Yeah lol. On night maps you’re essentially granted god made as a centre rectangle on your screen has day brightness so you can pick headshots very accurately whilst everyone else is just spraying in the dark.,pcmasterrace,2026-01-04 14:15:47,3
Intel,nxp5cb8,"Often lumen seems underbaked with all the artifacts it can cause, even in less photorealistic games.",pcmasterrace,2026-01-04 21:24:43,2
Intel,nxp89uw,"I just finished Control and it absolutely does. I built my PC to enjoy games at their highest fidelity, not to lower everything to their minimum to get the maximum advantage.",pcmasterrace,2026-01-04 21:38:17,2
Intel,nxpjcmw,>>boot a multiplayer game to sit and stare at the roses   Turning graphics down to gain an advantage has always been a thing in games with pvp,pcmasterrace,2026-01-04 22:29:56,2
Intel,nxmms3i,especially in counter strike where low setting is making you harder to spot enemy since everything is blocky including the model and the game without anti aliasing is terrible even though it is competitive gaming,pcmasterrace,2026-01-04 14:21:19,3
Intel,nxmj6ci,Yeah but now they dont even talk to you untill you spent your first billion on a datacenter,pcmasterrace,2026-01-04 13:59:54,0
Intel,nxmlxiw,A fiew years ago i would ask why you hate nvidia so much but.. yeah,pcmasterrace,2026-01-04 14:16:20,2
Intel,nxme46o,inis work to this day you can disabled pretty much everything,pcmasterrace,2026-01-04 13:29:07,2
Intel,nxmnx1o,"Doesn't even remove it, just changes it a bit as using AMD/NVIDIA control panel still works to change this a good bit.   It's not as powerful as filters, but it does work. I had to do it going from a TN LCD to an OLED.   The blacks were so deep I couldn't see anything in even a regular room on Buried City with the OLED, when with the TN LCD it was much more manageable.",pcmasterrace,2026-01-04 14:27:46,27
Intel,nxmpzd1,"Yes they are. Filters are merely pre-configured settings for things like brightness, contrast, and gamma. Those settings are still there, by the way. Also, anyone with a 10-year-old gaming monitor has access to the same in a menu somewhere. You can mimic the same with manual settings on most newer televisions.  Ultimately, this isn't as big of a thing as console players seem to think it is, and this move stops nothing but the whining.",pcmasterrace,2026-01-04 14:39:29,4
Intel,nxmsjmc,"There is options like ""shadow balance"" or ""night vision"" in a lot of new gaming monitors, they basically brighten shadows without changing overall gamma much.  Also, I am pretty confident that it's still possible to disable filters using nvidia inspector.",pcmasterrace,2026-01-04 14:53:37,1
Intel,nxmmxar,Awwww poor plebs,pcmasterrace,2026-01-04 14:22:09,-40
Intel,nxp4ldp,"Ive got shadow boost on monitor - “level 3” little bit helps, gamma 1.17%, saturation 65%, contrast 52%, brightness default, in game shadows ultra and grass shadows on turns down that brightness exposure from snow.",pcmasterrace,2026-01-04 21:21:13,0
Intel,nxqxs1m,Huh? It's just an NVIDIA feature that allows you to tune your graphics to your liking. I use it in a lot of games. It can really enhance the graphics of your game or tune the colors to something more to your liking.,pcmasterrace,2026-01-05 02:47:09,3
Intel,nxmjm43,Hated this in games like arms and squad.,pcmasterrace,2026-01-04 14:02:30,133
Intel,nxmkokg,"Yeah this has always been the way for competitive FPS, which frankly sux of course cause then we're playing a lesser, shittier looking game for more FPS, higher visibility, etc.  I stopped doing these kinds of things for better or worse I'd rather play the game in it's full graphics glory than to get an advantage. I used to min-max more, play lots of PvP games and take it all very seriously, now I play to have fun again.",pcmasterrace,2026-01-04 14:08:54,39
Intel,nxmo9ev,"Same thing with rust. The more potato your graphics are the better your advantage, screen brightness too as it’s pitch black at night",pcmasterrace,2026-01-04 14:29:43,4
Intel,nxmrcje,"Homie, that's HLL",pcmasterrace,2026-01-04 14:47:02,4
Intel,nxnkczi,They were literally deleting game files so that grass wouldn't render.,pcmasterrace,2026-01-04 17:07:32,2
Intel,nxmtyon,I remember doing similar things as early as Call of Duty 2 in 2005. You could force DX7 and see people through foliage.,pcmasterrace,2026-01-04 15:01:16,1
Intel,nxqhjab,Also pubg mobile,pcmasterrace,2026-01-05 01:20:44,1
Intel,nxmpeej,"Yeah, filters were a problem in counter strike for a while until they disabled them there too. It’s unquestionably a significant advantage and the scope goes far beyond anything brightness/gamma settings can do. People claiming otherwise just don’t have first hand experience of the possibilities",pcmasterrace,2026-01-04 14:36:10,4
Intel,nxmubli,"My monitor does just as well as Nvidia filters and takes me about 5 seconds to toggle. Easily good enough for DayZ, and I don't even bother for Arc because it's so rarely an issue.",pcmasterrace,2026-01-04 15:03:13,2
Intel,nxmkuvq,Any kind of auto HDR is stupid and whoever invented it should have their fingernails pulled till they admit it was just for advertising.,pcmasterrace,2026-01-04 14:09:57,13
Intel,nxqjap2,Yep,pcmasterrace,2026-01-05 01:30:12,1
Intel,nxmid8q,"I mean they are moving towards AI, didnt realize they meant Grok specifically",pcmasterrace,2026-01-04 13:55:06,19
Intel,nxmtjl5,No one said they were.,pcmasterrace,2026-01-04 14:59:01,0
Intel,nxq02h3,"There is no way to ""fix the hole"" though. PCs inherently give far more control to the user about it's settings than consoles. Nothing the game can do will ever be able to ""fix the issue""",pcmasterrace,2026-01-04 23:52:26,1
Intel,nxmu6sd,Seems to be good enough for most people.,pcmasterrace,2026-01-04 15:02:30,0
Intel,nxmk6we,"Although controllers do have minor aim assistance, On pc you can do so much more faster since you have way more buttons that are easily accessible.  I don't think aim assist is as insane as people say and I'm on console.",pcmasterrace,2026-01-04 14:05:59,-25
Intel,nxmvi7o,Black Ops 7 aim assist on release was just a full blown aimbot in disguise. Saw some clips from console after they turned it down and it's just hilarious.,pcmasterrace,2026-01-04 15:09:27,9
Intel,nxmnizw,Console players play with literal aiming cheats and you say its the pc players who are wrong? Shooters with aim assist should absolutely not have crossplay enabled.,pcmasterrace,2026-01-04 14:25:35,5
Intel,nxp44im,"It's not that mouse isn't good at tracking, it's that human response time dictates that you aren't going to have perfect tracing if the target knows what they are doing.  AD spam will always be effective again mouse because of human reaction time unless the game incorporates inertia.  Controller is much much worse than mouse and keyboard without aim assist in terms of tracking.",pcmasterrace,2026-01-04 21:19:04,3
Intel,nxr38kw,"If you use a controller on PC, do you get the same aim assist as console folks?",pcmasterrace,2026-01-05 03:17:05,1
Intel,nxpocya,"> All the serious shooters are on PC sure but all the pros play with controller.  Lmao what. I'm talking about Counter-Strike and Valorant. Even Siege and Fortnite pros play with M+KB.  > An algorithm's reaction time is much better than a humans, it's simply unfair.   Maybe it depends on how the aim assist is tuned/calibrated, all I know is that every time I try to play a shooter game with controller on PC, even with aim assist I get too annoyed after like 10 minutes and go back to using mouse and keyboard.",pcmasterrace,2026-01-04 22:53:57,1
Intel,nxnck05,"No, it's really not. Maybe you're just getting old and slow. you'd be what, mid 40s to early 50s? I've played fps games that didn't have it, and ones with it, both were fine. Been a PC player most of my life too and was there for the switch over to cross-play becoming mainstream as well, I didn't start doing worse due to console players with aim assist.",pcmasterrace,2026-01-04 16:31:45,-5
Intel,nxn4az5,"Make sense now, same feeling when I found out about the door/out-of-bounds glitches, ""Eh I'll be throwing nades into locked rooms now.""",pcmasterrace,2026-01-04 15:53:01,1
Intel,nxpsz2d,single player emersion is not the same as pvp.,pcmasterrace,2026-01-04 23:16:56,1
Intel,nxmecwo,ini my beloved. Cba to play ASA with all the garbage thats on the screen,pcmasterrace,2026-01-04 13:30:40,1
Intel,nxmrdhq,Then you should make sure youre not running the oled in some weird profile. Because properly setup blacks carry so much detail on oled.,pcmasterrace,2026-01-04 14:47:11,13
Intel,nxmriga,"This relates to me so much, OLED looks great but fuck man. When sneaking around and can't use any torch or light source makes it really hard to see shit. But them blacks tho",pcmasterrace,2026-01-04 14:47:57,3
Intel,nxmp3vj,Does profile inspector work?,pcmasterrace,2026-01-04 14:34:30,2
Intel,nxnfbz3,It’s definitely for more things than just brightness/gamma/color sliders.   You can use it for all sorts of funky effects like comic book filters etc. Things that monitors don’t have built in settings for.,pcmasterrace,2026-01-04 16:44:29,0
Intel,nxmniwi,Are you lost?,pcmasterrace,2026-01-04 14:25:34,12
Intel,nxp5d3a,Thanks!! Will try this out tomorrow! Cheers,pcmasterrace,2026-01-04 21:24:49,0
Intel,nxmkryu,"Squad was so frustrating for a bit. ""This squad won't see me, I'm fully inside this big bush""  Immediately domed from 100 meters because their SL was playing on low settings and my bush looked like a singular stick",pcmasterrace,2026-01-04 14:09:27,139
Intel,nxmk7e6,War Thunder....,pcmasterrace,2026-01-04 14:06:04,24
Intel,nxmmvgd,My friend always did this in Rust. He would run everything as low as possible. I get it gives a advantage but id rather have the graphics...,pcmasterrace,2026-01-04 14:21:51,20
Intel,nxrowbv,Pubg had this real bad too in the early days,pcmasterrace,2026-01-05 05:27:02,1
Intel,nxmw9na,"The main reason I don't play online multiplayer games is for this type of thing.   I like to utilise my hardware by running my games at the highest detail I can.  When that is a disadvantage online,  there's no point in playing that game for me.",pcmasterrace,2026-01-04 15:13:26,8
Intel,nxmo91i,"nah, them fingernails should turn into fish hooks... with an unyielding itch to develop around the anal area...",pcmasterrace,2026-01-04 14:29:39,3
Intel,nxnd6h6,"Saying *""AMD users can easily use gamma & adrenalin filters""* easily implies that the same can still be achieved by AMD users. Which obviously is not the case.",pcmasterrace,2026-01-04 16:34:36,0
Intel,nxmlma4,It has almost fuck all to do with more buttons in most fps games that are crossplay. It has to do with the potential for faster more precise aiming/turning of a mouse.   But that doesn’t matter for the vast majority of players because the skill level for MKB to match controller players with high aim assist games is ridiculous. When you’re playing against top players there is no room for error while aim assist is incredibly forgiving.,pcmasterrace,2026-01-04 14:14:29,24
Intel,nxmoazx,"It truly depends on the game. There's games where aim assist is incredibly powerful on a controller, and very clearly is an advantage as a result. If it's done right, it shouldn't be an advantage.",pcmasterrace,2026-01-04 14:29:58,3
Intel,nxmntom,"Nope, in games like CoD after MW2019 it's a chore to play with MnK... Controller is actually much better as if you know how to abuse their aim assist and have the right settings, it's superior to MnK. The only FPS I'll use a controller on matter of fact. Tried it on Arc and it's negligible, MnK is still much better.",pcmasterrace,2026-01-04 14:27:14,4
Intel,nxmlg92,Try Apex out.,pcmasterrace,2026-01-04 14:13:30,6
Intel,nxmm72s,"Depends on the game, as they’re all gonna be at different levels. But it’s one of those things that once you’re used to it, you would probably be surprised how much of a difference it can actually make.",pcmasterrace,2026-01-04 14:17:53,2
Intel,nxmm7zo,"I played console for +20 years and switched to PC roughly 3 years ago. Aiming is soooo much easier and faster, even compared to a fully optimized Xbox elite 2 controller setup (that thing was nuts).  Left hand on keyboard is still struggling but it's fairly easy to compensate with a mouse with a few more buttons.",pcmasterrace,2026-01-04 14:18:02,1
Intel,nxr7azd,Wouldn't a pc player using controller have the same aim assist as console players?,pcmasterrace,2026-01-05 03:39:55,1
Intel,nxmo4b9,"If your aim with a mouse is worse than someones aim with a controller, maybe you‘re just not good enough. You have a massive accuracy advantage with a mouse if you know how to use it.",pcmasterrace,2026-01-04 14:28:55,-9
Intel,nxp712g,"Considering that the entire COD and Apex pro scenes use controller, I think he has a point.  Not having to think about aiming frees your brain up to focus on other things.  Aim assist doesn't have issues with visual clutter, when manually aiming though, it directly increasing your target acquisition time.  It's also impossible to compete with the reaction time of aim assist in regard to tracking.  It can just lock onto a target far better than any human can.  It's why M&K players on Halo Infinite have a much much smaller perfect rate than controller players.",pcmasterrace,2026-01-04 21:32:32,5
Intel,nxp7j89,"That doesn't really track modern reality where developers have been making aim assist stronger and stronger.  When games are actively nerfing aim assist and it still manages to remain the most popular input method at the highest level of play (with very clear statistical advantage), then something has gone wrong.",pcmasterrace,2026-01-04 21:34:52,2
Intel,nxpiz4c,"FPS like cod have crazy aim assist with rotational input INCREASING your aim assist bubble so as you take gunfights you control yourself with your LS and let aim assist do its thing. I know I’ve gotten old (34) but I’ve played majority of my life on MnK, swapping to controller increased my KDA and made aiming a joke with how strong aim assist is",pcmasterrace,2026-01-04 22:28:09,2
Intel,nxmfjo6,also works in ASA you just paste it into the console we have multiple copy&paste in our discord prepared + start up options,pcmasterrace,2026-01-04 13:38:11,1
Intel,nxq9acr,Yeah def sounds like a crushed blacks calibration issue,pcmasterrace,2026-01-05 00:37:17,2
Intel,nxnwcg4,"It's a ROG Swift 32"" 4k OLED (PG32UCDM), and I am just running Gaming HDR with 78 brightness and 63 contrast, any higher and everything looks super washed out.  I tried with both HDR on and off, and didn't seem much of a change TBH, but if you see anything I am doing thats weird or I can test to change, let me know.  I've spent a lot of time already sitting in the extracts trains on Buried City night raids messing with settings that keep it decent enough to see like I used to on my TN LCD, but if I push it any further Cold Snap and Day Time look like ass.",pcmasterrace,2026-01-04 18:02:33,1
Intel,nxnvner,"100% a love-hate relationship, but I do have to admit I love it more than I hate it.   Really only a bigger problem in Arc Raiders for me since pretty much every other game lets you do brightness.",pcmasterrace,2026-01-04 17:59:24,1
Intel,nxnwefs,I don't know what profile inspector is?,pcmasterrace,2026-01-04 18:02:48,1
Intel,nxng5by,"The thing people are complaining about is lightening dark shadows. That's done through standard sliders and yes, monitors and more modern televisions can do EXACTLY the same thing. I know because I hit that ""FPS Mode"" button on my monitor every time I log in and laugh at these misinformed posts thinking 1) They've stumbled upon a new problem, and 2) Thinking there's a ""fix.""",pcmasterrace,2026-01-04 16:48:14,0
Intel,nxmnmkb,"No, the plebs crying made this happen",pcmasterrace,2026-01-04 14:26:08,-25
Intel,nxp5hcl,On interchange I turn back gamma 1.22%-1.25%,pcmasterrace,2026-01-04 21:25:22,0
Intel,nxmoulf,All small bushes disappeared from enough distance. This was a problem in dayz,pcmasterrace,2026-01-04 14:33:03,39
Intel,nxmy7bj,"I remember something like this in battlefield, can't even remember which one. Had to play that game at bare minimum settings so I'd stop laying down in an ""open field""",pcmasterrace,2026-01-04 15:23:18,11
Intel,nxpqa34,"Even when playing at max foliage settings, tall grass would completely vanish after 100m in that game for me",pcmasterrace,2026-01-04 23:03:34,1
Intel,nxmqudm,Everyone does this. Especially in a game like rust. Where graphics are the least concern.,pcmasterrace,2026-01-04 14:44:16,8
Intel,nxpoqnz,"That works both ways. You're functionally saying - if I don't have an advantage, there's no point in playing. You want your 'better' system to give you more than others.",pcmasterrace,2026-01-04 22:55:51,-2
Intel,nxn33be,"Windows HDR being outright garbage and the lack of monitor/computer being able to talk to each other is diabolical. There is no reason any of this isn't controlled by the OS over HDMI/DP and it's all possible with current hardware or minimal changes to it, but no HDCP is more important.  My AVR turns on when the TV turns on, the volume is synced, so is the audio stream, so is just about everything else, but on PC we get video, and audio.",pcmasterrace,2026-01-04 15:47:15,2
Intel,nxmnb0h,It was during the Christmas train event in Apex that I truly realised how fast these console players could nail me with laser weapons. That is when I uninstalled.,pcmasterrace,2026-01-04 14:24:20,8
Intel,nxmrxy0,"I played FPS games at a top level and I still think that aim assist in some games is ridiculous and unfair. Like in CoD in close combat you sometimes genuinely can not tell the difference between aim assist and humanized aim bot.  So I guess that my aim is bad now too despite being paid to play shooters and being known specifically for my aim? It's not just me, Shroud has said the same thing about some shooters. I suppose that he is bad too?",pcmasterrace,2026-01-04 14:50:20,9
Intel,nxmohuo,"In apex controllers on close range are unbeatable. On long range mnk has the advantage but I dont give a shit, i dont want to play vs someone who aim locks on my head its stupid.",pcmasterrace,2026-01-04 14:31:03,2
Intel,nxraxnq,"In spellbreak, players with controllers had homing projectiles, while mouse and keyboard people had to predict where people would be 5s in advance to hit the same ability. Crossplay killed that game.",pcmasterrace,2026-01-05 04:00:52,1
Intel,nxqa5zf,It’s a third party nvidia driver plugin. It can sometimes give extra graphic options for games.,pcmasterrace,2026-01-05 00:41:42,1
Intel,nxnhiya,"I understand what people are complaining about.   I’m just pointing out that your statement of filters merely being preconfigured settings for brightness/color/etc, isn’t entirely correct in the context of being able to do more with filters than built in monitor settings.   The two sharing some settings doesn’t make them 1:1 comparable when one has a whole swath of additional settings.   For people who are just learning about Nvidia filters for the first time here, they should at least get a proper idea of what the system actually does.",pcmasterrace,2026-01-04 16:54:33,0
Intel,nxmo3qc,"""pleb"" being used a decade late isn't something i was expecting to see",pcmasterrace,2026-01-04 14:28:49,8
Intel,nxmnqf9,why are you stuck in 2010,pcmasterrace,2026-01-04 14:26:44,7
Intel,nxmrwia,bruh,pcmasterrace,2026-01-04 14:50:07,1
Intel,nxp84m7,Battlefield Vietnam had this.,pcmasterrace,2026-01-04 21:37:38,3
Intel,nxnjmgw,"This is what I like to call the cheater's fallacy:  ""I have to cheat because eVeRuOnE eLsE is cheating, and even if they aren't it's their own fault.""",pcmasterrace,2026-01-04 17:04:08,4
Intel,nxprjnd,"Not really, I want everyone to have the same advantage.    If one person runs a game at max settings and can't see somebody hiding in a bush across the map, the person playing on low settings also shouldn't be able to see that player either.    Having the player on low settings able to see the character because the bush doesn't render is an unfair advantage.    They should implement some sort of visibility thing for those types of situations,  if a character should be obstructed from view, but the thing obstructing them doesn't render due to graphic settings, maybe render the character with a lower opacity so it is also harder to see for those people without the actual visible obstruction?",pcmasterrace,2026-01-04 23:09:55,6
Intel,nxpf6iv,In my experience windows hdr is better than on playstation or switch. It's just more complicated to setup.,pcmasterrace,2026-01-04 22:10:15,1
Intel,nxmv0k4,"Controller players don't like being reminded the game is doing the heavy lifting for the hardest part of an FPS for them. I forget the exact statistic, but at one point it was something like 99% of all CoD pros were running controller even if they were also top ranked in other games running mnk. That should tell you something.",pcmasterrace,2026-01-04 15:06:53,13
Intel,nxmtylf,"There are ridiculously good players on k/m as well who make it seem as though they have aimbot, just because their aim is so good. I would say I‘m a top 30% FPS player at max regarding skill and I hardly ever complain about cheaters and never about aim assist because it‘s never been a big enough issue to complain about. Sure, if a pro player on k/m is playing against a pro player on controller in a game with significant aim assist, I understand that it can be an unfair advantage. 90% of players are not professional though and just like to whine about others being better.",pcmasterrace,2026-01-04 15:01:15,-3
Intel,nxr0ypx,"No idea then, I have a 9070XT so I am on AMD.",pcmasterrace,2026-01-05 03:04:19,1
Intel,nxqfze5,I don't think turning down the graphics is considered cheating. Just a choice between aesthetics and function... A lot of people choose function over aesthetics in competitive multiplayer games.,pcmasterrace,2026-01-05 01:12:19,8
Intel,nxoxpu2,That’s why games have to ban people and have rules. Glad NVDA did this honestly,pcmasterrace,2026-01-04 20:49:10,-3
Intel,nxpw9a6,"I understand that. But if you have a better FOV, or further view/less fog, a faster refresh rate etc. you have a distinct advantage over 'poorer' setups. Functionally you're saying you want everyone locked to minimum specs.  There will never be a 'fair' playing field unless you bump everyone down to Doom graphics.",pcmasterrace,2026-01-04 23:33:15,0
Intel,nxmw4se,"Yeah, a lot of them complaint for years and ultimately just switched to controller. KBM is by far the superior control mechanic, but the aim assist in that game so heavy that it no longer is. It is just a legal aimbot at that point.",pcmasterrace,2026-01-04 15:12:44,8
Intel,nxmxgxk,"I helped CB and ESL with demos of cheaters back in the day. It can be very difficult to spot an aimbotter, but that is mostly because cheats have become so good at emulating human aim.  However, when an aimbot becomes too good it becomes too mechanical and you can spot it. That's what the aim assist in CoD sometimes looks like. It is just a legal aimbot.  It's not about losing to an equally skilled player, it's about losing to a worse player who simply has the computer do all the work. KBM is by far the superior control mechanic. When you put top controllet players without aim assist against top KBM players, the KBM players are going to win 99% of the time. So I don't mind them leveling the playing field a tiny bit, but in CoD it has gotten so had that pretty much every top KBM player switched to controller. So the best players stopped using the best controls because aim assist on controller is just too broken.",pcmasterrace,2026-01-04 15:19:36,3
Intel,nxdmcsx,I mean it depends on what budget you have,pcmasterrace,2026-01-03 03:49:10,1
Intel,nxdnt4u,Nvidia Geforce RTX 5070 is the most popular card at the moment.  Great quality/price.,pcmasterrace,2026-01-03 03:58:14,1
Intel,nxdtki8,I don't know overly much about prices but probably $400-$600 AUD,pcmasterrace,2026-01-03 04:35:40,1
Intel,nxdqsi8,"Yup, got a 16gb 5060ti today and I'm very impressed. In real world gaming scenarios, it's seemingly superior to the 3080 I had while using half the power",pcmasterrace,2026-01-03 04:17:13,1
Intel,ntzzijx,"Each GPU chip is unique. Some are better at overclocking or running cooler, causing stock performance differences.  It's called the Silicon Lottery",pcmasterrace,2025-12-14 16:23:04,127
Intel,nu0fe6w,What did you need 6 B580s for? AI farm?,pcmasterrace,2025-12-14 17:43:21,25
Intel,nu2dgac,"The best card here is about 2.3% better than the worst. I'd chalk that up to run-to-run variance especially with only 3 runs each. I'd be willing to bet some of these individual cards had a similar spread in results.  This is entirely normal. No two cards are entirely identical. One might have a tiny bit better cooler mounting or need a tiny bit less voltage to hit top clocks, and that one will be a tiny bit faster.  If you were to overclock or undervolt and chase a record, that top performer is the first one I would try first, but they are all so close that any one of these could be the technically best one.",pcmasterrace,2025-12-14 23:32:02,8
Intel,nu3fwgv,Any idea on how the temps vary with those scores?,pcmasterrace,2025-12-15 03:18:01,2
Intel,nu29vqq,"That doesn't look like too much of a variance to me, I bet nvidia and amd cards would act the same. Not all chips are equal, the silicon lottery determines what you get",pcmasterrace,2025-12-14 23:12:14,1
Intel,nu314vs,Seems within margin of error,pcmasterrace,2025-12-15 01:47:02,1
Intel,nu3403p,pretty close. was thinking of a sparkle b580 as my upgrade as well.,pcmasterrace,2025-12-15 02:04:20,1
Intel,nu3unzs,"Neet data. Not many people test a bunch of the same card against itself.  I know with cpus they talk about the ""silicone lottery"" - extreeme overclockers would look for the best CPUs and get a few percent extra performance. I'd guess thats whats happening here.",pcmasterrace,2025-12-15 04:56:59,1
Intel,nu4bp7a,"Furmark is known for a *lot* of run to run variance. You'd need to do something like ten runs, discard the outliers, and average them.  It's not very useful as a benchmark, doesn't run long enough, which is why you won't see many folk using it as a benchmark: It's a stress test.",pcmasterrace,2025-12-15 07:18:04,1
Intel,nu55567,"So we're talking about about Standard deviation of 78.2 and mean of 9187.6, which means less than 1% deviation from the mean for about 84% percentile of the cards. While sample size isn't to big to draw any conclusions and tests should be more thorough and controlled, this is not high variance.",pcmasterrace,2025-12-15 12:00:51,1
Intel,nu72hkh,Average variation +/- 66 with maximum of -113  Max is a 1.22%,pcmasterrace,2025-12-15 18:18:55,1
Intel,nu2qn6m,thanks. now i can tell if mine is good or bad. wiat wait wait... no ya didn't benchamrk it on a core ultra 9 100series. thats a laptop only chip!,pcmasterrace,2025-12-15 00:43:53,0
Intel,nu2qri9,"NO ONE NOTICED THE CORE ULTRA 9 ""100"" SERIES?!?! I think he meant 200.",pcmasterrace,2025-12-15 00:44:34,-1
Intel,nu29ydv,Fucking ai people and their retarded local llm bs,pcmasterrace,2025-12-14 23:12:38,-15
Intel,nu1j49c,"And not just GPUs but CPUs, vehicle engines, electric heaters, gas boilers, light bulbs, batteries etc  Basically anything will have a “tolerance” where the “output” will vary between x and y and is not a constant.",pcmasterrace,2025-12-14 20:55:37,36
Intel,nu2h4v3,"and also if you get a higher tier gpu then youre guaranteed to get a really good chip out of this lottery, such as gigabyte aorus xtreme cards",pcmasterrace,2025-12-14 23:52:10,2
Intel,nu0gi0o,Yeah I was planning doing some local llm inference,pcmasterrace,2025-12-14 17:48:54,23
Intel,nu3u4ps,Good question.  I wonder if they are being thermally restrained or power limited.,pcmasterrace,2025-12-15 04:53:04,1
Intel,nu3inr7,Very good GPU IMO the sparkle one looks better and clocks higher than stock,pcmasterrace,2025-12-15 03:36:00,1
Intel,nu69g0s,Hmm ok while I still have the cards not deployed in anything what benchmarks should I do like what exact testing are y'all looking for,pcmasterrace,2025-12-15 15:58:05,1
Intel,nu2ughs,Yes I benchmarked it on my mini PC it is a mobile chip,pcmasterrace,2025-12-15 01:06:12,2
Intel,nu2d053,"What I do with my money and my hardware should not upset you. When people say ""AI people,"" we are referring to large corporations and data centers buying up all the GPUs and RAM and using the AI to ruin our lives even more. What **local LLMs** are doing is to bring that power back into our hands, the consumers. The everyday person. Would you like it so that all things to do with AI belong to Google and OpenAI, or would you not want a world where us, the consumers, can fight back with our own models?  if you dont know what to say about a matter its best to enlighten yourself before shitting on something",pcmasterrace,2025-12-14 23:29:30,14
Intel,nu4a1zc,"And also humans, it's called genetic lottery.... Usually pcmr members don't fare too well on that. 😥",pcmasterrace,2025-12-15 07:03:04,3
Intel,nu3qgkg,"Nvidia bins their chips into very tight bins. Board partners aren't even allowed to sell a chip as overclocked when Nvidia deems it to not be, so there's less of a lottery going on than there is in other places.  That being said, there will still be sample variance.",pcmasterrace,2025-12-15 04:27:29,1
Intel,nu2v9r3,"Why is this guy getting downvoted? Local LLM inference is really cool and you get full control over every part of it. Maybe people are thinking ""AI bad"" but the random hobbyist buying $2k of hardware is not the same as the large corporations buying the entire world's DRAM supply.",pcmasterrace,2025-12-15 01:11:11,41
Intel,nu1jruw,Why go for Intel and not AMD or the one who shall not be named? Is Intel best?,pcmasterrace,2025-12-14 20:58:53,-13
Intel,nu2zhyf,what?!? how did you fit the gpu?,pcmasterrace,2025-12-15 01:37:06,0
Intel,nu4z9h0,"Yep, they have 2 chip models with one permitting factory of and one not. And the ones permitting also tend to get you higher speeds  But for example if you buy an aorus card, chances are you can't go any higher because the chip couldn't make it to be an xtreme model. On the other hand, an xtreme card probably will go up a bit extra",pcmasterrace,2025-12-15 11:10:15,0
Intel,nu3z1yj,Nuance is too hard for most people.,pcmasterrace,2025-12-15 05:30:00,8
Intel,nu7sluk,Most pple dont care they are like robots them selves they hear ai and immediately down vote some pple even believe that someone buying a few GPUs directly means they are taking away from gamers people dont care as long they have something to hate on,pcmasterrace,2025-12-15 20:26:14,3
Intel,nu1pdng,"From what I saw when I was looking, you can get decent amounts of vram for not too bad, I grabbed an intel b580 for my home server cause it was mega cheap and it can rip transcodes pretty ez",pcmasterrace,2025-12-14 21:26:04,17
Intel,nu2d97v,as others have mentioned currently the best bang for buck is intel for vram and also i got all of these cards for super cheap around 200-240 each also i dont want to support nvidia at all they can go fuck themselves and as for amd there is very little llm support on their cards,pcmasterrace,2025-12-14 23:30:55,16
Intel,nu25qj2,"that is 6 b580 carts at 12GB each right now they are selling for like 260 USD each at some retailers so that is 1,560 USD all before tax that is.  And you get 72GB vram to use as long as you have a system for using this.  And 3060 12gb are going for about 300-350 and the 5060 ti 16gb is going for about  400-450  then the 9060xt 16gb is going for about 380-450.  so the 3060 12gb is out, it is way to much  Then lets say he got 5 9060 XT 16gb at 380 that's 1900 and that would be 80 GB of vram or 4 cards at 1520 with 64GB of vram.  Then their is the 5060 ti 16gb it would be even more.  With Vulkan API working with different LLM stuff it is easy to use just about all cards now, aside from the compile process if you want to get maximum performance. Other wise LMStudio will get you going with Vulkan fairly decently.  EDIT: My mistake you can get the B580 right now for 239.99 from B&H photo.",pcmasterrace,2025-12-14 22:49:18,8
Intel,nu26t3l,Nvidia tends to be the most performative for AI but also most expensive. Intel is the best bang for buck if you're going for high vram counts. It'll be slower but at least the model fits. Amd isn't really a player in the AI space yet,pcmasterrace,2025-12-14 22:55:08,6
Intel,nuge5qp,How about an EU chip since you declared people should be independent of american ones hmm,pcmasterrace,2025-12-17 03:54:47,1
Intel,nu349na,beelink gti 14ultra look it up it has a full pcie slot,pcmasterrace,2025-12-15 02:05:56,2
Intel,nu698w6,"It is hard to predict how chips will overclock. We've had generations where CPUs were apparently binned very conservatively, so even way down the range chips would overclock like crazy (Intel Core 2nd gen) or allow you to unlock whole extra fully functional cores (AMD Phenom triple cores), but we've also seen generations where most headroom was already 'priced in' (Intel 13th/14th gen) or arguably even exceeded. We've seen generations with more and less headroom on the GPU side as well.  That being said, now that consumer GPUs are competing with AI chips, they'll want to squeeze every inch out of each wafer, so I suspect they'll bin things as tightly as possible.",pcmasterrace,2025-12-15 15:57:08,2
Intel,nu6d5hz,"Yeah thats a fair point, some generations can be liklier to overclock than others",pcmasterrace,2025-12-15 16:16:03,1
Intel,nw0g76t,"> TLDR in CPU intensive games, if your CPU is maxed, it holds the GPU back with it, which leads to some pretty significant drops, but don't sweat it. A 5600/5600x is a suitable CPU upgrade and will very well balance the system  i'm using intel 225f to pair with the b580 and they work well",pcmasterrace,2025-12-26 11:58:13,2
Intel,nvxl4zm,so ur tldr is that if you get bottlenecked from ur cpu the gpu doesnt perform well? sorry for the sarcasm but who wouldv thought! merry christmas,pcmasterrace,2025-12-25 22:14:41,1
Intel,nw3o9mi,"Nice, the 12400f is definitely a solid pairing with the B580! Way better than my old 8400 for sure. How's it handling CPU heavy stuff like open world games? Been thinking about upgrading myself but trying to squeeze more life out of this setup first lol",pcmasterrace,2025-12-26 23:12:45,1
Intel,nw0j3gr,"Lmao ur right it's stupid But I was doing research for this specific combo and couldn't find much, so I wanted to post my findings. Also, this is a problem specifically with arc GPUs, where the card requires a lot of CPU overhead to work well. Officially intel says not to pair this card with less than 10th gen, but it works so oh well 🤷",pcmasterrace,2025-12-26 12:24:08,1
Intel,nw3puq8,>How's it handling CPU heavy stuff like open world games?  does assassin's creed shadows count as open world? i think it's not so cpu intensive though. 225f+b580 can produce 4x fps at 4k medium quality  https://preview.redd.it/3ilcoxnptm9g1.jpeg?width=3840&format=pjpg&auto=webp&s=61369f28fca9c392917690bc631e2c0ecb667faf,pcmasterrace,2025-12-26 23:22:13,2
Intel,nwbto0a,"Tengo el mismo micro el i5 8400 y me interesa la b580 lo recomendas? Tengo un core 225f nuevo, esperando a que pueda comprar las ddr5 para poder usarlo xd , pero me llama mucho la atención la b580",pcmasterrace,2025-12-28 07:13:50,2
Intel,nwt5h99,Ja es un scheda grafica muy potente. En mi ciudad costa €300 por el version steel legend. Lo siento mi español es una mierda 😅,pcmasterrace,2025-12-30 22:09:51,1
Intel,nx97l0z,"The way DayZ combatted this was putting a noise filter in low light, that way if you turned up gamma it would look like a sand storm.",pcmasterrace,2026-01-02 14:11:52,583
Intel,nx8pisn,">Using the Nvidia App filters, players could tweak the look of the game to make night scenes look significantly brighter. By changing contrast, brightness, colour and saturation values, PC players with Nvidia hardware were essentially making the darkness of night a non-factor, allowing them to see rival players much easier than the game naturally allows.  Pretty sure this is possible regardless of Nvidia settings. I'm not sure if it's still the case, but back in the day Rust would determine the dark parts of the screen and make them pitch black to account for people merely cranking their gamma.",pcmasterrace,2026-01-02 12:10:52,335
Intel,nx92y6i,"Anyone whose moderately determined will figure out how to get similar results by tweaking monitor and driver settings. Also, how does this game not have basic options for brightness/gamma calibration? Is that stuff not essential for OLED/HDR users?",pcmasterrace,2026-01-02 13:44:30,96
Intel,nx8x08i,Next they gonna ask monitor manufacturers to remove those filters?,pcmasterrace,2026-01-02 13:06:49,58
Intel,nx9r8f1,Too bad these gaming monitors like the ASUS rog ones have this exact filter built in and is undetectable. Just saw 10 vids of it on TikTok,pcmasterrace,2026-01-02 15:54:54,15
Intel,nx9fphu,"This is ignorance every single monitor and tv you can fix this setting the only good using the AMD and NVIDIA is you can do it per game, since all games have different color mapping, plus fix color settings is a most every monitor, tv doesn’t have the same screen for example oled, led, etc.",pcmasterrace,2026-01-02 14:57:15,17
Intel,nx9bwue,I just use it for rtx hdr. Still working as of 2 hours ago so not sure if there's an update for the app that stops it working I dont have yet?,pcmasterrace,2026-01-02 14:36:32,4
Intel,nx9jenv,Hello tarkov my old friend. Crank the gamma up again.,pcmasterrace,2026-01-02 15:16:37,10
Intel,nxakzj0,HDR and brightness up already do a decent job,pcmasterrace,2026-01-02 18:13:48,3
Intel,nxaimfx,They can't do anything about the big low light advantage you get from just having OLED.,pcmasterrace,2026-01-02 18:02:59,3
Intel,nx9ktix,"If it isn't glitching behind locked doors for better loot, ziplining to areas deemed out of bounds, lowering graphics to see through bushes and trees, backstabbing or extraction camping, it's something else. Cringey little incels will always do anything they can  (except just getting better at a game) to gain that sweet advantage over everyone else. Nothing new.  Edit: Being downvoted by said incels.",pcmasterrace,2026-01-02 15:23:43,2
Intel,nx9w43s,Wonder if known nvidia inspector exploits are patched? New games tend to forget it exists.,pcmasterrace,2026-01-02 16:17:54,1
Intel,nxaelyi,Did no one learn anything from Tarkov having and then disabling these?,pcmasterrace,2026-01-02 17:44:27,1
Intel,nxafhte,Dead by Daylight players sweating right now,pcmasterrace,2026-01-02 17:48:33,1
Intel,nxctinj,"Coming from someone who used them because I have vision loss issues, this sucks.",pcmasterrace,2026-01-03 00:58:54,1
Intel,nxf83wd,"They deactivated it 100%, if you try to mess with the Nvidia profile inspector while the game is running you are gonna get a message like “profile inspector detected” and the only option is to exit the game",pcmasterrace,2026-01-03 11:29:29,1
Intel,nxg0r89,"I just want my RTX Dynamic Vibrance back, RTX HDR works tho..games looks so much better with these on imo. I used to use Reshade in games.",pcmasterrace,2026-01-03 14:40:44,1
Intel,nxg1azw,When I see stuff like this I'm so glad that I primarily play single player games. There's so much less to worry about,pcmasterrace,2026-01-03 14:43:42,1
Intel,nxgu68g,"Dark should be dark. I could not imagine playing game like EFT with the most garbage graphics possible where closed off rooms was still perfectly lit on by a magic global illumination BS! All because PvP junkies cried that they don't see s##t. So silly nikitka made his game to look LITERALLY like s##t with all the washed out lighting on maps! Zero shadows, night have luminated objects like walls, some vehicles etc. Literally I never played worst game in lighting regards. PvP games are infested by sad people that want to have some magic eye sight and cry when there is dark shadow and nights are realistically dark!   I love SP games because I can mod stuff to be dark like in real world and immerse myself into this world and not feel like some mutated owl with magic eye sight. PvP junkies ruin games.",pcmasterrace,2026-01-03 17:05:22,1
Intel,nxji4y9,This is why I don’t play games like this.,pcmasterrace,2026-01-04 00:54:26,1
Intel,nxool67,that's why I launched the game now and I was like where the fuck are my filters?,pcmasterrace,2026-01-04 20:07:01,1
Intel,nx8pcwc,Cheaters going to cheat,pcmasterrace,2026-01-02 12:09:34,-5
Intel,nx9ne27,"This is pointless, you can still install Reshade into overlay software such as Lossless Scaling or Magpie and use them to scale the game window, and you'll be able to make the Reshade effect work on any window, unless you disable all the capture APIs such as DXGI and WGC, which also means that video recording software such as OBS will not work either",pcmasterrace,2026-01-02 15:36:29,-1
Intel,nxakb7d,Try hards will do anything they can for an edge. It’s so sad.,pcmasterrace,2026-01-02 18:10:43,0
Intel,nx9jqzd,What's the work around ?,pcmasterrace,2026-01-02 15:18:20,-3
Intel,nxbvlpy,"Honestly if you have a good set up and OLED, the dark isn't bad anyway",pcmasterrace,2026-01-02 21:56:53,0
Intel,nxcstbt,So peanut is gonna struggle in dark places??? I must say im.excited to see the result,pcmasterrace,2026-01-03 00:54:56,0
Intel,nxctx3t,With the amount of cheaters in the game currently they should allow it if you turn off cross play so you don't have an advantage over the console kids.,pcmasterrace,2026-01-03 01:01:11,0
Intel,nx99doi,Just use nvidia control panel?,pcmasterrace,2026-01-02 14:22:06,-10
Intel,nxaak63,PC players gonna PC player,pcmasterrace,2026-01-02 17:25:25,-7
Intel,nx9ytqs,They did something similar with RUST too I think. People used gamma to avoid the pitch black night's darkness,pcmasterrace,2026-01-02 16:30:39,51
Intel,nx9dh0w,I remember the days of easily seeing at night   But yea. Definitely glad they added this later on,pcmasterrace,2026-01-02 14:45:11,142
Intel,nx9y5jc,"Yup, they have the best fix.",pcmasterrace,2026-01-02 16:27:31,14
Intel,nxaoglb,"As an original DayZ mod player (and part of the group who started the first US servers) I keep feeling like I'm in that ""First time?"" meme whenever I see a headline about Arc Raiders.",pcmasterrace,2026-01-02 18:29:34,9
Intel,nxdhubr,Suoer cool,pcmasterrace,2026-01-03 03:21:11,1
Intel,nx8x1zs,Also why using Nvidia Profile Inspector can trigger bans.,pcmasterrace,2026-01-02 13:07:09,97
Intel,nx8z8zn,"It's similar to how turning the foliage to low on PUBG worked, at least it did when I used to play it like 8 years ago. By having it set on medium or above you was at a disadvantage.",pcmasterrace,2026-01-02 13:21:35,26
Intel,nx9hjn8,Shit my monitor can do this,pcmasterrace,2026-01-02 15:06:58,7
Intel,nxao8t3,Yeah typically you can darken the in games settings and then go to Nvidia control panel and edit the brightness there. That's always been a thing in any game tbf.,pcmasterrace,2026-01-02 18:28:34,2
Intel,nxb2wqy,"I doubt it's because of gamma and brightness, in counter strike it was breaking the game because people with certain filters could see through smoke, I think similar thing was happening here",pcmasterrace,2026-01-02 19:37:13,1
Intel,nx9mneb,You can still turn up gamma with the Nvidia app and Nvidia control panel. They just disabled being able to do it with overlay,pcmasterrace,2026-01-02 15:32:50,26
Intel,nx9o5t1,I just use the settings on my monitor to mess with that. It usually looks better than messing with in game options anyway,pcmasterrace,2026-01-02 15:40:14,6
Intel,nxa5w9f,"I believe it doesn't support HDR and some areas are pitch black on OLED. I suppose the flashlight is meant to help with that, but give away your position.",pcmasterrace,2026-01-02 17:03:33,4
Intel,nx973ia,Hopefully it’s enough to discourage a good percentage of people.,pcmasterrace,2026-01-02 14:09:01,12
Intel,nx9wuhh,A simple profile switch on my monitor does the trick.,pcmasterrace,2026-01-02 16:21:20,2
Intel,nxa8eqk,i use monitor then vibrance gui to put colour back in,pcmasterrace,2026-01-02 17:15:21,2
Intel,nx9zhao,The difference is opening an overlay and toggling on a preset takes a lot less time than mashing the buttons on your monitor when the scene changes to night.,pcmasterrace,2026-01-02 16:33:43,2
Intel,nxdvmba,Because arc raiders is trash,pcmasterrace,2026-01-03 04:49:24,0
Intel,nx9mie8,RTX HDR won't be blocked,pcmasterrace,2026-01-02 15:32:08,-1
Intel,nxbc5yu,Backstabbing and extraction camping are not like the others. Those are actual gameplay options.,pcmasterrace,2026-01-02 20:22:08,3
Intel,nx9ux7e,"Yep. Absolutely disgusting what people will do in order to ""prove"" they are good at something.  Stopped playing competitive shooters 15 years ago. Just got tired of the bullshit, and that was back then.",pcmasterrace,2026-01-02 16:12:19,0
Intel,nxa4h16,Bruh I'm on OLED and just want to be able to who is shooting me from the corner of the extract.,pcmasterrace,2026-01-02 16:56:52,2
Intel,nxbzfj4,Who hurt you?,pcmasterrace,2026-01-02 22:16:06,0
Intel,nx9v1oj,Thats why theres no point try harding any game. People will just cheat their way.,pcmasterrace,2026-01-02 16:12:54,-3
Intel,nx9xkk0,Adjusting my monitor color settings makes me a cheater?,pcmasterrace,2026-01-02 16:24:47,10
Intel,nxa6hx4,"Man I don't even play AR, but I hate this type of argument so much. We shouldn't do anything to make it harder to do something bad, because people will still be able to do bad thing anyways. Will it solve the problem? No. But a minor improvement for a negligible cost is still worth it.",pcmasterrace,2026-01-02 17:06:24,10
Intel,nxe039x,"'it's pointless to stop pedophillia because other countries have legal child marriages' type of arguement, young people are so fucking nihilist they would rather starve to death than grab food literally next to them.  THIS IS JUST A STEP into either implementing wide anti-tampering techniques like cleverly hidden noise ala DayZ, or standardizing properly implemented calibration tools for stuff like HDR.  You take action to make change happen.   Being a nihilist and DOING NOTHING solves gues what... NOTHING.   We have shitty implementation for shit like HDRT because you gamers rather would chrip about 'woke games' than making noise about stuff like proper HDR implementation.",pcmasterrace,2026-01-03 05:20:48,1
Intel,nxrb5lt,"The game needs to be fixed. It is absolutely terrible for visuals and lighting. Either they implement it properly, let us all do it with in-game settings, or we will find a way to fix it ourselves outside of the game. Nobody should have to buy a specific monitor or panel type just to be able to see what is on the screen in this game…",pcmasterrace,2026-01-05 04:02:11,1
Intel,nx9sbts,Use your flashlight in game,pcmasterrace,2026-01-02 16:00:04,4
Intel,nx9asdg,"What you can do in control panel is very limited compared to the filters. Same with monitor settings.  Tarkov banned these for the same reason, as well as some other games.",pcmasterrace,2026-01-02 14:30:08,3
Intel,nxajg4f,"The thing is there's hardware solution, your TV. Also, don't act like xim and cronus aren't the 2 most bought PS5 accessories. Nearly all known console Apex Legends predators use the zen to my knowledge.",pcmasterrace,2026-01-02 18:06:46,5
Intel,nx979we,"I would imagine this as I do recall this exploit being used, aleast back in Battlefield V and forcing the LOD to the lowest, allowing you to see who was the enemy very well.",pcmasterrace,2026-01-02 14:10:03,37
Intel,nx9xt98,"Same in Hunt Showdown, people would derender trees and buildings to see across the map.",pcmasterrace,2026-01-02 16:25:55,11
Intel,nxaf1m5,It can trigger bans? I use it for FPS caps in certain games thats kinda mental,pcmasterrace,2026-01-02 17:46:29,2
Intel,nxbp2du,"How so? Nvidia Profile Inspector allows you to tweak various driver-level settings that aren't exposed in the Nvidia App or Nvidia Control Panel, either globally for all programs, or only for specific ones.  Once you change a setting, that's it. It's done. You can close the app now. It doesn't have to remain running while you're playing a game. So how would the game detect NPI and ban you for using it?  Or are you saying that having that simply having that app on your PC will trigger the ban, even if you've not used it at all, let alone for the particular game? Given that NPI is a portable program (i.e. it doesn't get 'installed' anywhere, it's just an EXE), that would mean that the game would have to scan the entire PC? That's a very time-intensive process.",pcmasterrace,2026-01-02 21:25:08,1
Intel,nxas1pd,"How would someone even detect usage of Nvidia profile inspector? It doesn't touch the game files in any way. It just tells your GPU how to handle things.   Like it's a tool that allows you to change the profiles of a game... I wouldn't think that would breach pretty much any EULA.  Sure you can say, ""gain unfair advantage"" but... It's not an exploit, vulnerability, cheat/hack, etc. It's just using the Nvidia driver to its fullest.",pcmasterrace,2026-01-02 18:46:01,1
Intel,nxah58c,Any game that bans people for stuff like this is a game I don't care about being banned from.,pcmasterrace,2026-01-02 17:56:09,-11
Intel,nx9aedi,I still do that today.    Single player campaign runs at ultra high ray tracing DLSS everything settings.   Multiplayer runs on low settings except fog is disabled or view distance increased.    Been a habit since quake 2,pcmasterrace,2026-01-02 14:27:54,15
Intel,nxarzk0,"I'm used to doing this just cuz my monitor is naturally pretty dark, even when I tweak the settings on The panel I still have to turn up the gamma some... About 2 weeks ago I definitely noticed that things look different in Arc raiders. I figured it was something that embarked it on their side, but it must have been this Nvidia change?  . It is a welcomed one. Now nobody can try and get around that, while I still am able to keep the brightness for other areas. It worked out in the end for me and my use case but crazy it took this long lol",pcmasterrace,2026-01-02 18:45:45,2
Intel,nxahbcm,Why? Who cares? People can do whatever they want.,pcmasterrace,2026-01-02 17:56:57,-18
Intel,nxa1zdp,Honestly I thought everyone just did It via Nvidia control panel by just wacking up the gamma that's the way peanut showed how he did it,pcmasterrace,2026-01-02 16:45:22,-8
Intel,nxajbrz,Why don't you go back to consoles if you hate pc customization,pcmasterrace,2026-01-02 18:06:13,-18
Intel,nxagtxg,"You can save display settings profiles and have them on a key shortcut or use the companion application to do it, at least that's how my Gigabyte ultrawide works.",pcmasterrace,2026-01-02 17:54:43,6
Intel,nxa280n,Tbh if your upping your gamma you might as well keep it up at all times playing arc so many times people will be hard to see in the shadows just in buildings on daytime raids,pcmasterrace,2026-01-02 16:46:29,3
Intel,nx9mq6l,Thats a filter though?,pcmasterrace,2026-01-02 15:33:12,6
Intel,nxaktjj,"fr, anybody who’s seen how dark this game is on an OLED knows",pcmasterrace,2026-01-02 18:13:03,6
Intel,nxakqf3,Those corners etc are intentionally dark.,pcmasterrace,2026-01-02 18:12:39,3
Intel,nxr7sfs,can't even play on night map with an oled screen.,pcmasterrace,2026-01-05 03:42:44,1
Intel,nxcfpp9,"Certainly not you, with your unbelievable wit.",pcmasterrace,2026-01-02 23:42:49,0
Intel,nxa1mdj,There are plenty of games without mass cheaters,pcmasterrace,2026-01-02 16:43:41,2
Intel,nxa46jq,Right?! I'm on an OLED.  I'm severely nerfed fighting on most maps because the moment anyone is in any slightly shadowed area they are absolutely pitch black and invisible on my screen.,pcmasterrace,2026-01-02 16:55:31,12
Intel,nxb77co,Does it help you gain an advantage?   Clearly the devs think its cheating if they're updating the game to make it harder to do.,pcmasterrace,2026-01-02 19:57:54,1
Intel,nxauukk,"If you are doing it to gain an advantage, yeah.  I think intent matters a whole lot for these discussions.  Being able to see at night because you have a shitty monitor is different than adjusting your monitor to get an advantage",pcmasterrace,2026-01-02 18:58:55,-1
Intel,nxa8n1a,"Nah the devs need to do something about how dark the game is in some situations: I've got 2 PC's right next to each other, and on my main PC even *dark* areas are visible to me (on a pricy HDR monitor), but on the other computers monitors those same dark areas are literally pure black areas you cannot see in. For example, in Stella Montis the ventilation shaft with the ladder? On my main PC I can see in there (*barely) without my flashlight, but enough to navigate through it (though I may not see someone quickly or easily), but on the other computer its literally *pitch black*, meaning just by having a different monitor I don't *100% need the flashlight*, and on another I'm literally blind.  That in and of itself is an unfair advantage, just based purely on hardware, and I set an Nvidia filter to make it look *similar* to the main PC. In neither situation was I attempting to make things unfair, simply *as visible* as they should be (neither by getting my expensive monitor, nor using the filters), nor was I completely eliminating shadows and dark areas.  Don't get me wrong, I get people were using the same filters to completely eliminate the dark areas (that's unfair), but it's INSANE that the game doesn't have even BASIC gamma settings to make the game more playable on certain hardware combos. Because without it the literal hardware itself can completely nerf or cheat for you.",pcmasterrace,2026-01-02 17:16:26,8
Intel,nx9cih2,Tarkov added their own filters to the game.,pcmasterrace,2026-01-02 14:39:54,6
Intel,nx9hlck,Sure I guess having all those options are kinda extra but raising the gamma/contrast is really all needed to see again. Not everyone likes to play the game not being able to see anything. I get that’s how the game was made but I’d rather not lol. Arc raiders can have spots so dark even the flash light doesn’t reach much,pcmasterrace,2026-01-02 15:07:12,1
Intel,nxayd6i,It's disingenuous to act like it's not way more prevalent on PC.,pcmasterrace,2026-01-02 19:15:26,-2
Intel,nx9bsdt,"Yup. You could also use hidden filters that gave you a competitive advantage.      It's why I always say if you play online games but use NIS in other apps, to make sure it's closed or you revert to the stock config before booting the game because some ACs can tell you've modified the Nvidia defaults.",pcmasterrace,2026-01-02 14:35:50,8
Intel,nxa4bjv,It used to be so buggy I had missing textures without any cheats.,pcmasterrace,2026-01-02 16:56:10,7
Intel,nxbzsum,"Because you've modified the GPU driver outside of what it is meant to be doing by default. NPI changes the driver database itself for the specific game or the default global profile that the Nvidia drivers use in games if a game doesn't have a specific profile within the Nvidia app.  There are driver checks that anticheat systems perform. Using NPI can invalidate the driver check, especially when you enable flags that are not exposed within the Nvidia app or GeForce drivers and doubly so when you use NPI to selectively not render certain textures like buildings or foliage.  NPI has been used in games like Battlefield or Hunt Showdown for competitive advantages by telling the GPU to not render walls so you can see through them or to not render grass or environmental hazards like smoke and fog.",pcmasterrace,2026-01-02 22:18:01,1
Intel,nxb808i,"i know that arc raiders anti cheat will trigger anytime you have nvidia profile inspector open. It will not ban you, but it will close the game.",pcmasterrace,2026-01-02 20:01:46,5
Intel,nxb3a63,"It is an exploit. You can use the profile inspector to make the GPU driver not render certain textures letting you see through objects or through walls and foliage.      It's quite easy to detect because you're modifying the GPU driver itself, which then interacts with the game engine and the game's anti cheat.",pcmasterrace,2026-01-02 19:39:02,1
Intel,nxaw6cf,"I meant all the settings there too sorry. Yeah. I darken the in game settings even lower in game gamma sometimes and then use the Nvidia settings to bring them all up one. I don't play Arc Raiders not a fan of it, so I can't say.   I don't even press Apply in the control panel. I just hit cancel once I'm done needing it and boom back to my normal settings.",pcmasterrace,2026-01-02 19:05:08,1
Intel,nxasl4y,It’s a PvP game,pcmasterrace,2026-01-02 18:48:30,10
Intel,nxauagg,So people don't cheat and ruin other people's fun,pcmasterrace,2026-01-02 18:56:20,2
Intel,nxb5xcx,Peanut mentioned      # Peanut is a hack,pcmasterrace,2026-01-02 19:51:43,5
Intel,nx9mwxi,"It's not technically a filter as such, since it gets turned on for applications from the driver level, works differently from normal filters AFAIK",pcmasterrace,2026-01-02 15:34:08,0
Intel,nxbpg6o,Yes and on LED it's still possible to at least distinguish figures.  it's near impossible on OLED without a small gamma bump.  luckily that can still just be done in Control Panel just at the monitor level instead of game level.,pcmasterrace,2026-01-02 21:26:58,1
Intel,nxr9h15,I highly doubt it…this generation is completely socialized to a pay to win attitude. They are not going to “earn” their way through like the rest of us who put in the work to succeed…,pcmasterrace,2026-01-05 03:52:29,1
Intel,nxa4pq6,"Everyone has different monitors, there will never be a totally fair playing field. It’s very possible that some people bought their monitors secondhand and never noticed that the settings were already adjusted. My point is that you don’t know the situation.",pcmasterrace,2026-01-02 16:57:59,8
Intel,nxahc6r,Welcome to the linux gaming experience.,pcmasterrace,2026-01-02 17:57:03,-5
Intel,nxeg8l9,"Embark didn’t update the game to change that, so no lol",pcmasterrace,2026-01-03 07:30:34,0
Intel,nxbb97z,You realize that’s the exact reason they removed it right? In the same sentence you said it’s okay to have a monitor so you can see at night and then proceeded to say you shouldn’t use it to gain an advantage. What the fuck are you talking about bro,pcmasterrace,2026-01-02 20:17:38,1
Intel,nxrbz7x,"Exactly, every one of my monitors is different. Even the two that are exactly the same. One I can see fine in the dark, the other is pure black. My cheap upper monitor is clear as day on night raids with default settings.  So, there is a hardware advantage for sure. They need to fix the lighting and give us in-game settings and presets.",pcmasterrace,2026-01-05 04:07:07,2
Intel,nxarpbl,And they are also very limited on what you can do compared to Nvidia  filters. Nothing you can do in Post FX can make nighttime look like daytime like you could with Filters. Some of the settings  also hit performance fairly hard,pcmasterrace,2026-01-02 18:44:26,0
Intel,nxdac0j,I leave a global setting on for all my games. Never been kicked off. Maybe it's because I only use it to globally enable DLSS4?,pcmasterrace,2026-01-03 02:36:26,1
Intel,nxb4i45,"That's not an exploit. An exploit in the sense you're using (cheating) means you're taking advantage of a flaw or bypassing protection systems. That's not what is happening.   ""Modifying the GPU driver itself"" you know that simply updating the drivers on your GPU modify the drivers right?  You're not exploiting anything. You're using a legitimate feature of the GPU. It's not like someone did this and broke Nvidia'e TOS. The driver is literally designed to do this.",pcmasterrace,2026-01-02 19:44:53,-7
Intel,nxeducr,Peanut is fucking great     Fantastic entertainer,pcmasterrace,2026-01-03 07:09:56,1
Intel,nx9n0ti,"Interesting, does require the overlay and filters enabled to work however.",pcmasterrace,2026-01-02 15:34:41,5
Intel,nxe6403,"You can enable it via the driver or as an overlay via the Nvidia app. The filter version gives you control over saturation/contrast/etc, the driver version doesn't.",pcmasterrace,2026-01-03 06:06:23,1
Intel,nxr7wri,my oled is cranked and it's still pitch black lol,pcmasterrace,2026-01-05 03:43:25,1
Intel,nxgaj08,I don't think you understand OLED monitors honestly lol,pcmasterrace,2026-01-03 15:31:47,1
Intel,nxdslqb,????,pcmasterrace,2026-01-03 04:29:10,0
Intel,nxdr4el,"There's a difference between a slight adjustment and making things super clear though.  Anyone who does the latter is absolute filth, but if you're on an OLED or something in the day sure give it a little boost.",pcmasterrace,2026-01-03 04:19:21,2
Intel,nxbnlok,I'm saying intent matters a lot,pcmasterrace,2026-01-02 21:18:09,0
Intel,nxawgmh,I mean you could use the in game filters to get much more clarity at night and in general. I’m not saying nvidia isn’t more powerful. Was more just making a statement they (battlestate) wanted control over it.,pcmasterrace,2026-01-02 19:06:28,1
Intel,nxbf4hg,"No. Modifying the driver to force it to not render certain things in a game is an exploit.  The filter agreement I could see as not an exploit.  But changing the driving to see through walls, etc is certainly an exploit.",pcmasterrace,2026-01-02 20:36:48,9
Intel,nxe57yi,"Bruh.  It’s using an external program to circumvent a gameplay mechanic to gain an advantage and it’s explicitly forbidden by the developer.  Idk how much more cut and dry you can get.   You can argue the semantics all you want, at the end of the day, the developer gets to determine what they determine an exploit is and it’s clear where they drew the line in this case.",pcmasterrace,2026-01-03 05:59:24,2
Intel,nxjdvdj,"You don't think that ""not rendering cover"" is an exploit that should be prevented? Might as well just fess up and admit you're a cheater.   Also, this isn't a court of law. Just because ""Nvidia let's me do do it"" is _zero_ reason why a game publisher needs to allow it. It's totally unrelated.",pcmasterrace,2026-01-04 00:31:48,1
Intel,nxgek4m,![gif](giphy|DFNd1yVyRjmF2),pcmasterrace,2026-01-03 15:51:32,1
Intel,nx9nufj,"RTX HDR does not require nvidia overlay to be enabled, you can turn on RTX HDR without installing the nvidia app via Nvidia Profile Inspector",pcmasterrace,2026-01-02 15:38:42,1
Intel,nx9n7dk,"You'll find that all the filters will disappear, but RTX HDR & RTX Vibrance will still be available.  Same as for EFT and a number of other games that have the normal filters blocked.  The overlay doesn't stop working, since that again works from a driver level.",pcmasterrace,2026-01-02 15:35:35,-1
Intel,nxekhik,"Playing some multiplayer games on linux results in bans just for using linux, because game devs think that only cheaters use linux.",pcmasterrace,2026-01-03 08:06:53,0
Intel,nxdr8ly,You people are intolerably stupid,pcmasterrace,2026-01-03 04:20:06,1
Intel,nxc624x,"You can't see through walls with Nvidia Profile Inspector. That's not how that works. Unless you're specifically telling the walls not to render, which isn't a native option in the application.   Nearly all the things you can modify deal with effects and how your GPU handles them. Walls are not effects. So if it's modifying walls... Then that's not the intended application of it. Hardly a NPI issue.",pcmasterrace,2026-01-02 22:50:21,-1
Intel,nxe5t8m,"Nah you don't get to call something a pineapple because you don't know what to call it lol.   Call it the proper term, a violation of the TOS or EULA or whatever it's listed under. It's not an exploit, pretty simple.",pcmasterrace,2026-01-03 06:04:02,1
Intel,nx9nytz,I never said it did but I assume you can enable filters via profile inspector too right?,pcmasterrace,2026-01-02 15:39:18,2
Intel,nxel8r8,"brother, as a fellow Linux guy, that was a completely insensible reach.",pcmasterrace,2026-01-03 08:13:24,2
Intel,nxdrdtf,So you're just pro cheating?,pcmasterrace,2026-01-03 04:21:03,1
Intel,nxcnpo2,"If you can change how your gpu renders things, and it gives you an advantage in the game and it is not approved by the developer, then it’s an exploit.",pcmasterrace,2026-01-03 00:26:55,2
Intel,nxf6jvl,Ok buddy.,pcmasterrace,2026-01-03 11:16:23,1
Intel,nxds8rt,If you think that changing the color settings of your monitor constitutes cheats there is absolutely no helping you understand,pcmasterrace,2026-01-03 04:26:46,0
Intel,nxcq6ey,"...by that logic, having a 5090 gives you an advantage over someone that has a 1080 because a 5090 renders things differently than a 1080, so having a 5090 is an exploit.  Now as for ""not approved by the developer"", there are PLENTY of things not explicitly approved by a developer. MMO mice, for example, that enable you to have 10 buttons on the side of your mouse may give you an advantage over someone that does not. I don't see that explicitly approved by nearly any developer, so that must be cheating, right?",pcmasterrace,2026-01-03 00:40:22,-2
Intel,nxdtk07,"I said the degree to which you change them is important.  If it's meant to be difficult to see enemies at night, and you change some setting that makes it easy to see enemies at night, I'd call that cheating.  If it's meant to be difficult to see enemies at night but due to your environment or something you can't see at all and you make it so that it's difficult to see them but at least possible, that's of course fine.  Isn't necessarily a hard line or anything, but if you're there in the settings going 'heh heh heh this'll make it so I can see enemies really easily at night', and maybe making profiles to switch while in game...",pcmasterrace,2026-01-03 04:35:35,0
Intel,nxcy7wl,"Swap the word approved for permitted.  Ultimately it’s up to the developer to determine what is an exploit and what isn’t. It’s their game, their terms of service. The make the rules.  If they see it as an exploit, then it is one.  Good example is overwatch, if you have an ultrawide monitor it’s cropped on the sides because they see the additional screen space as an advantage. So if you mod your game somehow to get an uncropped ultrawide view, then having a nicer ultrawide monitor is an exploit.",pcmasterrace,2026-01-03 01:25:55,1
Intel,nxra8pn,"I change certain settings for almost every program I open. There is no universal setting that works for everything. The good programs have built in settings with presets. The garbage ones like arc raiders require significant outside changes just to see anything on screen. Everything I do is manual though. Through windows, Nvidea, and monitor settings…",pcmasterrace,2026-01-05 03:56:59,1
Intel,nx0z44d,"Rx 6800, not sure i would rely on intel driver support being long lasting",pcmasterrace,2026-01-01 03:33:25,6
Intel,nx17qcu,"The RX 6800 XT for sure. That gpu is equivalent to a 3080 ti out of the box and if you overclock it I have seen people hit RTX 3090 scores with it! If you can manage to find one for a good price, try to snipe an RX 7800 XT. The 7800 XT is even more powerful with better driver support for longer.",pcmasterrace,2026-01-01 04:34:23,3
Intel,nx10wfz,"I have two custom builds, one with a 7900xtx and another with an ARC B570. I like the RX 6800 because of its 16GB of VRAM; its performance is good, and you can play anything you want on it. Intel's option is fantastic, the best blue GPU, but times are tough, and I don't like 12GB of VRAM if you can get something better.",pcmasterrace,2026-01-01 03:45:35,2
Intel,nx154vl,6800 is better  b580 doesnt like old hardware and drivers are still early,pcmasterrace,2026-01-01 04:14:53,2
Intel,nx1gqen,"Currently using a 7700xt for this game and it's holding up but wow, it's a demanding title",pcmasterrace,2026-01-01 05:48:56,1
Intel,nx108dj,why?,pcmasterrace,2026-01-01 03:41:05,1
Intel,nx158ir,intel doesnt have decades of gpu driver development like amd and nvidia.  intel doesnt play well with old hardware  intel needs resizeable bar in bios settings to work properly  intel might fail and abandon gpu support. intel isnt doing good right now. 13th and 14th gen cpu had failures company lost 50% of its stock value in 1 year.,pcmasterrace,2026-01-01 04:15:37,3
Intel,nx1vemn,"Intel has been supporting and maintaining GPU drivers since the early 2000s.  In most years Intel was the largest GPU vendor by shipping video products in the world.  Intel's not doing great at the moment, but it is now part-owned by the US government and Nvidia, like how the Chinese do things.",pcmasterrace,2026-01-01 08:10:15,1
Intel,nw0haj8,"I don't really have anything bad to say about arc. It works, it's priced pretty decently. So I guess uh....not enough blue?",pcmasterrace,2025-12-26 12:08:10,1
Intel,nxjgswm,Nice pc man  Although you should move the aio to the top or change it's orientation as you want the pump to be the lowest point in water cooling loop,pcmasterrace,2026-01-04 00:47:13,2
Intel,nxr02g6,"Did you buy that psu along the GPU?  I wouldn't buy the GPU usually, as it can be as slow as pascal era mainstream gpus and something used could be both cheaper and faster, but prices are diffrent across the world.  However the PSU is an E tier unit so I wouldn't recommend buying it ever, and it only has 3 year warranty so if you keep upgrading at the same pace, your psu warranty will run out before you're ready to upgrade the GPU.",pcmasterrace,2026-01-05 02:59:20,0
Intel,nxr6rmk,"Yes, the last power supply that I have finally gave up after 4 years of usage so I included that along with my purchase of the 3050. I wouldn't expect much on that power supply even if it is in the E tier or in the 80 PLUS standard tbh.",pcmasterrace,2026-01-05 03:36:55,1
Intel,nxqx5l8,Make sure RAM is in slots 2 and 4 (from CPU).  For DDR4 on ryzen you have to carefully select the SOC voltage to make it run.  Also try to set it to 2133 speed.,pcmasterrace,2026-01-05 02:43:46,2
Intel,nxqj492,"Is xmp enabled in the bios? Idk about arc raiders but when borderlands 4 released i had nothing but ram issues from it being enabled,  shut it off and no more crashes",pcmasterrace,2026-01-05 01:29:13,1
Intel,nxqzizt,"Yes, I definitely installed the RAM on DIMMA2 and DIMMB2 and even tested by swapping them, ran MemTest86 and Windows Memory Diagnostic.   I have set specific voltages in the BIOS but I will add this to my checklist.",pcmasterrace,2026-01-05 02:56:24,1
Intel,nxqzcec,I've tried it both without XMP enabled (running at 2400GHz) and with XMP Profiles-1 and -2 enabled (running at 3200GHz),pcmasterrace,2026-01-05 02:55:25,1
Intel,nxr166m,"do you also have a GPU (video card)? It might be set to too high a speed - either open the case to give the GPU more air, or reduce its speed in Afterburner. Also, if the GPU is heavy (>2pounds) check for sag (loss of contact with PCIe slot), mitigate with a prop.",pcmasterrace,2026-01-05 03:05:31,1
Intel,nxr4aht,"I've tried the case both upright and flat on the MOBO side to eliminate GPU sag as an issue. Case has been open and airflow shouldn't be an issue. It always flags RAM, never GPU.  Edit: Currently running MSI Overclocking Scanner [driver v591.59, NVAPI mode] to see if this gives any insight.",pcmasterrace,2026-01-05 03:22:59,1
Intel,nx5elyy,Already seen card prices here in Canada jump 10-15% since last night. It's insane.   Now is not the time to build.,AMD,2026-01-01 22:05:04,5
Intel,nxlg7un,"Hi, I was wondering if there's any reason to worry if my 7800X3D sometimes spikes for 1-2 seconds to 100°C while gaming and then goes back to the usual temp. I have noticed the highest temp recorded by HWiNFO at one point was 104°, though I never noticed it on the OSD while in a game and never noticed a performance drop. Is there a problem with the cooling or something that could damage my CPU or is it just a sensor bug/issue?",AMD,2026-01-04 08:46:39,3
Intel,nx583ui,"If you're looking to do a PC build...just don't.  If you NEED to do one, do it right now. It's not getting any cheaper this year.",AMD,2026-01-01 21:31:55,4
Intel,nx5jal3,"Here's a dumb question that would be absolutely ridiculed if I dared to create a whole thread around it.  Is there any truth to my hypothesis that Play Station PC ports are likely to be relatively well-optimized for AMD GPUs, given that the Play Station 5 itself is indeed some variant of RDNA? I recently got a 9070xt and have been overall very impressed, but its achilles heel seems to be ray tracing. This isn't exactly surprising to me, as I researched my GPU options to death before buying one, and the general consensus is that Nvidia is stronger in the ray tracing department. But if I were to boot up, say, Ratchet and Clank Rift Apart, a game that supports ray tracing at 60 fps on the base Play Station 5, could I expect it to perform better than a similarly demanding game that wasn't particularly optimized for AMD hardware?  It's largely hypothetical question, as I already own the GPU, am satisfied with the GPU, and of course did my due diligence before buying the GPU so I would know exactly what to expect. But I just haven't really heard much discussion of what, if any, overlap we get optimization-wise for games that were optimized first and foremost for the AMD-based Play Station 5.",AMD,2026-01-01 22:29:53,2
Intel,nx6hz7y,"Thinking about doing a platform upgrade from a 5800X3D to a 9800X3D, how much of an improvement will I see with my RX 7900 XTX?   Obviously I know that DDR5 is priced high now but I think it's only going to get worse if I wait. I live near a Microcenter as well so I'll be doing one of their combos with the CPU, Mobo, and RAM.",AMD,2026-01-02 01:47:36,2
Intel,nx8k7tp,"Early 2025 I was thinking about upgrading to AM5 but there's no way that's happening, I only got a sapphire nitro+ 9060 XT 16gb on Black Friday.  Current setup is Ryzen 3600 on Gigabyte b450 Elite v1, 16gb ram 3200, 9060 XT. My question is, would an upgrade to 5800X make sense? It costs 165 euros where I am and it's the only upgrade I can make that I see. I play games like Helldivers 2, BF6 nowadays. Also I play on 1080p.   Thank you.",AMD,2026-01-02 11:26:25,2
Intel,nxim4kb,"Hi all  I'm about to give my water-cooled 6950xt to my brother as I picked up a 9070xt.  As I've got to.out the og heatsink back on I'd like to.replace the pads ofc. Does anyone know the sizes needed.  I'd also throw a kryonaut grizzly bear pad on the GPU, would this be a 1mm pad?  I'd like to get this right as he's on a 5700XT so it will be a good upgrade for him.  Many thanks.",AMD,2026-01-03 22:08:10,2
Intel,nxaeni9,"I could use some suggestions on upgrading a desktop box my son built for me in 2013. It was used for my graphic arts business (Adobe Suite) and has performed admirably for the last 12 years. It's running Windows 10 and most of the patches will not install. It can't be upgraded to Windows 11, and while I realize that every MS upgrade I ever did in the past caused major mayhem, I probably should go ahead and do it before it quits running altogether.  Below is a list of what he ordered and put in it.   What should I order that will swap out and last me another 5-10 years? I just used this for work and internet. No games.  • MB Gigabyte|GA-970A-UD3P AM3+R   • VGA Sapphire|100365BF4L R9 270 2GD5   • PSU Roswell|RX850-S-B 850W RT (has been replaced)   • CPU AMD|8-Core FX-8350 4.0G 8M R   • SSD 256G|Samsung MZ-7PD256BW R   • MEM 8Gx2|Corsair CMZ16GX3M2A1600C9  It also has a DVD RW Drive and I added a 12TB WD Hard drive   I'm sure most of you folks can look at that list and quickly see what I need to change. I'm thinking CPU, Motherboard and RAM? Thanks for your expertise.",AMD,2026-01-02 17:44:40,1
Intel,nxc2q2i,"I typically wouldn't do a pre-built but considering I can get my hands on this right now if I wanted and the prices of things going up, would this be worth grabbing?  $1,649.99 AMD Ryzen 7 9800X3D, AMD Radeon RX 9070XT 16GB, 32GB DDR5 RGB,2TB NVMe SSD  [https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5](https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5)  Thank for the input in advance!",AMD,2026-01-02 22:33:02,1
Intel,nxeipp1,"Quick sanity check: Am I right to say that there are no new production of X570 boards at the moment, and therefore I should just sit tight with my Asus X470 Stix-F board until the RAMmegeddon eases before moving up to AM5/AM6?",AMD,2026-01-03 07:51:43,1
Intel,nxfttws,"Bonjour, j'ai un vieux pc qui a malheureusement commencé à rendre l'âme fin 2025 et je dois donc me dépêcher d'en racheter un avant que les prix deviennent exorbitants. Je recherche un Pc fixe (si possible prémonté étant donné que je suis peu doué là dessus) pouvant faire tourner les jeux d'aujourd'hui (E33, Dlc Baldur's Gate etc...) et si possible ceux de demain.   J'ai un budget correct (1200 euros max) et je risque pas de faire grand chose à part jouer dessus.    Merci d'avance pour vos avis !",AMD,2026-01-03 14:01:31,1
Intel,nxqoxma,"I just installed my new RX 9070 XT today, replacing my RTX 3060 Ti, and after getting the new drivers set up and the old ones gotten rid of, i'm having an issue of intermittent audio crackling. Is there a know simple fix for this?",AMD,2026-01-05 01:59:57,1
Intel,nxdpvtk,"https://i.redd.it/n3zqy5xj72bg1.gif  Apparently my driver stopped updating nearly 2 years ago, and I was never concerned about it. Do I need to do some ridiculous workaround here?",AMD,2026-01-03 04:11:23,0
Intel,nxmzstu,"Hi. HWiNFO is a very reliable monitoring tool, so unless there is a known open issue regarding sensors for your CPU SKU, I'd trust these temp readings.  I don't use a Ryzen 7 7800X3D, but the maximum operating temperature (Tjmax) for the 7800X3D is 89°C. If you're seeing temperatures over 100°C, that's likely a cooling problem that could damage your chip over time. I'd probably check my cooling system and setup if I were you. That said, another 7800X3D user might think differently, so maybe there is nothing to worry about.  Based on my experience, CPU temps over 100 °C usually indicate poor thermal management or inadequate cooling.",AMD,2026-01-04 15:31:19,1
Intel,nx5b3qi,You still can build a PC as long as you know where to get the parts you need at a price you can afford despite the crappy RAM and GPU prices.,AMD,2026-01-01 21:47:15,2
Intel,nx8r8gw,"China stolen Samsung DRAM tech, this year we may have influx of chinese cheap RAM from CXMT to save us",AMD,2026-01-02 12:24:29,1
Intel,nx92ypu,"> how much of an improvement will I see with my RX 7900 XTX?  Up to 50% but this is assuming heavily CPU bottlenecked games (stuff like Battlefield 6, Factorio, Stellaris etc). Less than 15% in a standard AAA grade single player title if you play at 1440p. 0% if you play at 4k.   There's no single metric here as it really depends on a game. If you love 4X games like Stellaris I would upgrade. If you prefer Silent Hill or Alan Wake 2 I wouldn't.",AMD,2026-01-02 13:44:35,2
Intel,nx92d32,"It actually might make sense considering you are playing CPU heavy games at a relatively low res. I would also check if 5700X is available since it's pretty much the same thing as 5800X, except often a bit cheaper.   I see techspot actually tested BF6:  https://www.techspot.com/review/3043-battlefield-6-cpu-benchmark/#2025-10-15-image-png  3600 got 62 fps 1% lows and 86 averages whereas 5800X reached 80 fps 1% lows and 108 averages. So theoretically up to 30% better. Still, in both cases it's playable, fps dropping to 62 probably won't kill you.",AMD,2026-01-02 13:40:59,2
Intel,nxq9rvb,It can vary model to model. I watch search for the sku you purchased and if you can't find it try contacting the manufacture to see if they can tell you. EVGA used to be good about providing this info but it really depends.,AMD,2026-01-05 00:39:44,1
Intel,nxav8dz,"CPU, motherboard and RAM yes. AM5 and DDR5 are the newest.  I don't like windows 11 and I will keep using windows 10 for as long as I can. Unless you absolutely need to upgrade I wouldn't bother.",AMD,2026-01-02 19:00:42,0
Intel,nxev4pq,"According to the review on that site it comes with 5200 MT/s RAM which is not ideal. 6000 matches the memory controller's speed so that's what I would recommend. It's not a big issue, only a small performance difference. Other than that it looks like a solid setup.",AMD,2026-01-03 09:40:03,1
Intel,nxn91rb,"it's not constantly hitting 100 °C so idk what to think of it, hasn't happened today yet and I've been monitoring it so maybe it's nothing",AMD,2026-01-04 16:15:19,1
Intel,nxaknxe,"Thank you for your reply!  Price difference for me between the 5700x and the 5800x is 10 euros so cost isn't something to consider in my case. I'll look up thermals to see if it makes a difference. The benchmark you linked is so helpful for my purposes, kudos!",AMD,2026-01-02 18:12:20,1
Intel,nxngkvz,"It's good that it doesn't happen constantly, but even if those readings occur occasionally or intermittently, it's generally not a good sign.  However, if it hasn't happened again today, and you're under similar or identical workloads to when you had those readings, you probably have nothing to worry about. It could just be a few inaccurate readings.  Continue to monitor your CPU temperatures and, if you notice occasional readings over TJmax again, it's worth checking your current thermal management (thermal paste, contacts, etc.) and cooling setup (fans, AIOs, and/or liquid cooling). Prevention is better than cure.",AMD,2026-01-04 16:50:14,1
Intel,nvc9b2o,"The Radeon RX 9060 XT offers the highest raw frame rates at 1080p, outperforming the competition by roughly 4-5% on average.  The RTX 5060 provides nearly identical performance but adds the advantage of DLSS 4 for superior upscaling and image quality.  While the Intel Arc B580 is the slowest card, its 12 GB of VRAM allows it to handle Ultra settings that cause the 8 GB cards to stutter.  Ultimately, the video recommends the 16 GB version of the RX 9060 XT as the best long-term choice for modern gaming.",AMD,2025-12-22 08:32:19,150
Intel,nvcj3xg,Had to sell the 6600 XT and went for the 9060 XT 16GB to play at 1440p. I’m loving it,AMD,2025-12-22 10:10:46,36
Intel,nvim4sd,I got my 8GB 5060Ti open box excellent BestBuy for $309. It was brand new.,AMD,2025-12-23 09:07:15,4
Intel,nvotif9,Personally out of the 3 I'd pick the 5060. Transformer model is but better than FSR4 at 1080p,AMD,2025-12-24 08:41:46,4
Intel,nvgivw5,"Bought a 9060XT 8GB for 247e (renewed on Amazon, Black Friday stuff) and sold the temporary 4060 non-TI 8GB for 220e on marketplace. Good deal...",AMD,2025-12-23 00:02:30,2
Intel,nvahjmg,Only compares 8GB cards from teams red and green since it’s only considering <$300.,AMD,2025-12-22 00:43:27,7
Intel,nw2aruf,😮🫳🍿,AMD,2025-12-26 18:41:22,1
Intel,nwnskte,"I found an openbox 9060 XT 16GB at Microcenter for $305 and jumped on it. Very impressed so far, especially with undervolting.       I have the Powercolor Reaper model and it is legitimately impressive that they were able to make it that small.",AMD,2025-12-30 02:33:44,1
Intel,nvbruur,"I feel like the HUB guys are going too hard with their VRAM crusade. Why recommend a GPU that's slower now just because it might be faster in the future?   A slight downgrade in render resolution or texture quality is hardly even noticeable, and with looming shortages I feel like most studios are going to start optimizing for lower VRAM further reducing the long term disadvantage of 8GB GPUs.",AMD,2025-12-22 05:48:05,-14
Intel,nvcdgdm,"The real answer, buy a used 2080ti. Usable VRAM, DLSS4, it still is 250W so it can run on most PSUs.  It is the most balanced option if you can't afford a 9060XT 16GB.",AMD,2025-12-22 09:14:12,-15
Intel,nve7fwf,"all of them are power hungry junk, where are good cards?",AMD,2025-12-22 16:44:49,-9
Intel,nvm6i3z,real hero here,AMD,2025-12-23 21:49:44,4
Intel,nwe5oim,"Intel is on the right path, but they need to start using 384-bit memory interfaces on 12GB cards instead of the 192-bit memory interface they used on this card.",AMD,2025-12-28 17:23:33,1
Intel,nvm9ob0,"The 5060 will crush, without less than a 40 percent difference, from dlls alone. Then add frame gen. WOW I can't believe you can get away with this.",AMD,2025-12-23 22:06:17,-11
Intel,nvftrl1,5060 then cuz way better in AI  5% performance cut to gain 2x-3x AI speed,AMD,2025-12-22 21:41:30,-32
Intel,nvfjawi,What processor are you using with 9060 xt?  Is it the same as you were using with 6600 xt?,AMD,2025-12-22 20:46:22,9
Intel,nvjicw9,"Can you tell me how well it runs games at 1440p? Have you played some of the demanding ones like Black Myth Wukong, Stalker 2, etc? Do you play at medium? high? I assume FSR is always on.   And also what's your target FPS? Would really appreciate the feedback, because I have the same card and I'm thinking on switching to 1440p but I don't know what monitor would be good refreshrate-wise",AMD,2025-12-23 13:36:03,1
Intel,nvb3bfo,"Well yeah, the cheapest RX 9060 XT 16GB is [$380](https://pcpartpicker.com/products/video-card/#c=596&sort=price&page=1&P=11811160064,51539607552) and the cheapest RTX 5060 TI 16GB is [$430](https://pcpartpicker.com/products/video-card/#sort=price&P=11811160064,51539607552&c=593). When you're comparing $300 GPUs, you're not going to bring up a GPU that's nearly another hundred dollar.",AMD,2025-12-22 02:55:15,48
Intel,nvcb05n,"That would be a completely new phenomenon if you look at the past. Sure, some (probably indie) studio will optimize their games, but they would have done so already because they care about their customers.  Nothing will change with the current devs or tech, it's just a temporary issue that memory is that expensive. The prices will be lower in 2027, or we'll get used to it and buy more expensive stuff.",AMD,2025-12-22 08:49:20,9
Intel,nvda1uj,"6 ish year old product that is out of warranty from some rando, is not exactly comparable here and definitely not a ""real answer""",AMD,2025-12-22 13:46:25,18
Intel,nvd5m8s,"Dunno why you're being downvoted, the 2080 Ti is still very good value for the price and often has good OC headroom. Beats 5060 in most cases and you're right about 11GB being decent",AMD,2025-12-22 13:18:38,0
Intel,nvckhpi,The real answer is to stop being cheap and spend money on your hobbies.,AMD,2025-12-22 10:24:05,-26
Intel,nvgc2fa,you tell us,AMD,2025-12-22 23:21:59,3
Intel,nvhwm4g,Why do power requirements matter?   Electricity costs pennies,AMD,2025-12-23 05:17:09,1
Intel,nwno1my,So many think memory bandwidth matters more than it does. The 5060 ti has less bandwidth than the B580. Architecture matters a lot.  More bandwidth would do next to nothing for it.,AMD,2025-12-30 02:09:09,1
Intel,nvr8r0s,AMD cards have upscaling and framegen as well...,AMD,2025-12-24 18:35:16,2
Intel,nvgtg9q,"Can you elaborate what do you mean by ""AI speed""?",AMD,2025-12-23 01:04:59,27
Intel,nvlqflt,No. Dlss and frame gen is much less impactful in terms of performance boost at the low end and the latency is more noticeable. It’s also half the vram.,AMD,2025-12-23 20:25:01,6
Intel,nvm9h9y,WOW 25 so far for the TRUTH. HUB and fooling now.,AMD,2025-12-23 22:05:16,-5
Intel,nvfl8ph,"Yes, same processor, 5600x",AMD,2025-12-22 20:56:36,6
Intel,nvjk7kj,"yeah seriously, here b580 is noticably cheaper for example.",AMD,2025-12-23 13:47:03,4
Intel,nvcwed4,"Yeah, I just wanted to point it out because there’s people like me to whom prices in dollars means nothing (or who don’t read the title) and then waste time watching an irrelevant video (though I skipped to the conclusion so not that much time).🙂",AMD,2025-12-22 12:11:20,-9
Intel,nvdtbxt,"Point being? If a cap blows because it's old any repair shop can fix it, If it's a fan dying you can fix that yourself.  On the other side there's not much the warranty can do for running out of VRAM.",AMD,2025-12-22 15:34:11,-6
Intel,nvcpxpj,"In this economy? It doesn't make any sense to not keep perfectly usable hardware that does the job just fine out of a landfill.   A 2080ti or a 3070 or AMD equal is more than enough performance for most people. Easily, and is way better bang for your buck.",AMD,2025-12-22 11:15:29,10
Intel,nvgxp18,"In a time of global economic uncertainty, it's a horrendous time to overspend on hobbies.",AMD,2025-12-23 01:31:13,4
Intel,nvhzp95,"heat, noise, size, messy cables",AMD,2025-12-23 05:41:18,1
Intel,nvuuu5m,"Correct, but they do not have commercial dlss support. How many games do you not have the ability and ww do?   Thanks for the dowmvotes nvidia. Amd brainwashed.   Just to let you know: you have all been played. Look closer.",AMD,2025-12-25 11:21:47,-2
Intel,nw6d88g,"They're seemingly referring to the speed of running LLMs locally using that GPU, unless I'm also out of the loop. A good sub to look into that stuff would be /r/LocalLLM   I wouldn't recommend doing that with a 5060 but the 16gb version must be the best choice in that price range and would handle the very small models easily and the small ones with a little slowness.",AMD,2025-12-27 11:35:46,1
Intel,nvcxw7p,"News flash: We're always in ""this economy"". I know someone who works at a fucking McDonalds, has a kid, and spends more money on his hobbies than you do.",AMD,2025-12-22 12:23:17,-11
Intel,nvlzofj,nothing global about it,AMD,2025-12-23 21:14:23,-2
Intel,nvipuzv,"So you prefer low power for lower heat and smaller size.   I'm not space conscious, so those things don't matter.   What's with messy cables? The PC sits under the table, so it also doesn't matter how ugly it is.",AMD,2025-12-23 09:44:14,2
Intel,nvw2mmm,"Well actually people have been modding games to put FSR where neither AMD or NVIDIA added official support.   Pretty much every game had amd nvidia and even intel upscaling these days.   In fact, when i still had my 3080ti, i was able to use AMD’s framegen in many games (cyberpunk, dying light, talos principle) because NVIDIA didnt provide any option for 3000 series.   I still bought nvidia because amd doesnt offer any cards at 5080 level, so no brainwashing here. You’re completely uneducated blinded by consumerism",AMD,2025-12-25 16:47:51,3
Intel,nvimsbd,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-23 09:13:38,1
Intel,nveay56,They sound irresponsible.,AMD,2025-12-22 17:02:16,10
Intel,nvfpbcp,"If you spend more on your hobbies than the things you actually need, you might be financially irresponsible",AMD,2025-12-22 21:18:17,8
Intel,nvj2hzu,"the unhinged extra power cables attached to the card itself, it makes everything harder to handle  and heat isn't just about the size of the case, it's noise and comfort of the room  and no, AC doesn't solve that as it's another source of noise that is even worse than the PC itself, only to be used when the weather is too bad to live otherwise",AMD,2025-12-23 11:42:04,0
Intel,nvwscdp,"Oh yeah? And who do you think had went through hundreds of accounts taking about that mod?   Came out DEC 22 2023 I remember the day I went from 22fps in Alan wake 2 to 50 (3070). I posted in this site non stop ban after ban just to try and get you this information. Go on the forums you will pick me out if you look back.   I, in all seriousness, would not be surprised if you know about that mod from ME.   Therefore, I am not blinded. I simply understand that an entire company propped up by manipulation on social media (and GPU) should not exist, and a real competitor would be in their place. There I have just demonstrated that not only am I not blinded, it might be you. Good on you for knowing about that mod (serious).   I will tell you another secret, maybe not meant for you. If you don't mind using dlss and mfg? 5060ti 16gb all day for 1440p or lower. Not only is it hundreds of fps, it has valuable vram on it that will see the card rise in price since it's discontinued.   Hows that for an uneducated prediction?",AMD,2025-12-25 19:18:39,-2
Intel,nvjm540,Gotcha.  What's your preferred card?,AMD,2025-12-23 13:58:18,1
Intel,nvkhap4,from this generation that'd be RTX Pro 4000 Blackwell generation SFF with replaced cooler  personally I own a passive A2000 SFF that replaced my modded 1650 (KalmX was released too late),AMD,2025-12-23 16:39:40,2
Intel,nvkshx0,"Hopefully I am wrong but there is no aftermarket cooler for the RTX Pro 4000 SFF, right ?  https://n3rdware.com/gpu-coolers",AMD,2025-12-23 17:35:10,1
Intel,nvkw08y,"unfortunately no, nothing ready to use that I know of  if you have access to measuring equipment machining a shim isn't even that expensive, haven't seen any publicly available projects for it yet",AMD,2025-12-23 17:52:24,1
Intel,nvkx5mu,"Hmm that sounds tricky.  I’m thinking about getting PCI express extensor and a GPU holder to be able to use it with my MS-A2, keeping the GPU externally til the n3rdware cooler is available.",AMD,2025-12-23 17:57:58,1
Intel,ntamglk,"From r/radeon   * Ray Caching: Only available in Warhammer40K today, more games next year. * Ray Reconstruction: Only available in Black Ops 7 today with more games next year. * AI Frame Gen: Available in Black Ops 7 today with 40 games by end of 2025.",AMD,2025-12-10 14:35:10,103
Intel,ntak2ov,It's almost 2026 and AMD keeps reinstalling the AMD Install Manager that I do not want and have to keep manually uninstalling. Stop this AMD.,AMD,2025-12-10 14:21:39,300
Intel,ntam2kl,What is fsr redstone? and which games use it?,AMD,2025-12-10 14:32:55,85
Intel,ntak8pe,"I got a notification for the update in AMD Adrenalin Edition, but it does not appear in the actual install manager lol",AMD,2025-12-10 14:22:35,48
Intel,ntbp2n9,I just tested the release on four machines (76X&78XT/78X3D&79XTX/97X&9070XT/75F&76XT). Every system still suffers from crashing drivers when hardware-accelerated apps are used (Chrome/Discord/etc.).  Please fix. :),AMD,2025-12-10 17:49:10,24
Intel,ntanibq,so can I open adrenalin on this one with a rdna 2 igpu and rdna3 gpu or is it still broken like the last version,AMD,2025-12-10 14:41:05,20
Intel,ntalx8c,<--- Int8 rdna2 enjoyer,AMD,2025-12-10 14:32:04,81
Intel,ntaottt,did they fix enhanced sync and noise suppression yet,AMD,2025-12-10 14:48:21,35
Intel,ntayll4,Did this driver fix purple visual glitches with the RX 7700 XT? It's a known bug that appeared after the driver 25.4.1,AMD,2025-12-10 15:39:11,14
Intel,ntav0ai,"The ignorance by amd of Rx 7000 users is astounding tbh, but this is 2025 AMD not prior AMD where they would try to appease a larger user base.  It's going to make me rethink my loyalty for future gfx purchases",AMD,2025-12-10 15:20:57,51
Intel,ntam4ms,So we cant test path tracing performance yet on Cyberpunk? Lol,AMD,2025-12-10 14:33:14,31
Intel,ntbddly,"This is a very underwhelming update for RDNA 4 users I get that this technology needs to mature, but they should already be at a point where the implementation is across more wide array of games. My fallen RDNA 2 and RDNA 3 brothers will be remembered. The only reason AMD gpus are still relevant rn is price, nvidia tax is crazy. GG",AMD,2025-12-10 16:51:29,11
Intel,ntbtbc6,"Thanks for nothing again, AMD.  Signed, 7900 XTX user.",AMD,2025-12-10 18:09:31,27
Intel,ntanq1w,So is there any point to installing this if I'm on RDNA2 and don't have any of the issues that they fixed?,AMD,2025-12-10 14:42:16,20
Intel,ntboygm,This is the worst driver amd made 9060xt for me. 2 games instantly crashes. Indiana jones and silent hill 2. With this driver if you enable ray tracing game hangs and give error.i already report bugs in 25.12.1 and same with 25.11.1 and amd does nothing. every ray tracing titles works ok with 25.10.1 driver and this is bad. amd does not listen users anymore. anyone has any crashes happen like me?thanks...,AMD,2025-12-10 17:48:37,8
Intel,ntan1w5,Nothing on Oblivion Remastered crashing? Intermittent application crash or driver timeout on 9000 series when playing Battlefield 6?,AMD,2025-12-10 14:38:30,14
Intel,ntcc5fr,AMD Software still crashes randomly,AMD,2025-12-10 19:40:52,7
Intel,nti2mdh,#AmdNeverAgain Give Fsr4 on rdna3,AMD,2025-12-11 17:48:52,7
Intel,ntba2eq,New update new problems,AMD,2025-12-10 16:35:17,6
Intel,ntayron,"The adrenalin app just auto updated my 9070xt mid game, now my screen is black with no signal output to my monitor but my music is still playing lol. I waited for 10mins then I had to hard restart my computer for it to say the update failed",AMD,2025-12-10 15:40:02,11
Intel,ntamhuj,Pretty dissapointing ngl,AMD,2025-12-10 14:35:22,22
Intel,ntb58wr,Should I get the RTX 5070 ti or 5080 at msrp? I am currently selling my XTX after radio silent news about FSR 4 int 8 on it.,AMD,2025-12-10 16:11:45,22
Intel,ntak0ko,Everything is RDNA 4 exclusive? awesome /s  RIP finewine.,AMD,2025-12-10 14:21:19,58
Intel,ntb9myj,Please add the broken noise suppression to “Known Issues”.,AMD,2025-12-10 16:33:11,5
Intel,ntbbh4c,"If  this driver update keeps crashing my gpu im not leaving 25.9.2 for a while, im also starting to think about selling my gpu and get nvidea, and really black ops 7 why not a real game like cyberpunk i dont want to waste 70 euro for fifa with guns",AMD,2025-12-10 16:42:08,6
Intel,ntbih3y,"Can confirm on my 9060XT that Silent Hill 2 is still crashing and Avatar Frontiers of Pandora currently has a bug when FSR4 is enabled where the entire screen starts flashing like a strobe light, shadowy areas seem to trigger it. This is with both games fully patched & up to date.",AMD,2025-12-10 17:16:44,5
Intel,ntbq2n7,"Let me see - all the new ""Features"" will be available for Cyberpunk 2077 in at least 1 year time and ONLY with RDNA4 ??",AMD,2025-12-10 17:53:59,6
Intel,ntcf8iq,AMD NoiseSuppresion still broken. Since September!,AMD,2025-12-10 19:56:49,5
Intel,ntedkus,"Are pink artifacts fixed on RX 7700 xt, anyone ? It was bugged in 25.11.1 driver last month.",AMD,2025-12-11 02:20:15,5
Intel,ntb4cu0,Where‘s support for 7000 series? Wtf is this dead meat,AMD,2025-12-10 16:07:26,14
Intel,ntaofpr,I’m on a 6000 card is there literally no reason for me to download this,AMD,2025-12-10 14:46:12,19
Intel,ntasl1x,"all i want is to be able to capture clips in my games but for whatever reason amd either doesnt understand im in the game, recognizes the game wrong (battlefield 6 shows as elder scrolls online which i dont even have).",AMD,2025-12-10 15:08:16,5
Intel,ntdy3yt,It's december and still no FSR4 for vulkan.,AMD,2025-12-11 00:46:14,5
Intel,ntf1v9l,25.11.1 was dog water driver timeout city for me I'm just gonna assume this new one will also be the same.,AMD,2025-12-11 04:59:50,5
Intel,ntaql28,Is this worth updating to from 25.11.1  Is it more stable?,AMD,2025-12-10 14:57:42,6
Intel,ntc0jb7,"I had to downgrade to 25.9.1 to have some level of stability, can somebody confirm that the new driver is safe to upgrade to without it messing stuff up?",AMD,2025-12-10 18:44:11,7
Intel,ntawnis,Still no fsr 4 support for rdna3 🙄,AMD,2025-12-10 15:29:24,9
Intel,ntb91tu,"Guys calm down. RDNA3 being moved to maintenance mode is part of their new strategy, no longer ""Fine Wine"", the new approach is Stale Ale. That way their products remain DOA after launch and people won't keep them very long.",AMD,2025-12-10 16:30:16,13
Intel,ntatk4y,idk why I find it so funny that a specific Roblox game got called out in the patch notes,AMD,2025-12-10 15:13:24,3
Intel,ntavzqu,Did they fixed the amd noise supression not turning on?,AMD,2025-12-10 15:26:00,3
Intel,ntbglg2,"Anyone know why Cronos: The New Dawn has been showing [""FSR 4""](https://i.ibb.co/nqW2VMng/Cronos-The-New-Dawn-2025-12-04-02-28.png) for me on a 7900 XT for a few weeks? At first it was 3.1.  I know it can be modded in but this is on a new Windows 11 install and I haven't done any modding.",AMD,2025-12-10 17:07:23,3
Intel,ntbm1c9,"Looks like new chipset drivers, too.",AMD,2025-12-10 17:34:16,3
Intel,ntc41oy,"I thought the application freeze fix might have stopped monster hunter wilds from crashing on me but nope still does it (DXGI_ERROR_DEVICE_REMOVED,)",AMD,2025-12-10 19:00:46,3
Intel,ntcb4ur,/u/amd_vik are you aware of assetq corsa evo vr not working on AMD cards since 25.9.1 ? It displays the left and right eyes out of alignment and therefore fails to show a cohesive single image.,AMD,2025-12-10 19:35:42,3
Intel,ntcgpo5,so no fsr4 support on Vulcan still? this is getting ridiculous,AMD,2025-12-10 20:04:11,3
Intel,ntdhmve,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Thank fucking god.,AMD,2025-12-10 23:10:51,3
Intel,ntdxxbg,Still experiencing 100% gpu usage almost constantly as soon as you boot up BF6 on newer drivers after 25.10.1 and higher temps in general  I'm locking my FPS to 144 but the older drivers is showing better overall temps and less gpu usage for me 🤔  [https://imgur.com/a/ctbMCx7](https://imgur.com/a/ctbMCx7),AMD,2025-12-11 00:45:09,3
Intel,ntf7tyk,BF6 crashing after a few minutes in game with that driver on a 6800xt,AMD,2025-12-11 05:46:22,3
Intel,nth0425,"ever since 25.9.2 still same bug is present even now and now it causes even more problems because ML based FSR and FG fails when it happens: Adrenalin app just shuts down randomly even when idle, no errors, no driver timeout, no dx12 trimeout, just adrenalin itself gets shut down in random times. why wont you guys do something about it finally? Seriously its been so long now... im on 9070XT Steel Legend Dark Edition from ASRock, 80% of your users or more report the same issue FIX IT for the love of GOD. I tried everything hoping its on my side but windows reinstall, DDU and AMD cleanup app and fresh driver install nothing helped its still here",AMD,2025-12-11 14:36:29,3
Intel,ntsip7g,"Both 25.12.1 and 25.11.1 drivers have the same bug on RX 9060 XT. When my screen goes blank and later i wake up screen, i have two mouse cursors on the screen, until i launch some app and then will second, fake cursor disappear.",AMD,2025-12-13 09:49:20,3
Intel,ntaw89a,I hope this fixes the many crashes I've had since the last update...,AMD,2025-12-10 15:27:13,5
Intel,ntc9i8c,"Still enjoying the piss out of the 7900XTX on 25.9.2. It chews through everything I throw at it at the settings I choose, don't care about new driver releases unless a new game I want to play doesn't play well on whatever driver I currently have installed.",AMD,2025-12-10 19:27:35,5
Intel,ntb1a2q,"Even tho I have a 9070xt this is still so underwhelming… We waited 6 months and got basically nothing yet. Sorry for all rdna2, 3 users.  Fun fact: Its been years now that the adrenaline software cant be opened, the only fix ist to press win+p and select only main monitor. Than start it, than swap monitor profile again…   Definetly buying nvidia next time, not supporting this big company anymore, which is behind in every aspect. Image you just want to play alan wake 2 (looks beautiful).",AMD,2025-12-10 15:52:19,9
Intel,ntaqa2o,"ass. no support for rdna2/3, no new features for rdna2/3, rdna4 have only one game that support all of that, redstone framegen almost identical to 3.1 framegen, frame pacing still there.",AMD,2025-12-10 14:56:05,13
Intel,ntazsry,hardware unboxed tested it and frame facing is broken when amd frame gen is on sadly,AMD,2025-12-10 15:45:05,4
Intel,ntaqj5o,So the HDMI crashing issues should be fixed in this version yes?,AMD,2025-12-10 14:57:25,2
Intel,ntayomq,Any news on fixing the gpu vram leak issue on bf6? Sorry I’m lazing not reading the patch notes,AMD,2025-12-10 15:39:36,2
Intel,ntb8vq9,25.12.1 does not even install on my Minipc (780M) + 6650XT eGPU Setup.   I thought I might fix 25.11.X not opening in an eGPU Setup.   Guess I will be running 25.9.2 for another few Months.  God why something always break? I thought it would be better going all AMD for the eGPU setup.,AMD,2025-12-10 16:29:25,2
Intel,ntbdrmk,"Yeah I'll still be with 25.9.1 until the texture flickering is fixed in BF6, also instant replay just didn't work in 25.11.1 for me.",AMD,2025-12-10 16:53:24,2
Intel,ntbt5fb,Will this help Warzone not look so blurry on 7900xt? Game is unplayable,AMD,2025-12-10 18:08:44,2
Intel,ntc1hhd,So there seem to be two links - going through support>picking GPU(9070XT in this case) downloads the 25.21.1 win 11-b.exe file meanwhile going from this release note article it downloads the win11-c.exe . Any difference?,AMD,2025-12-10 18:48:42,2
Intel,ntc4frb,"im using 6800xt the driver page has the win11-a version and article have win11-c version. which one should i choose i really dont know and this ""different builds"" confusing a lot of people",AMD,2025-12-10 19:02:40,2
Intel,ntcb9km,"Genuine question, why all the hype and rush to release this today when it has just two games to showcase the benefits?",AMD,2025-12-10 19:36:22,2
Intel,ntcl3bs,Jesus how long has that Cyberpunk Pathtracing crash been in the known issues. It feels like it's been more then half a year.,AMD,2025-12-10 20:26:14,2
Intel,ntcrk7m,Installed with no issues,AMD,2025-12-10 20:58:08,2
Intel,ntdoxgx,"I can’t play Warzone because I can’t update my bios, there doesn’t seem to be a recent bios update available for my Acer Nitro 5, using Adrenaline. Anyone know if this will help?",AMD,2025-12-10 23:53:00,2
Intel,ntfskqf,Doesn't look like they fixed the bug with Enhanced Sync not working properly with Freesync.,AMD,2025-12-11 09:00:35,2
Intel,ntgkfzg,Any fix planned for 9070 users who cant enable Hardware Lumen on Oblivion Remastered? Game crashes as soon as we turn on the option.,AMD,2025-12-11 13:04:42,2
Intel,nthjjtu,Still no fix for Battlefield 6 for those with AMD 6800M GPU. I swear my next setup is going away from AMD if this is not resolved anytime soon.,AMD,2025-12-11 16:15:50,2
Intel,ntkinfb,u/AMD_Vik      In 2022 AMD made changes to OpenGL Driver. So since 2022 the extension gl\_ati\_fragment\_shader is missing in the driver. It cause problems in older games like Call of Duty 1 from year 2003. Stutter on some maps and broken water rendering because the games can't use the extension anymore.     Our Community is waiting since 3 years for a fix.,AMD,2025-12-12 01:38:24,2
Intel,ntn9tly,in black ops 7 only 25.9.2 driver work better even new 25.12.1 much worse fps drops,AMD,2025-12-12 14:01:19,2
Intel,ntp4ou0,Very unstable for me (7900XTX). Driver keeps crashing even when I'm just watching videos. Reverting to 25.11.1,AMD,2025-12-12 19:39:09,2
Intel,ntq9ysh,i just had to roll back to 25.9.2 because 25.12.1 kept crashing my system with poe2   even GGG straight up said don't use 25.10-25.12,AMD,2025-12-12 23:26:01,2
Intel,ntsszh2,"After observing you guys for a few days xD, 25.12.1 was installed along with new chipset driver on my system.  To my surprise, unlike previous 25.11.1, Adrenalin interface now runs properly with igpu enabled.  I need to test it out with real games, but for now, I've dodged instant roll back.  FYI, If you're using two or more GPUs, including igpu, on a single system with muti-monitor. Download the C package(1.65GB one including rdna1&2+3&4).",AMD,2025-12-13 11:33:00,2
Intel,nttlrcj,"NoUnfortunately, they don't work (( Random crashes remained + In some games, the inability to use frame generation through drivers was added (( Sad ( R5 3600 32gb ram Rx 7700 xt ) Rolled back to 25.9.1 everything works with it",AMD,2025-12-13 15:00:42,2
Intel,ntyj52n,"I had a very weird issue:     My PC would just crash when i did an Windows Defender Scan (only Full Scan, it worked fine with QuickScan or other programms like Malewarebytes) like the power was cut. I did a number of things even rollback the chipset driver but that didn't help. Then i rolled back to 25.9.1 + the newest chipset driver and everything worked fine again.   In case somebody had a similiar issue",AMD,2025-12-14 10:21:51,2
Intel,ntz35tj,"Anyone having problem with AMD overlay with this update? Somehow not showing at any game even if enabled, if I click to different monitor, it shows up. But when I click back to the game it disappear again.",AMD,2025-12-14 13:16:47,2
Intel,ntzh5jv,AMD Wattman settings don't apply for the first time they're set. They have to be changed and applied to a different setting and then to the desired one back and forth to get them to work. I use wattman to set my custom fan curve and it's been glitchy since 25.11.1.,AMD,2025-12-14 14:45:37,2
Intel,nu4w43f,Should I download the new driver version if I have 6800XT? There is nothing in the patch notes about this series... And if yes - why?,AMD,2025-12-15 10:40:37,2
Intel,nu8xc58,"Error code 182 for my AMD Radeon™ 780M integrated GPU on my Ryzen 7 8854HS CPU.  All other driver updates before 25.12.1 worked fine on my Lenovo Legion Slim 5 Gen 9, but this one says my GPU is incompatible, even though AMD's driver download page is providing [this download link](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe) to the installer:  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-8000-series/amd-ryzen-7-8845hs.html",AMD,2025-12-16 00:02:21,2
Intel,nuazy8e,7000 Series web browser glitch? and sound glitch? huh,AMD,2025-12-16 09:03:04,2
Intel,nwzds1t,"So I upgraded on a 6800xt and lost a lot of features like video recording, screenshotting, custom game profiles, and hotkeys. Is that intended?  Crazy to be missing hardware supported features",AMD,2025-12-31 21:37:22,2
Intel,ntb2bag,"Is it safe to update, 25.9.1 is stable for my 9070XT and causes zero crashes with the timeout bullshit from clock speeds going to 3300+ MHz",AMD,2025-12-10 15:57:21,4
Intel,ntapbf5,What does fsr Redstone means ?,AMD,2025-12-10 14:50:59,4
Intel,ntb3ek3,"Is this driver more stable than 25.11.1 it was causing driver time outs and i even got a blue screen. I rolled back to 25.9,2 and now im scared to update to this one lol",AMD,2025-12-10 16:02:45,2
Intel,ntc136m,These comments are all over the place is it better than 25.11.1 or not? 😂😂,AMD,2025-12-10 18:46:48,3
Intel,ntc1b9d,"So in short, still no support for 7000/6000 series, yipee",AMD,2025-12-10 18:47:54,4
Intel,ntfncpt,"Idk what happened but after this update my game crashed and then my PC crashed and when I turned it back on AMD Adrenaline disappeared from my PC, completely gone. What did you do lol.",AMD,2025-12-11 08:07:39,4
Intel,ntavbyt,For Sale: 7900 XTX - $50 OBO  I know these are no longer desirable due to being left in the dust by AMD after only a few months of real support but hopefully it will be at least a good paper weight for someone.,AMD,2025-12-10 15:22:38,3
Intel,ntb0knq,So now driver frame gen is gone? Unless the game specifically supports it? And the overlay as well? Both are completely gone now after the update...,AMD,2025-12-10 15:48:53,2
Intel,ntb1r9y,What about the fixes for the 7900xtx crashing all the time?,AMD,2025-12-10 15:54:38,2
Intel,ntim4nj,"«#AmdNeverAgain” Where’s the Christmas gift in the form of FSR 4 for RDNA 3? In the new 2026 year, it might be time to think about switching to Nvidia.",AMD,2025-12-11 19:23:42,2
Intel,ntbtv4u,"Toujours pas de FSR4 pour les séries 7000 ? C’est mort. Pour ma part, je n’achèterai plus de cartes AMD. Si Nvidia continue à proposer son DLSS pour les anciennes cartes, alors mon choix est fait.",AMD,2025-12-10 18:12:11,3
Intel,ntaitkk,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 14:14:29,1
Intel,ntaqe06,"Downloads ""whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe"" for 9070XT, ""whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe"" for 5700 XT  What does it mean?",AMD,2025-12-10 14:56:40,1
Intel,ntatgqs,It took a while for DLSS 4 to get implemented in a good way on 40 series cards too but a version made it there. Give it time. Now if they can just start prodding developers to incorporate that as well it’ll be worth it. Not enough games yet but here’s hoping!,AMD,2025-12-10 15:12:54,1
Intel,ntau6wp,Any update the in fact that and adrenaline software is not working when second monitor is connected? Especially using iGPU for second monitor ?,AMD,2025-12-10 15:16:42,1
Intel,ntauct1,Omg I think they fixed the LG oled tv reboot bug,AMD,2025-12-10 15:17:33,1
Intel,ntb6asm,Should i install it directly or should I use AMD cleanup utility first?,AMD,2025-12-10 16:16:51,1
Intel,ntb80pv,some one have problem with instaling?,AMD,2025-12-10 16:25:13,1
Intel,ntb8lei,Does this fix the driver timeouts that were happening with Edge? I had to revert the November update because of that problem,AMD,2025-12-10 16:28:00,1
Intel,ntb9m3i,Any fix or still need iGPU disabled for 7000 and 9000 cards?,AMD,2025-12-10 16:33:04,1
Intel,ntb9mdl,The update is still not showing up in install manager,AMD,2025-12-10 16:33:06,1
Intel,ntbb7t2,Honestly this software was the bane of my card for the longest time. Not having it anymore stopped so many weird bugs and crashes.,AMD,2025-12-10 16:40:52,1
Intel,ntbbs61,Does AMD's Instant Replay record still bug out?,AMD,2025-12-10 16:43:39,1
Intel,ntbd79c,Anyone tried the new fsr redstone yet? I am hoping for a big improvement over the old fsr,AMD,2025-12-10 16:50:36,1
Intel,ntbif1z,do you guys remove the old drivers before you install new ones? or just install ontop,AMD,2025-12-10 17:16:28,1
Intel,ntbp8r8,The path tracing crash STILL on Cyberpunk is absolutely wild to me. Finally AMD has a card capable of playable raytracing but we can't use it on the 'Crysis' of modern times to even test it out.,AMD,2025-12-10 17:49:59,1
Intel,ntbtqi7,Adrenalin doesn't show this update for me yet lol,AMD,2025-12-10 18:11:33,1
Intel,ntbw2oz,"> Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Glad for this, it was annoying that we were stuck in 25.9.2",AMD,2025-12-10 18:22:46,1
Intel,ntc4icg,Are the issues with SecondLife fixed? Last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:03:00,1
Intel,ntc4u1h,9060 non XT 8GB can do the math 7900 XTX Nintendont,AMD,2025-12-10 19:04:36,1
Intel,ntcbbg1,"I’m at work, so I cannot check for myself: does this fix the constant crashing in Oblivion when hardware lumen is turned on?",AMD,2025-12-10 19:36:37,1
Intel,ntcdlil,Has anyone tested Marvel Rivals on 25.12.1 version of the driver? The only stable driver that works without crashing on that game is the 25.8.1 version.,AMD,2025-12-10 19:48:14,1
Intel,ntchv47,Adrenalin Panel not showing bug still present… :-((((,AMD,2025-12-10 20:10:02,1
Intel,ntcyy65,Am I the only one who doesn't have the new option in the drivers for frame generation with a 9070 XT?  https://i.redd.it/7r86hslx3g6g1.gif,AMD,2025-12-10 21:34:34,1
Intel,ntd63ea,"why are there 2 versions, b and c, 1.65Gb and 991Mb, release notes and through the support page, and is it stable or shall i just keep 25.8.1 as any other seems to crash call of duty, regular, other games seem fine,  ryzen 9 7950x3d/rx7900xtx",AMD,2025-12-10 22:09:16,1
Intel,ntdfkjs,Did it fix the god of war 2018 checkered shadows?,AMD,2025-12-10 22:59:20,1
Intel,ntdjg2j,"I spent all this time with 25.9.2 on my 9060xt because the following ones were disgusting to me, I will give this new update a chance and let's hope everything improves a little!!",AMD,2025-12-10 23:21:02,1
Intel,nteannh,Arc raiders crashes are fixed or not?,AMD,2025-12-11 02:02:44,1
Intel,ntek6ze,"Makes my 9070 XT to constant run on 100% load in bf6 no matter if i play or sit in the menu. Cause device hung, graphic glitches and high temps.   Same with all drivers above 25.9.   25.9.1 works flawless with no errors and the load varies depending on the scenery as it should.",AMD,2025-12-11 03:00:09,1
Intel,nteshht,Noticing in Hogwarts Legacy with the new FSR and FG enabled over a period of like 30 seconds my 9070XT will go from \~250W used and 200 FPS and then drop down to say 120W used and 90 FPS and then after a short period go back up again. With FG disable it stays consistent 140 FPS-ish,AMD,2025-12-11 03:54:05,1
Intel,ntfd9ta,"FYI for ""Driver Only"" guys, 25.12.1 still have an issue to install this option.  l've open ticket to support team for last 2 versions. but I can't follow their request to observe the issue.  Don't know how long to keep using extracted file method. lol  Will see how 25.12.1 ""driver only"" perform.",AMD,2025-12-11 06:33:01,1
Intel,ntfu45p,oh nice! they fixed the FSR4 Quality Presets artifact issue,AMD,2025-12-11 09:16:24,1
Intel,ntg07nz,"When AMD finished Orange, Yellow Green, PurpleStone, can we unlock FSR Infinity?",AMD,2025-12-11 10:18:18,1
Intel,ntgsz5x,"Is it worth updating to this latest driver? I am not planning to use frame gen, is the image quality better or are there any fps improvements in games?",AMD,2025-12-11 13:56:05,1
Intel,nti5u4n,"Updated to 25.12.1 now, before I was on 25.8.1, have a Rx 6800 XT and a Ryzen 7 7700X. Also updated my Chipset-Driver today. Haven't testet much yet, played now for like 1 hour Space Marines 2, watched some Youtube vids since I updated. So far looks ok. Only thing that worried me first was that I found in my Reliability History, 2 critical entries of LiveKernelEvents of code 1a8. But these were written down by Windows on the time, while I was updating my driver. We will see, if anything happens I will keep you updated.",AMD,2025-12-11 18:04:23,1
Intel,ntigzns,"Despite the device ID-based driver update blocking set in August, it has worked until now. The windows tried to install some driver on the 6700XT just now, and unfortunately, it also replaced the software itself somehow. threw an error message too.  Manual update would not go through unless i removed the driver update block.   What a sad situation.",AMD,2025-12-11 18:58:17,1
Intel,ntnwqsn,"Anyone else has problems with CS2/Fortnite? Started happening after i updated drivers to 25.11 My whole PC would randomly freeze for like a minute with the ""AMD software detected that a driver timeout has occurred"" error. Once the PC unfreezes i must kill the game from task manager.",AMD,2025-12-12 16:00:37,1
Intel,nto9zy8,Does it fix the arc raiders dxgi crash of the previous driver?,AMD,2025-12-12 17:05:14,1
Intel,ntpp1wj,"How do I downgrade from this driver?   I’ve tried four different older drivers and all of them give me error 182 – GPU is not supported (RX 9070 XT) during install.   I’ve already used DDU and the AMD Cleanup Utility, but the only driver I can install successfully is 25.12.1.",AMD,2025-12-12 21:27:18,1
Intel,ntqd768,pc started to crash 7900xtx... reverted to 25.11.1,AMD,2025-12-12 23:45:59,1
Intel,ntw41n8,Hi me and other people I know. Also forums and Facebook pages . Have had an issue with the frame gen after 25.9.2 . When they released new features we have all had issues where its drops fps and is completely unplayable. Has this been fixed in 25.12.1 I have 7900 xtx 7800x3d. Friend has 9070xt 9800x3d Both have issues. And im on windows 10 he's on windows 11. I used ddu and tried all settings on frame gen and other settings to fix it. Not to mention the drivers where stutters and lower fps without frame gen. Thanks,AMD,2025-12-13 23:14:13,1
Intel,ntwqp19,"When I enable V-Sync in the game, I experience lag; it only runs smoothly with V-Sync enabled when I also activate the performance overlay. This problem has existed since driver version 25.11.1.",AMD,2025-12-14 01:37:28,1
Intel,ntytwdj,"I have a second card from amd. And both cards have driver problems. Now I have an rx 9070 xt oc. I don't do any undervolting. Everything is at factory settings including the bios. I had 4 driver failures in 7 hours. What good is FSR if the driver doesn't work? It would be good if you finally solved this problem. I can stand it for a while, but if it continues like this, I'm leaving AMD.",AMD,2025-12-14 12:03:18,1
Intel,nu0egj3,"Wish they would acknowledge the bug where turning on GPU scaling and integer scaling adds more input delay, so for example the mouse movement will feel sluggish.  Been having this issue for 3 months now since nya bought a a 9070 XT",AMD,2025-12-14 17:38:40,1
Intel,nu0h263,"On the RX 7600S graphics card, Adrenalin does not launch at all, and during installation it removed the driver PCIVEN_1022&DEV_15E2&SUBSYS_15131043&REV_60.",AMD,2025-12-14 17:51:39,1
Intel,nu2b8e5,"How are those with a Cezanne CPU supposed to install this?  Selecting the 5750G from the drivers download page offers 25.21.1, yet none of the 3 variants of the installer support it.  * whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe (Vega, supposedly?) - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe (combined? ""Systems with RDNA series graphics products"") - nope  Each of them return Error 182.  Even the minimal web installer, amd-software-adrenalin-edition-25.12.1-minimalsetup-251207_web.exe, only offers 25.8.1.  VEN_1002&DEV_1638 is nowhere to be found in the .inf for any of the 25.21.1 variants.",AMD,2025-12-14 23:19:40,1
Intel,nu3psfe,I'm still hesistant to upgrade on this driver until they resolve these driver timeouts hell I'm even on 25.9.1 still experiences time to time TDR's.,AMD,2025-12-15 04:22:56,1
Intel,nu4kg83,"Is it worth for my 9060XT to go from 25.11 to the latest, Im having some problems where ghost of tsushima crashes.",AMD,2025-12-15 08:42:46,1
Intel,nu51zrq,Driver is making valorant run like crap for me idk why .,AMD,2025-12-15 11:34:40,1
Intel,nv057ke,im using an rx6600 and up until today i was fine avg 200fps on r6 today the game says its at 22 usaeg when it avgs 1-4 and now it has major fps drops/tears,AMD,2025-12-20 08:25:06,1
Intel,nv2wvfi,This driver constantly crashes call of duty for me. Whatever windows update installs(which seems to be 25.10. something) is the most stable there is. 9070XT.,AMD,2025-12-20 19:41:18,1
Intel,nvf8wml,"Parabéns, fiz a atualização para 25.12.1 e agora não consigo jogar nada sem travamentos, além do google estar lento",AMD,2025-12-22 19:52:06,1
Intel,nvtvg24,Anyone see if this fixes the issue of the graphics sliders not working at all and being stuck?,AMD,2025-12-25 05:17:11,1
Intel,nw7hqmm,Still crashes. They will never fix it. Just buy nvidia or intel.,AMD,2025-12-27 16:03:55,1
Intel,nw7j7uy,"u/AMD_Vik It says ""Intermittent application freeze when using the in-game Radeon™ Overlay."" in fixed issues but I've actually had my whole system lock up because of what seemed to be adrenalin having issues with the performance overlay....  I noticed one thing that pointed towards the overlay specifically: I was going through Adrenalin and when I was on the recording tab I switched to performance; it seemed like Adrenalin froze so I clicked Smart Tech. to see if it would respond.  Initially it didn't, before eventually swapping to the smart technology screen. I then went back to the record tab and tried again: same results.  That's about all I've got for specific steps. I closed Adrenalin and went back to doing whatever and I noticed my fans turned on and like two minutes later when I went to close my browser my cursor stopped before I got to the corner of my screen and I needed to hard power down my system.  Not sure if this is at all related to that issue. But i had it happen on the last driver as well, and came here trying to see if there was a known issue...",AMD,2025-12-27 16:11:24,1
Intel,nxehl3e,Any chance to support VR HP reverb G2 (WMR) 60hz mode with Oasis driver? I'm locked in win10.,AMD,2026-01-03 07:42:05,1
Intel,nxhczy3,"Indiana Jones crashing every 5 minutes, cant complete the game. Its just freezes and the PC barely responsive with these Timeout messages.  9070 with 5800x3d",AMD,2026-01-03 18:31:17,1
Intel,ntakvhf,FSR Redstone support? Will my minecraft machine run faster now?,AMD,2025-12-10 14:26:09,0
Intel,ntap40e,Gonna be able to play modern titles on my HD5750 thanks to redstone !,AMD,2025-12-10 14:49:52,1
Intel,ntdi8fj,All that build up for dog water. Built my first PC in March and went with a 9070xt full of hope. I'm beginning to understand why AMD is so widely despised.,AMD,2025-12-10 23:14:14,1
Intel,ntclu0p,Windows just installed the June   update from AMD. The fck is this,AMD,2025-12-10 20:29:51,1
Intel,ntcrpsp,I'm not seeing the new update in AMD Install Manager,AMD,2025-12-10 20:58:55,1
Intel,ntd2bv7,so in 2028 10 games will have it like FSR4 XD,AMD,2025-12-10 21:50:50,1
Intel,nte3gul,Anyone knows if it fixed the crashes with Oblivion Remastered and Silent Hill?,AMD,2025-12-11 01:18:47,1
Intel,ntf7sz1,"""Intermittent application crash or driver timeout may be observed""  This is not an issue tied to a few games.... is a wide issue for me with a 9060 XT whenever i rise my monitor refresh rate.",AMD,2025-12-11 05:46:09,1
Intel,ntaqgkm,"Ray Caching in 40K?  Not sure how they got this to work on the tabletop in real life but sounds awesome  In all seriousness there are a large number of games in the Warhammer 40K universe, any chance they are saying which one?  Space Marine 2 Darktide Battlesector   Etc",AMD,2025-12-10 14:57:02,50
Intel,ntavwoz,"so pretty much nothing for today, shrug...",AMD,2025-12-10 15:25:34,10
Intel,ntapnnp,Is there a partial list of the 40 games with the new frame gen? Is it something different from the fg we already have?,AMD,2025-12-10 14:52:46,9
Intel,ntbyc9u,There are well over 100 warhammer 40k games. Did they not specify?,AMD,2025-12-10 18:33:41,1
Intel,ntam71a,they wont,AMD,2025-12-10 14:33:37,53
Intel,ntf1fzq,"It's so annoying.  I would keep it if it didn't constantly pop up trying to get me to install ""AMD Chat"" and ""AMD Privacy View"".  I don't want your shovelware AMD, take a hint.",AMD,2025-12-11 04:56:41,20
Intel,ntbob9s,"There should be an option during install to exclude it, it can't be that hard to do. Same as you, u/MihawkBeatsRoger , I also uninstall it afterwards.       Notifying u/AMD_Vik",AMD,2025-12-10 17:45:29,18
Intel,ntasnol,"This.   Why I took it out are my own reasons and quite frankly, irrelevant. It's my PC and I don't want it. So please AMD, listen to me and keep it off.",AMD,2025-12-10 15:08:39,22
Intel,ntb2rjk,"Focus on serious matters, this is a joke. If you do not want it feel free to install the driver only version, and be happy u have that choice. If you want the full features of adrenalin, well install manager is one of them.",AMD,2025-12-10 15:59:33,-2
Intel,ntapviv,It's a rebranding of the entire FSR ecosystem. What's new today is machine learning enhanced frame generation for RDNA4 cards. You can enable it in the driver for any game with FSR 3.1.4 or newer.,AMD,2025-12-10 14:53:56,130
Intel,ntb2cv7,It adds denoising for Path tracing. In theory it should look way better now,AMD,2025-12-10 15:57:34,7
Intel,ntap7sr,All the games that don't use bluestone,AMD,2025-12-10 14:50:26,24
Intel,ntbfl85,"Only one , the new call of duty ATM. So if you enjoy shitty games , have at it",AMD,2025-12-10 17:02:21,5
Intel,ntalgrc,"Same, and I'm still on the October drivers",AMD,2025-12-10 14:29:29,19
Intel,ntaosq5,You can download it from the website. The app release notification always lags behind the site. This is nothing new.,AMD,2025-12-10 14:48:11,9
Intel,ntcmrfi,9070xt i see brave or discord freezing and lagging when watching a YouTube video still. I dont understand how hardware acceleration bug hasn't been fixed yet. Wtf are they doing.,AMD,2025-12-10 20:34:27,13
Intel,ntcuvcq,Yup same here. Had to roll back to October to fix again,AMD,2025-12-10 21:14:37,7
Intel,ntem0sw,25.9.1 works on my 9070 XT. Everything after that is a mess for me,AMD,2025-12-11 03:11:43,7
Intel,ntfe4wd,Tagging u/AMD_Vik  so they are aware of the issues.       I encountered the same problems on my 6800xt. Figma on chrome is causing random BOSD. The system will just restart without notice. Every single web app seems unstable on my system and memory usage is all over the place. Rolling back to 25.9.1 doesn't fix everything but it eliminates 70% of the issues..,AMD,2025-12-11 06:40:43,4
Intel,ntf2h1v,Oh well. :/  Funny thing is I rebooted my PC again for a Windows update. The first thing that greeted me after opening a web browser was the driver giving up the ghost.  On 25.11.1.,AMD,2025-12-11 05:04:26,2
Intel,nvesl6s,Ive been wondering what this seemingly random crashing has been. Thanks for this comment!,AMD,2025-12-22 18:30:18,2
Intel,ntgomie,"9800x3d, 6950xt, no issue with either chrome or discord or firefox with hardware accelerated set",AMD,2025-12-11 13:30:30,1
Intel,ntkqfzv,"Me too.  Installed 25.12.1, whenever I use YouTube in Full Screen, the whole system freezes, while the sound is still audible, then I have to hard-restart my PC. Happened three times, decided to downgrade to 25.11.1 again.",AMD,2025-12-12 02:24:44,1
Intel,ntaon6n,"This should be fixed, I'm not sure why it was omitted from the release notes",AMD,2025-12-10 14:47:21,29
Intel,ntbfx3u,<--- inte 8 rdna3 enjoyer,AMD,2025-12-10 17:04:01,36
Intel,ntbh75k,"How do I set this up, can't find any info",AMD,2025-12-10 17:10:24,1
Intel,ntaukfz,"I can't speak on enhanced sync, but noise suppression is still busted and not working =/",AMD,2025-12-10 15:18:39,17
Intel,ntarxtz,"I'm piggybacking, because I need that info too",AMD,2025-12-10 15:04:52,4
Intel,nwscpi6,"I can't seem to keep framerates under control in a lot of games, generally smaller simpler games, with the new 9070xt. Enhanced sync, vsync, chill, boost, whatever I do I'm still wondering why my pc is at 100% gpu, 600fps, and 300w power draw playing something like minecraft or geometry dash.  Even with a 144hz display. I'd be happy locked at 60 even.",AMD,2025-12-30 19:52:48,1
Intel,ntb6txh,Ok I thought I was the only one having the enhanced sync issue because no one replied to any of my posts about it. I use it because then I can lock my fps to 120 (on a 4K OLED TV) and use enhanced sync instead of Vsync because of the screen tearing when locking to 120. Now I have to do the frame lock to 117 which is fine but just annoying me I'm missing out on 3 fps lol it was causing issues in a few games where it would stutter like crazy and it all came down to enhanced sync. I don't use noise suppression so I don't know what's up with that.,AMD,2025-12-10 16:19:26,1
Intel,ntbjznn,"Been using it for a few hours with the 7900XTX, so far so good.   Hopefully it's 100% fixed.",AMD,2025-12-10 17:24:13,6
Intel,ntbqeln,I hope they fixed it. I will test it now,AMD,2025-12-10 17:55:35,3
Intel,nteci65,"Did the typical test that I usually do and it didn't show up for me and I'm on the RX 7700XT as well. So hopefully, it's fixed.",AMD,2025-12-11 02:13:47,2
Intel,ntb5cx9,"AMD stopped giving a shit about it's fans once the company was saved and they started raking in the money. The change in tone was clear as day. That said, I'll still buy their GPUs because I hate Nvidia far more and I don't see that changing.",AMD,2025-12-10 16:12:17,26
Intel,ntch9q3,"yeah my next one will be Nvidia, better features, better performance espacialy with RT/PT   And apperently longer support... and AMD cards in a simmilar performacne bracket don't even cost THAT much less sooo.... jeah I am mad aswell",AMD,2025-12-10 20:07:01,16
Intel,ntk9244,"I agree. AMD has shown poor support for 7000 series owners. If there was a FSR4 int8 leak, AMD should officially release FSR4 for 7000 series owners.  I bought my 7800xt only 2 years ago before RDNA4 cards came out.  Nvidia provides DLSS4 upscaling to their older generations like rtx3000 series",AMD,2025-12-12 00:39:32,3
Intel,nw3y7cj,"Your system is almost exactly like mine, did you also have crashing problems while having the Xbox Gamebar DVR feature turned on? I would have constant driver timeouts until I turned it off.",AMD,2025-12-27 00:12:39,1
Intel,ntd2msi,looking back rn i think it wasnt worth the 100 dollars gain i gotwhen my 7900xt does consume more than rtx 4070ti and i do have shit features even the antilag+ scam that was one of the main reason i bought the GPU isnt here anymore .,AMD,2025-12-10 21:52:20,1
Intel,ntffgd0,And just like that comment and user deleted themselves 😂,AMD,2025-12-11 06:52:35,1
Intel,ntaoqmx,"If you're referring to the app crashes with RTPT reflections enabled, we're working with CDPR on a fix",AMD,2025-12-10 14:47:52,56
Intel,ntcukk4,Signed /another 7900xtx user,AMD,2025-12-10 21:13:09,15
Intel,nu3780v,"I came over from NVDA last March, bought a 7900xtx, RMAd it a few weeks ago due to pink/purple pixelation that would randomly happen. Now it's non stop driver timeouts and random performance issues every time I boot my PC or games. I am never buying another AMD card. I'd rather get ripped off by NVDA and not have constant headaches.",AMD,2025-12-15 02:23:41,1
Intel,nugitp7,"Which driver are you currently on? I'm just curious; personally, I'm on 25.9.2, and surprisingly, I have 0 problems, unlike with previous versions. Should I try 25.12.1?",AMD,2025-12-17 04:25:49,1
Intel,ntapar1,"Nope. Generally if the driver does not massively increases performance in some game, or you don't have any issues or the issue you have isn't fixed, then it's not worth updating, unless there is some new feature you want.    I reverted back to 25.9.1 (from the top of my head) because with any newer driver BF6 crashes randomly, and neither DICE nor AMD seem to give a damn about it.    And before someone asks, I tried any other fix on the internet for Battlefield and nothing else worked.",AMD,2025-12-10 14:50:52,18
Intel,nte7fbw,Same here. Anything above 25.9.2 crashes ray tracing games like Silent Hill  2 and Oblivion Remastered.   Ihr never had a more crash prone GPU than the 9070XT.,AMD,2025-12-11 01:43:09,3
Intel,ntc8k09,"Try this, taken from another comment branch https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 19:22:53,1
Intel,ntissbw,Might potentially be fixed by a recent Windows update?  24H2 (and an earlier mini-patch that included this) apparently resolved a lot of crashing for folks.  See [here](https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/),AMD,2025-12-11 19:57:19,2
Intel,ntaohu1,"Yeah I was really hoping they'd have got buy in from a decent number of devs with updates to big RT showcase games like Indiana Jones, Alan Wake 2, Cyberpunk, etc. But Black Ops 7 and Warhammer 40K... and that's it (for the RT features)?",AMD,2025-12-10 14:46:32,9
Intel,ntb73f1,5070ti fs  basically a better 9070xt,AMD,2025-12-10 16:20:42,13
Intel,ntcro7s,"I’d wait on 5080ti with more VRAM but these are going to be obscenely expensive knowing nvidia + current RAM prices. Both 5070ti or 5080 are more of a sidegrade than upgrade, not worth the hassle IMO.",AMD,2025-12-10 20:58:42,2
Intel,nted84w,Get a 5070ti. I never thought I would say that. But this is what is is.. 9 months after release and the drivers are still D.S.,AMD,2025-12-11 02:18:08,2
Intel,ntb6h5d,What about a secondhand 5070ti?,AMD,2025-12-10 16:17:43,3
Intel,ntbx8qa,"I mean, I wouldn't get either. 5070 TI is a sidegrade from the XTX, and 5080 is only slightly better. DLSS and RT would be the only reason.",AMD,2025-12-10 18:28:25,2
Intel,ntbzcmw,Sidegrading for an upscaler sounds like a joke.,AMD,2025-12-10 18:38:32,2
Intel,ntam4ba,"I think Linux developers are doing some experiments As of now, FSR 4 (FidelityFX Super Resolution 4) does not officially support RDNA 2 or RDNA 3 GPUs, even on Linux. However, thanks to Develer’s work on VKD3D-Proton 3.0, there is partial and unofficial support for RDNA 3 under specific conditions.  RDNA 3: Partial Support via Develer’s VKD3D-Proton  - Develer’s VKD3D-Proton 3.0 includes support for FP8 (8-bit floating point), which is required for FSR 4. - This means RDNA 3 GPUs (like RX 7600, 7900 XT/XTX) can run FSR 4 in some games via Proton, even though AMD doesn’t officially enable it. - Global override toggles in AMD’s 25.9.1 driver can bypass the FSR 4 whitelist, allowing it to run in FSR 3.1-compatible games.  I hope they succed it will be a slap in the face.",AMD,2025-12-10 14:33:11,26
Intel,ntakqc7,This has been announced for months.,AMD,2025-12-10 14:25:21,25
Intel,ntazxem,Yeah AMD refusing to port features to any card released before the 9 series makes supporting them really hard.,AMD,2025-12-10 15:45:42,8
Intel,ntakt3q,Say thanks they haven't demoted 7000 series to only game drivers,AMD,2025-12-10 14:25:47,8
Intel,ntamwc6,"Your best case is your RX 7900 turning into Balsamico, whatever that means.",AMD,2025-12-10 14:37:39,1
Intel,nte0i5m,"Its because RDNA 4 added hardware that 3 and 2 don't have. Now before I get kicked to death by angry people, there is a version of FSR Redstone that uses and INT8 path that is compatible and will work on 2 and 3, however that has not been launched today and AMD have not confirmed it will be.   That isn't to say they won't do it, but right now it's not been announced. Perhaps there will be enough noise to get AMD to change their mind or it might be that they want to get it out on their latest cards first before complicating matters with older RDNA support.  Only time will tell",AMD,2025-12-11 01:00:33,1
Intel,ntaoydb,"Bro the AI accelerators completely got revamped, upscaling technique isn't usually the indicator for 'fine wine', it is when non-upscaling raw performance numbers improve.",AMD,2025-12-10 14:49:01,2
Intel,ntf8us3,Same boat here. Tired of trying.,AMD,2025-12-11 05:54:50,1
Intel,nte6mdt,Thanks for testing. Have you perhaps tested Oblivion Remastered?,AMD,2025-12-11 01:38:14,1
Intel,ntf8lpl,Finally fixed! It's a christmas miracle!!!,AMD,2025-12-11 05:52:44,7
Intel,ntf2wry,I regret getting this 7800xt,AMD,2025-12-11 05:07:41,2
Intel,ntazn6o,Any card released prior to the 9 series.  Amd could give 2 shits as they chase the AI bubble (jokes on them if I was an exec I'd double down on the consumer market to insulate from the impending bubble burst),AMD,2025-12-10 15:44:19,17
Intel,ntaw82d,sadly,AMD,2025-12-10 15:27:12,5
Intel,ntedpok,"Yep, I go back between 23.9.1 and 25.9.2. I couldn't be happier.",AMD,2025-12-11 02:21:03,2
Intel,ntnjpo8,"If it's any consolation, I was on an NVidia card for 2+ years where I wasn't getting the DLSS updates. Then they actively removed features when they went to the NVIDIA app.  Looking at AMD's roadmap, RDNA4 looks like a stopgap anyway until RDNA5 (prob will be called UDNA?) comes out. So in another year and a half I'll be in the same situation with my 9060XT.",AMD,2025-12-12 14:55:34,2
Intel,ntbhjqe,"Use OBS, replay buffer",AMD,2025-12-10 17:12:09,3
Intel,ntg1daf,Was just thinking of giving a shot for Indiana Jones and the Great Circle - I guess not anymore since FSR4 doesn't work with it..,AMD,2025-12-11 10:29:39,2
Intel,nuji470,"That was a terrible driver for me also. New one has been night and day improvement, give it a shot.",AMD,2025-12-17 17:12:44,1
Intel,ntasabf,"Microsoft had bugs also causing hanging crashes. Everyone loves to blame GPU drivers immediately, but check this out:  https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 15:06:41,21
Intel,ntars3b,I also want to know this.,AMD,2025-12-10 15:04:00,3
Intel,ntasr85,"I'm wondering the same thing, 25.11.1 is still the most stable for me!",AMD,2025-12-10 15:09:10,3
Intel,ntbzovt,Wondering too. I bumped back down from 25.11.1 because it was unstable on my machine.,AMD,2025-12-10 18:40:09,2
Intel,ntf7xwz,Stay on 25.11.1 if you are on RDNA 1 or 2,AMD,2025-12-11 05:47:16,2
Intel,ntgynxw,squash hard-to-find sharp reach memorize fade husky divide subsequent plough   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:28:31,1
Intel,ntlwtqy,"~~It messed up my audio, now everything sounds 8-bit. If you're on RDNA4, avoid this update.~~  EDIT: it's not the drivers, after much tinkering I was about to deduce that it was my monitor. So it should be ok to update",AMD,2025-12-12 07:15:44,1
Intel,ntelprh,I can't use anything above 25.9.1 on my 9070 XT,AMD,2025-12-11 03:09:46,2
Intel,nth7dkb,forget it they gave u the middle finger move on fuck both amd and nvidia,AMD,2025-12-11 15:15:41,3
Intel,ntb6lhi,won’t happen,AMD,2025-12-10 16:18:18,4
Intel,ntbt1wv,"Fine wine is only a thing for very few and specifics types of wine, typical wine still goes bad over time.",AMD,2025-12-10 18:08:16,4
Intel,ntbeh2e,What is the source for this or is it trust me bro?,AMD,2025-12-10 16:56:49,4
Intel,ntecwqr,They should just remove this feature. It never worked from day 1..,AMD,2025-12-11 02:16:12,1
Intel,ntbm91h,what is the difference,AMD,2025-12-10 17:35:20,1
Intel,ntcnmrm,"Can you tell me if this is also applicable to 25.12.1? There are several (frustratingly unlisted) VR-specific fixes aligned, one of them closely relates to what you've just described",AMD,2025-12-10 20:38:48,2
Intel,ntekn1e,Same here. 25.9.1 makes my problems go away,AMD,2025-12-11 03:02:56,1
Intel,ntfr6xt,Same for my 9070 XT. Device hung error,AMD,2025-12-11 08:46:36,3
Intel,nu0gqvz,"Thanks for reporting, had that once with 25.11.1 + 9070XT (W10) before reverting to 25.9.1 (since then, it never reappeared).",AMD,2025-12-14 17:50:07,1
Intel,ntb6g84,i think your sorry should extend to people with rdna4 cards because this is pretty underwhelming,AMD,2025-12-10 16:17:35,1
Intel,ntoma7o,Do you get a firmware update pop up? Is this one?  https://i.redd.it/w46j86mnct6g1.gif,AMD,2025-12-12 18:06:40,1
Intel,ntmvi3j,"I'm familiar with this impacting United Offensive, I don't believe we're reintroducing this old vendor specific extension, however. I do have a ticket for the performance issues though; I don't believe this is related to the missing extension.",AMD,2025-12-12 12:32:54,1
Intel,nu65nki,"Tested for 2 days(1day and 22hrs uptime)  No crash, No BSOD for me so far. Nothing strange.  MS Edge, Google Chrome video playback, youtube...etc all play nice while gaming on main monitor.  Diablo 4, MSFS 2024, Doom dark age, Forever winter(UE5), Witchfire(UE4)...etc All run fine.  Lossless scaling runs fine on spicy vids to all of the above games xD  HWinfo64 and MSI Afterburner, RTSS all run as they should.  (Win11 25H2 uptodate, X670E, igpu(98x3d)+7900xtx+6400 3gpus, 2 monitors, hybrid mode)  Edit) rx 6800 + r7 7700x on win11 25H2, X670E, Single monitor, igpu-disabled -> runs fine.  rx 6700xt + i7 8700k on win11 25H2, Z370, Single monitor, igpu-disabled -> seems good.",AMD,2025-12-15 15:39:46,1
Intel,nuizppc,"25.11.1 had pink artifacts glitch on chromium browsers with 7700 xt but i installed 25.12.1 yesterday and no issue so far, i did not see artifact pink glitches or sound issue so far ?",AMD,2025-12-17 15:42:58,3
Intel,ntemk82,My 9070 XT hate every driver above 25.9.1,AMD,2025-12-11 03:15:11,1
Intel,ntcw96j,I updated to this driver and immediately got a BSOD. Rolled back to October 25.10.2 again,AMD,2025-12-10 21:21:24,3
Intel,ntgzi2n,offer steep theory scale straight obtainable physical ad hoc selective test   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:33:06,1
Intel,ntc7ylk,thats what I wonder too! Is it more stable??,AMD,2025-12-10 19:19:58,3
Intel,ntb2i9s,haha r u fr,AMD,2025-12-10 15:58:18,7
Intel,nteeogf,la même. C'est scandaleux,AMD,2025-12-11 02:26:49,2
Intel,ntbcqw3,"Means that they've created separate driver packages tailored for the specific gens (A rdna1/2, B for RDNA3/4, C - combined fat package that contains both drivers for systems that might have both gens on the same machine (igpu + dgpu) )",AMD,2025-12-10 16:48:24,5
Intel,ntb2cwp,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 15:57:35,1
Intel,ntcubg5,"Cleanup utility first, always!",AMD,2025-12-10 21:11:55,3
Intel,ntbx46o,I also wanna know best way updating drivers. DDU kinda annoying but maybe must be done i don't know,AMD,2025-12-10 18:27:48,1
Intel,nte7te5,It's doing it there too.,AMD,2025-12-11 01:45:34,1
Intel,ntcm462,Never seen any crashes on it with latest driver prior to today 9070xt w11,AMD,2025-12-10 20:31:13,1
Intel,ntctang,"Just tested it tonight, and for me it's working fine, 9060xt here, windows 11 with the latest update, although i play with the ""medium"" preset which disables ""lumen"", can't say it might work for you but you can try it if it still crashes constantly",AMD,2025-12-10 21:06:50,1
Intel,ntdql6b,Might want to check [https://www.reddit.com/r/radeon/comments/1pjeonb/fyi\_fsr\_ml\_framegen\_requires\_windows\_11/](https://www.reddit.com/r/radeon/comments/1pjeonb/fyi_fsr_ml_framegen_requires_windows_11/) :|,AMD,2025-12-11 00:02:35,2
Intel,ntltbvr,nope. I still crash,AMD,2025-12-12 06:44:56,1
Intel,nujivd3,"I fixed my arc raider crashes (mostly in blue gate map load) by running DDU, installing 25.10.1 (down from 25.11.1), and deleting shader caches (dont know if the shader cache delete helped or not). I upgraded to the newest drivers the day after they released and haven't had a single crash since in arc raiders, including w overlay.",AMD,2025-12-17 17:16:27,1
Intel,nts13sv,"for me, DDU in safe mode, disconnect internet, install 25.9.1 fine for me(9060XT).  I've tried 25.10/ 25.11 and revert back to 25.9.1 with this way. Now observing 25.12",AMD,2025-12-13 06:52:17,2
Intel,nujhigl,Yes. I downgraded from 25.11.1 because of the crashing. Now been on 25.12.1 all week and havent had any issues come up. You also get proper fsr4 upscaling now.,AMD,2025-12-17 17:09:46,1
Intel,nv4w9xy,"I just want to say I think I found the culprit. It also happens on the winupdate one too, because it started crashing all the time.  Core clock boosts itself WAY past what it is declared on the card(I got a Sapphire 9070XT Nitro, supposed to be 3060MHz). Here's the moment before it crashes to a black screen:  [afterburner screenshot](https://i.ibb.co/3YpJFtzM/Screenshot-2025-12-21-030537.png)  The dip in clocks is the moment it crashes. As you can see, it is running well above boost clocks. Hence, freezing in a few minutes, proceeded by a black screen, and a crash. The ups and downs are from me alt tabbing in the graphs, by  the way.   This is with core clock -200mhz applied in Afterburner and no crashes, boosts to just above declared boost clocks. Here the dips in up and down on power are probably me toying around how much exactly -mhz is needed.  [afterburner -200mhz](https://i.ibb.co/YTQfGJtc/11111.png)  All of the crashing behavior so far is replicable in COD, CS2, Cronos New Dawn.  u/AMD_Vik",AMD,2025-12-21 02:43:00,3
Intel,nw7st51,thanks for reaching out - funny timing; I noted that on the internal ticket for this issue yesterday having seen other accounts of end users noting this issue persists even with 25.12.1. Perhaps the fix aligned to that point release somehow slipped.,AMD,2025-12-27 16:59:39,1
Intel,ntbhx4e,I still have my 5670,AMD,2025-12-10 17:14:01,2
Intel,ntfsndh,"No problems with RX 9070 xt in ARC raiders, i have win10",AMD,2025-12-11 09:01:20,1
Intel,ntazsxe,It’s for Darktide apparently,AMD,2025-12-10 15:45:06,23
Intel,ntauw3f,any game with fsr 3.1 fg also has the new fg since drivers override it. it’s also why they stopped versioning fsr. any game with fsr 3.1 should just automatically have any new version of fsr when the drivers update,AMD,2025-12-10 15:20:21,11
Intel,ntfnlow,"It's for Darktide. But it's not even ready for launch there, either.",AMD,2025-12-11 08:10:07,1
Intel,nthk5bz,I forgoed any amd software entirely  Use more clock tool  10x better with 0% of the bloat   ^^ helped me get my 4th in world furmark score (7900xtx user),AMD,2025-12-11 16:18:42,2
Intel,ntbig0n,If you want to be in control of what’s on your computer then Windows is not the OS for you,AMD,2025-12-10 17:16:36,19
Intel,ntbjons,"Dumbest take one can have, since installing only the driver won't let you manage the settings at all.  Which has nothing to do with this useless launcher no one wants or needs.",AMD,2025-12-10 17:22:43,16
Intel,nteh1sb,Found the install manager dev lol,AMD,2025-12-11 02:41:05,3
Intel,ntaqzke,Thanks.,AMD,2025-12-10 14:59:49,13
Intel,ntb7o1t,Unfortunately Redstone FG is bugged with poor frame pacing,AMD,2025-12-10 16:23:28,21
Intel,ntaqis1,Nice to see the innovation continuing on,AMD,2025-12-10 14:57:21,19
Intel,ntbic15,But only on the 9060 and 9070 right?,AMD,2025-12-10 17:16:03,1
Intel,ntaoqu8,Yeah same,AMD,2025-12-10 14:47:53,2
Intel,ntapwco,"Remember when you could click ""Check for Update"" inside the AMD Software and if there was an update, it would download and install it for you?  Glad they fixed that awful experience, and we have the Installation Manager now.",AMD,2025-12-10 14:54:03,27
Intel,nte60vn,I remember this mentioned since the  GCN 1.0 days. Lol,AMD,2025-12-11 01:34:30,6
Intel,ntfp000,"On my end, the driver crashes. Most of the time it manages to recover (sometimes it will crash a few more times before stabilising). Sometimes it doesn't recover (leaving only 1 of my monitors working), so I had to reboot. Then after rebooting, strong chance it'll crash again the moment I open my browser.",AMD,2025-12-11 08:24:09,5
Intel,ntwnl8a,"\+1 on this. Most games crashed drivers with any newer drivers except 25.9.1, but poe2 i cant play with vulkan or Directx 12 only with Dx11",AMD,2025-12-14 01:17:11,2
Intel,nuur9u6,"My experience with switching to amd was so smooth and perfect until 25.9.1. Everything after that just caused stutter issues in games, programs randomly crashing, drivers crashing completely causing my pc to reboot, this is so sad i hope they fix this soon and bring back a stable version asap. Rolling back to 25.9.1 now aswell until that happens.",AMD,2025-12-19 12:46:06,2
Intel,nv8ptlv,přesně zustávám na 25.9.1 všechno jiné crash,AMD,2025-12-21 18:59:03,1
Intel,nw3xh0a,"I had been having the absolute worst time with drivers when I first bought my 7600XT, but finally found stability with 25.8.1 (and turning the Xbox Gamebar DVR off...) but I'm so paranoid now to update my drivers again. The only reason I decided to check on updates now though is a sudden appearance of my screen flashing black at random times.",AMD,2025-12-27 00:08:20,1
Intel,nth6kuu,Are you able to tell us what the error code is on the BSOD? I don't suppose you have a kernel memory dmp pertaining to one of these failures over at      C:\Windows\MEMORY.DMP,AMD,2025-12-11 15:11:30,4
Intel,ntap5oq,Thanks will give it a try after I finish work,AMD,2025-12-10 14:50:07,9
Intel,ntchncg,"Wait, AMD Customer Support told me that 2 monitors connected to iGPU and dGPU has never been officially supported and that this configurations breaks performance… so they told me bullshit?",AMD,2025-12-10 20:08:56,1
Intel,nte3vcl,Any update on three Oblivion Remastered and Silent Hill  2 Remake crashes? A lot of us are still with the September drivers because of them.,AMD,2025-12-11 01:21:16,1
Intel,ntcb9cq,<--- Ditto,AMD,2025-12-10 19:36:20,6
Intel,ntbpv70,Optiscaler lets you inject it. Do not use in multiplayer games though.,AMD,2025-12-10 17:53:00,3
Intel,ntauof3,it cannot possibly be this difficult to fix when there’s already community workarounds,AMD,2025-12-10 15:19:15,9
Intel,ntb6tpy,both are still broken somehow,AMD,2025-12-10 16:19:24,1
Intel,nwsjipr,running at 600 fps with vsync on means that something’s terribly wrong with something in your software that’s breaking vsync. that’s definitely not normal,AMD,2025-12-30 20:25:38,1
Intel,ntb77ho,both have been broken since 25.10.1. enhanced sync just makes your display run at an extremely low framerate when freesync is on and then noise suppression just doesn't even turn on. I don't understand how they haven't fixed either of these yet. they haven't even acknowledged it,AMD,2025-12-10 16:21:15,3
Intel,nte5171,"I have to do the same. My monitor is  240Hz and the TV  120Hz and I have to use Chill, which sometimes will cause stuttering, because enhanced sync always causes stuttering.   Man I'm starting to miss the NVidia setting of just putting vsync on in the driver and everything just working.",AMD,2025-12-11 01:28:24,1
Intel,ntcnloa,I did some testing AND as far as I can tell I do think it's actually fixed finally,AMD,2025-12-10 20:38:39,3
Intel,ntbd1ml,I would continue buying their GPUs if they gave me something to buy.  The XTX has no upgrade path on RDNA4.,AMD,2025-12-10 16:49:51,18
Intel,nteixfg,"I had Nvidia for years, the main reason I switched was that the drivers went to shit last year. I'm just sick of them in general, too. The 7800 XT I bought has been one of the most trouble free cards I ever had, aside from Adrenalin randomly closing in certain versions.",AMD,2025-12-11 02:52:24,2
Intel,ntdc84n,"If I could get my hands on a 5070 Ti I’d happily switch. AMD likes to take advantage of the underdog, for-the-people image whenever it’s convenient but they’ll just as quickly throw us under the bus and fuck us raw once they’ve got the bag.  Is Nvidia a gang of greedy fucks? Sure. But at least the bullshit’s right out front where you can get a good strong whiff of it. You know what you’re in for.",AMD,2025-12-10 22:41:15,4
Intel,ntm5vgi,"I purchased a 7700 XT and a 7600 8gb I'm March and while I'm satisfied with performance, it would definitely be awesome to have FSR 4 on both cards as FSR 3 and 2.2 (overwatch )leave alot to be desired",AMD,2025-12-12 08:42:08,2
Intel,ntapkhc,It's been so long bro :( Hopefully the fix comes with ray regeneration support?,AMD,2025-12-10 14:52:19,23
Intel,ntbfm3b,"Hey Vik, is there any info for FSR4 Vulkan support?  It's quite sad to see that there still isn't support for it as it has been 9 months by now since the release of the 90 series  Also is there any info about the EAC issue with Star Citizen and the latest drivers?",AMD,2025-12-10 17:02:29,13
Intel,ntc52w2,"Amd Noise Supression doesn't work, when I try to turn it on, nothing happens, but in 25.9.1 it works",AMD,2025-12-10 19:05:49,8
Intel,ntcc596,"Hey amd\_vik is amd Aware of the 1 year on going Darktide issues with amd  ( GPU , and specially X3D cpus ? ), and that even the Dev of Darktide ( Fatshark ) seemingly gets ghosted by amd ?  heres some more info specially first links includes a few Dev comments  [https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462](https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462)  [https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f](https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f)  [https://forums.fatsharkgames.com/c/darktide/performance-feedback/97](https://forums.fatsharkgames.com/c/darktide/performance-feedback/97)",AMD,2025-12-10 19:40:50,5
Intel,ntdbffr,"Vik, weren't you on holiday leave? xd",AMD,2025-12-10 22:37:02,4
Intel,ntc7hrz,Any fixes for the SecondLife issues we've had the last few months? last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:17:39,2
Intel,ntbwr0w,Will this update fix some of the artifacting I’m seeing in cyberpunk with fsr enabled?,AMD,2025-12-10 18:26:01,1
Intel,ntcji37,Also getting driver timeouts in Cyberpunk with RDNA3 with raster or RT. I did not have these problems with my RDNA2 card.,AMD,2025-12-10 20:18:15,1
Intel,ntcuy8r,"The AMD FSR ML-based Frame Generation option in the Radeon panel disappears in Windows 10.  So I have a question: Is ML-based Frame Generation no longer usable in Windows 10? This option is available in Windows 11, but not in Windows 10.",AMD,2025-12-10 21:15:00,1
Intel,ntdcmj2,Can I join if mine's just an XT?,AMD,2025-12-10 22:43:23,1
Intel,ntawh67,What GPU are you using?,AMD,2025-12-10 15:28:30,2
Intel,ntfkwf8,Try reinstalling Windows. That fixed it for me.,AMD,2025-12-11 07:43:52,1
Intel,nte7o94,"This doesn't work. We are talking about games that crash with or without it, the only difference being the older AMD driver working.",AMD,2025-12-11 01:44:41,3
Intel,ntc9ed0,I already install the latest update before update drivers its not update related. Vulkan driver is the problem in indina jones and silent hill 2 after windows update 25.11.1 not crashing ray tracing enabled but in 25.12.1 its broken again. So driver is the problem...,AMD,2025-12-10 19:27:04,2
Intel,ntamwm4,"They said earlier in 2025 they were working on FSR 4 support for RDNA 3, and then it leaked in September with the INT8 version...",AMD,2025-12-10 14:37:41,10
Intel,ntal44u,"They might as well have lol, they aint getting no new features",AMD,2025-12-10 14:27:29,14
Intel,ntbssdw,They also promised features to the few of us who bought 7900 XTX. Good luck defending them when it's your turn to be disappointed.,AMD,2025-12-10 18:06:58,2
Intel,ntbim9d,I expected them not to abandon their king card lmfao. Who does that,AMD,2025-12-10 17:17:26,2
Intel,ntar1pe,"Not really, they teased the possibility of including other architectures.",AMD,2025-12-10 15:00:06,2
Intel,ntimm5h,Maybe next time you should read the whole thread before replying.,AMD,2025-12-11 19:26:09,1
Intel,ntaqnxp,"It's also related to getting new features in generations other than just the latest one, ""bro"".",AMD,2025-12-10 14:58:07,1
Intel,nthyzs0,"I have the 7800 xt hellhound i F love it, tbh i care less about this redstone thing but its frustrating why a 2 year old lineup is abandoned all of a sudden",AMD,2025-12-11 17:30:59,2
Intel,ntbkahh,"> I'd double down on the consumer market to insulate from the impending bubble burst  If that bubble bursts nobody is going to have much money to spare for consumer goods. That bubble bursting will tank the entire economy along with it.  *Long* term that might work out better, though.",AMD,2025-12-10 17:25:41,3
Intel,ntbdi9g,further reminder amd is not your friend sadly,AMD,2025-12-10 16:52:07,9
Intel,nth472g,Same for me but Doom Eternal. I play at 4k and it needs upscaling at that res.,AMD,2025-12-11 14:58:42,1
Intel,ntfl1sl,What if I'm on RDNA 4?,AMD,2025-12-11 07:45:19,1
Intel,ntheoob,Yeah there are no good choices,AMD,2025-12-11 15:52:11,1
Intel,ntbmlpj,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html),AMD,2025-12-10 17:37:04,1
Intel,ntcu71s,Adrenalin is for GPUs.   Chipset is for CPU & mobo.,AMD,2025-12-10 21:11:19,1
Intel,ntb6pmb,"Yeah im sorry for all of us, already shopping for a 5080 rn…",AMD,2025-12-10 16:18:51,1
Intel,ntpptsg,Yes that’s the one. I have no idea where to turn lol,AMD,2025-12-12 21:31:29,2
Intel,ntnglvb,Sad news. Nvidia still supporting old extensions.,AMD,2025-12-12 14:38:54,2
Intel,ntbuxj0,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-12-10 18:17:20,1
Intel,ntdymrv,oh wow i haven't had any issues yet but that doesnt mean much. 25.11.1 i didnt have issues for a week or so.,AMD,2025-12-11 00:49:22,1
Intel,ntcmz77,"Interesting, I’ll test it today. I was crashing non stop on 25.11.1 so hopefully this update fixes it",AMD,2025-12-10 20:35:33,1
Intel,ntlyzyv,Same,AMD,2025-12-12 07:35:41,1
Intel,nv973go,"I have the same model GPU inconsequentially boosting well above the advertised clocks (nearly 3.4GHz) in both windows 10, 11 and fedora 43 with no issues.  This has been discussed several times on this community; whilst the clock behavior may surface other issues or instabilities on the system, it's not in itself the cause of problems.",AMD,2025-12-21 20:28:00,2
Intel,nw7vn10,"I actually have one more potentially related thing for you!   During the game I tried to turn the overlay on using my hotkey. Noticed it didn't. Since I've seen this before (we can call this a ""soft lock"") I tried to open the full screen experience with the hotkey. Which brought up my mouse (was using a controller in game before pressing the keys) but I could not move it...  My workaround has been: ctrl+alt+esc to task manager, tab to the search bar, type ""radeon"" and force kill the host service.  The instance I reported before this was a ""hard lock"" that I've noticed while trying to use my browser over a borderless game running, before this time where it was when the gpu wasn't under any actual load as far as I knew.  Glad to hear it's a known issue and not my hardware though... Thanks for getting back to me!",AMD,2025-12-27 17:14:03,1
Intel,nxaf1nm,Which driver version DOESNT have this issue?   I've tried going back all the way to .10 and it's all having the issue...,AMD,2026-01-02 17:46:29,1
Intel,ntg8mgs,6970 here,AMD,2025-12-11 11:36:12,1
Intel,ntc5yzi,Literally the one game I don't play lol,AMD,2025-12-10 19:10:11,9
Intel,ntb8cnz,"Yay, I own that one",AMD,2025-12-10 16:26:50,3
Intel,ntestwe,"It's not even out for Darktide yet either. Fatshark clarified that it's experimental and needs more work, so it's not in the live build",AMD,2025-12-11 03:56:23,3
Intel,ntazo47,So it's under the umbrella of the fsr4 override if I understood this correctly. For the fsr2 and 3.0 games I can use optiscaler right? Sorry I just bought a 9070xt coming from nvidia so I need to get used to these things.,AMD,2025-12-10 15:44:27,4
Intel,ntifqj7,I used to do that but a few games can use the FSR4 in driver upgrade.  The enhanced sync was nice too when it worked.,AMD,2025-12-11 18:52:16,2
Intel,ntbjqio,"Unfortunately I play games and run software that require Windows so I have it on a separate drive. When I do switch to it (and I update the driver to take advantage of new features), this shit typically happens along with a slew of forced updates.  You are right though, I do primarily run CachyOS.",AMD,2025-12-10 17:22:58,17
Intel,nte7ly0,found the linux user,AMD,2025-12-11 01:44:17,4
Intel,ntd5qr8,You're talking nonsense.  Engineer managing 2k endpoints and several hundred servers.,AMD,2025-12-10 22:07:29,4
Intel,ntcmjlp,Wasn't the dude's claim it has been always bugged with AMD,AMD,2025-12-10 20:33:21,6
Intel,ntctlcs,🌍👨‍🚀🔫👨‍🚀,AMD,2025-12-10 21:08:21,1
Intel,ntasjqd,It's barely an improvement.,AMD,2025-12-10 15:08:04,12
Intel,ntcmoxo,It's branding,AMD,2025-12-10 20:34:05,1
Intel,ntbkqjy,"Yes, RDNA4 refers to the RX9000 series.",AMD,2025-12-10 17:27:52,3
Intel,ntb0ccn,"Uninstalling the install manager brings back the ""check for updates"" functionality until you update again (and have to re-uninstall the install manager)",AMD,2025-12-10 15:47:46,13
Intel,ntp6j29,I have one of these captures if you want it (error code 0x00000119). I've been having a TON of driver timeouts and BSOD for the past couple of driver versions and I've had to roll back to October to resolve them. Seems like any app that has hardware acceleration enabled causes it and exasperated when viewing the system via RDP.,AMD,2025-12-12 19:48:47,1
Intel,ntap8zv,Let us know how it goes!,AMD,2025-12-10 14:50:37,9
Intel,ntci6s3,"I don't know how much of an impact this could have on perf since it's not something I've measured. I personally wouldn't do this, though. With a dGPU installed I keep iGFX off.",AMD,2025-12-10 20:11:40,3
Intel,nted5dt,"performance wise it should only be a couple frames of latency, when doing rendering on dgpu and going out through igpu it'll just copy over the frame buffers.   Main impact is on pcie bandwidth as it'll use up quite a lot there, and to a smaller degree RAM load, so you definitely don't want to run some other dynamic load on the igpu when gaming to overwhelm its pcie link. I think on 7000/9000 it's x8 so it may be fine? But I'm really not sure could be x4 too",AMD,2025-12-11 02:17:41,1
Intel,nth79az,"We're tracking a failure in silent hill 2 remake, I believe a fix is aligned to a future release. I'll need to check in with oblivion remastered",AMD,2025-12-11 15:15:04,2
Intel,ntdvql1,"Do you have to do that convoluted setup and download the drivers from Limewire, or has Optiscaler wrapped it in to their application?",AMD,2025-12-11 00:32:26,2
Intel,ntbvuyt,"So, no official release... ;(",AMD,2025-12-10 18:21:45,1
Intel,nte1rh2,Any tutorial for a noob on RDNA2?,AMD,2025-12-11 01:08:16,1
Intel,ntbfsb9,what workaround?,AMD,2025-12-10 17:03:21,6
Intel,ntbgebv,"Same issue with fsr4 on rdna1-3.   It shouldn't be this difficult, it's in a perfectly working state made possible by like one guy's few days worth of work.   And yet AMD just doesn't do it...",AMD,2025-12-10 17:06:24,2
Intel,ntbmhrj,FUG,AMD,2025-12-10 17:36:32,1
Intel,nwtucuk,"Oh, definitely not normal for sure... but I have this issue on multiple games and I did not have this issue on the 6080 it replaced. This seems to only be impacting my 9070.",AMD,2025-12-31 00:22:00,1
Intel,nte56dv,Enhanced sync makes games super stuttery even in  25.9.2.,AMD,2025-12-11 01:29:17,2
Intel,nteewr6,So I tried out enhanced sync and the game that I first noticed the issue on is no longer an issue. I've checked like 5 other games and no issues so far. Maybe it's fixed?,AMD,2025-12-11 02:28:10,1
Intel,nteepyk,I have my 5070ti build hooked up to my 240hz Ultrawide Oled because of Multi-Frame Generation. I was hoping that would be part of Redstone but it's not.,AMD,2025-12-11 02:27:04,1
Intel,ntczm93,"Such a relief, but i am also annoyed because they are ignoring 7000 series... I can literally use FSR 4.0.2 on my 7700XT and it is WAY better than FSR 3.1....",AMD,2025-12-10 21:37:51,2
Intel,ntcztos,I hope it is fixed for me as well 😭🙏. Thanks for the info.,AMD,2025-12-10 21:38:50,2
Intel,ntchg2c,yep would have upgraded but with an XTX.... you can cut your vram in 2/3 and have less Raster performance for a good upscaler and better RT performance it's such a stupid fucking problem....,AMD,2025-12-10 20:07:57,5
Intel,ntaq6sy,"That's not something I'm privy to, but it could be worth reaching out to them to request looking into if they're not already.",AMD,2025-12-10 14:55:36,32
Intel,ntbho9v,"I'm not privy to any of the FSR stuff - that's a different team to mine. I can pass on the feedback.  The Star Citizen EAC issue should be addressed, please let me know how it is.",AMD,2025-12-10 17:12:47,23
Intel,nte0zy9,i still am!   so many fixed issues out of the release notes that I felt the need to stick around and help clear things up in the communities I frequent. I'll go back into hiding again soon,AMD,2025-12-11 01:03:33,5
Intel,ntciqi1,"I've seen something like this over at OCUK Forums but weren't given enough data to work with. We've attempted to reproduce a corruption issue but apparently we've not been successful.  Can you give me a step by step breakdown on how to hit this, as well as a clear depiction of the issue?",AMD,2025-12-10 20:14:24,3
Intel,ntqc750,"No, XT peasants needs to form their own group.",AMD,2025-12-12 23:39:51,2
Intel,ntcxhw3,6800XT.,AMD,2025-12-10 21:27:27,7
Intel,ntanrvb,Some of their marketing said they would like to get it working if possible.,AMD,2025-12-10 14:42:33,8
Intel,ntarc4r,"There are already third party options, but it would be nice to see if Steam Machine drives INT8 FS4 support since it runs on RDNA 3 tech. Let's see what happens in 2026.",AMD,2025-12-10 15:01:37,5
Intel,ntbkv5b,Yeah there are going to be serious consequences as major retirement funds have invested in all these AI stocks because they have made so much money.,AMD,2025-12-10 17:28:30,3
Intel,ntflk2b,"Give it a try, for my 6800xt it's crashing in almost all games...  ![gif](giphy|QMHoU66sBXqqLqYvGO)",AMD,2025-12-11 07:50:12,1
Intel,ntsqrgy,"Sorry, out of curiosity, if you close it, it won't let you play? What do you get? Could you send me a photo so I can understand?",AMD,2025-12-13 11:11:24,1
Intel,ntnluq2,"I agree. Please can you raise a ticket requesting support for this over at our GPUOpen and ask other end users and developers to upvote it and leave a comment registering their interest? (please share a link to it here if you do) https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues  As far as I'm aware, the impacted titles are: IL-2 Sturmovik: 1946, Neverwinter Nights Diamond Edition and Call of Duty. If there are any others, I would really appreciate you letting us know.  E: I believe it's posted here: https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues/80",AMD,2025-12-12 15:06:46,2
Intel,nthltl0,"Just an update - I ended up running DDU and re-installing the latest update and now things are pretty stable, no driver timeouts from hardware accelerated apps either. Could be something to do with the architecture change between driver packages - but doing a complete removal between updates seems required now.",AMD,2025-12-11 16:26:46,1
Intel,ntcop7s,I never seen 1 crash on 25.11.1 although I did use the preview update for windows 11 last week which fixed some amd gpu related crashing and that solved my arc raiders random crashing,AMD,2025-12-10 20:44:09,1
Intel,nv9cafd,"My apologies then - it seems latest driver on Windows seems to be the source of issues then, seems more people have issues posting on /r/AMDHelp , also with 9070XT's. Seems all device hung errors and timeouts recently posted are with 25.12.1. I had no issues on cachyOS (Hyprland) running CS2 too, latest amdgpu.",AMD,2025-12-21 20:55:29,1
Intel,nxbs3rv,I believe this was introduced with the 25.20 driver branch. it shouldn't be present in 25.9.1/2,AMD,2026-01-02 21:39:50,1
Intel,ntb0k7p,"yes, 3.1 is where AMD adopted the same modular approach as nvidia so any game at fsr 3.1 or above just runs at whatever latest fsr version your driver supports, which is currently 4 although now the versions aren't numbered anymore",AMD,2025-12-10 15:48:50,6
Intel,ntcauqa,Hell yeah 👍🏻   Impressive you can run that on a 5x86,AMD,2025-12-10 19:34:18,4
Intel,ntbryby,"Since you're already an advanced user, perhaps you could block it from installing by selectively blocking AMD in your hosts or pi-hole? It's not a dumb solution, but it's better than having to deal with push-installs.",AMD,2025-12-10 18:02:57,2
Intel,nthi3lk,I might be an ass but I’m not wrong,AMD,2025-12-11 16:08:50,2
Intel,nthi8dc,Sorry  If you’re a **consumer** and want to be in control of what’s on your computer then Windows is not the OS for you,AMD,2025-12-11 16:09:28,2
Intel,ntfql24,"Yes, If you mean the bad frame pacing when fps is lower.  I still opt to spent 1-200 hrs of my gaming session with FSR 3 frame gen, 7900xtx.  It's not that bad when the output is close enough to monitor max hz, similar to what hardware unboxed did in thier test.  The generated frame still comes out too early but it has to wait for the monitor's nest refresh which is consistent.",AMD,2025-12-11 08:40:22,1
Intel,ntc2hr1,ty,AMD,2025-12-10 18:53:26,1
Intel,ntb1b5l,"u/amd_vik it sounds like this person doesnt want the manager to install again, but I am pretty sure you can do custom option to uncheck it. If you do express of course it will put it back sschuler.",AMD,2025-12-10 15:52:28,9
Intel,ntpa4lm,can you run analyze -v in windbg or fire it over to me via your preferred file sharing method?  I personally like to use https://send.vis.ee,AMD,2025-12-12 20:07:35,1
Intel,ntcew16,Can confirm this issue is fixed for me on 9800x3d + 9070xt (I had this issue on 25.11.1 and reverted to 25.10.2 until today) 👍,AMD,2025-12-10 19:55:01,7
Intel,ntb65up,"Seems to be working fine, though when I was installing the driver my igpu showed up separately from the dgpu in the installer with a download link. But when re-running it they both show under 25.12.1  Should I be installing some separate older driver for it to keep things like hw accel working or was that just some hiccup?",AMD,2025-12-10 16:16:11,6
Intel,ntaufrk,Oh great will also test after work it’s been headache since last driver update,AMD,2025-12-10 15:17:58,4
Intel,nthzjga,Thank you for taking the time to respond. This has been very frustrating.,AMD,2025-12-11 17:33:43,2
Intel,ntlgdax,"I'm sorry to comment directly to you here. Do you have any report about monster hunter wilds performance drops in recent GPU drivers ?    I'm using 9070xt.    I have to use version 25.3.1 to play wilds with no stutters, anything newer gives a lot of stutters in many places.",AMD,2025-12-12 05:02:06,1
Intel,ntjjshb,"Yeah you still have to download it on your own, the creator of Optiscaler already said they aren't going to bundle it probably due to the whole legality around it.",AMD,2025-12-11 22:14:22,1
Intel,ntbpcf0,"i saw a post that detailed how to essentially replace noise suppresion with the working version in newer drivers, you can probably find it here somewhere",AMD,2025-12-10 17:50:29,2
Intel,nwtxl54,yeah something’s definitely wrong. i’m assuming you’ve already tried ddu?,AMD,2025-12-31 00:39:33,1
Intel,nter7t7,worked fine for me idk,AMD,2025-12-11 03:45:44,1
Intel,nter4u6,still busted for me,AMD,2025-12-11 03:45:12,1
Intel,ntef1nl,"Also from what I can tell, enhanced sync is fixed at least on my end.",AMD,2025-12-11 02:29:00,1
Intel,ntbytau,Thank you for this! been waiting for a fix with Star citizen.,AMD,2025-12-10 18:35:58,7
Intel,ntcdxlc,Yeah SC seems to be working for now.,AMD,2025-12-10 19:49:59,5
Intel,ntcib7n,"Bonjour, pour le moment sur Star citizen le problème avec EAC fonctionne pour la 7900XT. Merci d avoir réglé le problème. Bonne fêtes de fin d'année.",AMD,2025-12-10 20:12:16,2
Intel,ntcro8s,That's good to hear. What about Noise Suppression not working since 25.9.2?,AMD,2025-12-10 20:58:42,1
Intel,ntcnbes,Hmm let me try. So pretty much having installed the latest driver (25.12.1) I just open SecondLife. I look closely at my avatar/character and my skin looks like this  [https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4](https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4) (excuse my outfit but just easier to show)  this is how it's supposed to look and also does on 25.9.1 [https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4](https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4)  I've heard that this doesn't occur on linux but only windows (But I don't have linux so can't say for sure)  I think you need PBR / Materials or some reflection on your skin to see the issue.   If you fly up to around 2000+ meters above ground it becomes easier to see  These are my settings [https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png](https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png)  I have an rx 7900XTX,AMD,2025-12-10 20:37:14,3
Intel,ntcnscv,"Hello! I am actually one of the developers on the client team for Second Life, and I have been trying to figure out how to get in touch - we have found at least one nasty bug on some of the Strix Halo chips with the current drivers.  Can you send me a message here so we can exchange emails?",AMD,2025-12-10 20:39:36,3
Intel,nu85qao,😭😭😭😭,AMD,2025-12-15 21:31:35,1
Intel,ntdy4eq,"I had a similar issue with my 6800xt and the other thing that helped was to sit it to fullscreen or borderless and swap back and forth. Now I'm only playing in fullscreen (which is annoying), but it doesnt crash anymore.",AMD,2025-12-11 00:46:19,1
Intel,ntf5uuk,I have the same card and exactly the same problem. Can't install newer drivers or BF6 just constantly crashes.  I'm on 25.10.2 tho,AMD,2025-12-11 05:30:42,1
Intel,ntapogt,"And it is, and they did, we have the leaked int8 version from September... Just needs official driver implementation now.",AMD,2025-12-10 14:52:54,6
Intel,ntu98tq,"Before the Black Ops 7 (which I don’t own) integration to Warzone, I could click off it & carry on. But since the integration it just closes the game.",AMD,2025-12-13 17:06:00,2
Intel,ntudqmy,"Yes, i have created this github issue.",AMD,2025-12-13 17:29:47,3
Intel,nva2mbl,"If those failures are avoided by clock limiting the board, the problem area could be a different domain entirely (CPU, memory, power, etc.).  The linux remark is interesting, it kind of calls back to similar failures with NV31 in certain apps like Helldivers 2; we had a little internal discussuon about how the amdgpu kernel driver managed to mostly avoid such issues, though I dont recall the outcome.  If you get the opportunity, I'd recommend a suite of system integrity routines as a sanity check; please take a look at [one of my older posts](https://old.reddit.com/r/Amd/comments/1l9ox9r/amd_software_adrenalin_edition_2562_optional/nn3yuay/) for some background.",AMD,2025-12-21 23:17:59,1
Intel,ntb3jrw,"They aren't numbered in the sense of like 4.0.2 or like there won't be an ""fsr 5""? Thank you very much btw, very helpful info!",AMD,2025-12-10 16:03:28,3
Intel,ntdazyo,Like a charm. :D,AMD,2025-12-10 22:34:45,1
Intel,ntbte7r,"I probably could, but AMD (and any other company, really) should be following the users preference anyways. It is a band aid fix and doesn't solve the problem.  Not a bad idea though.",AMD,2025-12-10 18:09:54,5
Intel,ntwpskf,I've been using computers since dos 3.  You're a spanner.  I'm sure MacOS is soooooo much more open.,AMD,2025-12-14 01:31:33,3
Intel,ntbtmtr,"Thank you for the idea, I just tried a custom install during an update, was given 2 choices (update/dont update driver and install/dont install privacy view). After installing drivers, step 2/2 was installing the install manager.    After updating through adrenaline using the custom option, I attempted reinstalling again using the auto-detect, custom install. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.   Installing via the WHQL package, custom install follows the same steps as above. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.",AMD,2025-12-10 18:11:03,1
Intel,ntpczrx,Here you go: [https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw](https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw)  I did run in windbg but I have no idea how to save the output unless you just want a copy + paste of it here haha,AMD,2025-12-12 20:22:52,1
Intel,ntci8tp,Appreciate the feedback,AMD,2025-12-10 20:11:57,6
Intel,ntb8e8t,Thank you for confirming.  That interesting though. I think the most seamless way to support products from both branches is to use the AMD auto detect tool. Can you tell me how the iGPU is represented in Windows' Device Manager?,AMD,2025-12-10 16:27:02,5
Intel,ntbkinx,"[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html)  https://i.redd.it/3vxsa8yave6g1.gif  If you suspect the installation is incorrect, download the package that includes the IGPU driver using the link provided above. The basic version does not include the IGPU driver, but provides a separate download option during installation.  Anyway, it seems like a lot of bugs have been fixed in this version.",AMD,2025-12-10 17:26:48,1
Intel,ntbxdgl,"if you can find it, you will be the goat",AMD,2025-12-10 18:29:03,3
Intel,nwv35gt,"This isn't every game, this is only some games. Not all games have a native vsync option either. That being said, from what I can find, this is a known issue.  https://steamcommunity.com/discussions/forum/1/601900047372731730/  https://www.wumeicn.com/screen-tearing-fix-for-rx-9070xt-and-freesync/",AMD,2025-12-31 04:49:26,1
Intel,nteuykg,"Yeah everything was fine until I tried to play Resident Evil 2 Remake, then the issue was back but not in the other games. This is a very odd issue.",AMD,2025-12-11 04:10:49,2
Intel,ntehipf,Nvm the issue I was having just happened in Resident Evil 2 Remake.,AMD,2025-12-11 02:43:56,1
Intel,ntciae3,Thank you for letting us know 👍,AMD,2025-12-10 20:12:10,7
Intel,ntcoi7s,appreciate the info. I'll ask our technicians to check in with the settings you've provided,AMD,2025-12-10 20:43:12,3
Intel,ntfvp58,I can confirm there is no issue in linux. A windows version running under proton in linux has no issues as well.   In the video there is flickering on head and body. I see only flickering on the head (when running it on the windows pc)  But my body has no layers attached - the body in the video usually comes with layers. But all heads have multiple transparent layers. The problem occurs even when that layers are not in use and are fully transparent.   Probably related.,AMD,2025-12-11 09:32:57,1
Intel,nte0wcf,"Hey there, thank you for reaching out!  I don't suppose it would be possible for one of our devrel folks to contact you via a linden lab email address like business@lindenlab.com?",AMD,2025-12-11 01:02:56,3
Intel,ntuvq16,"So if you click dismiss, the game closes, did I understand correctly? It doesn't let you enter the COD HQ ? I'm telling you this because I too should update the bios, in fact it happens to me too, but I click dismiss and it lets me play anyway.",AMD,2025-12-13 19:03:40,1
Intel,nva7np2,"for CS2, it was the newest driver that caused crashes exclusively, but on that driver I also got stronger boosts off the bat, hence it crashed faster. Now on 25.10.1(from windows update), COD still crashes with a black screen then tab to desktop with a driver timeout detected. Looking at afterburner(just using it to monitor clocks, no OC/UV applied or anything) the moment the GPU touches 3300+ I get thrown to the desktop. Can't even finish the training course even with ""speedrun strats"" before it crashes. It boosts [momentarily to 3300+](https://i.ibb.co/bgLFC0dp/coreclockcrash.png) and I get a screen freeze, crash, and sent to desktop with a driver timeout.   [These](https://send.vis.ee/download/103635cf66bdb907/#t2lRq409eeNwv6AaafhKJA) are both my crash report submissions. I'd go tomorrow over the stress tests, but I have managed to complete Time Spy/Steel Nomad without issues. And like I said, my system has has 0 issues before on a 2080ti.",AMD,2025-12-21 23:47:10,1
Intel,nvccr7w,"FYI, I passed [everything.](https://imgur.com/a/WyB9FeE)  This leaves the driver only. I made sure windows update didn't download its own driver this time, installed 25.12.1, still getting driver timeouts and crashes in games. I don't know what to tell you. Memtest86 also passed without any issues.",AMD,2025-12-22 09:07:04,1
Intel,ntb5lb9,"there won't be an ""fsr 5"" because any game with fsr implemented from here on out should, in theory, be compatible with every future version of fsr made, so numbering them isn't as meaningful. they're probably just going to stick with unofficial codenames like redstone for diffrentiation. Nvidia still uses versioning for DLSS despite it using the same system because it's good for marketing and diffrentiation so I'm not sure that dumping the version numbers is a wise decision but it also makes sense",AMD,2025-12-10 16:13:25,6
Intel,ntbuj2m,"I agree with you wholeheartedly, but super users do what they do best - sudo that shit. x)",AMD,2025-12-10 18:15:24,2
Intel,ntgknre,"I've never had AMD Chat or Privacy View force install, I hate they show up in the available software to install when updating, but I just dont click to install them lol, just update the gpu/chipset drivers",AMD,2025-12-11 13:06:05,1
Intel,ntpzp27,I guess a snippet of the faulting component from the output would work.  This is a minidump. Do you have a kernel memory dump>?,AMD,2025-12-12 22:25:23,1
Intel,ntqn26t,"sorry i missed this, seems it had expired. maybe someone downloaded it before i did?",AMD,2025-12-13 00:46:30,1
Intel,ntbdwdm,"Right now in devmgr with re-running the driver installer from the site things look like this https://u.numerlor.me/2faMBA . I also remembered adrenalin has full driver details and everything looks fine there https://u.numerlor.me/w1Snxw https://u.numerlor.me/EOclpA so I think it was just the installer being a bit confused.  Compared to the installer on the first screenshot, when doing the actual update (from inside adrenalin) the Radeon Graphics was a separate item, and had a ""Download driver"" or something along those with the link I mentioned",AMD,2025-12-10 16:54:02,5
Intel,ntbapq8,What about the combined exe? It's still available? That will install both gen but was bugged with control panel disappearing on previous driver.  The combined exe is around 1.6GB.,AMD,2025-12-10 16:38:26,3
Intel,ntc59vr,This might be it? Worth a shot I suppose.  Edit: This worked for me on the latest driver  [https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2025-12-10 19:06:45,3
Intel,nwvgoe4,"i don’t have this issue in any of the same games, but i have no idea what could be causing it in your setup and not mine though",AMD,2025-12-31 06:30:39,1
Intel,ntgiwq2,geenz@ but yes,AMD,2025-12-11 12:54:38,2
Intel,nvf2a9r,any news? SL are not updating their customers with anything constructive and it is affecting most of us.,AMD,2025-12-22 19:18:19,1
Intel,ntvblbb,"Hmm, when I can, I’ll have another look! Thanks!",AMD,2025-12-13 20:31:34,2
Intel,nvd9inn,"I see. Is this specific to CS2 or does it occur with other apps on your end?  We're presently tracking and working on TDRs in that game specifically, though I'm kind of worried in a way that clock limiting works around this failure.",AMD,2025-12-22 13:43:12,1
Intel,ntr6yhj,"I do not, only the minidump but I've uploaded it again here [https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN\_IxLREw](https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN_IxLREw)",AMD,2025-12-13 02:56:08,1
Intel,ntbd7hc,"Yes it should be fixed under that scenario, and the combined package is linked on the release notes:  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html  https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe  Kind of guessing here but I believe the '-c' towards the end of the file name denotes a combined package spanning RDNA support.",AMD,2025-12-10 16:50:38,6
Intel,nthusz8,This worked for me btw - did it a few days ago before these drivers dropped. When I update I'll be using the same method.,AMD,2025-12-11 17:10:36,2
Intel,nwwqpp5,"Are you running 4k in freesync on a 9000 series card?  I'm going by the radeon performance metric overlay saying minecraft/etc is using 300w power.  UE5 games are fine, games with an internal frame cap don't have an issue (well, they have their own frame pacing issues but that's not this).  I can always tell when framerate is going nuts because I can hear the squealing in my speakers when the gpu is at 100%. It's especially bad in menu's. If I turn off features/settings that improve quality or try a lower in game resolution, it gets much worse.",AMD,2025-12-31 13:15:38,1
Intel,nth5y4y,thanks a bunch. I'll pass this on to my ISV contact and see where we get with that.,AMD,2025-12-11 15:08:11,2
Intel,nvfir9a,You can find it here [https://github.com/secondlife/viewer/issues/5048](https://github.com/secondlife/viewer/issues/5048),AMD,2025-12-22 20:43:28,1
Intel,nub8ufp,news ?,AMD,2025-12-16 10:31:04,1
Intel,nve66vp,"COD is the greatest offender - I can't even get through the training course for Zombies without a black screen>driver timeout message, even if I try to speedrun it in a way (because I've attempted it so many times) it is inevitable it's going to crash, that one crashes with this [error](https://i.ibb.co/KjxynXH5/image.png).  Again, NO OC is applied. Other than the ram running at 2666, which as stated with both mem tests successful and went through both by Karhu's test and Memtest, have no issues. Including no issues with my previous GPU,2080ti, again. CS, I can't even start a match with friends because it'll inevitably crash randomly, sometimes it is within 5-10 mins, sometimes it is near instant in a couple of minutes. Tried everything from 25.12.2 to 25.9.1. PSU is a RM1000e, using the 12pin cable natively from the PSU. It is all the way in, this PSU I specifically even got for this GPU as I didn't want to use an adapter to power the card from all the experiences I've read with the 12pin + adapters.  Here is also a [video](https://www.youtube.com/watch?v=cSkaI6WSfJY) of it happening.",AMD,2025-12-22 16:38:34,1
Intel,ntvi492,"huh, that's odd. Do you have any larger files over at       C:\Windows\LiveKernelReports\WATCHDOG\",AMD,2025-12-13 21:08:32,1
Intel,ntbt0lr,"Installed the c one. And seems to be working fine. 780M and 6800 here. Still when selecting a specific GPU for a specific app, both energy saver and performance show 6800. This bug has been forever. And it's probably just a registry key when the driver install. Win11.",AMD,2025-12-10 18:08:05,4
Intel,nx0meck,"i’m using a 1440p freesync monitor, i basically always have fps counter on in all of my games so i can verify that vsync always works. even works without the freesync monitor. frame rate only ever goes uncapped when i disable vsync. is it only an issue at 4k?",AMD,2026-01-01 02:07:41,1
Intel,nwkq3wh,No updates there,AMD,2025-12-29 17:14:59,1
Intel,nuyojnk,I do actually have one in there that's 17MB from a BSOD yesterday caused by the AMD driver,AMD,2025-12-20 01:45:48,1
Intel,nwyqsn1,it's tagged as a milestone for feb,AMD,2025-12-31 19:33:08,1
Intel,nonhqm9,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",AMD,2025-11-13 16:12:15,85
Intel,nonf76t,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",AMD,2025-11-13 15:59:52,123
Intel,nonf5bq,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",AMD,2025-11-13 15:59:37,78
Intel,nooaz8h,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,AMD,2025-11-13 18:34:35,19
Intel,nonkrkq,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,AMD,2025-11-13 16:27:06,12
Intel,nonnn81,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too 😿.",AMD,2025-11-13 16:41:13,10
Intel,nonfuov,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,AMD,2025-11-13 16:03:01,16
Intel,noob2qb,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? I’ve spent 1 entire afternoon try every solutions given by Google but today the problem is still there…,AMD,2025-11-13 18:35:02,7
Intel,nopg6ma,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",AMD,2025-11-13 21:59:54,8
Intel,nonp8tv,So does this mean Arc Raiders will stop randomly crashing in Windows?,AMD,2025-11-13 16:49:00,10
Intel,nonw7rh,Just installed these zero issues so far!,AMD,2025-11-13 17:23:17,5
Intel,nondz23,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",AMD,2025-11-13 15:53:59,24
Intel,nonlldq,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,AMD,2025-11-13 16:31:10,4
Intel,noofqtg,There was a long delay with the blank screen. Made me a bit nervous,AMD,2025-11-13 18:57:20,4
Intel,noo2zob,At this point i'm sure that cyberpunk will never be fixed.,AMD,2025-11-13 17:56:32,9
Intel,noolxx3,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,AMD,2025-11-13 19:27:59,8
Intel,nonj6l9,No fix for being unable to enable Noise Suppression...,AMD,2025-11-13 16:19:20,8
Intel,nooktgl,When does Linux get this,AMD,2025-11-13 19:22:26,3
Intel,nop2o04,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",AMD,2025-11-13 20:52:28,3
Intel,noqem0g,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",AMD,2025-11-14 01:15:45,3
Intel,noqnucr,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",AMD,2025-11-14 02:10:59,3
Intel,nou4y1d,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,AMD,2025-11-14 17:01:23,3
Intel,nouw9o1,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",AMD,2025-11-14 19:17:52,3
Intel,novj51b,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",AMD,2025-11-14 21:17:52,3
Intel,npexfdr,Windows update keeps trying to update my driver.,AMD,2025-11-18 00:54:41,3
Intel,noo4qjo,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,AMD,2025-11-13 18:05:01,5
Intel,noniqz3,No FSR4 on RDNA3 no care,AMD,2025-11-13 16:17:12,15
Intel,noo25hd,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",AMD,2025-11-13 17:52:27,6
Intel,nonthc8,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,AMD,2025-11-13 17:09:50,2
Intel,nooud97,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,AMD,2025-11-13 20:10:18,2
Intel,nortjvj,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,AMD,2025-11-14 07:15:55,2
Intel,nos3s8s,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",AMD,2025-11-14 08:57:09,2
Intel,not49x9,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",AMD,2025-11-14 13:52:51,2
Intel,noux5p5,This driver was way better than the version before it(for me at least).,AMD,2025-11-14 19:22:23,2
Intel,novpivg,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",AMD,2025-11-14 21:50:55,2
Intel,np4ombz,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",AMD,2025-11-16 11:19:30,2
Intel,npp1qov,For me the driver just times out randomly during normal stuff like youtube shorts. Today I opend steam and the driver timed out. That never happend with 25.10.1.,AMD,2025-11-19 16:55:57,2
Intel,nqawzsb,"The Adrenalin Software instantly closes and restarts if I try to click on the ""Record & Stream"" tab (no crash/error report, it simply closes and then restarts in background).       Dunno if it's from 25.11.1 or not, it was the first time I was going to try it. Didn't tried a DDU full reinstall either, just a simple reinstall of the driver but for no use. Guess I will just use other software for recording so whatever but I'm curious if it's really a driver issue since I got no report pop up at all.  Gpu is a 9060 xt 16 gb.",AMD,2025-11-23 03:42:39,2
Intel,npaw51d,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",AMD,2025-11-17 11:47:34,3
Intel,nondc4t,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",AMD,2025-11-13 15:50:55,7
Intel,nonmrak,"Brooooo, they didn‘t fix the flickering in BF6 when recording…",AMD,2025-11-13 16:36:52,2
Intel,nonvub9,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.  * Fucking LOL.,AMD,2025-11-13 17:21:27,2
Intel,nonmi72,25.10.2 completely broke vsync... not even a mention about this in the notes?,AMD,2025-11-13 16:35:38,2
Intel,noncnxo,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-13 15:47:39,1
Intel,nonguv3,There is new AFMF features too.,AMD,2025-11-13 16:07:56,1
Intel,nonmglo,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,AMD,2025-11-13 16:35:26,1
Intel,nonn4xw,bf6 fps drop fixed?,AMD,2025-11-13 16:38:44,1
Intel,nonvhb6,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,AMD,2025-11-13 17:19:40,1
Intel,noo456j,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",AMD,2025-11-13 18:02:08,1
Intel,noo651n,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",AMD,2025-11-13 18:11:49,1
Intel,nooad23,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",AMD,2025-11-13 18:31:40,1
Intel,nooeeia,How is the driver ? 7700 XT here.,AMD,2025-11-13 18:50:53,1
Intel,noojnun,Finally a potential fix for CPU metrics? Look forward to seeing if it’s true!,AMD,2025-11-13 19:16:39,1
Intel,noovdps,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,AMD,2025-11-13 20:15:26,1
Intel,noozjd6,do yall use ddu for every driver or do yall just update it with the app?,AMD,2025-11-13 20:36:35,1
Intel,noozq5o,"New AMD update 👏👏👏👏, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",AMD,2025-11-13 20:37:32,1
Intel,nop06vu,I just can’t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video I’m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,AMD,2025-11-13 20:39:55,1
Intel,nop4b7m,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",AMD,2025-11-13 21:00:46,1
Intel,nopfrqo,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,AMD,2025-11-13 21:57:50,1
Intel,nopilp6,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,AMD,2025-11-13 22:12:30,1
Intel,noplmto,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,AMD,2025-11-13 22:28:42,1
Intel,nopnmjz,Think this broke Vulkan in POE2,AMD,2025-11-13 22:39:24,1
Intel,nopyn4b,"Hi [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/), other players and myself are having problems using DXVK with latest drivers. From driver timeouts to black screen. I'm particularly having problems with Fallout New Vegas, but there is reports in other games. How can I help in fixing these issues? Examples: [https://github.com/doitsujin/dxvk/issues/4999](https://github.com/doitsujin/dxvk/issues/4999), [https://github.com/doitsujin/dxvk/issues/5204](https://github.com/doitsujin/dxvk/issues/5204), [https://github.com/doitsujin/dxvk/issues/4851](https://github.com/doitsujin/dxvk/issues/4851)",AMD,2025-11-13 23:41:55,1
Intel,noqjzdo,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",AMD,2025-11-14 01:48:12,1
Intel,nor6g8r,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,AMD,2025-11-14 04:06:42,1
Intel,nor7il2,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,AMD,2025-11-14 04:14:01,1
Intel,nor9p0f,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,AMD,2025-11-14 04:29:34,1
Intel,nord0sz,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,AMD,2025-11-14 04:54:17,1
Intel,norxf8j,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",AMD,2025-11-14 07:53:31,1
Intel,nos6z6k,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",AMD,2025-11-14 09:29:57,1
Intel,nos7i23,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:35:25,1
Intel,nos7vbg,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",AMD,2025-11-14 09:39:11,1
Intel,nosa7uh,subtract strong cats brave outgoing husky coordinated important rustic juggle   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-11-14 10:02:49,1
Intel,nosrlfs,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,AMD,2025-11-14 12:33:14,1
Intel,nosysjm,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",AMD,2025-11-14 13:20:27,1
Intel,not149u,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",AMD,2025-11-14 13:34:35,1
Intel,notb7lg,I'm glad the CPU metrics are showing again,AMD,2025-11-14 14:31:49,1
Intel,notcd57,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",AMD,2025-11-14 14:38:07,1
Intel,notlcun,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",AMD,2025-11-14 15:24:40,1
Intel,notm5ep,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),AMD,2025-11-14 15:28:36,1
Intel,notufou,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,AMD,2025-11-14 16:09:14,1
Intel,noue3ki,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",AMD,2025-11-14 17:47:54,1
Intel,nouooi9,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",AMD,2025-11-14 18:39:53,1
Intel,noutw0a,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 😅   What a fucking joke",AMD,2025-11-14 19:05:43,1
Intel,nov8foi,Shits been crashing my system since the update :( sapphire 7900xt,AMD,2025-11-14 20:21:15,1
Intel,novg42t,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",AMD,2025-11-14 21:02:02,1
Intel,nowdvrw,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,AMD,2025-11-15 00:14:02,1
Intel,noydj17,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",AMD,2025-11-15 09:37:48,1
Intel,noypu29,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",AMD,2025-11-15 11:42:58,1
Intel,noyv323,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",AMD,2025-11-15 12:27:57,1
Intel,nozb3zp,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",AMD,2025-11-15 14:16:00,1
Intel,noze8xv,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,AMD,2025-11-15 14:34:56,1
Intel,nozoxq5,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",AMD,2025-11-15 15:35:11,1
Intel,nozv077,У меня Мультимедиа контроллер выдает ошибку. Для этого устройства отсутствуют совместимые драйверы. (Код 28),AMD,2025-11-15 16:06:54,1
Intel,np0n0ro,Noise Suppression still broken. 3rd release without that functionality in a row.,AMD,2025-11-15 18:33:00,1
Intel,np0qihb,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalación del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",AMD,2025-11-15 18:50:19,1
Intel,np0sz88,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",AMD,2025-11-15 19:02:53,1
Intel,np2gy28,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",AMD,2025-11-16 00:44:12,1
Intel,np2igku,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,AMD,2025-11-16 00:53:10,1
Intel,np2iy25,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",AMD,2025-11-16 00:56:07,1
Intel,np2n7ns,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",AMD,2025-11-16 01:22:23,1
Intel,np2rc23,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,AMD,2025-11-16 01:46:41,1
Intel,np3zqgd,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me 🙏",AMD,2025-11-16 07:09:06,1
Intel,np4btup,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",AMD,2025-11-16 09:09:18,1
Intel,np4c4bj,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",AMD,2025-11-16 09:12:17,1
Intel,np59xsp,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,AMD,2025-11-16 14:05:46,1
Intel,np5tc80,Still not working AMD NOISE S,AMD,2025-11-16 15:57:27,1
Intel,np5w51d,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,AMD,2025-11-16 16:12:01,1
Intel,np6sb4d,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",AMD,2025-11-16 18:55:53,1
Intel,np75mw5,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,AMD,2025-11-16 20:02:08,1
Intel,np7fiy7,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,AMD,2025-11-16 20:52:53,1
Intel,np9tmrb,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",AMD,2025-11-17 05:29:10,1
Intel,npa497n,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",AMD,2025-11-17 07:05:30,1
Intel,npbc7th,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",AMD,2025-11-17 13:41:36,1
Intel,npbdww5,"Unfortunately, version 25.11.1 does not start with Windows.",AMD,2025-11-17 13:51:35,1
Intel,npcr8ua,Is AMD going to come up with another driver soon?,AMD,2025-11-17 18:04:51,1
Intel,npd465l,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",AMD,2025-11-17 19:07:02,1
Intel,npeqls5,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,AMD,2025-11-18 00:15:16,1
Intel,npgfe4k,"After installation 25.11.1 (from 25.10.2)  black screens entered the chat. After DDU and rollback to 25.10.2 they stayed, and after rollback 25.9.1 the same... RX 5700 XT. Sadly 😞.",AMD,2025-11-18 07:12:00,1
Intel,npgq8pe,"is there 25.11.1 for windows 10? the filename that i downloaded from AMD website is ""whql-amd-software-adrenalin-edition-25.11.1-win11-s"" where usually its filename includes windows 10 along the lines",AMD,2025-11-18 09:03:29,1
Intel,npgujea,"they need to fix the BF6 texture corruption glitch, it's annoying af. had to roll back to 10.2",AMD,2025-11-18 09:49:30,1
Intel,nph1gio,Any word on fixing the driver timeouts on the 7900xtx its a bloody joke worst gpu i have ever bought,AMD,2025-11-18 10:58:38,1
Intel,nphl085,Any of you also have issues with afmf2 and the game not opening adrenalin software or showing performance counter after enabling it?,AMD,2025-11-18 13:22:24,1
Intel,npikkr4,"this shit was fucking with my PC, DDU current drivers and reinstalled 25.10 straight from Gigabyte Program and everything works again",AMD,2025-11-18 16:26:09,1
Intel,npnxcnt,getting bsod randomly since 25.9.1 sad..,AMD,2025-11-19 13:20:00,1
Intel,npowfg1,"I started having an issue since the 25.11.1 update with unreal editor where all of my tools menus instantly close, nothing else changed except for this driver update and I've heard of Nvidia having similar issues with driver updates in the past so I think it may be the cause, Going to revert to an older driver and see if it works",AMD,2025-11-19 16:29:43,1
Intel,npwkypv,"I've spent the last few days uninstalling, reinstalling, DDUing, doing everything I could think of to get Adrenaline to start/work. It would show the splash screen and then quit. No way of re-starting it. Couldn't open anything that used Vulkan and got errors. Couldn't install the Windows Store version cause ""driver error"". I eventually used DDU one last time and uninstalled everything AMD and was able to just install the driver through MyASUS. Now I'm able to open all the software again that wasn't starting before. I'll be holding off on installing Adrenaline again anytime soon. Sucks cause I want the features, but I couldn't use the programs anyway. I miss having nvidia.",AMD,2025-11-20 20:27:52,1
Intel,nq842b2,"It seems on the latest Radeon driver that freesync is broken within CS2 when running fullscreen windowed. Freesync works initially when the game starts. But as soon as I alt tab, freesync breaks and I get screen tearing. I rolled back to 25.9.1 and I can confirm it works again as expected. So it seems this is a recent regression. Can we get this addressed please? u/AMD_Vik",AMD,2025-11-22 17:50:50,1
Intel,nq9u9z3,"Been having issues with VLC freezing and stuttering during playback (video only, not audio) since anything after 25.9.1. Guess I'm gonna roll back to that until it gets figured out.... really frustrating.",AMD,2025-11-22 23:33:52,1
Intel,nqwbryc,Substance Designer won't start with this one. Access violation with amdvlk64.dll. Adrenaline won't start either,AMD,2025-11-26 15:56:06,1
Intel,ns8k1w2,"Sorry but for me the drive give me crash pop up message every time i boot up my pc. Also just right now i got a freeze, black screen to all my monitors.",AMD,2025-12-04 12:42:18,1
Intel,ns9soky,The worst driver this year so far,AMD,2025-12-04 16:45:18,1
Intel,nscxupo,"Still havent fixed the noise cancellation lmao, guess its another month+ of old version :) Thanks amd, truly doing wonders.",AMD,2025-12-05 02:44:53,1
Intel,nsgsekn,CS2 crashing with driver timeout after tabbing out or watching streams on 2nd screen 7900xtx,AMD,2025-12-05 18:29:05,1
Intel,nsqr7j8,"When is 25.12.1 coming out? I have read only bad things about 25.11.1 here, so I wanted to skip this one.",AMD,2025-12-07 10:40:19,1
Intel,nonf78x,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",AMD,2025-11-13 15:59:52,1
Intel,nond6d4,So no redstone yet,AMD,2025-11-13 15:50:09,-1
Intel,nonqjy0,FSR AI frame gen??? Didn’t they say that’d it would also have a driver toggle?,AMD,2025-11-13 16:55:25,1
Intel,nonv0vm,Did AMD ever add support for Cronos?,AMD,2025-11-13 17:17:25,1
Intel,nonxx39,Well Star Citizen will load now!  Now some longer term testing....,AMD,2025-11-13 17:31:41,1
Intel,nonw8zf,Anybody tried this with Anno 117 yet? I’m hoping it helps performance,AMD,2025-11-13 17:23:27,0
Intel,nooyqhv,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,AMD,2025-11-13 20:32:28,0
Intel,nooyuwp,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,AMD,2025-11-13 20:33:06,0
Intel,noqrxh3,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,AMD,2025-11-14 02:35:08,0
Intel,nozwu6t,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,AMD,2025-11-15 16:16:36,0
Intel,np07ekg,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,AMD,2025-11-15 17:12:12,0
Intel,noni2qa,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,AMD,2025-11-13 16:13:55,-4
Intel,noqc54j,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,AMD,2025-11-14 01:00:45,-1
Intel,np4dff7,"Here we go again, jetzt stürzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen außer XMP war aktiviert, dann stürzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das übernehmen müsst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team Grün nicht.",AMD,2025-11-16 09:25:48,-1
Intel,nonpv4u,Yeah same here LG c5 42inch 😰,AMD,2025-11-13 16:52:03,22
Intel,noockre,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesn’t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",AMD,2025-11-13 18:42:09,17
Intel,noprnhq,"I have this but on display port, HDMI works fine",AMD,2025-11-13 23:01:25,5
Intel,nonyety,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",AMD,2025-11-13 17:34:06,10
Intel,nopqt8d,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",AMD,2025-11-13 22:56:47,2
Intel,nonu691,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,AMD,2025-11-13 17:13:13,2
Intel,nop2vm5,I have the same issue with display port but it’s okay with hdmi :/,AMD,2025-11-13 20:53:31,1
Intel,nq0dwdl,"Honestly, I plan to make sure my next display has Display Port in it. Mostly for linux though.",AMD,2025-11-21 12:22:58,1
Intel,nonpu8n,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",AMD,2025-11-13 16:51:56,79
Intel,nonjytd,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,AMD,2025-11-13 16:23:11,26
Intel,noo9nj4,"V25.10.2  here… I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",AMD,2025-11-13 18:28:20,4
Intel,not85q8,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",AMD,2025-11-14 14:14:59,4
Intel,nonkdfa,combined again it looks like 🤷‍♂️,AMD,2025-11-13 16:25:10,1
Intel,nongchq,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,AMD,2025-11-13 16:05:27,98
Intel,nono7wt,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",AMD,2025-11-13 16:44:00,3
Intel,nonhdck,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",AMD,2025-11-13 16:10:27,6
Intel,np0qz7g,You try install last chipset driver ?,AMD,2025-11-15 18:52:41,1
Intel,nongxu7,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",AMD,2025-11-13 16:08:21,1
Intel,nonrxcq,So it's the driver that's why that happens 😡 and it's not fixed?,AMD,2025-11-13 17:02:09,0
Intel,noogyei,Thank you for your service,AMD,2025-11-13 19:03:14,8
Intel,nopxjjg,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",AMD,2025-11-13 23:35:26,7
Intel,nov7gjn,Any update mate?,AMD,2025-11-14 20:16:03,1
Intel,nosfu5h,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",AMD,2025-11-14 10:57:06,0
Intel,nonw38z,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",AMD,2025-11-13 17:22:40,5
Intel,npdh2mf,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",AMD,2025-11-17 20:11:40,3
Intel,np42etk,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",AMD,2025-11-16 07:35:21,2
Intel,noroh5d,"I'm the opposite, I just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",AMD,2025-11-14 06:29:26,1
Intel,nonifp9,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDU’d it again to go back to 25.9.2 since games were stuttering.",AMD,2025-11-13 16:15:40,11
Intel,noozgtx,Same.,AMD,2025-11-13 20:36:13,3
Intel,nop6flo,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",AMD,2025-11-13 21:11:26,3
Intel,noxqsoq,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",AMD,2025-11-15 05:51:40,1
Intel,nosbqvm,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,AMD,2025-11-14 10:17:54,1
Intel,nozhfiv,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,AMD,2025-11-15 14:53:42,1
Intel,nopl6z7,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",AMD,2025-11-13 22:26:21,1
Intel,noppntf,If it still crashes set RTX Global Illumination to Static.,AMD,2025-11-13 22:50:26,4
Intel,nor7jw2,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",AMD,2025-11-14 04:14:16,2
Intel,nonlw78,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",AMD,2025-11-13 16:32:40,11
Intel,notyc45,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as they’ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since they’re much deeper in engine code/inputs.",AMD,2025-11-14 16:28:32,2
Intel,noocnzc,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it 🤓",AMD,2025-11-13 18:42:35,1
Intel,noo416z,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",AMD,2025-11-13 18:01:35,1
Intel,not2qjr,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",AMD,2025-11-14 13:44:02,2
Intel,nopbmoh,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,AMD,2025-11-13 21:37:14,2
Intel,nosw536,Ugh,AMD,2025-11-14 13:03:59,2
Intel,nooumki,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",AMD,2025-11-13 20:11:38,3
Intel,noqgvkg,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",AMD,2025-11-14 01:29:31,1
Intel,nos3g9h,"Linux doesn’t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, that’s when you get driver updates, and they’re completely different from windows branch.",AMD,2025-11-14 08:53:47,1
Intel,noqn7n2,just uninstall it I prefer manual check myself.,AMD,2025-11-14 02:07:15,2
Intel,nor7u07,So AMDs default driver overclocks and doesn’t reflect that in the values?,AMD,2025-11-14 04:16:14,1
Intel,nqsncxf,Same issues here i underclocked it but this new update just made it worse,AMD,2025-11-26 00:03:57,1
Intel,np5tu2z,ok it is still crashing ... complete reboot :(,AMD,2025-11-16 16:00:03,1
Intel,nq4e73q,"I feel like that crash is more on DICE's side, since Nvidia users get the same exact crash, although less often.  I tried everything I saw on the internet, nothing really works. Sometimes I can play for hours on end, other time game just crashes randomly after 10-15 minutes.  I am going to try to downgrade to 25.9.1 and see how it fares, since I remember that driver being really stable for me (6800XT).  Edit: been playing for 4 hours, no crash yet. Never had such a long session without the game crashing.  Will update in the next few days.  Edit 2: haven't crashed once, been playing at least 2 hours every evening.",AMD,2025-11-22 01:28:20,1
Intel,nope0rx,Okay.,AMD,2025-11-13 21:49:03,1
Intel,nonl1up,I’m hoping Valve’s new steam machine will push them on that since it’s RDNA3 based.,AMD,2025-11-13 16:28:30,19
Intel,noukbhw,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,AMD,2025-11-14 18:18:29,3
Intel,nooggfu,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",AMD,2025-11-13 19:00:46,4
Intel,noozx8g,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,AMD,2025-11-13 20:38:33,1
Intel,noptibm,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",AMD,2025-11-13 23:12:01,1
Intel,noo53y9,welcome to amd,AMD,2025-11-13 18:06:50,0
Intel,nonvvaf,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,AMD,2025-11-13 17:21:35,1
Intel,nooca2m,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",AMD,2025-11-13 18:40:45,1
Intel,np1vdc1,Same. Never even had Ryzen master installed.,AMD,2025-11-15 22:35:21,2
Intel,npiam42,"I'm receiving the same error in Event Viewer, but I have installed Ryzen Master. Most likely it's also a component of the Adrenalin drivers for system tuning and monitoring.  Registry search shows two keys for ""AMDRyzenMasterDriverV30"" (in both CurrentControlSet and ControlSet001): Computer\\HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\AMDRyzenMasterDriverV30  The ImagePath points to: C:\\Windows\\System32\\AMDRyzenMasterDriver.sys and the file exists. It's valid.",AMD,2025-11-18 15:37:33,1
Intel,nowsbia,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,AMD,2025-11-15 01:45:44,1
Intel,nonegtb,What is redstone?,AMD,2025-11-13 15:56:21,7
Intel,nonnq47,What's weird is Black Ops 7 has ray regeneration.,AMD,2025-11-13 16:41:37,7
Intel,none418,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",AMD,2025-11-13 15:54:38,3
Intel,nontlx6,vsync issue fixed with win 11 KB5068861 update.,AMD,2025-11-13 17:10:27,12
Intel,nonxa48,had no issues with vsync on 25.10.2,AMD,2025-11-13 17:28:33,4
Intel,nons4sz,works fine for me,AMD,2025-11-13 17:03:10,6
Intel,noorn1m,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,AMD,2025-11-13 19:56:34,1
Intel,nopcb8w,"That it did, lol. My only complaint.",AMD,2025-11-13 21:40:35,1
Intel,noqh6ym,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",AMD,2025-11-14 01:31:26,0
Intel,nonl36f,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,AMD,2025-11-13 16:28:41,3
Intel,noovbth,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,AMD,2025-11-13 20:15:10,2
Intel,nonpyiq,"Fps drop over time? That's a game issue, it's got a memory leak",AMD,2025-11-13 16:52:31,5
Intel,nopz2ou,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",AMD,2025-11-13 23:44:30,1
Intel,nov7ewn,I’d settle for bf6 going one entire game without drivers crashing the game and freezing pc,AMD,2025-11-14 20:15:48,1
Intel,noorxgl,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",AMD,2025-11-13 19:58:00,1
Intel,nov7k59,Crashes?,AMD,2025-11-14 20:16:34,1
Intel,nowyxe0,I have this problem in all games.,AMD,2025-11-15 02:28:26,1
Intel,nprco16,"Hello, I've been having this issue and I have exactly your gpu and cpu, whenever I played valorant and I alt tabed many times the screen goes black and keyboard become unresponsive but I can still hear friends in discord and they can't hear me, after conctacting valorant support and messing with alot of settings I think  what fixed it for me is to add these in windows defender exclusions : C:\\Riot Games\\VALORANT\\live\\VALORANT.exe   C:\\Riot Games\\VALORANT\\live\\ShooterGame\\Binaries\\Win64\\VALORANT-Win64-Shipping   C:\\Program Files\\Riot Vanguard\\vgc.exe   C:\\Program Files\\Riot Vanguard\\vgm.exe   C:\\Riot Games\\Riot Client\\RiotClientServices.exe   I hope this helps",AMD,2025-11-20 00:04:27,1
Intel,not2cbd,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",AMD,2025-11-14 13:41:44,1
Intel,np3fwq0,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",AMD,2025-11-16 04:25:12,1
Intel,norugaj,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",AMD,2025-11-14 07:24:32,3
Intel,nopmfkv,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,AMD,2025-11-13 22:33:01,2
Intel,noscuea,Epic version runs just fine.,AMD,2025-11-14 10:28:45,3
Intel,not9drm,Cyberpunk GOG last version patch runs fine on this driver.,AMD,2025-11-14 14:21:42,1
Intel,nosnl0p,"Hey there, can you give an example of how this looks now versus how it's supposed to?",AMD,2025-11-14 12:03:38,2
Intel,noso7o5,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,AMD,2025-11-14 12:08:26,1
Intel,nou0ebb,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,AMD,2025-11-14 16:38:48,1
Intel,nox9yy0,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,AMD,2025-11-15 03:41:47,2
Intel,nou7nae,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,AMD,2025-11-14 17:15:04,2
Intel,noypui8,"The game is booting, this message was for the 25.10 they just didn't removed it",AMD,2025-11-15 11:43:04,2
Intel,noza0c5,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,AMD,2025-11-15 14:09:14,1
Intel,np31dy5,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,AMD,2025-11-16 02:48:35,1
Intel,npgrqyr,"First time yes, i downloaded with -s letter, but the last time i downloaded smth like -combined(1.6 gb). All two's is for WIn 11.",AMD,2025-11-18 09:19:37,1
Intel,nqit1yt,"To be clear, are you able to confirm that VRR is disabled after you alt-tab? Do you have a display-side OSD to verify?",AMD,2025-11-24 12:56:50,1
Intel,nsv6cts,"Good call, it caused nothing but problems for me and pretty severe. Were talking driver timeouts with black screens and even a couple bluescreens.",AMD,2025-12-08 01:50:11,1
Intel,nonny1j,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",AMD,2025-11-13 16:42:40,5
Intel,nopggve,My 9070 xt crushes while I try to use fsr 4 on new drivers,AMD,2025-11-13 22:01:20,1
Intel,noo04cb,Why don't you try it and let us know if you can. Would be helpful for lots of us,AMD,2025-11-13 17:42:31,1
Intel,nont8g8,It's in Redstone. Still not out yet,AMD,2025-11-13 17:08:37,4
Intel,nopd6c2,Didn't work for me...,AMD,2025-11-13 21:44:51,1
Intel,not23h8,Wait until you see how much your browser's cache is churning...,AMD,2025-11-14 13:40:17,2
Intel,notlyfp,Why cant you use Adrenalin? I'm using it on 25.9.1,AMD,2025-11-14 15:27:40,1
Intel,nq0kohy,I just received a windows extension update for my LG monitor. If you can boot up go check.,AMD,2025-11-21 13:08:34,1
Intel,nopw101,The last time I had this problem it was a RAM issue.,AMD,2025-11-13 23:26:32,4
Intel,npd560g,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,AMD,2025-11-17 19:11:57,1
Intel,norotfv,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",AMD,2025-11-14 06:32:31,1
Intel,nood411,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,AMD,2025-11-13 18:44:41,9
Intel,noo4uio,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",AMD,2025-11-13 18:05:33,4
Intel,nooyzy7,Do u reintall already up to date chipset drivers?,AMD,2025-11-13 20:33:49,1
Intel,norplxi,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,AMD,2025-11-14 06:39:36,1
Intel,nonuzmx,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",AMD,2025-11-13 17:17:15,3
Intel,nonxvx2,doing so (separation) will create a freak out shitstorm part 2.,AMD,2025-11-13 17:31:31,11
Intel,nonzgmu,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,AMD,2025-11-13 17:39:17,15
Intel,nonz6zk,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),AMD,2025-11-13 17:37:57,11
Intel,nons9ct,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,AMD,2025-11-13 17:03:49,19
Intel,nooofaj,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",AMD,2025-11-13 19:40:29,2
Intel,nony71m,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,AMD,2025-11-13 17:33:02,16
Intel,nonscqs,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,AMD,2025-11-13 17:04:17,1
Intel,nonuj7z,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,AMD,2025-11-13 17:15:00,23
Intel,nonzcwc,Thank you for communicating,AMD,2025-11-13 17:38:47,11
Intel,nononki,Unfortunately happens to me too. So for me it’s a big issue as I can’t update to this driver until it is fixed 😰,AMD,2025-11-13 16:46:06,5
Intel,nooyj1v,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.  Issue goes away using a non 4k 240hz display.     I believe this system crash is deeply related to DSC on Windows.  I only got these two PC bsods when I bought a 4k 240hz display.  Returned a monitor (bad oled) and the issue went away.  Got a new oled a few weeks ago and now I have these bsods again.     Never had a bsod before I got these 4k 240hz displays.  Fresh Windows 11 installs too between both PCs and between my first and second oled.  Systems are both solid and stable.     Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.  Hopefully someone else had experience with them on 4k 240hz.",AMD,2025-11-13 20:31:24,6
Intel,nons1mi,Thank you AMD my bad for getting upset,AMD,2025-11-13 17:02:44,5
Intel,nongngq,Thank you.,AMD,2025-11-13 16:06:56,2
Intel,noobm6s,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why it’s failing. Would be cool to see the technical details if that’s possible. (I’m actually more interested now on why it’s not working vs just getting it fixed).,AMD,2025-11-13 18:37:36,2
Intel,nop1khf,Thank you!,AMD,2025-11-13 20:46:53,2
Intel,nop0wol,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,AMD,2025-11-13 20:43:34,1
Intel,nonlavb,Redstone when?,AMD,2025-11-13 16:29:44,0
Intel,nonhqde,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",AMD,2025-11-13 16:12:13,15
Intel,nonlm2a,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,AMD,2025-11-13 16:31:16,13
Intel,nonz8d7,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",AMD,2025-11-13 17:38:09,6
Intel,nonjrum,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",AMD,2025-11-13 16:22:14,5
Intel,nonovmq,Non pc monitor tvs are sometimes cheaper especially for larger sizes. I’m on lg c5 oled 42inch and it only has hdmi…,AMD,2025-11-13 16:47:11,5
Intel,noo0hf6,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",AMD,2025-11-13 17:44:18,3
Intel,nonq4py,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",AMD,2025-11-13 16:53:21,2
Intel,noolj45,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",AMD,2025-11-13 19:25:57,1
Intel,noqqvuj,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",AMD,2025-11-14 02:28:55,1
Intel,nosnlnp,> Are y'all playing on televisions?  Do you guys not have phones?,AMD,2025-11-14 12:03:46,0
Intel,nonhcwn,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,AMD,2025-11-13 16:10:23,2
Intel,noqf5pn,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,AMD,2025-11-14 01:19:04,7
Intel,noqno3l,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",AMD,2025-11-14 02:09:58,1
Intel,nov6ye9,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",AMD,2025-11-14 20:13:22,1
Intel,noyds7c,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",AMD,2025-11-15 09:40:31,1
Intel,nsoev4p,Why does it seem like driver quality/support has gotten substantially worse this past decade? Are we running out of skilled software engineers or is hardware just getting too out of hand?,AMD,2025-12-06 23:58:22,1
Intel,noshsep,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,AMD,2025-11-14 11:15:06,4
Intel,nonx5ls,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",AMD,2025-11-13 17:27:56,1
Intel,np52n5a,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",AMD,2025-11-16 13:17:38,3
Intel,np1d4kt,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",AMD,2025-11-15 20:53:05,2
Intel,nonis5q,OK thought I was the only one. 25.10 is bad bad,AMD,2025-11-13 16:17:22,4
Intel,nood354,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",AMD,2025-11-13 18:44:34,1
Intel,nos072w,Thanks for testing it,AMD,2025-11-14 08:20:41,1
Intel,np22kzb,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",AMD,2025-11-15 23:18:12,1
Intel,nosh56b,I thought FSR 4 was only on RDNA 4? 🤔,AMD,2025-11-14 11:09:11,1
Intel,nozuikm,My thoughts exactly. Thanks.,AMD,2025-11-15 16:04:18,1
Intel,nopzlun,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,AMD,2025-11-13 23:47:39,2
Intel,noq7kwh,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,AMD,2025-11-14 00:33:58,2
Intel,not1lyv,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,AMD,2025-11-14 13:37:28,1
Intel,noojjne,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",AMD,2025-11-13 19:16:05,1
Intel,nooj67g,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",AMD,2025-11-13 19:14:14,2
Intel,nopbvvl,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,AMD,2025-11-13 21:38:30,5
Intel,nouxgnr,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,AMD,2025-11-14 19:23:57,1
Intel,norm1yc,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",AMD,2025-11-14 06:08:00,3
Intel,npawrxf,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",AMD,2025-11-17 11:52:53,1
Intel,noo4anu,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,AMD,2025-11-13 18:02:52,8
Intel,nosbylr,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",AMD,2025-11-14 10:20:00,0
Intel,nphuo0h,I guess you can't drop any hints as to whether this work with CDPR also involves adding Ray Regeneration to the game 👀?,AMD,2025-11-18 14:14:35,1
Intel,noojmmw,Fun fact - i am dual booting and on Linux this bug is not existent...:)),AMD,2025-11-13 19:16:29,2
Intel,noockb2,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",AMD,2025-11-13 18:42:05,2
Intel,nqw6fd5,Hi. Did you ever resolve this? I'm getting the same error. Thanks.,AMD,2025-11-26 15:29:31,1
Intel,noznq2r,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",AMD,2025-11-15 15:28:41,1
Intel,nongh5i,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,AMD,2025-11-13 16:06:05,7
Intel,noozl1i,It's a thing you can search for on Google,AMD,2025-11-13 20:36:49,1
Intel,nonsdkw,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,AMD,2025-11-13 17:04:24,3
Intel,nonm0k8,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",AMD,2025-11-13 16:33:16,-1
Intel,nonxcza,ahh i'm on Win 10 so probably why I didn't see it.,AMD,2025-11-13 17:28:57,2
Intel,noorjtx,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,AMD,2025-11-13 19:56:07,2
Intel,nonn11b,"Yes, but was it in the previous WHQL driver ? I'm not sure.",AMD,2025-11-13 16:38:12,1
Intel,nons5g1,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),AMD,2025-11-13 17:03:16,2
Intel,nos7pyk,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:37:40,1
Intel,nqrxf23,"I tried everything I saw online: meshes on low, XMP lower/off, chipset drivers reinstall and other stuff. Nothing worked.  I downgraded back to 25.9.1., haven't had a crash in days.  Kinda miss the improvements for AFMF they brought with 25.10 for other games, but eh I'd rather play BF6 without it crashing randomly.",AMD,2025-11-25 21:39:49,1
Intel,nov8osy,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",AMD,2025-11-14 20:22:35,1
Intel,np3gi86,Either launch with curseforge or rollback,AMD,2025-11-16 04:29:38,1
Intel,nopmt4r,"Damn, didn’t work for me last driver either. I can get FSR4 to work in other games just not BF6",AMD,2025-11-13 22:35:02,1
Intel,nr5w3fb,Sorry for not replying in time with the pictures but I just saw that on Twitter that Beat Saber and AMD are now aware of the issue. The distorted flickering issue on the walls.  https://xcancel.com/BeatSaber/status/1993629046802882685  However there's another issue. I had not actually tried to use an Index at 90Hz until the other day. I discovered that the latency bug is back for 90Hz mode. As in I have to adjust the photon latency to ~5ms in the Steam debug commands to make it usable but not fixed. Just like in the the drivers before 24.12.1.   120Hz mode still works fine.,AMD,2025-11-28 03:55:10,1
Intel,noxy5g3,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",AMD,2025-11-15 07:00:01,1
Intel,noz0zh9,You 100 procent sure on this?,AMD,2025-11-15 13:12:06,1
Intel,np59d5s,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,AMD,2025-11-16 14:02:08,1
Intel,np32vom,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",AMD,2025-11-16 02:57:54,2
Intel,npgs1he,Driver with -s letter after black screen and reboot PC tells me that this driver isn't for my graphic card🤡,AMD,2025-11-18 09:22:45,1
Intel,nswxbvi,"I had randomly black screens with 24.2.1, this was annoying as hell. Had to DDU the Driver and went back to 23.11.1, after this everything was fine.",AMD,2025-12-08 10:15:23,1
Intel,nonplo5,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,AMD,2025-11-13 16:50:46,1
Intel,noolo6b,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",AMD,2025-11-13 19:26:39,2
Intel,nov5qbd,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,AMD,2025-11-14 20:06:52,1
Intel,nq588dp,What do you mean extension update??? Do you mean lg firmware update or something Ina  windows update? Where do I find this?,AMD,2025-11-22 04:54:10,1
Intel,noo85c3,They do not.,AMD,2025-11-13 18:21:19,5
Intel,np5srze,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",AMD,2025-11-16 15:54:29,1
Intel,noo8tps,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,AMD,2025-11-13 18:24:30,11
Intel,noo2nnu,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,AMD,2025-11-13 17:54:54,8
Intel,noolzkz,"AND is taking away one additional driver feature per day, you say?",AMD,2025-11-13 19:28:13,1
Intel,nooy45h,"Yes, I’m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select “GPU” you get a file that has a different dimension from the one you download if you choose “CPU”. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with “minimal_install), but Adrenalin App does not open.",AMD,2025-11-13 20:29:17,4
Intel,noo1i55,Thank you for explaining it before the rage baiters go nuts.,AMD,2025-11-13 17:49:17,0
Intel,noo3cx3,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,AMD,2025-11-13 17:58:18,46
Intel,nooncln,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",AMD,2025-11-13 19:35:04,3
Intel,noo0xcy,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",AMD,2025-11-13 17:46:27,14
Intel,nopu61n,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",AMD,2025-11-13 23:15:46,6
Intel,nopc45s,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",AMD,2025-11-13 21:39:38,2
Intel,noo3fsu,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,AMD,2025-11-13 17:58:41,24
Intel,npp1edb,Was yours the DisplayPort config or HDMI? I may have a fix for this ready if you're available test,AMD,2025-11-19 16:54:16,1
Intel,nopvrx5,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,AMD,2025-11-13 23:25:04,3
Intel,noo53xx,Already launched in COD 7,AMD,2025-11-13 18:06:50,3
Intel,nonp7d7,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so you’re saying i shoulf switch to hdmi?",AMD,2025-11-13 16:48:48,3
Intel,nonwqs3,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,AMD,2025-11-13 17:25:53,1
Intel,norbib0,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,AMD,2025-11-14 04:42:59,1
Intel,noni0s3,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",AMD,2025-11-13 16:13:39,1
Intel,nor1k1x,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,AMD,2025-11-14 03:34:28,1
Intel,nonzc4h,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",AMD,2025-11-13 17:38:40,2
Intel,noshb1m,With the compiled leaked DLL you can use it on RDNA3 as well.,AMD,2025-11-14 11:10:42,1
Intel,nosbtoj,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",AMD,2025-11-14 10:18:39,2
Intel,notnotg,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,AMD,2025-11-14 15:36:20,1
Intel,noqg8tt,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),AMD,2025-11-14 01:25:43,4
Intel,noonewp,Thank you! Exciting keen to see what it’s like,AMD,2025-11-13 19:35:23,1
Intel,noosgem,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",AMD,2025-11-13 20:00:38,2
Intel,nopjngc,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,AMD,2025-11-13 22:18:03,2
Intel,nphu5po,"The issue is if you try to use path tracing. Which to be fair, you probably shouldn't unless the miracle of them getting Virtuous to implement Ray Regeneration in Cyberpunk happens.",AMD,2025-11-18 14:11:53,1
Intel,npb27so,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,AMD,2025-11-17 12:35:58,1
Intel,nopnm90,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,AMD,2025-11-13 22:39:21,1
Intel,nqw82e8,"Yes. So far, so good. I'm not 100% sure what fixed it.      I uninstalled both Adrenalin and Ryzen Master standalone applications. Deleted the ""amdryzenmasterv"" keys. Rebooted.  Then I installed Adrenalin and used the Ryzen Master installer in Adrenalin (Performance > Metrics > Install Ryzen Master).  I think this problem might have something to do with a handshake breaking between Ryzen Master and Adrenalin, after upgrading just Adrenalin.   From now on, I'll probably do clean installs, removing and reinstalling both Adrenalin and Ryzen Master, through Adrenalin Performance tab.",AMD,2025-11-26 15:37:49,2
Intel,nozpqvb,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",AMD,2025-11-15 15:39:28,1
Intel,nonmi38,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",AMD,2025-11-13 16:35:37,4
Intel,nony81v,lmao chill out dude go touch some grass,AMD,2025-11-13 17:33:11,6
Intel,nonozwd,Could be grounds for lawsuit… That’s funny!,AMD,2025-11-13 16:47:47,3
Intel,norvwn6,Because of MPO.,AMD,2025-11-14 07:38:45,5
Intel,noq77oq,yeah same with 25.11.1 25.9.2 works for me,AMD,2025-11-14 00:31:52,1
Intel,nonv3ns,"25.10.2 was the previous WHQL, so also yes :P",AMD,2025-11-13 17:17:48,2
Intel,noo30h0,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",AMD,2025-11-13 17:56:38,1
Intel,noxixmd,Which driver version and does it still crashing?,AMD,2025-11-15 04:47:44,1
Intel,nopn1gw,OK I will install it now and test it and get back to you. Give me 10 mins.,AMD,2025-11-13 22:36:16,2
Intel,noppoge,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,AMD,2025-11-13 22:50:32,2
Intel,nr7hjjv,I'll work with the engineer from that ticket check if that issue has somehow regressed.,AMD,2025-11-28 12:30:29,2
Intel,nrptkfo,We've not been able to reproduce this internally so far. Can you remind me which GPU (was this a 7900XTX?) + connectivity method you're using?,AMD,2025-12-01 14:48:30,1
Intel,noztt6w,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,AMD,2025-11-15 16:00:37,2
Intel,np0adh6,"Yup just need to say ""No""",AMD,2025-11-15 17:28:00,2
Intel,npgto9y,"whew thanks, good think i noticed it first before updating. i have 25.10.2 and 25.9.2 here and they both have windows 10 along their filename so i might as well asked.",AMD,2025-11-18 09:40:23,1
Intel,nonps7q,I don't see how it would work on 23.9.1 lol,AMD,2025-11-13 16:51:40,-1
Intel,nov73co,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",AMD,2025-11-14 20:14:06,1
Intel,noprr9f,I did it this morning before the new driver and confirm chipset drivers were untouched,AMD,2025-11-13 23:02:01,3
Intel,noooxx5,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and don’t use the latest drivers. At least AMD owned up to it so I can’t be too upset but hopefully they really do fix this soon as new users may not understand what’s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly it’s stable for them and they don’t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs don’t always have a DP connector at all.,AMD,2025-11-13 19:43:06,4
Intel,noo4q8p,"ah, that explains it. Thanks. :)",AMD,2025-11-13 18:04:59,1
Intel,nooab1c,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named “minimal install”). Obviously I’m referring to AMD driver download page.",AMD,2025-11-13 18:31:24,1
Intel,nop73kl,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",AMD,2025-11-13 21:14:50,1
Intel,novl7li,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",AMD,2025-11-14 21:28:30,1
Intel,noo4i0q,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",AMD,2025-11-13 18:03:52,21
Intel,noo7r27,What about Noise Suppression not working since 25.9.2?,AMD,2025-11-13 18:19:27,8
Intel,np8f5i6,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,AMD,2025-11-17 00:06:17,2
Intel,nopub91,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",AMD,2025-11-13 23:16:35,1
Intel,noo3ufw,Hell yeah 🙂 amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,AMD,2025-11-13 18:00:40,7
Intel,norjsvf,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",AMD,2025-11-14 05:48:36,2
Intel,nonrmjz,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",AMD,2025-11-13 17:00:38,9
Intel,noo81ru,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,AMD,2025-11-13 18:20:51,5
Intel,nosmcf6,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",AMD,2025-11-14 11:53:57,3
Intel,noniq65,That was my very first actual driver issue I experienced with AMD.,AMD,2025-11-13 16:17:06,3
Intel,noshl11,Oh that's nice! I'll look into it when I get the chance.,AMD,2025-11-14 11:13:15,1
Intel,nosh6j0,Cool. Thank you,AMD,2025-11-14 11:09:32,1
Intel,notszvs,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience 😖,AMD,2025-11-14 16:02:13,1
Intel,not1h8l,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,AMD,2025-11-14 13:36:42,2
Intel,npb8iqb,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",AMD,2025-11-17 13:18:49,1
Intel,npbldor,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",AMD,2025-11-17 14:34:50,1
Intel,noprwpb,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,AMD,2025-11-13 23:02:52,2
Intel,nonmz2f,"Fair enough, and yeah sooner the better for all of us",AMD,2025-11-13 16:37:55,0
Intel,noofit0,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,AMD,2025-11-13 18:56:16,-1
Intel,nozg3tu,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",AMD,2025-11-15 14:45:59,1
Intel,nopq3va,Fingers crossed,AMD,2025-11-13 22:52:53,1
Intel,nsxlbh0,"Thanks for attempting to retest.  It's a 7900XTX with an Index connected via DisplayPort. I am on the latest 25.11.1 driver.  I run a monitor at 4k 120Hz 10bpc with HDR Off, which uses DSC, as my main and only display. I tried disabling DSC in the monitor settings which runs at 4k 120Hz 8bpc with HDR Off but I don't think I noticed a change in latency. I thought that DSC on and off on two different devices might contribute to the problem but I'm not sure.   I have also tried running the Index under a RX480 on another PC and I fairly certain the latency looks different under 90Hz and looks similar under 120Hz. Can't play much to test though as an RX480 runs the Index at a very blurry setting. Getting around to doing this test is what took me so long to reply.",AMD,2025-12-08 13:31:32,2
Intel,ntkkg7z,Were you able to find the issue?,AMD,2025-12-12 01:49:15,1
Intel,np0lx2u,"Allright ty, will Install new, any differences in performance?",AMD,2025-11-15 18:27:26,1
Intel,nphlmf1,"im running 25.11.1 on win10 7900xt. no problems besides afmf2 breaking the performance overlay, which ive had for multiple updates now",AMD,2025-11-18 13:25:51,1
Intel,nonq3uo,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,AMD,2025-11-13 16:53:14,2
Intel,npb8jiw,Thank you for this. This was very helpful. Got adrenaline working fine now.,AMD,2025-11-17 13:18:58,2
Intel,noot79m,"I wish my LG C4 42"" had a display port. Its my primary monitor.",AMD,2025-11-13 20:04:24,3
Intel,nop8j9i,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man it’s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for me… and have zero time to reinstall Windows.",AMD,2025-11-13 21:21:55,1
Intel,nqeioib,"Don't do that, i'm suffering with both 7900XTX + RVII (and even with RX6400)",AMD,2025-11-23 19:07:08,1
Intel,npaqybw,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",AMD,2025-11-17 11:00:13,3
Intel,nopyh74,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",AMD,2025-11-13 23:40:57,4
Intel,notchza,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,AMD,2025-11-14 14:38:51,2
Intel,nosoenw,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",AMD,2025-11-14 12:09:54,1
Intel,notd4le,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",AMD,2025-11-14 14:42:17,1
Intel,notu48n,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,AMD,2025-11-14 16:07:41,1
Intel,npblkwc,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,AMD,2025-11-17 14:35:55,1
Intel,nphlnml,also i have coil whine since this driver 25.11.1. ?!  also in idle sometimes...  very strange driver...,AMD,2025-11-18 13:26:02,1
Intel,noru29k,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,AMD,2025-11-14 07:20:44,1
Intel,np08w4v,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",AMD,2025-11-15 17:20:05,1
Intel,ntmuect,We've still not been able to reproduce this unfortunately. I'll need to check in when I'm back at work next year,AMD,2025-12-12 12:24:51,2
Intel,np0tp7f,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,AMD,2025-11-15 19:06:34,2
Intel,nphr5q3,"did you download the same filename with the one i mentioned? i tried downloading windows 11 link and it also gave me the same filename, lol",AMD,2025-11-18 13:55:53,1
Intel,nopc4t4,No you can't.,AMD,2025-11-13 21:39:44,1
Intel,nonrg54,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",AMD,2025-11-13 16:59:46,0
Intel,npbfbpp,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",AMD,2025-11-17 13:59:52,1
Intel,noq4fcn,"They are TV's, not pc monitors. Buy the right tool for the job",AMD,2025-11-14 00:15:47,-2
Intel,noxv18g,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",AMD,2025-11-15 06:30:19,1
Intel,np729v3,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",AMD,2025-11-16 19:45:29,1
Intel,npiownv,"Yeah, I'm facing the same issue on RX 9060 XT   Is it a GPU driver issue, or a Windows issue that Microsoft needs to fix?",AMD,2025-11-18 16:47:14,1
Intel,nrkoujc,since last BF6 Update i had zero crashes also on 25.11.1,AMD,2025-11-30 18:13:02,2
Intel,noruco5,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",AMD,2025-11-14 07:23:31,2
Intel,np08z2a,What about 25.11.1?,AMD,2025-11-15 17:20:29,1
Intel,npkeuqy,Yeah same for me. Considering how similair win10 and 11 are under the hood i just went with it. Still absolutely no problems sofar.,AMD,2025-11-18 21:52:44,1
Intel,nopdsez,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,AMD,2025-11-13 21:47:54,3
Intel,noo8n6z,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",AMD,2025-11-13 18:23:39,2
Intel,nonsm12,"Look online for fsr 4 on 6000 and 5000 series, you will understand,    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",AMD,2025-11-13 17:05:33,2
Intel,np73g8a,Did you reboot after setting that key? Is the display with chrome still only partially updating?,AMD,2025-11-16 19:51:24,1
Intel,norvx55,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",AMD,2025-11-14 07:38:54,1
Intel,npkhv83,thank you,AMD,2025-11-18 22:08:07,1
Intel,nopey1i,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",AMD,2025-11-13 21:53:39,1
Intel,noozt1l,"Not a typo, I was asking about something else and he missed my point...",AMD,2025-11-13 20:37:57,2
Intel,norw6su,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",AMD,2025-11-14 07:41:30,2
Intel,nopm704,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",AMD,2025-11-13 22:31:45,2
Intel,nopq646,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",AMD,2025-11-13 22:53:13,0
Intel,mz2hn4c,"What a disgusting build, I love it",AMD,2025-06-21 23:44:28,157
Intel,mz2c56w,the content we crave,AMD,2025-06-21 23:11:17,79
Intel,mz2taf0,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",AMD,2025-06-22 00:56:32,49
Intel,mz35qhi,What GPU are you using in your build?  All of them,AMD,2025-06-22 02:15:29,15
Intel,mz34fmt,you're one hell of a doctor. mad setup!,AMD,2025-06-22 02:07:07,6
Intel,mz38u8t,The amount of blaspheming on display is worthy of praise.,AMD,2025-06-22 02:35:37,4
Intel,mz4f388,Brother collecting them like infinity stones lmao,AMD,2025-06-22 08:29:44,3
Intel,mz4ibrt,I'm sure those GPUs fight each others at night,AMD,2025-06-22 09:02:18,3
Intel,mz4o6eq,Bro unlocked the forbidden RGB gpus combo,AMD,2025-06-22 10:01:39,4
Intel,mz3lb45,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,AMD,2025-06-22 04:02:59,3
Intel,mz419ab,What the fuck,AMD,2025-06-22 06:15:48,3
Intel,mz520aa,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,AMD,2025-06-22 12:03:18,3
Intel,mz8w6af,Yuck,AMD,2025-06-23 00:36:46,3
Intel,mz3q5i1,Wait until you discover lossless scaling,AMD,2025-06-22 04:40:21,2
Intel,mz4pnpm,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,AMD,2025-06-22 10:16:23,2
Intel,mz4vx72,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",AMD,2025-06-22 11:13:47,2
Intel,mz57f8x,Now you just need to buy one of those ARM workstations to get the quad setup,AMD,2025-06-22 12:42:21,2
Intel,mz5dj5p,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,AMD,2025-06-22 13:22:04,2
Intel,mz65vu4,Love it lol. How do the fucking drivers work? Haha,AMD,2025-06-22 15:55:37,2
Intel,mz6knzs,What an amazing build,AMD,2025-06-22 17:11:07,2
Intel,mza30vq,wtf is that build man xdd bro collected all the infinity stones of gpu world.,AMD,2025-06-23 05:11:08,2
Intel,mzdg22n,You’re a psychopath. I love it,AMD,2025-06-23 18:23:11,2
Intel,mzeff3z,This gpu looks clean asf😭,AMD,2025-06-23 21:12:27,2
Intel,mzf9oh7,The only setup where RGB gives more performance. :D,AMD,2025-06-23 23:54:00,2
Intel,mzgj5a3,Now you need a dual cpu mobo.,AMD,2025-06-24 04:36:20,2
Intel,mzjl4ek,Placona! I've been happy with a 6700xt for years.,AMD,2025-06-24 17:04:15,2
Intel,ng0v4qd,absolute cinema,AMD,2025-09-24 21:52:34,2
Intel,mzaqf4v,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",AMD,2025-06-23 08:51:27,1
Intel,mz3qf7i,"Brawndo has electrolytes, that's what plants crave!",AMD,2025-06-22 04:42:29,48
Intel,mz2vfon,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",AMD,2025-06-22 01:09:53,21
Intel,mz3a7jh,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",AMD,2025-06-22 02:44:38,15
Intel,mz3f8hm,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",AMD,2025-06-22 03:18:58,3
Intel,n031c2v,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",AMD,2025-06-27 15:50:28,1
Intel,mz3fahp,Team RGB,AMD,2025-06-22 03:19:20,16
Intel,mz775k1,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",AMD,2025-06-22 19:03:06,4
Intel,mz3q4dh,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",AMD,2025-06-22 04:40:06,14
Intel,mz5nt69,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",AMD,2025-06-22 14:21:37,3
Intel,mz4qjhz,"OpenCL works on all of them at once, and is just as fast as CUDA!",AMD,2025-06-22 10:25:02,3
Intel,mz5onps,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",AMD,2025-06-22 14:26:11,2
Intel,mz5oxpc,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,AMD,2025-06-22 14:27:41,2
Intel,mz737je,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",AMD,2025-06-22 18:42:52,2
Intel,mzavujs,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",AMD,2025-06-23 09:45:37,1
Intel,mz3m009,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),AMD,2025-06-22 04:08:09,9
Intel,mz57a7w,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,AMD,2025-06-22 12:41:24,6
Intel,mz3lspz,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,AMD,2025-06-22 04:06:39,5
Intel,mz3kt6w,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",AMD,2025-06-22 03:59:14,3
Intel,mz3l3jt,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",AMD,2025-06-22 04:01:24,7
Intel,mz3qt8d,Thank you so much for the very detailed response!,AMD,2025-06-22 04:45:35,3
Intel,mz5oyvv,Well worth it!,AMD,2025-06-22 14:27:51,3
Intel,mz5zat7,Thank you my man!! Looking forward to run some tests once I get home.,AMD,2025-06-22 15:21:59,2
Intel,mz74o6f,That's awesome!,AMD,2025-06-22 18:50:23,2
Intel,mzbns72,"Yes, but SLI is a bad description for it.",AMD,2025-06-23 13:13:43,1
Intel,mz3s5tj,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",AMD,2025-06-22 04:56:27,18
Intel,mz4kejl,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",AMD,2025-06-22 09:23:30,7
Intel,mz64tvp,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",AMD,2025-06-22 15:50:15,3
Intel,mz3smwy,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",AMD,2025-06-22 05:00:24,3
Intel,mz40qgf,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",AMD,2025-06-22 06:11:00,3
Intel,mz56bwd,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",AMD,2025-06-22 12:34:46,4
Intel,mz4wpgy,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",AMD,2025-06-22 11:20:29,4
Intel,mzffsev,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",AMD,2025-06-24 00:29:44,2
Intel,mz4ih7t,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",AMD,2025-06-22 09:03:49,1
Intel,mz4olvb,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",AMD,2025-06-22 10:05:55,2
Intel,mz4mwra,Why are you connecting the monitor to the gpu and not the mobo?,AMD,2025-06-22 09:49:01,0
Intel,mzeajzd,"👍   thanks for the info, this'll definitely come in handy eventually.",AMD,2025-06-23 20:49:01,1
Intel,mz4oaqj,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,AMD,2025-06-22 10:02:50,2
Intel,mzehy8b,No worries mate. Good luck,AMD,2025-06-23 21:25:07,2
Intel,mz4zjpa,"For some reason I switched up, connecting to the gpu is the way to go. I derped",AMD,2025-06-22 11:44:11,3
Intel,nlb3nwr,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-10-25 13:40:37,1
Intel,ms76zj5,It's alive. Rejoice.,AMD,2025-05-14 01:54:03,3
Intel,ms6f1il,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-05-13 23:11:19,1
Intel,m84i6ct,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",AMD,2025-01-20 06:59:20,21
Intel,m84uer1,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",AMD,2025-01-20 09:01:59,16
Intel,m8861s4,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,AMD,2025-01-20 20:45:52,7
Intel,m80r0p3,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,AMD,2025-01-19 18:16:28,33
Intel,m8efiwt,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,AMD,2025-01-21 19:23:32,2
Intel,m84nhes,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,AMD,2025-01-20 07:50:12,2
Intel,m83he9u,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",AMD,2025-01-20 02:32:38,-7
Intel,m862icn,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",AMD,2025-01-20 14:58:27,7
Intel,m84neo0,I'm fairly sure they use dxvk for d3d9 to 11.,AMD,2025-01-20 07:49:28,4
Intel,m872p8h,Could just be a cache issue,AMD,2025-01-20 17:49:03,2
Intel,m8c5h0v,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,AMD,2025-01-21 12:24:17,1
Intel,m85qkad,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,AMD,2025-01-20 13:49:31,3
Intel,m80ufhx,"According to the graphs, AMD has slightly less overhead than NVIDIA.",AMD,2025-01-19 18:32:18,80
Intel,m8290el,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",AMD,2025-01-19 22:33:50,10
Intel,m874iee,"Lowest with DX11 and older, but not with the newer APIs",AMD,2025-01-20 17:56:51,1
Intel,m81i5d3,And when is the last time HUB did a dedicated video showing the improvement in overhead?,AMD,2025-01-19 20:25:39,0
Intel,m873isl,or it's just a cache/memory access issue,AMD,2025-01-20 17:52:35,1
Intel,m83l8d5,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",AMD,2025-01-20 02:54:04,21
Intel,m83sg28,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",AMD,2025-01-20 03:39:34,17
Intel,m83s1d0,"Intel uses software translation for DX11 and lower, so it does matter for them.",AMD,2025-01-20 03:36:52,0
Intel,m82afin,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",AMD,2025-01-19 22:40:55,-14
Intel,m82o5am,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",AMD,2025-01-19 23:53:09,0
Intel,m862pny,That's not true. Intel's issue is being too verbose in commands/calls.,AMD,2025-01-20 14:59:30,0
Intel,m83h5jp,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",AMD,2025-01-20 02:31:29,-15
Intel,m83sird,HUB used DX12 games that also showed the issue.  It's something else.,AMD,2025-01-20 03:40:04,6
Intel,m87xk13,"The comment to which I am replying is talking about nVidia, not Intel.",AMD,2025-01-20 20:07:14,5
Intel,m84dadg,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",AMD,2025-01-20 06:15:54,9
Intel,m83slz3,That's actually... just worse news.,AMD,2025-01-20 03:40:39,5
Intel,lfjff1l,I always dreamt of the day APUs become power houses.,AMD,2024-07-29 19:57:14,56
Intel,lfj5g73,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",AMD,2024-07-29 19:03:41,21
Intel,lfltm14,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",AMD,2024-07-30 05:13:45,2
Intel,lfqfwra,Damn Why is AMD even involved in iGPU,AMD,2024-07-30 23:50:46,1
Intel,lfjm4t2,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",AMD,2024-07-29 20:32:18,-11
Intel,lfjhomu,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",AMD,2024-07-29 20:09:09,47
Intel,lfjtsec,almost there,AMD,2024-07-29 21:13:13,3
Intel,lfkaj8b,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",AMD,2024-07-29 22:50:53,1
Intel,lfkuvgo,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",AMD,2024-07-30 00:57:59,0
Intel,lfkjnlw,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,AMD,2024-07-29 23:47:05,-4
Intel,lfjfk07,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",AMD,2024-07-29 19:57:57,22
Intel,lfkemqm,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,AMD,2024-07-29 23:15:53,2
Intel,lfjlhvn,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",AMD,2024-07-29 20:28:55,3
Intel,lgze3vw,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",AMD,2024-08-07 18:47:35,1
Intel,lfjrf1q,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",AMD,2024-07-29 21:00:13,1
Intel,lfjr0pr,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",AMD,2024-07-29 20:58:06,-7
Intel,lfjet3n,yes its so bad. better go buy some steam deck or ally x,AMD,2024-07-29 19:54:02,-11
Intel,lfjomos,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,AMD,2024-07-29 20:45:29,11
Intel,lfji4cg,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",AMD,2024-07-29 20:11:25,17
Intel,lfk18sm,How are they going to feed all those CUs? Quad-channel LPDDR5X?,AMD,2024-07-29 21:55:13,5
Intel,lfkuy27,That's considerably faster than an XSX.,AMD,2024-07-30 00:58:27,2
Intel,lfkvkit,>That's tapping on 4070/7800 levels of performance.  What is?,AMD,2024-07-30 01:02:29,3
Intel,lfmp8zh,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",AMD,2024-07-30 10:56:08,3
Intel,lfjj0he,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",AMD,2024-07-29 20:15:59,5
Intel,lfm3fxr,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,AMD,2024-07-30 06:54:17,1
Intel,lfkw2is,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,AMD,2024-07-30 01:05:44,2
Intel,lflubg9,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",AMD,2024-07-30 05:20:30,2
Intel,lfjw9yq,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",AMD,2024-07-29 21:27:05,3
Intel,lfkbfbe,It's called satire. You're just salty because you're the butt of the joke.,AMD,2024-07-29 22:56:19,-3
Intel,lfkw8g2,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,AMD,2024-07-30 01:06:50,5
Intel,lflsl6l,Praying the blade16 gets it.,AMD,2024-07-30 05:04:09,1
Intel,lfk3os9,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",AMD,2024-07-29 22:09:30,11
Intel,lfk4vp7,256 bit bus + infinity cache.,AMD,2024-07-29 22:16:36,12
Intel,lfkfxeg,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,AMD,2024-07-29 23:23:53,2
Intel,lfl3c3y,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",AMD,2024-07-30 01:53:05,1
Intel,lfl04sh,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",AMD,2024-07-30 01:32:08,1
Intel,lfovbfq,The rumored 40CU strix halo chip. Not the actual chips released this week.,AMD,2024-07-30 18:37:40,1
Intel,lfkzt9q,7500mhz ram and the 780m,AMD,2024-07-30 01:30:05,2
Intel,lflujq4,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",AMD,2024-07-30 05:22:43,2
Intel,lfm7511,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",AMD,2024-07-30 07:34:59,1
Intel,lfk4w6h,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",AMD,2024-07-29 22:16:41,8
Intel,lfkvrtv,Literally where did you see 40-60% uplift at half the power?,AMD,2024-07-30 01:03:49,5
Intel,lfnnej3,> 40-60% performance uplift at half the power  Source?,AMD,2024-07-30 14:48:25,1
Intel,lfm3q9d,"i chuckled, then again im not a fanboy of anything",AMD,2024-07-30 06:57:22,-1
Intel,lflvl1g,Dont expect 40CUs in a handheld anytime soon,AMD,2024-07-30 05:32:53,9
Intel,lfmyyqu,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",AMD,2024-07-30 12:16:43,1
Intel,lg35wq0,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",AMD,2024-08-02 03:44:51,1
Intel,lgyqo0o,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",AMD,2024-08-07 16:49:14,1
Intel,lfp60n3,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,AMD,2024-07-30 19:33:48,1
Intel,lfql0n0,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",AMD,2024-07-31 00:22:30,4
Intel,lfo4zrj,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,AMD,2024-07-30 16:22:11,1
Intel,lfoeo9v,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,AMD,2024-07-30 17:12:32,0
Intel,lukc8v1,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",AMD,2024-10-30 18:32:02,1
Intel,lukp0ww,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",AMD,2024-10-30 19:35:13,1
Intel,lukywwo,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",AMD,2024-10-30 20:22:39,1
Intel,ldaak7j,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2024-07-15 13:10:50,1
Intel,leiilpv,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2024-07-23 08:23:24,1
Intel,lekd2f5,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,AMD,2024-07-23 16:24:13,30
Intel,lejyiil,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",AMD,2024-07-23 15:07:15,20
Intel,lelur0p,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",AMD,2024-07-23 21:04:22,7
Intel,lek4mor,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,AMD,2024-07-23 15:39:41,2
Intel,leouddh,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",AMD,2024-07-24 11:04:39,1
Intel,lep6hwc,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",AMD,2024-07-24 12:39:31,1
Intel,leufb7c,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",AMD,2024-07-25 09:17:02,1
Intel,lehh8b4,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-07-23 02:41:24,1
Intel,len76ez,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,AMD,2024-07-24 01:57:07,1
Intel,lelfwyp,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,AMD,2024-07-23 19:47:16,0
Intel,lelodyi,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",AMD,2024-07-23 20:31:10,0
Intel,leki2kn,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",AMD,2024-07-23 16:50:30,3
Intel,lemusx8,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",AMD,2024-07-24 00:37:13,1
Intel,lenkqpy,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,AMD,2024-07-24 03:30:22,0
Intel,lem1iup,"Installs beta software, proceeds to complain about it",AMD,2024-07-23 21:41:28,1
Intel,lenbfz4,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,AMD,2024-07-24 02:25:00,1
Intel,lem77tu,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",AMD,2024-07-23 22:13:57,0
Intel,lelhk36,What Ghost of Tsushima issue?,AMD,2024-07-23 19:55:44,1
Intel,lelridi,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",AMD,2024-07-23 20:47:19,8
Intel,lf385p0,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",AMD,2024-07-26 20:25:40,1
Intel,leorvpo,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",AMD,2024-07-24 10:41:40,6
Intel,lelhp6y,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,AMD,2024-07-23 19:56:28,0
Intel,lem0nam,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",AMD,2024-07-23 21:36:35,-2
Intel,lf3gd3s,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",AMD,2024-07-26 21:11:19,0
Intel,lf88lah,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,AMD,2024-07-27 19:04:01,0
Intel,lezwia9,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",AMD,2024-07-26 06:45:51,1
Intel,lem6kr4,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,AMD,2024-07-23 22:10:14,8
Intel,lf1fo06,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,AMD,2024-07-26 14:36:17,2
Intel,lenktr1,The documentation for it would still be in their archives,AMD,2024-07-24 03:31:01,-2
Intel,lep98lz,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",AMD,2024-07-24 12:57:51,5
Intel,ky7tcb2,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",AMD,2024-04-05 19:25:59,24
Intel,ky7p0fb,Wish Arc cards were better. They look so pretty in comparison to their peers,AMD,2024-04-05 19:01:17,11
Intel,ky7t8hc,Thats actually a pretty solid and accurate breakdown.,AMD,2024-04-05 19:25:23,4
Intel,ky7m91o,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,AMD,2024-04-05 18:45:54,12
Intel,kyooqk9,3080 still looking good too,AMD,2024-04-08 22:34:34,2
Intel,kyakde9,What they have peaceful then 4k series?,AMD,2024-04-06 07:27:42,1
Intel,kyjljxe,Just get a 4090. I will never regret getting mine.,AMD,2024-04-07 23:42:07,1
Intel,kys0jes,i miss old good times where radeon HD 7970 as best single core card cost around 400$,AMD,2024-04-09 15:02:55,1
Intel,kzdsbrd,"Damn, the A770 is still so uncompetitive...",AMD,2024-04-13 13:49:40,1
Intel,kybklob,"It's like the free market priced cards according to their relative performance. How weird, right?",AMD,2024-04-06 13:42:41,2
Intel,kyjjx67,How is that possibly annoying,AMD,2024-04-07 23:31:52,0
Intel,kya236v,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,AMD,2024-04-06 04:17:14,3
Intel,kyaw0hp,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",AMD,2024-04-06 09:51:52,1
Intel,kybpb3p,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",AMD,2024-04-06 14:15:00,2
Intel,kygdnfc,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,AMD,2024-04-07 11:17:10,1
Intel,kys12cm,8gb perfectly fine today :),AMD,2024-04-09 15:06:00,1
Intel,l9ad3sk,"Ah yes sure, now where did I leave my 1500 euros?",AMD,2024-06-19 10:11:00,2
Intel,kybkrrc,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",AMD,2024-04-06 13:43:53,10
Intel,kymgwzk,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,AMD,2024-04-08 14:36:56,1
Intel,kya4qoq,"Yeah, i like the black super series.",AMD,2024-04-06 04:40:54,1
Intel,kyw7k0z,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",AMD,2024-04-10 08:27:23,0
Intel,kybtcsj,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",AMD,2024-04-06 14:41:11,2
Intel,kxhli0e,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,AMD,2024-04-01 02:17:59,224
Intel,kxl9t8e,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",AMD,2024-04-01 19:43:02,25
Intel,kxiush3,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",AMD,2024-04-01 10:12:15,114
Intel,kxrny0e,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",AMD,2024-04-02 22:36:02,17
Intel,kxkeqm3,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",AMD,2024-04-01 16:50:42,29
Intel,kxhn7gu,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",AMD,2024-04-01 02:30:21,121
Intel,kxi9i5m,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",AMD,2024-04-01 05:48:52,67
Intel,kxpi7rl,"Yo, I saw the title and thought this gotta be Gnif2.",AMD,2024-04-02 15:15:20,8
Intel,kxhii78,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",AMD,2024-04-01 01:56:41,36
Intel,kxisjb3,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",AMD,2024-04-01 09:45:49,36
Intel,kxhfw6h,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",AMD,2024-04-01 01:38:50,59
Intel,kxiukyk,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",AMD,2024-04-01 10:09:50,13
Intel,kxiah6c,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",AMD,2024-04-01 05:59:50,25
Intel,kxlnigb,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",AMD,2024-04-01 20:59:38,20
Intel,ky0wzku,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",AMD,2024-04-04 15:28:04,5
Intel,ky567n0,Long but worth it read; Well Done!,AMD,2024-04-05 08:38:06,4
Intel,kxnqc72,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",AMD,2024-04-02 05:31:11,3
Intel,ky1f7to,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,AMD,2024-04-04 17:07:58,3
Intel,l012ykv,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",AMD,2024-04-17 19:05:55,3
Intel,kxitz3a,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",AMD,2024-04-01 10:02:50,25
Intel,kxmpmyk,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,AMD,2024-04-02 00:54:21,5
Intel,kxp7mvs,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,AMD,2024-04-02 14:13:09,5
Intel,kxq8p0p,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",AMD,2024-04-02 17:41:45,5
Intel,kxr0ydr,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,AMD,2024-04-02 20:16:04,5
Intel,kxtpd72,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",AMD,2024-04-03 08:19:54,6
Intel,kxj7ncd,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",AMD,2024-04-01 12:18:50,12
Intel,kxirbw1,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",AMD,2024-04-01 09:31:11,15
Intel,kxnysdb,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,AMD,2024-04-02 07:08:39,3
Intel,kxi4dih,100% all of this...  Love looking glass by the by,AMD,2024-04-01 04:54:44,8
Intel,kxt140w,How does say VMware handle this? Does it kind of just restart shit as needed?,AMD,2024-04-03 04:01:28,2
Intel,kxibc53,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",AMD,2024-04-01 06:09:51,17
Intel,kxizp6h,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",AMD,2024-04-01 11:05:58,3
Intel,kxju0p0,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",AMD,2024-04-01 14:52:01,3
Intel,kxjywwd,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,AMD,2024-04-01 15:20:47,2
Intel,kxkj3fj,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",AMD,2024-04-01 17:15:05,2
Intel,kxilacf,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",AMD,2024-04-01 08:13:50,5
Intel,kxikwgx,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",AMD,2024-04-01 08:08:54,1
Intel,kxnag16,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",AMD,2024-04-02 03:12:09,1
Intel,kxqkz3h,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",AMD,2024-04-02 18:48:54,1
Intel,kxk4suo,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",AMD,2024-04-01 15:54:31,0
Intel,kxjykgb,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,AMD,2024-04-01 15:18:47,0
Intel,kxnctg8,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",AMD,2024-04-02 03:30:01,0
Intel,kxierbw,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",AMD,2024-04-01 06:50:41,-7
Intel,kxxhwq9,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",AMD,2024-04-03 23:01:13,-1
Intel,kxip0e1,TL;DR. **PEBKAC**.,AMD,2024-04-01 09:01:51,-23
Intel,kxk9iir,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-01 16:21:24,-2
Intel,kxksj8e,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",AMD,2024-04-01 18:06:47,-4
Intel,kxo5btd,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,AMD,2024-04-02 08:32:08,-5
Intel,kxiw3lo,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",AMD,2024-04-01 10:27:10,47
Intel,ky1fyc2,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,AMD,2024-04-04 17:12:00,3
Intel,kxjwsde,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",AMD,2024-04-01 15:08:22,26
Intel,kxte67y,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,AMD,2024-04-03 06:04:47,2
Intel,kxkf630,"Thanks mate I appreciate it, glad to see you here :)",AMD,2024-04-01 16:53:06,14
Intel,kxtip4r,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",AMD,2024-04-03 06:57:08,5
Intel,ll8wytp,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",AMD,2024-09-03 02:42:30,1
Intel,kxhow6p,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",AMD,2024-04-01 02:42:51,34
Intel,kxhpa3h,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",AMD,2024-04-01 02:45:39,13
Intel,kxjf8yq,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,AMD,2024-04-01 13:17:38,14
Intel,kxpa05g,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,AMD,2024-04-02 14:27:21,-3
Intel,kxiv9ac,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",AMD,2024-04-01 10:17:32,19
Intel,kxp8y84,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,AMD,2024-04-02 14:21:05,8
Intel,kxjfdjy,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",AMD,2024-04-01 13:18:34,4
Intel,kxj3tba,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",AMD,2024-04-01 11:45:39,7
Intel,kxjhcp0,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",AMD,2024-04-01 13:32:25,4
Intel,kxjknpx,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",AMD,2024-04-01 13:54:35,3
Intel,kxtwy1v,"Funny, I saw the title and thought the same too!",AMD,2024-04-03 09:54:20,6
Intel,kxhlmwx,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",AMD,2024-04-01 02:18:57,30
Intel,kxn102r,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",AMD,2024-04-02 02:07:08,-1
Intel,kxnsbw0,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,AMD,2024-04-02 05:52:30,7
Intel,kxjj86s,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",AMD,2024-04-01 13:45:07,6
Intel,kxjs7vy,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",AMD,2024-04-01 14:41:18,-3
Intel,kxi3d8c,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",AMD,2024-04-01 04:44:52,11
Intel,kxvte63,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",AMD,2024-04-03 17:32:25,4
Intel,kxmufyt,ursohot !  back to discord rants...,AMD,2024-04-02 01:24:48,-6
Intel,kxix377,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,AMD,2024-04-01 10:38:16,24
Intel,kxmy36x,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",AMD,2024-04-02 01:48:12,9
Intel,kxjbu8k,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",AMD,2024-04-01 12:52:07,4
Intel,kxlfj2c,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,AMD,2024-04-01 20:14:49,1
Intel,kxnky9y,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",AMD,2024-04-02 04:38:17,0
Intel,kxj2kjm,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",AMD,2024-04-01 11:34:09,5
Intel,kxta6ee,"It doesn't handle it, it has the same issue.",AMD,2024-04-03 05:22:41,2
Intel,kxj4eg4,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2024-04-01 11:50:55,11
Intel,kxj38ou,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",AMD,2024-04-01 11:40:25,6
Intel,kxiu2ph,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",AMD,2024-04-01 10:03:58,4
Intel,kxidqq0,Me neither. I use a RX580 8GB since launch and not a single problem.,AMD,2024-04-01 06:38:22,2
Intel,kxie3oi,Because they're talking absolute rubbish that's why.,AMD,2024-04-01 06:42:43,-15
Intel,kxj72uk,You are one of the lucky ones!,AMD,2024-04-01 12:14:06,9
Intel,kxue41z,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",AMD,2024-04-03 12:32:07,2
Intel,kximvz5,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",AMD,2024-04-01 08:34:35,15
Intel,kxjkdyv,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",AMD,2024-04-01 13:52:49,1
Intel,kxi3fxr,lol your flair is Please search before asking,AMD,2024-04-01 04:45:36,-1
Intel,kyy38w2,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-10 17:04:31,1
Intel,kxipuql,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,AMD,2024-04-01 09:12:36,-1
Intel,kxt2f9e,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,AMD,2024-04-03 04:12:16,1
Intel,kxiexwv,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",AMD,2024-04-01 06:52:56,31
Intel,kxxifs5,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",AMD,2024-04-03 23:04:27,4
Intel,kxkxwhq,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,AMD,2024-04-01 18:36:38,7
Intel,kxo5nh7,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,AMD,2024-04-02 08:36:20,7
Intel,kxmvpp1,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",AMD,2024-04-02 01:33:01,46
Intel,ky1ipao,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",AMD,2024-04-04 17:26:58,2
Intel,kxkcepy,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",AMD,2024-04-01 16:37:46,28
Intel,kxs8nai,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",AMD,2024-04-03 00:45:36,4
Intel,kxk4crx,"""NVIDIA, it just works""",AMD,2024-04-01 15:51:58,14
Intel,kxncqt4,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,AMD,2024-04-02 03:29:27,5
Intel,kxof5tw,What is the AMD Vanguard?,AMD,2024-04-02 10:31:39,9
Intel,kxtr5do,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",AMD,2024-04-03 08:42:33,10
Intel,kxnum1q,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,AMD,2024-04-02 06:18:22,7
Intel,kxjkmnv,You misspelled $2.3T market cap....,AMD,2024-04-01 13:54:24,11
Intel,kxjp8qb,"Okay yeah fair enough, hadn't considered this. Removed it from my post",AMD,2024-04-01 14:23:19,2
Intel,kxxn4fl,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",AMD,2024-04-03 23:33:02,2
Intel,kxpe18q,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",AMD,2024-04-02 14:51:06,1
Intel,kxlmn5s,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",AMD,2024-04-01 20:54:42,1
Intel,kxjv1e3,This is not a fix. It's a compromise.,AMD,2024-04-01 14:58:00,13
Intel,kxjpkam,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",AMD,2024-04-01 14:25:16,3
Intel,kxtj7av,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,AMD,2024-04-03 07:03:13,1
Intel,kxmam0y,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",AMD,2024-04-01 23:20:26,8
Intel,kxxefr8,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,AMD,2024-04-03 22:40:23,1
Intel,kxpad65,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,AMD,2024-04-02 14:29:30,-1
Intel,kxjq477,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",AMD,2024-04-01 14:28:37,22
Intel,kxi6i64,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",AMD,2024-04-01 05:16:16,18
Intel,kxllisv,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",AMD,2024-04-01 20:48:17,4
Intel,kxoidrh,The comment I quoted was talking about people playing games having issues.,AMD,2024-04-02 11:05:13,5
Intel,kxoc6dt,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,AMD,2024-04-02 09:57:53,3
Intel,kxoib9e,The thing I quoted was talking about people playing games though.,AMD,2024-04-02 11:04:33,2
Intel,kxjibo8,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",AMD,2024-04-01 13:38:59,5
Intel,kxj9jkm,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",AMD,2024-04-01 12:34:08,5
Intel,kxjdtt9,"Idk, I don't use Linux",AMD,2024-04-01 13:07:13,-13
Intel,kxjdrs5,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",AMD,2024-04-01 13:06:49,-1
Intel,kxigqbh,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",AMD,2024-04-01 07:15:19,33
Intel,kxj2oqt,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",AMD,2024-04-01 11:35:13,1
Intel,kxj4abt,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:53,-1
Intel,kxih6b1,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),AMD,2024-04-01 07:20:59,28
Intel,kxm7xhx,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",AMD,2024-04-01 23:03:36,1
Intel,kxuiptm,Because adding a feature for a product literally gives users more control for that product.,AMD,2024-04-03 13:05:04,1
Intel,kxine7u,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,AMD,2024-04-01 08:41:11,2
Intel,kxis9nq,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",AMD,2024-04-01 09:42:40,5
Intel,kyhsjnw,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-07 17:08:48,1
Intel,kxjqk3k,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",AMD,2024-04-01 14:31:18,-3
Intel,kxzlw7y,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,AMD,2024-04-04 09:36:41,1
Intel,kxmwxwt,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",AMD,2024-04-02 01:40:54,8
Intel,kxj49ms,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:43,-2
Intel,kxs4to2,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",AMD,2024-04-03 00:21:22,6
Intel,ky39ja5,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,AMD,2024-04-04 23:11:22,5
Intel,ky4zrtz,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",AMD,2024-04-05 07:20:00,3
Intel,kxldpfb,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",AMD,2024-04-01 20:04:38,15
Intel,kxp3oh8,*wayland users have joined the chat,AMD,2024-04-02 13:48:33,11
Intel,kxm4qt3,You're falling for slogans.,AMD,2024-04-01 22:43:30,1
Intel,kxobyv3,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",AMD,2024-04-02 09:55:25,13
Intel,kxpaw46,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,AMD,2024-04-02 14:32:39,11
Intel,kxojs3c,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),AMD,2024-04-02 11:18:39,5
Intel,kxtnu71,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",AMD,2024-04-03 08:00:44,2
Intel,kxjpcl3,Honestly after a trillion I kinda stop counting 😂🤣,AMD,2024-04-01 14:23:58,3
Intel,kxjvfz1,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",AMD,2024-04-01 15:00:22,8
Intel,kxpf9fv,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",AMD,2024-04-02 14:58:15,9
Intel,kxodaii,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",AMD,2024-04-02 10:10:50,6
Intel,kxjvmo3,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",AMD,2024-04-01 15:01:28,8
Intel,kxpamp2,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,AMD,2024-04-02 14:31:05,-3
Intel,kxy4p6p,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",AMD,2024-04-04 01:24:00,1
Intel,kxpia4a,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",AMD,2024-04-02 15:15:42,2
Intel,kxjr4lw,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",AMD,2024-04-01 14:34:44,9
Intel,kxp7oc3,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",AMD,2024-04-02 14:13:24,-1
Intel,kxi7ym2,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",AMD,2024-04-01 05:31:48,3
Intel,kxm9n9f,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",AMD,2024-04-01 23:14:25,2
Intel,kxk5inl,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",AMD,2024-04-01 15:58:39,2
Intel,kxiim2c,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",AMD,2024-04-01 07:39:33,-5
Intel,kxih401,Oh then just ignore my comment 😅,AMD,2024-04-01 07:20:10,0
Intel,kxjfryq,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",AMD,2024-04-01 13:21:24,3
Intel,kxiojjd,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,AMD,2024-04-01 08:55:52,8
Intel,kxiiqcv,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",AMD,2024-04-01 07:41:05,-14
Intel,kxin4tk,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",AMD,2024-04-01 08:37:50,-14
Intel,kxmwd7i,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",AMD,2024-04-02 01:37:14,1
Intel,kxioc93,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,AMD,2024-04-01 08:53:17,6
Intel,kxiqori,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",AMD,2024-04-01 09:23:10,1
Intel,kxiuak1,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,AMD,2024-04-01 10:06:29,1
Intel,kxit1y6,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,AMD,2024-04-01 09:52:00,-2
Intel,kxjg5xf,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",AMD,2024-04-01 13:24:09,-2
Intel,kxjr7cc,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",AMD,2024-04-01 14:35:12,7
Intel,kxzn1iw,"Too soon to tell, but hopes are high.",AMD,2024-04-04 09:50:05,2
Intel,kxo5u7w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-02 08:38:44,1
Intel,kxoprjw,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",AMD,2024-04-02 12:09:39,12
Intel,kxm2qa6,"Agreed, they cannot rest on their laurels.",AMD,2024-04-01 22:30:48,2
Intel,kxn01lt,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",AMD,2024-04-02 02:00:52,27
Intel,kxnsapp,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",AMD,2024-04-02 05:52:08,3
Intel,kxpuexg,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,AMD,2024-04-02 16:23:44,2
Intel,kxpwkoo,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,AMD,2024-04-02 16:35:41,1
Intel,kxk96s0,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",AMD,2024-04-01 16:19:33,5
Intel,kxpcxh7,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",AMD,2024-04-02 14:44:41,4
Intel,kxiic2i,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",AMD,2024-04-01 07:35:56,37
Intel,kxi921e,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",AMD,2024-04-01 05:43:54,14
Intel,kxijoyb,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",AMD,2024-04-01 07:53:26,20
Intel,kxiqghx,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,AMD,2024-04-01 09:20:14,17
Intel,kxiitb5,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,AMD,2024-04-01 07:42:10,20
Intel,kxj5139,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,AMD,2024-04-01 11:56:29,7
Intel,kxio9nt,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",AMD,2024-04-01 08:52:23,12
Intel,kxn5a9z,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",AMD,2024-04-02 02:35:33,2
Intel,kxjrku0,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,AMD,2024-04-01 14:37:29,6
Intel,kxipvh2,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",AMD,2024-04-01 09:12:52,15
Intel,kxjy6gb,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,AMD,2024-04-01 15:16:31,-2
Intel,kxp15kv,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",AMD,2024-04-02 13:32:11,6
Intel,kxn7ur7,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,AMD,2024-04-02 02:53:24,2
Intel,kxq0m39,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",AMD,2024-04-02 16:57:48,0
Intel,kxq98bx,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",AMD,2024-04-02 17:44:39,3
Intel,kxm4q67,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",AMD,2024-04-01 22:43:23,6
Intel,kxq0fuf,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",AMD,2024-04-02 16:56:51,2
Intel,kxpfg1v,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,AMD,2024-04-02 14:59:19,-1
Intel,kxin2k0,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",AMD,2024-04-01 08:37:02,17
Intel,kxj2kf3,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",AMD,2024-04-01 11:34:06,1
Intel,kxnjdov,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",AMD,2024-04-02 04:23:59,-2
Intel,kxisrca,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",AMD,2024-04-01 09:48:29,5
Intel,kxs5a0e,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",AMD,2024-04-03 00:24:15,2
Intel,kxj34w0,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",AMD,2024-04-01 11:39:28,-6
Intel,kxindr9,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",AMD,2024-04-01 08:41:01,-7
Intel,kxiniuo,Oh and XE also have bug feature reporting.  Omfg!!!!,AMD,2024-04-01 08:42:51,-4
Intel,kxl4asu,Nobody is 100% right ;),AMD,2024-04-01 19:12:15,-3
Intel,kxta5m0,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),AMD,2024-04-03 05:22:28,2
Intel,kxiq2zk,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",AMD,2024-04-01 09:15:31,-5
Intel,kxjix5f,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",AMD,2024-04-01 13:43:03,-1
Intel,kxjz1ko,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",AMD,2024-04-01 15:21:32,2
Intel,kxthgxe,What about using a DP to HDMI 2.1 adapter for that situation?,AMD,2024-04-03 06:42:39,2
Intel,kxnvnrf,"2021 my guy, it's right there on the date of the article.",AMD,2024-04-02 06:30:33,6
Intel,kxqftwv,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,AMD,2024-04-02 18:20:45,-1
Intel,kxp8mfb,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,AMD,2024-04-02 14:19:07,2
Intel,kxipvcp,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",AMD,2024-04-01 09:12:49,18
Intel,kxj4mkp,And I guess infallible game developers too then. /s,AMD,2024-04-01 11:52:55,7
Intel,kxjlszk,So you decide what criticism is valid and what not? lol,AMD,2024-04-01 14:01:58,7
Intel,kxio3k4,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,AMD,2024-04-01 08:50:13,8
Intel,kxioj2i,"Yup, but do you see them making a big press release about it?",AMD,2024-04-01 08:55:43,5
Intel,kxno85r,that is not how it works but sure,AMD,2024-04-02 05:09:33,2
Intel,kxtv199,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,AMD,2024-04-03 09:31:19,2
Intel,kxjk8f2,>whine about Redditors.  The irony.,AMD,2024-04-01 13:51:48,0
Intel,kxu2whw,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",AMD,2024-04-03 10:58:25,0
Intel,kxqg0v8,learn to comprehend.,AMD,2024-04-02 18:21:49,3
Intel,kxiqgpx,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,AMD,2024-04-01 09:20:19,8
Intel,kxj4whx,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",AMD,2024-04-01 11:55:21,5
Intel,kxnjs9x,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",AMD,2024-04-02 04:27:38,0
Intel,kxl4djq,"No, that would be you obviously /s",AMD,2024-04-01 19:12:41,-2
Intel,kxivsl5,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,AMD,2024-04-01 10:23:43,-2
Intel,kxivodj,"Yea, given the state of XE drivers every major update has come with significant PR.",AMD,2024-04-01 10:22:23,-1
Intel,kxnxxva,Why not ;),AMD,2024-04-02 06:58:11,0
Intel,kxqg47j,Go word salad elsewhere.,AMD,2024-04-02 18:22:19,-1
Intel,kxnwc84,"I have replicated the issue reliably yes, and across two different systems.",AMD,2024-04-02 06:38:43,3
Intel,kxjrbmq,If discord crashes my drivers.. once every few hours. I have to reboot,AMD,2024-04-01 14:35:55,6
Intel,kxo4jke,Discord doesn't crash my drivers  I don't have to reboot.,AMD,2024-04-02 08:22:06,0
Intel,kpp4kwl,Really love how the 6000 series radeons look.,AMD,2024-02-09 21:57:31,11
Intel,kpqv9od,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",AMD,2024-02-10 05:25:10,6
Intel,kpougfk,That's a good looking line up,AMD,2024-02-09 20:58:04,3
Intel,kps7pkq,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",AMD,2024-02-10 14:18:43,2
Intel,kpr86tx,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",AMD,2024-02-10 07:45:28,3
Intel,kpq3r57,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",AMD,2024-02-10 01:49:13,3
Intel,kptibdx,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,AMD,2024-02-10 19:15:04,-1
Intel,kptwmeu,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",AMD,2024-02-10 20:44:28,3
Intel,kpv2g8f,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,AMD,2024-02-11 01:23:45,1
Intel,kpv5euk,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",AMD,2024-02-11 01:44:32,3
Intel,kpvwyyr,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",AMD,2024-02-11 05:16:13,2
Intel,kcvx2pq,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,AMD,2023-12-11 10:20:41,9
Intel,kcvsq1w,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",AMD,2023-12-11 09:20:24,15
Intel,kcvzwca,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,AMD,2023-12-11 10:58:26,2
Intel,kcyc7u2,That 7900xtx sale number is insane,AMD,2023-12-11 21:59:22,2
Intel,kcytq9l,That just shows that most people that buy GPU's don't know a thing about them.,AMD,2023-12-11 23:54:41,1
Intel,kcwedyi,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",AMD,2023-12-11 13:30:14,14
Intel,kcvzjgq,best discounts were 6750xt 6800 and 7800xt,AMD,2023-12-11 10:53:41,1
Intel,kdazjvv,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,AMD,2023-12-14 10:36:15,1
Intel,kcvv71l,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",AMD,2023-12-11 09:54:52,7
Intel,kcwe3k6,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",AMD,2023-12-11 13:27:46,5
Intel,kcw3vwl,"They're not out of stock there, duh",AMD,2023-12-11 11:47:20,6
Intel,kcyhmsr,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,AMD,2023-12-11 22:33:46,2
Intel,kd0h0lm,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-12-12 07:54:01,1
Intel,kcxlwiu,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",AMD,2023-12-11 19:18:18,3
Intel,kcxu0yw,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",AMD,2023-12-11 20:09:16,1
Intel,kcx65jb,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",AMD,2023-12-11 16:46:01,2
Intel,kcw55l4,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",AMD,2023-12-11 12:01:31,1
Intel,kcy5bwz,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",AMD,2023-12-11 21:17:17,1
Intel,keln136,the card is pretty bad if you missed that somehow,AMD,2023-12-23 12:50:02,1
Intel,kcw5qf2,AMD probably ships leftover to countries in which they know it will sell,AMD,2023-12-11 12:07:48,3
Intel,kdhrs0l,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",AMD,2023-12-15 17:44:32,1
Intel,kemomla,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",AMD,2023-12-23 17:01:05,1
Intel,nxa9ue8,"Personal computing is in such a weird place these days.      M5 in the ipad pro is faster than the B50 in Blender and AI image generation. Mini with the M4 pro is also faster and can access \~48GB of unified ram for vram.      I say this as an Arc/nvidia/apple user. 5 years after Apple left Intel, Intel is still 5 years behind.",Intel,2026-01-02 17:22:05,8
Intel,nx9rf7h,How is the AI running on the B50?,Intel,2026-01-02 15:55:47,1
Intel,nxaevc4,Where did you purchase the B50?,Intel,2026-01-02 17:45:39,1
Intel,nxc8zfu,nice case,Intel,2026-01-02 23:05:56,1
Intel,nxdwmf4,"I love the look of those ARCs, like IBM retro hardware.",Intel,2026-01-03 04:56:11,1
Intel,nxlmpl5,"love the size of it. love the psu, are there any psus in this formfactor that are more powerful?",Intel,2026-01-04 09:46:01,1
Intel,nx9s5yu,"cute fan lol  like other user, how is the B50 performance?",Intel,2026-01-02 15:59:17,1
Intel,nxahmts,"As an ardent and lifelong Apple hater, I must admit that they will probably come out much stronger and on the very top of the current chaotic situation if they manage to keep the current price/perf ratio of their offerings. Even with the Apple tax, they are unmatched right now.",Intel,2026-01-02 17:58:24,4
Intel,nxeazfq,"weird comparison. the mac mini is the real beast and its in part thanks to not having to cater to OEMs, but the 48gb mini pro is $1800 vs this $350 drop in card so that's a strange comparison.   m5 in the ipad has only about 150gb/s of bandwidth. good for light inference but I really doubt its practical for actual scale production.",Intel,2026-01-03 06:46:02,6
Intel,nxlml7c,"apple stuff is hard to get used to for many pc nerds and mechanical engineers and engineers in this field. When pro software like catia/nx nativly will work nativly on arm then maybe the big car/air/motorcycle/""every day crap all around us"", then product developers will adopt arm/apple. but right now x86 is the king for these guys/this sector that design all stuff u see around u.",Intel,2026-01-04 09:44:53,1
Intel,nxb0egt,"its not hard, you can literally just buy them on newegg",Intel,2026-01-02 19:25:10,3
Intel,nxbf6k9,"[https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007](https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007)  They're finally back in stock as of this reply, though likely not for long.",Intel,2026-01-02 20:37:05,3
Intel,nxg3fij,"> Personal computing is in such a weird place these days  Apple laptops beat both Intel and AMD's desktop GPU lineup in Blender. Apple's tablet beats Intel's desktop Pro GPU in Blender.    Just plain weird. That's all. And if you cant get PCIE power settings right, both Apple options do it pulling less power than Arc cards idle at.",Intel,2026-01-03 14:55:14,1
Intel,nxgupsq,"It's not exactly weird. They went PPC before Intel because powerpc was more effective for workloads most people used macs for. The switch to Intel was just because Intel had node leadership and performance leadership. Instead stick to A-chips for low power mobile where intel gave up on servicing. Intel loses node leadership to TSMC for a long time, Apple moves on to everything in-house is a pretty logical progression.  Apple having bespoke solutions isn't new either. they've been doing it since their G workstation days. Their current situation is pretty much on brand for apple, but the difference is the huge mistakes intel made (particularly firing so many top engineers) that led to staff fleeing to other companies, including leadership at Apple processor design.  basically apple did their own thing as usual and did a great job don't get me wrong, not taking anything away from apple. the biggest difference however was intel's CEO and board destroying the company.",Intel,2026-01-03 17:07:56,1
Intel,nv0zs3r,"If only Intel had stayed in the memory business!   They'd be enjoying Micron valuations and wild profits and performance from copackaged CPU+GPU+LPDDR of their own design and manufacture...     But no, they'd rather invest billions in buying donuts as a service, or whatever their crazy investements went into.",Intel,2025-12-20 13:16:05,44
Intel,nv0mnlo,"damn an iGPU using 32GB of vRAM, I wonder if they're testing a Panther Lake laptop with 48GB RAM or even more (since X7 & X9 Panther Lake only accepts soldered memory)",Intel,2025-12-20 11:24:26,12
Intel,nv2mj8t,"If Intel is really about to release a B770, honestly the **only thing that could make it competitive is the price**. (FOR ME, competitive in 2026 means <400€) From a performance standpoint, it would need to undercut existing GPUs quite aggressively to make sense, especially given how crowded the mid-range already is.  That said, I’m pretty skeptical about how realistic that is. **With the recent RAM shortages and rising memory costs**, pricing a new card competitively while still keeping margins doesn’t sound easy at all. Memory is a huge part of the BOM, and we’ve already seen how shortages can push prices up across the board.  So unless Intel is willing to take a serious hit on margins (which seems unlikely), I’m not convinced the B770 will land at a price point that truly shakes up the market. Happy to be proven wrong, but for now the pricing question is the big unknown for me.",Intel,2025-12-20 18:47:27,5
Intel,nv30mtq,So there's a 20GB variant. A 28GB variant and a 32 GB variant?,Intel,2025-12-20 20:01:42,2
Intel,nv40zo0,Optane was practically **built** for the type of AI workloads that they're shoveling money at.  If Intel didn't give up literally only a matter of months before GPT released and the bubble began in earnest lol,Intel,2025-12-20 23:28:07,18
Intel,nv29blj,"If Intel stayed in memory business, it would be long dead in the 80s and killed by Japanese memory companies. CPU remains the top niche area with less competition and deeper moat. See how China has quickly come up with their GPU designs? Well it will take at least another decade for them to make 2nm CPUs",Intel,2025-12-20 17:39:17,12
Intel,nv2qfnf,"are people high or something? intel was losing money on optane and their SSD business became irrelevant the minute regular memory manufacturers slammed the market. don't get me wrong, they were some of the most durable on the market, but they were no where near printing money on the memory business.  optane may have survived if their nodes were on schedule, keeping CXL support on schedule, but not because it was profitable.",Intel,2025-12-20 19:07:24,8
Intel,nw63y3k,CXL killed octane it’s that simple. No one wanted to be locked to just Intel. CXL was and is just better,Intel,2025-12-27 10:05:29,1
Intel,nvsy1nn,"I feel like the price has to be more than competitive. If they can undercut competition cards of the same performance by 100 or so (or maybe offer rebates or freebies) they could potentially steal the market in that category. With Nvidia and amd cards being tried and true for many many years, I feel like their marketing needs to grab the attention of consumers in a somewhat drastic way.",Intel,2025-12-25 00:56:50,3
Intel,nvbtvc9,If the b770 is 5060ti levels even €500 is competitive,Intel,2025-12-22 06:05:31,1
Intel,nw63tp3,Yeah sure it would’ve been perfect but CXL killed octane and offers pretty much everything it did while not being loved to just Intel lol,Intel,2025-12-27 10:04:18,1
Intel,nv4153e,"it's not like any of this AI garbage right now is profitable for anyone except nvidia and the hardware companies anyway, it's not stopping everyone from shoveling money into it",Intel,2025-12-20 23:29:02,6
Intel,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
Intel,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
Intel,ntkfg69,"> With up to 192GB of VRAM across eight GPUs in a single system, Battlematrix positions itself as a relatively cost-effective alternative to other professional GPU ecosystems for AI inference workloads.",Intel,2025-12-12 01:18:37,21
Intel,ntmjjev,Hope they do some image and video generation benchmarking as well. Nice to see someone testing AI rigs out there.,Intel,2025-12-12 10:55:28,5
Intel,ntlssa2,Wish they’d give prompt processing speeds. AI coding generates very few tokens compared to input. Nvidia seem to dominate here.,Intel,2025-12-12 06:40:14,3
Intel,ntxw9ob,"How many concurrent users will this serve, 30 devs would be nice",Intel,2025-12-14 06:36:27,1
Intel,ntsaqxn,:),Intel,2025-12-13 08:27:35,2
Intel,nvplqob,"These 12Xe3 cores are pretty neat, and because it fits in a normal socket it isn't ludicrously expensive to make.  I suspect we'll see a ton of these different form factors for this chipset.",Intel,2025-12-24 13:00:31,4
Intel,nvpsi8z,Mac Pro Trashcan 2.0 is crazy,Intel,2025-12-24 13:45:33,6
Intel,nvt0j8h,"> These 12Xe3 cores are pretty neat  have there been any leaked benchmarks or gaming FPS?  on paper they look good, but... some synthetic benchmarks suck",Intel,2025-12-25 01:15:41,3
Intel,nvte7q0,No one knows. Synthetics seem to put it roughly at a 3050m.,Intel,2025-12-25 02:59:28,2
Intel,nwfd6hr,"3050 to 3050ti mobile if leaks are to be believed. Could get a bit better than that if software is still not mature, so I'm calling a max of 3060M performance.",Intel,2025-12-28 20:51:38,1
Intel,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,51
Intel,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
Intel,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
Intel,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
Intel,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,8
Intel,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,8
Intel,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,2
Intel,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,3
Intel,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,4
Intel,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
Intel,nspzeik,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,5
Intel,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
Intel,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,3
Intel,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
Intel,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,4
Intel,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,6
Intel,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,2
Intel,nsv64t7,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,3
Intel,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
Intel,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
Intel,nsyv727,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
Intel,nv5fgk8,"should have a 3y warranty on it. submit an RMA ticket  regarding the actual query though, the silicon is the same the 14900ks is just a slightly better bin. you wouldn't notice the difference at stock let alone normalized for energy consumption",Intel,2025-12-21 04:48:32,8
Intel,nv5nfm2,"I have i9 14900ks, what I did is that I reset bios settings to optimized defaults and then I limit pl1 and pl2 to 150w and enabled XMP, these are the only two settings i changed, the rest is default, and temperatures are in check, i still get the same performance, and it’s very efficient in gaming that way, the extra heat and power consumption of 253 or 320 are not worth it, I recommend just get the ks and make these two changes and forget it.",Intel,2025-12-21 05:49:09,4
Intel,nv5h74y,The performance difference will be tiny and definitely not noticeable with a 3090. Go for the cheaper chip.,Intel,2025-12-21 05:00:55,3
Intel,nv5hr1e,"Get the KS for better silicon quality only limit power , set pl1/2 253w and set it to 350 or 325A, definitely want the better 14900ks silicon quality it’s overall better and better IMC as well. It’s a better bin and typically only the best 14900k will run stable 6.2ghz and at lower voltages even if you limit your chip to 6ghz",Intel,2025-12-21 05:04:58,2
Intel,nv5yk6v,If it doesn’t cost you extra then get the 14900ks and lock all the cores to 5.6 and power limit 256w,Intel,2025-12-21 07:30:16,2
Intel,nv70zt0,"A 14900KS is nothing more then a binned 14900K. Running a 320W/400A extreme setting is not advisable with a AIO. I run my 14900KS on custom loop with 320W/307A performance setting and it does not thermal throttle at all. If you get lucky, you could get a 14900K that can run KS settings. Performance in that case is( should be) identical. Without benchmarks i can't really tell the difference between 125/253/307 and 320/320/400 except the heating of my room.",Intel,2025-12-21 13:28:34,2
Intel,nvehvk5,"As an update - I went ahead with the 14900ks and also changed my cooler to a 420mm AIO.   Ensured latest bios update then set Intel presets (performance) but also went ahead and reduced PL to 150w, set temp limits to 70c, system agent voltage to 1.12, 307A, and I was blown away by the temps!! I am getting basically identical performance (+ few fps) to my previous 13900ks, but a whole whopping 30°c cooler in game!!! I would average 80-85, now it’s sitting super chill with same in-game settings on BF6 & ARC at 50-60c.   Thank you everyone for your inputs, I sincerely appreciate it and I’m extremely happy with the outcome!",Intel,2025-12-22 17:37:19,2
Intel,nv6um6s,I also PL1/2 at 150. My temps stay under 60c when gaming.,Intel,2025-12-21 12:41:07,1
Intel,nv7icoh,"if the cooling wasn't sufficient disable HT(useless for gaming) and undervolt it this lower CPU temperature by 20c, in games the CPU temperature should be around 65c.",Intel,2025-12-21 15:16:34,1
Intel,nvb3hok,Im using a duel air tower for my 14900k game temps are at 60 to 70,Intel,2025-12-22 02:56:21,1
Intel,nvkr6z0,"14900KS is just a better binned 14900K. All things equal, you should have lower temps/voltage/power draw for the same exact workload/clocks on a 14900ks vs a 14900k. How big of a delta between the two comes down to how well you struck the silicon lottery with the KS.",Intel,2025-12-23 17:28:35,1
Intel,nv62yz6,"I have the K version only because of the onboard gpu. In case my GPU gives issues and I'll still be able to boot. But otherwise there is almost no performance gain. I ran my i9-14900k pl on 320 watt and did a cinebench benchmark, temps were ok: average 94c, max 98c with a 360 aio.  That said, go for the cheaper version if you don't need onboard gpu.  Edit: I have my pl on 253w now. No need to go any higher.",Intel,2025-12-21 08:13:54,-4
Intel,nv6s3tx,5 years warranty on 13 and 14gens now.    I have the 13900ks. Run it at 253w. Clock locked at 5.5ghz. Temp 80c and cinebench 23 39k,Intel,2025-12-21 12:20:08,6
Intel,nvx118g,Why not keep pl2 at 253 and 1 at 150/185 ? Did you try undervolting? Most of them can take 50mv offset with 75 /85 needing a bit more stability testing. Can also cap the vr limit and iccmax. I feel like going 150 pl2 makes you miss some performance in games unless you had thermal issues and doing it to keep it from thermal throttle.,Intel,2025-12-25 20:10:57,1
Intel,nv6848y,"Thank you for the feedback. Forgive me for the dumb question; if I ran either a 14900ks or a 14900k at these settings, would they both have the same temps? Or would the KS still run hotter?",Intel,2025-12-21 09:06:11,0
Intel,nwbwttq,"Dropped the voltage further down by -0.10000 and now I’m getting 58°c core temp and max 65°c package temp under load. Really happy with this, and with some tweaks to my in-game video settings I’m able to still maintain a framerate that matches my screen refresh rate.",Intel,2025-12-28 07:43:31,1
Intel,nv7zbxm,This post is about the K and KS. Both have the same iGPU,Intel,2025-12-21 16:46:01,3
Intel,nv7imu3,even better. i take it they extended tbe warranty period for those products?,Intel,2025-12-21 15:18:10,1
Intel,nvgy84t,For 5.5 39k in CB23 is a little low,Intel,2025-12-23 01:34:28,0
Intel,nvx2yr5,"I have tried and tested all my games, i saw absolutely no difference between 253w, 150w, 125w or even 100w, the fps were exactly the same, the only difference was in temperatures, performance wise i saw no difference between any of them, i was using 100w before but then I switched to 150w because I thought it was too low, even though the performance is still the same as 100w, just higher temperatures, my cooler is pretty good kraken elite 360, it never goes above 80 even on 253w but I just like to keep temperatures between 50-70 while gaming.",Intel,2025-12-25 20:22:49,1
Intel,nv7bdbq,The ks should run cooler because of the better bin. Less voltage being required to hit certain frequency points.,Intel,2025-12-21 14:35:05,2
Intel,nv84jg4,"Oh shit, I thought only the K had an igpu! Should have gone for the ks version lmao",Intel,2025-12-21 17:12:59,1
Intel,nv7jm5d,Yes because of the degrading issue.,Intel,2025-12-21 15:23:39,2
Intel,nvhfrt3,Lol stock is 5.8ghz lol and most stock after the update get 35k,Intel,2025-12-23 03:22:04,1
Intel,nvhi35n,All core cinebench is not 5.8... I get 39k stock what are you talking about lol,Intel,2025-12-23 03:36:52,1
Intel,nvhitar,Search on reddit on 13900-14900k.  After the code update stock most 13-14k can barely do 35k. Dont like to your ego brother.   So millions on reddit are getting those score and you are the special bin whose getting a higher score.   Mr 1 post and 7 comment history lollllllllllllllllllll,Intel,2025-12-23 03:41:34,2
Intel,nvhk1pd,LOLOLOLOLOLOL HAHAHAAHAHAHAH ARE YOU DUMB? This really shows you don't have a 13900k or 14900k,Intel,2025-12-23 03:49:33,1
Intel,nvkpd9m,"My guy what are you talking about? 5.5 ghz for 39k is a good score on 13900k. My 14900KS completely stock does 41.5k and downclocks to about 5.5-5.6 ghz with hyperthreading on. If he's got HT off, his score is even better.   You have to be rage baiting.",Intel,2025-12-23 17:19:30,2
Intel,nvkq3xk,Talk to him not me... The guy said 35k is the score 😂😂😂😂😂,Intel,2025-12-23 17:23:13,2
Intel,nsktadm,Will be a interresting CES,Intel,2025-12-06 11:12:47,21
Intel,nso11hn,I'm half-expecting this to show up as a server-only AI-focused SKU with video outputs removed.,Intel,2025-12-06 22:37:18,9
Intel,nswpbo1,Merry Christmas everyone,Intel,2025-12-08 08:52:05,2
Intel,nswyceh,"4070 performance for $400, I'm calling it. Would have been great if this had come out right after the wave of negative press that the 5070 received for only being 10-15% better than the 4070 with a mediocre 12 GB of VRAM, but I feel like Intel missed the boat again if the Steam Hardware survey is anything to go by, the 5070 has really made a comeback with recent sales.",Intel,2025-12-08 10:25:38,2
Intel,nsofcmj,They can't even ship B60's.,Intel,2025-12-07 00:01:12,3
Intel,nsp4pld,"What is taking Intel so long?      It's already been almost a year after Battlemage's initial launch. And for what? RTX 5060 performance at the same price with some extra VRAM?  I had really hoped Intel would be able to gain ground on their competitors. At this rate, we'll get the ARC C770 to compete with the RTX 6060 in another 3 years.",Intel,2025-12-07 02:38:20,1
Intel,nsm5mtm,Aren't they always,Intel,2025-12-06 16:34:23,3
Intel,nt8d8jl,give it some time...,Intel,2025-12-10 03:42:11,1
Intel,nt8d10w,Merry Bitmas,Intel,2025-12-10 03:40:49,1
Intel,nt9tjuy,"4070 performance for... used 4070 price, now with driver issues and an objectively worse upscaler!  intel greatest hits",Intel,2025-12-10 11:20:47,0
Intel,nsol1s5,Sure they can if you search for it   B60  https://www.idealo.at/preisvergleich/OffersOfProduct/207972918_-arc-pro-b60-sparkle.html   Or b50 https://geizhals.at/intel-arc-pro-b50-a3584363.html,Intel,2025-12-07 00:34:27,7
Intel,nsq0noc,"Intel's GPU division has been operating at a loss never mind Intel as a whole and ARC series cards aren't as popular as the enthusiast circles would have you believe. Coupled with how expensive R&D is for things like GPUs, it's hard for them to pump out a competitive product while remaining just profitable enough to undercut AMD and Nvidia.",Intel,2025-12-07 06:21:43,8
Intel,nswuidx,What is taking Nividia so long with the super cards?,Intel,2025-12-08 09:46:34,2
Intel,nt0mfun,"Battlemage gpu chips are made through TSMC and Intel is getting screwed on supply, this is why even if the B770 comes it will only be a small amount. Hopefully Intel can put together enough rare earth to pump out discrete Celestial Gpus but it takes time to ramp everything up. In addition Intel has their chiplet design, EMIB that could take off soon. They may be able to bring Apple back into the fold, but let us hope Discrete Arc lives on.  I have learned to not have expectations for anything that is outside my direct control, I do the best I can to just go with the flow. Whatever will be, will be.",Intel,2025-12-08 22:47:54,1
Intel,nsn1l8j,There's definitely been lame ones.,Intel,2025-12-06 19:20:53,3
Intel,ntco711,"I'm fully aware it's not a great deal, but that's my expectation when it comes to Arc.",Intel,2025-12-10 20:41:40,0
Intel,nt0k5mq,"These are European links and will be out of stock. I found a mom and pop place back in my old stomping grounds in San Francisco, and they normally only sell B60s in prebuilt systems but a special order is possible.   Intel can't rely on TSMC for Battlemage supply, so let us pray that Discrete Celestial GPUs are made (entirely) at IFS and release in 2026 / 2027.   May your Bits Byte Hard, long live the Arc.",Intel,2025-12-08 22:35:24,2
Intel,nt0ni97,"If CES 2026 comes and goes without any details for Discrete Arc GPUs then it could be awhile. The main thing being promoted is Panther Lake which should be made entirely at IFS, a step in the right direction. The TSMC monopoly is destroying the industry and it hurts companies here in the US.",Intel,2025-12-08 22:53:46,1
Intel,ntafqck,Not sure how the link being European matters. They are European shops and the B60 is in stock.,Intel,2025-12-10 13:56:29,1
Intel,nqdrca0,"We kind of know where it will land. It will be a 3050M level chip, maybe a bit better, but will have improve scaling and frame gen.   8060S is 40 RDNA 3.5 units. One Xe3 unit is about 2.1 RDNA 3.5 units. That put it at about 65% of Strix Halo, though it will have a worse memory bus and no MALL cache. Somewhere in that range.   So that 60% is almost bang on the 3050M. Maybe a bit better. It won’t be like the 4050 but 3050M isn’t bad for an iGPU that fits in a normal socket",Intel,2025-11-23 16:52:41,14
Intel,nqe5eoy,"> and no MALL cache  PTL does have 8MB of memory side cache, fwiw.",Intel,2025-11-23 18:04:37,13
Intel,nr20ozq,"I don’t know how this score compares to the 3050M. I only know that this score is about 55% of the B580. And the B580, at 2K and 4K, is about 1.7–2 times the performance of the desktop 3050",Intel,2025-11-27 14:11:52,1
Intel,nu0mh30,and also an iGPU won't be stuck with 6GB of vRAM 😅,Intel,2025-12-14 18:17:52,1
Intel,nqga537,Really don't understand why they don't go with a larger cache.  Pretty sure they still have a bunch of cache chunks spread all around the SoC.,Intel,2025-11-24 00:43:14,6
Intel,nqg407n,"in configs without die-to-die memory performance in general should be worse if bandwidth limited. despite not being specifically dedicated to onboard memory like Lunar, people are still planning configs with local LPDDR5x, though peak bandwidth is limited by a 128bit bus.",Intel,2025-11-24 00:08:08,4
Intel,nqfqmb2,I see it personally as not really being at 20W if you’re asking much from the GPU. It’ll increase the juice dynamically if it gets demanding enough. So it’ll be hard to say unless you force the power limits way down manually.,Intel,2025-11-23 22:49:52,2
Intel,nnbivzp,"I actually like the idea of discrete GPU naming scheme for the new iGPU, 300 series for integrated graphics is really makes sense but they should add 'M' suffix to make it clear.",Intel,2025-11-05 22:28:54,36
Intel,nnbkl4o,"I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  Also looks like the 3\_8 and 3\_6 versions are differentiating maximum boost clocks, though I wonder if instead those may reflect that configurable upper TDP bound. Might make sense for 65W and 80W to be differentiated if that will coincide with anything about the ""experience-based"" PL1 behavior.",Intel,2025-11-05 22:37:53,14
Intel,nnbr1vs,There are 2 dies.  One for professional workload which will be mass produced.  Second is for gaming. Even a 10-12 core xe3 will be barely enough for modern 1080p. Lunarlake can only run alanwake at 1080p at low settings getting only 25 frames so even if this is 50-100% better this is the minimum for a 2026 product.   I see no point of a 8 xe3 core system when all people will do is just complain.,Intel,2025-11-05 23:13:11,7
Intel,nncz7bz,"quite confusing, Xe3 should start from C (celestial), if using name like B390  we think this  is a battlemage Card (Xe2)",Intel,2025-11-06 03:36:19,5
Intel,nnftti4,"I wonder if rumors about Zen 6 clocking way higher than current cpus turn to be true, and the 5.1ghz max on mobile PT mean Amd might have an edge in next generation   Only time will tell",Intel,2025-11-06 16:14:24,2
Intel,nndm2ga,please add M for Mobile or i for iGPU  * Arc B390M Xe3 Graphics * Arc B390i iXe3 Graphics,Intel,2025-11-06 06:31:30,3
Intel,nnc0ph2,"A clock speed regression vs the prior gen on N3B, with a remark that it's difficult to cool, really isn't a good look for the process side. 18A branding with more like N4 performance...",Intel,2025-11-06 00:08:27,-3
Intel,nnjsaw2,Will the 10 core Xe be better than radeon 890m or worse?,Intel,2025-11-07 05:14:22,0
Intel,nnbhb9s,"i mean i get this is a laptop part but man 16 threads is not much to phone home about when it comes to horsepower, isnt next gen desktop aiming for something like 48 threads?",Intel,2025-11-05 22:20:34,-6
Intel,nnbj579,Still weaker than x3d,Intel,2025-11-05 22:30:15,-16
Intel,nnm29u0,Yes indeed we need that M&M. Mobile platforms are not a priority for me and are dedicated mobile gpus really comparable to Big Boy Discrete GPUs? It is very confusing.  Lunar Lake laptops should fall in price. Has anyone used Lunar Lake and if so which models? Buying latest gen is for guinea pigs and the rich!,Intel,2025-11-07 15:46:06,3
Intel,npu5d43,Doesn't the B already serve that purpose?,Intel,2025-11-20 13:03:06,1
Intel,nnbmjlf,">I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  The 10 Xe3 core model is a binned down 12 Xe3 N3 die, and I doubt yields are so bad that they would even be able to find more dies where they have to disable more cores.   The other die is the 4 Xe3 Intel 3 die, so you can't go up from there.",Intel,2025-11-05 22:48:16,14
Intel,nnd5hpv,To be fair Alan Wake 2 low settings look great. This was covered by DF awhile back they said in some ways Alan Wake 2s low settings look better than some modern games high.,Intel,2025-11-06 04:19:26,9
Intel,nnclctl,"I mean, but that logic, most of Intel's historical bigger iGPUs don't make sense. There are use cases other than AAA gaming. Media creation is another big one.",Intel,2025-11-06 02:10:09,7
Intel,nnde8rz,"Xe3 is not GPU family name but GPU core architecture, it's like Nvidia Ampere, Ada Lovelace. But Alchemist, Battlemage, Celestial is GPU family name.   Panther Lake 12Xe3 being B series GPU makes sense because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture. Intel confirmed Celestial will have Xe3P.",Intel,2025-11-06 05:25:14,10
Intel,nngpesf,"Think of it like AMD Zen X+ nodes. Ryzen 8000 is more or less a laptop only APU series on Zen 4+.   Xe3 is a half-generation, it doesn't get the letter upgrade to C, but it gets the 3, signifying a new architecture, but not a new generation.  Zen 4+ is a half-generation, it doesn't get the number upgrade to 5, but it gets to be 8000-series, signifying a new architecture, but not a new generation.",Intel,2025-11-06 18:45:02,0
Intel,nnklixi,"Mobile Zen 6 is likely to come around the same time NVL does. Both should use N2 and will presumably have similar frequencies, well above any 18A parts.",Intel,2025-11-07 09:57:31,2
Intel,nnlg7cm,"Considering the timing of 2nm, zen 6 would be around late 2026, and mobile zen 6 late 2027 wide availability. for whatever reason amd takes forever despite high mobile demand, but this quarter it looks like it worked out for them (maybe a big bump up from x3d sales).",Intel,2025-11-07 13:50:18,1
Intel,nnbigvq,What're you doing on a laptop?,Intel,2025-11-05 22:26:37,19
Intel,nnbiwg3,"These are for thin and light office notebooks and light gaming. Think Lunar Lake. For CPU power, Nova Lake H will exist.",Intel,2025-11-05 22:28:59,8
Intel,nnbowvu,It's for handhelds and office laptops not hyper enthusiast shit.,Intel,2025-11-05 23:01:09,2
Intel,nncdk44,Nobody buys AMD laptops,Intel,2025-11-06 01:23:02,11
Intel,nnbk5wm,">Still weaker than x3d   Source?? Also Panther Lake is H series only, HX will be based on Nova Lake.",Intel,2025-11-05 22:35:40,5
Intel,nnco8fw,Yeah so it is weaker for gaming with a dGPU than the 0.2% of laptops currently sold that have either a 7945hx3d or 9955hx3d that makes up for less than 0.1% of all laptop users. What's your point?,Intel,2025-11-06 02:27:36,1
Intel,nnz3axo,"While I haven't used it daily or anything, and I've only done initial setup on the Lunar Lake, the feedback we've gotten both on Arrow Lake and for Lunar Lake (e.g. 268V and 265H) Dell models is that it's a big increase in battery life and performance. The integrated graphics (e.g. 140V and 140T) are very capable compared to a **workstation** grade NVIDIA Ada 500 GPU, but they are not even comparable to a gaming GPU like the GeForce 4060 or even a 5050.  The integrated graphics do however get used for 99% of all workloads unless explicitly specified because they are vastly more battery efficient and draw less power compared to a dedicated NVIDIA chip, meaning you can have a much smaller external power supply, and your graphics performance in those basic desktop workloads with one of these chipsets will be **much** better than previous generation Intel chips. Exceptions are obviously something like gaming or AutoCAD that specify to use the high performance dedicated graphics chip.  140V/140T are barely functional for modern AAA gaming, but if you stay 5-10 years back for AAA titles you might be okay. It will smoke most Indie games. Just look at the per title benchmarks for a 140V/140T and you can see if your game benches. You could probably get away with a lot of functional mobile gaming without a dGPU, but I wouldn't expect to be able to play a recent Call of Duty or Black Myth or anything with anything like an acceptable framerate at a decent resolution. This integrated graphics chip compares very favorably to its more common Ryzen 7 equivalent, I believe the 780M, and it's a very good APU for handhelds overall due Lunar Lake's power efficiency compared to other X86 chips.  You have to understand that for these next two generations Intel seems to be making big strides in terms of both power efficiency and integrated graphics for mobile, it's a very attractive option and the first time I've seriously considered a laptop without a dGPU. I think Panther Lake is going to be a very nice kit next year for both laptops and handhelds and give AMD a run for its money.   I suspect AMD genuinely needs a new APU graphics architecture implemented next year to keep up, which I expect them to. Not a bad problem to have.",Intel,2025-11-09 18:46:41,2
Intel,nnbn2as,"Yeah for sure. It's a small die and should be yielding pretty high. See also the number of 4+8+4 SKUs. Looks like the larger CPU tile is also yielding decently, so not a ton to cut down.  I'm partly saying that because a larger Intel3 die was certainly possible. Even if it was 6 Xe3 cores and built as half of the larger die (just one of the two render slices)  it would fill the void a bit more.",Intel,2025-11-05 22:51:02,8
Intel,nndnorc,Which is rather silly. They should've named celestial Xe3 and the current Xe3 as Xe2P,Intel,2025-11-06 06:46:30,8
Intel,nndrn8v,"> But Alchemist, Battlemage, Celestial is GPU family name.  Specifically, *discrete* GPU family name.   > because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture  That is simply not true. Xe3 brings much bigger changes over Xe2 than Xe3p does over Xe3. That's why they were named that way.   > Intel confirmed Celestial will have Xe3P.  No, they actually haven't said anything about Celestial (again, as a dGPU) at all. They said that C-series naming (i.e. NVL iGPU) will start with Xe3p.",Intel,2025-11-06 07:23:49,3
Intel,nnbivk1,Highly immersive porn on the go,Intel,2025-11-05 22:28:50,14
Intel,nnc0bgd,These are H series chips. Even the U series chips don't go down to LNL min power levels.,Intel,2025-11-06 00:06:14,3
Intel,nnbkp8h,"NVL-H is 400 series, *replacing* this, next year. Not supplementing this lineup.   Adding more cores won't do anything for gaming.",Intel,2025-11-05 22:38:29,6
Intel,nnbkxvp,PTL extends up to the -H series too.,Intel,2025-11-05 22:39:46,3
Intel,nnbkts2,"HX this year will still be Arrow Lake.   Nova Lake will be a full line up, with S, U, H, and HX, but end of 2026 / early 2027",Intel,2025-11-05 22:39:10,5
Intel,nnbl91n,">Source??  You can't seriously be asking for a source for whether or not this part will be able to power dGPU gaming laptops better than X3D chips.   >Also Panther Lake is H series only, HX will be based on Nova Lake.  Not till late next year or early 2027. It's all arrow lake till then.",Intel,2025-11-05 22:41:25,5
Intel,nnc0woc,> Source?   Common sense suffices. It's a tick core with a clock speed regression at that.,Intel,2025-11-06 00:09:35,3
Intel,nnc2335,"Surely a cost decision. The 4Xe die, including the choice of Intel 3, is supposed to be the cheapest thing to deliver an acceptable mainstream PC experience. They need PTL to be a proper volume runner and start displacing the RPL that's still a large chunk of sales. WLC should hopefully finish the job.",Intel,2025-11-06 00:16:15,9
Intel,nnblgop,Just get a Vision Pro?,Intel,2025-11-05 22:42:32,7
Intel,nnbyre8,"Tbh, more cores would just make that go faster, but 16 would already be plenty. Especially for something like that where it's probably going to be a linear analysis and ram constrained if they actually modelled the gas (which would not necessarily be required).",Intel,2025-11-05 23:57:18,1
Intel,nnbmp20,Lmao this is funny we both responded to the same comments with the same things within like 2 minutes of each other.,Intel,2025-11-05 22:49:05,3
Intel,nnbrc9f,I don't see anything wrong with asking for actual benchmark information especially when there isn't anything official. X3D is nice but it isn't the end all be all. I would be curious to see if Intel can manage to compete.,Intel,2025-11-05 23:14:48,0
Intel,nnc5j69,"Oh I totally agree, but it would've been nice you know? Jumping to 6 Xe3 is a  significant area increase for a tiny tile. I understand exactly why the 4-10 gap exists, but I can't say I don't wish there was something to fill that gap if only because it looks weird.  I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.",Intel,2025-11-06 00:35:47,5
Intel,nnd2wbt,"PTL's main changes are fixing MTL/ARL's terrible SoC design, which should net a few % performance. It'll see a mild IPC increase, getting a few more % performance. And it'll lose a bit of clockspeed, erasing most of those gains.  Expect PTL to be very similar performance to ARL, but with lower power consumption, a much better iGPU, and most importantly to Intel: Using their own fabs instead of TSMC.  It absolutely won't be X3D in gaming.  Edit: Actually shocked that people think this would compete with X3D.  9955HX3D is \~16% faster than a 275HX in gaming...and a 275HX itself is easily 10%+ faster than a 285H in gaming.  Not even Intel themselves are claiming this. Their own marketing refers to PTL as ""ARL performance with LNL efficiency"". Nobody realistically expects PTL-H to see a 25%+ gaming improvement over ARL-H. The fact that IPC increase is less than 10% and clockspeed is slightly lower than ARL-H should make this obvious",Intel,2025-11-06 04:01:09,3
Intel,nnc1auf,CGC is a LNC tick. This is well known at this point. And we see it's even a clock speed regression.    Even entertaining the notion it will close the gap to AMD's X3D chips is just delusional.,Intel,2025-11-06 00:11:48,1
Intel,nncm4mg,"Oh, yeah, I get you. Wish they could give more granularity. Just personally think some sacrifices are worthwhile if it can condense Intel's mobile lineup back down to something sane again.   > I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.  Yeah, should be a good fit. Shame they don't have anything with a bit more CPU umph, though. 4+8+4 and only up to 5.1GHz is *fine*, but not great. Especially without an HX replacement.",Intel,2025-11-06 02:14:47,8
Intel,nnj3sul,"Two options seems right, either you care about it or you don't.",Intel,2025-11-07 02:28:49,1
Intel,nnh5cgt,"Its not ""delusional"" to want to see actual numbers instead of speculation. I have been in this game long enough to see plenty of speculation even with accurate information not give the actual numbers.",Intel,2025-11-06 20:02:16,1
Intel,nncnlt8,"Given how well ARL HX was received in gaming laptops, I think they may wait to have something from NVL take that top spot. 5.1ghz does seem low though. ARL-H will happily do 5.4 and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP.  I suspect these may not be totally final clocks though they do seem reasonable.",Intel,2025-11-06 02:23:44,3
Intel,nncrc61,"Honestly, surprised ARL-HX is doing as ok as it is. The deficits of the architecture in gaming are well known. If it could hit the same clocks and core counts, PTL should look a lot better still. And all that besides, ARL's cost structure is horrible. For Intel's own sake, the sooner they move on, the better.   > and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP  From the same leaker, these chips are at 65W or even 80W TDPs, so they're not merely power limited. It seems that 18A just significantly underperform some expectations, though in line with some rumors and the gist of the revisions Intel's been making to its projections over the last year or two.  > I suspect these may not be totally final clocks though  If they're defining SKUs and such, these clocks need to be finalized for all practical purposes.",Intel,2025-11-06 02:46:29,0
Intel,nnd4ak4,"If I'm reading correctly those are max power limits, not the TDP,  though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  As for why ARL HX is doing well in gaming laptops, I think a good bit of that is also part of what made it lackluster on desktop. It doesn't really scale up that well with higher TDPs and power limits, but it does seem to scale down. The 285HX with its 55W TDP and 160W max limit doesn't perform far off the 125W TDP and 250W max of the 285K.  It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system. The 9955HX3D is very impressive, but quite a lot of laptop buyers seem to value the ability to do more laptop-like things with their gaming laptops than the extra frame rate. I'm hoping this gets shaken up as AMD adopts new packaging tech as seen in Strix Halo.",Intel,2025-11-06 04:10:51,2
Intel,nndr8zp,"> If I'm reading correctly those are max power limits, not the TDP, though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  Not guaranteed given it's just a twitter leak, but I'm assuming the leaker is using the term TDP consistently with Intel's historical usage, i.e. PL1. For ARL, 115W is PL2. I would also assume there wouldn't be the disclaimer about it being hard to cool if they cut the PL2 so much, though PowerVia does create some interesting complications there, so maybe not quite apples to apples.  Either way though, don't think it should have much impact on ST boost. You're talking a good 70%-ish of power going to compute, so even at 65W PL2, that's still 40-50W available for one core. Should be *easily* sufficient to hit whatever the silicon is capable of.  > It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system.  Yes, and this is something I've very much looking forward to with NVL-HX. At this point, the biggest demerit of the -HX platform vs -H is the use of standard DDR5 vs LPDDR. That's because it's still based on the desktop silicon with the different SoC/hub tile. But with NVL using a shared SoC die, they should be able to offer an -HX platform with the core counts people expect (though probably limited to single die 8+16), but the power/battery life advantages of -U/-P/-H. In general, should help make the -HX more of a straight-up upgrade than the tradeoffs one faces today.  AMD has this situation even worse today, because there's a much bigger gap between their desktop SoC architecture and the mobile one. Though as you say, they may also bring them closer together in the future.",Intel,2025-11-06 07:19:59,5
Intel,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,11
Intel,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,6
Intel,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
Intel,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,2
Intel,nn1205o,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Intel,2025-11-04 08:46:02,2
Intel,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
Intel,np6680l,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Intel,2025-11-16 17:04:39,1
Intel,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
Intel,nmg20dw,Oh... for sure 😁,Intel,2025-10-31 22:15:08,3
Intel,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
Intel,nn1h3l3,Great work dude! Only 200MHz to go 😉,Intel,2025-11-04 11:15:21,2
Intel,nmilk0q,Car coolant in the freezer 😁,Intel,2025-11-01 11:12:18,2
Intel,np782zx,That's the way! Let us all know the results.,Intel,2025-11-16 20:14:39,1
Intel,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,3
Intel,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
Intel,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
Intel,np7d3w5,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Intel,2025-11-16 20:40:17,1
Intel,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
Intel,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
Intel,npa5wyd,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Intel,2025-11-17 07:21:57,1
Intel,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
Intel,nmoztyk,But Core Ultra 255Hx is almost $150 more than Zen 4 8945HX on lenovo Legion. That price gap is enough to upgrade 5060m to 5070m.,Intel,2025-11-02 12:36:20,5
Intel,nmzufnv,"Can I redeem the codes to my accounts on a different Intel system? I bought a B860 motherboard and an Ultra 5 245k, but I won't be building that system till Christmas. I'm currently running an 8700K on a Z370.",Intel,2025-11-04 02:52:11,2
Intel,nmquye0,Idk if I did it wrong but redeemed my cpu but not my arc card on the website. Couldn’t contact support because it kept throwing invalid captcha at me.,Intel,2025-11-02 18:34:43,1
Intel,nqarc70,just purchased a laptop from micro center with a 275hx but don't know how I would redeem this specific offer as I only get the one that lets you pick 1 of 4 games. Does mine not qualify/is micro center not participating?,Intel,2025-11-23 03:04:10,1
Intel,noa52e1,"nope, you need to have installed Ultra processor to get promo game, because Intel used software to check it",Intel,2025-11-11 13:57:33,1
Intel,nlbgoss,I really would like to know how it will compare to AMD Ryzen AI MAX+ 395 (Strix Halo) APU?,Intel,2025-10-25 14:51:22,11
Intel,nlawueh,That naming scheme really is complete and utter dogshit,Intel,2025-10-25 12:59:00,7
Intel,nlp3wrh,Sounds like fooz will be getting a taste soon? Christmas or Q1 2026?,Intel,2025-10-27 19:03:55,1
Intel,nlrilyu,Can we just toss random darts at tech words and numbers to assign a different naming scheme to each and every different Intel Product?,Intel,2025-10-28 02:55:34,1
Intel,nlz6ywu,I'm looking forward to check how those series will perform!!,Intel,2025-10-29 09:04:56,1
Intel,nlbj26i,"This isn't a ""halo"" product. An upgrade to Arrow Lake/Lunar Lake when it comes to iGPU, but not Strix Halo. It won't cost as much as well.",Intel,2025-10-25 15:03:33,18
Intel,nlbihha,look forward to new APUs,Intel,2025-10-25 15:00:35,2
Intel,nlhde6w,Wait for Nova Lake AX. It will have 48 GPU cores instead of 4/8/12.,Intel,2025-10-26 14:36:10,2
Intel,nlpqfuu,Likely will be worse since it has only 1/3 the power usage and significantly smaller chip size.,Intel,2025-10-27 20:58:10,1
Intel,nmbs4xl,"It's meant to confuse you in purpose, so you ignore it and go by the 3/5/7/9 scheme. Marketing success is dependent on the company leading the customers to the way they want it. So it needs to be complex and confusing.",Intel,2025-10-31 06:11:07,1
Intel,nldt8zb,"also why is everything in a lake. like really that's the last place you want your chips to be.  and why is it 255k, 285k and so on instead of just 250k and 280k.  and why is it arc xe3 and not just xe3 ahh  so many weird conventions really. no consistency at all. just pure confusion.  apple and nvidia get it right too, it's really not that hard.",Intel,2025-10-25 22:15:21,1
Intel,nlclyxd,"'Strix Halo' is lees about being a halo product, because it’s just an AMD internal naming scheme for the set of specific APU products. APUs that will follow 'Strix Halo' will be called 'Gorgon Point'. Since 'Strix Halo' is already out and the Panther Lake in question is about to come out it might be better to compared it to the next AMD 'Gorgon Point' APU line? But I still think it can be compared to 'Strix Halo'. At least I’m looking forward to see the comparison.  Edit: changed 'Strix Point' into 'Gorgon Point' since that’s what I meant.",Intel,2025-10-25 18:23:40,-9
Intel,nlguu28,You’ll be waiting till 2027 on the amd side.,Intel,2025-10-26 12:44:06,3
Intel,nmbs0u8,All halo iGPUs are way too overpriced. Even regular iGPUs are overpriced going into $1K laptops.   And if you want to spend that money you can do it today with Strix Halo.,Intel,2025-10-31 06:09:58,1
Intel,nlq5vco,"Lesser power usage doesn’t always equals to performance losses.  See the Apple Axx SOCs and the Qualcomm snapdragon SOCs.  Even with the x86-CPUs over time they had more performance gains while maintaining almost the same power consumption.  Not anymore, but they could catch up with better chip design?",Intel,2025-10-27 22:20:54,1
Intel,nlg1l4u,>also why is everything in a lake. like really that's the last place you want your chips to be.  Easier to cool them,Intel,2025-10-26 08:16:14,7
Intel,nlp4s9i,"Stop trying to find meaning in the naming scheme. Patience grasshopper, all will be revealed in time.",Intel,2025-10-27 19:08:23,2
Intel,nlpqrfz,Like how is every chip company so bad in naming stuff?  Intel names things from geographical stuff to avoid politics etc. So I get the lakes  The numbers I dont.,Intel,2025-10-27 20:59:49,1
Intel,nlcna8j,"> APUs that will follow 'Strix Halo' will be called 'Strix Point'.  What? No, that's not what any of these terms mean. The Strix lineup is already out. ""Strix Halo"" is literally their halo, big compute/iGPU product.  PTL will compete with normal Strix Point and its refresh next year.",Intel,2025-10-25 18:30:27,10
Intel,nlp4f4m,Really? This should be good for Intel in 2026.,Intel,2025-10-27 19:06:31,1
Intel,nmbt7jh,These Halo iGPUs are meant for LLM first and foremost. If you just want to game just get a normal RTX laptop.,Intel,2025-10-31 06:22:00,1
Intel,nlqexwa,They are like 20% more efficient.  You are talking about 200% more efficient if it hits the same performance.    They might as well make it a mobile chip if that is how good it is. Would be great on phones.  Would be better than most pcs right now on 5W envolope.,Intel,2025-10-27 23:11:50,1
Intel,nmbsirj,"It's intentional, and non specific. So most go by the 3/5/7/9 naming. In order for them to have max profit, they need to lead you to the chips they want you to buy.",Intel,2025-10-31 06:15:03,1
Intel,nld2j3h,'Strix Halo' is a product line as 'Gorgon Point' is also/another product line. Both have different variants.  In the case of 'Strix Halo'  [Check](https://hothardware.com/news/amd-strix-halo-cpu-rumors)  Find the official lineup here  [AMD](https://www.amd.com/content/dam/amd/en/documents/partner-hub/ryzen/ryzen-consumer-master-quick-reference-competitive.pdf)  Edit: changed 'Strix Point' to 'Gorgon Point' since that what I meant.,Intel,2025-10-25 19:51:15,-3
Intel,nlqjpxg,👍,Intel,2025-10-27 23:38:19,1
Intel,nld2x4w,"Yes, Strix Point lives *alongside* Strix Halo. Strix Point is already out, and came out *before* Strix Halo at that. It is not the followup to Strix Halo.",Intel,2025-10-25 19:53:17,8
Intel,nld43st,You are right. I meant 'Gorgon Point'.,Intel,2025-10-25 19:59:34,1
Intel,nln841i,"Gorgon point is just a refresh of strix point. These terms mean less in the next gen anyway as the follow on products have quite different names. Medusa point isn't really the successor to gorgon point, for example.",Intel,2025-10-27 13:21:07,2
Intel,nlp48zs,My brain hurts and I’m still confused,Intel,2025-10-27 19:05:38,1
Intel,nmbrwqs,That's why you have guys that are completely ChatGPT-levels of confident when they are completely out of whack with the info lol. No worries.,Intel,2025-10-31 06:08:49,2
Intel,nkxsilf,"I think him saying ""unreleased products"" could mean it's still coming.",Intel,2025-10-23 11:21:44,26
Intel,nkz30mp,I think B770 is locked in and to release shortly isn't it? sure I saw a leak of packaging details etc.,Intel,2025-10-23 15:39:54,9
Intel,nkzp226,"We finally got reviews for the B50 and it is a SFF gem! That said… I wish Intel would get better at promoting upcoming products, they seem to be slowed down by the restructuring.  I’m going with the flow, whatever will be will be. I’m waiting for Dividends to kick back in so I can retire with Intel.",Intel,2025-10-23 17:25:52,4
Intel,nl8a9mb,"Boy, I am glad they picked an easy naming convention.   /s  After Xe, Xe1, Xe2 no doubt next would be Xe3, then Xe3p  After A-series, B-series, no doubt next would be C-series.    Too bad those generations do not line up",Intel,2025-10-25 00:22:50,1
Intel,nkxmfto,Intel is not serious with dGPUs,Intel,2025-10-23 10:33:35,-5
Intel,nl0j2ic,"If it was coming, you expect them to talk about the future even a little bit. Instead, nothing.    They cancelled Celestial over a year ago now. Sounds like things haven't improved since.",Intel,2025-10-23 19:51:32,7
Intel,nlec4sq,"I think him saying he doesn't talk about unreleased products is 100% bullshit. He talks about upcoming tech constantly. This is not just unreleased, it's unannounced, unclaimed and nonexistent outside of pure speculation.",Intel,2025-10-26 00:09:57,2
Intel,nm6dcp4,B50 is interesting once the software is there (planned Q4).,Intel,2025-10-30 11:58:35,1
Intel,nkxz8vy,"Yes, they are literally just playing. It's all a game lol",Intel,2025-10-23 12:08:42,12
Intel,nkz3604,"They have released about the same number recently as AMD, I'd say they're pretty serious. The B50 is a pretty compelling product too, big features for the price.",Intel,2025-10-23 15:40:37,2
Intel,nlbmybg,They are as serious as AMD.,Intel,2025-10-25 15:23:34,1
Intel,nl1d1w8,This interview happened during the quiet period so I don't think he could talk about the future,Intel,2025-10-23 22:26:07,6
Intel,nlbmm1g,"They cancelled Celestial over a year ago now. Sounds like things haven't improved since.   MLID people showing themselves.  Tom Peterson previously said Celestial discrete hardware was already done and they were working on Druid. So if they cancelled it, it must have really sucked.  I am sure after they release Celestial 70 series beater, MLID will come and say they cancelled the 90 series beater and it's for sure dead from now on!",Intel,2025-10-25 15:21:47,2
Intel,nlc5u9q,Where did you hear that it was cancelled?,Intel,2025-10-25 17:01:38,1
Intel,nkzrbdu,"We are supposed to get B60s also but they will likely be very limited and part of Battlematrix. Intel is moving very slow with Pro and Consumer GPUs and they can’t rely on TSMC for supply and obviously are not ready to manufacture through IFS? We are getting left in the dark, all we can do is wait.",Intel,2025-10-23 17:36:29,3
Intel,nkzb6g5,There has been no update regarding celestial dGPUs internally.,Intel,2025-10-23 16:19:27,4
Intel,nl9n96c,AMD is skipping a generation to focus on the next. Intel has lost its focus on GPUs. These are not the same things.,Intel,2025-10-25 06:15:43,1
Intel,nlc8fdp,Lol no. ARC was 0 margin product. Now it's fate depends on the whims of VPs not engineers.,Intel,2025-10-25 17:14:34,1
Intel,nlcandy,"You could likewise point not that there was no word about dGPUs in the PTL presentation either. I think people need to accept that it's just not happening, at least for the foreseeable future.",Intel,2025-10-25 17:25:50,1
Intel,nlbpug0,"> MLID people showing themselves.  I'm not getting this from MLID.  > Tom Peterson previously said Celestial discrete hardware was already done  No, that's absolutely false. Actually watch the interview instead of reading reddit comments. He said Xe3 (specifically in PTL), not Celestial, was done. And this was after the PTL tapeout was announced, so that didn't even tell us anything new.   And as we now know, they don't consider that even in the same family as what would be Celestial.",Intel,2025-10-25 15:38:24,1
Intel,nlc7sgw,Ex-Intel coworkers/acquaintances.,Intel,2025-10-25 17:11:23,2
Intel,nl34or5,>There has been no update regarding celestial dGPUs internally.  Do you have internal information?,Intel,2025-10-24 05:10:43,2
Intel,nl3q53x,Why would they? Battlemage is not even finished. Battlemage is not even 1 year old yet. They will still release B7XX gpus and probably B3XX.  I expect them to tallk about celestial by next year.,Intel,2025-10-24 08:35:10,2
Intel,nlafjvl,> Intel has lost its focus on GPUs  So despite them repeatedly telling you they have not... they have?,Intel,2025-10-25 10:48:26,1
Intel,nlcioic,"Battle mage is not a 0 margin product...  I know how much silicon cost etc due to my job. Believe me there is at least %30 gross margin in Battlemage and that is assuming somehow Intel got a worse price compared to my small ass company.     It's not profitable due to amount of R&D it takes to develop it, Intel earns a significant chunk for each Battlemage sold. They are simply not as greedy as Nvidia and AMD to earn market share.",Intel,2025-10-25 18:06:26,1
Intel,nlbtqtn,Dude XE3 even has some test shipment reports etc. It's too late to cancel.   Sure if it's not good maybe we will only see B580 replacement.    But it's literally impossible and stupid to cancel it right now. Especially given how much gross profit they made from B580,Intel,2025-10-25 15:58:32,3
Intel,nlcbiv4,"So, no news story has come out stating that?",Intel,2025-10-25 17:30:18,3
Intel,noidhfz,Xe3P-HPM suggests otherwise,Intel,2025-11-12 19:47:04,1
Intel,nlc8k5a,"Yes, through my ex colleagues",Intel,2025-10-25 17:15:14,2
Intel,nleb7hi,"No, I don't listen to them, they have a nasty habit of downplaying bad situations. I'm going by their actions.",Intel,2025-10-26 00:04:10,1
Intel,nlbuyry,"> Dude XE3 even has some test shipment reports etc  Celestial wasn't base Xe3, and didn't tape out before cancellation. What test shipments are you referring to? PTL?  Btw, they still aren't saying anything about BMG G31, and that was much further along than Celestial was.   > But it's literally impossible and stupid to cancel it right now.  You can cancel a product at any point before it's released. Anything else would be sunk cost fallacy. Surely you're aware of the massive budget cuts and layoffs they've announced. Not everything can survive.   > Especially given how much gross profit they made from B580  By all reports, BMG still wasn't profitable for them. Hell, even if it *was* profitable, doesn't mean profitable *enough* for Intel to keep funding it in this environment. They're prioritizing spending reduction, not profit maximization.",Intel,2025-10-25 16:04:56,2
Intel,nmbst69,Why would Intel tell you? It would just stall selling all current ARC cards.,Intel,2025-10-31 06:17:59,3
Intel,nlchyo2,No. Or at least not from any reliable source. Obviously discounting MLID and his ilk.,Intel,2025-10-25 18:02:48,2
Intel,noifll4,What about it? That some reference exists in drivers?,Intel,2025-11-12 19:57:24,1
Intel,nlc2d7k,"You don't even know that?   Lip Bu has been hiring gpu designers not firing them.   Most of the Cuts are from foundry side and slightly from gaudi side.    Intel if anything is focusing on gpus to create AI inference gpus.    Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.      You should check your sources, Lip Bu would cut 14A before he cuts Inference gpu side.",Intel,2025-10-25 16:44:12,3
Intel,nlc7zs4,BMG was 0 margin product,Intel,2025-10-25 17:12:23,3
Intel,noiihla,"Of which are recently implemented, as in 'in the past week' which would be exceptionally stupid to do for a cancelled project",Intel,2025-11-12 20:11:56,1
Intel,nlc3nqx,"> You don't even know that?  What do you claim I do not know?  > Lip Bu has been hiring gpu designers not firing them.  Celestial was cancelled under Gelsinger, as well as several rounds of client GPU layoffs. If Lip Bu is hiring anyone, it's not to build the team back up again.   > Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.   I am specifically talking about client graphics, yes. There's shared work, and arguably should have been more, but they were quite different. Client iGPU, client dGPU, and server dGPU were basically all separate SoC designs.",Intel,2025-10-25 16:50:50,2
Intel,noijing,They're also using Xe3p for NVL-P and that Island AI product.,Intel,2025-11-12 20:17:09,3
Intel,nlci8uq,"It wasn't though?  As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  I don't believe there is nothing Intel can do convince you. I heard these news every single time.     Also just look at job listings, there is many for gpu development.",Intel,2025-10-25 18:04:14,3
Intel,noimd5x,"Which means it is being used, produced, and cannot be disqualified yet, nor does anything, not one trustworthy source, show it is cancelled",Intel,2025-11-12 20:31:37,0
Intel,nlck9s5,"> As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  Go ahead and point out where *I* said ACM or BMG were cancelled. I have no idea who ""you guys"" are, nor do I care what others are or are not saying.  > I don't believe there is nothing Intel can do convince you.  Well yes, if they cancel a project, and I know they have, I'll say they cancelled it. The way Intel can convince me to say otherwise is by... not cancelling projects. And you *do* realize they haven't talked about client dGPUs past BMG in many, many months, right? It's not like Intel's really denying anything.  I'm not sure what you're looking for here. Should I lie and pretend not to know what I do? To what end? It's not like Intel's harmed by me saying they cancelled such a project. That's not something you can keep a secret indefinitely, and anyone who *really* cares already knows.  > Also just look at job listings, there is many for gpu development.  If you lay off 10 people, 5 more leave on their own, and then you backfill 4, that's still a net loss. They still need some people, but not as many as they had, and not for client dGPU.",Intel,2025-10-25 18:14:45,1
Intel,nled55i,"Intel have not claimed there to be upcoming Celestial, or more Battlemage or anything like it. How did their silence convince you of anything?",Intel,2025-10-26 00:16:21,1
Intel,nmbspxb,Actually after the Nvidia deal there's definite reason for Intel to cancel future ARC development.  They have a partner that makes better GPU than them. Why would they continue? If the Nvidia deal is successful I expect even their iGPU will disappear.  In no sense it makes sense to develop a line that's redundant with what your partner is doing.,Intel,2025-10-31 06:17:05,1
Intel,noinhcz,"> Which means it is being used, produced, and cannot be disqualified yet   Again, Xe3p lives for various things. Celestial, the client dGPU planned to be based on that IP, is dead. I'm not sure what contradiction you think exists here.    > nor does anything, not one trustworthy source, show it is cancelled   Intel's refusal to talk about future gen client dGPUs and the mass layoffs in the hardware team don't tell you anything? Or what about Gelsinger's remarks about less focus on dGPU?    This is how Intel usually handles cancellations btw. They simply pretend it never existed, unless investors really demand to know. Even then they like to delay the acknowledgement.",Intel,2025-11-12 20:37:24,3
Intel,nkl7ghk,Think pretty easy naming. Any X infront just automatically means better iGPU,Intel,2025-10-21 12:17:01,36
Intel,nklikpx,"Looks pretty snappy at \~75% faster than the top-end LNL chipset and this is with pre-release drivers.  Granted TDP is probably higher.  If they get the drivers cleaned up then it might release a bit higher which makes it a viable (though still materially weaker) thin and light XX50 dGPU model alternative some some of the market.  Should do well addressing the 'I want a thin and light laptop, but I want it to have an ok GPU' crowd.",Intel,2025-10-21 13:28:12,19
Intel,nkodq7t,"Panther Lake 12 Xe3 performance looks great to match RTX 3050 laptop performance because the entire chip only draw half power of RTX 3050. Seems like using 18A BPD really paid of to reduce CPU consumption by a lot with the helps of Tsmc N3E for iGPU.    Also it's so weird to see Asus Zephyrus G14 with Intel chip, usually it always has Amd CPU paired with Nvidia GPU. I heard G14 is pretty popular gaming laptop but this laptop during full load can use 120w+ power.    Using Panther Lake 12 Xe3 will makes this laptop looks even more appealing because it reduces power requirements from 120w+ to 45w but still giving about the same GPU performance which is insane. This is a massive game changer for people who use laptop as portable gaming machine and to those who travel a lot. I can totally understand why Asus this time use Panther Lake for G14.",Intel,2025-10-21 21:59:26,8
Intel,nkkzyp1,"Nice to see the G14 with an Intel CPU. Thought the lineup was AMD only tbh, while the larger G16 laptops get Intel.",Intel,2025-10-21 11:20:48,14
Intel,nkkoc9f,Naming for these chips are terrible,Intel,2025-10-21 09:31:57,10
Intel,nkmeuem,"Can Asus send me this laptop for review? I have 14 followers on snoozetube and 60% are probably bots, but bots are human too?",Intel,2025-10-21 16:22:11,2
Intel,nkzbmky,Can't wait for 14inch laptops with actually good battery life and convenience than the cheap gaming laptops it's going to kill,Intel,2025-10-23 16:21:35,1
Intel,nkld363,"Nah, should've kept that info at the end like every other Intel and AMD CPU ever made.  But otherwise this branding really feels like AMDs APU line, where they had to emphasize their iGPU was better than average.",Intel,2025-10-21 12:54:21,17
Intel,nkuhiob,"The possibility of being nearly 100% faster than Lunar Lake in some tasks, and minimum possibly 50% faster while being able to fit it into a sub 3lb/1.5kg design with a 80+wH battery is going to really nice. If the 4 LP-e cores scheduling work well and maybe a more efficient OLED panel you could easily get true 24 hrs use on x86",Intel,2025-10-22 20:56:33,3
Intel,nl9xb0y,"If the game could be 50–60% stronger, that would be That would be a killer",Intel,2025-10-25 07:53:56,1
Intel,nl9xpor,Its GPU part isn’t 18A at all — it’s actually N3E and 4Xe3 integrated graphics use Intel 3.,Intel,2025-10-25 07:57:55,1
Intel,nkljsi3,"That’s been true for the past generations, but it looks like it will change this generation",Intel,2025-10-21 13:35:30,8
Intel,nkkrh6o,Still better than Ryzen 365 AI pro MAX+,Intel,2025-10-21 10:04:31,54
Intel,nkm3cl3,"I disagree, GPU focused = X (like Xe3). Just takes getting used to , but otherwise it follows the same 3 7 9 scheme that probably didn't make much sense at first either :)",Intel,2025-10-21 15:22:49,3
Intel,nkv8b22,https://browser.geekbench.com/v6/compute/compare/5050048?baseline=4771132,Intel,2025-10-22 23:24:43,1
Intel,nkm4j1i,The typical consumer doesn't know anything about the last letter. Having it in front will be much more successful to communicate to consumers the difference.,Intel,2025-10-21 15:28:57,2
Intel,nkksh88,Yea putting ai the model name is disgusting 😂,Intel,2025-10-21 10:14:17,23
Intel,nkl1j6x,I can't wait for the Ryzen 688S AI Pro MAX+++,Intel,2025-10-21 11:33:19,12
Intel,nkmi2u0,"I agree with this and now snoozetube creators are doing 128gb reviews for the 365 AI Pro Max+ and glossing over the fact that it costs decent money but lacks any kind of power when compared to discrete GPUs.  Amd continues to pump out expensive APUs that are mediocre, while doing everything related to Radeon half heartedly.  Why is that?",Intel,2025-10-21 16:38:27,3
Intel,nkmgpw7,"Nerds argue over names for tech products but will eventually figure out some kind of logic in why they named it that way. Entire generations need to be released and compared.   As for average users they will always be perpetually clueless and unfortunately will become influenced by an influencer with no integrity and or a store associate who has been trained on scripts that make the most money for the store.  God help us all, I pray for Jesus - just like Pat Gelsinger, who will get no credit for the Intel turn around.",Intel,2025-10-21 16:31:44,4
Intel,nkpnp7f,Ryzen metaverse Ai max++ 3D Hypercache macroboost,Intel,2025-10-22 02:32:12,3
Intel,nklj3is,I think it's rather on point. the 395+ is a beast for running large MoE AI models. It's value for money in that respect is almost unbeatable.,Intel,2025-10-21 13:31:20,-4
Intel,nknc85p,you forgot the x3dx2    when both cpu tiles are stacked on 3d cache tiles.,Intel,2025-10-21 18:59:33,2
Intel,nklps98,Ultra TypeR S-line AMG M Bi-Turbo CCXR LM Harley Davidson Edition,Intel,2025-10-21 14:09:54,0
Intel,nko4b5d,"Very very few know anything more than that, usually completely unaware that there's a whole SKU number after that.  How many times do you hear stories about some user proudly boasting about having an i7, only to find out that it's like a 6th gen, and they don't even realize / believe that something like a i3-12100 is actually a better CPU.     The average user understands the difference between, say, a Core Ultra 5 and 7, because the ideal of 3, 5, 7, 9 being product tiers exist in plenty of industries, like BMW's product line. Bigger number = more performance. How? By how much? No clue to them.  So since the average user is going based off just the name 5, 7, or 9, having that X visible in a location they'll see is certainly very important. They'll notice the X.",Intel,2025-10-21 21:10:14,6
Intel,nklngwf,"It would be like a Chromebook named ""Chromebook CloudCompute+"" just because that's what those are built for",Intel,2025-10-21 13:56:45,14
Intel,nkns6e4,I personally prefer them Name it Ryzen 3 / 5 / 7 / 9. It’s easy to understand and easy to compare to intels naming but sadly both companies have ruined it now.,Intel,2025-10-21 20:13:23,3
Intel,nkoestf,That's honestly sounds even more cringe. Can you imagine Amd Ryzen 9 395X3DX2 AI Pro Max+? That's ridiculously bad LMAO,Intel,2025-10-21 22:05:28,3
Intel,nkmk3eq,BMW Individual M760i xDrive Model V12 Excellence THE NEXT 100 YEARS,Intel,2025-10-21 16:48:08,2
Intel,nkpsi6y,I actually did my research and found out that core ultra 5 125u is not much different from core ultra 7 155u... Ended up buying ProBook with core ultra 5 125u and saved money for upgrading the ram and SSD,Intel,2025-10-22 03:02:49,4
Intel,nkof5d7,"but i think its gonna happen aye,  i wonder if the RAM bandwidth needs for AI benefit from cache like games do, or are they better slapping more ram channels on it...",Intel,2025-10-21 22:07:26,1
Intel,nkq1gt0,"Yeah, all of the U chips within a generation are the same physical chip, just different bins (usually tiny clockspeed differences). I don't think they even have core count differences any more for the most part.",Intel,2025-10-22 04:06:17,2
Intel,nm4gnwa,Same core counts too,Intel,2025-10-30 02:20:15,1
AMD,nx8qncw,"As the comments on the site says, these are not new, they are new revisions of old boards.",hardware,2026-01-02 12:19:54,80
AMD,nx8nudr,>DDR4 memory that's available at more affordable prices  doubt.gif,hardware,2026-01-02 11:57:08,51
AMD,nx8k6wk,AM4 is the gift that keeps on giving.,hardware,2026-01-02 11:26:11,32
AMD,nx9tn8c,"Would be nice if they specified what the differences are, were there bugs in the old revisions? Are they cheapening out on the VRM? Do they support something new?",hardware,2026-01-02 16:06:19,7
AMD,nx99vvl,"We're not very far from them releasing new DDR3 boards. Maybe after a year or so, after a kit of DDR4 costs as much as a 5080.",hardware,2026-01-02 14:24:59,3
AMD,nxafwsh,I'd be curious the changes but personally I find this exciting. I have a AM4 rig that needs a new motherboard and I've been holding off because the options I could find were very limited. If this improves availability I'm all for it.  To be frank motherboard failures have been by far my most common failure point in builds over the years. And when a socket becomes out dated it can be very challenging to find something equivalent to replace them with. The used market can be a gamble. Breathing new life into a still useful system with a new motherboard is a good option to have.,hardware,2026-01-02 17:50:28,1
AMD,nxqh17i,"My new system:  9700X 64GB DDR5-6000 RTX 5070Ti   Old system 5900X 64GB DDR4-3600 Same GPU   My old system was a better performer. And get this, I cannot use the full ram speed on my new system, it freezes. My old system used up to 3600 on XMP with no issues, my new system IS 3600 and I'm told I should leave it there or issues will occur.",hardware,2026-01-05 01:18:01,1
AMD,nx91pyo,32 GB of DDR4 costs the same i paid for 64 GB of DDR5 lol.,hardware,2026-01-02 13:37:06,31
AMD,nx8qsb5,"Easy enough to find used...people will presumably realise the demand and raise the used price as well though.  Still, if you're happy with standard speeds then there's plenty of it out there for not much.",hardware,2026-01-02 12:21:00,10
AMD,nxav5nm,"It is possible to save like $100-150 on a 32GB set compared to DDR5.  Not enough to be worth it in my opinion, though.  Yea, DDR5 prices are even more painful, but if you HAVE to build a PC at the moment, AM5 still makes more sense in the long term.  There's more to value than just what things are like today.",hardware,2026-01-02 19:00:21,3
AMD,nxao7p2,Looking at Amazon right now: * 32GB of G.skill DDR4-3200 CL16 is $240 * The cheapest 32GB DDR5 kit shipped by Amazon (a G.skill DDR5-6000 CL36 kit) is $365  * 64GB of G.skill DDR4-3200 CL16 is $420 * The cheapest 64GB DDR5 kit shipped by Amazon (a G.skill DDR5-5600 CL36 kit) is $710,hardware,2026-01-02 18:28:26,1
AMD,nx9xeur,"It's tech stagnation *(beyond the simple fact these are revisions, not new products)* - not something to celebrate.",hardware,2026-01-02 16:24:02,9
AMD,nxbsrrz,>To be frank motherboard failures have been by far my most common failure point in builds over the years.   I would guess that is most people's experience as well.  Motherboards are easily the most component-complex part of a computer.  Just so many different things that can go wrong with them.,hardware,2026-01-02 21:43:03,3
AMD,nxal5j1,I didn’t last rebuild because my x570 board finally dies enough it wasn’t usually. It was a slow death spiral that finally came to a head when I had no more sata ports and only a partially functioning x16 slot.,hardware,2026-01-02 18:14:34,1
AMD,nxbsf49,Why’s that. I’m new the pc world and thought they were up there,hardware,2026-01-02 21:41:20,4
AMD,nx9eonz,I actually was tempted to upgrade to 32GB (2x16GB) but I looked at the prices.  No thanks.,hardware,2026-01-02 14:51:46,10
AMD,nxa3x89,RAM prices are so shit that the next system I buy might use SO-DIMM slots so I can cannibalize the 96GB RAM I have in my mini-PC/server/NAS that I paid $170 for a few months back. Then I'd pay $170 for something like 16GB to temporarily live in the NAS (which has 280GB optane for extra caching anyway).,hardware,2026-01-02 16:54:20,6
AMD,nxamx2j,8GB DDR4 costs the same I paid for 32GB of DDR4 lol.,hardware,2026-01-02 18:22:35,1
AMD,nx8tpoy,">Easy enough to find used  Ye, even used high performance B-die is still considerably cheaper than a C30 6000 D5 kit for the most part. If you settle for something like 3200C14 b-die you can score some deals.  If you go outside of b-die, there's droves of memory if you start looking.",hardware,2026-01-02 12:43:35,8
AMD,nx9ytsf,Bad news. The used market sucks too. I just ebayed 64gb of DDR4 that had been sitting in my drawer for about a year. I paid less than $100 for it new and sold it for $240.,hardware,2026-01-02 16:30:39,2
AMD,nxa757w,New tech is still coming out though... They are just supporting older products,hardware,2026-01-02 17:09:26,10
AMD,nxa4jt2,They are objectively worth celebrating. It's socket longevity regardless of where you try to plant the goalposts.,hardware,2026-01-02 16:57:13,11
AMD,nxaglwl,"It is for me.   My daughters Asus motherboard went from Ryzen3, Ryzen5 and finally a Ryzen7.   Bumped the ram from 16 to 32gb.  She plays very few games and would not notice the difference if I upgraded to an AM5 motherboard.",hardware,2026-01-02 17:53:41,2
AMD,nxlhbl9,"Not ""shit"". Well they had some really problematic PSUs, hot stuff. And with the RTX50/RX90 series launch they f'ed up their thermal conductive material but continued to deny the existence of any problems, which was disappointing.",hardware,2026-01-04 08:56:43,1
AMD,nxdy1i5,Might be easier to get a few so-dimm -> u-dimm adapters.,hardware,2026-01-03 05:06:04,4
AMD,nx9ywt9,I'm not picky I'm not going to spend $100 more for like 3% better performance so I'm just going to deal with losing a few FPS,hardware,2026-01-02 16:31:03,8
AMD,nxac8j7,"Sure, but if you need 64GB right now, DDR4 is a lot cheaper than DDR5.",hardware,2026-01-02 17:33:17,1
AMD,nxb84c5,Because this has been in the pipeline for a while.  You can pretty much guarantee that we're going to see product launches delayed and hardware specs scaled back because of these RAM prices.,hardware,2026-01-02 20:02:20,3
AMD,nxakda3,"What does that have to do with a ""new"" AM4 motherboard release?",hardware,2026-01-02 18:10:59,-1
AMD,nxbka6j,"Nothing, its just AM4 is a solid product since I was able to upgrade my kids PC multiple times without having to buy a new motherboard.",hardware,2026-01-02 21:02:00,2
AMD,nxaw0og,"I think they're saying that AM4 is still good enough for plenty of people, and having more options on the market is a good thing.  Not that AM4 motherboards disappeared, but certainly not quite the huge selection and availability as before.",hardware,2026-01-02 19:04:25,2
AMD,nx8kekz,I'm keeping my 5800X until DDR6.,hardware,2026-01-02 11:28:05,134
AMD,nx8frrc,TLDW:    14 game average (geomean):     A. 1080P (medium) using RTX 5090 GPU:          - 5600X 4% slower than 5800XT    - 5600X 20% faster than the 12400F using the same DDR4-3600 memory      - 5600X 9% slower than Intel Core Ultra 5 225F that uses faster DDR5 CUDIMM    - 5600X 17% slower than 7500F    - 5700X matched the performance of the 12400F using DDR5 memory     B. 1080P (ultra) using RTX 5090 GPU:       - 5600X 16% slower than 7500F     - 5700X matched the performance of the 12400F using DDR5 memory,hardware,2026-01-02 10:46:39,113
AMD,nx8imcq,AM4 really is a gift that keeps on giving.   I'll mention I've thing that doesn't get mention much in context of the videos - e-waste saved by allowing people to keep the same motherboard while getting meaningful CPU upgrade choices.   It's good for your pockets and for sustainability,hardware,2026-01-02 11:12:13,99
AMD,nx8h72c,I saw 5800xt and thought it was about GPUS... wtf,hardware,2026-01-02 10:59:29,24
AMD,nx8sc8w,"I literally just sold my 5700X, B450 , 32gb 3200mhz yesterday.  Did I make a mistake?  I bought a Ryzen 7 7700, B650 and 2×8GB 5600mhz today   Will be overclocking the ram to 6000mhz cl36  edit : the upgrade cost me 51 USD by the way, bought the upgrade on the used market. GPU is 5070Ti",hardware,2026-01-02 12:33:06,14
AMD,nx8gav7,u/hardwareunboxed  Thanks for clarifying why the Intel i5-12400F performs significantly below the Ryzen 5 5600X.,hardware,2026-01-02 10:51:28,44
AMD,nx8hub4,Except ddr4 memory has also increased in price. Just not by as much. Just double not 5x.,hardware,2026-01-02 11:05:16,14
AMD,nx95dks,"It's depressing that the 5800X3D has not deprecated one bit. If you bought your 5800X3D 3.5 years ago at $450 you could sell it used for $450 or more today, because AMD won't make any more of them.",hardware,2026-01-02 13:58:54,16
AMD,nxa9sn5,The 5800X I got 5 years ago is still trucking along just fine thanks.  It's getting passed on to my nephew next year. more because he wants a PC than because I feel a pressing need to upgrade.  We forget what a solid generation Zen3 was even before the X3D glamour.,hardware,2026-01-02 17:21:52,4
AMD,nx8fm2a,5700x3d upgrade path available as well for $355,hardware,2026-01-02 10:45:14,17
AMD,nxcske4,"I've been torn on whether or not I upgrade to 5xxx series cpus, 7xxx, or 9xxx... I'm currently using a 3600 still and the 5xxx would be a quick and easy upgrade but I'm not sure it'd be totally worth.  Jumping to 7xxx like a 7600x3d would mean a new mobo/ram, but the next gen 10xxx units are supposed to be am5 still so I could skip the 9xxx cpus.  I live a couple hours from a microcenter soooo bundles near me exist but idk",hardware,2026-01-03 00:53:33,2
AMD,nx9hr0w,"I upgraded my AM4 AB350 system from first gen Ryzen 1600 to 5700X3D and the gaming results lined up with the reviews. But these chips falter with high RT usage games that have even higher demands on the CPU and RAM.      For example, in Stalker2 CPU-limited scenario, 9800X3D was almost 2.5X of its performance. I doubt that non X3D would be more than 20-30% slower, so it'd be better to get AM5 instead.",hardware,2026-01-02 15:08:02,3
AMD,nx8lzup,In the UK the 14600kf is the same price as a 5700x and cheaper than a 5800x. Why do they always compare to a 12400f.   I’ve also never seen a 12400f perform this poorly other than when it’s these guys testing. They always try to make AMD look as good as they possibly can. If they compared to a 14600k the results would look very different indeed.,hardware,2026-01-02 11:41:52,8
AMD,nx9svcl,People are still believing the windows update boost!?,hardware,2026-01-02 16:02:39,2
AMD,nx9lyjz,So what would be the best choice of GPU for a 5700X on 1440p without bottlenecking it too much? RTX 5070 or RX 9070?,hardware,2026-01-02 15:29:24,1
AMD,nxc0owc,The 5600x is amazing. I kick myself in the head for selling mine when I upgraded to 9800x3d.. ended up building pc for my kid months later and had to use an i5-8500 that got recycled at work.,hardware,2026-01-02 22:22:35,1
AMD,nxf6nlm,"I'm so glad that I got a 5700X3D upgrade back in July, runs everything i throw at it without breaking a sweat. Managed to bag it for £200 with a discount.",hardware,2026-01-03 11:17:15,1
AMD,nxf9kcd,More proof that 6 cores is all you need for gaming,hardware,2026-01-03 11:41:29,1
AMD,nxlho64,"And we see also that the Intel Core Ultra 5 225F performs very well in game, at least they tested it",hardware,2026-01-04 08:59:51,1
AMD,nxlo5ia,I went from a 2600x to a 5600x. Nice upgrade. Same motherboard.  I'm happy for the foreseeable future.,hardware,2026-01-04 09:58:59,1
AMD,nxloqt6,what is fine wine with this test? runs as expected. alderlake espeically the locked ones with small cashe always performed on par with zen3 with slower ram.  am I slow(yes I am :P) but what is the specs of the ddr4/ddr5 ram used? remember quality ddr4 stick is extremely hard to get and very expensive too if u want b-die. is ths 3200c14 or 3600c14 or what is available today which is basically a step above jedec like 18-22-22-whatever..,hardware,2026-01-04 10:04:14,1
AMD,nxa9e7t,"I came very close to getting a 5700X or a 5800X. If I had known that the 5800XT would be *$200* right now, I would have done that instead of going full AM5. Not that I regret going AM5 exactly, especially with all the nonsense around RAM going on.  If you're on AM4 right now, get a 5800XT or something, they're crazy cheap.",hardware,2026-01-02 17:19:59,2
AMD,nxaeljd,"Thanks for the test. Imma stay on my 5800X3D till AM6 lol.  TBH, that upgrade from the 2700X to the 5800X3D was the best in-socket upgrade for me since Slot 1 (Pentium II 266 -> Celeron 950 with a PGA370-to-Slot 1 adapter).",hardware,2026-01-02 17:44:25,1
AMD,nxbkxzn,"Daily reminder that at the settings you actually game at, 1440p high, that you will be GPU limited on your 60 series Nvidia GPU on a 5600X.   If you have a 3600X it will be better to upgrade your GPU than going to a 5600X.  We are all probably GPU limited still.",hardware,2026-01-02 21:05:14,0
AMD,nx94ol7,Another garbage video from Hardware Unboxed 🤢🤢🤢  They gonna do everything to lower Intel cpus performance.,hardware,2026-01-02 13:54:47,-12
AMD,nx8r36h,What about 5950X to 9800X3D for high refresh gaming?,hardware,2026-01-02 12:23:21,-7
AMD,nx8rkqk,annoying they only did 1080p comparison,hardware,2026-01-02 12:27:08,-25
AMD,nx92l56,"Same, which was always the plan.  Still regret not switching to a 5700X3D when that was at its low, but 5800X is doing just fine for now.",hardware,2026-01-02 13:42:20,45
AMD,nx8mhum,"same, i want to upgrade but my old bastard ryzen 5 2600x still handle new games pretty well. so i guess waiting for DDR6 is better choice.",hardware,2026-01-02 11:46:03,13
AMD,nx9op07,"Last year I did a drop in upgrade, replacing my 3600X with a 5700X3D, and my 5700XT with a used 7800XT.  It plays 99.9% of titles at more than good enough settings.",hardware,2026-01-02 15:42:46,6
AMD,nx8t9b5,"Or until the cyclical DRAM market implodes again, and you can pick up cheap DDR5 sticks. It sucks how you have to time PC builds based on the DRAM cycle.",hardware,2026-01-02 12:40:11,13
AMD,nxjddu6,"i have a 5800x and just got a 5070ti and can play kingdom come 2 in 4k, everything maxed out with dlss turned off and get 80 fps.  the 3d chip would be nice to have as my performance in marvel rivals is basically the same as it was with a 3080.  but i’m not paying $500 for a used 5800x3d",hardware,2026-01-04 00:29:14,3
AMD,nxagjy2,"I also have the 5800X. Was really tempted to get the 5800X3D for a few times since then, but it's feels like a slight upgrade at most.  Though this is the first time I heard of an 5800XT I believe.",hardware,2026-01-02 17:53:26,1
AMD,nxawt2b,Same. My 5800X3D currently scales to GPUs I can't afford. By the time I can afford a 5090-class GPU that would benefit from a CPU upgrade at 4K I expect AM6 will be on its second generation.,hardware,2026-01-02 19:08:07,1
AMD,nxcey7i,2030 is a long time     ive heard many people say they're waiting for ddr6 but we'll see if you can wait for another 4 years,hardware,2026-01-02 23:38:33,1
AMD,nxaeb3i,This is the way.,hardware,2026-01-02 17:43:03,0
AMD,nxk8qw2,"When did they say they were using CUDIMMs? I didn't think anybody was using CUDIMMs outside of a few overclockers and Intel-sponsored experiments.  **Edit:** ah, 11:05. Thanks, AI.",hardware,2026-01-04 03:22:31,0
AMD,nxad72i,Yes! I'm still running my AX370 since 2017.,hardware,2026-01-02 17:37:50,6
AMD,nx8kdt1,Offset by anti-repair policies. The economy needs you to consoom,hardware,2026-01-02 11:27:54,23
AMD,nx8ycqn,Because people are too lazy to write Radeon or Ryzen.,hardware,2026-01-02 13:15:51,2
AMD,nx8vruk,"That’s an insanely good price, you also got AM5 so if prices calm down you can easily upgrade to an X3D chip.   If you held out on a larger upgrade, you probably would have lost access to a good upgrade path like you have now.",hardware,2026-01-02 12:58:19,27
AMD,nx9513g,"It's a solid upgrade, the only drawback is that just 16 GB of RAM might become a problem in the future.",hardware,2026-01-02 13:56:51,15
AMD,nx8teb4,"Its obviously a big advantage to be on the new platform, especially for a good price before things deteriorate even further.",hardware,2026-01-02 12:41:14,5
AMD,nx8u4xi,Do you feel like you made a mistake? Is the increased performance worth 51 USD in your eyes? That's the real question you should be asking.,hardware,2026-01-02 12:46:44,6
AMD,nx92dzb,"No, you didn't make a mistake even with the ram downgrade, as long as your games don't suffer from 16gb.   You'll be able to offload the ram and buy 32gb when prices stabilize and you'll be in current gen.  AM4 is great for those who don't want to move. Nothing wrong with doing that as well",hardware,2026-01-02 13:41:07,3
AMD,nx9raxp,"8GB sticks are a performance downgrade from 16GB because they have half the amount of banks so will have less throughput, I wish someone would benchmark that.",hardware,2026-01-02 15:55:13,3
AMD,nx8h3ux,5600x must have left the 11600k in the dust as well,hardware,2026-01-02 10:58:40,28
AMD,nxavtms,"There was a big dip around 2023 when you could get a new 5800X3D for $290, though back then you could get 64GB DDR5 for like $150 as well during the DRAM oversupply, so it made no sense to go AM4 for a new build. And as recently as August 2025 you could get a 5700X3D for $200. Now, with sky-high DDR5 prices and end-of-production it makes sense that prices have gone way up.",hardware,2026-01-02 19:03:29,4
AMD,nx9c0hn,"It makes sense they stopped production as it essentially cut production capacity away from more profitable current gen X3D CPUs. They are rarely found on the second market because who would do away with an X3D AM4 CPU without also getting rid of the rest?  Perhaps if AM5 CPU sales go down due to prolonged RAM pricing they might actually put it back into production, though I doubt it'd be significantly cheaper - if at all - than the original MSRP.",hardware,2026-01-02 14:37:07,7
AMD,nxbzn3y,"I think what's really saved Zen 3 especially is that most games today get built to run at 60fps on consoles with their slightly hobbled Zen 2 CPU's.  Even taking into account console optimizations, it still generally gives Zen 3 enough headroom to play all the same games at 60fps+ on PC as well.  In a world where devs were utilizing the CPU's in the consoles to push more in other areas than just framerate and 30fps was standard again, I think Zen 3 would have struggled a lot more.  I'd honestly found it a bit of a shame that this didn't happen for a long time cuz I wanted to see devs be more ambitious, but obviously it's a saving grace for many these days.",hardware,2026-01-02 22:17:12,3
AMD,nx8u25g,Bought a 5800x3d a few years back.. rocking a really old crosshair 6 motherboard (2017)… Just upgraded my 1080ti to a 5080 over Xmas… and now I cannot see a single reason why I would need to change either the CPU or the motherboard in the next five years.,hardware,2026-01-02 12:46:09,5
AMD,nx8tbtw,355 is crazy you can get a 14700k and still use DDR4 for that much.,hardware,2026-01-02 12:40:43,23
AMD,nx8vp63,"if you manage to find one, that is",hardware,2026-01-02 12:57:47,3
AMD,nxbxup2,Seems a very steep upgrade just to have last generation performance and no further upgrade potential.,hardware,2026-01-02 22:08:05,2
AMD,nx8mo0w,That's the path I went down for my final AM4 upgrade. I went from a 3800X to a 5700X to a 5700X 3d.,hardware,2026-01-02 11:47:29,1
AMD,nxa2pnw,When it comes to the CPU I definitely care more about non gaming than gaming.  It isn't a console.,hardware,2026-01-02 16:48:44,7
AMD,nxa6au1,That is user specific. Some people will keep a browser open with 50 tabs while playing a game. I myself close everything when I game other than maybe discord if I need to chat while playing but I know not everyone does it how I do.,hardware,2026-01-02 17:05:28,8
AMD,nxa6fej,Yeah at least 5700x will be an ideal pick,hardware,2026-01-02 17:06:04,3
AMD,nxbyi2u,">The dumb reviewers never take into account that while gaming you will have other apps open and in use.  Nah, I pretty much never do.  At most I'll have like a single Chrome page open for a guide or something, but even then I usually just use my phone instead.    Also, if you're using Exclusive Fullscreen, Windows does a pretty good job of making background processes irrelevant to performance.",hardware,2026-01-02 22:11:22,2
AMD,nx9npyn,Maybe that chipset is too old it doesn't even support pci gen 4,hardware,2026-01-02 15:38:06,3
AMD,nxa5x09,The 5700X3D is a good chip but its reduced clock speeds even compared to the 5800X3D does hurt its performance.,hardware,2026-01-02 17:03:39,2
AMD,nxc1dz2,"They aren't the same price once you consider that somebody would need to buy DDR5 memory and an LGA1700 motherboard to actually get the most out of it.  They aren't trying to make AMD look as good as they can, ffs.  It's embarrassing seeing this constant conspiracy crap from people who just hate when AMD products genuinely have good light to put on them.  The main point of the video is really just ""Hey, if you're on Zen 3 still, you should probably just keep cool and be happy what you have is still good"".  The point wasn't necessarily some Intel vs AMD battle or telling people what they should be buying right now.  It's literally the main text in the thumbnail telling you this! lol",hardware,2026-01-02 22:26:09,8
AMD,nx8rnxk,You havent seen 12400 perform like this because most reviews were using DDR5 and also latest windows updates as stated on the video fixed ryzen performance   Also here is video comparing 12400 with Ryzen 5500 both using DDR4 . Mind you  5500 is a decent bit slower than 5600  [https://www.youtube.com/watch?v=1i8cs74UC\_4](https://www.youtube.com/watch?v=1i8cs74UC_4)  Also where exactly is 14600k the same price. The cheapest i have seen on UK is 185 pounds at currys where as 5700x is 165 pounds. Granted thats not much of a difference but 5600x which performs almost as good as 5700x is 140,hardware,2026-01-02 12:27:50,17
AMD,nxb23ff,">Why do they always compare to a 12400f.   Yea stock 12400F is kinda meh, If they had overclocked results it would make some sense, cause the reason why 12400F was great buy cause you could ECLK overclock it to 5Ghz+ easily on an asrock b760m pg riptide(some other boards, if you could find them for cheap) so you essentially almost had a 12600k(which curiously missing from the test list..) for cheaper as the locked SA voltage wasn't that big of deal for an alder lake cpu with ddr5.  That being said I'm very surprised that stock DDR4 12400F loses that badly, like how is that gap so big. E: oh apparently it can't do 3600 Gear1 cause of the locked SA voltage so either 3600 G2 or 3200 G1, lol, even more of reason to have -K alder/raptor lake in the list then.",hardware,2026-01-02 19:33:20,3
AMD,nx8qvpv,"It's clearly weird that you only see this result with AMDunboxed. Search everywhere else and you'll find 5600x = 12400F   And yep skipping the regularly sub 200$, sometimes even 150$ (with BF6 code) 14600K in every comparison to make intel look worse is their intention",hardware,2026-01-02 12:21:45,-9
AMD,nxa1lw5,It's not boosting   This is basically performance locked by admin control which is now unlocked  I can provide you different benchmark links as proof,hardware,2026-01-02 16:43:37,14
AMD,nx9o6cb,"What kind of price differences are you seeing?  In addition, it comes down to this:  RTX 5070:  +DLSS and other aspects of Nvidia software dominance. +More robust performance with ray tracing turned on -12GB VRAM is already starting to show difficulties in some AAA now, let alone 2-3 years from now.  9070:  +16GB VRAM with performance that means when games really want more than that, you’ll want a new GPU +FSR4 is finally competitive -And yet is still more a promise than a reality, with few games supporting it -Drivers. AMD still has problematic drivers way more. -Ray tracing is now possible, still not nearly to the extent of Nvidia cards.  In the end, only you can decide which sounds better to you based on price and the facts above.",hardware,2026-01-02 15:40:18,1
AMD,nx95rnz,"But are they still making the X3D CCDs and 1st gen cache dies?   There's alot of extra steps that go into that  Die sanding, hybrid bonding, etc.",hardware,2026-01-02 14:01:12,11
AMD,nx93doc,AMD seems to have a compulsion to continue releasing AM4 CPus so they just might. :P  It truly is the GOAT socket.,hardware,2026-01-02 13:47:04,1
AMD,nx934fc,"When you switch from a 2 gen old workstation CPU to a current gen top of the line X3D gaming CPU, you may indeed find that your framerates increase...",hardware,2026-01-02 13:45:32,13
AMD,nx8voy0,Its to stress cpu properly,hardware,2026-01-02 12:57:45,26
AMD,nx95hz3,"720P would have been nice to see, but 1080P is low enough with modern high end cards to show a difference between CPUs.",hardware,2026-01-02 13:59:37,9
AMD,nx96m96,"somebody [Copponex] needs to have a deep investigation why cpu tests are done at the lowest possible resolutions [at 1080p or lower, which is the correct way] ...   edit: added the subtext.",hardware,2026-01-02 14:06:12,-6
AMD,nx9vpip,"I went from a 5600 to a 5700x3d last year, and it's a change that doesn't SEEM like it would be big just based on the average, but it feels a lot nicer in some titles. Some titles really aren't much faster, but when a game is faster it's often a LOT faster. RDR2 I noticed a huge difference, same with Cyberpunk.",hardware,2026-01-02 16:16:00,15
AMD,nxadgc9,"FR, thought it would be in the market for a good while. 5600 is gonna be here for a few years still.",hardware,2026-01-02 17:39:03,2
AMD,nxddhgl,"5700x3d is still super cheap on ebay, because Chinese price is insanely low.",hardware,2026-01-03 02:54:46,1
AMD,nxkcz59,"Switched my 5900x out for a 5700x3d a year ago when they were cheap, glad I did with all this craziness now.",hardware,2026-01-04 03:46:50,1
AMD,nxbk12h,At high resolutions and settings the X3D chips don't really add anything as you are back to being GPU limited. For the handful of people who play CS at 520p the X3D chips are great.,hardware,2026-01-02 21:00:46,-7
AMD,nx92u8j,You can upgrade to a used 5x00,hardware,2026-01-02 13:43:51,15
AMD,nx9rbcc,I went 3600 to 5800X. Which I got a month before the X3D reveal.  Boo.,hardware,2026-01-02 15:55:17,2
AMD,nxbtc2i,"Eh, this is not part of any 'cycle'.  This is a very unique and pretty much unprecedented(at least in a very long time) kind of DRAM pricing explosion.  One that isn't likely to get corrected anytime soon.",hardware,2026-01-02 21:45:49,7
AMD,nxjimt6,I considered used a 5700X3D but I probably missed the boat and couldn't find out cheap enough.  This was pre-DRAM apocalypse too.  But I have a 7800 XT so it's less of a meaningful upgrade.,hardware,2026-01-04 00:57:12,1
AMD,nxie789,Looks like Crucial's damage control bots came for me.,hardware,2026-01-03 21:29:34,1
AMD,nx8kqrl,"I know and you're correct. There's s definite push to force obsolescence in PC, but also in other industries",hardware,2026-01-02 11:31:03,13
AMD,nx9me28,AMD never should have had the same naming scheme for both CPUs and GPUs to begin with.,hardware,2026-01-02 15:31:31,42
AMD,nx95v4v,"Thanks, my upgrade feels justified now. Can't wait for the RAM to come so I can see the CPU fps gains!!",hardware,2026-01-02 14:01:47,3
AMD,nx95qgb,"Thanks, I am not a streamer or anything. At most I have discord running in the background when playing competitive games which are well optimized and wont be needing 32gb ram anytime soon.   When I play singleplayer games I play on Fullscreen and have nothing in the background and 16GB has never been an issue for me. Maybe next gen consoles will make 32gb mandatory but that's a ways off and hopefully by then ram prices will be normal again then I can upgrade to 32gb",hardware,2026-01-02 14:01:00,5
AMD,nx960ov,Thanks! I dont regret the upgrade. Looking forward to seeing the cpu gains,hardware,2026-01-02 14:02:41,1
AMD,nxekss8,"Waiting for the parts to be delivered. I don't think it'll be a mistake. I was CPU limited in the 3 games I have been playing. GTA 4 path traced, CP2077 path traced DLSS B, Lords of the fallen, even at DLAA 1440p if I remember correctly.",hardware,2026-01-03 08:09:33,2
AMD,nx9s50e,"Hardware unboxed did a 2×4gb/8gb/16gb/32GB video recently on Steve's own pc with a 3 year old install of windows 11, many chrome tabs open and discord open in the background.  Performance between 2×8gb vs 2×16gb is exactly the same in all but 1 game tested but if they closed the background tasks 16gb would be exactly the same at 32gb.  https://youtu.be/Bj5v52R4qnk?si=-UkOP5n9myB6Poxz",hardware,2026-01-02 15:59:10,7
AMD,nx8hj4c,Since it's matching 12400f ddr5  It should be faster than 12600k as well,hardware,2026-01-02 11:02:30,4
AMD,nxo239y,iirc it was fairly competitive between the two back in the day.,hardware,2026-01-04 18:27:31,1
AMD,nxnzopz,I'm kicking myself for not upgrading mine and my partner's computers to 5700x3d when Microcenter had them for $175 in late 2024.,hardware,2026-01-04 18:17:06,1
AMD,nxgqe93,zen 3 x3d will still be performant even if zen 3 cpu is struggling provided dev really take advantage of the cpu or being ambitious in game design correct?,hardware,2026-01-03 16:47:47,2
AMD,nx8wik5,"you're missing the point a bit. you have to take into account two factors:  * it's one of the best upgrade paths if you want to stick to AM4.  * AMD isn't producing 5700x3D anymore so price keeps rising as it's progressively harder to find  totally agree current prices are absurd and heavily scalped but only because some are willing to pay. I got mine one year ago to upgrade my previous CPU (5600x), and paid less than 200€ for it. in hindsight it was a great purchase.",hardware,2026-01-02 13:03:27,13
AMD,nx8u6nz,I'm not saying I would do it. But its there. Some people on 3700x want to keep their mobos and maybe save a bit,hardware,2026-01-02 12:47:06,3
AMD,nx9jir3,I think 14700K with DDR4 is actually slower than 5700X3D in gaming though.,hardware,2026-01-02 15:17:11,3
AMD,nxbv85t,Used on eBay is where to find it now,hardware,2026-01-02 21:55:03,1
AMD,nxc7umx,If it isn’t that then don’t compare them to a crappy 12400f when a 14600k is in the same price range.,hardware,2026-01-02 22:59:47,1
AMD,nxb2dh1,>Also where exactly is 14600k the same price  There was a period in the summer/late summer where they were 150€/$ for month or two,hardware,2026-01-02 19:34:40,6
AMD,nxce63r,"> Also where exactly is 14600k the same price   idk about other places, but in canada, the 14600k has had sales several times over the past few months at $230-240CAD https://ca.pcpartpicker.com/product/jXFmP6/intel-core-i5-14600k-35-ghz-14-core-processor-bx8071514600k  5700X's around $220CAD for the same period https://ca.pcpartpicker.com/product/JmhFf7/amd-ryzen-7-5700x-34-ghz-8-core-processor-100-100000926wof",hardware,2026-01-02 23:34:11,3
AMD,nx8shgv,"14600k performs way better than a 5600x tho and is worth the extra 40.   It has gone up a bit since a few months ago, but it’s still worth it over a 5600x or 5700x.",hardware,2026-01-02 12:34:15,5
AMD,nx8vh4c,"Personally I'm never buying a 13/14th gen intel chip because of the degradation. Yes, the 14600's are less likely to be effected compared to the higher end chips, but there's still been many reports of it happening. I don't really care how they compare to CPUs I'm never going to buy, as a 12400f comparison is a lot more useful.",hardware,2026-01-02 12:56:14,4
AMD,nx9vcw1,Well I'm assuming those two are the best GPUs to use with this CPU regardless of the price. anything above a 9070 or maybe 9070XT would be wasted GPU power. no?,hardware,2026-01-02 16:14:22,1
AMD,nx9jql6,i understand the technical reason for it. It just doesn't say much about how my 5600x will fair in 1440p.,hardware,2026-01-02 15:18:17,-9
AMD,nxa766x,why is a deep investigation required when it's common sense?,hardware,2026-01-02 17:09:33,10
AMD,nxc2q3z,You're gonna make Aussie Steve's head explode. lol  I dont think people are gonna realize you are being sarcastic.,hardware,2026-01-02 22:33:02,0
AMD,nxa5w8x,"I built my current rig in april of 2018. Picked up a 5800xt to replace a very dated r7 1800x because I wanted higher fps in MH wilds. its on a b350 motherboard. Given the current hardware situation, I may be using an almost 10 year old computer at some point and its still relatively ""high end"". AM4 was a great platform.",hardware,2026-01-02 17:03:32,8
AMD,nxa9v3f,Did your 1% lows get any better across the board? I'm really thinking about doing the same thing,hardware,2026-01-02 17:22:10,3
AMD,nxq9ptz,"I play a whole heap of wow and I wish I bought a 5700x3d, wow loves the x3d chips",hardware,2026-01-05 00:39:27,1
AMD,nxbk7o7,It probably didn't change anything as you are still GPU limited.,hardware,2026-01-02 21:01:40,-3
AMD,nxe90fl,"Nah, 5700X3D has been out of stock on Aliexpress for a while now, almost a year (last time they were on sale was November 2024 for Single's Day, ~140 USD). If you're stateside, that's probably because your market has the 5600X3D so there's less of a demand overall.  There's been a few sellers with stocks recently, but they're 300+ USD. Over double of what they were selling for a year ago.",hardware,2026-01-03 06:29:42,1
AMD,nxbkhsi,"In my region used CPU's sell for more than just buying a new one off of AliExpress, used prices are stupid.",hardware,2026-01-02 21:03:03,3
AMD,nx9uxdc,i use to think for an upgrade but i'm holding for now cause i just dont think its worth current situation. thats why i'm just gonna wait for DDR6.,hardware,2026-01-02 16:12:20,0
AMD,nxe2b7q,Stocks like Oracle which are proxies for OpenAI are already crashing hard. Investors are being heavily skeptical of the AI DC buildout.   I give it six months tops.,hardware,2026-01-03 05:37:16,3
AMD,nxa0tn5,Also true.,hardware,2026-01-02 16:39:59,5
AMD,nxd726d,"Well you can't blame them, intel didn't have gpu:s when they made zen, so they couldn't copy their naming to follow up rdna, like they did for cpu:s and motherboards. The cpu isn't too bad sure at least they started at 1000, but the mortherboard naming was so blatant to have X370 and X399 when intel is on Z270 and X299...",hardware,2026-01-03 02:17:17,1
AMD,nxbvbmt,"There ARE games that can still be a problem with just 16GB of RAM, single player or whatever.  Not a ton, but they absolutely exist.  And the crappy thing about it is that there's usually not a lot of ways to reduce RAM usage via options or whatever.  Often it's just a case of you either have the required RAM or you dont.  Maybe you can try and play with the Page File to help with stability issues or whatever, but in terms of performance issues, you're usually out of luck.    I also considered going with 16GB for now to save some money and then upgrade 2-3 years down the line, but I got lucky with a very good(relative) deal on a high performance 32GB kit the other day so figured I'd just spend the extra now and not worry about it again.  Plus I do play games like city builders and whatnot that can be memory hogs.",hardware,2026-01-02 21:55:31,2
AMD,nxa9htg,My main recommendation would be to debloat Windows down to the bone.,hardware,2026-01-02 17:20:27,0
AMD,nxbs01y,This does not test single rank vs dual rank. If you lock down a dual rank kit to half the size in software you still get the dual rank benefits.,hardware,2026-01-02 21:39:20,8
AMD,nxbwlzr,"To be fair, HUB does tends to have a fairly limited suite of games they test that dont tend to be that memory hungry.  It's a decent enough indication if you're mainly a 'mainstream' gamer, but I dont feel it was quite a comprehensive enough test to show where it can get people into trouble.  In fact, I think a video specifically talking about examples where 16GB isn't enough could be useful, not to push everybody into buying 32GB, but purely so people are informed on what types of games people might need to think about this more with.  Also, aren't DDR5 modules kind of 'dual rank' by themselves, in effective terms, compared to before?  Like, the whole single rank vs dual rank thing just doesn't apply as much to DDR5 like it did with DDR3 and DDR4?",hardware,2026-01-02 22:01:53,3
AMD,nx9v533,"Oh thanks haven’t seen that, I’m surprised it’s not noticeably worse performance.  Must be a big difference between server workloads and gaming because I’ve seen over 10% difference with x16 chips vs x8.",hardware,2026-01-02 16:13:21,2
AMD,nx8m7tk,"Yeah, but we all know that it isn’t faster than a 12600k",hardware,2026-01-02 11:43:44,22
AMD,nx90ggy,"I remember my 12700k ddr4 being better at gaming than a 5800x, I wonder how they compare to each other nowadays",hardware,2026-01-02 13:29:15,7
AMD,nx8wutr,"Yeah, below 200 it’s a good deal still.",hardware,2026-01-02 13:05:49,2
AMD,nx8udiw,"Yeah, it’s just not a sensible decision. I got 2 for 140ish last year from AliExpress and that was a banger deal.",hardware,2026-01-02 12:48:29,-1
AMD,nx9jm88,It isn’t.,hardware,2026-01-02 15:17:40,1
AMD,nxbykqk,No it really doesn't.,hardware,2026-01-02 22:11:44,3
AMD,nxbdjy8,"You have some benchmarks for that? The only ""background"" app I've seen that actually reduces performance is watching a video on second screen and even that just reduces gpu bound performance, not cpu bound.  Just because something ""uses resources"" doesn't mean a game will care about it.",hardware,2026-01-02 20:29:00,2
AMD,nxb884s,Discord memory usage isn't to bad on my system about 800mb and I only use it if in game voice isn't available. I also have a dedicated sound card I don't use on board audio never have so no issue with high cpu usage.,hardware,2026-01-02 20:02:50,1
AMD,nxci6gf,"The 7500f, 225f and 12400f are some of the cheapest entry points to their respective platforms. The point of the video is to gage AM4 is still a good enough platform vs the entry point of more modern (ddr5) platforms.",hardware,2026-01-02 23:56:34,6
AMD,nxa6tpg,"> 14600k performs way better than a 5600x tho     less so when limited to DDR4, yeah it's gonna be a bit better, but it really needs the DDR5 to achieve anything of much significance.    you're not going to see anything besides like a 5-10% improvement at most in games when you're stuck on ddr4",hardware,2026-01-02 17:07:56,5
AMD,nx988n5,That's amazing that Intel was able to release a 5600x killer three years later.,hardware,2026-01-02 14:15:37,1
AMD,nx9uxa9,"I have seen way more 9800x3ds fail than 14600k  If anything, the 12600k exist and went for as low as 100 dollars  But lets be real, you wouldnt have buy any intel cpu so why does the shitty 12400f need to be compared at all",hardware,2026-01-02 16:12:20,-1
AMD,nx8yd1b,Just set a power and voltage limit and they won’t degrade faster than any other cpu.,hardware,2026-01-02 13:15:54,-4
AMD,nxb4iba,"The real frustrating answer is “depends.”  The processor is your ceiling. If 80+ frames is what your processor can crank out for a game on settings you choose, then that’s all she wrote.  But wanna crank up GPU killing settings that don’t affect CPU performance? Then the more GPU the better.  The big wrinkle is that ray tracing tends to be heavy and both CPU and GPU due to aspects of how it’s rendered.  But to go back to a counter example, Counterstrike 2 would likely shrug at cranking up to 4K and heavy graphics pairing that processor with a GTX 5080 or similar.",hardware,2026-01-02 19:44:54,3
AMD,nx9pu1z,"WTF are you talking about? Resolution has zero effect on CPU performance, your 5600X will perform at 1440p the exact same it will perform at 1080p.  CPU tests are done at low resolutions only to ensure GPU performance isn't a factor that can interfere with the test.",hardware,2026-01-02 15:48:14,13
AMD,nx9kgqo,"Depends on your gpu, settings, games you play. You're the only one who can answer your real world experience.",hardware,2026-01-02 15:21:56,8
AMD,nxaklvc,"obviously it's not as common as its name implies, or we wouldn't have geniuses demanding useless benchmarks where all numbers are the same (why not go 4k with a gt730, when we're already making dumb requests?) and wayyyyy below the cpu's capabilites - which is the point of these tests; figuring out what they can deliver when they are not limited by other components.",hardware,2026-01-02 18:12:05,0
AMD,nxcrzh3,"maybe it's an ESL issue, but what do you mean by me being sarcastic? i'm saying anything above 1080p is useless for a cpu comparisson, which is exactly what steve is in agreement with and which contrasts the previous posters implied wish for 1440p/4k benchmarks.",hardware,2026-01-03 00:50:19,0
AMD,nxaglxf,"I'm also rocking a b350 board haha, it's getting up there in the years but hey it's still trucking. 5800xt is a great pick, hard to go wrong now that the X3D prices are sky high.",hardware,2026-01-02 17:53:41,2
AMD,nxapcht,"Yes, frame time consistency is the X3D's special sauce. FPS averages don't tell the whole story. IMO there's no going back, I'm a believer and my next CPU will be another X3D for sure.",hardware,2026-01-02 18:33:35,4
AMD,nxagwy4,"yes they did improve in most cases! I'd imagine a lot of that is just from the averages being higher too, but I'll take what I can get. At this point you probably won't be able to get an X3D 5000 series chip for cheap so it might not be worth it, but if you're on a 3600 or earlier or something, you could grab a 5700x or a 5800XT for a smaller but still fairly nice boost.",hardware,2026-01-02 17:55:06,3
AMD,nxbpjuf,It's a good thing I'm not talking about cases where I was GPU then.,hardware,2026-01-02 21:27:28,5
AMD,nx9xs1p,Zen 3 single-CCD chips (anything at/below 8-cores) removed the micro-stutter that affects all Zen & Zen 2 chips.,hardware,2026-01-02 16:25:45,8
AMD,nxaaria,"The 2600x is slow as shit. Just get something 5th gen, it'll be way faster and won't have the stutter.",hardware,2026-01-02 17:26:21,4
AMD,nxeh7ad,"Thanks, do you know which games are unplayable with 16gb? Probably Real Time Strategy games and city builders and Microsoft Flight Sim. I don't play any of those, not my type.   Unfortunately I 100% couldn't afford 32GB. I kind of upgraded just for the sake of upgrading.  5700X, B450, 2×16gb 3200mhz  to Ryzen 7 7700, B650, 2×8GB 5600mhz. The upgrade cost me 51 USD. My budget for upgrading was 50 USD😅  I bought the ram for 90 USD which in my country is a good deal. From what I've researched as long as you don't have anything unnecessary in the background 16GB is still enough for AAA gaming.",hardware,2026-01-03 07:38:48,1
AMD,nxaabjj,I don't think that'll be necessary. I mean I've been gaming with 16GB since 2019. I've only been on 32GB (ddr4) for the last few months.   At the moment 16GB is sufficient for gaming.,hardware,2026-01-02 17:24:18,2
AMD,nxb9y3m,Not even a little necessary with 16gb of ram,hardware,2026-01-02 20:11:13,2
AMD,nxeg8vl,"Yeah, I was originally planning on getting 1×16gb and getting another stick later on but I'm not willing to take the performance hit.  2×8gb is better, I'll have to just resell it for cheap once 2×16gb is affordable.",hardware,2026-01-03 07:30:38,1
AMD,nxehpto,"Real time strategy games, city builders and microsoft flight sim type games need 32gb I'm sure but from what I researched 16gb is enough for AAA gaming. I'm not a streamer and when gaming I don't have anything in the background unless I'm playing competitive multi-player games which are very well optimized and dont need 32gb even with discord running in the background.  I waa considering 1×16gb now then another later but later is potentially months to 2 years and I saw that single channel still leaves performance on the table so I went with 2×8gb.   Hardware unboxed tested 1×16gb here :  https://youtu.be/_nMu1KFkOC4?si=ohvJoY9Kq2_2GgTH",hardware,2026-01-03 07:43:12,1
AMD,nx9xl36,https://youtu.be/_nMu1KFkOC4?si=hYbSdY51rfnrJBc3  I was originally planning on getting 1×16gb then getting another down the line but here performance is actually worse unlike the 2×8gb configuration.,hardware,2026-01-02 16:24:51,1
AMD,nx91kvy,As per HUB conclusion because of windows latest update amd cpu performance improved significantly while intel remains the same   It should be the same as 12400f ddr5,hardware,2026-01-02 13:36:14,9
AMD,nx9slu5,Still faster,hardware,2026-01-02 16:01:25,2
AMD,nx8ym9y,indeed. but you won't find one at that price.,hardware,2026-01-02 13:17:34,9
AMD,nx9keze,Can you find newer benchmarks after the Windows updates that boosted AMD performance? If we take the hit 12400 took from DDR4 and apply that to the 14700k it seems like it would end up slower but I don't know how much 14000 series is affected.,hardware,2026-01-02 15:21:41,6
AMD,nx9add0,Are you stupid? The 12600k released a year later.,hardware,2026-01-02 14:27:45,2
AMD,nx8zd8p,"But that'll reduce performance, and at that point I'd be better off with the comparable AMD chip.  Also, it's just not worth the headache. I've had power limits disable themselves seemingly at random before, and I'd rather not have to re-do them after finding out the limits haven't been enabled for the past 6 months.",hardware,2026-01-02 13:22:21,8
AMD,nxby348,"I don't like RT nor care for it, I'm trying to play games at 1440p 100FPS or more and I'm willing to use FSR/DLSS to reach that. I mostly play single player games and rarely ever Esport or PVP games(other than CS2) and some PVE games. Here's a list of games I want to play and ONLY because of these games I'm upgrading:  Stalker 2   Space Marine 2   Dark Tide   Doom the Dark Ages   Resident Evil 8, 4 Remake, Requiem   Dead Space 1 Remake   Dead Island 2   Dying Light 2   Silent Hill 2 and F   Cronos the New Dawn   Atomic Heart   Metro Exodus  Other than these specially Space Marine 2, I have no reason to upgrade...I would have been perfectly fine with my 1080ti because I would be playing indie 2D games or boomer shooters 90% of the time.",hardware,2026-01-02 22:09:16,1
AMD,nxh5xks,"Thinks he wants to know how much slower 5600x vs xxx including gpu bound. But it depends on the gpu, games   For example: [https://imgur.com/a/2yfATKu](https://imgur.com/a/2yfATKu)",hardware,2026-01-03 17:59:31,-1
AMD,nxdh6tj,You:  > somebody needs to have a deep investigation why cpu tests are done at the lowest possible resolutions...  Also you:  > which is the point of these tests; figuring out what they can deliver when they are not limited by other components.  So apparently you figured it out after all.  Or am I overestimating your ability to understand you yourself wrote?,hardware,2026-01-03 03:17:11,3
AMD,nxrgj1n,Same here. I done almost the same last year except it was from a 5600x & I was going to use it in my 2nd PC to bring that up to speed but with the prices of the 57x3d atm I was considering just going back to that but those gains really do make a big difference & it'll likely shaft me for holding off until AM6.,hardware,2026-01-05 04:33:50,1
AMD,nxgg5oe,">Thanks, do you know which games are unplayable with 16gb? Probably Real Time Strategy games and city builders and Microsoft Flight Sim. I don't play any of those, not my type.  Yea, mostly those types of games, exactly.  Plus some of the big factory/simulator type games and whatnot.   And even if you do like those sorts of games, there are plenty of good ones where 16GB will still be fine anyways!",hardware,2026-01-03 15:59:16,2
AMD,nxehhb3,"That would've been even worse because it's single channel, not just single rank, which also halves bandwidth.",hardware,2026-01-03 07:41:10,1
AMD,nxb5wyy,">It should be the same as 12400f ddr5  Huh?  I mean same as an overclocked 12400F, sure, a stock 12700K is probably the same, couple of extra cores and slower speed.   But a stock 12400F is 4Ghz not 5Ghz+, it's slooow.",hardware,2026-01-02 19:51:40,3
AMD,nxbq93g,Delusional,hardware,2026-01-02 21:30:49,3
AMD,nxawdva,Got extremely downvoted for [suggesting that we need a new DDR4 shootout revisit](https://www.reddit.com/r/hardware/comments/1pwosq3/comment/nw5h0qr/). It would be really helpful to know what the fastest DDR4 CPU in 2026 actually is.,hardware,2026-01-02 19:06:06,9
AMD,nxd5yy0,"All I really found was [this comment on a reddit thread](https://www.reddit.com/r/discordapp/comments/1dqf5h7/does_discord_actually_drop_performance_in_games/nxcu0fp/), which talks about gpu, not cpu impact, also it's recording... so not really ""background"" and you can just disable it.   Some other random forum stufff ppl saying, not showing, that on a potato laptop it might affect things, which sure maybe some old low power dual/quad core might have issues, but tha's not the topic it's 5600 vs 5900x. Some talk about the overlay being bad, with no data, just disable it then whatever. Hence why I asked benchmarks cause ppl be saying all kinds of things, but no1 is showing anything about how having more cores fixes things let alone more cores of the same architecture.  Also i don't remember when it was, but discord did have brief bug where it chopped off -200 effective memory clock from gpu:s, but that was fixed real quick, idk if that affected anything else back then.  However there apparently is hub benchmark about stuff like this:https://www.youtube.com/watch?v=Nd9-OtzzFxs  Sure it's 3.5 years old and doesn't have a dual ccd cpu, only 8 vs 6 cores, but the performance hit on both is the roughly same and miniscule at that. I wouldn't be too surprised if the youtube video is responsible for nearly all of that small perf drop, even when cpu bound, which sure i thought cpu bound there would be basically 0 impact. The one 4k youtube video test results lowering the 5700x result more at 1080p is a bit weird and not lowering at 1440p so seemingly not gpu bottlenecked, idk what's going on in there.  E: noo they deleted their comments and the one after this one, god dammit i had nice one lined up  Well for any1 curious I did manage to drop factorio performance a bit with two 4k youtube videos up, [by whopping 5%](https://imgur.com/a/KO9Uev8) on 10k spm, and nothing on [50k spm one](https://imgur.com/a/sm9RgLl)(the final 1000 updates is higher cause i wiggled an 8khz mouse for ~half the duration) but I doubt ""more cores"" would make the drop less.  Which was their original point btw: ""I have went from 5600 to 5900x and the difference gaming with other apps open is night and day.""  I guess they realized that it's bullshit, good for them!",hardware,2026-01-03 02:10:54,3
AMD,nx9bwps,Oh I thought you said 14600k in the post I responded to. Maybe you haven't edited it yet.,hardware,2026-01-02 14:36:31,7
AMD,nx90wxi,"Raptor Lake doesn’t really consume that much power in gaming. It’s in productivity that the power consumption is high. It’s the same as overclocking a Ryzen to use 150w in gaming, it performs the same as at 65w.",hardware,2026-01-02 13:32:07,11
AMD,nx8zzx1,"No, it doesn’t reduce anything. 14600k can easily run at 5.5ghz Pcores under 1.3V.  Sounds like a skill issue on your part, if you set it in the bios it won’t ever reset.",hardware,2026-01-02 13:26:21,1
AMD,nxh9t1z,"That's not how it works. If you're GPU limited, what CPU you have doesn't matter, they will all perform within margin of error of each other. It would also not answer his question of ""how would my 5600X fair at 1440p"", because if you're GPU limited then you can't really see the capabilities of your CPU to begin with.  The only way to benchmark CPUs that gives you useful information is by removing GPU limitations entirely.",hardware,2026-01-03 18:16:51,3
AMD,nxdj91e,explain to me how you read what i wrote,hardware,2026-01-03 03:29:49,0
AMD,nxb8l19,"Take the DDR4 vs DDR5 performance difference on the i5-12400F and compare it with the i5-12400F vs i7-12700K (both on DDR4) benchmarks — you’ll notice the percentage uplift is very similar.  This clearly shows that performance scaling is not just about core count or clock speed.  If raw GHz or more cores were the only factors, then many older-generation CPUs—with higher clocks and more cores—should still perform on par today. But they don’t.",hardware,2026-01-02 20:04:34,5
AMD,nxazu2k,"I just checked your comment... No, you didn't suggest it, you demanded it while dismissing the work he does to do that sort of reporting as useless content just because you didn't enjoy it. I imagine that's why you got downvoted, not because people didn't want to see DDR4 retesting, I think many people want to see it.",hardware,2026-01-02 19:22:26,-1
AMD,nxkalie,"If you set a voltage limit that limits the peak boost clock, which will affect performance.  If you set a power limit only which doesn't get hit in gaming, you're not doing anything to reduce degradation (unless your CPU otherwise spends a lot of time under high-power multithreaded loads).",hardware,2026-01-04 03:33:11,1
AMD,nx95ckg,It's needless hoops to jump through in an attempt to justify buying faulty hardware.  Raptorlake should be avoided like the plague.,hardware,2026-01-02 13:58:44,5
AMD,nxhbvfq,"He's not interested in the capabilities of his cpu. He simply wants to know currently with my setup and near future, if I game at 1440p how much performance will I lose out with 5600x compared to xyz.",hardware,2026-01-03 18:26:10,-1
AMD,nxgebqf,In plain English.  Apparently you thought you were saying something different from what you actually said.  That's on you.,hardware,2026-01-03 15:50:24,1
AMD,nx95uvm,"I disagree, I find it funny how most people here look down on console users, but then can’t even navigate a simple bios menu and spend 5 mins setting it up.",hardware,2026-01-02 14:01:44,5
AMD,nxhgysh,"> He's not interested in the capabilities of his cpu.  He literally wrote ""how will my 5600X fair (sic) at 1440p"".  The answer is, it will fare exactly the same as it will at 1080p, because resolution doesn't affect CPU performance.  >currently with my setup and near future, if I game at 1440p how much performance will I lose out with 5600x compared to xyz.  This question doesn't make sense. There's nothing a tech reviewer can do that answers it for him. If reviewers turn up the resolution in the CPU tests, they are either going to get the exact same results as 1080p (if it's still not GPU-limited), or completely fail to measure CPU performance because the test became GPU-limited.  The only useful information you can obtain out of reviews is how good each CPU is when not GPU-bound (in which case resolution doesn't matter because it doesn't affect CPU performance), and how good each GPU is when not CPU-bound.",hardware,2026-01-03 18:49:11,4
AMD,nxhgics,"We dont even know the refresh rate, the games & future plans, settings dlss/fg, rt, low-med-high? How much is he willing to stomach for lets say 20% uplift in these specific cpu bound games?",hardware,2026-01-03 18:47:10,1
AMD,nxgmy83,"> somebody [Copponex] needs to have a deep investigation why cpu tests are done at the lowest possible resolutions [at 1080p or lower, which is the correct way] ...  subtext is hard",hardware,2026-01-03 16:31:39,1
AMD,nx9vepf,"People here look down on console and prebuild buyers but refuses to tune anything and drinks the HUB koolaid, jUst wOrk eXtra hOurs to buy the faster cpu",hardware,2026-01-02 16:14:36,6
AMD,nx9761o,"Shouldn't need to, should perform as expected out of the box without having to prevent it from killing itself.   I have no issues fucking about in the bios, but the chip shouldn't be configured out the box to degrade over time.",hardware,2026-01-02 14:09:25,2
AMD,nxhindo,It will not fare exactly the same as it will at 1080p and 1440p.  That depends on the gpu & the game selection & settings  i.e Theres a massive difference between 5600x and 9800x3d with 5090 at 1440p and 4k.   Yes he wont get his use case from reviews. He should outline his perimeters and look around from different outlets. Techpowerup is pretty good,hardware,2026-01-03 18:56:48,-1
AMD,nxhfl9c,Your comment read as you agreeing with the complaint about testing at low resolutions.  What you thought you were writing is not a reasonable reading of what you actually wrote.,hardware,2026-01-03 18:43:04,1
AMD,nxa5any,"Yeah, it’s a joke. They look down on people who want to get the most out of their hardware.",hardware,2026-01-02 17:00:44,3
AMD,nx9c05s,"I never said it should, it’s still an easy fix that takes 5 mins if that.",hardware,2026-01-02 14:37:04,5
AMD,nxhktho,"> It will not fare exactly the same as it will at 1080p and 1440p.  It will. Resolution has zero effect on CPU performance.  >That depends on the gpu & the game selection & settings  The GPU is selected to be one that will eliminate GPU-bound scenarios, hence this test was done with a RTX 5090. So no, it will not depend on the GPU, because the test is designed not to depend on the GPU.  >i.e Theres a massive difference between 5600x and 9800x3d with 5090 at 1440p and 4k.  Yes. You can only see that difference because the GPU used for the test was the 5090. Repeat the test with a 5060 instead, and suddenly the 5600X and 9800X3D appear to perform exactly the same, how curious!",hardware,2026-01-03 19:06:39,3
AMD,nxidgyj,"mind also explaining why it's not self-evident that ""somebody"" refers to coppex, when there's literally in the second half of the sentence the phrase  >investigation why cpu tests are done at the **lowest possible resolutions**  in my native language that's basically a given that it refers to whoever one responded like that.",hardware,2026-01-03 21:26:02,1
AMD,nwwuws4,"> According to the author, you can replicate the AMD B650 Southbridge Expansion Card for approximately 300 yuan (about $42.88). Although the expansion card is not commercially available, OSHWHub has integrated manufacturing services through its sister company, JLCPCB. It allows you to order custom hardware directly using the design files shared on OSHWHub, and JLCPCB will manufacture it.  The important bit, for anyone who'd want to buy one.",hardware,2025-12-31 13:42:23,132
AMD,nwxbyee,Gotta love chinese folks and their freinkenstein stuff. From old HEDT platforms with hacked up consumer chipsets and cheap motherboards as a result (X99/2011-3 being the most famous one) to laptop cpus hacked together to fit into a desktop motherboard and now this.  Maybe next they can start producing cheaper DDR5 sticks from harvested dram ics. That's be cool.,hardware,2025-12-31 15:18:41,60
AMD,nwwqxim,"[The guy also made a video talking about it on bilibili, only in Chinese, but some screenshots might be interesting.](https://b23.tv/BV1AXv4BvE4T)",hardware,2025-12-31 13:17:03,50
AMD,nwwqvuz,Yea. PCI Express has been the standard to connect southbridge chips for a long time. They act like PCIe switches too,hardware,2025-12-31 13:16:45,38
AMD,nwzh34r,"Am I crazy, or is the *outcome* that you basically just have a PCIe switch with a few SATA, M.2, and USB ports attached to it? I guess it's handy in that it bunches a *bunch* of stuff together into a very small PCIe slot. It's PCIe-economical lol  *Very* cool in its own right, but usually you'd just... get a PCIe card for whichever of these you need, right?",hardware,2025-12-31 21:55:35,8
AMD,nwx4gk5,Very cool but why gatekeep firmware behind a chat group signup?,hardware,2025-12-31 14:38:30,29
AMD,nwynznm,"Talking by experience, the really low budget boards will NOT run this for a tiny, simple fact.  Most don't have a pcie x4 slot.  Most cheap and budget boards have one x16 pcie slot for your gpu and - at most - two x1 pcie slots  So those a520, a320, even some b350 don't have pcie x4 slots.  Just pay attention to this, folks.",hardware,2025-12-31 19:18:20,7
AMD,nwwvjf0,I wish this was an option a few days ago. Just ordered a pcie sata card to make up for the lost ports going from a 470x to b850 lost me recently.,hardware,2025-12-31 13:46:15,4
AMD,nwws6ip,Is there a limit to how many times this can be daisy chained?,hardware,2025-12-31 13:25:15,7
AMD,nx10h59,That's actually pretty genius.,hardware,2026-01-01 03:42:46,3
AMD,nwyilo3,"I wonder if this would work on older platforms. It says it requires PCIe 4.0 x4, but shouldn't it just ""downclock"" on older standards since they're backwards compatible? Or does the Promontory chipset just flat out expect 4.0 and shit the bed otherwise?   I have an old Z170 build that isn't worth the effort to sell, and is perfectly usable for what I'm using it for (sitting on my workbench, mostly showing datasheets). I'm stuck on PCIe 3.0, with just one precious M.2 slot. I'd love to expand the I/O and M.2 capabilities, even if the M.2 slots won't run at PCIe 4.0 speeds (like I could give a shit about that).",hardware,2025-12-31 18:50:53,2
AMD,nwzum5e,"I wonder if this would work on a x670? The article specifically mentions b650 boards (lacking a chipset to start with), but could you you drop this into an x670 or x870 to build a small storage server?",hardware,2025-12-31 23:14:14,2
AMD,nwxkx9b,"I would love something like this if it could give me pcie 2 3x8 slots.  Omygosh think of the expansion...  you could have a high speed gpu, high speed nic(25gig+) and hba!",hardware,2025-12-31 16:03:41,1
AMD,nwzbe85,on this - I wonder whats the major differences between this and a pcie switch/bifurcation card? beyond my simplistic understanding of the two,hardware,2025-12-31 21:24:24,1
AMD,nx2hv0j,This is the kind of cool shit AIBs should be making,hardware,2026-01-01 12:06:26,1
AMD,nwx6x02,someone must have forgotten how daisy chaining's bandwidth actually works,hardware,2025-12-31 14:51:55,1
AMD,nwwqipg,"Yeah but I'd like to buy one that works, easily.  Please?",hardware,2025-12-31 13:14:22,-2
AMD,nwx3gqa,You get 1 more nvme. lol,hardware,2025-12-31 14:32:46,-9
AMD,nwwpn52,"Hello narwi! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-31 13:08:32,0
AMD,nwx0hq3,"> Schematics, a parts list, and comprehensive instructions for building the AMD B650 Southbridge Expansion Card are available at no cost. The author notes that you must flash the card with a special firmware for proper functionality. While the firmware is not publicly hosted, you can obtain it for free by joining the creator's QQ group.  Yea this is awesome",hardware,2025-12-31 14:15:42,68
AMD,nwxab8t,"The key is that jlpcb often has minimum orders.   When you tally it all up it might be around $42 dollars per board, but you'll need to order 5 boards and with assembly costs, shipping, etc you'll be into a few hundred dollars.  That's not to say it's not a reasonable proposition, you just need to get together a few people to make it worthwhile.",hardware,2025-12-31 15:10:08,34
AMD,nwyeab4,"They are already recycling ICs to produce ""new"" super cheap out of production stuff like DDR3 sticks. Before the RAM shortage you could even find 8GB DDR3 stick for only 5 buck on Aliexpress which is a pretty crazy deal for countries without a good used market.  Quality and compatibility between sticks can be a bit hit and miss though but it's nothing a TestMem cannot solve.",hardware,2025-12-31 18:28:53,22
AMD,nwzp1j3,My favourite hacked up Chinese things were the Nvidia 30 series mobile GPUs on PCie cards. They even put the 3080M 16GB on a PCIe card.  Nvidia shut that down ever since then though :(,hardware,2025-12-31 22:41:02,13
AMD,nwycvvl,>Maybe they can start producing DDR5 from harvested ICs  They already do. Hell you can buy your own blank DDR5 PCBs and make your own.   Eg: m[.]tb[.]cn/h.76Qe1PW,hardware,2025-12-31 18:21:47,14
AMD,nx3o3yq,Restricting tech items to China forces them to get creative. The US AI policies towards China will likely backfire. They're going to learn to optimize and get results with less.,hardware,2026-01-01 16:47:11,1
AMD,nwx4k98,It's surprising the article didn't report on the video you linked. Simply having AMD's southbridge running on an Intel B660 was mad.,hardware,2025-12-31 14:39:05,16
AMD,nwwxluy,thanks,hardware,2025-12-31 13:58:41,1
AMD,nwx4ad4,"Intel's version, DMI, has some extra proprietary bits glued on even though it's basically PCIe at its core. So this is relatively novel in the modern era.",hardware,2025-12-31 14:37:31,20
AMD,nwzyhxx,"I'm thinking this might be a higher-end, feature-dense, luxury option for people who buy those ~4x NVMe PCIe cards. With this option, you could add a bunch of SATA ports and some additional rear panel io- maybe a soundblaster card?",hardware,2025-12-31 23:37:40,4
AMD,nx371f4,"a pcie4 or 5 switch from PLX costs hundreds of dollars, the old 3.0 stuff is also 2-3x the price it was before 2021, these things cost only the price of a bunch of old recycled motherboards.  ultimately this is just the end result of egregious price gouging.",hardware,2026-01-01 15:13:40,4
AMD,nx0im84,Somehow it also turns one PCIe 4.0 x4 expansion slot - into two PCIe 4.0 x4 expansion slots.,hardware,2026-01-01 01:42:52,1
AMD,nwxom2r,Probably open distribution would attract lawsuits from AMD or something.,hardware,2025-12-31 16:21:54,30
AMD,nwy0ndi,The firmware was made by another person,hardware,2025-12-31 17:21:46,11
AMD,nwyrt6f,firmwares are still often close sourced and not open source it's a form of control over technology. and results in stuff like not sharing firmware.,hardware,2025-12-31 19:38:32,11
AMD,nx0ppcw,"Same shit with Discord and it happens a ton, especially with software that might be on shaky legal ground.",hardware,2026-01-01 02:29:28,3
AMD,nx0x11s,"Finding a 4 or 8x slot isn't too bad on some boards, but if they are only PCI-E 3.0 that's not a lot of bandwidth for all these devices.  Sure, maybe you aren't pushing the USB bandwidth at the same time as an NVME drive, but you might.",hardware,2026-01-01 03:19:20,1
AMD,nwy598a,This probably uses way more power than a standalone SATA controller and card.,hardware,2025-12-31 17:44:32,3
AMD,nwzvm2t,its not commercially available. you have to purchase the board in (small) bulk and possibly solder things,hardware,2025-12-31 23:20:19,3
AMD,nwwtip1,"if you have more than one x4 slot, you should be able to use multiple, depending on software supprt",hardware,2025-12-31 13:33:44,4
AMD,nwyq0f3,"The chipset in this AIC will handle the request from the chipset on the board and send the corresponding reply. Nothing else.\ It won't turn your Z170 into an AMD board.  This should work without a problem. It uses the AMD chipset, which means it won't be cheap, as if you use a Innogrit or ULS ICs for example.\ The AMD chipset wil provide more SATA and M2 ports, as well handling the speeds and connections better.",hardware,2025-12-31 19:28:59,2
AMD,nwzjyql,"I'm guessing old chipsets are much cheaper & more available than switch cards, and your mobo might not support bifurcation.",hardware,2025-12-31 22:11:56,6
AMD,nx2hz7w,"From a technical point of view, there is no difference. The Promontory 21 chip literally is a PCIe switch and a bunch of SATA/USB controllers. (AFAIK it also contains a tiny embedded CPU for the flashback function.)",hardware,2026-01-01 12:07:32,4
AMD,nwzneyc,"Yes, this shares all x4.. so if you have two nvmes, and access each on, get full x4 speed. If you access both at the same time, they get x2.   If it's bifucfcated, you only ever get half.  You also get more stuff, usb sata, etc.",hardware,2025-12-31 22:31:46,1
AMD,nwws2wl,yeah. maybe a bunch of people should get together and do a group order to have these be made and flashed.,hardware,2025-12-31 13:24:36,12
AMD,nwws80z,Until you get volume that isn’t happening unfortunately.,hardware,2025-12-31 13:25:31,7
AMD,nwx6mvw,Try to extrapolate. Try to see potential. Try to see something more than just the obvious,hardware,2025-12-31 14:50:25,2
AMD,nwwrhoz,or someone want to trade X4 slot for more IO,hardware,2025-12-31 13:20:45,6
AMD,nwwrkp5,Sometimes your requirements change over time.,hardware,2025-12-31 13:21:18,4
AMD,nwz634b,All the cool shit is in China,hardware,2025-12-31 20:55:43,32
AMD,nwya65u,"Or, just get the X series boards, get all the features and not have to do it through hardware that you hope works and ends up costing basically the same.",hardware,2025-12-31 18:08:16,20
AMD,nx033rs,"Give it a few months, they'll probably appear on Taobao, then Alibaba, then AliExpress, then eBay at a significant markup.",hardware,2026-01-01 00:05:43,9
AMD,nx1q4on,"Huh I just ordered from them for the first time ever a small custom board so I could create my own air quality device, and it did default me to 5 boards, but it was literally just over $5 for the order with no rush shipping, super cheap for a high quality board.",hardware,2026-01-01 07:15:58,3
AMD,nwz94ew,"Right before the price hike i got a brand new kllisre brand 1x16gb ddr4 stick for eq. of 15 usd, wish i got more lol",hardware,2025-12-31 21:12:09,3
AMD,nxerg1q,I got chinese brand DDR4 LRDIMM 64GB 3200Mhz JEDEC for like around $60 per dimm before the price hike. It's working fine on EPYC 7153 on ASRock Rack mainboard too. Wish I could get more but they are all out of stock now. :(,hardware,2026-01-03 09:07:36,1
AMD,nwyeao1,that's cool. Now you just need to find a source to harvest dram ics from.,hardware,2025-12-31 18:28:56,7
AMD,nx0b2wy,> Hell you can buy your own blank DDR5 PCBs and make your own.   Yes because the empty PCBs are the more expensive part of a stick :P,hardware,2026-01-01 00:54:31,2
AMD,nx2mnnc,pcie switches are pretty standard for ages.,hardware,2026-01-01 12:50:23,7
AMD,nx1eve7,"would need to check all the PCI-E versions of each interface up to the slot that it goes into on the motherboard, ultimately the maximum throughput would be limited by the link between that chipset and the slot on the motherboard it plugs into and all the devices going through the add-in card would have to share that bandwidth.",hardware,2026-01-01 05:33:08,2
AMD,nx08gpl,I'd say it's more likely that the creator just wants to get people to join their group so they can get feedback about whether the board worked and any problems they may have had and solved.,hardware,2026-01-01 00:38:24,9
AMD,nwzc1xj,IP Lawsuits in China? Against IP of a western company? Has anyone heard of such a thing?,hardware,2025-12-31 21:27:55,-3
AMD,nx1wdcl,It was reverse-engineered and dumped from existing BIOS.,hardware,2026-01-01 08:20:26,3
AMD,nwyeos6,"I don't mind much about power usage if it opens up more options. I only have the one pcie1 slot, and finding a board with one and the right number of 3.5 mm jacks for my old 7.1 amp was tricky.",hardware,2025-12-31 18:30:56,7
AMD,nwxx3t6,Which wouldn't be daisy chained but parallel.  You would need a M2 to PCIe slot adapter to daisy chain,hardware,2025-12-31 17:03:56,6
AMD,nwzq8qx,certainly - the plx/pex cards are quite expensive relative to this but do support 8x/16x too,hardware,2025-12-31 22:47:51,1
AMD,nwzqqbh,yes I'm interested from a pcie perspective for mi50 clustering... I guess the positive of the plx/pex line of switches is 8x/16x,hardware,2025-12-31 22:50:39,1
AMD,nx04t9j,count me in,hardware,2026-01-01 00:16:15,2
AMD,nwx32qo,You can order them on JLCPCB,hardware,2025-12-31 14:30:32,3
AMD,nwzx0pd,The West does very little for industrial policy like China does.,hardware,2025-12-31 23:28:53,11
AMD,nxei7s0,Can you use an X series motherboard on an Intel CPU? A Threadripper?,hardware,2026-01-03 07:47:29,1
AMD,nx11f3r,A significant markup would defeat the purpose since most would just opt to buy a higher end board.,hardware,2026-01-01 03:48:56,1
AMD,nx30kd3,"For bare boards they're great and super cheap. I'm talking about boards with parts and assembly, especially if you have more specialized parts that use the extended part list and/or can't use the fully automated assembly. Even then, it's not unreasonable pricing, it just adds up.",hardware,2026-01-01 14:32:42,2
AMD,nx1gegt,Typically SoDIMM sticks from what I've seen,hardware,2026-01-01 05:46:08,3
AMD,nx0284f,They are starting to fire up the patent engines pretty hard the last year or two. The US has been pushing for them to take patents more seriously for decades now but I suspect the result won't be what was expected.,hardware,2026-01-01 00:00:11,9
AMD,nx0cydk,This 'meme' is becoming standard reddit ignorance and racism. Companies local and foreign in China have been suing each other for IP infringement for decades.,hardware,2026-01-01 01:06:23,8
AMD,nwzh7my,"And that's the question ha, imagine these daisy-chained 10x deep in some monstrosity  I'm sure there's a limit",hardware,2025-12-31 21:56:17,6
AMD,nwzy4y3,"It would be pretty sweet to have a bifurcated x16 version of this with 4 chipsets on it. 8 m.2 and 16 sata, it's a whole nas on one PCIe card",hardware,2025-12-31 23:35:32,3
AMD,nwzv052,"PLX switches can be reconfigured to split all of the outputs down to x1 if you want, it's just a matter of configuration. One of the cards I have has DIP switches to set what mode it's in. I wondered how this worked for a PLX88048 because it doesn't support setting these from GPIO lines any more (unlike the earlier ones) - but looking at the board it's just got three EEPROM chips and you're toggling chip select on them. It of course still worked fine when I rewrote that config on one of the EEPROMs to have the outputs as x4/x4/x8/x16 rather than all x8.  I do have 8x MI50 hooked up to a LGA2066 board with two PLX8749 switches, running x8 to each. However I had some fun because the PCIe slot breakout boards I bought had the pinout mirrored - so I had to cut and splice the cables to move PERST and REFCLK to the other side of the connector (PCIe lanes being backwards is actually fine, the PLX chip will detect this during enumeration and adjust accordingly).  Another handy trick is with a PLX88048 card in a PCIe 3.0 x16 slot, if you connect PCIe 4.0 SSDs to it, it will talk PCIe 4.0 x4 to the SSDs and 3.0 x16 to your motherboard, so you can use them at full speed.",hardware,2025-12-31 23:16:38,2
AMD,nx04n0i,"I also have a MI50 machine with 9xMi50 32Gbs..   Im currently just using an eypc setup, but switching over to a PEX88080 switch board with 4x PCIE 4.0 x 16 as half my GPUs currently are just on x4 slots making much slower for those for transfers. This way I get my NVME/SAS back and my GPUs all sit on x16 electrical slots. I can then also use inifiniband to connect machines and see if I can get RDMA GPU to IB transfers working.",hardware,2026-01-01 00:15:11,2
AMD,nx0ckta,"The west had strong industrial policy in the past, but later taught to hate it irrationally.",hardware,2026-01-01 01:03:57,11
AMD,nx1gz99,"The ""significant markup"" will just be if you buy on eBay. The other three won't be much more expensive than Taobao, most likely.  Also ""just buy a higher end board"" is good advice if you don't have a motherboard yet. If you have an existing board it's cheaper than buying a whole new one.",hardware,2026-01-01 05:51:00,8
AMD,nxapdcx,Ah it’s not a meme it’s reality that China has a state policy to go after IP and doesn’t respect IP laws like western countries do. This is why people invest less in the Chinese economy and why some companies are not moving some operations to China.,hardware,2026-01-02 18:33:42,0
AMD,nwzx5xh,yes particularly on that latter part for my MI50 build - they're capable of PCIe 4.0 but using on an x399 board with PCIe 3.0 - which I believe would allow them to talk p2p over 4.0 but to the cpu with 3.0.  I'm just trying to find ways to keep down on cost as much as possible atm  EDIT: your build sounds super familiar - are you on the gfx906 discord?,hardware,2025-12-31 23:29:45,1
AMD,nx6t8j6,"Well our industrial policy was working in the mines, steel mills and in factories with often poor working conditions (at least in the UK). Obviously not that pleasant, so we moved into the service sector.   However we did that before the big rise in electronics which are probably nicer industries to work in. By being early and moving on, we missed the boat for things like this.",hardware,2026-01-02 02:55:50,2
AMD,nx00cqu,"> are you on the gfx906 discord?  Yeah I am, that'd be why.",hardware,2025-12-31 23:48:54,1
AMD,nx01v8c,that makes sense - we literally just had a similar conversation this week on the PLX/PEX offerings. small world!,hardware,2025-12-31 23:58:01,1
AMD,nxqjup4,"Fantastic write up! It has bothered me for years that ALL usb webcams run really hot, most over 100F (38C).   Are there any video webcam products to purchase for PC that avoid UVC, leveraging my PC's CPU/GPU encoders?",hardware,2026-01-05 01:33:07,64
AMD,nxr1b0w,"A great write-up, but a few comments.  > The protocol requires an enocded video stream   I don't think this is true?  Here's a project I found several years ago when I first started messing with Lattice FPGA's that does a raw steam implementation:  https://github.com/circuitvalley/USB_C_Industrial_Camera_FPGA_USB3  How we got here (external cameras always encoding) is due to images being very data dense and being a media that lends itself well to compression.  In the project above their full res / bit rate on an IMX477 can only run at 15 fps because that's 2.2 Gbps raw!   Seems like peripheral bus technology needs to progress where bandwidth no longer is the limiting factor, but that isn't happening because imager size (in pixels) keeps growing at the same rate.  This is how you find yourself going down the USB4 route.    What's left out of this conversation is there are mediums for external raw image feeds exactly what you want.  GMSL is a technology that does exactly what you are looking for, exposing the raw CSI data from the imager directly to the application processor.   Trouble with this approach is a) coax cables suck for a consumer application b) tuning matters and is imager/hardware specific so now you don't have your hardware abstraction to make all devices behave the same.",hardware,2026-01-05 03:06:17,11
AMD,nxr3r4i,This helps to explain why a 4k 60fps webcam costs so much when we have cameras in all sorts of other device applications that can achieve that resolution and framerate at significantly lower cost or greater utility.,hardware,2026-01-05 03:19:56,6
AMD,nxquc24,"I agree with the overall point — desktop USB webcams are well behind laptop and smartphone cameras — however your proposed solution sounds way more complicated than smartphone vendors adding a UVC emulation mode, so people can make use of the ISPs and (rather high end) cameras they already have.  >While USB 3.0 (5 Gbps) can theoretically handle this, the UVC protocol’s isochronous transfer overhead and the lack of a universal raw format make it impractical.  Also this aside seems poorly evidenced. Maybe you should delve deeper into why you believe isochronous mode is a problem. Cursory googling suggest it was the lowest-overhead mode, at least as of USB 2, and the overhead is much less than 100%.",hardware,2026-01-05 02:28:40,14
AMD,nxrajhe,"Hey OP, next time try telling the LLM to support your thesis instead of arguing against it.",hardware,2026-01-05 03:58:37,10
AMD,nxr40dx,What about newer USB-C cameras that might be able to use a display port alt mode over USB or similar?  Is that a route to an updated protocol?,hardware,2026-01-05 03:21:24,3
AMD,nxr5q5e,"Having a standard for RAW video over USB would also (hopefully) enable mirrorless cameras to adopt streaming like this. Instead of the current HDMI converters or proprietary software.   Eh who am I kidding, camera manufacturers will insist on proprietary software forever.",hardware,2026-01-05 03:31:00,3
AMD,nxrnhyf,X is not just Y — it’s Z,hardware,2026-01-05 05:17:11,1
AMD,nxqr5wt,"Very interesting, how does using your smartphone as a camera work ? Used my Android 16 phone as a webcamera from the USB settings and windows picked it up right away and the clarity was very good. Is it using the same protocol ? Does the smartphone locally encode ?",hardware,2026-01-05 02:11:46,4
AMD,nxraewo,Well people should really at this point consider using old smartphones as webcams anyway.   Things like an iPhone SE 2nd gen or 11 are perfectly good as a 12mp 4k webcam with crazy good signal and image processing. Otherwise these phones go to waste anyway. They destroy most webcams for image quality.  Can connect easily via connectivity camera or something like the iriun webcam.,hardware,2026-01-05 03:57:55,1
AMD,nxrau8p,"Laptops have already been shifting towards using MIPI for their webcams, especially those with Intel CPUs, the webcam modules are now just the sensors while the CPU handles the image processing through a dedicated hardware component (called IPU6 or newer for Intel). As a result of that webcam compatibility is now a shitshow on Linux since now ""every different sensor and glue-chip like IO-expanders needs to be supported separately"" ([source](https://hansdegoede.dreamwidth.org/29233.html)). Even worse is that at least with Intel they refuse to open source the code to control their dedicated ISP hardware properly so on Linux if you want to avoid the pain of setting up the proprietary stuff (just like the annoyance of having to set up the NVIDIA proprietary driver) you get to use the CPU to do the ISP's job via software instead ([source](https://hansdegoede.dreamwidth.org/28153.html)).  AMD is taking a similar route as well with their ISP4, although they're moving a lot slower than Intel with only a single laptop known to use it at the moment, and fortunately on Linux they're happy to let users use the ISP hardware properly and actually take load off both the CPU and the webcam module without requiring a big pile of proprietary drivers ([source](https://www.phoronix.com/news/AMD-ISP4-Driver-Linux-v7)).  In short, there's already some foundations for MIPI support in the PC space since Windows already has to support it for these IPU6/ISP4 laptops, but cross platform compatibility is definitely a big concern of mine, I've already had to pick and choose and scrutinize my laptop choices because of this stuff and I'd hate to see others have to think about this for plain old webcams again.",hardware,2026-01-05 04:00:19,1
AMD,nxqsr3z,Beautiful write up. I always wondered why there is nothing new and this explains a lot why,hardware,2026-01-05 02:20:09,-4
AMD,nxqsbd3,This is an absolutely amazing post and one of the primary reasons I stay in this sub!,hardware,2026-01-05 02:17:49,-2
AMD,nxqfzsg,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-05 01:12:22,-4
AMD,nxqosxj,Time to mod your webcam with huge heat sink and fans.,hardware,2026-01-05 01:59:16,15
AMD,nxqq9db,It may be easier to just get a camera of your choice with hdmi-out and a capture card which can spit out the raw video stream.,hardware,2026-01-05 02:07:02,28
AMD,nxrb9b6,>Fantastic write up!   I'm sure ChatGPT is very flattered.,hardware,2026-01-05 04:02:47,19
AMD,nxrescq,Didn't have to scroll far to upvote this.,hardware,2026-01-05 04:23:47,-2
AMD,nxrimq6,This is pretty clearly not entirely written by an LLM.,hardware,2026-01-05 04:45:24,1
AMD,nvehqj2,Has AMD made any RDNA 4 GPUs for laptop?,hardware,2025-12-22 17:36:36,251
AMD,nveicyq,"AMD has to actually ship product (and support it) for OEM to use them.  There are other considerations, yes, but multiple manufacturers have publicly stated that AMD just doesn’t have enough available for them to warrant making more than a few models.  Nvidia on the other hand has more than enough capacity to guarantee deals so they can easily just stick with them for an entire product stack and simplify procurement.  (For dGPU products. Future of Nvidia’s business direction withstanding at the moment.)",hardware,2025-12-22 17:39:46,235
AMD,nvei7gh,I imagine AMD don't commit the volume necessary to supply the OEMs   Look at the supply issues on the 9070/XT for most of the year. And the DIY market is small,hardware,2025-12-22 17:39:00,203
AMD,nvekxim,"1. Nvidia has brand recognition and simply sells better.  2. Nvidia can guarantee OEMs as much supply as they can move. AMD discrete GPUs for laptops are as good as vapourware.  3. Nvidia GPUs are more efficient and that really matters in a laptop. AMD GPUs really struggle with idle power draw especially, so even before you start to run anything on them, you’re on the back foot  4. Nvidia has their entire laptop stack (with the exception of a couple of SKUs) available at launch. AMD drip feeds its launches so the hype doesn’t remain.  5. A massive proportion of gaming laptop buyers buy them to do work. Almost none of that works on AMD hardware. Their GPUs are straight up not supported in V-Ray and Corona. They absolutely suck in Blender. A 7900 XTX chugging 350W gets is arse handed to it by a 14 inch MacBook running on battery for example. Nvidia GPUs are better for video editing, especially with the new NVDEC of 50 series.   6. The value argument doesn’t hold for AMD laptop hardware. They have worse features and don’t tend to cost much less. And they all have the same VRAM anyway so that is also not a selling point.  TLDR: Because they are worse products and there is more nuance to the laptop market than there is to the DIY gaming desktop market.",hardware,2025-12-22 17:52:35,80
AMD,nverhi0,"It's more likely due to AMD.    They simply don't ship that much volume and unlike nvidia who is constantly pushing volume of desktop GPUs, AMD seem to prefer the much more profitable AI/datacenter market.   Why integrate a worse, less efficient GPU at almost the same price?",hardware,2025-12-22 18:24:53,18
AMD,nvelbw4,"AMD isn't able to guarantee the same amount of supply as Nvidia (same situation with CPUs and Intel, although that's been improving)  And AMD has also been behind in terms of performance/watt since at *least* 2014, which is the most important metric by far when thinking about laptop GPUs.",hardware,2025-12-22 17:54:33,24
AMD,nvehi2k,Amd never released newer lineup this gen thats why.,hardware,2025-12-22 17:35:25,17
AMD,nvf6c3u,"They don't. AMD just hate the market for some reason and have never been good at supplying it.  Remember that a lot of laptop brands are largely made by a relatively small number of ODMs. Producing a modern laptop mainboard with discrete graphics is where an ODM leans heavily on their knowledge and relationship with Intel and Nvidia for circuit designs etc. Both companies are very focused on mobile and working with these ODMs. Both have a large lineup of products specifically engineered for mobile, AMD... not so much.  AMD hasn't been focused on mobile for a long time and ODMs have complained about the lack of relationship with the company, being left to figure stuff out themselves etc as far as putting together a mainboard featuring AMD products.  This generation they also just plain do not have the competitive products, there is no RDNA4 mobile chip which is strange. In the past there has also been complaints about inadequate supply of mobile dGPUs.",hardware,2025-12-22 19:39:04,15
AMD,nvf7jah,Hard to ship something that doesn't exist,hardware,2025-12-22 19:45:07,20
AMD,nvegyew,Power efficiency is everything in a laptop and AMD GPUs require a higher power draw to get a similar performance.,hardware,2025-12-22 17:32:35,20
AMD,nvghna1,"Likely more profitable to go with NVIDIA, whether that is in better prices on components from NVIDIA, or a lack of supply from AMD, or simply better brand recognition that attracts customers.",hardware,2025-12-22 23:55:06,3
AMD,nvgrjjr,"1. AMD only has enough current mobile dgpu skus to count on 2 hands at most 2. AMD doesn't commit to actually shipping high volume of mobile parts 3. AMD tries to force ""AMD advantage"" with various hardware requirements that makes it a pain in the ass for OEMs to ship either cheap base models or use Intel cpus as an alternative if there is a shortage of AMD APUs.  No sane OEM is going to sell amd dgpus when they have min spec requirements for CPU, Memory, Storage, Screen, and wifi/bt.",hardware,2025-12-23 00:53:24,3
AMD,nvhgaig,"All these choices are done via the GPU maker’s business development team going and getting it done by selling into the laptop OEMs. The BD team is enabled by leadership committing manufacturing resources. It’s not like the laptop OEMs just go decide what to buy and put in the systems. I could also see the laptop OEMs going to the silicon companies and sharing “we have consumer demand for an AMD GPU, can you please make some available to us?”  In short, AMD either hasn’t made it a priority or they don’t have the capability. Or consumer demand doesn’t exist such that either party makes it happen.",hardware,2025-12-23 03:25:22,3
AMD,nvhi5r8,Because Nvidia makes 95% of all GPU products… so laptop makers need to be ensured that mass product will be available. AMD likely doesn’t have the capacity that some SI’s are looking for.,hardware,2025-12-23 03:37:19,3
AMD,nvhsnnp,"Because they are not good and AMD is well know for making half ass bios and then never fixing it, especially on the GPU side of the house.   For example we are all still waiting for AMD to release the Linux nvme raid driver for the built in raid, for ALL AMD CPU platforms. Been what? 10 - 12 years now?",hardware,2025-12-23 04:48:11,3
AMD,nvjlvs5,"its the other way around, AMD only pretends to make mobile GPUs so they can lie to investors or something  seriously they have zero volume, idk why they even bother with it as a product stack. must have a government contract or something",hardware,2025-12-23 13:56:48,3
AMD,nvsznig,"The whole concept of the APU, AMD, and Intel to a lesser degree, are choosing to beef the iGPU in their APU to compete in the low and medium performance gaming laptops against the Nvidia dGPUs.  I personally have the theory that the handheld PCs were a proof of concept of how far AMD and Intel can take their APUs performance wise vs Nvidia discrete solutions in the mobile market.   AMD probably are thinking that if a single chip in the form of an APU can deliver a similar performance to two chips (CPU+dGPU) for 50-80% of the power budget, then they can corner the low and part of the medium performance gaming laptops market, after all you don't see a Nvidia dGPU in any of the current or future X86 handhelds, way too power hungry and ARM alternatives like the Tegra family fall considerably short, not to mention the still messy compability layer to run X86 games on ARM systems.",hardware,2025-12-25 01:08:56,3
AMD,nvf86y9,Supply Issues is most likely   \#1 Reason  \#2 They don't sale.   Its the perfect combo of why bother. R&D isn't cheap you need to recoup cost at the bare minimal for what you put into the device being sold. Companies aren't going to do each other any favors anymore then they do a consumer a favor.,hardware,2025-12-22 19:48:28,9
AMD,nvggnqd,"AMD doesn't have a mobile GPU lineup that has matched NVIDIA's for a while now (at least in the last 11 years), and what doesn't help is the efficiency at idle/low loads isn't great either which isn't a big deal in a desktop but much more important in a laptop, as you don't want heat to stick around in the laptop shell.  Also whilst the Nvidia mobile gpu shennagians weren't great with naming, they are generally more power efficient and performance ""per watt"" is better , especially if you are pairing it with a high end cpu or a thin laptop shell.",hardware,2025-12-22 23:49:14,6
AMD,nves026,"AMD makes limited laptop level GPUs. Additionally, for laptops the market is a bit more split. If people want a gaming laptop, they generally really want a capable machine, and Nvidia has the more significant brand recognition there. If they don’t want that, they generally explicitly don’t want a dGPU. So as an OEM, it’s hard to justify the volume necessary to support an AMD SKU rather than just consolidate around an Nvidia line",hardware,2025-12-22 18:27:25,3
AMD,nvezuog,"I think that the APUs are pretty good for laptops, the RX6600M didn't really take off, I haven't seen any laptop with 7600. If they don't come with a 9060 non-XT for laptop now, then they never will.",hardware,2025-12-22 19:05:59,3
AMD,nvegw55,"Wondering the same thing, and I'd love to see an explanation.",hardware,2025-12-22 17:32:15,5
AMD,nvei62i,"The key reason is just that Nvidia sells better. For an integrated product such as a laptop its very expensive to give consumers the choice, only for 90% of them to go with nvidia.   The technical aspects are not super important, because AMD did price some of their laptop GPUs aggressively to offset a hypothetical inferiority and almost nobody adopted it other than minisforum I think.   Its not a god situation for the market, its really nice to see framework making it flexible, but for many OEMs it seems just not worth it. The nvidia + Intel bundle situation might make it even worse for AMD in the laptop space in the future...",hardware,2025-12-22 17:38:47,4
AMD,nvhflr4,"As an oem manufacturer, why would I want to carry 2 sets of inventory?",hardware,2025-12-23 03:21:00,2
AMD,nvip7tg,Because AMD can't supply them realibly in any meaningful way that the manufacturers can design multiple lines across,hardware,2025-12-23 09:37:52,2
AMD,nvis47a,"Since GPUs come with the laptop, and are not generally replaceable, it usually comes down to a matter of ""can you promise X sales at Y cost"" and NVIDIA being a fuckin trillion dollar company likely has the ability to offer much more competitive pricing. This is just one factor though as I'm sure there are many.",hardware,2025-12-23 10:06:01,2
AMD,nvkk3e3,Costco Canada uses laptops with Ryzen/Radeon in most of what they carry.,hardware,2025-12-23 16:53:16,2
AMD,nvlqz93,I've had a laptop with a 6800s.  It received one driver update. ONE.,hardware,2025-12-23 20:27:54,2
AMD,nvltdmk,"Because they probably still get kickbacks from Intel, or other ""incentives"".",hardware,2025-12-23 20:40:45,2
AMD,nvlz38r,"Lisa ""we're going for marketshare this gen by abandoning mobile, low end & high end""",hardware,2025-12-23 21:11:16,2
AMD,nvqmad0,"AMD historically hasn't been able to supply these manufacturers consistently enough, they don't trust AMD.",hardware,2025-12-24 16:33:54,2
AMD,nvely9g,Cause they like money and amd doesnt make them money,hardware,2025-12-22 17:57:38,7
AMD,nveixhl,because market share is leverage and OEMs don't want to risk their nvidia partnership,hardware,2025-12-22 17:42:36,4
AMD,nvff5vu,"You already got your answer from other users here, but AMD and Intel seem to not even really be trying to unseat Nvidia dominance in the laptop dGPU space anymore. Their currently strategy is to try and undermine it by beefing up iGPUs to the point where some more budget oriented people may question if they even need to step up to a x50 or x60 class dGPU.  AMD's first crack at this with Strix Halo may have been a flop, but the strategy is sound. Intel's releasing their X line of PTL chips in a few weeks. I'm sure AMD is gonna double down next gen with this plan as well.",hardware,2025-12-22 20:24:38,4
AMD,nvehamz,"Thats why I bought a Framework 16.  To have a Radeon GPU cuz Nvidia is utter dogshit under Linux, especially since most laptops don't use a fuckin MUX switch.",hardware,2025-12-22 17:34:20,7
AMD,nvffzdt,Simple numbers game.  Nvidia - 92% market share   AMD - 7% market share   Intel - 1% market share    Which one are you going to go with if your goal is to sell the largest number of gaming laptops possible?,hardware,2025-12-22 20:28:52,2
AMD,nvv4uzf,"educated guess: a laptop with a Ryzen CPU and a Radeon gpu, what a wonderful reason to scream ""monopoly""! ☺️",hardware,2025-12-25 12:58:45,1
AMD,nw12k9d,I have a legion with a Ryzen 9 and a Radeon gpu..,hardware,2025-12-26 14:42:29,1
AMD,nw71v5o,More like why AMD hates laptop manufacturers.  AMD needs to release mobile GPUs first and deliver them in necessary quantities generation after generation. Also they need to provide them across different pricing brackets.,hardware,2025-12-27 14:36:27,1
AMD,nwpwhgt,Wrong question. Why does AMD hate laptop manufacturers and refuse to supply units to them?,hardware,2025-12-30 12:20:26,1
AMD,nvftzjh,"They don't hate them, AMD GPUs simply SUCKS for what Laptop requires most  \#1 Reason: EFFICIENCY, Nvidia destroys AMD gpus in terms of efficiency, not even close, and efficiency is everything in laptop, even 10W higher could become a problem. Nvidia just draws so much less power at the same performance level for many generations already ever since the first RTX. The 5700XT draws the same power as 2080ti while being way slower and has no feature, 7900XTX consume more power than 4090 while being destroyed in everything, even newest 9070XT consumes about 100W higher than 5070Ti even though it's still slightly worse, this might not be relevant in PC at all, but for Laptop this is super crucial, and AMD completely failed at this. Not to mention idle power draw is a problem for AMD gpu.     \#2: Laptops are always meant for WORK and PRODUCTIVITY, not just gaming. If you want gaming only just build pc or get a console, but laptops have to be able to do as many productivity tasks as possible, that's what laptop is made for, a portable pc for work, even gaming laptop is made so that people can work and game on them, and AMD failed this aspect as well, Nvidia not only offer far better feature sets, but also simply work way better and compatible with everything, never need to workaround or tinkering, everything works with Nvidia gpu, and this is massive for laptop     These are the 2 biggest reasons, it's never about stocks or lack of volume that some clueless comments here suggest. That's why AMD thrives with Console instead, because consoles don't need neither of the reasons I mention above, console doesn't care about efficiency since they always plugged in, and console is for gaming only, never for anything else, so AMD has no problem fulfilling the massive volume of console Gpus, because for console use case AMD gpus make sense, unlike laptop",hardware,2025-12-22 21:42:38,1
AMD,nvelvp4,Probably the same reason all the laptops are intel despite them getting clowned on. It's because their agreement says so,hardware,2025-12-22 17:57:16,0
AMD,nveluk8,Probably the same reason all the laptops are intel despite them getting clowned on. It's because their agreement says so,hardware,2025-12-22 17:57:07,-1
AMD,nvggjoq,NVIDIA optimus and better efficiency,hardware,2025-12-22 23:48:34,1
AMD,nvf3akf,"I actually saw this talked about somewhere. I think the people said the manufacturers have found that there is just huge demand for nvidia gpus from consumers. Dlss, framegen etc and the fact that they deliver very good performance with reasonably small power, it is a no brainer for them. Having an nvidia gpu in the laptop does free advertisement for them.",hardware,2025-12-22 19:23:26,1
AMD,nvei17v,"Why do most people buy Nvidia?  OEM's follow what sells, Nvidia sell units. Just look at all the posts we still see asking if AMD GPU's will burn up, the number of times a AMD GPU was lower cost but OP ends up going Nvidia as it's the brand that everyone buys.  It's just sales, Nvidia moves laptops.",hardware,2025-12-22 17:38:07,-3
AMD,nveh6sx,"Why do orange basket manufacturers hate bananas?   I noticed that, while looking for a new purse.",hardware,2025-12-22 17:33:47,-1
AMD,nveiort,because intel still lobbying  one exemple: [https://www.reddit.com/r/Amd/comments/16q44ar/eu\_fines\_intel\_400\_million\_for\_blocking\_amds/](https://www.reddit.com/r/Amd/comments/16q44ar/eu_fines_intel_400_million_for_blocking_amds/),hardware,2025-12-22 17:41:23,-11
AMD,nvh91ql,I know it’s bs. Yet virtually every handheld pc maker uses AMD 🤦🏻‍♂️,hardware,2025-12-23 02:40:30,-1
AMD,nvfsakn,"They don't make a dGPU for laptops, just iGPU. Though their iGPU is pretty good for most tasks besides gaming. AMD probably thinks if you wanna game, buy a gaming system like a PS5 or Steam Deck which hosts an AMD chip.",hardware,2025-12-22 21:33:47,0
AMD,nvhlluk,I have a last gen asus a16 advantage edition with a 7700s gpu. Rougly 4060M performance minus the nvidia rtx/ftame gen which i never use anyways,hardware,2025-12-23 03:59:47,0
AMD,nvejjxr,"It's old practices, even when AMD started dominating in CPUs you look at the market place and it's NVidia/Intel everywhere... That eventually changed due to the lunacy of not having the best CPUs ""because reasons""... The GPU side of things is going to be very hard as the stigma that AMD is a budget brand is what drives these companies to avoid good AMD GPUs as they fear the budget stigma will rub off on their premium products.",hardware,2025-12-22 17:45:41,-11
AMD,nvi4m8u,"They used to run too hot for laptops and nvidia drivers used to be the more reliable ones  Companies are very reluctant to change  This is what Steve Jobs noticed in hardware manufacturing when it came to knowing/not knowing how many precisely how many units were produced per unit of time, among other problems and common practices. The answer to everything was “well because we’ve always done things this way”  With Ryzen, there was a clear reason to switch, it had better performance and lower temps, everything you’d want for a laptop  TLDR: no reason other than “it’s always been this way.” No one changes until someone changes.",hardware,2025-12-23 06:22:33,-1
AMD,nvi5h9s,"Because gaming laptops actually suck for gaming, and Nvidia has a stranglehold on the creative market (who actually buy high end laptops) with CUDA.",hardware,2025-12-23 06:30:04,-2
AMD,nvfsoop,"AMD sold their mobile Radeon brand to Qualcomm when they were at their lowest, hence why Adreno is just an anagram of Radeon. Because of that deal, they can't have dedicated mobile GPUs under their Radeon branding...",hardware,2025-12-22 21:35:49,-7
AMD,nveu5h8,Nope. They wont launch a 9070M,hardware,2025-12-22 18:37:57,117
AMD,nvf3blw,question answered,hardware,2025-12-22 19:23:35,64
AMD,nvfs4is,"They made RDNA, RDNA2, RDNA3... almost no laptops.  Backroom deals, pressure or even manipulation from Nvidia might not be the whole story but I suspect it is going to be a significant factor...",hardware,2025-12-22 21:32:55,8
AMD,nvelsr1,"Chicken and egg, AMD hasn't been shipping product because laptop manufacturers do not buy them. They're finding success in their strix halo APUs, and that's likely their future in the laptop space.",hardware,2025-12-22 17:56:53,-44
AMD,nveiq2s,"Yeah, this is by far the biggest issue discouraging adoption by laptop makers.",hardware,2025-12-22 17:41:34,46
AMD,nvelh9x,Well they supplied the entire console market pretty well and their cpu market too so their track record is actually good,hardware,2025-12-22 17:55:18,3
AMD,nvgyss7,"AMD doesn’t actually make anything, do they?  All their chips are made at other fabs.  That means they’re competing with everyone else for the same production capacity.",hardware,2025-12-23 01:38:03,0
AMD,nveqtpa,"True points.  Adding to them that Nvidia is more reliable in hardware and software for decades.  AMD drivers for RDNA used to be problematic and they had higher RMA rates. For a DIY enthusiast this is less of an issue, but for a system integrator/laptop manufacturer it means costs. Even the basic support request is a cost and you may have to service an entire system in case of a RMA, instead of just the dGPU for a DIYer. Combined with AMDs notoriously low margins, a single support phone call can mean a financial loss.  Meanwhile Nvidia is generally stable. While sometimes the drivers cause issues in specific scenarios, there was no general issue with them as their was with early RDNA.",hardware,2025-12-22 18:21:38,16
AMD,nvfra3n,"7 Historically AMD's software stack had been so clownishly bad they were basically unusable. Until a couple of years ago you genuinely couldn't install the normal driver on their mGPU on many laptops, instead you'd get some branded locked down driver with missing features from the laptop maker from around the time they released the model and stuff just wouldn't run on a 2, 3, 4 year outdated driver. Sometimes they'd even have the older driver UI which hadn't been in use for literal years.  AMD being bad at software is not a meme.",hardware,2025-12-22 21:28:30,7
AMD,nvi51ow,"Why the heck are they always coming out with new codec’s lol  It always leaves AMD in the dust, makes it feel like Ryzen and Radeon will always be left behind when it comes to editing and streaming",hardware,2025-12-23 06:26:17,2
AMD,nvfka74,> Nvidia GPUs are more efficient  This is not really true. RDNA4 is just as efficient as Nvidia. rx9070 topped GN's efficiency charts for instance. RDNA2 was also more efficient than Ampere.  AMD also has Radeon Chill which is another tool you can use for power efficiency Nvidia doesn't have.  I agree with your other points.,hardware,2025-12-22 20:51:31,0
AMD,nvexbft,Who uses a laptop for blender or video editing? Literally cherry-picked use-cases to make AMD look bad.,hardware,2025-12-22 18:53:26,-27
AMD,nvhj90c,And they barely released anything last generation either.,hardware,2025-12-23 03:44:24,9
AMD,nvf18dd,Efficiency at low loads is also poor too: https://tpucdn.com/review/sapphire-radeon-rx-9060-xt-pulse-oc/images/power-video-playback.png,hardware,2025-12-22 19:12:59,15
AMD,nvekfcw,Not since Vega,hardware,2025-12-22 17:50:02,-11
AMD,nvhkr1q,"Actually, you could count AMD’s current generation laptop dGPUs on 0 hands, considering they don’t have a single RDNA4 laptop part this generation. We’ll see if that changes at CES next year (in ~2 weeks), but I wouldn’t hold my breath.",hardware,2025-12-23 03:54:13,2
AMD,nvkegfh,Weirdly their mobile chips are far more likely to be found in mini-PCs than laptops. I'm not sure why that is but presumably they've built a stronger relationship with the mini-PC firms.,hardware,2025-12-23 16:25:51,1
AMD,nvjxs9k,"RX 7600S exists in a variant within the cheaper Asus TUF, same as the RX 7700S",hardware,2025-12-23 15:03:08,0
AMD,nvehr27,"I've been debating recently on which GPU to get alongside a mobo upgrade, what do you use to have it switch to the dedicated GPU?",hardware,2025-12-22 17:36:40,3
AMD,nvek8ky,As someone that’s works with corporate GPU applications this is funny. If they were so bad why would they dominate the industry?,hardware,2025-12-22 17:49:06,7
AMD,nvfvvdu,"the idea of a mux switch in a laptop is hilarious, hardware fix for horrible drivers",hardware,2025-12-22 21:52:21,-2
AMD,nvgc595,"These numbers aren't relevant. They look like dedicated desktop GPU market share. In reality, I bet Intel is first in laptops, followed by AMD just due to the fact that most laptops don't need, nor want dedicated GPUs. As for gaming space idk",hardware,2025-12-22 23:22:26,-1
AMD,nvf3pnf,nah intel mobile cpus are just better rn,hardware,2025-12-22 19:25:35,13
AMD,nvi46x5,They’ve put double agents to make AMD mobile gpus suck (because they’re)?,hardware,2025-12-23 06:18:50,5
AMD,nvezdou,LOL,hardware,2025-12-22 19:03:37,11
AMD,nvfopof,"I call it the *OEM-factor*™ … Always gets immediately DENIED as non-existent, of course, since years.  Yet somehow we had *a completely heathy and quite balanced laptop-market up to the early 2000s*, where there were loads of potent AMD-powered offerings with AMD's *PowerNow!*-technology (Dynamic frequency scaling and power-gating for power-saving), and it was almost like fifty-fifty AMD vs Intel …   That was when Intel was still utter sh!t in that department and had even horrendous power-draw in mobile.  Until it all of a sudden all changed the precise moment Intel brought their infamous *Centrino*™ program to OEMs (paying system-integrators for equipping notebooks with Intel-chips)  — It has stayed as such since (+90% Intel).  Then the Intel-exclusive *UltraBook*-brand was the next, which a while ago just rolled over to be Intel *Evo*.  > Intel and Nvidia pay system integrators to use their own hardware over AMD's.  Yeas, and very handsomely at that. Still, *»Nothing to see here folks, just move along!«*",hardware,2025-12-22 21:15:05,-4
AMD,nvekqpa,"And despite the absolute surge of burned connectors since Blackwell, it is still AMD cards that get questions sbout catching fire",hardware,2025-12-22 17:51:38,6
AMD,nvela7a,"Lmao Intel isn't lobbying OEMs to choose Nvidia GPUs.  And also literally read the first paragraph of the article: ""between 2002 and 2007"".  And thirdly, the biggest proof Intel isn't lobbying OEMs (with what money?) Is how many design wins Snapdragon got",hardware,2025-12-22 17:54:19,23
AMD,nvemfxf,"Right... Intel is lobbying OEMs to use Nvidia products..  Maybe every once in a while you should actually read the articles you use as ""proof"".",hardware,2025-12-22 18:00:04,15
AMD,nvelpt7,"AMD had to consistently deliver a better CPU than Intel for several generations before mindshare in the public caught up.  Its going to be the same for GPUs: until AMD offers the better *all around* GPU, several generations in a row, and at a lower price, it's mindshare just isn't going to change in GPU.  And dont forget the impact Halo products have on their downstream products. The fact that the 5090 is the undisputed best GPU gives huge brand prestige to the whole product stack.",hardware,2025-12-22 17:56:28,14
AMD,nvfz8l1,what?,hardware,2025-12-22 22:10:06,1
AMD,nvivpn8,"> Because of that deal, they can't have dedicated mobile GPUs under their Radeon branding...  Oh shit, you'd better inform AMD because they've been using Radeon branding for all their mobile dGPUs all the way up to this very day.",hardware,2025-12-23 10:40:31,0
AMD,nvh56n9,They’ll put a 9050m in laptops after they release RDNA6 in 2030,hardware,2025-12-23 02:16:52,28
AMD,nvg5a6l,Why would laptop manufacturers do this to us?!,hardware,2025-12-22 22:43:10,53
AMD,nvj2qn5,Rdna2 was the only real competitive mobile showing in a decade plus. That was also when they got the most laptops including the flagship lenovo legion 7 in an AMD advantage model. Stop playing the victim and accept they have NEVER been competitive for more than 1 gen in a row and they do not make things easy for oems.,hardware,2025-12-23 11:44:06,27
AMD,nver688,"I fear ""success"" in Halo Strix is related to its limited supply: that is, the Strix Halo large APU price is so high, the demand is also pretty low → AMD can make enough dies to satisfy the small market.  Large APUs will find it tough to crack into the laptop gaming market in terms of market share; the only other large APUs are 1) consoles, which only survive on extreme optimisation, hundreds of millions of units, and they *still* sell at a heavy loss, and...  >As [IGN reports](https://www.ign.com/articles/microsoft-loses-between-100-and-200-on-every-xbox-sold), Spencer confirmed that the loss on each Xbox console is between $100 and $200 dependent on the model.   2) Apple's M-series Pro / Max / Ultra. These are also very expensive and kind of what one would expect a large APU to cost (+ the customary Apple tax).",hardware,2025-12-22 18:23:21,25
AMD,nveqsbl,"Chicken and egg doesn’t really work for B2B. The seller has to provide the product and push the deals, demand doesn’t just appear out of the air for products with (edit: particularly established) competition. The buyer doesn’t particularly care which egg it buys as long as it has enough of them to make what it wants to sell.  (And in this case the consumer doesn’t really care either, the overwhelming majority of any form of prebuilt including laptop is just “it’s in my budget, available, and the page says it’s good”.)",hardware,2025-12-22 18:21:27,62
AMD,nveyg5b,"Strix Halo has been a commercial failure.  There is only one commercially available laptop of it. No major OEMs are even interested in it, and it is now being sold to Chinese brands at a discounted price for mini PCs and handhelds.",hardware,2025-12-22 18:58:58,39
AMD,nvfez9n,"Most people talk about volumes and while that certainly does contribute, the main problem is that OEMs basically expect cpu vendors to handhold them to designing most of the system. Yes you read that right, designing the system as in mobo reference designs, cooling solution and beyond just what you imagine to be normal SW support. When qcom started to double down WoA to promote snapdragon elite, the higher ups complained about how they had to provide massive amount of support to OEMs because they were to used that and they have to massively ramp up on that for OEMs to take them seriously and not just put their SoC in some half assly designed gimped models",hardware,2025-12-22 20:23:41,14
AMD,nvemrk9,"I think APU's will be a big deal for AMD in the near future, they will be fast enough for everything and will keep getting better for the limited power envelope of laptops",hardware,2025-12-22 18:01:39,2
AMD,nven0z3,Supplying the console market is probably one of the reasons they don't have much stock left for laptops.,hardware,2025-12-22 18:02:56,76
AMD,nvf0m0e,The consoles are using old nodes tho,hardware,2025-12-22 19:09:49,10
AMD,nvenqkg,"Right, now look at their GPU track record, the relevant market here",hardware,2025-12-22 18:06:29,25
AMD,nvesa9n,"Desktop CPUs sure, but mobile CPUs they don't seem to supply a sufficient volume of either. Look at Strix Halo, a high margin performance leading part and a year after launch you can still count the number of laptops using it on one hand.",hardware,2025-12-22 18:28:49,17
AMD,nvi0fev,But they choose how to allocate their capacity,hardware,2025-12-23 05:47:07,4
AMD,nvhpn99,Cost wise its not effective either. Its the same reason why AMD fail miserably in the Pre Built space.  9060 xt and 5060ti. 350 v 420. $80 and 20% right?  But when you build a whole PC with the 9060xt for lets say $1000. The same with a 5060 Ti wil cost $1080. Only a 8% difference. A difference many would pay  And the difference gets smaller the higher your base pc is  Same thing applies in laptops where the sum of all parts makes the RTX GPU only slightly more expensive.,hardware,2025-12-23 04:26:49,12
AMD,nviwwwy,"They didn't. They simply were catching up to Apple and Intel who've had 10-Bit 4:2:2 HEVC decoding for a very long time. Even now, I don't think the 50 Series can decode 10-Bit 4:2:2 H-264 in hardware.   It is a big reason why most videographers and video editors use Macs. 10-Bit 4:2:2 HEVC is a very common acquisition codec now and quite frankly, it is laughable and unacceptable that neither Nvidia nor AMD supported them until this year. AMD still doesn't.",hardware,2025-12-23 10:51:50,13
AMD,nvh2us8,"Are you speaking to efficiency under load? Or while idle?  Idle power draw has been a weak spot for AMD GPUs for a while. I haven't looked into it since launch, but at the time initial reviews seemed to confirm this was still an issue for RDNA4.  High idle power draw is bad for laptops, obviously.",hardware,2025-12-23 02:02:46,8
AMD,nvhmegg,"I also agree in everything but that point as well. Don't know why you're getting downvoted, though not surprised since everyone here is speaking anecdotally. Also, while you posted an example from TPU there are other outlets such as [computerbase](https://www.computerbase.de/artikel/grafikkarten/amd-radeon-rx-9070-xt-rx-9070-test.91578/seite-9#abschnitt_leistungsaufnahme_gemessen_spiele_youtube_desktop) that backs it up (pretty much on par on idle, and daily usage/multi monitor)  I've actually made a post here: [https://www.reddit.com/r/hardware/comments/1l2vjuo/mostly\_positive\_reviews\_rx\_9070\_xt\_vs\_rtx\_5070\_ti/](https://www.reddit.com/r/hardware/comments/1l2vjuo/mostly_positive_reviews_rx_9070_xt_vs_rtx_5070_ti/)  Where I shared someone's finding on the matter with RDNA4 vs Blackwell (9070 XT vs 5070 Ti). At that time, when capping FPS, AMD by default simply does a better job with freq to hit a locked FPS target to save power, even doing as well or beating the 5070 Ti. In those comments you'll see (obviously so) that FPS capping saves you power anyways, but the matter is, RDNA4, especially when binned and power constrained (+drivers) can be effectively used in laptops.  Oh and btw,  >RDNA2 was also more efficient than Ampere.  IIRC pre-RDNA3 AMD only reported GPU power and not TBP (it was an [igors lab test](https://www.igorslab.de/en/graphics-cards-and-their-consumption-read-out-rather-than-measured-why-this-is-easy-with-nvidia-and-nearly-impossible-with-amd/))",hardware,2025-12-23 04:05:00,1
AMD,nvjm36w,"Yep. RDNA3 was really the only botched generation efficiency wise (post Vega). But just like with drivers, people will continue to believe AMD GPUs have worse efficiency for the next 10 years, regardless of how each generation actually does.  inb4 but rdna2 had a node advantage. Yes, and?",hardware,2025-12-23 13:57:59,-3
AMD,nvezpob,"That’s like the majority of professionals who need a GPU in their laptop, that’s not that cherry picked.",hardware,2025-12-22 19:05:17,28
AMD,nvezb25,You can’t think of a reason why someone may want to do video editing or Blender work on the go?   Or have a device capable of being a video editing platform anywhere to allow for working from home?,hardware,2025-12-22 19:03:15,20
AMD,nvezl9d,basically everyone using a Mac.,hardware,2025-12-22 19:04:40,16
AMD,nvha33b,"no Blender pros do buy them, as new laptop 40 and 50 series do very good in blender  new laptop “5090” GPU with 24GB VRAM almost matches a 4080 super desktop in blender performance at 175 watts   https://opendata.blender.org/benchmarks/query/?compute_type=OPTIX&compute_type=CUDA&compute_type=HIP&compute_type=METAL&compute_type=ONEAPI&group_by=device_name&blender_version=4.5.0  CAD professionals, with Recent Ai boom a laptop with CUDA support for ML and Ai it's an awesome buy  In steam charts 4060 laptop is the top……  Unlike what reddit thinks,   do you think companies pour millions in for gaming laptop manufacturing R&D to waste?  Gaming laptops do insane number of sales.",hardware,2025-12-23 02:46:47,8
AMD,nvelkrb,"They still do. RX 7600 is slower than a 4060 and needs 40-50W more power to run. To compensate, AMD had to reduce CUs from 32 to 28 and reduce power draw. On the other hand, the RTX 4060 is identical on laptop and desktop, down to the CUDA core count and power consumption. It’s the better product. And given that the 60 series is the most popular laptop GPU series, that really adds up",hardware,2025-12-22 17:55:47,22
AMD,nvetp5y,They are less efficient than nvidia and suck way more power at idle which is a big deal in laptops,hardware,2025-12-22 18:35:43,8
AMD,nvetuoh,"I dont need to switch it myself, since it works in handshake with the AMD DGPU and IGPU with integrated MUX switch.   So basically, desktop idle and browser stuff runs off the IGPU, everything else, DGPU 7700S handles it.   Also, if ya have a 7745h cpu, not worth swapping to the HX since its half the full core size, plus compressed Zen 5c cores.  Dont like that alot",hardware,2025-12-22 18:36:28,3
AMD,nvel0u2,Open source community distros running GUI desktops are a bit of a different experience than headless Linux servers running with enterprise licensed packages or Windows/Mac desktop.,hardware,2025-12-22 17:53:02,19
AMD,nvggnqa,These numbers are the *only* thing that's relevant.  We're talking about gaming laptop GPUs.  Manufacturers are going to sell what people want to buy.,hardware,2025-12-22 23:49:14,5
AMD,nvf6179,I was talking tech wise... also no. A quick gander on the passmark puts the top 10 laptop chips as 80% AMD with Intel taking the number 2 and 3 slots. So I'm going to gather that the intel mobile cpus are at minimum on par.. I'm sure theres a some variance in benchmarks but I'm lazy  https://www.cpubenchmark.net/laptop.html,hardware,2025-12-22 19:37:30,-1
AMD,nvhs8e1,"Qualcomm showing up in more and more laptops. ARM is the future, Apple proved that. And Qualcomm is the company best poised to make the ARM chips in the quantity needed, because they’ve been doing it for smartphones for decades.",hardware,2025-12-23 04:45:08,-1
AMD,nvezusp,"I know right! So pathetic what those companies are willing to do to put down AMD, but shoe is on the other foot now and AMD will stomp out intel in the CPU space and is gaining ground as Nvidia leaves gaming to propel the great AI scam.",hardware,2025-12-22 19:06:00,-2
AMD,nvj08ow,"Yeah, it all changed on a sudden because the Pentium M was just so good.",hardware,2025-12-23 11:22:13,1
AMD,nwpy5g7,and by absolutely surge you mean like 3 in total.,hardware,2025-12-30 12:32:50,1
AMD,nvelh9j,"I gave up pointing that out, people want to buy Nvidia and you cant change there mind even if it's lower cost or not setting on fire.  GPU's are like SSD brands, you just plug one in and it works yet a lot of normal/light PC users still think it's dark magic to change GPU brand.",hardware,2025-12-22 17:55:18,0
AMD,nvey1c4,"Exactly. My biggest piece of advice for tech illiterate people buying a laptop until about 7 years ago was ""make sure you get an Intel CPU"". AMD was producing some absolute donkeys that had no place in the midrange laptops they were going into. It took a while for that sort of advice to cycle out of people's minds, well after AMD started making decent mobile CPUs.",hardware,2025-12-22 18:56:55,3
AMD,nvii28a,"https://www.qualcomm.com/news/releases/2009/01/qualcomm-acquires-handheld-graphics-and-multimedia-assets-amd  https://www.fierce-network.com/wireless/qualcomm-buys-amd-handheld-assets-for-65m  Instead of downvoting me, just do research...",hardware,2025-12-23 08:27:05,0
AMD,nvg9hnc,Do you mean AMD?,hardware,2025-12-22 23:06:58,-14
AMD,nvjpt9x,"Why are there no notebooks with Strix Halo then?  Asus, the vendor AMD even gives preferential treatment, only made tablet - a device nobody asks for - out of Strix Halo so that they don't have to put it in a laptop. Hmm? Absolutely AMD's fault for not making the processor, right?",hardware,2025-12-23 14:19:24,-6
AMD,nvj1xjk,"IIRC Strix Halo has bespoke I/O and compute dies, so presumably either supply is high, there's a plan to productize those some other way later on, or somebody is losing money.",hardware,2025-12-23 11:37:10,1
AMD,nvev6sn,"Next gens RDNA lineup is fully dual use. The top spec goes into the workstation market and high end desktop, the second chip is the 9070XT replacement and a cut down version goes into the next xbox. Third chip goes midmarket gpu and Halo gpu die, and the small one for the ...point mass market apus.",hardware,2025-12-22 18:43:03,-9
AMD,nvet6lj,"No, GPUs aren't direct replacements in laptops. They have to develop two different boards for each model, if they want to offer both manufacturers. Customers are also very picky in gaming laptops. Almost all of them will pick an Nvidia version over AMD at a similar price. Just ask yourself how much cheaper a $1400 laptop would have to be to pick a 9060XT over a 5060ti? If the answer is >$100, then AMD would have to basically give away the chips for free.",hardware,2025-12-22 18:33:12,-11
AMD,nverlg3,This is so massively ignorant to talk about in the PC space. People do care... It's why Nvidia owns 92% of the PC graphic card market.  AMD has been struggling to sell it's products for ages in the GPU space... Both to end users and to board and laptop partners and it's not because they can't produce product...   APUs are AMDs future in the laptop market because AMD CPUs are unmatched in the x86 space. Strix Halo sells because the okay GPU is bolted onto world class leading CPUs.,hardware,2025-12-22 18:25:24,-24
AMD,nvetzk2,AMD has been betting on that for years but still have issues giving good supplies to OEMs  And they also take a long time to actually launch their products to OEMs,hardware,2025-12-22 18:37:08,19
AMD,nvjm5ki,"somehow they have enough silicon to make strix halo IO dies with an entire mobile GPU integrated into the SOC  it is definitely more deliberate, especially since mobile GPU chips are literally just desktop GPUs but in a mobile package and power limited",hardware,2025-12-23 13:58:22,3
AMD,nveofqs,It's hard to judge rn because of AI dram production which definitely is gonna cut into some production for some of the process nodes. Ideally though you'd have thought they'd be at least well suited to gaming laptops with their lower power console focused development.   Especially considering their APUs are almost best in class at this point so you'd thought they'd at least have it down in the lighter laptop selections.   I think its mostly just legacy volume contracts and risk adverse manufacturers keeping intel and nvidia dominating,hardware,2025-12-22 18:09:56,-11
AMD,nvf58ki,Yeah they are but uh track record is kinda always gonna be old. Plus I wouldn't say that the current situation is exactly a good example.,hardware,2025-12-22 19:33:25,1
AMD,nvepc8e,"I am, I'm talking about their console GPU manufacturing. I'm saying that they have good track record for chip manufacturing its at least a half decent indicator.",hardware,2025-12-22 18:14:24,-14
AMD,nvf738n,"AMD uses a single universal tapeout/photomask set to satisfy the vast majority of their entire desktop and server catalogues. Even a substantial proportion of AMD's available laptop offerings over the last 2 years have been -HX ones, which are just the desktop SKUs made into a solderable package. That's why it's always been relatively easy for them to maintain relatively consistent inventory, despite having far far fewer wafers coming in compared to what Intel can get.  The actual monolithic mobile SKUs are fundamentally different die designs to the other market segments. Strix Halo also uses a fundamentally different chiplet packaging method to server/desktop Zen, so neither the CCD or I/O die are compatible and makes it its own third, separate limited production assembly line.",hardware,2025-12-22 19:42:53,17
AMD,nveu96i,It's that actually production problems or do manufacturers just not adopt strixx halos for what ever reason they dont adopt AMD CPUs or GPUs in the laptop market?   Are there volume problems with those laptops? I can imagine strix halos packaging makes it hard to produce,hardware,2025-12-22 18:38:27,1
AMD,nvi2sgv,"Does anyone expect them to prioritize the single GPU buyer, and let large customers get whatever is left, or does the other way around seem more sensible?",hardware,2025-12-23 06:06:53,1
AMD,nvj3l0t,"To sum up, when your one & only selling point is lower upfront cost, you will have a lot of uncertainty and areas of business you won’t be able to compete in.",hardware,2025-12-23 11:51:04,3
AMD,nvxm62h,Oh no wonder photographers and stuff use Macs,hardware,2025-12-25 22:21:11,1
AMD,nvh3707,RDNA4 idle efficiency is good too. You might be getting confused with RDNA3 big GPUs which were chiplet based. They use more idle power because they are chiplet based.  https://www.techpowerup.com/review/asus-radeon-rx-9070-tuf-oc/41.html,hardware,2025-12-23 02:04:49,8
AMD,nvjn3ob,"Wouldn't call RDNA3 botched, it's just a price of doing chiplets. I'm sure they learned a lot from it.  rdna2 had a node advantage but it wasn't just the node advantage that made it more efficient it was also the infinity cache, the new cache hierarchy that improved memory bandwidth efficiency (AMD could have similar bandwidth with less memory bus width). Nvidia followed suite by adding a big L2 cache in the next generation.",hardware,2025-12-23 14:03:53,-4
AMD,nvxinwm,"I can think of plenty that do, and it always comes down to the same two reasons.    1. They (for entirely selfish reasons, unrelated to work) travel between multiple locations and need access to GPU computing power, while away from their (home) office.   2. They don't understand remote compute and streamline decoding. While using the worst internet sources while on the go.   It's not that other, faster and more reliable solutions exist. It's that they are typically ADHD gremlins that need simple, reproducable solutions that work, even if it takes 500x longer and has stronger caveats.   It's not a market issue. It's a user issue wanting a product to confirm to their lifestyle, that wasnt designed to. Laptops are not workstations, and don't have the P2Pr to even compete in the space. But so many refuse to be ""tied down to a desk"" that this is how things are.",hardware,2025-12-25 21:59:39,1
AMD,nvezomv,"Laptops are for convenience. Video editing and blender require some amount of time sitting down to do what you want that it's better off being done on a dedicated workstation.  ""I'm going to do a little blender and video editing on my laptop at the library today"" - said no one ever",hardware,2025-12-22 19:05:08,-19
AMD,nveuqbm,"Nah, idle power on Nvidia is tragic if you more than a single monitor unless they have identical resolution and refresh rate. My 5090 goes from ~40W to 90W by just plugging in a second display while doing nothing",hardware,2025-12-22 18:40:47,-7
AMD,nvez41n,I have the 7940HS and was playing on going to HX 370,hardware,2025-12-22 19:02:15,2
AMD,nveob81,That’s where it’s being used mostly. I know this your hobby but surely you can see the bigger picture.,hardware,2025-12-22 18:09:18,-4
AMD,nvfbe4w,yes tech wise is why intel cpus are better. [https://imgur.com/a/CyxoUZM](https://imgur.com/a/CyxoUZM)  On paper the 9955hx and moreso the 9955hx3d look very good but in practice they fall short.   The 9955hx3d and 9955hx are simply lackluster in terms of battery life with the 275hx giving double the battery life and when unpluged the performance of the 9955hx/hx3d tanks which causes the 275hx to pull ahead when not plugged in. Additionally ryzen cpus have more sleep state issues which again hurts the battery life although that could be considered a windows problem and not on amd.   On top of this the intel chips have thunderbolt support and come with better wifi cards which amd refuses to use.   Then on the ultrabook side of things intel lunar lake is just better in terms of having lower powerdraw and a stronger igpu compared to AMDs lower power options although both intel and amd get smacked by ARM chips so not really that important unless you need x86,hardware,2025-12-22 20:04:51,10
AMD,nvi445l,yeah I would say ARM chips are the best for most laptop users but for gaming you still want x86 at least for now.,hardware,2025-12-23 06:18:10,1
AMD,nwpyiem,"There have been over 50 documented from cablemod alone, wtf are you talking about",hardware,2025-12-30 12:35:30,1
AMD,nviydbu,"Yes, I know about that. It doesn't have anything to do with their desktop/laptop class products.  Also, what do you mean by "" they can't have dedicated mobile GPUs under their Radeon branding...""? There are literally dedicated radeon laptop GPUs available.",hardware,2025-12-23 11:05:16,0
AMD,nvgxn08,Does AMD make things that go “woosh” when you need them to?,hardware,2025-12-23 01:30:53,26
AMD,nvkuzin,Strix Halo is way too expensive and still sucks down too much while doing absolutely nothing.    OEMs find it cheaper just to use an Nvidia dGPU with any mobile CPU and the end result is an equivalent or better mobile product except in Cinebench and Blender. It's absolutely AMD's fault for making something that few people want.,hardware,2025-12-23 17:47:27,11
AMD,nvjusfv,You have to go ask amd why they don’t codesign laptop platforms with oems the way intel does with their evo platform. Nobody automatically deserves commercial success just because they exist. If intel invests money into oem platforms then that’s what amd must pay to do.,hardware,2025-12-23 14:47:13,10
AMD,nwpwjsl,Strx Halo alone cost more than entire notebook.,hardware,2025-12-30 12:20:55,0
AMD,nvnoole,"And Intel is closing in on the APU side as well. Lunar Lake was impressive but also a bit expensive and also seems to had some level of supply issue, but Panther Lake is probably rectifying that and being cheaper to make, too.",hardware,2025-12-24 03:13:06,1
AMD,nwpwyes,Strix Halo is manufactured in tiny numbers in comparison.,hardware,2025-12-30 12:23:58,1
AMD,nvj2zzb,Their problems existed far before AI was a thing. Whatever new thing pops up conveniently becomes their newest excuse.,hardware,2025-12-23 11:46:16,3
AMD,nveplis,"And the consoles are a completely different market    AMD do not commit volume to PC  gaming, look at the GPU shipments, a much better indicator    I didn't say they couldn't, I'm saying they dont",hardware,2025-12-22 18:15:41,20
AMD,nvj8xyi,"[Based on Amazon's best sellers](https://www.amazon.com/s?k=amd+nvidia+gaming+laptop&s=exact-aware-popularity-rank), AMD's monolithic APUs are way more numerous than the desktop-repackage CPUs (which makes sense, they're kinda crap for a *laptop*).  But note that I had to put ""nvidia"" in the search to filter for actual *gaming* laptops, and not [""""""gaming"""""" laptops](https://www.amazon.com/NIMO-FHD-Gaming-Laptop-i7-1165G7-GPU-Computer-Fingerprint/dp/B0F549GSX4?th=1) with no discrete graphics and undisclosed CPU model.",hardware,2025-12-23 12:33:17,6
AMD,nvewppd,"Unless you are doing one specific job, which is LLM inferencing, no matter what the hype says, Strix Halo is very poor value performance wise. RTX 5060 laptops are faster and generally half the price.   Nvidia GPUs are then also way better supported in professional applications than AMD GPUs.  And if you really wanted to do LLM inferencing on a ""budget"", a MacBook Pro is more efficient and faster anyway.",hardware,2025-12-22 18:50:31,17
AMD,nvi350i,"No, but that's the reality    They don't commit the volume to compete in the GPU space, never have    Until that changes, Nvidia are PC gaming, a consequence of sufficient supply",hardware,2025-12-23 06:09:49,2
AMD,nvz4sjn,"Well as an amateur photographer I can tell you it’s just one of many reasons. The display of a MacBook is always perfectly calibrated so the colours look correct. They have great battery life so you can work from anywhere. And they have the fastest single core performance of any computer and as a result, Lightroom works a lot faster",hardware,2025-12-26 04:28:36,3
AMD,nvidh49,"On laptop the goal is to have zero power cost for having a GPU, this requires that the GPU get put into D3cold when not used. So idle figure from TPU's article isn't relevant because you wouldn't want to leave the GPU up in an idle state (with integrated graphics scanning out the display), you'd tear it down completely and power it off.  Doesn't work if you want to eg. load stuff in VRAM like running a LLM though, or hook up a display to a port directly connected to the dGPU :/",hardware,2025-12-23 07:43:07,12
AMD,nviz97w,"Oh wow, finally! That is actually great to read!      For more than a decade, AMD cards, like my old R9 280X, just drank down the juice in multi-monitor idle, with sometimes double the power draw of comparable Nvidia cards.      Even a 3080 is frugal in multi-monitor use compared to a 6800 for example.",hardware,2025-12-23 11:13:22,1
AMD,nvijjc4,Thx for providing a source.  TIL.,hardware,2025-12-23 08:41:46,1
AMD,nvhvt6t,Never worked on the creative/film/event management field I guess?,hardware,2025-12-23 05:11:04,11
AMD,nvf12ov,"That’s a very valid student usage of laptops though, and you very conveniently leave out the work from home usage which does involve sitting down somewhere for some amount of time, while still needing the portability to bring the device between home and the office if needed.  A workstation would mean giving each employee two device for hybrid work, or forcing remote usage, neither of which would be an amazing option compared to just giving each employee a laptop to commute with",hardware,2025-12-22 19:12:12,14
AMD,nvju8t2,Discrete GPUs are not meant to be used as display adapters. [Even Windows has CASO](https://devblogs.microsoft.com/directx/optimizing-hybrid-laptop-performance-with-cross-adapter-scan-out-caso/) now. Shut that pig off when it's not rendering something.,hardware,2025-12-23 14:44:15,3
AMD,nvf1ri4,">My 5090 goes from ~40W to 90W by just plugging in a second display while doing nothing  Yea that's not normal, 2 displays have been fine on nvidia cards for... well as long as I've had an nvidia card so 2017, to stay in idle memory speed state and it's only couple of watts more. Maybe if you go high enough it changes like two 4k240 displays or one/both being 1440p500, but 4k240 + 4k144 brief moment i had that combo didn't do it on a 4070ti and was fully idle state. Or you have some power settings cranked up maybe those effect things. Obviously interacting with like browser stuff with 2 high refresh/res monitors does have more brief spikes than a single panel would, but that's not idle.    3 is a bit of mixed bag, but even that it much better than it used to be like ~4-5 years ago, when 3 monitors of any kind was max memory speed state and can still get idle at 3 panels if the refresh rates of all panels isn't high and even then it might go to half memory speed state instead of full speed on some specific combo. I do remember some talk about how the specific model also might affect things due to... something, rather than just pure refresh/res combo. thought that was mainly for 3(+) panels where it mattered, but maybe that could be what's happening even with just 2 panels.",hardware,2025-12-22 19:15:41,1
AMD,nvfgxyn,"Meh, not worth it tbh.  If it were a X3D version, yeah, but basic 370 is basically 3% faster",hardware,2025-12-22 20:33:56,3
AMD,nvesq0e,"I’m not sure what you mean by “bigger picture”, but the desktop GPU experience is why people in this sub would have the opinion that Nvidia on Linux is a poor experience.   It’s only recently (within three years) that Nvidia started releasing open source kernel drivers for their cards on Linux (because they moved their proprietary code into the device firmware). Prior to this (and even still) it wasn’t uncommon for Linux users to boot into a black screen after upgrade due to an Nvidia related problem.  This is different from large ML or HPC clusters which will use licensed drivers and CUDA libraries curated by someone like SUSE or RedHat for a hefty fee.   With regards to procurement, Nvidia has been popular because of how good CUDA is, how powerful/efficient  Nvidia cards have been since Maxwell, and how good their marketing is.",hardware,2025-12-22 18:30:57,5
AMD,nvibl9c,">The 9955hx3d and 9955hx are simply lackluster in terms of battery life with the 275hx giving double the battery life and when unpluged the performance of the 9955hx/hx3d tanks which causes the 275hx to pull ahead when not plugged in.  The problem is that the laptops that those chips go into often don't really care much about battery life at all. Desktop replacement/thick gaming laptops don't really prioritize battery life.   Even more so for the unplugged performance.   For the thinner gaming laptops that might, such as the Asus G16/G14, you see the -H series be used, or even LNL in that one experimental Acer laptop, an entire mobile class of CPUs you ignore in your comment.   With ARL-H, Intel is on par or has a slight lead in battery life, but also has a good bit worse nT perf/watt scaling.   Overall I still think Intel has the better mobile portfolio, but I don't think it's as lopsided as your comment makes it out to be.",hardware,2025-12-23 07:25:12,4
AMD,nvfjih9,"Cant believe i needed to turn on my vpn for that. I did think there would be variance in the test, is there a reason the amd chip doesnt ever reach its max tdp or is it limited? I'd also like to point out the x3d chip isn't on your graph and they behave wildly different.   I guess the thunderbolt is okay ish? And idk what the wifi card thing is about considering how many laptops just use a separate WiFi chip.   >although both intel and amd get smacked by ARM chips  Not surprising its like the whole point of RISC to be specialised it'll only keep getting better.",hardware,2025-12-22 20:47:29,0
AMD,nwq3xab,Cablemod? you mean the guy who was selling faulty cables and had to recall them?,hardware,2025-12-30 13:12:45,1
AMD,nvj2if2,They make the sound when the bicycle in the meme hit the ground,hardware,2025-12-23 11:42:10,2
AMD,nvkfgs4,"So you do know for a fact they don't do that, or is that just you making it up? It's funny that you on one hand deny one theory without much reasoning but take another one for granted.",hardware,2025-12-23 16:30:46,-2
AMD,nwrpqm7,Why does that not stop the tablet if you think that's actually true? Or the silly handheld devices?,hardware,2025-12-30 18:05:29,1
AMD,nveqya6,"Laptops are closer to consoles in alot of ways, tight oem integration, semi custom is also more common, power envelopes and form factor are much similar too.   Also why should we not include AMD's largest consumer GPU segment? Surely if you want to look at whether they can do the volume for an oem you would look at their total track record for oem gpu shipments? Of which the largest will definitely be the shipments to Microsoft and Sony.",hardware,2025-12-22 18:22:17,-14
AMD,nwpx6xv,amazon best sellers lies to you. Its a marketing algorythm.,hardware,2025-12-30 12:25:42,1
AMD,nveyjmu,You wouldn't put the strix halo in a laptop that could fit a 5060 in it i guess then. You'd be putting it in thin and light laptops where you don't really have the power envelope or space for a discrete gpu. So it doesnt really matter what its perf is vs a discrete gpu cause that laptop chassis isn't gonna fit one.  Also on a side note wasn't it really designed for Microsofts god awful Copilot+ PC rollout that flopped heavily i can imagine the inference based APU coupled with another discrete gpu is why?,hardware,2025-12-22 18:59:26,-5
AMD,nvxjblh,"""My boss is irrational and doesn't plan anything out. Just wings everything and expects me to fix it when it doesn't work, with this POS laptop that takes me 8 hours to render a 720p scene, that I should be mastering in 2160p and scaling to maintain quality. But they are too impatient, so everything looks like a streaming video at 240p on YouTube.""   Yeah, and the expectations in those toxic work environments are not based on reality but forced compromise. Because the people in charge hate that they have to use technology...",hardware,2025-12-25 22:03:39,1
AMD,nvev1yg,"I’m saying that the drum beat of bad driver support isn’t the whole picture. If they want something free with no official support they are gonna have issues.  But saying because of that NVIDIA is bad when they have 100,000s of Linux system running the world at the moment is short sighted.",hardware,2025-12-22 18:42:23,-4
AMD,nwpxvuk,linux desktop is a tiny market and is hardly a drop in the ocean when it comes to Nvidia usage in linux.,hardware,2025-12-30 12:30:52,0
AMD,nvfmzgo,[https://imgur.com/a/PmUFBnX](https://imgur.com/a/PmUFBnX) just over double the battery life compared to the 995hx3d and that is a best case scenario for the 9955hx3d because if you are turning the laptop off many times in the day the disparity will grow due to sleep state issues.  The wifi card thing is a hardware thing because intel makes the better wifi cards and while they do have multiple chipset versions(one CNVio that only works with intel cpus and one non-CNVio) Amd still doesn't use them probably due to cost but also there are some driver/hardware issues as even the non-CNVIO Intel BE200 is known to have issues with AMD cpus,hardware,2025-12-22 21:05:52,4
AMD,nwq4sj4,Why are defending a billion dollar company?,hardware,2025-12-30 13:18:14,1
AMD,nx1q3ia,We don't have faulty cables - the angled adapters are what we recalled. Our cables have been great and are used by people around the world. Countless 40 and 50 series GPU owners are using our 12V-2x6 cables.,hardware,2026-01-01 07:15:38,1
AMD,nvkgezz,"....if they do then where are the results? It's not a ""theory"", evo platform is a documented thing intel does with oems.",hardware,2025-12-23 16:35:25,11
AMD,nwpwni6,"> So you do know for a fact they don't do that, or is that just you making it up?  the laptop manufacturers openly stated this, so yes, we know as much as it is possible to know without actually working there.",hardware,2025-12-30 12:21:41,1
AMD,nx1qsb2,It does? The volume of Strix Halo products is miniscule.,hardware,2026-01-01 07:22:29,0
AMD,nvewoos,"They haven't released mobile RDNA 4, that's tells you all you need to know about commitment   If they actually competed on volume, don't you think it would make sense to actually launch some products?  AMD would rather sell them bundled with a CPU for a premium, why their APUs are so prolific",hardware,2025-12-22 18:50:23,23
AMD,nwqvrjp,"I've no doubt that they're fuzzing it up a bit, but it seems unlikely that the bias is in the direction of hiding higher-end laptops. Surely they want you to spend more money?",hardware,2025-12-30 15:45:29,1
AMD,nvf71ma,"Even then you wouldn't though. AMD says the Max+ 395 can sustain 120W and boost upto 140W. That's not really much less than most 5060s do paired with a Ryzen 7 or Ultra 7.   I have an Omen Transcend 14 with a Meteor Lake Ultra 7 and a 4060. The total power budget for them is \~85W because the laptop is powered via a 140W USB-C power adaptor. The 4060 can pull upto 65W. The newer 5060 model can pull 75W because the CPU is more efficient. As a result, my laptop is slower than a full-power 4060 laptop.  But the same is also true for the ZBook Ultra G1A. Also a premium, high-performance 14"" HP. Also runs through a 140W USB-C power adaptor. Also has the same 85W power limit. Is also slower than the ROG Flow Z13 tablet that has a 180/200W power brick with 20-25W higher power budget. It is also not really faster than my laptop for the GPU.  Of course, the ZBook's CPU is a lot faster. I guess a more fair comparison in that sense would be the 385, but that has a slower GPU.   All of that would be fine until you realise that the ZBook costs a LOT more than the Omen and the laptops otherwise are very similar. Similar battery life, same OLED display, very similar keyboard, trackpad and speakers, similar build quality and general handling.",hardware,2025-12-22 19:42:40,13
AMD,nvifhf3,theyre talking about desktop usage. nvidia lacks some features and gets performance drops in some cases in desktop linux.,hardware,2025-12-23 08:02:09,1
AMD,nvft7y1,Oh what laptop was this test ? I assume it was amd vs Intel versions of the same one.   Also the wifi card thing doesnt matter all that much anyways the perf of intel vs qualcomm vs cnvio is really not going to matter much considering they are all compatible with the same standards so you can get a better one if you want,hardware,2025-12-22 21:38:38,1
AMD,nwqk5l4,I see you have no argument.,hardware,2025-12-30 14:46:44,1
AMD,nx24y9w,"That's exactly the issue! But the existence of the tablet product shows they could be made, no problem, if vendors wanted.  Do you forget the random cheap chinese devices with these chips? They definitely aren't too expensive to be viable.",hardware,2026-01-01 09:52:34,1
AMD,nvf0bie,"Yeah what happened to that? Was it power?  >If they actually competed on volume, don't you think it would make sense to actually launch some products?  >AMD would rather sell them bundled with a CPU for a premium, why their APUs are so prolific  I don't want to be cynical and blame AI but I think they were putting their production into datacenter stuff instead",hardware,2025-12-22 19:08:22,2
AMD,nx1qd8s,"the bias is in direction of what they think you might get lured into buying. If their algorithm thinks you are mire likely to buy cheaper laptops they will show you cheaper laptops. Also another point about that list is that they list every SKU seperately, so if a product has many SKUs it will likely never be in the top list.",hardware,2026-01-01 07:18:21,1
AMD,nvf9deh,"The tdp for the 5060 in the transcend 16 is 140W which is more than the strix halo by itself which is what I really meant. I mean its the whole point of a integrated GPU less power, less space, less engineering complexity unified memory so on and so forth.",hardware,2025-12-22 19:54:30,-1
AMD,nvfyiwu,yes same exact laptop the XMG Neo apart from ofc the superior wifi card on the intel laptop and thunderbolt.,hardware,2025-12-22 22:06:16,2
AMD,nwqu12b,"Well since you asked so nicely, here are just a few examples   [https://www.youtube.com/@NorthridgeFix/search?query=4090](https://www.youtube.com/@NorthridgeFix/search?query=4090)   [https://www.youtube.com/watch?v=p0fW5SLFphU](https://www.youtube.com/watch?v=p0fW5SLFphU)  [https://www.youtube.com/watch?v=Y36LMS5y34A](https://www.youtube.com/watch?v=Y36LMS5y34A)  [https://www.youtube.com/watch?v=kb5YzMoVQyw](https://www.youtube.com/watch?v=kb5YzMoVQyw)  [https://gamersnexus.net/gpus/12vhpwr-dumpster-fire-investigation-contradicting-specs-corner-cutting](https://gamersnexus.net/gpus/12vhpwr-dumpster-fire-investigation-contradicting-specs-corner-cutting)",hardware,2025-12-30 15:37:08,1
AMD,nvf187l,">but I think they were putting their production into datacenter stuff instead    As is Nvidia, who are committing more   But despite that, Nvidia still commit enough volume to supply the gaming market",hardware,2025-12-22 19:12:58,10
AMD,nvg3r3x,You seem really hung up on single port type and WiFi card huh,hardware,2025-12-22 22:34:45,0
AMD,nx1puku,"northridge fix? so the guy fixing the cards burned by the faulty third party cable by cablemod? You do realize he is not representative of the market, yes?",hardware,2026-01-01 07:13:12,1
AMD,nvf3oi6,"I mean for a company having volume problems 73% year on year revenue increase in that segment seems pretty damn good. I mean looking at their financials I'm finding your point hard to believe, they had record radeon sales and increased gaming revenue from 0.5 to 1.3 Bn yet they're having volume issues...",hardware,2025-12-22 19:25:25,1
AMD,nvg4ssu,? no I was just saying they are the same exact laptop spec wise except those two parts although those two parts aren't really important in terms of power draw which would make the comparision accurate.   If they instead had different screens like oled vs ips that would cause a difference in power draw.,hardware,2025-12-22 22:40:29,2
AMD,nx1q6o1,"As mentioned above - our cables are not failing, our recall was specific to the angled adapter. Countless people worldwide are using our 12V-2x6 cables, and many have shown that they have more stability using our cable compared to cables that even come stock with the PSU and other third party competitors.",hardware,2026-01-01 07:16:31,2
AMD,nwloj0c,Please don’t tell me they’re going to slap the ugly HyperX logo on the lid,hardware,2025-12-29 19:55:56,18
AMD,nwm66sz,"Seems like the OLED displays and the inclusion of the 9955HX AMD chips are the big changes for the next year.     Shame the most powerful AMD CPUs are not going to be paired with the best GPUs and cooling in the Max systems.     HP used the 8940/45HX AMD chips in the regular Omen this year already but it's a strange mismatch putting that with a 5060 and a weaker thermal system that requires a power limit restriction. At that point, why bother with a 16 core CPU that's going to be hamstrung from the beginning?",hardware,2025-12-29 21:22:28,7
AMD,nwlzyl4,"Kinda weird to have 15"" relaunching when they have dedicated 14"" and 16"" lineups",hardware,2025-12-29 20:52:24,5
AMD,nwp3xir,just wish the thermals don’t implode and prices don’t make us cry,hardware,2025-12-30 08:04:48,3
AMD,nwozxw1,"wait what, Omen 15? whats gonna happen to the Transcend 14 then  having a 14, 15, *and* 16 inch laptop feels way too close with each other",hardware,2025-12-30 07:28:20,2
AMD,nwlxflb,Tasteful laptop design.   Difficulty: nearly impossible.,hardware,2025-12-29 20:39:59,11
AMD,nwvpp7c,I have never seen the HyperX logo before. Maybe they'll only put the OMEN branding.,hardware,2025-12-31 07:50:11,2
AMD,nwuks1r,the chiplet design from AMD are trash in mobile anyway due to their high idle power consumption.   You almost always gonna pick Intel for the same price or if the premium is small enough.,hardware,2025-12-31 02:53:08,7
AMD,nww2ud7,9955HX is worthless if not paired with 5070ti and up.,hardware,2025-12-31 09:55:07,2
AMD,nwsroc8,"> the most powerful AMD CPUs are not going to be paired with the best GPUs and cooling  I personally think is a shame that the 3xx HX product line isn't more widely available in gaming laptops(probably due to AMD's poor supply). Since going laptops with a ryzen 9xxxHX are more of a desktop replacement machine, due to the TERRIBLE battery life. To me it makes no sense buying a laptop to have it connected at all times for twice the price of a more performant desktop alternative.",hardware,2025-12-30 21:04:41,2
AMD,nwqn2se,>why bother with a 16 core CPU that's going to be hamstrung from the beginning  Well. Considering this has been happening for a long time. I would go for the simplest answers,hardware,2025-12-30 15:02:09,1
AMD,nwvpul7,Yeah and that too after 4 years! Why did they cancel it in the first place?,hardware,2025-12-31 07:51:33,1
AMD,nwv32tv,"From what I’ve heard they’re discontinuing the Victus line so the 15 will be a lower end model with worse build quality and maybe a couple features missing.   The Transcend 14 is a premium model. The Transcend is also only 140W USB-C so the GPU is power limited. The 15 might use the ugly barrel jack. Of course, a 240W USB-C could fix it, but it’s not happening",hardware,2025-12-31 04:48:56,1
AMD,nwqtwhf,> the simplest answers  Because the windows laptop market is so fragmented it’s broken.,hardware,2025-12-30 15:36:31,3
AMD,nwvq0lh,Why do companies keep doing this to us... It's almost like we aren't allowed to have nice things.,hardware,2025-12-31 07:53:05,1
AMD,nwwtfny,"That's too bad.  15 inches is the right size for a laptop that has a screen big enough to play on yet is small enough to carry around without much inconvenience.  16 really lends itself to living on the desk, and 14 is portable but you usually have compromises on thermals, battery and most importantly screen size.",hardware,2025-12-31 13:33:14,1
AMD,nw1bmjg,Will it come with vram,hardware,2025-12-26 15:35:35,279
AMD,nw1goyh,"I thought this was gonna be UDNA, not RDNA5.",hardware,2025-12-26 16:03:11,103
AMD,nw1na46,"Makes sense why sony want release ps6 in the end 2027,iirc rdna2 released in fall 2020 as ps5",hardware,2025-12-26 16:38:23,48
AMD,nw1bjrk,I'd be surprised if it didn't.,hardware,2025-12-26 15:35:09,35
AMD,nw1tmlg,18 months out the rumour might as well be a nothing burger.,hardware,2025-12-26 17:11:56,20
AMD,nw1cqmn,I’ve been hearing 2027 more often from the rumor mill sources and it’s incredibly disappointing. 2026 should have been a banner year for PC parts and now it’s looking like just the new CPUs are launching and nothing else,hardware,2025-12-26 15:41:46,42
AMD,nw1g4rx,in 2 weeks : OpenAI just bought all RDNA5 GPU stock until 2030,hardware,2025-12-26 16:00:12,9
AMD,nw20y8a,Completely depended on ram avaliabllity,hardware,2025-12-26 17:50:33,3
AMD,nw2j8ht,"RDNA6 rumored to launch in mid-2029. There you go videocardzzzzz, no need to post one more rumor about it. 🤣",hardware,2025-12-26 19:25:50,10
AMD,nw1vvrf,Oh neat.  More than a year and a half away.  Must be a slow news day.,hardware,2025-12-26 17:23:53,7
AMD,nw1vwce,"Cool, just in time for them to banish RDNA 3 and 4 to the maintenance branch.",hardware,2025-12-26 17:23:58,8
AMD,nw45xis,"Mid-2027 feels far, but at least it gives RDNA4 time to shine first.",hardware,2025-12-27 00:58:52,2
AMD,nw1c3dy,"Considering the current situation we are in, RDNA 5 GPUs will be BYOV (Bring your own VRAM)",hardware,2025-12-26 15:38:11,4
AMD,nw3he4q,"Damn, it's gonna be the new Vega, isn't it.",hardware,2025-12-26 22:32:52,2
AMD,nw34krp,GPUs have a two-year cadence. It's obvious that the next generation will come H1 '27. Until then we might get some refreshes from team red and green while team blue is on track to actually deliver some proper mid range GPUs.,hardware,2025-12-26 21:22:35,3
AMD,nw20e7q,4090 level raster perf and I box up my 4090s and buy these.  Nvidia drivers on Linux have me at wit's end,hardware,2025-12-26 17:47:39,-1
AMD,nw4c5cb,"This will get pushed back. High speed memory will still be in shortage and expensive.  There will be a refresh of RDNA4. Most likely, with more and faster GDDR6 memory. That is doable.  9070XTX or 9075XT 24Gb with 22 or 24Gbps GDDR6 memory. They can use the 9700AI pro board, but just fit lower density memory to half the board 16+8 = 24. Still on a 256bit bus.  The low density ram won't be very expensive.   UDNA seems to be looking more and more like UDNA=RNDA5. AMD is putting a lot of effort into making RDNA the majority of the future for AI. The AI 9600Pro is a great example of that.    AI 9600 Pro with 64GB memory would be nice. That would push AMD into more data centre spaces.    I also think top end AMD graphics will be back on HMBe..",hardware,2025-12-27 01:38:22,0
AMD,nw1nmgn,So AMD lied about RDNA4 not competing on high-end in order to launch RDNA5 faster than Nvidia's 60 series,hardware,2025-12-26 16:40:11,-10
AMD,nw1bc0m,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-26 15:33:57,0
AMD,nw1cr61,That’s DLC,hardware,2025-12-26 15:41:51,159
AMD,nw1il6b,"128bit is 4x 32bit chips to 4x vram controllers in gpu. 3GB gddr7 for 12GB.    192bit is 6 chips. 18GB.   256bit is 8 chips. 24GB.   384bit is 12 chips. 36GB.   512bit is 16 chips. 48GB.    They’ll do 256bit for the xx70 gpu and 128bit for the xx60 gpu as that is the most efficient design to keep the gpu chip smaller. The more vram controllers in the gpu, the larger itll be in area.   If they decide to go “5090” big, it’ll be interesting. They would want to go for 512bit gddr7 to compete but 384 may be possible if they dont wanna go that huge.   But more overall vram is likely, but more vram chips is not. Theyll use the same # of ram chips but at the 3GB density.   I dont know why i typed all of that nonsense… my brain got the tunnel vision and i had to do it. I need my coffee. Byeeeee",hardware,2025-12-26 16:13:19,42
AMD,nw2q8xz,"It's FSR 5, which eliminates VRAM using AI.",hardware,2025-12-26 20:04:00,4
AMD,nw1gtcr,yes. you can subscribe to 1Mb monthly vram for $50,hardware,2025-12-26 16:03:50,20
AMD,nw3me14,With a subscribtion for RAM. Unlock what you need.,hardware,2025-12-26 23:01:35,3
AMD,nw229uc,"Actually, maybe no. Some rumors say lower end parts will actually come with DRAM instead of VRAM. There  have been claims from like AMD or Mark Cerny or someone, that also said they are trying to drastically reduce memory bandwidth requirements of GPUs. So there might be truth tot hat rumor. It's also something they absolutely need to do if they want to create huge APUs for laptops, or other mobile gaming platforms like the Steam Deck 2 or rumored PS6 handheld. Maybe the PS6 even, will use DRAM instead of VRAM.",hardware,2025-12-26 17:57:26,4
AMD,nw21ek2,"No, we’ll have to download it from OpenAI’s Stargate monopoly.",hardware,2025-12-26 17:52:55,2
AMD,nw3r6e0,"I wonder why AMD and NVidia do not just put the RAM in the chip? Doesn't the M5 chip and the Strix Halo chips have the memory right on the die? Seems like they should be able to just do this with discrete GPU's as well? I know it's not simple and would take years to do, just wondering why they have not done it already.",hardware,2025-12-26 23:30:16,2
AMD,nw1nqy1,Honestly don't know which is which at this point,hardware,2025-12-26 16:40:49,61
AMD,nw24pac,From what I can tell udna and rdna5 are referred to interchangeably in leaks. We will likely have to wait for official confirmation to be reasonably sure about either.,hardware,2025-12-26 18:10:02,44
AMD,nw2ia54,It's gonna be some kind of DNA,hardware,2025-12-26 19:20:44,13
AMD,nw5irc4,No one knows not even AMD. At FAD they didn't even commit to a name. Nothing. This confusion will prob continue until Cerny does his Road to PS6 in 2027 :(,hardware,2025-12-27 06:41:54,4
AMD,nw74fkz,"UDNA launched already, as amdgpu-spirv. It’s AMD PTX, not an actual uarch.",hardware,2025-12-27 14:51:54,1
AMD,nw3n4py,UDNA and RDNA5 are the same thing,hardware,2025-12-26 23:05:58,1
AMD,nw3kmip,"Yes, let's hope the memory shortage doesn't ruin those plans.",hardware,2025-12-26 22:51:18,11
AMD,nw4jb7l,"You're on crack if you think the PS6 is coming in 2027 lol. Memory shortages mean Sony will be looking at at 12 month delay at a minimum, possibly more if things don't improve. You can't release a next gen console with the same, or maybe 24GB Vram, that would be a pathetic jump.  Sony will want this console to last 10 years at least.",hardware,2025-12-27 02:24:12,5
AMD,nw63uls,"AMD Radeon alone is nothing burger, unless AMD increase their GPU production by 5 times. Given how AMD only hold 7% dGPU market share now, so yeah 5 times just about right.",hardware,2025-12-27 10:04:33,3
AMD,nw4skc8,you can say that again,hardware,2025-12-27 03:24:42,1
AMD,nw1eo8w,There will be a horrible DRAM shortage likely for the entire year.,hardware,2025-12-26 15:52:21,81
AMD,nw1h9lx,Why? They always release it in 2 years cycle. RNDA 4 is 2025.    Same for RTX 6000. Why everyone talks like it's a big news,hardware,2025-12-26 16:06:15,48
AMD,nw3ktmv,Why would 2026 be a banner year? RDNA4 and Blackwell just launched this year. Normally it's a 2 year cadence for new GPU architectures.    2026 will be great from a CPU perspective. We get Zen 6 and Nova Lake.,hardware,2025-12-26 22:52:26,10
AMD,nw1iv3h,Would you prefer they launch in peak memory hellscape?,hardware,2025-12-26 16:14:48,15
AMD,nw8cehn,And basically nobody in the DIY market will buy a new CPU without some new RAM to put into a new motherboard...,hardware,2025-12-27 18:38:27,1
AMD,nw4izvp,"Going to be a ""slow news"" year. Youtubers are going to suffer with really desperate content in the next year, since there will be very minimal hardware launches.",hardware,2025-12-27 02:22:08,9
AMD,nw4j6s2,"It's going into virtually everything from the PS6, and PS6 handheld, all the way to next generation Xbox. Maybe even Steam Deck 2. It can't be Vega.",hardware,2025-12-27 02:23:23,6
AMD,nw39pml,"Only Nvidia ever had any routine two year cadence and even they've gotten off that a bit with the 50 series.    AMD have a very mixed record and never followed any real routine.  Sometimes they'd release just like one new GPU in a year on a new architecture, and then the next year they'll only release low-mid entries on a new architecture, and so on.    There's no rule that says we couldn't start seeing more like 2.5 years between releases.  Or whatever.  There's no rules about this stuff at all.",hardware,2025-12-26 21:50:19,7
AMD,nw518t8,">Nvidia drivers on Linux have me at wit's end  It's been pretty stable for a while, like two years.",hardware,2025-12-27 04:24:09,3
AMD,nw4jlxw,">9070XTX or 9075XT 24Gb with 22 or 24Gbps GDDR6 memory.  How would they do that, if 3GB GDDR6 memory modules does not exist, and no one is investing in developing it?  What they could do is a create a whole new 500mm^(2) die with a 384 bit bus, 12 memory modules, and 96 CUs of RDNA4. They have claimed that RDNA4 is pretty modular in design, so it wasn't hard to double the 9060xt design, to create the 9070xt. It's really double the chip. So 96 CUs would not be difficult for them.",hardware,2025-12-27 02:26:08,1
AMD,nw1w1tn,"I feel like AMD teases the possibility of releasing before Nvidia every gen since RDNA2, and then they always do the Nvidia - $50 approach.",hardware,2025-12-26 17:24:46,15
AMD,nw22zrl,"AMD will never, ever, launch their products before Nvidia, unless they know for sure how big the performance gap and Nvidia pricing is. Because it will be a disaster for their already weak sale, if they get undercut by Nvidia.    You already know this year they had to delay and then change the price of the 9070xt down to 600$ from 700$, because the 5070ti was 50$ cheaper than they expected",hardware,2025-12-26 18:01:10,11
AMD,nw1x7qx,Where did AMD say that?  Got a quote?,hardware,2025-12-26 17:30:57,8
AMD,nw2d0jj,What are you even talking about? There is no RDNA 4 product that competes with high end Blackwell.,hardware,2025-12-26 18:52:54,5
AMD,nw1ewll,I emailed https://downloadmoreram.com and asked if they would add VRAM to their lineup to help with the shortage.,hardware,2025-12-26 15:53:34,71
AMD,nw5yerp,PC brought to you by Paradox,hardware,2025-12-27 09:10:47,2
AMD,nw1vu5p,"People make this sort of argument in many generations that they'll use these 'in between' memory capacity chips and they just never really do.  Cuz of the lower volume, they tend to not be great in terms of cost per GB compared to high volume memory chips.    Basically, your whole premise relies a lot on Nvidia wanting to give us better VRAM capacity and selling us midtier GPU's with midtier naming and pricing(and low=low, high=high).  That's a lot of bones Nvidia will supposedly throw us in a time when everything is more expensive for them.   I just dont see it.  I think we'll get a continuation of what they're doing now, just with higher prices.  They're not gonna suddenly start to care less about having good margins and there's no indications consumers are ever gonna stop buying their GPU's.  They've got no incentive to be nice to us.",hardware,2025-12-26 17:23:39,18
AMD,nw1uhag,"I would love to see more VRAM but that would mean more expensive cards, which the original question was referencing due to the memory price surge.",hardware,2025-12-26 17:16:26,10
AMD,nw22kb1,> 512bit is 16 chips. 48GB.   you really think you're gonna see $1000 worth of VRAM on an AMD card?,hardware,2025-12-26 17:58:57,12
AMD,nw6814q,"Once you've gone with a giant bus for the top die it's difficult to go back since professional clients expect VRAM to go up every generation. 512 on die and maybe 448 for the 6090 if yields are bad.  Also would expect all the other buses to go down and the gap between top and mainstream to widen. If you look at rumored RDNA5 / PS6 / NextBox specs all the buses are down per tier. PS6 is down to 160-bit from 256-bit on PS5. Buses don't scale as well as logic with node shrinks so if you keep the same bus then more and more of the die will be the bus until the bus is bigger than logic. That's why the 70 class used to be 256 and is now 192, 60 class went from 192 to 128.  Also you can see this when Sony and AMD are talking about 'Universal Compression'. You need to work around this and make the uArch more bandwidth efficient.",hardware,2025-12-27 10:45:40,5
AMD,nw2u7rl,"There was a rumour that RDNA 5 would top out at 384 bits, matching with the rumour it would not be faster than the 5090.",hardware,2025-12-26 20:25:49,2
AMD,nwiyr86,Screw GDDR! They can go with 512bit of LPDDR5X at this point. Just put some decent SRAM L2 cache onboard and get \~600GB/s but with insane capacity,hardware,2025-12-29 10:58:48,0
AMD,nw2r4en,"Maybe RDNA5 is just a video decoder you can connect to a data center with for cloud gaming!  But only for 100 hours a month, more would be an unhealthy habit.",hardware,2025-12-26 20:08:48,7
AMD,nw1n4ot,That's Nvidia's plan.,hardware,2025-12-26 16:37:35,9
AMD,nw3k991,It increases in memory every month by 10% but if you miss a subscription it goes back down to 1Mb.,hardware,2025-12-26 22:49:10,0
AMD,nw5iisn,"It's basically a Zen strategy for RDNA5. Design costs are outragious on N3, so it's better to share GPU chiplets with mobile instead of designing seperate for each market. Makes sense to me.  Considering all the stuff they're prob working on for the first time mobile GPUs might finally not be completely BW choked and Medusa Halo could be insanely powerful.",hardware,2025-12-27 06:39:50,2
AMD,nw3sb0n,Both have had HBM SKUs,hardware,2025-12-26 23:37:07,1
AMD,nw3dyuz,Expensive. It will be expensive. It has been estimated Rubin desktop comes earlier in 2027 which will see a healthy price increase across the board.,hardware,2025-12-26 22:13:28,18
AMD,nw2czrj,"AMD was pretty clear on this when they announced UDNA last year. The exact wording was:  ""So, part of a big change at AMD is today we have a CDNA architecture for our Instinct data center GPUs and RDNA for the consumer stuff. It’s forked. Going forward, we will call it UDNA. There'll be one unified architecture, both Instinct and client [consumer]. We'll unify it so that it will be so much easier for developers versus today, where they have to choose and value is not improving.""  The only way RDNA5 makes sense would be as informal codenames for a group of products that were in the planning phase of development before this decision was made. I seriously doubt there will be any formal RDNA5 branding on anything, that would be like if they had called the Radeon 5700 XT ""GCN6"" rather than""RDNA1"".",hardware,2025-12-26 18:52:48,34
AMD,nwjee9x,"Argh. They kept making this big deal about UDNA being this glorious future, they really need to commit.",hardware,2025-12-29 13:03:50,2
AMD,nw5k19m,"24GB is fine if they fully lean into nextgen paradigms (neural textures, neural rendering (MLPs) and procedural content) and API (workgraphs + clean slate API overall).   12.5GB -> +22GB if they include DDR5 similar to PS5 Pro. But rn most rumour suggest it'll be 30GB so yeah zero chance it'll be 2027. Sony can't commit to anything until this current mess has a real end in sight.",hardware,2025-12-27 06:53:17,8
AMD,nw4nv5j,"Sony could have signed contracts for components before the price increase, and the supplier will be obligated to sell them at the price at the time the contract was signed,at least for several million units",hardware,2025-12-27 02:53:32,2
AMD,nw1f51v,Indeed. The big three are building capacity but the most serious chunk of that is being allocated to enterprise to ensure they can control the price of consumer memory.,hardware,2025-12-26 15:54:52,18
AMD,nw2fhpu,Well be lucky if by Black Friday 2027 RAM prices go back pre surge pricing.,hardware,2025-12-26 19:05:51,3
AMD,nw3kws5,Won't be over even in 2028 but supply and logistics should be less bad by then.,hardware,2025-12-26 22:52:56,1
AMD,nw1glrx,Make it the rest of the decade.,hardware,2025-12-26 16:02:43,-6
AMD,nw1wo6e,"RDNA4 in 2025 already bucks the '2 years' cycle since RDNA3 was 2022.  People were kind of hoping that with RDNA4 being something of a 'stopgap' product line/architecture, that RDNA5 would have gotten things back on track to the same 2 year cadence as before.",hardware,2025-12-26 17:28:04,12
AMD,nw1nc4n,AMD bad.   Need more RAM.,hardware,2025-12-26 16:38:40,7
AMD,nw89y8r,gotta generate clicks and engagement somehow.,hardware,2025-12-27 18:26:18,1
AMD,nw1x197,There's no suggestion that things will be any better by mid 2027.,hardware,2025-12-26 17:29:59,2
AMD,nw1me62,"Yes, because then that would allow a good half a year for prices to fall as few will buy at inflated prices. GPUs already aren’t selling now at inflated prices, and they certainly won’t in 2026 when AMD and Nvidia jack up prices due to claims of a memory shortage",hardware,2025-12-26 16:33:41,-6
AMD,nw3rpwv,50 series was delayed by 1 quarter.,hardware,2025-12-26 23:33:34,3
AMD,nw3ax75,"Its what I got, it's enough for me for the rest of my gaming ""career"", and the drivers won't be an afterthought to the people with access to meddle with them",hardware,2025-12-26 21:56:53,0
AMD,nw587q4,"I am happy to hear that, truly.   For three years it has been hit or miss.  There is always something off.  Currently its primary weirdness is my two portrait screens coming back from standby, or much less frequently upon first boot, in landscape along with poor CP 2077 perf.    The funny thing about Cyberpunk is it was fine until they got frame gen into the last Ubuntu driver update a number of months ago.  Now it's a stuttering mess.   there is a laundry list of shell game/whack-a-mole items that have come, gone, came back, maybe got fixed again.   I have this problem on a two separate 4090 rigs on two different hardware platforms (5800x3d/x570 and 9800x3d/x870e) and the jank is normally quite the sameness for a given distro and update level... i am bad about abating distros that have fallen into disuse.   I am hardly a linux pro but I have a comfort level with it.  I will happily shelve the 4090s when the aforementioned conditions have been met.",hardware,2025-12-27 05:14:54,-1
AMD,nw59wvj,">How would they do that, if 3GB GDDR6 memory modules does not exist, and no one is investing in developing it?     There are several ways to do it.  * Use the Ai Pro 9700 board, but fit only 24Gb. 8 x 2Gb on one side, and 4 x 2Gb on the otherside. The issue of this is halving the bus size for that last 8 GB. So unlikely. * Use the Ai Pro 9700 board, but fit 8 x 2Gb on one side and 8 x 1Gb on the otherside.  1 Gb chips already exist. * Just software lock out the last 8GB.    The board exists, its already double sided, this would be easy and cheap to do. 1Gb chips would be pretty cheap I imagine. Partners already have this board in production and in supply, it would be very low risk for them. It uses parts that are already in the supply chain. Even if they fit it with faster memory, that likely be shared with other products and historically that is what AMD has done. Faster GDDR6 has been around for years at this stage. No new drivers, just a bit of bios code change. Chinese remanufacturers already do this with Nvidia products without manufacturer support.  A whole new die would be expensive. They can certainly do that but that would take years and create an expensive and niche product.",hardware,2025-12-27 05:27:54,0
AMD,nw39497,"I dont think they've ever done any such thing.  Again, can y'all show me the quotes where AMD are saying these things?    I think y'all get all mixed up with rumors and somehow they turn into facts in your heads and somehow it's what AMD said and not just some random person on the internet.",hardware,2025-12-26 21:47:09,-5
AMD,nw3j65y,"These guys are awesome, what with this crazy memory shortage I don't know what we would do without downloadmoreram.com.   If you're downloading some ram don't forget to use code F*cktheRamCartel at checkout for a sweet 13.37% off your order.",hardware,2025-12-26 22:42:58,11
AMD,nwivw41,I can only find DDR4 there :'(   I wish they had DDR5 as well.,hardware,2025-12-29 10:32:50,1
AMD,nw2xhrf,3GB density chips are the new kid in town for gddr7. By mid 2027 it may be the primary density for mass production the same way that 2GB replaced 1GB chips a few generations ago and they stopped producing 1GB chips.   But if 2GB density is still available and significantly cheaper theyll do that and save the 3GB for their pro lineups. It does depend on the cost and whats got the most active production line for gddr7,hardware,2025-12-26 20:44:00,9
AMD,nw5h724,It's the new standard. I don't think they'll bother with 2GB except for very low end. 3GB modules are also much faster on path to 40gbps and a little further out we get 4GB modules. Not inconceivable that future x60 tier cards go down to 96bit bus. Really depends on how much AMD and NVIDIA can improve cachemem efficiency and little compute progression there is.,hardware,2025-12-27 06:28:09,2
AMD,nw2fam2,I can see a 2028 RDNA 5 refresh with 4GB modules.,hardware,2025-12-26 19:04:48,2
AMD,nw5hioo,"They'll just gimp mem PHYs and offer cards with a little more VRAM. Look at rumoured 18GB AT2 card, although there will prob be a refresh based on 4GB to satisfy the VRAM crowd as u/Dangerman1337 said.",hardware,2025-12-27 06:30:59,1
AMD,nw24thg,"Sure why not, they'll just put it on the pro lineup and charge double",hardware,2025-12-26 18:10:38,13
AMD,nw3k5ii,"Yea well no one predicted the memory shortage not even Samsung, Micron and friends.",hardware,2025-12-26 22:48:35,5
AMD,nw5hoz9,"That's based on the cut down AT0 config. The full AT0 die caps out at 512bit like 5090, well at least according to rumours.",hardware,2025-12-27 06:32:30,2
AMD,nw3jyi0,No it'll be faster than 5090 but it won't be faster than 6090. The next gen Xbox is supposed to reach above 5070ti performance and that'll be a smaller chip on a 192bit bus. Plus RDNA5 is getting big architectural improvements and a die shrink unlike RDNA4 that used the same node as high end RDNA3.,hardware,2025-12-26 22:47:28,1
AMD,nw39aka,"There's no way the architecture is exactly the same imho, as the typical workloads on CDNA are too different from those on RDNA.",hardware,2025-12-26 21:48:04,7
AMD,nw2mm7z,It was written at one point that RDNA is still the gaming architecture (as used in APUs as well) and CDNA the computing architecture but the new platform combining both is UDNA (RDNA5 + CDNA4 = UDNA1) while GCN was a different architecture to RDNA,hardware,2025-12-26 19:44:08,9
AMD,nw8t5fw,"Yeah I still think the PS6 is Sony's last traditional 'console'. Everything will be cloud based beyond 2035-2040. They'll want to make this console last a really long time....ie more beefy CPU and RAM initially, with a GPU upgrade 4-5 years down the line.   With the PS5 still selling like crazy, what incentive does Sony even have to launch a PS6 at all until 2029?    I can't see one tbh, devs are only starting to really push the PS5 now.",hardware,2025-12-27 20:05:29,2
AMD,nw8snln,"They wouldn't have signed contracts more than 2 years out for a console lol. They're still designing the hardware, let alone ordering mass quantities of compliments for it. Again, don't expect PS6 till Q4 2028 at the earliest. Distinct possibility of it being a 2029 launch now as well.",hardware,2025-12-27 20:02:49,2
AMD,nw5jq22,Very unlikely. The G7 memory chips they need have barely begun real HVM. Rn limited to RTX PRo cards and select Mobile on NVIDIA side + you don't sign LTAs this early on.,hardware,2025-12-27 06:50:29,1
AMD,nw2b8gt,And if they are reinvesting everything - who knows what will happen once the bubble bursts.,hardware,2025-12-26 18:43:45,2
AMD,nw1hfzk,"Nah, this too shall pass. The current prices are too big of an opportunity, and no-one can lock up dram with patents. Either demand will fall, or supply rises until margins come back down.",hardware,2025-12-26 16:07:12,19
AMD,nw1z48c,RDNA3 was released in DECEMBER 2022. Calling it 2022 is misleading,hardware,2025-12-26 17:41:02,21
AMD,nw3nv3e,"AI isn't just unprofitable, it's bleeding money at a ridiculous rate. Another year/4 quarters of massive losses on AI should hopefully lead to *something* changing.   I can't imagine companies just throwing billions into a blender for longer than another year. Microsoft is already scaling back.",hardware,2025-12-26 23:10:21,2
AMD,nw1vyau,"Claims, lol",hardware,2025-12-26 17:24:15,8
AMD,nw3mugx,"Nvidia already announced a next gen vera rubin successor to the rtx pro 6000 with 128gb vram for late 2026. At 512 bit, it should have 4gb gddr7 vram chips. leaked sk hynix roadmaps also showed higher capacity vram chips becoming a thing iirc so it makes sense.",hardware,2025-12-26 23:04:17,8
AMD,nw37r1q,">3GB density chips are the new kid in town for gddr7. By mid 2027 it may be the primary density for mass production the same way that 2GB replaced 1GB chips a few generations ago and they stopped producing 1GB chips.  Standard chip capacity goes up in doubles.  3GB is new, but it does not mean it's going to be the new standard.  We had 1.5GB chips as well in between 1GB and 2GB.  These sorts of in-between capacity chips dont tend to get used in consumer GPU's.  Not saying they absolutely wont here, just not sure why people are so incredibly confident they will.",hardware,2025-12-26 21:39:47,3
AMD,nw5hw5z,"That'll be very easy to accomplish. It'll prob match or beat 5080. GFX13 = Massive architectural overhaul, N3P and 70CUs.  Very interested to see what RDNA5 is about. Just a shame it's prob +2 years away unless VRAM mess sorts itself out in early 2027.",hardware,2025-12-27 06:34:17,2
AMD,nw4mkyp,It won't AMD are incompetent,hardware,2025-12-27 02:45:18,4
AMD,nw3xazq,What makes you think any of this?did a reputable leak something about this,hardware,2025-12-27 00:07:19,2
AMD,nw3kjx4,It won't be exactly the same but there will be more AI tech in the gaming version that would have normally been exclusive to cdna.,hardware,2025-12-26 22:50:53,5
AMD,nw4jco7,"I was under the impression it was on a logic block basis. For example, in the compute market you obviously don't need video out, texture mappers, RT hardware and the like, but you do need SIMD cores, a memory controller, and matrix math cores. With the split architectures they were pulling these universally required blocks from two different bins. A unified architecture means advances for one use can still benefit the other (say a really awesome scheduler or something). If they really double down on chiplets and can optimize things in ways that aren't obvious to someone like me, they might even be able to share some amount of silicon (obviously not all, but I think it can be more than just SRAM).",hardware,2025-12-27 02:24:28,4
AMD,nw5j2px,Then AMD does what NVIDIA does. Makes a shared foundation with extensions on top (logic blocks and cache customizations) for gaming and HPC respectively.,hardware,2025-12-27 06:44:42,5
AMD,nw32ytp,"I don't personally recall seeing that and very well might have missed it, but it does seem to run counter to the UDNA announcement.",hardware,2025-12-26 21:13:54,4
AMD,nwbhkjy,"You're prob right, especially considering how good GFN has gotten and how hard most normie gamers have with distinguishing frame-gen on vs off in latency. Add a controller to the equation and that becomes even less relevant.  We'll see just remember that prob even more so than PS4->PS5 PS6 has a very long crossgen period extending well into 2030s before it can shine, even if it launched in 2027 which seems next to impossible rn.",hardware,2025-12-28 05:31:59,3
AMD,nw2gzxk,"> if they are reinvesting everything  They are not.  We know from the earnings calls of those big memory suppliers that they are very skittish about building out capacity.  Which also tells you that this shortage is, in their eyes, **very** temporary.  Otherwise, they'd be on the hook for huge lawsuits as shareholders could sue for the missed opportunity.  I have a hunch that DRAM pricing will be a _lot_ better end of 2026 and basically where it is today in mid-2027.",hardware,2025-12-26 19:13:50,11
AMD,nw1iunm,"Or DRAM makers suspect this to be a bubble and will gladly take the money now but won't speed up production. If you build fabs beyond what was already planned you have to know they'll still be needed 5+ years from now, not just right this moment.",hardware,2025-12-26 16:14:44,13
AMD,nw1wdbo,"DRAM makers do not want to overreact and build up a ton of new capacity to meet demand, leaving themselves extremely vulnerable if that demand ever drops.  Why put themselves at such risk when they can just jack up prices 300%?",hardware,2025-12-26 17:26:28,3
AMD,nw1sr49,"Exactly, we have seen this before just not to the extent it is now. Vendors will ramp up production and suddenly there will be a market change and prices will crash. This cycle will likely take longer but it will happen.",hardware,2025-12-26 17:07:18,2
AMD,nw2flj8,RDNA 3 was supposed to be earlier than December 2022 originally.,hardware,2025-12-26 19:06:25,14
AMD,nw38dym,"Oh good lord. smh  Either way, it's a terrible claim.  Nvidia are the ones that (used to) do a 2 year cycle.  AMD has been all over the place, often only doing like one or two new GPU's per new architecture, which could sometimes release every year or so.",hardware,2025-12-26 21:43:16,-1
AMD,nw1wool,"Prices went up before supply lost. This is similar to how the middle men of gas and energy will jack up prices of gasoline the same day that crude prices spike. That’s not supply and demand, that’s just market manipulation. We haven’t lost DRAM supply at all, either.",hardware,2025-12-26 17:28:08,0
AMD,nw3ct85,Historically maybe...  If memory stays expensive companies would be more willing to produce in between capacities at comparable $/GB. Because the difference between boards populated with 3 or 4 GB chips would balloon.,hardware,2025-12-26 22:07:04,5
AMD,nw40pfa,"Because there no mention of 4GB production or roadmap.  Right now, 3GB is live in mass production, and they’re already sampling higher speed 3GB chips at samsung, hynix, etc. they’re set on producing 3GB chips. They’re committed to producing 3GB for years. Nobody committed to 1.5GB that’s why it never was a thing it was better to go to 2GB. Nvidia and amd will choose whatever the ram conglomerates choose to make and right now, its 3GB.   https://www.techpowerup.com/news-tags/GDDR7  You can see the news links when scrolling. 3GB is going to stick around for sure. 4GB has no confirmed production yet",hardware,2025-12-27 00:27:35,5
AMD,nxp27we,"Yeap, I mean at this point I'd be fine with Nvidia only releasing the 6090 and 6080 if those are more palpable at high MSRPs than waiting till they can release a 6050 and 6060.   Same with AMD.",hardware,2026-01-04 21:10:12,2
AMD,nw5i5u8,SW and marketing wings are run by clowns. The HW team is very capable. Look at RDNA4 and RDNA5 will be the first time they actually bother to put in the work. More budget and for consoles so it'll be good.,hardware,2025-12-27 06:36:39,1
AMD,nxp0joh,It's from Kepler who's very good about AMD Radeon leaks.,hardware,2026-01-04 21:02:23,1
AMD,nw5hz5y,"No MLID and then Kepler\_L2 didn't contradict any of it. Very early on, hopefully we get more certain and final leaks in 2027.",hardware,2025-12-27 06:35:01,1
AMD,nw5jbwk,"Basically what Kepler has already alluded to. Honestly surprised AMD took ML this seriously with RDNA4, but RDNA5 is probably the first time they built an architecture from the ground up around AI, so yeah a ton of learnings and low hanging fruits from CDNA for sure.",hardware,2025-12-27 06:46:56,5
AMD,nw2ov4m,They could not be sued by shareholders by not increasing capacity. That’s not how fiduciary duty works as they are protected by the business judgement rule.,hardware,2025-12-26 19:56:26,9
AMD,nw2lnke,"Good - And I hope we won't see a build up of coal and other trash sources of energy for ""AI factories"".",hardware,2025-12-26 19:38:51,0
AMD,nw5k95x,Same thing with RDNA 4. Really dissapointing how these launches keep getting pushed back.,hardware,2025-12-27 06:55:15,6
AMD,nw25j0b,This ain’t a market spike on crude oil my man.,hardware,2025-12-26 18:14:19,6
AMD,nw5hcq3,">4GB has no confirmed production yet  It does, Rubin CPX needs it.",hardware,2025-12-27 06:29:31,2
AMD,nxp2p2j,Interesting thought. A staggered launch could very well happen. Yeah if we have to wait for that then high likelyhood of no new GPUs whatsoever in 2027.,hardware,2026-01-04 21:12:26,1
AMD,nw6d5z6,"Some of the ""software"" issues of RDNA 2 and 3 were hardware issues that software couldn't fix. It's not just a matter of one part of the team being good and the other bad. I can remember at least one issue with the video encoder outputting the wrong resolution being a hardware issue.",hardware,2025-12-27 11:35:10,5
AMD,nxp19kw,"Yes but unfortunately they still fall behind everyone else in adopting the future. Somehow Intel got XeSS, XeSS-LL and XeSS-FG before AMD Redstone. Somehow Apple had a DLSS competitor (ai based) before FSR4. It's just so tiring.",hardware,2026-01-04 21:05:46,2
AMD,nw2qt3c,"Thanks for clearing that up, I always thought leaving an ""obvious"" money maker on the table would be something shareholders can pressure the C-level to do.",hardware,2025-12-26 20:07:04,2
AMD,nw268ng,I’m sorry you don’t understand what an allegory is,hardware,2025-12-26 18:18:01,-3
AMD,nw70xzq,A lot changed with RDNA 4. But yeah prev stuff was crap on HW side as well.   I was mainly referring to SW feature set deficit and bad pricing.  RDNA 5 better be good. All that poached talent and hiring has to result in something meaningful.,hardware,2025-12-27 14:30:50,0
AMD,nxp409g,"Agreed. It's a big joke. The old tiring AMD cheapskate strategy. Only pivot when the tech is mature which means by the time its ready it's a joke.  There are some early indications that they're turbocharging things moving forward. Then again we don't know where 60 series will be heading + the SW R&D teams can do all the work they want but if FSR team is complacent and no one gets game devs to implement tech then it's all pointless.    I wouldn't be surprised if NVIDIA completely flips the script with 60 series so AMD better bring some groundbreaking tech nextgen, because I don't expect 60 series to be Ada Lovelace++.",hardware,2026-01-04 21:18:31,1
AMD,nw3ikhx,"Do you not see the circular logic here? It's only obvious if it's obvious. Clearly it's not obvious, else people would be in broad agreement.",hardware,2025-12-26 22:39:34,6
AMD,nw39eli,"The shareholders usually have the power to call a meeting, to ask the c level to course-correct or in rare cases replace them. They might then sue for damages that occurred while that process takes place, but unless there was obvious neglect, not that certain to be awarded.  Edit: to look at it another way, if they could sue for damages easily, where would the money come from? The c level is only personally responsible in rare cases (otherwise nobody would want to do the job). It could come out of the company, but that is just the shareholders giving themselves a dividend, weakening the company.",hardware,2025-12-26 21:48:40,3
AMD,nwniijk,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **GPUs**  |CPU + GPU + RAM Config|Score| |:-|:-| |9800X3D, RTX 5090, 64GB|166,885| |Ryzen 9 9950X, RTX 5090, 64GB|160,564| |9700X, RTX 5090, 64 GB|157,538| |14700K, RTX 4090, 32GB|153,707| |9800X3D, RTX 5090, 64 GB|153,422| |265K, RTX 4090, 48 GB|146,045| |7800X3D, RTX 4090 (UV), 32 GB|142,168| |9950X3D, RTX 4090 (315 W), 64 GB|140,077| |9800X3D, RTX 5080, 32 GB|124,739| |13700K, RTX 4080, 32 GB|110,815| |9800X3D (65 W), RTX 5080, 32 GB|110,233| |5800X3D, RTX 5080, 32GB|107,994| |275HX, RTX 5090 M, 64 GB|106,856| |5950X, RTX 5080, 64 GB|104,847| |275HX, RTX 5090 M, 64 GB (HP OMEN MAX 16)|103,947| |9800X3D, RTX 5070 Ti, 64 GB|101,697| |11900K, RTX 5070 Ti, 64 GB|101,600| |7945HX, RTX 4090 M (Legion 7 Pro)|98,094| |285K, RTX 5070 Ti, 128 GB|97,859| |13700K, RTX 3090, 64GB|91,266| |9950X3D, RTX 3090, 32 GB|84,942| |5800X3D, RTX 5070 (OC), 32 GB|78,660| |13400F, RTX 4070 Ti, 32 GB|77,851| |14900KS, RTX 5060 Ti, 16 GB|62,959| |265KF, RX 9070 XT, 48 GB|55,898| |5800X, RX 9070 XT, 32 GB|49,156| |9800X3D, RX 9070 XT, 64 GB|48,769| |7950X3D, RX 9070 XT, 32 GB|48,123| |5700X3D, RX 9070, 32 GB|41,898| |5800X3D, RX 7800 XT, 32 GB|40,789| |5800X, RX 7800 XT, 32 GB|40,769| |9900KS, RTX 2080 Ti, 32 GB|38,859| |11900KF, RX 7080 XT, 32 GB|38,213| |M4 Pro (14C), 20C GPU, 48 GB|36,881| |9950X3D, RTX 3060, 96 GB|35,615| |5600X, RX 6800, 32 GB|35,349| |5700X, RX 7700 XT, 32 GB|30,313| |13600K, RX 9060 XT, 64 GB|29,954| |7800X3D, RX 9060 XT, 32 GB|29,122| |9700X, RTX 2070 S, 64 GB|27,940| |5800X3D, RX 6700 XT, 32 GB|25,563| |14400F, RX 6600, 32 GB|18,313| |M4 (10C), 10C GPU, 24 GB (Mac Mini)|17,996| |M4 (10C), 10C GPU (Mac Mini)|16,054| |M2 Pro (MacBookPro 16"")|13,425| |M3 (8C), 10C GPU, 16 GB|12,268| |7640U, 760M, 32 GB|5,054| |5700G, Vega 8, 64 GB|0| |4750G, Vega 8, 64 GB|0| |X1E80100, Adreno X1-85, 16GB|N/A|",hardware,2025-12-30 01:38:45,3
AMD,nwnigam,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **1T**  |CPU + GPU + RAM Config|Score| |:-|:-| |M4 (10C), 10C GPU, 24 GB, Mac Mini|677,0| |M4 Pro (14C), 20C GPU, 48 GB|667,0| |M4 (10C), 10C GPU, Mac Mini|653,0| |9950X, RTX 5090, 64 GB|567,0| |M3 (8C), 10C GPU, 16 GB|567,0| |265KF, RX 9070 XT, 48 GB|560,0| |14900KS, RTX 5060 Ti, 16 GB|559,0| |9950X3D, RTX 5080, 32 GB|553,0| |9970 (PBO +200), RTX 5090|537,0| |9700X, RTX 2070 S, 64 GB|533,0| |9800X3D (UV), RTX 5090 (UV), 64 GB|530,0| |9800X3D (65W), RTX 5080, 32 GB|528,0| |275HX, RTX 5090 M, 64GB|524,0| |9700X, RTX 5090, 64 GB|514,0| |14700K, RTX 4090, 32GB|510,0| |13700K, RTX 4080, 32 GB|495,0| |M2 Pro, MacBook Pro 16""|480,0| |7945HX, RTX 4090 M, 32 GB|473,0| |13600K, RX 9060 XT, 64 GB|471,0| |12900K, RTX 4090, 32 GB|460,0| |7800X3D, RX 9070, 32 GB|444,0| |11900K, RTX 5070 Ti, 64 GB|438,0| |X1E80100, Adreno X1-85, 16GB|438,0*| |13400F, RTX 4070 Ti, 32 GB|430,0| |11900KF, RX 7080 XT, 32 GB|417,0| |5800X, RX 9070 XT, 32 GB|396,0| |5950X, RTX 5080, 64 GB|390,0| |7640U, 760M, 32 GB|377,0| |5800X3D, RX 9070 XT, 32 GB|369,0| |5700G, Vega 8, 64 GB|367,0| |6600H (45W), 16 GB, Beelink EQR6 Mini|338,0| |9900KS, RTX 2080 Ti, 32 GB|333,0| |5700X3D, RX 9070, 32 GB|303,0| |5800X3D, RTX 5080, 32GB|272,0|",hardware,2025-12-30 01:38:24,4
AMD,nwpsl9h,"Nice, thanks for posting, here you have my 9800X3D results with just PBO+100 and conservative scalar x1, it's much better than the generic 9800X3D results you posted so far.  screenshot of my configs (balanced power blan, ZenTimings, CO values, etc):  [https://i.imgur.com/kAUmokG.png](https://i.imgur.com/kAUmokG.png)  (CPU Multi-core 6034, single-core 780, single-thread 562, MP Ratio 10.74x)  Tested on most recent Windows 11 25H2 build to this date.",hardware,2025-12-30 11:49:50,2
AMD,nwnf2za,"Their form + Maxon has it termed properly, but Computerbase graph suddenly mixes up the terminology. Maxon is accurate.  |Test|Maxon Terminology|Computerbase Terminology| |:-|:-|:-| |One Thread: 1T|Single Thread|Single Core| |Two Threads: 1C2T|Single Core|Single Core + SMT| |All Threads: nT|Multiple Threads|Multi Core|  We shouldn't use ""Single Core"" to mean two things. Maxon is much clearer.  And, before we have an endless debate: Maxon's Single Core test ***is*** a multi-threaded benchmark test. It limits the thread count to 2. Cinebench pushes *two* parallel instruction streams to the CPU.  Hopefully Computerbase and other outlets stick to Maxon's wording. Or just use the numbers: 1T, 2T (SMT), and nT.  An old but good read from AnandTech:  [https://web.archive.org/web/20221006033815/https://www.anandtech.com/show/16261/investigating-performance-of-multithreading-on-zen-3-and-amd-ryzen-5000?utm\_source=twitter&utm\_medium=social](https://web.archive.org/web/20221006033815/https://www.anandtech.com/show/16261/investigating-performance-of-multithreading-on-zen-3-and-amd-ryzen-5000?utm_source=twitter&utm_medium=social)",hardware,2025-12-30 01:19:21,3
AMD,nwniecd,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **nT**  |CPU + GPU + RAM Config|Score| |:-|:-| |9970 (PBO +200), RTX 5090|18,278| |9950X3D, RTX 5090, 64 GB|9,641| |14900KS, RTX 5060 Ti, 16 GB|9,563| |9950X, RTX 5090, 64 GB|9,235| |9950X3D, RTX 5080, 32 GB|9,166| |275HX, RTX 5090 M, 64GB|8,490| |265KF, RX 9070 XT, 48 GB|8,409| |265K, RTX 4090, 48 GB|8,316| |7945HX, RTX 4090 M, 32 GB|7,474| |M4 Pro (14C), 20C GPU, 48 GB|6,812| |14700K, RTX 4090, 32GB|6,425| |13700K, RTX 4080, 32 GB|6,206| |12900K, RTX 4090, 32 GB|6,024| |5950X, RTX 5080, 64 GB|5,846| |13600K, RX 9060 XT, 64 GB|5,271| |9800X3D, RX 9070 XT, 64GB|5,122| |9800X3D (65W), RTX 5080, 32 GB|5,104| |9700X, RTX 2070 S, 64 GB|4,875| |7800X3D, RTX 5080, 32 GB|4,134| |M2 Pro, MacBook Pro 16""|4,121| |11900K, RTX 5070 Ti, 64 GB|4,017| |M4 (10C), 10C GPU, Mac Mini|3,858| |13400F, RTX 4070 Ti, 32 GB|3,750| |M4 (10C), 10C GPU, 24 GB, Mac Mini|3,723| |5800X3D, RTX 5070 (OC), 32 GB|3,698| |X1E80100, Adreno X1-85, 16GB|3,504*| |5800X, RX 9070 XT, 32 GB|3,485| |11900KF, RX 7080 XT, 32 GB|3,417| |5700G, Vega 8, 64 GB|3,305| |5800X, RX 7800 XT, 32 GB|3,297| |5800X3D, RTX 5080, 32GB|3,027| |9900KS, RTX 2080 Ti, 32 GB|2,988| |5700X3D, RX 9070, 32 GB|2,799| |M3 (8C), 10C GPU, 16 GB|2,689| |7640U, 760M, 32 GB|2,367| |6600H (45W), 16 GB, Beelink EQR6 Mini|2,105|",hardware,2025-12-30 01:38:07,3
AMD,nwr5ble,"[Here's the first pass for my 5700X3D + RX 9070 setup](https://i.imgur.com/xaWb8vh.png). Note that I was at first getting 13k in GPU score, but increased to 34k when I set my Adrenalin tuning to Default. This benchmark might be a lot more sensitive with GPU Tuning (at least in my quick test).",hardware,2025-12-30 16:30:28,1
AMD,nww8o3q,"|CPU|7600X|12400 AVX512 on|5800X|M4 Max|M3 Ultra|M2| |:-|:-|:-|:-|:-|:-|:-| |Cooling|Arctic Freezer 34|TR AXP90-X53mm LP||||| |Motherboard|B650M-Plus TUF|B660M MORTAR||||| |BIOS|3040 (09/12/2024)|A.F0 (01/11/2024)||||| |RAM|32GB DDR5-5600|32GB DDR5-6000||||| |RAM timings|36-38-38-80-135|40-40-40-76-116-2T||||| |GPU|TUF 4070 Super|Gigabyte 2060 Super WF||||| |CPU Single Core|638|527||||| |CPU Single Thread|478|416|388|676|573|473| |MP Ratio|7,69||8,87|11,59|21,09|4,94| |CPU Multiple Threads|3679|CRASH|3441|7829|12082|2336| |GPU|81243|24305||68590|83865|7952|  I benched two of my systems - a gaming PC and a HTPC. Both Win 11-26200, latest nVidia driver. All stock, no OC (except XMP and auto-PBO). 12400 is early batches which had the AVX-512 unlocked. Tried turning off it in the bios - multi-core test still crashed (temps 71, so no overheating either). With the 8GB VRAM requirement, I think the 2060 Super should be the second weakest (non-mobile) RTX GPU after 3050, with GTX 1070 probably being the weakest compatible (min CUDA 5.0 & 8GB VRAM) one.",hardware,2025-12-31 10:49:32,1
AMD,nwnihg9,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **1T w/ SMT**  |CPU + GPU + RAM Config|Score| |:-|:-| |9950X, RTX 5090, 64 GB|763| |9950X3D, RTX 5090, 64 GB|748| |14900KS, RTX 5060 Ti, 16 GB|742| |9970 (PBO +200), RTX 5090|740| |9800X3D (65W), RTX 5080, 32 GB|733| |9950X3D, RTX 5080, 32 GB|730| |9700X, RTX 5090, 64 GB|724| |9800X3D, RTX 5070 Ti, 64 GB|710| |14700K, RTX 4090, 32GB|680| |13700K, RTX 4080, 32 GB|647| |13600K, RX 9060 XT, 64 GB|625| |7945HX, RTX 4090 M, 32 GB|620| |12900K, RTX 4090, 32 GB|602| |7800X3D, RTX 5080, 32 GB|570| |13400F, RTX 4070 Ti, 32 GB|563| |11900K, RTX 5070 Ti, 64 GB|533| |7640U, 760M, 32 GB|518| |11900KF, RX 7080 XT, 32 GB|512| |5800X, RX 9070 XT, 32 GB|497| |5950X, RTX 5080, 64 GB|486| |5800X3D, RX 9070 XT, 32 GB|464| |5700G, Vega 8, 64 GB|462| |6600H (45W), 16 GB, Beelink EQR6 Mini|429| |9900KS, RTX 2080 Ti, 32 GB|420| |5700X3D, RX 9070, 32 GB|411| |5800X3D, RTX 5080, 32GB|304|",hardware,2025-12-30 01:38:35,1
AMD,nwrzaij,"do new Cinebench versions inflate the score of the same CPU?  Antutu's score inflation is such a pain over on phone benchmarking, even just going 1 version up will jack up the exact same phone's score by 100k (which gets extra deceptive when said phone's a budget phone that only got like 500k to begin with, so itd look like the phone got a 20% performance increase out of nowhere)",hardware,2025-12-30 18:49:09,1
AMD,nwnjygb,"who cares, just use stockfish",hardware,2025-12-30 01:46:47,-10
AMD,nwqaqe4,"Dunno what I'm doing wrong but my 5700X3D + 9070 GPU Score is only 13k. My card is only pulling 120W, at stock settings but when I play intensive games like Expedition 33 it pulls the full fat 245W.",hardware,2025-12-30 13:53:45,3
AMD,nxmmzx7,Well my Colorful Vulcan OC 4070ti Super 330W + 3000mhz on core +2000mhz on memory + R7 7700 OC 5500mhz + 6000c26-34-34-56 GPU score I have 113k pts and 103k on stock so I found that memory OC gives most improvement!,hardware,2026-01-04 14:22:34,1
AMD,nwt67qi,I scored 112558 with my 5070 Ti. Posted about it on the PC Master Race reddit,hardware,2025-12-30 22:13:25,-1
AMD,nx08oqp,"I assume it's an average, will have a lot of people who are also running back ground apps or slow ram or maybe even lower power limit sin BIOS etc.",hardware,2026-01-01 00:39:44,1
AMD,nwnvsd3,"SMT doesn't automatically mean 2T as there are implementations with more than two (even if not common).  Single Core+SMT is more accurate (but multi-core for nT isn't very good either) so I'd skip both terminologies in favor of being specific (i.e., 1c3t if it's a 3 thread SMT single core, 2c4t, 4c4t, etc.).  In the context of this benchmark, 1T, 1C (or SMT) and nT probably makes the most sense.",hardware,2025-12-30 02:51:08,3
AMD,nwpmg7l,Interesting how 5800X3D plus 5080 is way below 5800X3D plus 5070.,hardware,2025-12-30 10:56:33,1
AMD,nwvgk7k,"No, completely different, multi-core scores of my 9800X3D across different versions of Cinebench:  R23: 24253  R24: 1454  R26: 6034  They update the CPU and GPU renderers with each new revision of the software",hardware,2025-12-31 06:29:41,4
AMD,nwov9jb,Why?,hardware,2025-12-30 06:47:26,2
AMD,nws4bi2,Same. my 9070xt just did 19k xd,hardware,2025-12-30 19:12:41,2
AMD,nwsy8z7,"& /u/FusionXIII   Try running it after a fresh reboot.  My 9070 XT spit out a 2983 score. The GPU memory controller was reporting 0-1% utilization and the GPU was using ~140W doing who knows what.  After a reboot I got 48532.   With my modest UV/OC (-25mV core, 2714MHz mem) it scores 50308.",hardware,2025-12-30 21:35:35,2
AMD,nx3ktlc,"The GPU bench seems to be all over the place. A friend of mine scored similarly with his 5070 Ti and beat another's 5080. Maxon lists the 5090 score as 166k, but I've seen scores ranging from 150k to 190k on the same 5090 silicon.   Such differences can't be explained by OC, silicon or temperature. Maxon needs to fix their bench.",hardware,2026-01-01 16:29:38,1
AMD,nwnz0mc,">SMT doesn't automatically mean 2T as there are implementations with more than two  Fair, but in my experience, those *usually* use more specific names: SMT**4**, SMT**8**, SMT**16.** I imagine it's unlikely any of those CPUs will work with Cinebench 2026:  >Cinebench 2026 will not execute on unsupported processors.  But I agree with you: the best choice is just using the numbers of how many threads (and whether CB is setting affinities to ensure they're localised to one core or it accepts whatever the CPU allocates). The number of threads removes all confusion & is universally understood across languages.",hardware,2025-12-30 03:09:02,5
AMD,nwvfv43,Because the person that tested with the 5080 might be using stock 5800X3D and the one with the 5070 is doing -30 all-core instead. There's a 9800X3D there scoring 5100 but mine scores 6000+ (PBO+100 per-core curve optimizer). Here's the proof: https://i.imgur.com/kAUmokG.png,hardware,2025-12-31 06:23:56,0
AMD,nwpp8be,"Because it's a better benchmark.   - works on pretty much everything, including linux and phones  - fixed length test so harder to cheese via overclocking/boost  - has proper NUMA support and can work with thousands of threads by design  - has a cluster branch for benchmarking distributed deployments  - opensource so you can verify architecture-specific code and fairness  - can be compiled with CPU-specific optimizations on the target machine (march=native)  - covers a good mix of SIMD, branchy, and memory bound code, known to be [the best stress workload](https://github.com/ThomasKaiser/sbc-bench/issues/55)  - some representative results can be easily found [online](https://openbenchmarking.org/test/pts/stockfish)",hardware,2025-12-30 11:21:18,1
AMD,nwtbzcu,My 9070xt did 48k and my 7800x3d did 4.3k,hardware,2025-12-30 22:42:22,1
AMD,nwt3y23,"Ill try and report back thanks :) edit: it works, got 52k on my 9070xt mercury oc",hardware,2025-12-30 22:02:29,2
AMD,nwtc788,With a -30mv and  +10% power with the OC switch on my 9070xt Hellhound 48438  7800x3d 4381,hardware,2025-12-30 22:43:30,1
AMD,nwrbo1k,It's a different benchmark and not necessarily neither worse or better as such.,hardware,2025-12-30 16:59:57,4
AMD,nwtcaql,Yeah I just needed to restart apparently,hardware,2025-12-30 22:44:01,1
AMD,nwtglnm,Ima keep tweaking my undervolt and overclock I literally just unboxed and installed this card 2 hours ago. Re-download cinebench and just got lucky with the whole new version   Theres someone posting a 5070ti with 110k  I am severely limiting my potential here,hardware,2025-12-30 23:06:43,1
AMD,nwtih62,Nvidia uses cuda so I think the numbers won't be similar. Others 9070 xt i've seen are around 50k also,hardware,2025-12-30 23:16:46,1
AMD,nwto0zy,If i understand CUDA and the parallel stuff it does  One would think essentially around double the score right?,hardware,2025-12-30 23:47:25,1
AMD,nv1xd7z,"Not impossible, but very unlikely. AMD is currently focused on upcoming Zen 6, and I guess their engineers are working on next gen CPU / chipset / AM port. Plus there are probably constraints with TSMC schedule, I think you have to ""reserve"" quite some time in advance for any order.",hardware,2025-12-20 16:34:49,116
AMD,nv1zvgv,"Probably the simplest would be to manufacture 5800x3d again.   (Newer cpus have ddr5 controller, they would have to redesign architecture.)",hardware,2025-12-20 16:48:08,45
AMD,nv2oafl,No.  Considering the tech news from the last few months apparently there isn't much money in the consumer market for something like this.,hardware,2025-12-20 18:56:21,12
AMD,nv1woql,"Maybe, but I'm starting to think they want people to sell off their old stuff. Just a theory of course.",hardware,2025-12-20 16:31:13,28
AMD,nv2guhn,"the thing is you can still get 32gb of ddr5 for two or three hundred bucks. it won't be a great bin, and definitely won't have the overclocking potential of high end hynix a-die but frankly the vast majority of people can/will just run JEDEC and not be able to see the difference. That may be easy for me to say sitting on 64gb of a-die running tight timings, but you/I seriously don't need high end ram to build a pc and run games flat maxed out. Prices have gone up but generally if you could afford to build a PC before the rampocalypse you can still do so now, you're just going to have JEDEC speeds. Which are actually, totally fine if you're using your pc and not a hardware snob (which I am admittedly). ​",hardware,2025-12-20 18:18:11,17
AMD,nv2ikz1,"Last AM4 cpu that got released is the Ryzen 5 5600F, which happened september. So only 3 months  I wouldnt be surprised if AMD comes up with something new or reintroduce 5800x3d and 5700x3d (these cpus only existed for a year or year n half. Which is very short)  Its been 9 YEARS since AM4 got released. AMD still releasing cpu's for it.. crazy",hardware,2025-12-20 18:27:08,17
AMD,nv2w2bo,No  Not enough volume for amd to restart production,hardware,2025-12-20 19:36:57,4
AMD,nv3o0w6,"The production of the X3D variant of the core chiplets has been discontinued. Presumable it was done so that they can use those lines to produce Zen 5 X3D variant. So unless you want them to discontinue the current gen to put old already somewhat obsolete generation back into production, it's not viable idea.",hardware,2025-12-20 22:10:34,5
AMD,nv3kzh9,"If I were a top AMD executive I'd be focusing on these things  1. Getting Radeon THERE for AI purposes   2. Making the EPYC line amazing for data centers   3. Finding ways to optimize costs and cut risks     Targeting budget customers is fairly low on any list I'd have.    There's probably some value in keeping Zen 3 dies in production but they'd get minimal priority for anything new or cutting edge. Minimal development efforts. Zen 4 is already getting ""old"" by industry standards. There's not much point to getting anything newer to work with AM4 IODs either.",hardware,2025-12-20 21:53:25,8
AMD,nv2edbh,5950X3D? Or even 5950X3D2?,hardware,2025-12-20 18:05:29,2
AMD,nvfdxfb,"Not only is it not likely for reasons of moving forward instead of back, you'd also crash the ""market"" the minute there's any supply, and it would likely spike prices on AM4 boards and DDR4 ram. The lack of demand is the only reason AM4 boards and DDR4 are relatively cheap right now. Can't get good AM4 CPUs, so the rest of the linked prices slump. Bring back competitive AM4 CPUs and the prices shoot up to match, and now AMD is stuck trying to sell ancient and EOLed designs into a market that's already buying every AM5 CPU they make.",hardware,2025-12-22 20:18:10,3
AMD,nv36p5d,"First of all, never trust inflated prices on eBay. It's well known that scam artists who have hordes of collectibles or scalped/limited items who will orchestrate sales of items at inflated prices to drive up the market. Yes, they have to eat the cut that eBay takes, but if all of a sudden they have 10-50 items that they can sell for 20-100% more, it's worth it.   Like what people have been doing with VHS tapes on eBay for the last 6 years.   Secondly, AMD hasn't stopped releasing CPUs for AM4. They recently released the 5500X3D for the Brazilian market in June, and the 5600F in others.   The biggest issue is there's only a handful of AM4 boards still for sales and DDR4 RAM in the retail channels is almost entirely gone. Sure you can find used RAM, but minting new products that requires parts that are no longer in production is suicide.   Micron recently said they'd keep making DDR4, but I ain't seeing much available in the US. I assume much of that is still being [shuffled to the enterprise market.](https://prerackit.com/memory-markets-in-turmoil-how-chinas-exit-from-ddr4-manufacturing-triggered-a-server-ram-pricing-crisis-in-2025/)",hardware,2025-12-20 20:34:46,7
AMD,nv7x3xu,Wafers aren't just sitting on shelf to be bought when in demand. They're order well in advance. It's unlikely they resume production of the 5700x3d which old got disconnected a few months ago (new AM4 cpu is completely put of the question) as it's a step backwards and a gamble that people would still be interested in AM4 several months time instead of shelling out a bit more for significantly higher preformance gains from their next generation of cpus in November.,hardware,2025-12-21 16:34:40,2
AMD,nvh40h7,">It got me thinking, is 5000 series AM4 on an old enough node that AMD could restart production cheap? Cheap enough to sell a high end x3d chip to satisfy people holding on to their old platform and RAM while the shortage is happening?  Why this will probably never happen:  * The vast majority of people do not upgrade their CPU. The entusiaste market is probably less than 1%. There is not enough volume. * Selling this processor will cannibalize their own current product line. * The stacked L3 cache could be better utilized on their current CPUs. * All fab production is concentrated on storage, ram, new CPUs and GPUs. New and Old.  The echo chamber that is PC enthusiasts has seriously distorted their perception of the PC market.",hardware,2025-12-23 02:09:42,2
AMD,nv397y8,Before the AI bubble I would have said no way. With the AI bubble making new unaffordable for the next 12-18 months at a minimum I'd say maybe.  People on AM4 aren't going to buy AM5 if they have to spend 200% of their budget to get it. DDR5 RAM prices are really going to cripple the PCbuild industry for the next year.,hardware,2025-12-20 20:48:24,2
AMD,nv3b00x,"There was some rumors that Zen4 was originally going to come to AM4. They could still do it. Maybe a lot of that work is already done. I'm skeptical there is still much DDR4 in production, and it'll soon all be gone. What we got on desktop was mostly left over stuff that servers didn't want. I don't think they would make these CPUs just for consumer when there is so much more profit in putting that silicon towards servers. If Mircon abandoned Crucial memory for consumers, that to me says a lot about where the real focus is for hardware makers. AMD right now just doesn't care much about desktop anything.",hardware,2025-12-20 20:58:02,4
AMD,nv29ebj,"Yes, they have actively done so in the last year and are being pushed to re-release the 5800x3d by retailers.",hardware,2025-12-20 17:39:41,2
AMD,nv4gclk,"Very unlikely and most probably something on the lower end I’d think.   But a 5950X3D would be incredibly cool though, and a nice upgrade for my 5700X3D. 😄  On the other hand, there are a lot of people out there, me included, on AM4 that aren’t planning to upgrade to AM5 any time soon. So that would be a way for AMD to keep selling CPUs to existing AM4 users of whom they otherwise wouldn’t make money from.",hardware,2025-12-21 01:02:36,1
AMD,nv7ewr5,"From one side Zen 6 has to sell but RAM/(SSD) prices may block AM4 to AM5 upgrades (how many still on AM4?). Then if going AM5 is to expensive then people may wait till Zen 7 which may be AM6 and if it's revealed early that Zen 7 is AM6 then it will decrease AM5 interest even more.  Zen 6 will launch ""late"" 2026, so 2027 is the year it will be in mass availability. RAM prices may be more sane at that time. Especially when people will be waiting for X3D (if the base variants get rekt again by existing X3Ds).  Refresh of 5800X3D wouldn't hurt but I doubt they would be offer more without a longer development cycle (a new design) after which it may turn out it's all for nothing.",hardware,2025-12-21 14:56:36,1
AMD,nv9peun,they do release 5000 series x3d chips but using leftover chiplets that were binned too low for the expensive parts. ie the 5500x3d. its not in production anymore.,hardware,2025-12-21 22:04:38,1
AMD,nvamixt,No - production has already stopped for the 5000 series. It would be prohibitively expensive to have TSMC manufacturer last-gen chips in another run.,hardware,2025-12-22 01:12:18,1
AMD,nvferit,"Would be nice when the 5700x3D and 5800x3D would be produced again, but very unlikely.",hardware,2025-12-22 20:22:32,1
AMD,nvhz5no,Sure they could. AMD or Intel could make a modern chiplet CPU with a memory controller that runs DDR400.   There just isn't any financial incentive to do so.,hardware,2025-12-23 05:36:58,1
AMD,nvscq8b,"they did release a ""new"" one this year, the 5500x3D for latam.",hardware,2025-12-24 22:29:39,1
AMD,nw7yvm9,I hope they do. Been thinking of shifting from Intel to AMD for a long time now.,hardware,2025-12-27 17:30:39,1
AMD,nwpeonj,AMD is not designing or manufacturing anything for AM4 now. They found some binned chips to re-release under different name but thats about it.,hardware,2025-12-30 09:45:33,1
AMD,nv2je9u,Why would they do that when they can just sell a new x3d chip at inflated prices and have every single one of them sold before they've even been shipped?,hardware,2025-12-20 18:31:21,1
AMD,nv3fizo,they just released the 5600F in September.,hardware,2025-12-20 21:23:01,1
AMD,nv4hdev,"AM4 is just too good for me to ditch. My 6800XT holds it back in 4k gaming, 64GB 3600Mhz CL16 was dirt cheap and both single threaded and multithreaded performance with my 5950X is more than I really need.",hardware,2025-12-21 01:09:01,1
AMD,nv2i61h,"Considering they launched the Ryzen 5 5500X3D only 6 months ago, i would say they can, the problem is if they will. Unfortunately it can take a long time to design or adapt existing designs, test and then manufacture them in sufficient numbers for release, if they started now it would probably take a year or more to see them on the market. A 5950X3D does sound cool as hell, maybe i would upgrade from my current 5700X3D, though this CPU is perfect for my needs and the gaming i do.",hardware,2025-12-20 18:25:01,-1
AMD,nv33kwe,Imo the 5950X3d is the most likely option as the very last cpu on am4. dual 3XD ofc and with people starting to switch to more expensive and advanced packaging then Cowos there will be enough cowos allotment that amd can buy for older designs,hardware,2025-12-20 20:18:03,-1
AMD,nv3u5oc,"Restart? They never stopped releasing them, with the latest being the  5500X3D released in June this year.",hardware,2025-12-20 22:46:36,-1
AMD,nv3h2vr,"DDR6 is coming out soonish in 2027, so when that happens the cutting edge ai shit will all switch over to that. So ddr5 availability should go up permanently after that. It's gonna be a dry ass desert until then but it won't be forever. DDR5 came out in 2020 so it's already fairly old, making a switch back to 4 even temporarily quite unlikely.",hardware,2025-12-20 21:31:35,0
AMD,nv3i37j,"The only one that would *maybe* make sense, and I stress it's a biiig maybe at this point, would be a 5950X3D. After that, no more AM4 anything. Otherwise, probably best to keep on cranking 5800X3Ds.",hardware,2025-12-20 21:37:17,0
AMD,nv2dc8m,"You can get a 5700x3D, which is a lot cheaper.",hardware,2025-12-20 18:00:17,-2
AMD,nv2pscd,"They could mix tiles, but would be a hard sell for motherboard partners. And why buy a am5 x3d when am4  x3d gives almost the same gaming performance?",hardware,2025-12-20 19:04:02,-2
AMD,nv2towu,"They can restart Zen 3 X3D production, with a dual CCD V-Cache 5950X variant and should do so IMV.",hardware,2025-12-20 19:24:29,-4
AMD,nv1zhnp,AM4 Zen 6 Fan Edition backport manufactured on an Intel node,hardware,2025-12-20 16:46:06,67
AMD,nv25szg,"Dont think they're advocating for proper new designed chips, just respinning up old ones.    As for TSMC, I'm sure if AMD really cared to do this, they could find some way to give up some more leading edge capacity for older node capacity.  Perhaps via agreement with some other company who would love to bump themselves up the waiting list.    I think bottom line is that AMD isn't gonna be overly concerned with things.  They're still gonna be making lots of money on all this AI stuff themselves selling CPU's and GPU's, they can take hit on the consumer side for a bit.    I also think paying high prices for AM4 processors is very stupid.  While DDR5 has certainly ballooned building on AM5 platform, the reality is that total system costs are still only gonna be like 15-20% higher.  Small enough difference where it will probably still be worth it to go with AM5 in the big picture.",hardware,2025-12-20 17:20:08,28
AMD,nv1yle7,"Two years in advance, I believe...",hardware,2025-12-20 16:41:19,7
AMD,nvis9qj,"seeing ""Zen 6"" is crazy when i still am thoroughly satisfied with my Zen 2 3900X. Although i suspect it's finally showing some wear.",hardware,2025-12-23 10:07:30,1
AMD,nv3gsg4,"There are actually older Zen3 parts with the DDR5 controller, too. As I understand it, Zen cores are largely decoupled from the wider system with their IO die interfacing with the various external components, so that may be the only bit which really needs changed to support one platform it another.",hardware,2025-12-20 21:29:58,12
AMD,nv3pc79,"Part of me wants to believe it would be possible for AMD to use the old DDR4 IO die and pair it with newer compute dies with Zen 4 or 5, but even if that was true they have no financial incentive to do it.",hardware,2025-12-20 22:18:06,4
AMD,nv26o2w,Or variants of it we never got (5900X3D or a 5950X3D).   It's not like TSMC 7 and 6 have companies fighting each other for their wafers at this point.,hardware,2025-12-20 17:24:49,9
AMD,nv3t9cs,Im down,hardware,2025-12-20 22:41:17,2
AMD,nv6okgx,"Also, the current price explosion of ddr5 is a bubble by an overleveraged market with inelastic demand that will likely be mostly gone by the time any of these new models would make it to the market.",hardware,2025-12-21 11:48:38,6
AMD,nv6n0ey,The consumer market is waiting for 8k monitors with high refresh rates. And cpus and motherboards with connections that support that.,hardware,2025-12-21 11:34:05,-7
AMD,nv2bsmj,They might want it but the rest of us want DDR5.   Nobody's getting what they want in 2026 except the billionaires.,hardware,2025-12-20 17:52:17,19
AMD,nv356y8,"They want money, and they will do whatever it gives them. Old stuff is sold, makes no profit, so yeah they want more sales. Anyways, stick to ddr4 and if ddr5 demand falls then they will have to adjust.  W11 EOL+ Crypto+ AI ... That is tiny compared to 9.000.000.000 humans that use a pc.",hardware,2025-12-20 20:26:45,0
AMD,nv35o6r,And X3D doesn’t have too much to gain from “good” RAM.,hardware,2025-12-20 20:29:16,14
AMD,nv2imvs,"What about people who just want an upgrade? I would pay a few hundo to get best in-slot CPU on my current platform for a quick bump to give me another year or two.  Right now that's not possible, 5800x3d is no longer manufactured and 2nd hand is prohibitively expensive...  Why not fill that gap with brand new silicon?  The question is - is there fab capacity to make it?",hardware,2025-12-20 18:27:24,-4
AMD,nv3m5ck,It was pretty clear they were dumping and clearing out stock when the 5500X3D launched during the summer.   EPYC with Zen3 and X3D is probably EOL now. Which is the main reason why AMD kept the desktop parts around as well.,hardware,2025-12-20 21:59:56,14
AMD,nv2znuj,"5100x3d in 2 years, trust",hardware,2025-12-20 19:56:18,6
AMD,nwpfouh,to be fair thats just scrapped garbage from downbinned chips that would otherwise had to be melted down as broken. they didnt really manufacture AM4 chips 3 months ago.,hardware,2025-12-30 09:54:46,1
AMD,nv8suys,"Gonna be plain, with IBM and Cisco starting to pivot, avoiding too much waste of Radeon dev time on AI crap may end up being wise.",hardware,2025-12-21 19:14:18,1
AMD,nwpgk0q,"The DIY market is much more than 1% (dell estimates 15% for example). However very few of the DIY upgrade CPUs. The longevity of platforms like AM4 is nice in theory, but for vast majority of PC users it does not matter because by the time they need to upgrade they will be buying next socket anyway.  Funny thing, the store i buy stuff from offers free assembly if i buy PC parts. I can choose to have it delivered already assembled, but only if i also buy a case otherwise they refuse to ship naked assembled mobos. So i just assemble myself, but im sure there are plenty of people who will take this service and wont care about socket at all.",hardware,2025-12-30 10:02:37,2
AMD,nv3uv0a,5500X3D is just a stockpile of chips that couldn't be sold as better chips.   AMD likely was building that stockpile ever since launch.,hardware,2025-12-20 22:50:51,6
AMD,nv8t12e,Assuming that the AI crap doesn't crash before then.,hardware,2025-12-21 19:15:08,2
AMD,nvcswtd,ddr5 is unavailable becuase dram manufacturing is not miang the ddr5 chips that go into desktops and increasingly not even ddr5 chips that go into servers. why do you think ddr6 would be any different?,hardware,2025-12-22 11:42:14,1
AMD,nv2im3w,Even these have gotten insane. Used 5700x3Ds used to go for 150-175 eur. Now they're 300..,hardware,2025-12-20 18:27:18,5
AMD,nvauuxn,u/bobalob_wtf there isn't enough demand to justify a new chip SKU for better raw performance or performance per watt.  It is more likely that they'll do a new manufacturing run of an old SKU.  Will it be cheaper? They may pass on the savings of the older process node to you if it is a competitive advantage.  If now stocks of current SKUs are unavailable then buy used?,hardware,2025-12-22 02:03:22,2
AMD,nv2oi5x,"I think you're right. AM4 was a great, long-lived platform--I'm still on it myself--but I don't think anyone building now should really be looking at AM4.  Maybe build with 16 GB RAM if you're really on a tight budget, and start with an R5 7600, and upgrade to Zen 6 and more RAM when prices come back down out of the stratosphere.",hardware,2025-12-20 18:57:27,17
AMD,nv4araf,">As for TSMC, I'm sure if AMD really cared to do this, they could find some way to give up some more leading edge capacity for older node capacity  Is the 7nm node fully booked?  Would AMD have to give up anything to get more of it?",hardware,2025-12-21 00:28:15,5
AMD,nv89zms,"Nostalgia is one hell of a drug. We saw the same with people exaggerating the longevity of Sandy Bridge during the pandemic. Both SB and Zen3 have been absolute champs, and the latter is mostly great if you already have it, but some people are close to deluding themselves because of desperation over prices and lack of realistic options to build new in the current situation.",hardware,2025-12-21 17:41:08,3
AMD,nv66l4u,"Those 15% are a 200 dollar difference for a 32 GB computer. That can be a very significant fraction of disposable income, and the benefits of a DDR5 CPU aren't that big of a deal in this price range anyway.",hardware,2025-12-21 08:50:34,1
AMD,nv3t9zl,"the mobile chips have a bunch of zen versions (2,3,3.5,4,5) running ddr5/lpddr5",hardware,2025-12-20 22:41:23,12
AMD,nv3lv55,"> There are actually older Zen3 parts with the DDR5 controller, too.  Ye and Zen 4 with DDR4 controller from Zen 3 as well. They both tested the new IO die with the old architecture and Zen 4 with the old IO die during development.",hardware,2025-12-20 21:58:18,10
AMD,nv2fihy,"For the silicon itself no, but if you want X3D parts packaging is the bottleneck.",hardware,2025-12-20 18:11:23,22
AMD,nv8dxu7,The average consumer doesnt even have a gpu that can run 2k. What are you talking about,hardware,2025-12-21 18:01:06,6
AMD,nv3imxt,"Not really, it's closer to less than 2billion people that have a pc and almost all of that is gonna be a basic pc that isn't upgraded. Like with everything a company makes way more profit on a overpriced ""pro"" ai chip than a cheap user version and when you have a trillion dollar order for ai gpus you will have that 50k dollar ai gpu be prioritized over a 1k consumer gpu.  The real problem is that these orders are for speculative demand and so they are just selling all of their future product without regard to how much will actually be needed.",hardware,2025-12-20 21:40:22,2
AMD,nwpffbc,the whole point of x3D is to increase cache hit rates and decrease RAM hit rates. You get more benefit from x3D the less RAM matters.,hardware,2025-12-30 09:52:22,1
AMD,nv36vod,they definitely help a lot.,hardware,2025-12-20 20:35:42,1
AMD,nv2lt5j,"where were you last year when they were practically giving away 5700x3d on ali express? it was very clear at the time it was a very limited time deal as the chips were out of production and AMD was just using up all the silicon that didn't bin well enough to be a 5800x3d. if you were ok with your current performance and it wasn't worth the time when fantastic AM4's were only $150 shipped, why FOMO and panic now? In any case anyone building \*now\* can still get a 7500F which will out perform any AM4 CPU for $150, paired with whatever mobo is cheapest and JEDEC tier ram, then upgrade the CPU and ram in the future and be sitting pretty. Fab space is booked up for years solid, scheduling a brand new run specifically for people who missed the boat on a cheap drop in AM4 life extension isn't going to happen and wouldn't be economical for anyone if it did.  Also, are you aware you can currently just get a 5900x from ali for $250 all in? That's your one step AM4 life extension solution right there, they're available, and quite good chips. 12 core, 4.8Ghz boost, they're more than enough to get you a few more years.",hardware,2025-12-20 18:43:42,7
AMD,nv2ufow,"> Right now that's not possible, 5800x3d is no longer manufactured and 2nd hand is prohibitively expensive... >  >  >  > Why not fill that gap with brand new silicon? >  >  >  > The question is - is there fab capacity to make it?  And would it be profitable for AMD to release it at prices that you and others wouldn't consider prohibitively expensive?  I took a quick look at ebay, and it looks like the 5800x3d is going for just under $500, and the 5700x3d just under $350.  What price would those CPUs have to sell for new for you to consider them a good buy?",hardware,2025-12-20 19:28:21,4
AMD,nwpfi3k,> What about people who just want an upgrade?  they should be looking at AM5 setups.,hardware,2025-12-30 09:53:04,1
AMD,nv3uq6n,It was pretty clear they were making brand new silicon when the 5800XT and 5900XT dropped last year. They might have stopped this year despite several new AM4 releases but availability makes me think they never stopped at all.,hardware,2025-12-20 22:50:02,-5
AMD,nvez17x,"If AMD handled 10% of nVidia's output, they'd basically 2x their market cap.   There are crazier gambits to take.",hardware,2025-12-22 19:01:52,2
AMD,nwpo4zi,Exactly!  AMD AM4 did have a long run tbh but everyone with an AM4 i've built for is either happy with how the system performs or jumped to AM5 or LGA 1700.,hardware,2025-12-30 11:11:36,1
AMD,nvajhct,"Yeah exactly, they were all the ""bad"" 5600X3D yields which are all just 5700X or 5800X bad yields too slapped with an 3D V-Cache die on top. In no world is AMD going to TSMC and purchasing wafers to make exclusively 5500X3Ds or any other old chip. They especially wouldn't bother purchasing wafers for some transient rise in DDR5 and to give customers on older platforms upgrade options. It also takes years to do that sort of stuff and thats using old process nodes and architectures.  Even backporting a new architecture to an old platform would require re-validating for a new platform and for DDR4 memory now, getting partners to release BIOS updates and to get them on board etc. It's not that simple to just move Zen6 to AM4 for instance by replacing an I/O die and calling it a day. I can't imagine AIBs being happy about losing new motherboard sales either.",hardware,2025-12-22 00:54:31,2
AMD,nvnxy59,They will only use ddr6 because it's much better but current producers will keep making ddr5 for a while because it's in demand and will sell. It's only a problem because we stopped making ddr4 because there wasn't much demand for it once ddr5 was cheap and widely available. If this crunch happened last year before ddr4 production was wound down it wouldn't be a huge problem either. This is the worst timing.,hardware,2025-12-24 04:14:01,1
AMD,nvcwwj0,"a used 5700x3d is 300€ and a used 5800x3d is 350-400€. It is just not worth it, you can buy a brand new 14600k+a decent mobo for that amount.",hardware,2025-12-22 12:15:23,2
AMD,nv2k0zf,"Hey Admirable_Bid2917, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-20 18:34:29,2
AMD,nv4cogy,"The exception is if you already have solid DDR4. In that case, it can be a pretty good idea to make a new AM4 build.",hardware,2025-12-21 00:40:00,4
AMD,nv3ekjm,"> I don't think anyone building now should really be looking at AM4.  You're joking, right? As if anyone aside from Brad Pitt's, Elon Musk' kids or some wealthy politician could even afford actually *newly* released stuff these days, given all the price-increases …",hardware,2025-12-20 21:17:44,-16
AMD,nvlzwq5,"I am pretty sure AMD still has 7nm booked for products like the console chips (PS5, Xbox Series) and some long running embedded designs. How much they could reallocate to old Zen 3 AM4 designs is an other question.",hardware,2025-12-23 21:15:36,2
AMD,nvdldlh,"Maybe, maybe not.  But I definitely think they could figure out something to get it no matter, given leading edge manufacturing tends to be more desirable overall.",hardware,2025-12-22 14:52:16,1
AMD,nvaip3w,"Sandy Bridge's longevity wasn't exaggerated, that arch absolutely slapped till 2020. Basically if you had an OC'd 2600K or something you were sitting on that till Intel 12th gen for a great improvement in ST perf. Or you bought Zen3. If you needed MT perf from Sandy Bridge, you likely upgraded to Zen2 or Intel 10th gen for a nice leap in performance. If you really needed MT perf gains, you were likely on Ryzen or Intel Extreme chips anyways almost every new gen.",hardware,2025-12-22 00:50:11,2
AMD,nwpez0b,its like people claiming 1080ti is still great despite being worse than the budget variants of modern cards and having none of the modern features.,hardware,2025-12-30 09:48:13,2
AMD,nv8q2e2,>We saw the same with people exaggerating the longevity of Sandy Bridge during the pandemic.  Did we? Personally I was on Ivy Bridge until 2022,hardware,2025-12-21 19:00:16,3
AMD,nvdltcw,"$200 difference in a $1000-1200 build isn't insignificant, but it's also probably not worth hobbling your system over, either.  Going AM5 gets you better performance for the entirety of its life, it gets you upgrade options in the future, and it also gives you better longevity so that no matter what, better stuff will be available by the time you actually do really want/need to upgrade again.  $200 over a five year ownership period is just $40/year(or like $3/month).  I think people should remember to consider this kind of perspective on things when buying a PC, at least for anybody who cares about getting good overall value.",hardware,2025-12-22 14:54:38,1
AMD,nv2n1vm,"I mean....that's pretty much true for everything now though, isn't it?  Given how Sammy's more or less fucked the entire market, not just DIY, and some hyperscalers are finding it pretty damn hard to jump to newer EPYC platforms, I can see there being enough demand to justify firing the Zen 3 X3D line up for a little bit.",hardware,2025-12-20 18:50:03,-5
AMD,nwpf8ei,pretty sure the average GPU can output in 1080p.,hardware,2025-12-30 09:50:37,1
AMD,nv9n1kt,"I am talking about upgrading from current consumer tech.   Current tech can easily run 2k monitors just fine, even with APUs. My point was not about GPUs specifically, nor about gaming, it was about 8k monitors. Dual resolution monitors are a thing, you know.",hardware,2025-12-21 21:51:53,-2
AMD,nv3tx24,[TechSpot/HUB tested with 6 different RAM kits](https://www.techspot.com/review/2915-amd-ryzen-7-9800x3d/)  There was less than 2% variance between them.,hardware,2025-12-20 22:45:10,16
AMD,nv39i23,"5900x sounds perfect for my needs, thank you :)  Why are 5800x3d going for silly prices then? Just random market insanity?",hardware,2025-12-20 20:49:55,2
AMD,nv3uhr3,People are gonna cry and want $200 5800X3D forgetting it had an MSRP of $450...,hardware,2025-12-20 22:48:37,4
AMD,nv3vm0v,X3D is a separate production line with its' own constraints. There is limited packaging available for X3D that is not shared with the normal SKUs.  X3D being EOL is not tied to the normal Zen 3 SKUs. The normal SKUs they can shurn out as long as there is demand from both server and desktop.  X3D however might very well be supply constrained. And making lower margin Zen 3 variants might cut into scaling Zen5 X3D SKUs.,hardware,2025-12-20 22:55:24,10
AMD,nvhqc95,"They'd be wise, in that scenario to focus on GPPU stuff and only have AI gains that come with overall uplift because, well, bubble.",hardware,2025-12-23 04:31:39,1
AMD,nvc3mxw,"Yep this is exactly the case some people find themselves in. They have something like an i5 6600K or 7600K which are 4c/4t CPUs, completely outdated in 2025. Yet they have 16GB or even 32GB of DDR4 that is still usable! Upgrading to something like a Ryzen 5500, 5600 or 5700X on a cheap B450 or B550 board is a huge upgrade for very little cash, and they spend $0 on RAM as they're reusing what they already have.",hardware,2025-12-22 07:36:15,5
AMD,nvai6f6,"Sure, but the majority of consumers for most of AM4's life cycle were on 16GB of RAM. [Go look at Steam Hardware Survey in May 2021](https://web.archive.org/web/20210512095214/https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam?platform=pc) at the height of AM4's popularity and performance leadership (well into Zen3) only 12% of Windows systems had more than 16GB and people were still rolling with Intel too back then. You'd be hard pressed to find anyone with 32GB of RAM on AM4 really in their old systems if they're still using them. 32GB really only became very prolific with AM5 and Z690 thanks to DDR5 density. I guess if you're okay with having 16GB of RAM it would be okay, but at that point might as well just sit on one stick of 16GB DDR5 or 2 8GB sticks of DDR5 till this whole AI memory shortage blows over.",hardware,2025-12-22 00:47:06,2
AMD,nvqoju1,If it's solid DDR4 it'll probably fetch a good price at the second hand market.,hardware,2025-12-24 16:45:52,1
AMD,nwpetsn,"if you already have solid DDR4, you arent going to be gaining anything from a new chip.",hardware,2025-12-30 09:46:52,1
AMD,nv442xz,"There's a whole 30%ish of the American population (loosely college educated several years into their career and moderately successful small business owners) that are doing better than ever.   The median person (not highly educated, minimal to moderate career development) might be feeling financial pressure, but there's enough of the top third (100 million people) RIFE with cash to prop up entire industries.      There's a lot of people for whom a new computer every few years is something like 1-3% of their disposable income.",hardware,2025-12-20 23:47:22,4
AMD,nvoyk0f,"They could reallocate whatever they like, I suspect.  But I think it's *very* unlikely there will be any significant production of AM4 X3D SKUs.  X3D was created for Epycs, with the consumer parts being essentially the bin rejects - and who the hell is buying Zen3-based Epycs now?  AMD will carry on producing AM4 CPUs for a while - they're cheap to make, thoroughly supported and more than good enough for a pretty wide range of use cases.  I'm still using one myself as a daily driver and it's absolutely fine.  But X3D is an expensive process and AMD aren't going to restart an obsolete version of it to produce low-price consumer SKUs.",hardware,2025-12-24 09:31:59,1
AMD,nvdmof2,"And I'm still on Ivy Bridge today with my 3570k.    But I'm also under no illusions that my system is massively outdated and has been for quite a while and that playing most of the latest heavier hitting games is basically a total impossibility.    You and I were just being cheap/patient bastards.  It had nothing to do with our CPU's genuinely being great CPU's up through 2022, much less today.",hardware,2025-12-22 14:59:17,2
AMD,nv2zgj8,"> I mean....that's pretty much true for everything now though, isn't it?  Not for stuff made with traditional packaging tech",hardware,2025-12-20 19:55:12,5
AMD,nwq20oo,I've only ever heard people and companies say 2k in reference to 1440p so idk what you mean,hardware,2025-12-30 13:00:20,1
AMD,nvhkes2,"You are vastly overestimating the prevalence of ""spec out the CPU socket type for the PC I'm building"" hobbyists as a percentage of the consumer market.  Most of 'the consumer market' buys common PCs off the shelf at big-box general retail store, and 8K dual-resolution monitors have never once entered their mind.",hardware,2025-12-23 03:51:57,2
AMD,nv42y1e,"you can definitely squeeze an extra 2-3% by fully tuning hynix a-die, EXPO profiles are \*really\* loose in all the subtimings. But most people won't bother. As someone who spent a lot of time tuning ram the juice definitely isn't worth the squeeze for normal people.",hardware,2025-12-20 23:40:16,4
AMD,nv3p46b,"Just supply and demand. X3D's are hype (with good reason), the AM4 ones  are out of production, and people who have them aren't upgrading/selling because they are still very capable. The price isn't proportional to performance, the 5950x or 5900x for example are within 5% in performance but half the price since the x3d hype is so powerfull. Not that they don't deserve that hype, but other excellent options are overlooked as a result of how they dominate the discussion around cpus for gaming.",hardware,2025-12-20 22:16:50,4
AMD,nv4dwmk,"I bought a 5700x3d last fall for $135 including shipping and tax, and at that price it was a no-brainer to upgrade.  But I had already decided that $450 for a 5800x3d was too much when I already had a 5600x.",hardware,2025-12-21 00:47:27,3
AMD,nvktyda,"Much of the work on AI optimizations would also carry over GP-GPU.   At some level tensor multiplication is tensor multiplication.   There are cases where one set of tradeoffs is more important in one use case vs another but overall... a rising tide lifts all ships.     My suspicion for these is that much of the reason why nVidia started focusing on ray tracing and DLSS is that the uarch optimizations that happen to be somewhat useful for those are VERY useful for general AI training. I'd have to dig into details though.   I'd actually agree that using machine learning to do upscaling is an overall smarter and more efficient way of doing things than just brute forcing more raster calculations. Frames upscaled by DLSS are something like 200-400% more energy efficient (AI generated info take with caution) and the amount of die space dedicated to tensor cores is pretty minimal, just a few percent.",hardware,2025-12-23 17:42:27,1
AMD,nvcjg95,"I’m not talking about blanket recommendations, please keep in mind steam hardware survey is not representative of commentators here.   Many of us may have picked up 32GB of fast DDR4 during the over supply for really cheap. Or even 64GB like me.",hardware,2025-12-22 10:14:06,0
AMD,nv7fc83,Yup. I got into pc building again this year. Comfortably afforded a 5090/9800 etc etc. the poor are worse off than ever but the PMC middle class- especially the child free are doing just fine.,hardware,2025-12-21 14:59:09,1
AMD,nwq5b3t,"Then every single one of them are wrong, because 2k is 1080p. 1440p is the rarely used 2.5k. Heres a handy graph: https://upload.wikimedia.org/wikipedia/commons/0/0c/Vector_Video_Standards8.svg",hardware,2025-12-30 13:21:25,2
AMD,nvjghah,There is no other reason to upgrade. Consumers have other desirable products to shop for.,hardware,2025-12-23 13:24:32,1
AMD,nvcmowu,"> I’m not talking about blanket recommendations, please keep in mind steam hardware survey is not representative of commentators here.  You never said that lol.  >Many of us may have picked up 32GB of fast DDR4 during the over supply for really cheap. Or even 64GB like me.  Okay and you're the minority of gamers/AM4 users.",hardware,2025-12-22 10:45:12,1
AMD,nwq86m7,Ah okay I get it thanks,hardware,2025-12-30 13:38:52,1
AMD,nuvx0lm,No one is buying this for 800  Stop the sensational headlines  You can get a 9800x3d and 32gb ddr5 cl30 kit at MARKET PRICE for under 800,hardware,2025-12-19 16:33:05,408
AMD,nuvxupa,"Another clickbait article, yeah no shit if you order by highest price sales on ebay it'll look like this. If you instead look at recent sales this month they're all around $390-530.",hardware,2025-12-19 16:37:07,94
AMD,nuzu0bp,Trash fake news. Delete this garbage,hardware,2025-12-20 06:35:55,5
AMD,nuwxrrc,Glad I bought my 64GB of RAM months ago. Cost less that $100,hardware,2025-12-19 19:35:30,5
AMD,nuwq8mq,How many people on am4 actually made the leap to 5800x3d than just the normal 3600 > 5600x/5700x and thats it or 1600x to 3600x to just 5600x?,hardware,2025-12-19 18:56:54,3
AMD,nv0hu86,Why do we still allow Tom's Hardware articles in this sub at all?,hardware,2025-12-20 10:36:00,3
AMD,nuw6vyx,There was a brief period these chips were an excellent buy and very cheap but that ended a while ago. Last time I checked at the middle of this year they were £350+ and now I see they are £450+.  I get that it allows you to max out an AM4 system and if you have a bunch of DDR4 that may be a wise investment still but the days of it being the undisputed price:performance king are long over.,hardware,2025-12-19 17:21:38,5
AMD,nuvtqz8,"Hello I_Love_Cape_Horn! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-19 16:17:26,2
AMD,nuwiuja,Honestly I want a 5600x3d with the lower power draw and TDP. My mATX system is somewhat small and I’m not playing the most demanding games on it but I’d like to get a few more years out of AM4,hardware,2025-12-19 18:20:36,2
AMD,nuzlkb7,"oh ye, i am selling my one for $69420...now go write another article",hardware,2025-12-20 05:24:02,2
AMD,nv03534,"I upgraded my AM4 AB350 system from first gen Ryzen 1600 to 5700X3D and the gaming results lined up with the reviews. But these chips falter with high RT usage games that have even higher demands on the CPU and RAM.    For example, in Stalker2 CPU-limited scenario, 9800X3D was almost 2.5X of its performance. I doubt that non X3D would be more than 20-30% slower, so it'd be better to get AM5 instead.",hardware,2025-12-20 08:04:11,2
AMD,nuwg9pf,"What? I mean if you'd actually wanted to spend 800$ on just the CPU, why not just get a cheap AM5/1851 one and then get yourself some overpriced RAM instead?  This makes no sense at all.",hardware,2025-12-19 18:07:58,3
AMD,nuwyxiv,"A 7600X is faster than a 5800X3D in most games, if you are buying a 5800X3D for $800 you are an idiot.  A 5600x will still be GPU limited at the resolutions and settings people actually play games at.",hardware,2025-12-19 19:41:34,2
AMD,nuvxgle,No shit.  This happens with all older hardware as companies want you to buy new stuff.,hardware,2025-12-19 16:35:14,2
AMD,nuyqqpa,I was thinking to sell my old DDR4 memory....,hardware,2025-12-20 01:59:20,1
AMD,nuz5dtf,This crap of supply and demand can stop anytime now.,hardware,2025-12-20 03:32:36,1
AMD,nuz7xl1,The RAM I own will appreciate faster than gold in the next year or two woohoo,hardware,2025-12-20 03:49:34,1
AMD,nuzgtjq,What enthusiasts are on 3 generation old hardware? I consider myself a pretty big enthusiast and never skip more than 1 generation.,hardware,2025-12-20 04:50:09,1
AMD,nv019a6,Jumping from am4 to am5 is impossible though because of insane prices too,hardware,2025-12-20 07:45:39,1
AMD,nv0nfhr,I just bout a Ryzen 7 7000 series drum 220 brand new.,hardware,2025-12-20 11:32:05,1
AMD,nv0tj1j,Someone wanna give me $1000 for my... DDR3!,hardware,2025-12-20 12:27:28,1
AMD,nv0ypmd,Am4 is dead.   LGA 1700 is where it’s at rn. Having compatibility for either ddr4 or ddr5 ram is much better than am4.   And the lga 1700 CPUs especially the 13th and even more the 14th gen intel CPUs like the 14700k are way faster than any am4 cpu as they compete with am5 CPUs anyways.,hardware,2025-12-20 13:08:24,1
AMD,nv1ur9z,"Maybe I should sell mine, I'm using it in an HTPC that I rarely game on anyway.",hardware,2025-12-20 16:20:57,1
AMD,nv387oo,I just sold mine for 375$ yesterday.,hardware,2025-12-20 20:42:54,1
AMD,nv807rw,"Hey I’ve got a 3800X, I’ll sell it for 3.8 million USD if anyone is interested.",hardware,2025-12-21 16:50:33,1
AMD,nvc21an,"and Ryzen 5 5500 is only $75. sure, having two more cores and 6x the L3 cache is nice, but not 10x the price nice.",hardware,2025-12-22 07:20:33,1
AMD,nuxfsfx,This is more sad than anything,hardware,2025-12-19 21:10:13,1
AMD,nuvzbwi,something something... turntables.,hardware,2025-12-19 16:44:17,1
AMD,nv39ar3,"Pleased my 5950x, 3090ti and 64gb of ddr4 are still keeping up just fine….",hardware,2025-12-20 20:48:50,1
AMD,nuw0llz,"There are still places that do sales on AM5 stuff. You can buy a 7800X3d , 32GB ram and motherboard for $580. There is literally no need to spend $800 for a 5800X3D",hardware,2025-12-19 16:50:28,0
AMD,nuwjz7y,Gamers would rather pay 800 for this than get Intel. The gamer brain rot is real.,hardware,2025-12-19 18:26:07,-3
AMD,nuvxsgy,Computers are quickly becoming a luxury like in the 90s.,hardware,2025-12-19 16:36:49,-1
AMD,nuw8wrz,"they are trying their best to milk the market. Push prices up...   Fucking Tech websites, all in the pocket of other big corpo's making artificial scarcity.",hardware,2025-12-19 17:31:36,-1
AMD,nux8y3k,"A couple of sales, (literally just a few) on ebay is no big deal...there's wacky people doing wacky things out there all the time.  The two or three people are pointing out on ebay are dwarfed by the normal sales flow from normal websites...     ""a thing happened ***once***!  let's all discuss it like it's a regular occurrence and changing the status quo! ""    it's all so tiresome.",hardware,2025-12-19 20:34:13,27
AMD,nuvxsbh,"Check eBay.  [https://www.ebay.com/sch/i.html?\_nkw=5800x3d+cpu&\_sacat=0&\_from=R40&rt=nc&LH\_Sold=1](https://www.ebay.com/sch/i.html?_nkw=5800x3d+cpu&_sacat=0&_from=R40&rt=nc&LH_Sold=1)  $800, no not quite.  Over MSRP?  Yes.",hardware,2025-12-19 16:36:48,23
AMD,nuybnat,"Yep.   https://www.microcenter.com/product/5007092/amd-ryzen-7-9800x3d,-asus-b650e-e-tuf-gaming-wifi-am5,-gskill-flare-x5-series-32gb-ddr5-6000-kit,-computer-build-bundle  CPU, memory and motherboard $679.00",hardware,2025-12-20 00:25:52,3
AMD,nuxd62y,> at [MARKET PRICE](https://youtu.be/5KXrQYWbbIs?t=17),hardware,2025-12-19 20:56:34,1
AMD,nv01auj,What motherbard ?,hardware,2025-12-20 07:46:05,1
AMD,nv2vhyh,"Yeah this headline is stupid. I see *new in box* 5800X3D selling for $500-600 on ebay, and plenty of used sales in the last few days between $380-500. The price has risen a bit, but not that much. It's been expensive ever since production of the 5700X3D ended.",hardware,2025-12-20 19:33:57,0
AMD,nuw7w2c,"And even then, sales VOLUME is what matters. There will always be people who can't do basic arithmetic, who will then buy at these prices, instead of selling their old system and getting something brand new. That doesn't mean that $400 for a 5800X3D is sound market pricing, or that it is worth this much.",hardware,2025-12-19 17:26:34,14
AMD,nux817d,"I bought a pair of 16 gig sticks in an attempt to beat the first round of tariffs. At the time I thought they were DDR4 and I wasn't paying much attention. They came in, I see they are DDR5 and think ""ok, future rig then."" They came in around 120 bucks. Fast forward to today, I check the price of the same pair of sticks, it's over 400. Genuinely considering selling the sticks for a better AM4 CPU.",hardware,2025-12-19 20:29:19,1
AMD,nv1uece,"I went from 2700X to 5800X3D and upgraded ram from 16GB to 32GB, so I'm good until Zen 6 at least. I got it for 340€ like 5 months after release in germany. If I sold it now I would make 50€ profit, its going for around 390-400€ used on ebay. Not bad.",hardware,2025-12-20 16:19:03,1
AMD,nux0gbj,"A 5600x will play games just fine, at the resolutions and settings people play games at you will still be GPU limited on a RTX 5070.",hardware,2025-12-19 19:49:24,1
AMD,nv10bcf,I went from the 5800x to the 5800x3d. Stuck the 5800x in my daughters PC instead of buying her a 5600x like I did for my son's PC.,hardware,2025-12-20 13:19:44,1
AMD,nuwm8c2,"It really only makes sense if you already have an AM4 motherboard. Otherwise, if you're doing a DDR4 build Intel 14th gen outperforms it.",hardware,2025-12-19 18:37:12,8
AMD,nv1zj4d,You can limit the power usage on the 5800x3d or 5700x3d as well if you find a deal on one of those.  I have the 5800x3d and the low power usage is great.  In a lot of games the chip is only drawing ~50W.,hardware,2025-12-20 16:46:19,1
AMD,nuwkmx2,Because no one sane would do that... they simply took the most expensive offer on eBay and made a clickbait title around it... I may list my old PC parts for 10k; maybe they shall made another such article...,hardware,2025-12-19 18:29:21,11
AMD,nuw3s4d,"Or it's discontinued and a sketchy seller is hoping to con someone who doesn't know any better. I've seen Ryzen 5 2600's going for 200+ on Amazon from 3rd parties while official channels were selling Ryzen 5 5600's for 130 and 3600's for 90.  Honestly, whenever I see a price on Amazon that's not remotely close to a whole number or 25 cent increment (eg. 137.53), I pause and see if something's fishy.",hardware,2025-12-19 17:06:12,13
AMD,nuw1zkt,"Seconding this, a lot of flagship CPUs released in the past 20 years are still absurdly expensive. QX9770, FX-60, P4 EE, 6950X, 9900K, etc. some of them more expensive than others, but the top CPU for a dead socket is still a pretty penny",hardware,2025-12-19 16:57:17,1
AMD,nuwafnv,Could also be used by sellers to fulfill insurance replacements.,hardware,2025-12-19 17:39:15,1
AMD,nuzi5cp,I thought we all loved Capitalism?,hardware,2025-12-20 04:59:16,1
AMD,nux0pp4,"You can still buy laptops for $200 on Amazon, they good enough to do most computing tasks.",hardware,2025-12-19 19:50:46,5
AMD,nuwepit,I had a PC in the 90s didn't know it was a luxury back then guess I was a lucky kid,hardware,2025-12-19 18:00:15,4
AMD,nuwjwna,"Compute has never been cheaper and fairer. You can get a Mac Mini right now for $450, which will do all you need, including light productivity, sans gaming and heavy 3D rendering etc, for the better part of the next decade.   All the harder compute that you need you can rent and get the best value, instead of shelling out for hardware that becomes obsolete within 2 years:  * for inference either get the subscription or pay as you go on Replicate, OpenRouter etc.   * for gaming get GeforceNow or similar service for $10/$20 a month. On MacOS it runs natively in AV1 with Cloud GSync, there's virtually no lag and even in very dark scenes you can barely tell it's a stream. Hardware in the back gets upgraded every 2 years, and 5 years of the service cost less than the GPU it's running on right now.",hardware,2025-12-19 18:25:46,-1
AMD,nv13sos,"I get your whole point, but I just want to pinpoint the wacky part, maybe they have a reason for it. Like, one quick example, they have the whole built already working, and the CPU dies or gets fried, or god knows what, and they want the same CPU, idk. Which goes hand in hand with your other point of it being 2-3 sales. The CPU is not being made anymore, so Ebay is the only alternative. Maybe even a collector. There are wacky people out there thought :P",hardware,2025-12-20 13:43:08,0
AMD,nuvyc81,Still too expensive   Still the price of a non x3d zen4/5 or raptor lake + ram kit which outperforms this,hardware,2025-12-19 16:39:29,19
AMD,nuy93oc,Yea I sold mine for $425 two weeks ago after upgrading to 9800x3D for $440,hardware,2025-12-20 00:10:24,3
AMD,nuwmtx7,"People are insane, in what world does that purchase make any sense.",hardware,2025-12-19 18:40:08,1
AMD,nuxvltb,"A fair number of these purchases are probably going to end up with fraudulent charges (item not as described, etc) because eBay tends to side with the buyer.",hardware,2025-12-19 22:36:46,1
AMD,nv2w89z,It's worth whatever it consistently sells at. It seems to consistently sell for >$400 used.,hardware,2025-12-20 19:37:50,1
AMD,nuwqrn0,"It's a lot like the ""car is worth $2000 but needs $2000 in repair"" work making it worthless but they're low credit high APR folks and have cash and $2000 in repairs makes sense to get it going another 2 years",hardware,2025-12-19 18:59:31,1
AMD,nv5shn8,Did you ever use more than 32? How often do you need more than 16 really? I'm asking because I stuck with just 2x8 and never had any issues yet.,hardware,2025-12-21 06:32:58,1
AMD,nv67xle,And they were right.,hardware,2025-12-21 09:04:18,1
AMD,nuzh31n,"Unless you play heavily CPU bound games, which many BRs, and simulation/factory games fall into.",hardware,2025-12-20 04:51:52,6
AMD,nv2wocg,"""Games"" are not a uniform performance load. Tons of games will be CPU bound with *any* CPU, including the 9800X3D, even at 1440p.  As always, know your workloads and purchase accordingly.",hardware,2025-12-20 19:40:14,7
AMD,nuwqvyj,"For productivity yes, but not for gaming.",hardware,2025-12-19 19:00:07,-3
AMD,nuwn1s8,"If you filter on Sold listings, people *are* paying $4-500+ for these.  It’s still insanity even if the headline is sensationalist.",hardware,2025-12-19 18:41:13,7
AMD,nuye7zb,Oh? I have a 6950X sitting in a closet…,hardware,2025-12-20 00:41:41,1
AMD,nvi69cl,"For the older ones it might be people wanting to build top-tier rigs of INSERT_YEAR?  I should get on with my want of a top 2012-2013 build, but knowing me I’d probably go seeking for silly things like Titan Z or ARES 2/3.",hardware,2025-12-23 06:37:02,1
AMD,nuw2ya1,I had a 9900K in my workstation at the office back in the day.  Solid performer for the money.,hardware,2025-12-19 17:02:02,0
AMD,nuxyrzw,Adjusted for inflation it’s around $7000. Yeah they were luxury,hardware,2025-12-19 22:55:10,4
AMD,nwlzxzc,"Got a used Pentium I in 1995 (my first PC) and it was about the equivalent of 80 USD, but that meant a lot more in local buying power.",hardware,2025-12-29 20:52:19,1
AMD,nuxwmxj,"Mac's can run games  (native ports) decently too, considering their loss power draw.",hardware,2025-12-19 22:42:45,2
AMD,nuxegrj,Stop being logical   We need 64gb and a 5090 to browse reddit and watch youtube,hardware,2025-12-19 21:03:20,3
AMD,nwm0c5u,> for gaming get GeforceNow or similar service for $10/$20 a month.  Streaming is not and will not be a valid option for gaming.,hardware,2025-12-29 20:54:14,1
AMD,nuxeceb,"Okay Sam, no need to advertise your shitty services.",hardware,2025-12-19 21:02:43,-1
AMD,nv62uqm,"Even then, it would be cheaper to get a whole new mobo, CPU, and RAM on AM5 and end up with better performance.  It's an absolutely ridiculous price, no matter what. You'd have to just be a blithering idiot to go for it.",hardware,2025-12-21 08:12:41,4
AMD,nv6lbmy,"If that situation forces a sale at $800 though, that does suggest that the market is very shallow.  Generally one goes on eBay and picks the cheapest or 2nd cheapest that doesn't look shady.",hardware,2025-12-21 11:17:45,1
AMD,nuwqh45,"One just sold for $600  Sold Dec 19, 2025    Brand New  [51 product ratings- AMD Ryzen 7 5800X3D Processor - NEW in Sealed Box](https://www.ebay.com/p/4053561416?iid=168015596458#UserReviews)  **$600.00**  or Best Offer  \+$7.50 delivery  Located in United States  [View similar active items](https://www.ebay.com/sch/i.html?_nkw=AMD+Ryzen+7+5800X3D+Processor+-+NEW+in+Sealed+Box&_id=168015596458&_sis=1)  [Sell one like this](https://www.ebay.com/sl/list?mode=SellLikeItem&itemId=168015596458&ssPageName=STRK%3AMEWN%3ALILTX)  gangster1234484 100% positive (522)     Being kind and logical, that's what you're missing :) <3",hardware,2025-12-19 18:58:05,-31
AMD,nv2vx8m,"The RAM kit alone is more than the 5800X3D.  People aren't buying the 5800X3D over a new AM5 processor. They're buying it over a whole new platform, which is ~$1000 including board and RAM.",hardware,2025-12-20 19:36:12,-1
AMD,nuzrjzn,What?  I'm a Top Rated Seller and very happy,hardware,2025-12-20 06:13:52,1
AMD,nwlxys6,Theres also folks who do their own repairs so thats 2000 in repairs becomes 200 in parts + couple of your weekends gone.,hardware,2025-12-29 20:42:37,1
AMD,nv0z33q,14700k slams a 5800x3d both in gaming and especially on productivity tasks.,hardware,2025-12-20 13:11:08,3
AMD,nux0vvp,5800X3D is [13% slower than a 14700K with DDR5-6000 in gaming](https://www.techpowerup.com/review/intel-core-i7-14700k/18.html). Unfortunately I can't find a direct comparison with the 14700K using DDR4.,hardware,2025-12-19 19:51:39,1
AMD,nv10uf1,"For some upgrading to AM5 would indeed cost a lot right now, but I also feel like a lot of people don't realize that the slowest AM5 CPU (7600X) is just as fast as the 5800X3D. Including for gaming.",hardware,2025-12-20 13:23:21,4
AMD,nv20xqj,Nice! Those go for around $125+ on eBay regularly. People want to max out their old rigs,hardware,2025-12-20 16:53:46,1
AMD,nwm06wu,As i am writing this reply from 32GB and a 4070S i can confirm we can do with a slightly less for reddit.,hardware,2025-12-29 20:53:32,1
AMD,nwnhqs9,"It absolutely is. It almost feels local. And on Mac it's AV1 so it even looks almost local. It for sure looks better than anything my PC can pull off.  I can play on my phone with a simple android controller. When Im visiting my folks, all I gotta do is download the app on their smart TV and log in, and pick right up since the saves sync via steam.  Back home I simply turn it on in my TV in the living room, on my laptop wherever, or in the TV in my room upstairs.  Not need to carry anything. Don't care if one TV is occupied. It's always dead silent, since theres no 500w machine that needs to be cooled.  You don't event need to take my word for it, you can try it for like 3 bucks for 48hrs.",hardware,2025-12-30 01:34:28,1
AMD,nuwzngm,"Pretty sure it's just not showing the ""Best Offer"" price it *actually* sold at.",hardware,2025-12-19 19:45:15,21
AMD,nuy0cew,What's so rude about what he said?,hardware,2025-12-19 23:08:57,7
AMD,nv2wy4l,No its not  Newegg has combo deals ram and good board for 400ish dollars  A crappy 7600 beats the 5800x3d,hardware,2025-12-20 19:41:43,1
AMD,nuxw4rc,"There are edge cases where no CPU can touch the X3D chips (e.g. Factorio, Baldur's Gate 3).",hardware,2025-12-19 22:39:50,3
AMD,nv3is85,"Hardware Unboxed's 5800x3d vs 12700kf (with ddr4) comparison was pretty much a wash, particularly if you are >1080p. Nominal win for the 5800x3d for gaming if you are being charitable. (I have both, but haven't done any real comparison testing, particularly because I run 4k.) So add on top some clock speed gains and there you go.",hardware,2025-12-20 21:41:10,3
AMD,nv3suby,Yeah I don't really get it. When they were sub 200 or even sub 150 for a 5700x3d it was an awesome buy but now it just feels like people panic buying.   Maybe they were waiting for zen 6 and just are afraid that ddr5 will be screwed for many years but I don't see the point in paying that much when you can still buy a faster 7700x microcenter bundle  even with 32 gb of ram for 500.  Even with ram like this I don't see why you would pay over like 250 tops for a 5800x3d.,hardware,2025-12-20 22:38:50,3
AMD,nwozpbc,You must be playing something like turn based games or what is the reason you arent aware of the massive input delays?,hardware,2025-12-30 07:26:12,1
AMD,nv788ag,Something to understand anout the average /r/hardware poster is that they believe that X3D chips are magic performance improvers that are instantly better than every other CPU ever produced by impossible margins.,hardware,2025-12-21 14:15:27,2
AMD,nv634a1,"Not sure why you're getting downvoted. Micro Center too has brought back their bundles with RAM for around $500. And, yes, a 7600 beats a 5800X3D. A 7500f goes toe to toe with it.",hardware,2025-12-21 08:15:24,2
AMD,nuy96u7,"Even in factorio once you go to [high spm comparisons](https://factoriobox.1au.us/results/cpus?map=9927606ff6aae3bb0943105e5738a05382d79f36f221ca8ef1c45ba72be8620b&vl=1.0.0&vh=) rather than the the low SPM ones, raptor lake/non X3D zen4/5 does fine.",hardware,2025-12-20 00:10:56,6
AMD,nwlynwf,Basically if your primary worload is larger than Intel L3 cache but smaller than the X3D cache and results in high cache hit rates for AMD and not Intel you get crazy boosts. Works in a lot of MMOs for example.,hardware,2025-12-29 20:46:03,2
AMD,nwlz5ux,"Its not that deep. A person wants to build a PC but does not know much about hardware. So he goes and watch some youtuber techfluencer for advise. He finds a video stating DDR5 is getting expensive, buy DDR4 (recent HUB video for example). He also finds video 5800x3D best DDR4 CPU for gaming. He looks no further and just buys those parts.  Got to remmeber most people dont follow the hardware news like we do. They spend 30 minutes on youtube and make their purchasing decisions.",hardware,2025-12-29 20:48:30,1
AMD,nwpyqrq,"On ShadowPC or PS+ Streaming, yes, there's a delay. On GeForce now, there's barely any.",hardware,2025-12-30 12:37:12,1
AMD,nwlx6fh,Depending on what your usecase the x3D can be magic. For example in mmos that get very CPU bottlenecked when there are a lot of players in the same place x3D due to much higher cache hit rates can mean as much as triple the framerate in raids (when it matters most).,hardware,2025-12-29 20:38:42,1
AMD,nvredov,Does Samsung actually have N2 equivalent node ready though? Since it's SF2 is basically just SF3 renamed.,hardware,2025-12-24 19:05:44,10
AMD,nvqd8sj,">As ebn notes, since 2nm currently represents the cutting edge, TSMC’s 3nm output in 2027 is expected to lag by at least two generations under Taiwan’s N-2 principle.  ~~14A~~ edit: A14 won't be out by 2027 though. Maybe if TSMC is counting A16 as a complete node jump, rather than what it sounds like as N2 with BSPD being only meaningful for HPC.  Problem is, and this applies to both Samsung and Intel, is if remaining what is realistically N-1 in 2027 is still enough for Samsung and Intel to actually benefit. Because I don't think the nodes they will have available then are going to be much better than the N3 family either.  >while Google’s TPU team reportedly visited the Taylor fab to discuss potential production volumes, as the search engine leader is currently moving to sell TPUs—previously used exclusively for internal workloads—to external customers such as Meta.  I would be very surprised if this is true, precisely because the reporting that Google is looking to sell TPUs to external customers, and still want to use what will likely be an inferior node to what other customers/competition may end up using on TSMC.  It's one thing to use a lower end node or less competitive processor for your own internal workloads, where the cost you save remaining internal can offset the disadvantages. It's different when you have to then turn around and sell that to external customers and make it worth it for them too though.",hardware,2025-12-24 15:46:03,5
AMD,nvt25yu,"""Google Eye"" making my tired brain thinking they renamed their smart glasses.",hardware,2025-12-25 01:28:21,1
AMD,nwaw7wl,Samsung is killing it lately.,hardware,2025-12-28 03:09:32,1
AMD,nvs4lgc,TSMC onmy has N4 in the US currently and the N-2 rule would prevent them from producing N3.,hardware,2025-12-24 21:39:16,3
AMD,nvt5sn8,"I feel like both Intel and Samsung think theirs belong to the same generation of TSMC 2nm simply because it uses GAAFET, when actual pitch and yield matter...",hardware,2025-12-25 01:55:57,3
AMD,nvqk7bx,"On your second point, I kinda read that as in ""(implied) has contracts to sell"", because of the specific reference to Meta.  If they don't have to compete to close the client, then they don't need to be paying for top-of-the-line nodes, do they?  The needs for the end product are probably specified in said contracts if those exist, and if they don't (yet) exist, then they are subject to discussion and change anyway...",hardware,2025-12-24 16:22:49,5
AMD,ntam20b,Well that’s not cured my impotency.  Damn.,hardware,2025-12-10 14:32:50,51
AMD,ntavky1,"Radeon subreddit is in shambles because it doesn’t support RDNA3, they’ve got pitchforks out and are claiming they’ll go nvidia next gen, lmao",hardware,2025-12-10 15:23:55,166
AMD,ntam26q,"I wonder if GN or HWUnboxed will roast AMD for their [misleading ""performance"" charts](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-5_videocardz.jpg) like they did for NVIDIA and MFG. I have no problem with Upscaling performance, but once you start introducing Frame Generation [like AMD has here](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-4.jpeg), you're muddying the waters of what is ""performance"".",hardware,2025-12-10 14:32:51,107
AMD,ntami4s,Launched without 7000 or earlier support despite the leaks.  lol?,hardware,2025-12-10 14:35:25,35
AMD,ntb2qza,AMD never misses an opportunity to miss an opportunity.,hardware,2025-12-10 15:59:29,28
AMD,ntalbqm,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-10 14:28:41,2
AMD,ntam44j,"Marxist-Leninist-based Upscaling. No wonder they're called Team Red. /j  This is an improvement, but I'm more excited for the hardware of UDNA.",hardware,2025-12-10 14:33:09,7
AMD,ntctzdb,The Ray Regeneration thumbnail is legit just a contrast filter lmao,hardware,2025-12-10 21:10:15,2
AMD,ntas2fn,Would they be using their ML/AI cores of e.g Strix point/Krackan point for this too?,hardware,2025-12-10 15:05:33,1
AMD,ntanqfv,Hopefully this leads to them FINALLY being competitive with nvidia in the high end again sooner than later,hardware,2025-12-10 14:42:19,-3
AMD,ntcmnc9,"It's really obnoxious that they released this but didn't roll it out to Adrenalin yet. I had to DDU and reinstall it twice, since Windows overwrote it immediately with 25.10.30 the first time.  AMD owes me 10 minutes of my life back, is what I'm saying. /firstworldproblems",hardware,2025-12-10 20:33:53,0
AMD,ntazpqs,does this work on RDNA 3.5? (890m specifically?),hardware,2025-12-10 15:44:40,0
AMD,ntcoxp6,"This wasn't the road I wanted AMD to pursue, the ""fake frames"" like Nvidia currently getting a lot of heat from.  But it just shows that AMD has no guidance except copying everything Nvidia does. Grow a pair and just make your technology better because it **IS** good right now, just not in the test metric Nvidia wants to push on consumers which is basically a big fat lie in promises and practical performance.",hardware,2025-12-10 20:45:18,-7
AMD,ntamg91,Still balding here....,hardware,2025-12-10 14:35:06,27
AMD,ntb8it8,"It'll probably at least partially support rdna3 eventually, but it's pretty obvious that AMD just needed to get this out into the wild with at least rdna4 support asap.",hardware,2025-12-10 16:27:39,88
AMD,ntcmb31,"I mean yeah, RDNA3 doesn't have the physical hardware for this ML-based stuff. If they bring Redstone features to RDNA3, it'll be entirely different.",hardware,2025-12-10 20:32:10,34
AMD,ntfq0pp,"I was on AMD for my last 3 GPU upgrades.  I had a Radeon 6800, then i saw AMD announcing that they'll put that card series into legacy support, which slightly pissed me off. They still make and sell 6000 series cards.  Then there's the fact that partial FSR4 support is possible on older cards, but not released or enabled by AMD.  I don't really care about the upscaling part of it, the 6800 chewed up any game i threw at it at 1440p without RT. I wanted it because TAA or FSR3 are horrid when it comes to image quality when you use them as AA solutions.   Playing something like Final Fantasy 7 Rebirth was a travesty if using TAA or FSR 3. Such a beautiful game that looks like a smudged mess with those solutions.  So i got an Nvidia 5080 for black friday sales. I basically just don't trust AMD's GPU division to not abandon even their 9000 series once they release a new series. And i'm done giving money to a  company division that is content in merely keeping their cards in ""Nvidia -50$"" price range for much lower feature sets.  I HATE AND LOATHE Nvidia as a corporation and hate that i gave them money, but i ultimately just picked the better product for my needs.  Their CPU division is banging and my 5800x3D looks like it'll keep chew anything i throw at it for a good while still, but AMD's GPU division can go fuck itself for now.",hardware,2025-12-11 08:34:32,18
AMD,ntbbpeq,Anyone with a brain could look at RDNA3 and realize it wasn’t a major architectural shift over RDNA2. People read about a couple of low-precision math instructions and assumed RDNA3 had closed the gap with Turing. Honestly I don’t expect AMD to truly lock their feature set in until UDNA launches.,hardware,2025-12-10 16:43:16,58
AMD,ntb6rwl,Not so Fine wine now eh? Lol.,hardware,2025-12-10 16:19:10,54
AMD,ntcyiq9,"My problem whit the 9070XT, which isn't a cheap card by any standards, is the issues I came up against given the supposed 2.1a ports. The ports are not full bandwidth, I have multiple 4K monitors connected and get stuttering, freezing, and timeout issues constantly. I have to manage the displays as if I purchased a cheap card, lowering Hz here, color range there, etc. I can't run all my monitors at full specs at the same time! RIP I should have purchased a 5070ti...  Having to run my LG C5 and Alienware AW2725Q at 60Hz is crazy. Switching settings every time I want to play is such a pain.",hardware,2025-12-10 21:32:27,4
AMD,ntj8d0k,I just build a amd pc after many years with just laptop and I saw Radeon subreddit. Is AMD really deserve that hate or Radeon subreddit is just that toxic :D?,hardware,2025-12-11 21:16:28,1
AMD,ntl9fo1,"They shouldn't worry too much, as even RDNA4 doesn't support it in almost any use cases except for very specific games.",hardware,2025-12-12 04:17:34,1
AMD,ntcvekr,I went from 7900XTX to 5080 exactly because of this. And VR,hardware,2025-12-10 21:17:14,-4
AMD,ntcnufo,Some of us refuse to use FSR lol,hardware,2025-12-10 20:39:53,-8
AMD,ntarmxb,HUB is saying it sucks.   https://www.youtube.com/watch?v=LpAZF_-qsI8,hardware,2025-12-10 15:03:14,115
AMD,ntbyua6,"What's misleading about this? Nvidia specifically marketed their GPUs as ""4x"" performance or whatever and compared GPUs with FG/DLSS off with DLSS and FG on. These charts from AMD are specifically performance charts for FSR which means the point is FG performance.",hardware,2025-12-10 18:36:06,20
AMD,ntaqf4f,Gn have published a video but I have yet to watch it,hardware,2025-12-10 14:56:49,8
AMD,ntb92yp,AMD's GPU market share has gone so down that most people actually don't care.,hardware,2025-12-10 16:30:25,7
AMD,ntb6f8r,"I don't think they will do a ""AMD IS LYING!!!!"" with a stupid thumbnail like they did to Nvidia.",hardware,2025-12-10 16:17:27,4
AMD,ntbhezv,oh NOW frame generation is bad when AMD gets their hands on it  give me a break,hardware,2025-12-10 17:11:30,-30
AMD,ntbg65a,"amd repeatedly said before this launch that it was exclusive to rdna4, people's own fault if they decided to assume they were lying",hardware,2025-12-10 17:05:17,80
AMD,ntb5kvy,"I actually thought it wasn't leaks, but AMD's own statements that claimed something like them wanted to make this adoptable for multiple older architectures, and things out there. Maybe I misunderstood that. Either way, I'm glad I went with Nvidia last generation.",hardware,2025-12-10 16:13:22,15
AMD,ntanwj0,"It looks dissapointing, so you didnt lose much",hardware,2025-12-10 14:43:14,11
AMD,ntbxa5h,"Leaks from where? If it was some of the typical suspects, I'm *shocked* that they would make shit up.",hardware,2025-12-10 18:28:36,2
AMD,ntb70jl,They are RDNA 3.5 and ML Redstone is only for RDNA 4,hardware,2025-12-10 16:20:19,14
AMD,ntas74e,How can they be competitive with nvidia in the high end if they don't have any high end RDNA4 graphics cards and Redstone is exclusive to RDNA4?,hardware,2025-12-10 15:06:14,51
AMD,ntavics,Lol. It’s not even 4x frame gen. How would they compete with a mid level card like the 9070xt?,hardware,2025-12-10 15:23:33,3
AMD,ntj3kia,Actually that Windows owes you time...,hardware,2025-12-11 20:52:24,2
AMD,ntb022h,"Nope, RDNA4+/UDNA (we presume will also support this) exclusive.",hardware,2025-12-10 15:46:21,14
AMD,ntecnj2,This is what AMD has always done.  It’s part of their origin story.  AMD literally copied Intel’s silicon to break into the CPU market back in the 70s,hardware,2025-12-11 02:14:40,4
AMD,ntcjoin,Holy shit my teeth are straightened and I think one grew back,hardware,2025-12-10 20:19:10,11
AMD,ntft1mn,"FSR4 INT8 already runs on RDNA1-2 and 3. Even some RDNA1 and Radeon 7 on GCN5.  XeFG also runs on DP4a or SM 6.2 path on the same GPUs.  If Intel can, certainly AMD can. They just don't want to.",hardware,2025-12-11 09:05:22,15
AMD,ntd7hfj,"Yeah. I think many people have unreasonable expectations. Still, we know there's an int8 version of fsr4 upscaling out there which works pretty well. If AMD just officially published that I suspect a lot of RDNA 2&3 owners would be pretty happy. (Maybe with the added promise of trying to achieve something similar with FG and ray reconstruction)",hardware,2025-12-10 22:16:23,21
AMD,nticeee,Yes they do. Shader cores are more versatile than matrix cores and can do everything they can at a lower efficiency/performance. AMD gating FSR's newer versions and NVIDIA gating DLSS behind never hardware with that excuse is bullshit.,hardware,2025-12-11 18:36:11,2
AMD,ntfnaem,Yeah but AMD claimed “Architectured to exceed 3.0Ghz”. It was a major architectural shift since even RDNA4’s boost clock did not exceed 3.0Ghz,hardware,2025-12-11 08:07:01,1
AMD,ntbx4os,AMD Vinegar™️,hardware,2025-12-10 18:27:52,67
AMD,ntfkqcu,This myth is so heavily reliant on the R9 290X surpassing the GTX 780Ti its not even funny.   Probably users younger than those cards in here lmao,hardware,2025-12-11 07:42:14,10
AMD,ntd8jgx,But muh drivers,hardware,2025-12-10 22:21:46,12
AMD,ntdupr2,"not sure how old your card is, but is just got it recently and im running 120hz on lg c4 and lg ultrawide monitor(not sure exact model) on 240hz no issues.",hardware,2025-12-11 00:26:28,14
AMD,nuj4tli,...at 60Hz? That don't sound right. You sure your cables are up to spec?    Even RX 6000 could already do 4K120 10b HDR.,hardware,2025-12-17 16:07:34,2
AMD,ntaygim,"Yes, it was a good video, discovered what other outlets did not and [Tim also did mention that Frame Generation does **not** increase performance, just perceived smoothness.](https://youtu.be/LpAZF_-qsI8?t=793) Kudos to that video, they (or Tim) did great work.",hardware,2025-12-10 15:38:30,37
AMD,ntcouu3,It's still presenting the FPS increases without any context for how compromised the experience is compared to regular frames. But I agree that using it as a tool to lie about performance uplift of a new product compared to the old one is considerably more dishonest.,hardware,2025-12-10 20:44:55,10
AMD,ntfe2o5,"> What's misleading about this? Nvidia specifically marketed their GPUs as ""4x"" performance or whatever and compared GPUs with FG/DLSS off with DLSS and FG on. These charts from AMD are specifically performance charts for FSR which means the point is FG performance.  What's misleading is they're making out that the FPS you're getting is more performance and making the game ""faster"" (says it in the top right of the chart, their words not mine), yet the latency is increased and delays inputs, that's anything but faster or more responsive gameplay. If this was just upscaling I would have no problem, but as I said, once you start adding Frame Gen as ""performance"" and making the game a less responsive experience, it's not faster, you're delaying inputs for perceived smoothness in the image. Both practices are dumb and misleading and I do not advocate for either what NVIDIA or Radeon have done with marketing their products. What NVIDIA has done is worse, but go to the root of both marketing strategies and both AMD and NVIDIA are pretending like Frame Generation = more responsive and more/faster performance.",hardware,2025-12-11 06:40:10,-1
AMD,ntayv9q,"I saw it, he doesn't even mention the graphs being misleading from the slidedeck (it's clear from the video he has access to the slides), but GN does look at latency results. In the end, I wish he was harsh like he was with NVIDIA because honestly, AMD is just copying NVIDIA's homework and using the same BS playbook, pretending Frame Gen is increasing frame rate and [making the game's performance ""faster""](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-5_videocardz.jpg) (their words not mine).",hardware,2025-12-10 15:40:31,44
AMD,ntffh8o,"DW bro [we got you caught in 4K calling them ""fake frames"" just earlier this year.](https://www.reddit.com/r/hardware/comments/1n0qtts/nvidia_geforce_rtx_5090_and_the_age_of_neural/navh6bm/) Before you say ""that's just one post or is a joke, [here you are doing it again in a separate thread](https://www.reddit.com/r/hardware/comments/1n67v2u/steam_hardware_software_survey_august_2025/nc3jscw/). Enjoy!",hardware,2025-12-11 06:52:47,10
AMD,ntcwqkw,"Bro lmao, every time I see you defending AMD like their white knight. Redstone is clearly DLSS 1.0 and AMD rushed to release it because they're getting stomped in software features.  Though in AMD fashion, they manage to incredibly disappoint as always and reviewers are just letting you know.",hardware,2025-12-10 21:23:44,15
AMD,ntcdfr5,"There is a leaked INT8 path for FSR4 that works on older hardware and has been implemented by others (i.e., on Linux in Proton-GE).  It works pretty good already, which makes AMD's reticence to put it out officially *baffling* -- especially since they aren't putting RDNA4 into APUs for a while yet, and want to sell a bunch of those in gaming-focused handhelds and Steam Machines.  AMD has a good software product here for once and they need a broad installed base to drive developer adoption, but they don't seem to care and it's infuriating.",hardware,2025-12-10 19:47:23,9
AMD,ntb7jhe,"No one cared about the new framegen (on older cards), what people wanted is FSR4 upscaling on RDNA3/2, which already has been proven to work well on Linux",hardware,2025-12-10 16:22:51,21
AMD,ntb7i81,"They aren’t even RDNA 3.5, they are XDNA 2. That’s why I asked whether they will leverage XDNA 2.",hardware,2025-12-10 16:22:40,1
AMD,ntcwcvn,"4x frame gen is really niche, there are so few cases where it actually makes sense and doesn't cause an unacceptable amount of artifacting and/or latency. You pretty much need a 240hz+ monitor, for one thing. I don't imagine that being a major factor in almost anyone's purchase decision.",hardware,2025-12-10 21:21:54,10
AMD,ntecyfo,"4x FG is a gimmick at this point. Maybe FG progresses to the point that it's viable in the future, but it really isn't right now.",hardware,2025-12-11 02:16:30,0
AMD,ntj3wwv,Yeah that's fair. Windows loves to replace new drivers with shitty old ones against your will.,hardware,2025-12-11 20:54:08,1
AMD,nthzskw,"The fact that the INT8 version got leaked the way it did says...something.  It is pure speculation at this point by anyone except AMDs management as to why they have not released drivers that include INT8 for older RDNA versions but I think based on the independent testing from that leaked version that not officially releasing it is bad.  It is one thing to want to sell new cards but it is quite another to have something like the INT8 be out in the wild and then try to ignore its existence for the owners of cards not really that old.  And given how they recently they tried to put cards that they still sell into ""maintenance mode"" it really does seem like some parts of AMDs management is not making good decisions for their customers.  Now maybe their data shows that such decisions are better for their quarter to quarter bottom line but I really do question if that is the case.  I'd have to see some *hard* data to prove to me that whatever additional profits they are making but implementing these decisions are adding value to the company/brand over these anti-customer moves.",hardware,2025-12-11 17:34:59,7
AMD,ntfwepl,"It's pure market segmentation. Even if it doesn't work the same, they could still allow older cards to get it.",hardware,2025-12-11 09:40:20,4
AMD,nu1sgb7,People use fsr3 to play at higher frame rates. So why would they release something that almost entirely fails to do that on the most sold gpus which are the low end 6600s and 7600s?,hardware,2025-12-14 21:41:03,0
AMD,nuj45yt,"It runs, but performance impact is considerable and inconsistent and quality isn't on par with FP8 either, no?   Could be they decided against releasing it in this state because of it.",hardware,2025-12-17 16:04:21,0
AMD,ntfpmhl,The unreasonable expectation that cards that are still being sold get feature support,hardware,2025-12-11 08:30:26,22
AMD,ntiko9j,My 280X survived until Overwatch 2.,hardware,2025-12-11 19:16:24,1
AMD,ntfjb4m,"I have had enough discussions with them, and the thing they call driver is adrenaline software which is driver control panel, clearly not understanding what a driver is.",hardware,2025-12-11 07:28:33,7
AMD,nte6pop,The only benefit I can think of is maybe better motion clarity by pushing the FPS to 200+ or something if you have a really good monitor (i.e. an OLED). But even then the hit on latency means it would only work well for games where latency is not hugely important.,hardware,2025-12-11 01:38:48,5
AMD,ntbm7zy,"If I recall, the main harshness on Nvidia was the way they were trying to push reviewers to only review performance with frame generation enabled, an inherently dishonest take.  To be fair, I also haven't seen the video on this yet either, but I think ""frame gen generally sucks in these ways"" is pretty well established and those factors are unlikely to change drastically",hardware,2025-12-10 17:35:11,32
AMD,ntaza57,"I saw first 3 minutes, he did said he didn't had much time for this video  but still doesn't excuse the ""less harsh"" opinion on amd.  Will update this after watching whole video",hardware,2025-12-10 15:42:33,2
AMD,ntco5v8,"Both are bad, at least this is being used to show \*gains* from having the feature on or off, rather than presenting it as \*gains* over the previous generation of cards for the purpose of representing a new product's performance as higher than it really is",hardware,2025-12-10 20:41:29,2
AMD,ntcge1q,Proton-GE doesn't implement the INT8 model it's the FP8 model running through the cooperative matrix extensions added to Mesa.,hardware,2025-12-10 20:02:33,11
AMD,ntba3rb,"I mean, why did you expect this? They never said they will do it.",hardware,2025-12-10 16:35:28,5
AMD,ntc43xm,Another reason to dump Windows,hardware,2025-12-10 19:01:04,-1
AMD,ntccvm5,"I wish they would.  Isn't the NPU in the 90x0 an XDNA also?  That's what is doing this upscaling.  But, who knows if Windows is locking it down for CoPilot, or if it is accessible in the same way as the one in the GPU (since it is technically part of the CPU).  I bet they will figure it out though.  With Intel about to release their own competitor to it (maybe... no GPU benchmarks yet), they will want to use all they have to combat it.",hardware,2025-12-10 19:44:30,2
AMD,ntflv8o,Yes everything is a gimmick until AMD releases a shittier version of it and then gets praised sky high.,hardware,2025-12-11 07:53:10,4
AMD,nteo83v,With a 4k240hz oled (or any other ultra high refresh rate oled) mfg 4x is literally transformative despite the 2 Steves telling you otherwise.,hardware,2025-12-11 03:26:02,5
AMD,ntfhasu,"Seconding what the other comment says, it's great.",hardware,2025-12-11 07:09:21,1
AMD,ntfwryr,The market segmentation is stupid.  Why NOT want to sell more RDNA3 GPUs besides RDNA4? WHY make consumers reluctant to buy more AMD products? It's so stupid.,hardware,2025-12-11 09:44:05,0
AMD,nu1tg5u,"Path Tracing, DLSS Transformer and Ray Reconstruction Transformer can run on laptop 2050. Or MX 570.  What's your point exactly?",hardware,2025-12-14 21:45:58,2
AMD,nuj4d7t,Performant impact isn't that considerable.  It's consistent.  Quality is almost identical.,hardware,2025-12-17 16:05:20,1
AMD,ntcrhll,"Because it was already developed, exists, and all they had to do to get the easy win was launch it officially.   But they chose not to do so while still making every APU including $1500+ Strix Halo products that could use it their only option.",hardware,2025-12-10 20:57:47,10
AMD,ntctx6t,"It’s a bit more complicated. Krackan point/Strix Point are APUs, is everything on the same chip so… why not?  Also I’m on Linux, firmware/drivers are there but there is no program/frameworks using them… So it’s purely a drivers issue from amdgpu to actually schedule compute on the NPU.",hardware,2025-12-10 21:09:57,0
AMD,ntl8x1f,You guys say this but people still consider FG a gimmick despite FSR3 supporting FG for years now (and doing a decent job with it too tbh unlike the upscaler).,hardware,2025-12-12 04:14:28,2
AMD,nthvii0,Not every negative Nvidia comment is a pro-AMD comment. Stop promoting tribalism for 2 giga-corporations that don't give a shit about you.  I genuinely don't think 4x FG is a valuable feature at this time. The latency hit and the image degradation are not worth the smoothness.,hardware,2025-12-11 17:14:06,1
AMD,nthilxh,"I really expected them to announce an MFG, you know damn well that they must have been working on it since the moment they caught wind of Nvidia having it.  It must be a lot harder to do as well as it's already being done by Nvidia than people expect - everyone seems to think everything these days is practically just checking a box off.",hardware,2025-12-11 16:11:17,1
AMD,nth33tu,"You think shareholders are going to like that old product is cannibalizing new? This is exactly what happened with the 1080 Ti.  Sure, it's irrational from a consumer and engineer perspective, but nobody cares about them. They only care about the shareholders.",hardware,2025-12-11 14:52:51,4
AMD,nu2efaq,Doe dlss transformer only give 5 extra fps on a 2050?  And path tracing is designed to look good not help games run better. Your example would make sense if there was somehow less light bounce than in rasterized modes.  Fsr4 on rdna3 fundamentally fails at the thing its most used for.,hardware,2025-12-14 23:37:23,1
AMD,ntff06j,"The xdna npu is a completely different architecture (based on xilinx IP) than the rdna4 ml extensions, which are a set of new shader instructions.  They don't really have anything in common in terms of architecture, I'd be surprised if the amdgpu driver ever ""supports"" both, as it'll be effectively adding an entire new driver stack beneath that interface for the npu, and much of that interface would simply be not relevant to the npu (and likely the npu will need new interfaces that aren't relevant to the GPU side of things either). It'll just be functionally 2 different drivers sharing a name.",hardware,2025-12-11 06:48:30,1
AMD,nthxujn,"Neither of the things you mentioned are even remotely true. Latency hit is negligible if you're close to base FPS of 60 or higher, Nvidia Reflex is far far better than AMD's Anti lag. And Reflex 2 will kill the latency debate with FG once and for all.  And personally I haven't noticed any image quality issues either. The FG model they trained is very good. It's an amazing technology for me, I have a 360 Hz OLED monitor and it's sublime to game in the  200-360 FPS range.   Also let's not pretend to not know fans of which corporate, Intel, Nvidia or AMD represents a literal cult.",hardware,2025-12-11 17:25:25,1
AMD,ntl813r,"Lol, the 'shareholders' do not care about Radeon consumer products. Most AMD shareholders probably aren't even fully aware of these DIY discrete GPUs, they are a rounding error in the business.  The amount of sales lost to people buying RDNA3 over RDNA4 due to FSR4 is essentially $0 in the grand scheme.",hardware,2025-12-12 04:09:10,2
AMD,nus4b0g,It would be hilarious if a Samsung chip has newer RDNA IP compared with AMD’s own APUs.,hardware,2025-12-19 00:48:25,47
AMD,nurxs6p,Is Juno expected to be a mobile version of RDNA4?,hardware,2025-12-19 00:10:46,16
AMD,nurt5xq,">PhoneArt reckons that the prime core will reach a maximum clock frequency of 3.9 GHz. A commenter, Erencan Yılmaz, reckons that this figure should be reduced to 3.8 GHz, due to power consumption considerations when looking at a 2 nm GAA-based design.  What does this even mean? Are they implying that GAAFETs have poor power frequency scaling at the highest end of the V/F curve or something?   Anyway, interesting to see the % Fmax gap between TSMC and Samsung based designs for the P-cores remain around the same.",hardware,2025-12-18 23:43:22,6
AMD,nus8su5,Very interesting. I would like to know more,hardware,2025-12-19 01:15:15,4
AMD,nv7bqho,"Like Google, Samsung has already taken steps to move away from ARM GPUs.  The end game is to replace the ARM CPU with a RISC-V one. These steps are just for readiness.",hardware,2025-12-21 14:37:22,1
AMD,nusuuy0,It's actually quite probable since it actually features AI upscaling and frame generation and rdna 3.5 doesn't support it,hardware,2025-12-19 03:26:02,29
AMD,nurzhys,"> What does this even mean? Are they implying that GAAFETs have poor power frequency scaling at the highest end of the V/F curve or something?  If you look at the actual comments, they supposedly have their own leaks of the specs showing 3.8. The power consumption comment was a reply to some rando comparing clock speeds to Snapdragon.  Just lazy reporting.",hardware,2025-12-19 00:20:39,24
AMD,nusnf8n,"Money  Making GPU tech from scratch is stupidly hard. And it’s going to be hot garbage for a few generations until you polish it.  Look at Intel, despite making iGPUs for decades, still had tons of trouble making decent architecture for performance.   Qualcomm bought their GPU tech from AMD (was ATI back then).  Apple managed to do it, but I bet they were working internally for a few iterations before it was good enough to ship.   Just licence the GPU and not reinvent the wheel if you don’t need too.",hardware,2025-12-19 02:41:28,5
AMD,nuspwmp,"Apple didn't make their own gpu from scratch tho. It's more like a custom power vr gpu in it's first iterations. The first ""custom"" apple gpu is a power vr gpu with ""image block"". Which is actually a feature that no other tile based gpu had, only the adreno 840 released this year has this feature, imagination and Mali don't have it yet.",hardware,2025-12-19 02:56:00,4
AMD,nut4dot,"I think people forget a fully featured GPU arch is not just a collection of ""dumb"" SIMT compute elements.  There is a lot of arcane knowledge. Modern GPUs have so many specialized parts that are domains onto themselves - display drivers, video encoders/decoders, TMUs, RT engines, schedulers, memory & cache management....",hardware,2025-12-19 04:24:55,2
AMD,nusq211,Yea I wasn’t even confident they did it either. Just further showcases how difficult it is.,hardware,2025-12-19 02:56:54,6
AMD,nujwgoc,"If you saw the similar Hardware Unboxed video from a few days ago, this one agrees with it and presents the info in a different way.  AMD urgently needs to fix this.",hardware,2025-12-17 18:24:13,89
AMD,nuka0y6,Very clear Redstone needed more time in the oven. Also it's going to struggle to gain traction unless they add support for older cards.,hardware,2025-12-17 19:31:31,47
AMD,nuk936d,The frametime issues are truly catastrophic. Looks worse than when I would force Crossfire on games that didn't support it even. I don't understand why they thought it was a high-quality release to represent the brand. WTF man.   At least it looks good and they can theoretically fix the frame pacing. They never did on FSR 3.,hardware,2025-12-17 19:26:50,39
AMD,nukpkp9,Maybe Nvidia was up to something with their Flip Metering stuff. The frame pacing of DLSS FG/MFG is flawless.,hardware,2025-12-17 20:49:06,34
AMD,nuksrii,FSR 3.0 all over again. Their framegen was also unusable on launch. They really never miss a chance to miss a chance,hardware,2025-12-17 21:05:06,15
AMD,nuzrsfr,"V-Sync on driver level, cap frame rate -3 of your refresh rate   It mitigates the frame pacing in CP2077 and completely solved it on different games  I know it's a crutch but at least it's something until they solve it.",hardware,2025-12-20 06:15:59,2
AMD,nv4pyk9,"It seems fixable by software (driver), so I do not despair as much as many others.  They needed something out to show their progress.  I'm hopeful it will get much better in the coming months.",hardware,2025-12-21 02:03:43,2
AMD,nulpsw5,"What confuses me about this is that the framerate and frametime graphs displayed by MSI Afterburner in many games tend to not be flat with DLSS-FG on my 4090. In fact, FSR-FG often appears flatter. However, the DLSS-FG tends to subjectively feel smooth to me (so long as it's not inheriting stutter from the rendered frames).  Does anyone have any explanations for this in light of the HUB and DF videos? Could the flip metering hardware of the 50 series be playing a significant role here (I think both HUB and DF used 50 series cards to compare FSR Redstone to)? Is there an issue with using MSI Afterburner's framerate and frametime graphs for this purpose (I can't seem to post a screenshot unfortunately)?",hardware,2025-12-17 23:58:13,0
AMD,nukcko7,"With nvidia bowing out of consumer GPUs next year, AMD is lining up the fine wine perfectly.   When they fix it next year, I hope the media covers it equally as positively as they were critical.",hardware,2025-12-17 19:44:12,-34
AMD,nuof036,"Yeah, I think in Digital Foundry's podcast they called out the Hardware Unboxed content as excellent and basically said ""I am not sure we even need to make a video now, but we will"".  And I think it's good that they did, more attention on this can only be a good thing.",hardware,2025-12-18 12:51:53,23
AMD,nulojj0,"They will fix it in RDNA5, just like how adding frame generation was a “fix” for FSR3 and it being limited to RDNA4",hardware,2025-12-17 23:51:08,27
AMD,nunwy0x,It might be an unfixable hardware flaw.,hardware,2025-12-18 10:14:20,7
AMD,nukdehz,Why urgently? It's an optional feature.,hardware,2025-12-17 19:48:16,-87
AMD,numqkwq,"The ""time in the oven"" is releasing the feature in RDNA5, not in RDNA4. Just like how frame generation was a demo feature for RDNA3, Redstone is a demo feature for RDNA4 with the real version in RDNA5.",hardware,2025-12-18 03:48:57,19
AMD,nutn9rx,Nvidia did it by adding ML hardware support to cards well before they were needed with the RTX 2xxx cards. It's surprising AMD waited so long to do it. They must have thought traditional algorithms would work just fine.,hardware,2025-12-19 06:45:16,5
AMD,nukldy9,"Yeah seems so. However, if they have to spend more engineering time/power on improving advanced features I would expect porting them to the older gens is pushed further down the timeline.",hardware,2025-12-17 20:28:05,8
AMD,nukdrqz,What did you expect though? It’s AMD. Their software is always a couple years behind.,hardware,2025-12-17 19:50:00,23
AMD,nukzyif,Don’t they have flip metering on RDNA 4?,hardware,2025-12-17 21:40:49,12
AMD,nuu8czd,"it's not flawless, unfortunately. for some games though. yes, it's miles better than FS FG, but there is still room to improvements  take Indiana Jones with path tracing for max GPU load, take RTX 5090, run it with 4xFG without frame cap and check msbetweendisplaychange with capframex. it will have the same sawtooth graph with some short lived frames, but to a lesser degree ofc. it will be very noticeable to the naked eye on OLED monitors because they will flicker due to those variations  reflex by itself (and reflex is forced on when FG is used) also adds not so perfect frametimes that can be seen with msbetweendisplaychange in some heavy games (Cyberpunk 2077 would be another example)",hardware,2025-12-19 10:07:32,2
AMD,nulqznp,"Has anyone done some god quality testing on the frame pacing of no flip metering vs flip metering? The only such coverage I recall finding is this is [this Gamers Nexus clip](https://youtu.be/Nh1FHR9fkJk?t=1922), but they only tested this on two games, and only one of the two showed an obvious framepacing improvement from the 4090 to the 5090.",hardware,2025-12-18 00:05:00,4
AMD,num6zny,"Someone should remind them of that old adage: ""Better to remain silent and be thought a fool than to open your mouth and remove all doubt.""",hardware,2025-12-18 01:42:44,11
AMD,nv6v9dw,"Unfortunately, this only solves the tearing issue, but doesn't completely address the frame pacing issue. It also deprives us of the low latency benefits of Antilag2 and adds a sync delay, although not as significant as without the frame rate cap. The increase in latency can be easily verified using the reflex monitoring built into the Optical scaler.",hardware,2025-12-21 12:46:18,3
AMD,nulx49m,"There are different statistics that you can use to populate your frame time graph, each of which are valid depending on what you’re trying to show.   Pure frame time measurement, as in “this is how long it takes to process each frame” is valid. But so is ‘ms between presents’ and ‘ms between display change’, the first being the timing of the frames being presented to the render queue, and the latter being the rate that the actual display updates and shows the new frame. Both of these measurements are captured by presentmon and frameview. I’m not sure exactly what measurement afterburner uses, but I suspect they are measuring pure frame time. But if you looked at time between display change it would probably show the issues that hardware unboxed and digital foundry showed.",hardware,2025-12-18 00:40:32,12
AMD,nwjgh4k,DF wrote their own software which physically inspects frames to measure frame pacing because they found the existing apps were insufficient. They talk about in one of the podcast episodes from 2022 or something like that. Its in the segment where they talk about automating the part that makes the graphs and it saving them a lot of time.,hardware,2025-12-29 13:17:39,1
AMD,nukejjq,Its rumored they will reduce production however nothing is confirmed. Nvidia still makes A LOT of money from gaming and wil lnot be giving it up. They were about $100 million short of a new record in gaming revenue last quarter.,hardware,2025-12-17 19:53:43,23
AMD,nukeu9i,People said they will bow out of GPUs since 40 series just launched,hardware,2025-12-17 19:55:10,26
AMD,nukn50r,"With nvidia bowing out of consumer GPUs next year   People has been saying this for YEARS now and their gaming market share/revenue has only gone UP and UP. I don't know why anyone with enough sanity would believe this stupid narative   And let me ask you, if shortage hits Nvidia and forces them to reduce gaming GPU production, why would you think AMD will be safe from it?",hardware,2025-12-17 20:36:55,17
AMD,nukdlhh,Nvidia aren’t bowing out. They’re just reducing production on the 5000 series. They’re also going to be launching the 6000 series.,hardware,2025-12-17 19:49:11,14
AMD,nur3s7f,lol,hardware,2025-12-18 21:25:30,1
AMD,nukqlr8,"[amd isn't nvidia, they don't release slop. they want to make sure they realease quality products for gamers](https://www.reddit.com/r/hardware/comments/1nw18md/comment/nhdqwi7/)     \- you",hardware,2025-12-17 20:54:18,121
AMD,nukeqz0,"> With nvidia bowing out of consumer GPUs next year, AMD is lining up the fine wine perfectly.  If they don't fix this ""optional feature"" (and future ""optional features"") AMD will be lining up a miraculous market share loss against no competition.",hardware,2025-12-17 19:54:43,41
AMD,nukhhd6,If they leave things broken developers will ignore Redstone. Redstone is supposed to be AMD's answer to Nvidia's suite of DLSS features.,hardware,2025-12-17 20:08:21,40
AMD,nukk6if,"because all of this is hurting their brands even further, as if gating FSR4 (ML) behind RDNA4 didn't piss people off enough.",hardware,2025-12-17 20:21:58,11
AMD,nutl10s,"There doesn't seem to be anything preventing RDNA4 from running this correctly, it has support for hardware flip-metering. AMD engineers just fucked this implementation up and need to fix the software.",hardware,2025-12-19 06:26:10,9
AMD,nwjflfe,"well, to be fair Nvidia utilized ML hardware in 2xxx cards a year after the release and before any other card releases. So the hardware adding was needed for their planned use.  Meanwhile, while Nvidia was already using ML for DLSS2, AMD publicly stated that ML and AI will cause Nvidia to go bancrupt and AMD wont do it. Well, we see the results and how long AMD took to change direction.",hardware,2025-12-29 13:11:51,1
AMD,nuknxc1,"If I had to choose between them supporting my card fully and them fixing up and keeping Redstone competitive, I'd take the latter.  I bought my card for the features it had at the time of purchase. I didn't expect future new stuff beyond maybe FSR4. Making an official WMMA / INT8 version for games to fall back on would be more than enough, but I don't expect that to come.",hardware,2025-12-17 20:40:53,10
AMD,nuke9sa,remind me who had gpu driver issues this gen again?,hardware,2025-12-17 19:52:25,-26
AMD,nul5srb,"Hadn't heard about it but they support ""Hardware Flip Queue Support"", which I think is the same thing?  But they [advertise it](https://www.notebookcheck.net/fileadmin/_processed_/2/e/csm_RDNA_4_Architecture_Press_Deck_page-0005_768d67dd27.jpg) with the following benefits:  1. Offloads video frame scheduling to the GPU 2. Saves CPU power for video playback  I don't think it has a role in frame generation, or even gaming, I think it mostly has to do with video playback.  Maybe it is the same thing and Redstone is just bad at frame pacing anyways?",hardware,2025-12-17 22:09:50,12
AMD,nums78s,"As someone who came from a 40 to 50 series GPU, I can tell you it's amazing. It literally fixed VRR flickers for me and the pacing is flawless. The effect is exacerbated if you have an OLED display as it has near instant pixel response time.   The end result is a flicker-free smooth gameplay. It's hard to explain but it feels like I'm playing games on a thin fabric, it's that good.   So if anyone's on an OLED and hates bad frame pacing with VRR flickers, upgrading to a Blackwell GPU is the way to go, thanks to its HW flip metering logic.",hardware,2025-12-18 04:00:18,11
AMD,num59uo,I can only talk from personal experience but I have a 40 and 50 series gpu   I find Frame gen literally unusable on the 40 series card whereas I can literally not tell it is on with the 50 series   It felt like fucking magic to me   The 50 series is also a lot faster overall but I went up to 4K at the same time and am getting less frames so it’s not just more performance,hardware,2025-12-18 01:31:38,8
AMD,nv0zl6n,You should apply this to yourself instead of typing it out bro,hardware,2025-12-20 13:14:44,0
AMD,numbwu8,"> But so is ‘ms between presents’ and ‘ms between display change’, the first being the timing of the frames being presented to the render queue, and the latter being the rate that the actual display updates and shows the new frame.  So I take it that the former is the time between frames entering a queue of frames to be sent to the monitor, while the latter is the time between those frames actually being sent to the monitor?  Anyways, I installed PresentMon, and using various metrics:  - FrameTime-Display - FrameTime-Presents - FrameTime-App - Ms Between Display Change  I couldn't notice any difference in these graphs between DLSS-FG and FSR-FG in Cyberpunk and Avatar (This is on 40 series, so no flip metering). With MSI Afterburner, the lines for both framerate and frametime appeared much flatter for FSR-FG in both games (even though it didn't subjectively feel smoother than DLSS-FG to me).  At times, FSR-FG has felt noticeably _less_ smooth than DLSS-FG in Avatar, but they both felt about the same on this particular occasion.  Others are reporting that DLSS-FG felt much smoother to them after upgrading from 40-series to 50-series, so I wonder if DLSS-FG isn't much smoother than FSR-FG on the 40 series. Also, I wonder if some of the FSR-FG framepacing issues are inconsistent, getting okay-ish frametimes on some occasions, but othertimes getting awful frametimes in the same game.",hardware,2025-12-18 02:13:40,2
AMD,nuktpl9,so what does that mean for consumers if nvidia is going to reduce production but still wants similar gaming revenue?,hardware,2025-12-17 21:09:51,-5
AMD,numzn9c,Wym they're clearly bowing out right now. They only shipped 11 million GPUs this quarter compared to amds 900k.,hardware,2025-12-18 04:55:46,14
AMD,nukkow2,> They’re also going to be launching the 6000 series  That's a mid 2027 launch at best,hardware,2025-12-17 20:24:33,5
AMD,nukdskf,"40% is significant, and you know they're going to be top-heavy too, they're not going to waste hardware on budget 6060s. $3k GPUs is inaccessible for 99% of gamers that it's basically bowing out of the segment.",hardware,2025-12-17 19:50:06,-1
AMD,nul5hin,Perfection,hardware,2025-12-17 22:08:15,52
AMD,nun2fx7,"To be fair, if it's true Nvidia is cutting GPU supply by 40% soon, AMD will probably have no problem clearing their inventory if it's the only thing available at a reasonable price.",hardware,2025-12-18 05:17:42,3
AMD,nukoloi,"You realize most people pick up Radeon GPUs because they're incredible value for money. It's the raster and VRAM that is the attraction, Redstone is just a cherry on top.  Seriously, this is textbook example of loss aversion. Had there been no Redstone everyone would have been fine, now we get something as a bonus and although it's not quite ready yet it somehow diminishes the original value of the product?",hardware,2025-12-17 20:44:16,-63
AMD,nukpi6r,"Don’t worry, I don’t think its an either/or. Its just an order of priority. Ignore the doomsayers, Radeon’s given every indication they intend to bring FSR4 to RDNA3. They aren’t even putting RDNA4 in their APUs in 2026, so supporting it going forward is pretty much a necessity for those lower power devices.",hardware,2025-12-17 20:48:46,-4
AMD,nukmb3b,"Literally right now, as we speak, NVidia drivers are mostly great while AMD struggles with whole host of problems introduces by fresh branch.",hardware,2025-12-17 20:32:43,41
AMD,nuu24sd,Adrenalin has been crashing so much on my windows clean install that I just said fuck it and installed a Linux distro to see if the problem is software or hardware lmfao  What a horrible purchase I made with my 9070 XT. I regret it SO MUCH. By the way I bought it because of FSR4 support specifically.,hardware,2025-12-19 09:05:35,6
AMD,nuoum01,Dude the current 25.10 drivers are the worst they've been in years and some people are even using 2024 May drivers because for some reason later drivers cause hard pc shut downs in the Spiderman trilogy.,hardware,2025-12-18 14:28:47,10
AMD,nur7u9d,"Nvidia, AMD, Intel. You just hear about Nvidia's more because they sell like 95% of all GPUs",hardware,2025-12-18 21:45:46,5
AMD,nukeqnu,Oh right i forgot AMD has the best software. Their upscaling tech is years ahead of everyone else.   I heard they give out free handjobs with each GPU purchase. They’re just that good! That’s why everyone owns one right?,hardware,2025-12-17 19:54:41,7
AMD,nun5sfk,"I can't remember if it was Digital Foundry or Hardware Unboxed, but someone mentioned it was the same thing. The video frame scheduling on GPU.",hardware,2025-12-18 05:45:33,4
AMD,nv8fpxc,"wow, that's good to know. honestly this makes me want to downgrade from a 4090 to a 5080, lol. frame gen on 40 series is almost unusable due to VRR flicker, i had no idea 50 series fix it.",hardware,2025-12-21 18:09:57,2
AMD,nutnt8d,On a 4070 Super I can't tell when frame gen is on. I used it in Cyberpunk to get above 60 FPS. The base framerate was in the 50's with all the cool path tracing stuff and I couldn't tell it was starting in the 50's.,hardware,2025-12-19 06:49:56,1
AMD,nund103,">This is on 40 series, so no flip metering  I believe both Hardware Unboxed and Digital Foundry showed the issues specifically on Radeon GPUs using FSR FG, No?  There are going to be differences between how AMD and Nvidia handle frame pacing, even outside of the FSR/DLSS conversation. There could be something about how Nvidia handles frame pacing that is better for FG, but is not a part of DLSS FG specifically.",hardware,2025-12-18 06:50:00,3
AMD,nuto4cf,Nvidia will increase prices.,hardware,2025-12-19 06:52:37,2
AMD,nukm1u4,So then you agree they’re still making GPUs then?,hardware,2025-12-17 20:31:25,16
AMD,nukeabb,Everything after your first sentence is pure speculation. Nvidia has like 95% of the GPU market. They aren’t just going to sell 6090s lol,hardware,2025-12-17 19:52:29,23
AMD,numkmf5,"Friend - they'd reduce production on top tier as  * AMD doesn't compete on that level * their OEM numbers are probably 60-65% of their volume  If anything, they will pump out 6060s to keep AMD out of Steam Surveys since the die will be miniscule thus high yield per wafer, in consumer eyes, and most important - in affordable products thus increasing their user base and indirectly influence.  They can afford for 6090 tier buyers to eat the cost - as they've done willingly for halo GPUs since reviews existed.",hardware,2025-12-18 03:08:46,10
AMD,nuklwvj,Chip yields increase exponentially as area is reduced linearly. They will try to sell traditionally 50-tier sized chips in 70-tier cards and be not much less profitable than the huge AI chips while hedging against the bubble popping.  They could also do another generation of dual fabs where Samsung or even Intel produces consumer chips while TSMC fabs for their data centre designs.,hardware,2025-12-17 20:30:44,7
AMD,nusvzrm,In 2021 Nvidia GPUs were all sold out and going for 2-3x MSRP (when you could actually find one) because of crypto and AMD still couldn't take advantage of the situation.,hardware,2025-12-19 03:32:56,7
AMD,nukv46n,"And they are 'incredible' value for their money because they offer similar features. No one is going to buy an AMD card without FSR, FG, etc... in todays market.   Raster time is over",hardware,2025-12-17 21:16:50,39
AMD,nukydp9,"Hmm no, If I wanted raster I would buy last gen used or on sale as usual, this gen was different because it's supposed to be the one that gets upscaling, ray tracing and frame gen right so NVIDIA tax becomes unjustified.  I'd get a new GPU to step up to 144fps and that requires both upscaling and frame gen, actually I could do without ray tracing but the reason to have a >60fps display is that frame gen is supposed to be ok at higher frame rates. Redstone is not.",hardware,2025-12-17 21:32:59,23
AMD,numj3yr,"I always love that response in a literal thread about AMD's driver/software issues/bugs.  I'm surprised bro didn't just say  ""I have a 6600 XT and have no issues.""",hardware,2025-12-18 02:58:53,23
AMD,nuvvf4g,Sounds like a console fits better for you,hardware,2025-12-19 16:25:27,-1
AMD,nun6ur9,This is no surprise. The 5070 Super 16GB will be the new 6080,hardware,2025-12-18 05:54:31,1
AMD,nukvsco,"you realize the dram shortage plays in AMD's favor right?  game devs aren't going to optimize their games, your best hope is nvidia figures out their fake vram neural texture compression just so you could have the privilege of paying $800+ for a xx70 gpu with MAYBE 6gb of VRAM in 2026/2027  and by then AMD will have ironed out Redstone, maybe even be on UDNA and have it backported to GPUs with 16gb+ of VRAM  raster will prevail",hardware,2025-12-17 21:20:10,-51
AMD,nupdx5h,8GB at any rate with how difficult it will be to acquire VRAM,hardware,2025-12-18 16:12:15,-2
AMD,nukw3my,"Most NVIDIA cards except for the 5070 have the exact same VRAM as the AMD counterpart, so I don't know how this plays into AMD's favour 😂  Also Redstone wasn't important according to you, now it's suddenly important",hardware,2025-12-17 21:21:43,43
AMD,nukx0jf,"talking mid to long term, when nvidia cuts 40% of gpu production, they will recoup costs by selling $1000 midrange gpus while AMD will continue to provide GPUs with more vram at better value and feature parity  redstone will be fixed, that's the point.",hardware,2025-12-17 21:26:12,-7
AMD,nukxbqj,"Ah yes, because AMD is obviously not affected by a GLOBAL shortage and definitely doesn't need to cut production and raise the price  The fact that Samsung just reported that they have no stock at all definitely won't affect AMD but only NVIDIA",hardware,2025-12-17 21:27:45,38
AMD,numi3sc,"Last quarter shipping numbers were 94% to 7%  NV cutting it be 40% is only ~38% drop, still flooding the market >6:1 over AMD.  These people are over dosing on the kool aid.",hardware,2025-12-18 02:52:29,23
AMD,nupdek7,But have you considered that AMD is our lord and savior?,hardware,2025-12-18 16:09:39,12
AMD,nu6itoa,Venice is the next release. This would have leaked ages ago unless they are going for dual supplier.,hardware,2025-12-15 16:43:40,67
AMD,nu85hvw,"AMD already announced they taped out Venice on TSMC 2nm back in April.  [https://www.amd.com/en/newsroom/press-releases/2025-4-14-amd-achieves-first-tsmc-n2-product-silicon-milesto.html](https://www.amd.com/en/newsroom/press-releases/2025-4-14-amd-achieves-first-tsmc-n2-product-silicon-milesto.html)  If they use Samsung at all, It is probably going to be something else.  That said, there are going to be Zen 6 EPYC SKUs that use different chiplets (there are at least two chiplet variants).  So it is possible that ""Venice"" is referring to a family of CPUs and there could be some Samsung based SKU in there.  But I think it is more likely that this headline is wrong, and AMD is contemplating Samsung for a different product.",hardware,2025-12-15 21:30:25,28
AMD,nu9olpx,If i had to guess AMD is probably looking to offload lower end and mobile SKU's to Samsung to save on costs and get more TSMC production dedicated to more important datacenter/mainstream silicon. so Samsung might take over APU silicon for desktop and mobile along with motherboard chipsets and maybe some entry level desktop GPU's and most laptop GPU's.,hardware,2025-12-16 02:44:07,7
AMD,nu7hoe6,"AMD and TSMC have been pretty intimately close ever since AMD ditched global foundries. Also TSMC isn't too kind on companies that are unreliable partners, AMD just hopping to Samsung would be pretty shocking",hardware,2025-12-15 19:31:50,11
AMD,nu6ds4n,Moving away from TSMC is always a good thing,hardware,2025-12-15 16:19:06,16
AMD,nuabdii,"Ram shortage.. You need a deal for quota package memory and GPU, if not.. Who's gonna buy a new CPU.",hardware,2025-12-16 05:17:15,2
AMD,nu752yb,TSMC ?: [https://www.tomshardware.com/pc-components/cpus/amds-first-2nm-chip-is-out-of-the-fab-epyc-venice-fabbed-on-tsmc-n2-node](https://www.tomshardware.com/pc-components/cpus/amds-first-2nm-chip-is-out-of-the-fab-epyc-venice-fabbed-on-tsmc-n2-node),hardware,2025-12-15 18:31:07,1
AMD,nu6e6ma,Why would AMD hamstring their very populair and performant server line by switching to a subpar node? Delays or price hikes at TSMC?,hardware,2025-12-15 16:21:02,-1
AMD,nu70t3j,So they were unable to secure capacity at TSMC?,hardware,2025-12-15 18:10:59,0
AMD,nu75jht,I thought Venice was old news a long time ago.      [https://www.techpowerup.com/review/amd-3800-plus-venice/](https://www.techpowerup.com/review/amd-3800-plus-venice/),hardware,2025-12-15 18:33:16,20
AMD,nu6tsr2,"Probably not for launch, but filling up capacity later on.",hardware,2025-12-15 17:37:25,5
AMD,nu7zru4,Possibly Samsung will be building the IMC,hardware,2025-12-15 21:02:00,4
AMD,nu9vsh3,Maybe lower end skus?,hardware,2025-12-16 03:27:56,5
AMD,nu9bwyi,Maybe iodie? Gpu?,hardware,2025-12-16 01:28:18,2
AMD,nu9w62q,"Moving from TSMC's 2nm process to Samsung's 2nm would likely be a big downgrade. I believe AMD would have to compromise on frequency, power efficiency, or yield and even in the worst case it has to tweak the architecture.",hardware,2025-12-16 03:30:22,3
AMD,nugamgk,The power efficiency throws itself out of the window,hardware,2025-12-17 03:32:04,1
AMD,nucd02i,"It goes both ways. if TSMC provides capacity to their latest nodes to every company that asks for it, giving AMD less allocation than they want, then AMD has to look elsewhere to fill the gaps. Can't rely on any one company for everything.",hardware,2025-12-16 15:05:37,6
AMD,nuh2iti,They are business partners not spouses in a relationship,hardware,2025-12-17 07:04:28,3
AMD,nu70z4r,"Unless you want to compete at selling the best high performance accelerators, then it makes sense why it is AMD and not Nvidia",hardware,2025-12-15 18:11:47,-10
AMD,nu78ka4,"Yea, at best it's dual sourced, or maybe it's going to be used for the IODs, if Venice actually uses Samsung (which I doubt).   FWIW, another prominent (though I'm very dubious about how accurate he is) twitter leaker, jukon, thinks it is for the PS6.   IMO, this rumor ends up going nowhere. We went though similar things with Samsung 4 and 3nm.",hardware,2025-12-15 18:47:26,3
AMD,nu6ijlw,"Because AMD has significantly more understanding and insight regarding the processes involved, than random gamers?",hardware,2025-12-15 16:42:19,57
AMD,nu6i1o2,"Gives them more leverage in negotiating prices with TSMC, also could be looking at Samsung as a second source; AMD is probably doing enough volume now in the datacenter market that they can justify the costs of adapting their design to Samsung's fabs, and then use the Samsung chips for lower end datacenter products.",hardware,2025-12-15 16:39:55,5
AMD,nu6gnkd,GPUs more important?,hardware,2025-12-15 16:33:09,1
AMD,nuaayv0,Vertical integration in CPU/GPU/memory is now the hottest thing. Amd wants one contractor to provide all the comments and and can have a streamline input in the entire stack without delays in communication,hardware,2025-12-16 05:14:07,1
AMD,nu6ngps,Because TSMC has too much demand and can’t keep up? They have also raised their pricing a absurd amount as they had no real competition,hardware,2025-12-15 17:06:09,1
AMD,nu89vub,"Why would Nvidia hamstring their own very popular GPU line by switching to a subpar node with their Ampere generation?   Not saying I fully believe the rumor, just stating that it's not a totally implausible suggestion.",hardware,2025-12-15 21:52:15,0
AMD,nu6h06j,"Writing is on the wall with China and Taiwan, AMD won't be the first to move either.",hardware,2025-12-15 16:34:51,-5
AMD,nu74azc,Capacity for Venice was secured eons ago. They're already sending samples to hyperscalers...,hardware,2025-12-15 18:27:28,9
AMD,nu77vyr,wow TPU is way older than I thought lol,hardware,2025-12-15 18:44:16,16
AMD,nuaq2v8,"I remember that specific article! What a trip down the memory-lane … Thanks for this!  > Advanced Micro Systems (AMD) has released a new revision of their Athlon64 S939 […]  *Oh dear, the glorious socket 939 and its Athlon&nbsp;64* — Makes me a bit melancholic already.   *That was the time to be alive*. That was true journalism at heart from enthusiasts!   Explaining every short and abbreviation with the respective written-out long version and bringing out pieces for actual hardware-hits (instead of today's hit-pieces over the next refresh-cycle), where you could readily feel the joy of the editors themselves writing the article, describing new hardware between the lines.  Not the nonsense clickbait sh!t we have to day, which fabricate news around a single ~~Twitter~~ *~~X~~* *Twix*-post from some leaker no-one knows anyway …  Back then, you could go days, not seldom even a week without a single hardware-news, and no-one bat an eye, as it was only fueling anticipation and building up anticipatory excitement.",hardware,2025-12-16 07:24:53,5
AMD,nuxvnlr,Athlon 64 3000+ was my first CPU that I got brand new. I upgraded from a very ageing Pentium 133MHz. 8 years of new releases brought 13.5x of clock speed increase as well as a lot more instruction sets. Now my current PC is on an over 9 years old platform...,hardware,2025-12-19 22:37:02,1
AMD,nu877jr,"Doesn't really make much sense given it'll go against their super successful scaling strategy with their CPU products.  They cant just swap TSMC for Samsung chiplets and have everything all work out the same because they're designing these Epyc packages based on a very specific die size for the CCD's.  This makes scaling super easy for them in so many ways.  Adding in some new CCD based on a wholly different process tech seems like it would throw everything out of whack, no?  They've never done anything like this before.  They've always had the same process tech for all CCD's of the same type, for all ranges of CPU's of that architecture.  Dense/C versions are a bit different, but those are also a strategically produced different line.  You wouldn't do that just for a different process technology alone.",hardware,2025-12-15 21:38:58,8
AMD,nu9dy42,"I/O die only makes sense if they are getting wafers cheap as they don't benifit from the leading node density as much, but they may like it for low power.  Midrange GPU would make sense.  One of the mid/low end APUs would make sense too.  PS6 handheld or even the main console APU could potentially work as well, but that would require consistent parametric yields since they don't have opportunities for performance binning.",hardware,2025-12-16 01:40:33,6
AMD,nuh2g8l,Versus current gen 5/4nm?,hardware,2025-12-17 07:03:49,1
AMD,nu77pv1,"Venice isn't their AI GPU lineup, and AMD already confirmed they will use TSMC N2 for their MI400 series IIRC.",hardware,2025-12-15 18:43:29,15
AMD,nu88ok3,"This is about CPU's, where AMD is currently the top dog.  Much like how Nvidia used Samsung for Ampere GPU's while still being competitive, it's possible AMD could utilize Samsung for Zen 6 and still be very competitive.  Especially because AMD's whole chiplet scaling strategy is still a lot more cost effective than Intel's messy bullshit.  Epyc CPU's are also not typically being pushed to higher limits, so efficiency sweetspots matter a lot more, and that's not always gonna be such a huge difference in terms of performance per watt.  It might hurt them a bit more in consumer, but overall they're probably pretty confident in their architecture teams to maintain improvements without relying entirely on process node gains.  Intel's P-core team definitely has plenty of question marks surrounding it since Alder Lake.",hardware,2025-12-15 21:46:13,8
AMD,nuas24r,"> Because AMD has significantly more understanding and insight regarding the processes involved, than random gamers?  Dude, don't you think such a assessment is a 'lil bit too much to drop casually?! o.*0*  *This is Reddit!* 90% of users here are armchair-generals or virtual CEOs, who actually know their sh!t …",hardware,2025-12-16 07:44:09,4
AMD,nu6yvrt,That and Nvidia and Apple bought up all of TSMCs 2nm production time.,hardware,2025-12-15 18:01:46,11
AMD,nu806xp,"This is just a meaningless appeal to authority with no logical backing. Moreover, any decision to make chips in Samsung, whether they turn out true or not, will also involve non-technical factors like cost. They could very well choose to go with a subpar process if it is way cheaper. Beancounters often hold significant decision-making power.     And historically Samsung has had a number quality issues versus the having the same architecture made at TSMC, so these concerns are pretty fair.",hardware,2025-12-15 21:04:10,5
AMD,nu6omfc,Perhaps this would make more sense if this wasn't:   * just a rumor  * no mention on whether this was for CCDs or IODs  * being fabbed at Samsung which has a horrendous track record   But sure.,hardware,2025-12-15 17:11:51,5
AMD,nu6pawf,"that's not a reason to choose Samsung over TSMC, try again",hardware,2025-12-15 17:15:16,-2
AMD,nu6vep5,"FWIW that is not necessarily how prices are negotiated between designers and foundries.   The roadmap has more effect, and TSMC, Samsung, or Intel being in the picture is rarely used to drive prices lower.   Sure competition among fabs and packagers may set the ballpark of costs. But designers rarely use the threat of going with a different process to get a lower/better price from TSMC, Samsung, etc.   If AMD is going w Samsung it is likely because Samsung's roadmap may align w AMD requirements for a specific design.   The reason why this news may be unlikely is that AMD does not have an stablished silicon team with Samsung (that I am aware of). And that usually is how you can tell any type of significant volume from a large design team is going to be on a given foundry.   However Samsung does have a very nice roadmap for the type of large dies AMD has in their pipeline for their monolithic SoCs (mainly their APUs and GPUs). So there could be some alignment there.   For the CCDs and IO chiplets + 3D cache, it seems AMD is very aligned with TSMC and the packaging flow there.",hardware,2025-12-15 17:45:18,5
AMD,nu6nkji,"I understand dual sourcing for lower end SKUs, but the article makes it sound like the entire new EPYC line is moving to SF2, a very likely worse node than N2. This would impact the performance and potentially allow Intel to close the gap further, so I’m struggling to understand why. If this article is true, there could be something off about N2.",hardware,2025-12-15 17:06:40,1
AMD,nu6qamx,"Most analysts have AMD as a top 5 TSMC customer, and Venice is a flagship product from AMD.   It's hard to believe Mediatek can tap TSMC N2P for their next smartphone chips, but AMD couldn't for their high margin server CPUs?",hardware,2025-12-15 17:20:07,5
AMD,nwip1vg,Because samsung made them a dirt cheap offer and Nvidia had no competition at the time.,hardware,2025-12-29 09:29:00,1
AMD,nu6pkp9,"Every year there's new rumors about AMD using Samsung for their 4nm and 3nm node, and then people like you always say the same comments like this, and then it never ends up happening. But then a new Samsung node gets announced, and the cycle repeats.   *Sigh.*",hardware,2025-12-15 17:16:36,8
AMD,nuatm27,"Yeah, the Chinese overtaking Taiwan we're told to happen the next Monday morning since the 1970s …  Even announcing another round of *»This is the year of the Linux-Desktop going mainstream«* has more credibility to it, and actually is becoming reality rather sooner than later, thanks to Valve's Steam now, also their SteamDeck.  ---- Intel just dug up that age-old Taiwain-spectre, to scare investors and governments into submission for subsidies, and y'all damn fools all bought readily into that nonsense and manifested a threat, China really couldn't care about less.  *China has a host of other problems of greater importance* — Taking out Taiwan, will inevitably result in vaporizing it for EVERYONE, which wouldn't help Peking/Beijing one bit anyway to begin with. It's a futile undertaking and they know it.",hardware,2025-12-16 07:59:10,1
AMD,nu78t9q,I still remember being excited about getting a Denmark (Opteron 165 - basically dual core Venice) and OCing it like crazy.  Those were the days... when dual core felt like overkill and a 50% OC - clocking higher than the fastest available SKU - could be reached with the stock cooler on an entry level part.  today's CPUs a way better and way more boring. Almost zero point to OCing.,hardware,2025-12-15 18:48:37,17
AMD,nuaraja,"> They cant just swap TSMC for Samsung chiplets and have everything all work out the same because they're designing these Epyc packages based on a very specific die size for the CCD's.  Hasn't they've done so before already? AFAIK a bunch of CCXs were dual-sourced (TSMC, Samsung) and AMD even opted for *a three-pronged strategy* on sourcing (TSMC, Samsung, GlobalFoundries) for a single design of their CCXs (Ryzen, Threadripper, Epyc), no?  > They've always had the same process tech for all CCD's of the same type, for all ranges of CPU's of that architecture.  You think that Samsung's and GlobalFoundries' 14nm back then were identical down to the last bits?  Yes, they were the same process, but GloFo surely made quite a bit of custom tweaks on their own, don't you think?",hardware,2025-12-16 07:36:41,3
AMD,nwiomnj,AMD desperately needs a I/O die upgrade so this may be their solution.,hardware,2025-12-29 09:24:55,1
AMD,nufdwq6,"> as they don't benifit from the leading node density as much, but they may like it for low power.   AMD has been wanting to jump up to DDR5-9000 for a while if you've been reading the tea leaves/what comes out of their PHY guy via AGESA update, but the I/O die simply isn't up to the task as currently built. Jumping up to 2nm for a faster switching frequency and just adding a <profanity> load of transistors to make it less fragile solves that problem adequately. It's not elegant, but what's the point of more advanced materials (or transistors, as it were) if you never use them?",hardware,2025-12-17 00:15:24,1
AMD,nuarqgk,"> Especially because AMD's whole chiplet scaling strategy is still a lot more cost effective than Intel's messy bullshit.  Still baffles me how Intel can't let go of their big-die philosophy, tanking their margins and destroying yields that way since years while even crippling their overall volume — Still times higher manufacturing-cost than AMD.",hardware,2025-12-16 07:41:00,1
AMD,nu71w8e,LOL. They did not. Both AMD and Intel are also doing bring up of high volume SKUs on N2.,hardware,2025-12-15 18:16:07,12
AMD,nu9ejj0,Nvidia has no products on 2nm. And AMD was first to tape out on 2nm with this CPU. Meaning they get all the early production to themselves. Think.,hardware,2025-12-16 01:44:04,3
AMD,nu6t6vv,"Interesting. Yet, I am quite certain the people at AMD and Samsung (or TSMC for that matter) still have significantly more understanding and insight regarding the processes involved than random terminally online gamers. Just a hunch.",hardware,2025-12-15 17:34:26,7
AMD,nu6pcwn,">I understand dual sourcing for lower end SKUs, but the article makes it sound like the entire new EPYC line is moving   Which makes no sense because AMD already confirmed that Venice will be fabbed, at least for some part of it, on TSMC 2nm.   >SF2, a very likely worse node than N2  You are probably right, but the article also mentions SF2P, which very well could be Samsung's next real node jump after SF2 having very minimal PPA benefits over SF3 GAP.   So maybe they could catch up to TSMC 3nm rather than being decently worse than it, as they currently are.   >This would impact the performance and potentially allow Intel to close the gap further, so I’m struggling to understand why.  TBF I doubt Intel and Samsung are going to be in much different places if Venice is fabbed on SF2P and DMR on 18A, and AMD's design side also just gaps Intel's, so they could still win at what could be considered node parity.",hardware,2025-12-15 17:15:32,5
AMD,nuepyfg,It definitely feels like an ego thing that they didn't just copy Ryzen's super successful and ultimately quite straightforward chiplet scaling strategy.,hardware,2025-12-16 22:01:47,1
AMD,nu720x0,Who goes first? :),hardware,2025-12-15 18:16:44,-1
AMD,nu9ijc0,Late 2026,hardware,2025-12-16 02:07:50,-1
AMD,nu845yn,Your own recent posts are out of touch even if you claim authority because you're a director.  These Samsung foundry rumors are like Intel foundry rumors. I'll believe it when I see it.,hardware,2025-12-15 21:23:46,4
AMD,nu75gkk,">Interesting. Yet, I am quite certain the people at AMD and Samsung (or TSMC for that matter) still have significantly more understanding and insight regarding the processes involved than random terminally online gamers. Just a hunch.  Maybe. However, that may be why u/heylistenman is asking why AMD is rumored to be switching foundries, to get insights from people more involved in the industry than random gamers.",hardware,2025-12-15 18:32:53,4
AMD,nu6ye24,We’re not allowed to discuss this rumor as mere enthousiasts frequenting a forum about hardware? Why do you think AMD would move EPYC to SF2 instead of N2?,hardware,2025-12-15 17:59:26,2
AMD,nuk1fn8,"> It definitely feels like an ego thing that they didn't just copy Ryzen's super successful and ultimately quite straightforward chiplet scaling strategy.  Well, I think Intel at least *tried* to copy AMD's chiplet-paradigm helplessly for the last couple of years …  It seems that Intel was taken totally by surprise on anything Chiplets and got caught with their pants down (again), when AMD brought it this quick to market, when having worked on them for a decade plus.  Though IF Intel would already worked on anything *disintegrated silicon* for said 10 years by then (like Intel claimed when announcing their heterogenous '*Mix and Match*'-approach around 2018), *Intel would* ***not*** *have needed +6 years for finally reaching a fairly comparable design-approach* only half a decade later in 2023 with Meteor&nbsp;Lake (also Arrow Lake, Sapphire Rapids, Ponte Vecchio).  Since sure enough, all these disintegrated designs \*somehow\* brought Intel truly massive troubles engineering-wise and they had tremendous difficulties to overcome those for years — The years-long horror-stories over validation and dead-on silicon past tape-outs on *Sapphire Rapids* and *Ponte Vecchio* for example are testament to that … *Meteor Lake* was also everything but a performer.  ---- The issue at hand for Intel was quite many-layered …  * Intel hardly knew how to do it (despite claiming otherwise for years). *Shocker!*  * Intel had no real equivalent to AMD's SuperGlue aka *Infinity&nbsp;Fabric*™   That's for sure the most striking one, obviously — The reason they had to wait for PCi-E 5.0 to become a thing, for ""Intel's"" *CXL* to finalize upon it (which in itself is basically just AMD's former *CCIX* in disguise as Copy-pasta anyway done out of spite).  * Intel haven't remotely had adopted a design-strategy by then, which would've offered so to speak ""intelligent"" chiplets/tiles, which could be freely thrown together randomly at will on a (PCi-E) bus (which AMD actually evidently can with their CCX and I/O-dies since ages).  So Intel's claims before the press in 2018, of already working on disintegrated silicon since years already (and that AMD *weren't* actually as spearheading as they became), was pure virtue-signaling, a blatant lie.  As a result, Intel's IP-blocks still remained virtually *dumb* (in the sense of *head*|*less*) for years on out afterwards and most of it had to be *re-engineered from scratch* in all this other trouble like lay-offs, down-sizing or Intel's road-maps being constantly thrown out again (only to start over once more).  The reason why AMD had a years-long edge on chiplets from the onset, was since Intel just couldn't place or handle the stuff as ***independent*** *IP-blocks* — Everything was grown intercoupled and -linked together.  AMD on the other hand already had a decade-long headstart, due to their already well-tuned modular concept of IP-blocks (building-block principle with IP-blocks to freely chose from) attuned for the console-era from back then for the console-contracts (those, Intel always made fun of since).  So Intel allegedly been working on chiplet-esque designs for years already, was utter bullsh!t, and that's actually the sole reason WHY Intel struggled so hard for years with anything Tiles — They started at 0.",hardware,2025-12-17 18:49:09,1
AMD,nuk6muc,"> It definitely feels like an ego thing …  Well, yeah. It definitely IS a ego-thing for Intel — They called them *Tiles* purely out of spite.  Since you can't just call basically the very same what your competitor has, the same name, can you?   That's not how Intel rolls nor would have ever done anyway, even if doing so, would've saves them a lot of engineering-pain in the arse, even more teething-problems and cost them years of falling back behind their only lone competitor …  Truth be told, Intel's *Tiles* are basically in essence just Copy-Pasta: *A botched Copy'nPaste-job from AMD's chiplets*, yet relabeled as 'Tiles', just for Intel trying to pretend to have their ""own"" chiplet-esque implementation, even if it's basically the very same …  Wanna hear a joke? When Intel back then around 2018 out of the blue announced their heterogenous *Mix and Match*-stuff (pretending, they'd already worked a decade on this by then), [the actual effing PowerPoint-slides actually didn't even incorporated anything called 'Tiles']( http://web.archive.org/web/20210923105815/https://newsroom.intel.com/wp-content/uploads/sites/11/2018/08/monolithic-vs-heterogeneous-infographic.pdf) — *Called them* ***chiplets****!*  Yet some big weak weasel's ego was hurt in Santa Clara, and they renamed it *Tiles* shortly afterwards.  > … just copy Ryzen's super successful and ultimately quite straightforward chiplet-scaling strategy.  They joke is, due to both of them share a cross-patent agreement, Intel even would've had access to AMD's patented stuff over everything chiplets — Chances are Santa Clara chose not to over license-fees, or spite …  *That's how you're f–cked over by your own ego* — Rather throw years of potential lead into the gutter, instead of even *thinking* about having to share some meager percentage of profits of yours with others.  That's Intel for you. A bunch of braggarts and loud-mouths, wo always think they can do and know better, yet fail nigh every single time and still can't bring themselves to be humble for once …",hardware,2025-12-17 19:14:41,1
AMD,nu74y11,"I believe  For N2; APPL A20/M6, NVDA Feynman, INTL Nova Lake   For N2P; APPL A21, MTEK 9600, QCOM SDE8G6, AMD Venice  AVGO, MRVL, AMZN, MSFT, GOOG have also volume contracts for n2/n2p production with several SKUs in bring up already.",hardware,2025-12-15 18:30:28,7
AMD,nu73haz,AMD is actually rumored to have the first TSMC N2 products out.,hardware,2025-12-15 18:23:35,9
AMD,nu9jdza,Late 2026 is Rubin and it's on 3nm. 2027 is Rubin Ultra also on 3nm.,hardware,2025-12-16 02:12:56,3
AMD,nu70lyz,"You're absolutely allowed to discuss the rumor. I am also allowed to point these discussions are way beyond the pay grade of a lot of people in this sub.  FWIW Designs for 2nm generation are already in production, so that ship has sailed. If AMD was to be planning on doing EPYC on SS 2nm, they would be already be at the bring up stage or close to it. Which does not seem to be the case.  Edit: However Samsung's foundry roadmap aligns with some of AMD's monolithic die roadmap, so it would make sense for AMD to at the very least explore their foundry options there. But for 2nm nodes, those negotiations should already have happened a while back.",hardware,2025-12-15 18:10:02,5
AMD,nu77jap,"AMD only ever claimed N2, which is interesting because Mediatek had no problem specifying N2P, though IIRC their original press release *also* only had ""N2"".   Given that AMD seems like they will launch their N2 products the earliest, I think they might be on N2 rather than N2P, but who knows. The differences between the two nodes seem very minor anyway.",hardware,2025-12-15 18:42:38,5
AMD,nu9l88v,"Is it strange that i feel like 3nm is just old tech now ;)  Google AI says  ""Nvidia is heavily invested in 2nm chip technology, planning its next-generation ""Feynman"" AI architecture on this advanced process node via TSMC, targeting mass production around late 2026 or 2027, following its current ""Rubin"" (3nm) chips, with its cuLitho tech aiding the shift for improved efficiency and speed, aiming to maintain leadership in AI hardware despite rivals also targeting 2nm""  Google AI wouldn't lie would it? ;)  Watch Nvidia 2nm get delayed until 2029 because no ram available ;)",hardware,2025-12-16 02:23:59,0
AMD,nu76zq6,">You're absolutely allowed to discuss the rumor. I am also allowed to point these discussions are way beyond the pay grade of a lot of people in this sub.  I'm just baffled why you brought it up if you seem like you *agree* with the premise of the original comment anyway, other to just dunk on gamers? Lol.   >FWIW Designs for 2nm generation are already in production,  Only 18A and Samsung 2nm, both of which are likely not on par with TSMC's 2nm node. Meaning calling them part of the ""2nm generation"" is a stretch.",hardware,2025-12-15 18:40:04,4
AMD,nu9lszd,Google AI is telling me the correct info: https://i.imgur.com/HPg7fhN.png,hardware,2025-12-16 02:27:25,1
AMD,nu84bbi,"> I'm just baffled why you brought it up if you seem like you agree with the premise of the original comment anyway, other to just dunk on gamers? Lol.  It's their pastime.",hardware,2025-12-15 21:24:31,3
AMD,nu9mco8,Another AI miracle.,hardware,2025-12-16 02:30:40,0
AMD,nu9oa12,"AI has fuzzy memory like humans. The way attention works in AI is based on context. Certain keywords increase or decrease certain likelihoods.   Vague prompts increase the likelihood of a hallucinated response.. In my screenshot you can tell it also provided the picture of Nvidia's official roadmap. This is called grounding.   Can't trust AI 100% time just how you can't trust humans 100% of time. But there are ways to increase accuracy via grounding, or prompts. We're still figuring out how to extract the best out of these models. They aren't perfect.",hardware,2025-12-16 02:42:12,1
AMD,nxgkf8w,"Good for what? If you mean good for the price, 9060xt offers better fps per dollar if you have RT off.",buildapc,2026-01-03 16:19:38,2
AMD,nxgkiug,Yes that is good.,buildapc,2026-01-03 16:20:07,1
AMD,nxgmd6n,It will game.,buildapc,2026-01-03 16:28:55,1
AMD,nxgmr1r,Yes.,buildapc,2026-01-03 16:30:43,1
AMD,nxgnx69,The 7500F combos well with everything that isn't a 4090/5090,buildapc,2026-01-03 16:36:14,1
AMD,nxgo4o1,"Depending on what your use case is, you may be better off with a 16GB version card, especially if you're planning to be at 1440p/2160p for resolution. So, in this case it's still a good pairing but I would consider the AMD Radeon 9600 XT 16GB which can typlically be found around the same price as the 5060 Ti 8GB, or the 16GB version of the 5060 Ti or the RTX 5070 if you can swing it.",buildapc,2026-01-03 16:37:12,1
AMD,nxgottp,"Its a perfectly fine cpu and gpu.  I wouldn’t recommend it for 4k gaming, but a 7500f and 5060ti (preferably 16 gig of vram vs. 8) will do reasonably well at 1080p or 1440p.  The next question is price.",buildapc,2026-01-03 16:40:26,1
AMD,nxgqawp,"is the Intel Corei5-12400 better? my budget is like 1500€, probably more because I want to upgrade it over time. CPU 's and GPU prices are rising so that is my main priority",buildapc,2026-01-03 16:47:21,1
AMD,nxgtbzt,>is the Intel Corei5-12400 better?   No and nope.  Think about a 9070 non-XT,buildapc,2026-01-03 17:01:25,1
AMD,nxijify,"The 12400 usually is a little better than a 5600x, worse than a 7600x.  A 14600k is usually as good, or a little better than a 7600x in gaming with the cost of efficiency.    The benefit of LG1700 with the 14600k, is you can find mobo’s that use DDR4.",buildapc,2026-01-03 21:55:17,1
AMD,nxobfbn,For $20 that’s a steal,buildapc,2026-01-04 19:08:04,22
AMD,nxoc13r,"$20 yes, pcie4 and stronger cpu.",buildapc,2026-01-04 19:10:43,12
AMD,nxoe3zv,Color me suspicious. What is this “offer”?,buildapc,2026-01-04 19:19:59,4
AMD,nxog5t5,"For $20?  Definitely, the 5600x has twice the cache which makes a significant difference in most games.",buildapc,2026-01-04 19:28:58,3
AMD,nxob79y,"I definitely would. However take into consideration if you’re on a very tight budget that you’ll have to buy a separate fan, but this won’t cost a lot anyways.",buildapc,2026-01-04 19:07:04,3
AMD,nxodsse,Yes. I have that exact setup. Worth it.,buildapc,2026-01-04 19:18:35,1
AMD,nxodyr8,"IMSMR, The X is unlocked and you can group the cores and run them all at top burst. Cool accordinly.",buildapc,2026-01-04 19:19:20,1
AMD,nxoeqto,Yes.,buildapc,2026-01-04 19:22:49,1
AMD,nxoffgn,"Defintely, yes.",buildapc,2026-01-04 19:25:50,1
AMD,nxoy0qr,5600x is goated,buildapc,2026-01-04 20:50:33,1
AMD,nxq7g1y,"$20 is insanely good. If it's not a scam, yes do it.",buildapc,2026-01-05 00:28:15,1
AMD,nxoeb60,"absolutely not  edit: x yes, non X no",buildapc,2026-01-04 19:20:52,-2
AMD,nxoi6sy,"He says he already has a Ryzen 5500, most likely he can reuse his fan",buildapc,2026-01-04 19:37:56,5
AMD,nxogsa8,5600 and 5600X are essentially the same thing.,buildapc,2026-01-04 19:31:42,5
AMD,nxokff6,"damn, true",buildapc,2026-01-04 19:48:09,1
AMD,nwz58ha,I upgraded from a 5800X3D (already faster) to 9800X3D and for what I wanted it was totally worth it. I’m mostly playing fps games like BO7 and BF6 at 1440p 240Hz and the 5800X3D was showing its limits. If I was playing at say 144-165Hz/fps then it wouldn’t have been worth it. The big factor is what fps are you aiming for and how much is the 1200KF holding you back in your games?,buildapc,2025-12-31 20:51:06,26
AMD,nwz62yd,"Can't give you any firm estimates, but if you already have the RAM (worst cost part right now for anyone upgrading) then yeah, the 9800X3D is an amazing chip for gaming. Your upper limit will generally be limited by your GPU, so instead what you'd expect to see is an increase in your 1% lows, meaning the overall gameplay experience becomes smoother with less small stutters. I had the 12600K paired with my 4070 Ti, and upgraded to a 9800X3D, seeing this kind of improvement, but note also I was on a DDR4 build so part of that was undoubtedly the RAM moving up a generation. But in tests, [the 9800X3D stomps the average FPS and 1% lows of the 12th gen Intel chips](https://gamersnexus.net/cpus/rip-intel-amd-ryzen-7-9800x3d-cpu-review-benchmarks-vs-7800x3d-285k-14900k-more#9800x3d-gaming-benchmarks) often by 50% or more.  In my personal experience and having had it for just under a year now, it just feels more ""stable"". On my 12600K I'd play games that would sometimes have inexplicable micro-stutters in the frame rate which I later learned were those 1% lows I'd read about. The AMD chip smoothed those out and now the games feel markedly more consistent.  Do note though that if you do productivity, e.g. video editing, 3D modeling, etc., the 9800X3D isn't optimal for them. It's no slouch, and I still do that as a hobby myself, but if your daily pay relies on lots of threads then something like the Intel Core Ultra 9 or the 9950X3D is a better choice. But for gaming, the 9800X3D really is the sweet spot for price and performance.  Also, it can be cooled with air alone. I have a Peerless Assassin 120 on mine and it has no trouble at all. Liquid cooling is excessive for this CPU; only do it for the aesthetics if you care.",buildapc,2025-12-31 20:55:42,4
AMD,nwz92hr,"I’m thinking your current setup with the i7-12700 and a 4070 Super is a pretty solid system. Like the folks have said here, if you are seeing low frame rates and/or stuttering in the games you play, then it would be worth it. If you are enjoying your games and they are running smooth with your current system, keep it awhile longer.   Also, what resolution and size is your monitor. That would make a big difference if you should upgrade.",buildapc,2025-12-31 21:11:53,5
AMD,nwzj32e,12700 is great. Id save for a monitor or speaker upgrade.,buildapc,2025-12-31 22:06:56,3
AMD,nwz6dkn,Nah I would until Zen6.,buildapc,2025-12-31 20:57:17,9
AMD,nwz7c1k,Not with memory prices you are better off holding out. They are still producing the 14th generation I would wait for a killer deal on a 14700K. But only if you are finding your CPU constantly running 100% in most games. Or you think you can recoup most of the cost on resale. I upgraded my 12600K to the 14600K and other than a slight increase in my 1% lows not a huge difference. I'm playing at 1440P with a 4070. It was FOMO and the whole future games thing lol.,buildapc,2025-12-31 21:02:31,5
AMD,nwz5k74,"You can look up reviews online. I'd say you're fine with your current setup, and a GPU upgrade is likely to benefit you more.",buildapc,2025-12-31 20:52:53,2
AMD,nwzlh95,"No, if you are money conscious at all. It’s better spent going to a higher tier gpu. 5070ti/5080 and selling your gpu would net better frames in most instances.  I always say though, don’t overlook a monitor upgrade. There’s many things a basic spec sheet won’t tell you when moving to a more expensive display.",buildapc,2025-12-31 22:20:38,2
AMD,nx0pwr7,I just did this upgrade. Kept my same GPU (6800xt). Upgrade 100% worth it,buildapc,2026-01-01 02:30:52,2
AMD,nwzbc5u,"Everything outside 1080p will be anyway limited by the GPU, my guess is 10% on 1080p and max 3-5% on 1440p or 4k, again both gaming options mostly limited by the GPU",buildapc,2025-12-31 21:24:05,2
AMD,nwz9lfa,Since you already have DDR5 memory it will be worth it.  We will not see Zen 6 X3D until Q1 2027 I believe so going AM5 now then flipping the 9800X3D in a year maybe worth it.,buildapc,2025-12-31 21:14:40,1
AMD,nwzbt0j,Yes for shooters big difference if you are aiming for high frame rate with high refresh rate monitor in 1080p and 1440p.,buildapc,2025-12-31 21:26:36,1
AMD,nwzy57s,"Yes, period",buildapc,2025-12-31 23:35:35,1
AMD,nx0d1rf,"When you say mainstream shooters, do you mean like BF6/Warzone/CS/VAL? Then 100% yes worth it. The x3d chips keep frametimes consistent which removes stuttering and helps your aim be consistent.     If you arent doing competitive fps or rts style gaming then no its not really worth it.    Source: I play bf6/wz and have tested 7500f vs 9700x vs 7800x3d vs 9800x3d by buying and returning processors lmao. The game is incredibly smooth on the x3d chips.",buildapc,2026-01-01 01:06:59,1
AMD,nx18yrp,"with the current ram prices, is it worth it?",buildapc,2026-01-01 04:43:56,1
AMD,nx21qsu,"For the games you play no. The 12700K is still a good chip and the fact you have DDR5 RAM VS DDR4 already helps a lot. I would wait for the new generation of Intel or even AM6 on AMD. If you play higher resolution gaming like 4K, the GPU is the one that will hold you back. I say stay where you are. But if you have disposable income, do whatever your heart desires.",buildapc,2026-01-01 09:17:59,1
AMD,nwz8ntr,leaving intel for an x3d will be the best decision you ever make,buildapc,2025-12-31 21:09:42,1
AMD,nwzxrag,I went from a 14700k to a 9800x3d and the difference has felt pretty huge.,buildapc,2025-12-31 23:33:18,0
AMD,nwz6y0v,"Yes it’s worth it. Plus the AM5 platform it better and will still have a few more generations left, whereas you have no upgrade path on your current",buildapc,2025-12-31 21:00:23,0
AMD,nwz414b,Nah,buildapc,2025-12-31 20:44:33,-1
AMD,nwzaehx,what a waste of money. everyone knows GPUs are what you need to max out on,buildapc,2025-12-31 21:19:03,-2
AMD,nx0d9zn,Second this for what its worth. The consistency in frames and improvement in 1% lows for someone latency sensitive is amazing.,buildapc,2026-01-01 01:08:28,7
AMD,nwzcuo2,"It's more nuanced than your black/white answer and video. If you use DLSS at 1440p for example, you should be comparing 720p or 1080p resolution depending on quality level. Also some games like 4X/Sims/WoW scale more dramatically even at 1440p.",buildapc,2025-12-31 21:32:15,7
AMD,nwzmv5d,Also the 1% lows are much better with the 9800x3d.,buildapc,2025-12-31 22:28:38,4
AMD,nwzbtao,"These hardware unboxed videos aren’t worth toilet paper since they don’t test using any upscaling.  Real people use DLSS and when you start using upscaling, you put more load back on the CPU, and the 9800X3D starts showing more of an advantage.",buildapc,2025-12-31 21:26:38,0
AMD,nwzvbwp,"Well you can either be right or wrong. And you're the one that first came in with an ""aktually"" reply in the first place with some stupid ""lmao wrong"" comment. Pretty ironic for you to say this.",buildapc,2025-12-31 23:18:36,5
AMD,nx06ttm,"Hello, your comment has been removed. Please note the following from our [subreddit rules](https://www.reddit.com/r/buildapc/wiki/rules):  **Rule 1 : Be respectful to others**  > Remember, there's a human being behind the other keyboard. Be considerate of others even if you disagree on something - treat others as you'd wish to be treated. Personal attacks and flame wars will not be tolerated.    ---  [^(Click here to message the moderators if you have any questions or concerns)](https://www\.reddit\.com/message/compose?to=%2Fr%2Fbuildapc&subject=Querying mod action for this comment&message=I'm writing to you about %5Bthis comment%5D%28https://www.reddit.com/r/buildapc/comments/1q0mwdt/-/nwzvssl/%29.%0D%0D---%0D%0D)",buildapc,2026-01-01 00:28:32,1
AMD,nx93c93,have you considered 5600X and 5700X?,buildapc,2026-01-02 13:46:50,3
AMD,nx95q0v,The 5600X isn’t that big of an upgrade.. I’d go for a 5800XT or just save for AM5.  Alternatively you can try to hunt down a 5700x3D or 5800x3d chip.,buildapc,2026-01-02 14:00:56,3
AMD,nx9b376,"Some say it's not that big of an upgrade, but according to Passmark it is 31% faster single core than the Ryzen 5 3600 and 25% faster multicore. That should be a noticeable difference. And if that's the extent of what your budget allows, then I'd say go for it.",buildapc,2026-01-02 14:31:51,3
AMD,nx9756v,"Considering you are using a 3060....probably not. The card isn't strong enough to really push your CPU limitations unless you are playing very light wait esports titles.  You aren't feeling your CPU's age in a game like Clair Obscure, its most likely the GPU. At 1080p ultra, the gpu can only give you around 40 FPS. Turn down your settings.  So imagine spending the money on a CPU and you get 0 performance improvement in your game. Where as you could sell your 3060, use that money towards a 9060XT 8GB that will give you 50% more performance.",buildapc,2026-01-02 14:09:17,2
AMD,nxa5pmh,"I would either save up for a 5800x or save up more for a better gpu. Going 3600x to 5600x doesn't seem all that good. You get icp improvement,  but no extra cores. You will be better off saving towards a better gpu. Your CPU will still bottleneck a stronger video card, but you will still get a better gaming experience than upgrading the CPU.",buildapc,2026-01-02 17:02:40,1
AMD,nx930st,It seems to be a 5600x with slightly lower clocks as the rest of the data looks perfectly the same - yes I'd go for the upgrade,buildapc,2026-01-02 13:44:56,1
AMD,nx95hpi,go for it.,buildapc,2026-01-02 13:59:34,1
AMD,nx93lsq,"I have, but It seems like prices just keep increasing lol, not even mentioning its getting harder and harder to find them. I could just barely afford the 5600t",buildapc,2026-01-02 13:48:23,3
AMD,nx96m1w,"Ngl I dont think that's feasible for me, at least not in a timely manner. Getting an am5 chip would probably mean getting brand new ram as well, and a motherboard. And honestly I'm sorta vibin with the 64gb I got.  As long as I can play games like destiny 2, Helldivers and clair fine I'll be okay.",buildapc,2026-01-02 14:06:10,3
AMD,nx99az9,"Really? Cause it feels like my CPU is the bottleneck. I'm playing at mid-low with like 60-70 resolution scaling, and I'm getting like 40-50 frames in game no matter what I do. I am playing at 1080p tho so would that be it?",buildapc,2026-01-02 14:21:40,2
AMD,nx93p1a,Perfect! Thank you.,buildapc,2026-01-02 13:48:54,1
AMD,nx94c5p,"I understand. Asked because where I live, 5700X is same priced as 5600T, so totally worth it.",buildapc,2026-01-02 13:52:44,2
AMD,nx9aijo,I upgraded from a 3600 to 5800xt without changing motherboard or ram. That's the path I recommend.  But you probably won't notice the jump much until you also upgrade the GPU,buildapc,2026-01-02 14:28:35,2
AMD,nxa0y1d,i would 100% get the 5600T on a deal.       can't buy it here from the U.s. i think its a special global version for certain regions  but that 5600T it's the same as 5600X version with l3 cache wise!       its almost the same thing but way more affordable! hell yeah i would,buildapc,2026-01-02 16:40:33,1
AMD,nx94r0j,"No worries, honestly i feel like if I was a month or two earlier I'd actually be able to afford it as well. Which completely sucks but what can you do",buildapc,2026-01-02 13:55:12,2
AMD,nxp9i07,"For just the CPU and mobo for 1000 euros you can easily get the 9800X3D with 500-600 euros to spare for a motherboard, but the performance difference is pretty minimal. The 9800X3D is THE gaming CPU, which is important for comp games where you'll most likely be playing in 1080p, which is more CPU intense. You're better off getting the 9800X3D",buildapc,2026-01-04 21:43:55,12
AMD,nxpkr91,"For a 1000 you can get 9800X3D, decent motherboard AND RAM. Although 7800X3D could be better bang for the buck, with this budget that you have on your disposal, there is no reason to go for 7800X3D...",buildapc,2026-01-04 22:36:31,4
AMD,nxpuanu,"If you've gone for so long on a 13700k why do you suddenly need the latest and greatest? If you needed it, you would have bought the 7800X3D when it launched. You're paying too much for just an 8 core CPU.",buildapc,2026-01-04 23:23:30,4
AMD,nxpc3im,"I was on the fence with the same thing and went with the 7800 to save $100 and I'm glad I did based on what I've read here. I built my PC specifically for games, and that's it. Diesel tech by trade, so all computer work at my job is done with manufacturer software on company tough books.",buildapc,2026-01-04 21:55:53,2
AMD,nxqufqm,I mean depends on what you mean by 'worth it'. The 13700k runs fortnite perfectly fine so I'm not sure why you're looking for an upgrade,buildapc,2026-01-05 02:29:13,2
AMD,nxpg842,"The 13700k is superior in anything but gaming and even at that, if all you are playing is fortnite, I don't see the pt.",buildapc,2026-01-04 22:15:07,4
AMD,nxprqaw,Might as well go for the 9800X3D. It’s only about 5-10% faster but you have the budget so may as well.,buildapc,2026-01-04 23:10:49,1
AMD,nxpxe7t,You don’t need either for fortnight.,buildapc,2026-01-04 23:38:59,1
AMD,nxq64s1,There is such little difference between the two it hardly matters.,buildapc,2026-01-05 00:21:55,1
AMD,nxqpshu,If you have that budget then the why not the 9800 will crush Fortnite,buildapc,2026-01-05 02:04:33,1
AMD,nxpo9pw,"Your cpu is already really good, why would you upgrade? Hell even a 10th gen intel is still pretty good.",buildapc,2026-01-04 22:53:30,0
AMD,nxpos5n,not worth it,buildapc,2026-01-04 22:56:03,0
AMD,nxpste0,The amd CPUs will eventually kill themselves.  Why would anyone buy amd until they identify the problem and recall them all??,buildapc,2026-01-04 23:16:08,-10
AMD,nxpog86,I thought 9000 series had better power draw?,buildapc,2026-01-04 22:54:25,1
AMD,nxq0pvr,That’s almost exclusively on asrock mainboards these days for the rest the risk is virtually nonexistent. Also with that reasoning why would anyone buy or have bought a new gpu in the past 3 years? The problem of melting 12hvpwr connectors is far more prevalent.,buildapc,2026-01-04 23:55:36,3
AMD,nxqamx2,"nice try asrock its your fault and not amds, we not falling for this bait",buildapc,2026-01-05 00:44:04,3
AMD,nxqarou,its not but ok,buildapc,2026-01-05 00:44:44,-1
AMD,nxhzzaj,"those games are heavily CPU bound, you wouldn't see much uplift from upgrading your GPU there regardless of your resolution  fwiw I also play valorant and ff14. my frames in both those games basically stayed the same when I upgraded from the 3070 to 9070xt  I upgraded my CPU a month after I upgraded my GPU. so when I upgraded from my 10700k to 9800x3d, I got a 400 - 500 fps increase in Valorant (yes lmao) and a 100 fps increase in ff14 idling at my house",buildapc,2026-01-03 20:19:04,3
AMD,nxht1mq,[https://i.gyazo.com/0f3f52823b94186f6e40b07fe4416ee3.png](https://i.gyazo.com/0f3f52823b94186f6e40b07fe4416ee3.png)  My timespy screenie if anyone wants to see that,buildapc,2026-01-03 19:45:07,1
AMD,nxhw2kf,Plugged into the GPU on the motherboard? What exactly do you mean by that?  Also what resolution are you playing at?,buildapc,2026-01-03 19:59:46,1
AMD,nxhxx8f,"In addition to the comments already made, I would like to add that those games generally run better on nvidia cards than AMD, because they were designed to do so (background gimmicks and stuff). The overwhelming majority plays on nvidia cards, so games get way more optimised to cater to the biggest chunk.",buildapc,2026-01-03 20:08:51,1
AMD,nxhz6c4,Cheers for the answers. I play on 1080p,buildapc,2026-01-03 20:15:04,1
AMD,nxi5cb8,"I tried everyting. Then I got a tip and turned a AMD setting off in the bios, then it got resolved. Atleast for my 9060 XT 16GB. I can check later what its called, if not someone knows it.",buildapc,2026-01-03 20:45:58,1
AMD,nxi5fx1,"If you're cpu bound, crank the graphics settings to the max... Might as well and it may help the cpu a bit",buildapc,2026-01-03 20:46:29,1
AMD,nxi94rn,At 1080p your 5600x cpu is holding you back.,buildapc,2026-01-03 21:04:45,1
AMD,nxhwe0c,"In 1080p, the games you play likely don't benefit from a strong GPU.  You can either return the GPU and upgrade the CPU instead or get a 1440p monitor to take advantage of your new GPU.",buildapc,2026-01-03 20:01:20,0
AMD,nxi9u1z,"That's oddly convenient lmao, but yeah I got told otherwise I'd get a fps boost (maybe). I was looking at that cpu, but it's quite expensive for my dumbass",buildapc,2026-01-03 21:08:12,1
AMD,nxhzryd,"You can click the different metrics in the graph to pinpoint what is the bottleneck. Such as CPU load, GPU/CPU temp  Edit: is CPU utility at 100% all the time?",buildapc,2026-01-03 20:18:04,1
AMD,nxi9j0j,Would appreciate it,buildapc,2026-01-03 21:06:42,1
AMD,nxi9mav,"I can give it a shot, I need to wait till pay day to see haha",buildapc,2026-01-03 21:07:09,1
AMD,nxic6ur,Would playing on a higher resolution resolve that? Or the 5600x is just that bad,buildapc,2026-01-03 21:19:47,1
AMD,nxhz8m1,What do you mean by take advantage?,buildapc,2026-01-03 20:15:23,1
AMD,nxiu1e5,"Yeahhhhh, it's the best consumer gaming CPU available so it's definitely not cheap, but you also absolutely don't need it.   I don't know how caught up you are with the current state of PC building, so if I could info dump a bit here:  Unfortunately you came at a catastrophically bad time to upgrade your CPU. Because of the DRAM crisis happening at the moment, DDR5 RAM is absurdly expensive, so that would be another huge additional cost if you wanted to upgrade. On top of needing a new motherboard since any other modern AMD CPU is on AM5, a platform newer than your current motherboard supports (and that expects DDR5 RAM).  If you wanted to stick with your current platform on AM4, your best option would be getting any of its 3d CPUs. But they're also absurdly expensive and impossible to find. They're out of production and heavily coveted, so their used prices are insane right now.  Outside of that, the best upgrade path for your current motherboard is the 5900xt. It's not an insane uplift from what you have now, you can look up performance comparisons to see if it's right for you.  If you were willing to swap motherboards but still stick with a DDR4 platform, you could also check out Intels LGA1700 offerings from their 12th Gen to 14th Gen line of CPUs. Unfortunately again, their 13th and 14th Gen CPUs are hard to recommend, since they suffer from a degradation/oxidation issue which just outright breaks them. Intel implemented several fixes but it's hard to tell if they're 100% good now. Even aside from that, 13th and 14th have worse performance if you don't have DDR5, so...   Which leaves you with Intels 12th CPUs. The higher end should outperform the 5900xt I believe? But again, check to see if the performance lift is right for you.  It's possible to upgrade to DDR5 still, if you wanna get out cheap you can fish for a deal (which has increasingly become harder) or maybe cough up the money for 16GB. It's a cluster fuck right now if you couldn't tell lol",buildapc,2026-01-03 22:47:58,1
AMD,nxibq87,[https://i.gyazo.com/3a337a8fbafedb2426a22831e5efb9c7.png](https://i.gyazo.com/3a337a8fbafedb2426a22831e5efb9c7.png)  No it's not,buildapc,2026-01-03 21:17:31,1
AMD,nxihrzk,"AMD fTPM switch, disabled. Try and see if it works. Did it for me.",buildapc,2026-01-03 21:46:57,1
AMD,nxieex2,"A higher resolution will use more of your gpu, you won’t get a higher frame rate than now though.   So instead of getting 100fps at 1080p you might end up still getting 100fps but at 1440p. It’s all going to depend on the game though",buildapc,2026-01-03 21:30:37,1
AMD,nxhzypk,"Because you have a powerful GPU now, you can swap to a higher resolution without losing much framerate, maybe not at all even. Your GPU has lots of capacity to do that.",buildapc,2026-01-03 20:18:59,0
AMD,nxj0sor,"That's some solid information, I appreciate it! I'm wanting to make my pc last for awhile, so spending a little more on a good cpu isn't too much of an issue. Kinda kept up to date on the news and it sucks because I've only really started getting into it haha. I also didn't realise my motherboard was that outdated, my pc building skills are very questionable and getting windows again might be a pain in the ass. How good would a 3d cpu do me on my current setup? Dunno if I fancy a ""slight"" improvement. Also what do you mean by a 16gb?   Appreciate the extra info too!",buildapc,2026-01-03 23:23:17,1
AMD,nxjynp2,"Yes that looks like timespy should look like, with your graphics card maxed during the GPU test 1 and 2.  Are you gaming at 1080p? In that case the cpu was already close to being the bottleneck for the games you used to play.",buildapc,2026-01-04 02:25:37,1
AMD,nxi98fh,"Oh I see, I might consider it. Been on a 27 inch for awhile",buildapc,2026-01-03 21:05:15,1
AMD,nxl6jky,"You wouldn't have to (typically) buy or install Windows again, that should just work even if you make an upgrade.  The 3D CPUs by AMD are best in class for gaming, imo it's worth looking at video comparisons of them if you wanted to see what they're like compared to everything else. For 1080p gaming they provide huge performance uplifts bar none. Though the 5800x3D is matched with recent, higher end CPUs that use DDR5.   As good as they are, some people find them overkill for higher resolution gaming, and I think that's a fair criticism too. The higher res you go, the gap closes \*a lot\* since that is more of a GPU bound scenario. Even in GPU bound scenarios, they are better at 1% and 0.1% lows, so there is still a benefit. I got the 9800x3D cause I spend the vast majority of my time playing CPU intensive games, even though I run 1440p.  They're basically the Ferrari of gaming CPUs, but once again, you absolutely do not \*need\* them. It's like getting a 5090 as a CPU, it sounds nice until you look at the price and potentially how overkill it is. There are plenty of cheaper upgrades you can make that will give you very nice performance uplifts.  I'd also argue if your 5600X is treating you fine, you could just keep that and ride it out, even if it is a bottleneck.  Also by 16GB I just meant 16GB RAM. Which would be a downgrade for you since I see you have listed 32GB RAM in your original post.",buildapc,2026-01-04 07:19:47,1
AMD,nxlxb38,"I was looking at quite a few yesterday new + used. There's not many 3d chips like you said that's compatible lol. Was considering a 5800x3d used but wanted to do abit more research first. I am the same as you with whole cpu heavy games so I am leaning towards it. The 5600x is ""okay"" can wait to see what comes up. but I'd personally like an upgrade. I don't suppose you know any cpus that will give that significant uplift unless it is just the 5800x3d",buildapc,2026-01-04 11:20:13,1
AMD,nxm36c9,"If you wanted to stay on your current motherboard, your best bet outside the 3D CPUs is probably the 5800xt. It's unclear to me if the 5900xt would have more gaming performance, its main draw is extra cores which games don't really use.  Hardware Unboxed has a very relevant video for you on this, and if I'm being honest, the uplift from 5600x to 5800xt seems incredibly miserable. You can check it out for yourself: [https://www.youtube.com/watch?v=RijAyVshtok](https://www.youtube.com/watch?v=RijAyVshtok)  If you wanted to stick to a DDR4 platform, it would be worthwhile to look up how the 5600x compares to the 12700k, but that seems very pitiful as well unfortunately.  My personal opinion is neither of these upgrades are worth it at all.  If you're looking for any worthwhile gains, I think it would be best to look at moving to a DDR5 platform based on your budget and what you need.",buildapc,2026-01-04 12:10:02,1
AMD,nxmtuig,"Your parts are absolutely fine. Maybe consider a 2TB SSD though, especially if you play newer games and several of them at the same time. You'll want to keep 15%+ free space as well.",buildapc,2026-01-04 15:00:39,3
AMD,nxmfwk6,What's your total budget ?  I'm going to assume the goal is mostly gaming   Also how are you on peripherals?,buildapc,2026-01-04 13:40:26,2
AMD,nxn4zpu,sometimes you can find the ryzen 7 7800x3d for almost 100€ lower price than the 9800x3d. Their performance with the 9070xt is very similar currently with the 7800x3d even edging out the pricier 9800x3d on some titles. in the grand scheme its not that much but I made a very similar build just recently and saved on that.  I also went with a msi mag 750w power supply and in my opinion 1000w seems quite excessive.  Those things save you some cash but yout rig definetly will live a long life with those parts. Having more is not bad is what im saying but those things that I mentioned were ways I avoided paying as much.  Its very good,buildapc,2026-01-04 15:56:15,2
AMD,nxo6eqx,"1000w is a bit overkill for the hardware, also the Corsair units are good but in most regions very expensive. I would check some tier lists to sort what’s available to you but the montech century ii is an A tier psu they typically goes for half the price of the Corsair unit",buildapc,2026-01-04 18:46:10,1
AMD,nxmlb0c,"Around 2k euros, streched a bit.  Yeah, my goal is mostly gaming, I'm currently using 27"" Acer monitor.",buildapc,2026-01-04 14:12:39,3
AMD,nxpdejp,How much would be the maximum that you can spend?  The best upgrade you could make would be to a 5600x/5700x and an RX 9060xt 16gb. This would be a massive upgrade and feel like a totally new system.  On the cheaper side I'd check for a Ryzen 3600 and a used RX 6600XT/6650XT or 6700xt which you should get for 250-300gbp.,buildapc,2026-01-04 22:01:57,18
AMD,nxpb8oh,"BF6 is actually pretty demanding on both the CPU and GPU, and while your GPU is certainly the bottleneck, both parts are pretty out of date. How much are you willing to spend to improve the system, because that'll really dictate just how much extra performance you can get",buildapc,2026-01-04 21:51:57,7
AMD,nxpcfs9,"You've got a phenomenal upgrade road so don't think about getting an entire new system just yet. Your motherboard should be compatible with Ryzen 5000 series CPUs (after a BIOS update) so you could purchase a Ryzen 5500 for not a lot of money and get a substantial upgrade. Or a 5600 and get even more power. Your R1400 is beyond outdated at this point and has no business moving BF6 with stable frame rates, no matter what GPU you use.  After that, I'd consider a used GPU. Because of your 550W PSU I'm not entirely sure on what you should choose.",buildapc,2026-01-04 21:57:27,6
AMD,nxpc86o,"Maybe better off looking for a 30 series card, and All pcie x16 slots are a standard size",buildapc,2026-01-04 21:56:29,5
AMD,nxpg8tg,5700X + 9060XT 16GB would do wonders for you.,buildapc,2026-01-04 22:15:13,3
AMD,nxpgqpc,CPU would be a great upgrade. Just stick one of the 8-core Vermeer chips on there and you're good for a few more years at least. You will have to update your BIOS.,buildapc,2026-01-04 22:17:35,2
AMD,nxpidiz,"Try save for a 9060XT, 16GB version if possible, and a 5600X/5700X/5800X(T), will be a big bump in CPU and GPU performance without having to touch ram/ssd/motherboard/case/psu. Oh and btw if you can afford one first, BF6 is very heavy on the CPU, so not sure what the limit would be in your case but it's probably worth it to check.",buildapc,2026-01-04 22:25:18,2
AMD,nxpk9zi,You have any AM4 set up. Get a Ryzen 7 3700 or Ryzen 7 5700/5800x3d and a NVIDIA 3070 equivalent.,buildapc,2026-01-04 22:34:17,2
AMD,nxpkhj4,You want a 5500 or 5600 and a cheap graphics card like a 7600 or 6700xt if you are looking at cheap,buildapc,2026-01-04 22:35:16,2
AMD,nxpnx6e,"You're in a tight budget my friend, try to go for a 1660S second hand, I played BF6 in a 1650s, 40 fps in FullHD, 60-70 in 720p, with R5 3600.  For 200 or a bit less you can find a 4060 too, I was lucky enough to find one brand new on Amazon for 250€ (217£) in black Friday a few weeks ago, since the economy is tight that was a good deal, if electricity costs are a problem, the 4060 is also very efficient and doesn't consume much.  Sincere isn't a good time to upgrade the pc, hopefully the situation will turn around...I wish you the best of luck.",buildapc,2026-01-04 22:51:47,2
AMD,nxphvrm,"Thanks for the input so far everyone :)     I should have said, I am looking at doing a full new build in the future once prices have stopped being crazy, if that ever happens, but I'm willing to wait a couple years for that. I mostly play indie titles so don't need much power usually.     I've found a cheap second hand Ryzen 3600XT which I think is also AM4 and compatible with my motherboard, so that's a slight upgrade... not bought anything yet though.",buildapc,2026-01-04 22:22:57,1
AMD,nxpk7fr,"for your use case you don't need a 9060xt like people are saying.. 16gb vram for 1080p? sounds like an overkill and since you are right on money, id recommend you post this question on r/lowendgaming and see what they tell you there",buildapc,2026-01-04 22:33:58,1
AMD,nxpnyav,"Not sure about BF6, but I have this exact combo (5600x/9060 XT 16GB) and was just playing 2018 God of War at Ultra settings and getting 100+ FPS.",buildapc,2026-01-04 22:51:56,2
AMD,nxpi53a,"Ideally as little as possible, I'm treating this as a pretty temporary upgrade just to have a bit of fun with my mates on BF6. Like £50-100 for CPU and £100-150 for GPU?",buildapc,2026-01-04 22:24:12,3
AMD,nxpia83,An 8-core vermeer chip?... I'll have a google.,buildapc,2026-01-04 22:24:52,1
AMD,nxplhog,"5800 is fine, 5700x is also fine, 5700 is not fine.  5700x/5800/5800x are fine because they have 32MB cache. 5700 has only 16MB so I would advise against it.  Edit: another minus of the 5700 non-X only has PCIe 3.0, unlike the rest that have 4.0 (not a factor for OP because his board doesn't have it either but still, a reason to avoid it).",buildapc,2026-01-04 22:39:57,1
AMD,nxpilxi,"If you go at least 5600X + 9060XT 16GB that will last a solid amount more years, you'll probably be able to jump to AM6 instead of AM5 or at least get close.",buildapc,2026-01-04 22:26:25,3
AMD,nxpjprt,"This is not reality, prices never stop being crazy. The longer you wait, the higher the prices will be. This is basic inflation. The only thing that might go down a bit is RAM, but don't bet on it happening soon, if at all.",buildapc,2026-01-04 22:31:40,1
AMD,nxplfuw,Thanks for the tip I've done that :),buildapc,2026-01-04 22:39:42,2
AMD,nxplt0m,"Cpu wise you can definitely find a new ryzen 5 3600 or 5500 for around 50-80 pounds and those will get you to like 90-110 average fps on low settings but with drops to 55-70 in very high action scenarios. Just need to update bios and slot in, Whatever cooler you have rn will also work fine for either.  Gpu wise you aren't really getting anything new in that price range so used is the only option. And the rtx 2060 you mention can just about get you to 100 fps average on 1080p low.",buildapc,2026-01-04 22:41:28,6
AMD,nxpq9nw,"What do you mean by temporary? Will you upgrade when? Next year? In 2 years? If you cheap on the GPU too much, there might be a next game you want to play with your mates in 6 months and then you have to pay for a GPU again, since the 2060 is on the edge of okay right now.  If you just add 50e(euro in my country), to both CPU and GPU, you can find 5600x, which is still a great cpu. Then you for 200 you can find 4060/5060 used or even 4060ti(or amd equivalent) if you are lucky. This will get you relatively juicy fps and might last you couple years well.",buildapc,2026-01-04 23:03:30,3
AMD,nxpp2vo,"That should be plenty to work with. Granted, I'm in the US and I'm not super familiar with how the used market is across the pond, but given the prices here, you should easily be able to find something like a Ryzen 5 3600, or a Ryzen 5 5500. Hell, for closer to £100, you might even be able to find a 5600 or 5600x, and that'll last you a good long while.  As for a GPU, that's gonna be the more pricey part for sure. If you're willing to blow out your budget a little, you can buy a brand new Arc B570 for just £200, and it comes with Battlefield 6 (or some other game).   https://www.overclockers.co.uk/sparkle-intel-arc-b570-guardian-oc-10gb-gddr6-graphics-card-gra-spk-03924.html  Otherwise, if you're really set on your budget, then on the used market I'd look into an RX 6600 / RX 6600 XT / RX 6650 XT for probably closer to £100, or an RTX 3060Ti / RX 7600 for closer to £150. Again, that all depends on your used market, and I dunno how it is over there, but I wouldn't pay much more than what I listed here for those parts",buildapc,2026-01-04 22:57:31,1
AMD,nxpldoi,"Vermeer is the name for regular Ryzen 5000 chips.   https://www.techpowerup.com/cpu-specs/?f=codename_=Vermeer   You want to avoid the [Cezanne]( https://www.techpowerup.com/cpu-specs/?f=codename_=Cezanne) chips if possible, which are essentially laptop CPUs and do not perform quite as well, largely because they have a lot less cache. They also don't support PCIe 4.0 like Vermeer does.   The R5 5700 is one of those. It gets a lot of attention for being the cheapest 8-core Ryzen 5000 chip but in reality it's not much faster than a previous gen 3800X.",buildapc,2026-01-04 22:39:25,3
AMD,nxpoizf,"Sorry, I meant the x3d version of the 5700/5800. A 3700 would also be a huge upgrade for him, is fast and cheaper than the X3d's.",buildapc,2026-01-04 22:54:47,1
AMD,nxpjuzj,"Oh interesting. That is tempting. Any reason you'd go 9060XT over an Nvidea equivalent? I don't really have a preference, I just know the Nvidea numbering a bit better. Would you suggest buy these parts new rather than second hand?",buildapc,2026-01-04 22:32:21,1
AMD,nxpqvh1,"Most games are fine with just 8GB VRAM. There are a handful of games that will exceed it if you use RT/PT with very high/ultra details, so he is technically correct.  However, considering that your motherboard only has PCIe 3.0, I would not get an 8GB card. Of course, if the price difference between the two versions of the card is huge, c'est la vie, get the 8GB one.  Another thing, I find myself repeating in cases like yours: you will need a BIOS update in order to use a 5000 series Ryzen.  DO NOT update the BIOS until you have that CPU. You will probably lose compatibility with your current CPU.",buildapc,2026-01-04 23:06:33,3
AMD,nxpri3i,"The AM4 X3D CPUs are very expensive, not sure he will have a budget for those. It might be better to save those money and put them towards a better video card.  A 5600x/5700x/5800x should be fine.",buildapc,2026-01-04 23:09:42,1
AMD,nxpl9fh,"Nvidia has higher CPU overhead so will perform worse when the CPU is the limiting factor, plus their options just aren't worth it right now, the 9060XT performs within 5% or so for a lot less cost, and it also has PCIE X16 while the 5050/5060/5060TI have X8 which can make a difference on PCIE 3.0 systems like yours.  As for second hand, it can be an option, but RDNA4 (9060/9070 and their XT variants) have a lot of improments to help make them more competitive with Nvidia, compared to RDNA3 RDNA4 has better encoding, better efficiency (especially in the 9070), better upscaling with FSR4/Redstone, other tech under the Redstone umbrella (once it actually gets added to games), much better RT performance, and some workstation type applications also perform much better.",buildapc,2026-01-04 22:38:51,1
AMD,nxple03,"9060 XT is usually cheaper than the 5060 Ti and very close performance.  They're both good cards, just kind of comes down to pricing/availability differences in your area and whether you feel the Nvidia features are worth the extra money to you.  Just make sure you get a 16 GB version if you buy either card. I'd go new to have a warranty since the hardware market is likely to be pretty buyer-unfriendly for a while.",buildapc,2026-01-04 22:39:28,1
AMD,nxroc0t,"Which is why I recommended the 3700. Its what I had prior toy 5800x3d. Its faster than my new chip but doesn't have the ""cool"" x3d on it.",buildapc,2026-01-05 05:23:01,1
AMD,nxpp8aj,"From what I remember, the PCIe version only matters in VRAM limited scenarios, and both cards behave pretty much the same in such a scenario and outside of it.  If OP is in a VRAM limited scenario, he is pretty much toast, because he only has PCIe 3.0 (50% performance penalty for not using PCIe 5.0).",buildapc,2026-01-04 22:58:17,1
AMD,nxppzpq,"Sorry, what does this mean? When fiddling with the BF6 graphics settings I am over my avaiable VRAM usage, it's like 4800/4000 or something, my card is 4GB VRAM, so makes sense.  Are you saying even if I got a card with 8GB VRAM it might not make a difference because of PCIe version?",buildapc,2026-01-04 23:02:06,1
AMD,nxptkgb,"It will make a difference. And I don't think BF6 is one of the games that need more than 8GB VRAM.  There are a handful of games, that at 1080p, with very high/ultra settings + RT/PT need more than 8GB VRAM.  In those games you can simply drop the details to high and simply not use RT/PT (I would not be considering using those anyway with such a card).  I was simply correcting the other poster.  The performance penalties when using PCIe 3.0 instead of 5.0 are minimal when you have enough VRAM, and 8GB VRAM should be enough for 1080p.  If you can fit in your budget a card with more VRAM, it would be better, but not if the difference in price is too great.",buildapc,2026-01-04 23:19:54,2
AMD,nxrf0me,"That CPU is a bottleneck. At least upgrade to a Ryzen 5000 series cpu.  My system is 5700x, 9070 XT, 650w PSU and it runs great. Not sure about your 600w PSU though. That's cutting it really close.",buildapc,2026-01-05 04:25:13,4
AMD,nxrj0tv,that cpu is absolute ass  psu is bad,buildapc,2026-01-05 04:47:52,1
AMD,nxr0z23,">AMD Ryzen 7 1700  The RX 9070 is significantly more powerful than your current Ryzen 7 1700. Since you already have an AM4 motherboard, you can drop in a Ryzen 7 5800XT. This is a massive leap in performance and will handle the RX 9070 much better. You MUST update your BIOS to the latest version before swapping CPU.  >600W Thermaltake  If it's an older ""Smart"" series (white or bronze), I'd recommend a high quality 750W Gold unit  >DDR4 32 gb Corsair Vengeance Pro (I tried going into BIOS to put my ram speed to its intended speed but it blue screened, so it's stuck at 2133 MHz)  Your RAM is stuck because 1st generation Ryzen (like your 1700) and B350 boards had very picky memory controller. Try setting the speed manually to 3200 MHz. Ensure the DRAM voltage is set to 1.3V. Sometimes ""Auto"" sets it too low (1.2V), which causes the crash.  >Also, my second storage is hdd 1.8 tb but apparently ssd is much better so theres that, and I have only two fans at the front of my case not including my cpu cooler fan.  Your B350M Gaming Pro has one M.2 slot. Buy a 1TB or 2TB NVMe SSD. It will make your entire computer feel faster, not just games.  Two fans aren't enough for a highend build. Try to have at least two intake (front) and one exhaust (back) to create a steady airflow.",buildapc,2026-01-05 03:04:23,1
AMD,nxrb0v7,"I think the cpu will be a bit of a bottleneck. It's not that bad though; you will see a big performance upgrade even with just the 9070 xt and your current cpu.   Also, the PSU may not be enough to power the 9070 xt, the recommendation is a 750 W psu with certification 80 plus bronze or higher. Maybe you could pick the 9070(non XT) which requires less power, and use the money you save on a better cpu, like the ryzen 5500, 5600, 5700 etc.(they're all more or less the same).  An SSD will just improve loading times in games, I don't think it's necessary for you to buy it now if your budget is tight  Do you have a fan in the back of your case? I would invest in one if not",buildapc,2026-01-05 04:01:24,1
AMD,nxrpvr9,"As other said, the CPU is lacking.    You could also keep your eyes out for a second hand 5000 like a 5800x, 5900x or 5950x",buildapc,2026-01-05 05:34:09,1
AMD,nxqz62a,Psu is to small. Minimum requirement is 750w. Also your cpu will significantly slow the 9070 down. Buy a new psu with thw 9070 and plan to upgrade cpu very soon.,buildapc,2026-01-05 02:54:27,1
AMD,nxrflig,"That 750w requirement is in case you have a power hungry CPU like Intel. My build is Ryzen 5700x, 9070xt and a 650 w PSU and I haven't had any issues. My previous GPU was the 3080 which used even more power, but my total system draw was only like 450-470 watts.",buildapc,2026-01-05 04:28:15,0
AMD,nxrhgh8,Actually thats minimum requirement. Recommend is 850w. Intel aint pulling that much difference. Yes a high tier efficient 650w can get the job done but dosent make it a good idea. I mean 700 bucks for a gpu to be iffy with a psu isnt something I like to do. But to each there own!,buildapc,2026-01-05 04:38:32,1
AMD,nxat9sh,Can you get a used 5600?  That’s what you should get.,buildapc,2026-01-02 18:51:40,16
AMD,nxb0dja,"I would argue that your biggest bottleneck is your GPU. It will depend on the games you play of course, but the 3600 is quite powerful. I would highly consider getting a GPU upgrade instead, depending on your local market conditions.    As for the 5500X3D, it's is a great chip for gaming, and is a significant upgrade at high framerates. For 200USD it's not a bad price, but if you can get a 5600X for <100USD, it's value starts to fall off.",buildapc,2026-01-02 19:25:02,7
AMD,nxasjud,5500X3D is a fair bit faster than the 3600.,buildapc,2026-01-02 18:48:21,8
AMD,nxavpnj,5500x3d for sure $200 you are so lucky! be sure to update bios on your motherboard too for the 5000 series amd compatibility  the x3d is a game changer,buildapc,2026-01-02 19:02:58,4
AMD,nxasgmo,Save the Money.,buildapc,2026-01-02 18:47:55,5
AMD,nxbe0xi,I would do it because the shelf life on good AM4 CPUs is shorter than GPUs. You can't get a 5800X3D anymore for example but there are plenty of GPUs to buy,buildapc,2026-01-02 20:31:20,2
AMD,nxcf6er,"If you can get a 5500x3D for $200, that's a good deal these days.  It's a solid upgrade.",buildapc,2026-01-02 23:39:49,1
AMD,nxczcxq,5500X3D is a good choice for that price compared to other AM4 options.   https://gamersnexus.net/cpus/am4-lives-amd-ryzen-5-5500x3d-cpu-review-benchmarks,buildapc,2026-01-03 01:32:35,1
AMD,nxavsm9,5600 is good upgrade if your current is 3600 (and save money for future as well am6 )  but u need x3d cache that look for 5700x3d rather than 5500x3d.,buildapc,2026-01-02 19:03:21,0
AMD,nxau0fy,"Yeah, is even cheaper, but it has no x3d, and i would use the pc for gaming more than anything",buildapc,2026-01-02 18:55:03,1
AMD,nxawn5f,"I was looking for a 5700x3d or even a 5800x3d but in Argentina there is no way i find any, and in amazon USA the prices make no sense, like 600 USD or something like that xD",buildapc,2026-01-02 19:07:19,3
AMD,nxb5hjx,From what I can see the 5500x3d performs better or the same as the 5600 but the price différence might be too much tho.,buildapc,2026-01-02 19:49:37,6
AMD,nxdf5o4,"GamersNexus has the 5500X3D anywhere from ""barely better than a 5600X"" to ""better than a 14900K"" depending on the game tested.  I have a R5 3600 PC, not my primary but commonly used, that I would upgrade to a 5500X3D if it were available to me at that price. I can personally comfortably afford it though. If I were in a lower cash flow situation, the 3600 keeps up quite well for its age.",buildapc,2026-01-03 03:04:47,2
AMD,nxbfrop,oh i forgot .. new are damn expensive i was talking about use easy to find in my country 5700x3d cost like 55k pkr almost like 196.39 usd,buildapc,2026-01-02 20:39:59,1
AMD,nxb0ffi,They make more sense then paying 1k for ram,buildapc,2026-01-02 19:25:18,0
AMD,nx4e1qd,"I see some people on youtube say the 9070XT would be better for 1440p, but, I am currently rocking a 650W 80 Plus Gold power supply, and it seems the 9070XT requires at least a 750W. Buying a new PSU (\~130€) is not in my plan, not to mention the extra 300€ for the GPU.  And someone suggested me to just get the GPU and then see if my Ryzen 5 3600 is limiting FPS or not. If it is, then I upgrade it. Sounds like solid advice, but at the same time, lowkey afraid of the 5700X going out of stock like the 5700X3D did.",buildapc,2026-01-01 18:58:11,6
AMD,nx4niyl,I have been happy with the 9060xt 16gb been playing around with it and it is solid. Which model are you looking at?,buildapc,2026-01-01 19:45:16,4
AMD,nx5adx2,"yes, you could go for a 9070 non xt.  5700x is only 65W, so a 650W should be fine.  If you upgrade to a 9060xt you could always get a 1000w psu for future upgrades....",buildapc,2026-01-01 21:43:33,2
AMD,nx4ehdc,"Yes. Make sure it's the 9060 XT 16 GB.  >If AMD decided to make a 5900X3D, that would be instant buy for me depending on the price.  Meh",buildapc,2026-01-01 19:00:22,1
AMD,nx4ts62,Absolutely,buildapc,2026-01-01 20:16:44,1
AMD,nx528kp,"9060XT is a great card, I have the gigabyte OC 16GB card and it is fantastic. paired with a ryzen 5 7600x",buildapc,2026-01-01 21:01:13,1
AMD,nx5ncft,"I think the 5700x is a noticeable increase with respect to your current 3600. I just watched a few benchmarks and there seems to be an increase of around 20-30% in fps(paired with a 4090 ). But, there is like a 50% increase between the 9060 xt and the 9070(non xt). I would consider, if your pc purpose is just gaming, using the money for the 5700x and getting a 9070(if it's available around 550). Plus, Gpus are in an all time low right now, and cpus are usually decently priced. You might even catch a good used x3d in the future. But anyways, that's what I'd do.",buildapc,2026-01-01 22:51:44,1
AMD,nx8k2ub,"I have a 5800X, 32gb DDR4-3600 ram and a Radeon 9060 XT 16gb and a 1440p screen, 850w psu. I haven’t made any significant tests but I am totally satisfied with it. Helldivers 2 on ultra, native res, max fps is around 90. Cyberpunk 2077, every setting ultra, no path tracing, medium ray tracing, fsr4 quality and frame generation is plus 100 fps.  I know it’s not a lot to go by but it’s just my two cents. I’m totally satisfied with my purchase. I can set everything to ultra with ray tracing and get playable 60fps in basically any game 1440p, some with upscaling. From the top of my head the 9060 XT is very power efficient and uses 130w and can go down to a little over 100 with undervolting, which is very impressive. I gave mine a conservative undervolt and it’s very silent even under full load.",buildapc,2026-01-02 11:25:12,1
AMD,nx5od7n,"The 9070(non xt) has lower power requirements(<300 W). I  think your power supply can handle it, as long as it's a good brand. And regarding the 5700x going out of stock, I'd say don't sweat it. The 5700x, 5600x,  5600 and 5500 are all pretty close in gaming performance and not all will go out of stock.Watch a few benchmarks yourself and you'll see that there is not that great of a difference between these cpus. Plus the cpu is one of the safest parts to get used.",buildapc,2026-01-01 22:57:26,2
AMD,nx4nrky,"It will limit it. When I switched from r5 3600 to R5 7600 (while keeping rtx 3070) I noticed immediately better stability, far less fps drops, higher 1% lows, and overall much smoother gaming experience. R5 3600 is a nice CPU. But it's time to change it.",buildapc,2026-01-01 19:46:27,1
AMD,nx4nymw,Very good to know. Which CPU are you rocking it with?  I'm looking at the  Sapphire PULSE AMD Radeon RX 9060 XT 16GB GPU GDDR6,buildapc,2026-01-01 19:47:25,2
AMD,nx5n6h1,"Interesting, I thought that if a GPU requires like 650W PSU, I would have to get a slightly higher watt PSU like 750W because of the CPU as well.  If the 9070 just needs 650W... then I guess I could consider it. Just have to see if it's worth the extra 200€ to me.  Upgrading to a 1000W PSU in the future would not be ideal because of the power bill though.  Thanks for the suggestion!",buildapc,2026-01-01 22:50:50,1
AMD,nx4k01h,Yeah I would be getting the 16GB version.,buildapc,2026-01-01 19:27:47,1
AMD,nx5d5t5,I was planning on getting the gigabyte one but it went up 40€ overnight so now I'm looking at the sapphire pulse,buildapc,2026-01-01 21:57:39,1
AMD,nx5swve,"I also watched a few benchmarks of the 5700X paired with a 4090 from a youtube channel called    Hardware Tested.   Wouldn't the 3600 limit the 9070? Your suggestion does sound compelling, but considering I will be sticking with AM4, I don't wanna spend much money on this (even though I am financially stable).     I looked up used 5700X3D, which is what I really wanted and they are going for over 300€. I can afford it, but, like I said, don't wanna spend much on this AM4 upgrade, otherwise it would make more sense to go all in for AM5.       AMD has the opportunity here to make more money if they put the AM4 X3D chips back in production.",buildapc,2026-01-01 23:22:42,1
AMD,nx66yr3,"My power supply is **Corsair TX-M Series TX650M**.   \> Watch a few benchmarks yourself   The more benchmarks I watch, the more indecisive I get haha been watching benchmarks for about 2 weeks now to make sure I put my money into something that would last me another 5 years.  Would love to get an AM4 X3D CPU but they are all out of stock, brand new. And used ones are too expensive making them such a bad value.  Considering I am looking to put money into something that will satisfy my gaming needs, I know that theoretically it \*\*is\*\* worth getting the 9070XT over the 9070, but I keep asking myself, is the 9070 really really worth it, considering I don't play the latest AAA games? Like I said in the post, the lastest AAA games that I would only care to play are Cyberpunk 2077, God of War 1 + Ragnarök, Marvel's Spider-Man 2, Borderlands 4, and that I have a bunch of older AAA games in my backlog to play.   Would also need to know if that card would work on my motherboard? I see it is PCI-e 5.0, but my motherboard seems to only have PCI-e 4",buildapc,2026-01-02 00:41:17,1
AMD,nx4obsi,"Damn... Lowkey expecting that.  Thanks for letting me know of your experience!     And well, 150€ for the 5700X sounds nice. It would hurt to pay +300€ for an AM4 CPU when there are way more powerful AM5 CPU's for less than 300€ (even though I would end up spending more on new motherboard + ram).",buildapc,2026-01-01 19:49:15,1
AMD,nx5pb79,"no gpu needs 600W except for a 5090.  a 9070xt is 330W + 65W is less than 650W.  I will note that you have other power needs like ram and motherboard.  PSUs like to operate around 50% of their capacity to be the most efficient.  You will be less efficient, but well below the 650W maximum.  Probably closer to 400-425W.  You will lose some efficiency, but it will work.  Just make sure you have a good PSU (at least gold)",buildapc,2026-01-01 23:02:44,1
AMD,nx4mk8m,Have fun with your new rig!,buildapc,2026-01-01 19:40:34,1
AMD,nx67r1a,"I think the combo 3600+9070 would still give better fps than 5700x+9060 xt. There aren't that many videos on yt of the 3600, but there are quite a few of the 5600, with which you could kind of approximate. the benchmark  with the  5600+9070 gives like 30% more fps than the other combination.   Plus the advantage of getting the 9070 is that you could still use the gpu on am5 later, or upgrade the cpu when prices of the x3d inevitably come down. Overall, my advice comes from the belief that cpus through time have remained decently priced, while we have a history of pretty badly priced gpus.",buildapc,2026-01-02 00:45:45,1
AMD,nx69goa,"you can run those games with a 9060 xt on high above 60 fps, and if you don't like the performance, fsr 4 will give you a boost too. Also, 9060 xt 16 gb will probably allow you to play for the next 2-3 years the upcoming AAA games. 9070 will probably give you 4-5 years.  9070 is more future proof, but if you just want to play what's already released, the 9060 xt is more than enough. pci express is backwards compatible, so any pcie5 card will work on your motherboard.",buildapc,2026-01-02 00:55:40,2
AMD,nx4pc9o,In my case perhaps switching to AM5 and ddr5 also contributed to the overall performance. But I believe your CPU will limit it anyway.,buildapc,2026-01-01 19:54:20,1
AMD,nx5t5vt,I'm confused. Then why do the GPU specs say they require a specific amount of watts? I looked up the 9070 on AMD's website and it says 650w.,buildapc,2026-01-01 23:24:04,1
AMD,nx4uufo,Thanks!,buildapc,2026-01-01 20:22:11,1
AMD,nx6mifn,"hmm you made good points. I feel like if I can stretch my budget from the 400€ 9060XT to the 600€ 9070, then I can stretch it a little further and get the 9070XT for an extra 100€, so 700€ in total. Not sure if in a few years I'll be thinking ""eh, I should've gone for the XT version""...      The problem is the power supply, which I really don't/didn't want to upgrade. That would be another 130€ on top of the 700€, so 830€ without the new monitor. I can afford it, but spending this much money right now is not ideal.   I'll have to think between the 9060XT and 9070. You have been a massive help and I really appreciate you taking a bit of your time to explain. Thank you very much",buildapc,2026-01-02 02:15:18,1
AMD,nx5u0ef,"to provide overhead for the rest of your system.  If you had a 12600k, or any intel 13th or 14th gen cpu it would pull over 120W.  Their recommendations are a catch-all situation.  So it may or may not apply to you.  I wouldn't run a 9070xt on a lower end 650W, but I would with a low power cpu and higher end 650W.",buildapc,2026-01-01 23:28:53,1
AMD,nx6mipt,"Oh, interesting... I did not know about this. Thanks for letting me know",buildapc,2026-01-02 02:15:20,1
AMD,nxe6u8c,"Totally up to you, both are fine buys",buildapc,2026-01-03 06:12:09,16
AMD,nxe8j4i,For only AU$90 difference it’s worth it IMO.  If the difference were AU$200+ then probably not worth.,buildapc,2026-01-03 06:25:43,20
AMD,nxewjmw,"There is not much difference, i would save up to stronger gpu",buildapc,2026-01-03 09:51:59,8
AMD,nxekyeg,what resolution are you targeting?,buildapc,2026-01-03 08:10:52,3
AMD,nxea0ix,9800x3D if it’s only $100 more expensive.,buildapc,2026-01-03 06:37:59,4
AMD,nxeb0dj,I would get a 9700X because I reckon I would like to spend that money on some games. But people say no so I would save money and get 7800x3d.,buildapc,2026-01-03 06:46:15,5
AMD,nxe77eu,depends on a games you play,buildapc,2026-01-03 06:15:04,2
AMD,nxf0td4,£45 difference go for the better one  but personally would go with a 9700x it runs super cool and uses bugger all power  performance wise its a lot more rounded and very close to both the x3d chips in games   after using the r7 7700 for 7/8 months with my 9070xt it will be a perfect match,buildapc,2026-01-03 10:27:50,2
AMD,nxe7ais,"worth, get the 9800",buildapc,2026-01-03 06:15:46,1
AMD,nxe84tt,"It goes like this: you have the money --> yes, you don't think it's worth it --> no You will get more performance, but it's up to you wether you pay extra for a little push.",buildapc,2026-01-03 06:22:29,1
AMD,nxf62qp,"This small difference in price, I'd get 9800X3D. Where I am buying the difference is about 150 USD, so I'll just get 7800X3D",buildapc,2026-01-03 11:12:21,1
AMD,nxgvree,"I have had both and the 9800x3d runs a lot cooler. If heat is an issue for you (small room?), then grab it for sure.",buildapc,2026-01-03 17:12:51,1
AMD,nxhrs89,https://www.techspot.com/review/3017-ryzen-9800x3d-vs-7600x-cpu-scaling/  Both are overpriced and won't get you any real performance over a 7600x or 9600x,buildapc,2026-01-03 19:39:06,1
AMD,nxf3li2,"Hot take: 7800x3d, 5070ti",buildapc,2026-01-03 10:51:28,-1
AMD,nxi4ki2,I’m not telling anyone to save up anymore just get it today cause you never know next years price or shortage. Once China makes moves on Taiwan the CPU chips are finished.,buildapc,2026-01-03 20:42:08,2
AMD,nxf1cm1,is anything I would go for the 9700x and save even more money that would last him a good 5 years,buildapc,2026-01-03 10:32:23,2
AMD,nxf12y4,ignore them the 9700x is an amazing gaming chip they have just been caught up in the hype train   and its a 65w tdp for a super cool and efficient system even a £20 air cooler will keep it cool as a cucumber,buildapc,2026-01-03 10:30:06,4
AMD,nxhrjw7,I'd even go for the 9600x. It's €100 cheaper and get the same performance in games.,buildapc,2026-01-03 19:37:59,2
AMD,nxga9rv,Naaah diffrence is acutally very big. Especially in the 0.1 and 1% lows is the diffrence between 9700X and 9800x3D very very big.,buildapc,2026-01-03 15:30:30,1
AMD,nxg8qmx,"I always try and think about resell later as well. That goes for everything you buy though, not just PC components. Sometimes spending a little more would be worth it when you would get a better resell later when you upgrade again",buildapc,2026-01-03 15:22:54,1
AMD,nxj5bpj,"Honestly fuck it. if this is the prices we get with Jensen and Sam, I really wouldn’t mind rolling the dice with Chinese taking over fabs. Im sorry I just really miss cheap Chinese pc parts",buildapc,2026-01-03 23:46:54,1
AMD,nxf46f0,"One main difference I’ve noticed between the non X3D and X3D is that even when non X3D chips can technically perform as well as an X3D in terms of FPS, the X3D chips are much more stable. They produce less stutters,hitching, and crashes.",buildapc,2026-01-03 10:56:20,8
AMD,nxf1kmb,And as I said I would rather spend the money on a game for my computer,buildapc,2026-01-03 10:34:18,-4
AMD,nxhroq9,It's not...  https://www.techspot.com/review/3017-ryzen-9800x3d-vs-7600x-cpu-scaling/,buildapc,2026-01-03 19:38:38,1
AMD,nxji9ft,Takeover could take years the Taiwan chip plants are designed to self implode in the event of an invasion.,buildapc,2026-01-04 00:55:09,1
AMD,nxlhb6q,"Think you watch too many movies. Implode? Lol  Also China has no reason to ""invade"" when year by year Taiwan's economy is more and more reliant on China, and all businessmen move. Their simple gigantic population and economy will eventually absorb them.  Amphibious assaults are basically impossible.",buildapc,2026-01-04 08:56:36,1
AMD,nx8l3fc,Get the 7800x3d system.,buildapc,2026-01-02 11:34:05,1
AMD,nwxmkqg,thermalright ps120,buildapc,2025-12-31 16:11:48,5
AMD,nwxozo9,Thermalright has tons of price range options. But i recommend any 360mm rad aio. I personally run the TR frozen vision 360 because i wanted a lcd screen. It’s like 85 USD currently. Water cooling has gotten much more affordable and reliable. But if you want purely best reliability get a thermalright peerless assassin or any air cooler that’s similar. Air cooling will literally never fail you other than the fan maybe going bad.,buildapc,2025-12-31 16:23:45,2
AMD,nwzdhzf,get an air cooler. so much easier to deal with. NH D15 if you want to be fancy. or a deepcool ak620 / thermalright peerless assassin 120 if you want to save a pretty penny. good cpu. enjoy your build,buildapc,2025-12-31 21:35:51,2
AMD,nwxqksq,"Air. Phantom spirit. I recoil at how many people I see buying liquid coolers. I know many people will defend them, but they are just straight up trash, in my opinion. They are unreliable. They go bad sooner. They are way more expensive. I can go on. Yes you can find reliable AIOs, but you will be spending double, triple, quadruple the price for something that WILL eventually go bad, and when it does, it's putting wear and tear on your CPU.  Or you can spend $40 on an air cooler that will last forever and cool anything. Simple design = better if its providing the same level of cooling, which it is. Less room for things to fail.",buildapc,2025-12-31 16:31:35,2
AMD,nwxnt16,My first build I used a Corsair AIO. It died in 2.5 years. Swapped to Noctua NH-D15 and never looked back. It's now cooling a Ryzen 9950x3d.,buildapc,2025-12-31 16:17:53,1
AMD,nwxnt77,What case are you going to use?,buildapc,2025-12-31 16:17:54,1
AMD,nwxn338,I prefer AIOs over huge air coolers so I don't slice my hands up if I have to reach into it. I had a Thermalright Phantom Spirit and it was great but I replaced it with a Thermalright 240 Aqua Elite AIO. Cools pretty well the same but it's a lot cleaner looking and my hands appreciate it. Thermalright is a great value for cooling products.,buildapc,2025-12-31 16:14:19,0
AMD,nwxq7nx,It really is as simple as this it also makes the builds a little easier. You can use whatever fan you want with it and match the build etc.,buildapc,2025-12-31 16:29:45,1
AMD,nwxwisl,"I've have had 2 AIOs. First one worked just fine for 8 years, second one is in my current setup, no problems.   AIOs is very reliable, at least my opinion, and I like it because it's very silent system.  However, next setup I build, I will choose air cooler, because they pretty good nowadays.",buildapc,2025-12-31 17:00:58,2
AMD,nwxoa5q,I haven't chosen yet because I want to choose all the parts first and then find a case that will fit,buildapc,2025-12-31 16:20:16,1
AMD,nwxo7m6,How and why are you reaching into your case so often and cutting/scraping your hands every time you do so?,buildapc,2025-12-31 16:19:54,2
AMD,nwxq49y,"Pick the case class first (mid-tower vs compact), then cooling becomes an easy decision. The 9800X3D does not need an AIO to perform at its best. High end air cooler should be plenty (dual tower). PC parts picker is helpful. You can add you components and it gives you compatibility options.",buildapc,2025-12-31 16:29:17,2
AMD,nwxro5a,Often enough that my knuckles and hands thanked me,buildapc,2025-12-31 16:36:59,0
AMD,nxffrsx,"Its good enough, wait for DDR5 prices to come down before upgrading.",buildapc,2026-01-03 12:30:28,2
AMD,nxffykg,"A 5700X got a better price/performance ratio in most locations. The combination of 5700/5800X and 5070 level GPU is fine.  If you're looking forward to AAA gaming I'd go with 16Gb of VRAM especially if you're playing in 1440P or higher, alternatives to consider are 9070/9070XT/5070ti",buildapc,2026-01-03 12:31:53,1
AMD,nxfg7gm,What are your fps aspirations for cs2?,buildapc,2026-01-03 12:33:45,1
AMD,nxfh5ft,yes go with it,buildapc,2026-01-03 12:40:46,1
AMD,nxfj8ru,"Yes. I have the same Vermeer b0 5800x cpu that you’re talking about and I’m running a 5070ti, 32gb gskill 3600mt/s, asus rog strix b550-f with a 750w gold psu. I’m loving it so far.   I have a velvety smooth register, all services related to windows bloat are killed, and no unnecessary background tasks running. I also made sure that my foreground apps take precedence- just simple stuff.   I overclocked the cpu and undervolted by -5 on all cores. It’s smooth as butter.   My PNY 5070ti OC is overclocked and runs perfectly matched to my cpu- not bottlenecking at all.   I use OBS when I stream and it only takes about 2 or 3% of my cpu to handle that- so it’s perfectly fine.   One important thing to note here- you need to have this paired up with a 1440p monitor. That takes the heavy load off of the cpu. If you’re gaming on 1080P then the cpu gets a little flustered and sputters.",buildapc,2026-01-03 12:55:36,1
AMD,nxfl12u,"You have the am4 Mainboard already? If not and you have to buy a mainboard too, get a 14600k and a 1700 ddr4 Mainboard. Much more performance than am4",buildapc,2026-01-03 13:07:42,1
AMD,nxfhj7a,"we dont know when the ai bubble is going to burst, and the damage is done. micron would rather get ai cash rather than consumer cash",buildapc,2026-01-03 12:43:34,1
AMD,nxfxxf4,"Thanks! I actually had ChatGPT help me write my original post. I don’t really play AAA games though I mostly play more standard games like CS2, Minecraft, Forza Horizon 5, GTA V, and similar. So 12 GB VRAM on the 5070 should be more than enough for my needs. Or? 😂",buildapc,2026-01-03 14:24:59,1
AMD,nxfkum7,"Any FPS improvement would be good for me. Right now I’m playing at around 100 FPS + sometimes thermal throttling, so... :D",buildapc,2026-01-03 13:06:30,1
AMD,nxfm0xp,Yeah but ram prices right now is fucking nuts.   I am running 2×8GB DDR5 because there is no way I am paying that much for 2×16gb,buildapc,2026-01-03 13:14:09,1
AMD,nxfy0wi,"And yeah, i am playing on 1440p, and now i have 6gb vram so…",buildapc,2026-01-03 14:25:31,1
AMD,nxg2tdg,I mean if you're planning to play GTA6 and Forza 6 or want Minecraft mods and Forza 5 to look really good you may need more than 12Gb of VRAM. If a 9070(XT) is cheaper or the same price I'd go for these.,buildapc,2026-01-03 14:51:55,1
AMD,nxfrc3e,"This cpu would drastically improve performance then. With your combination you’d be looking at about 350 average fps (@1080). I only asked as, for more competitive players, AM5 would offer a meaningful boost to that, which may be beneficial if you’re on a 360hz monitor or higher. So the cpu would bottleneck the gpu in this game, but you’re probably past the point where that matters.   I don’t know as much about other games, but I do know resolution makes a big difference. The 5070 should run everything well for a few years, provided you’ve no 4k aspirations.",buildapc,2026-01-03 13:46:44,1
AMD,nxfx6nr,"Thanks for the input! I get that AM5 could give a bit more performance, but upgrading to a new motherboard + DDR5 RAM would probably cost me like 300€ more, which I’m not really willing to spend right now. My DDR4 rams are only half a year old, so they’re basically new. I’m on a 240Hz 1440p monitor, and in CS2 I usually play on 4:3 stretched",buildapc,2026-01-03 14:20:45,1
AMD,nxfy39d,Sounds like a great choice!,buildapc,2026-01-03 14:25:53,1
AMD,nxk63kz,"Only 10 fans? What are you planning on playing, solitaire or minesweeper?",pcmasterrace,2026-01-04 03:07:13,2
AMD,nxleyjh,ah! you have build THE BOAT! sweet!,pcmasterrace,2026-01-04 08:35:11,2
AMD,nxl9rnw,"Hahaha, you are right, I am playing CS2, Valo, Overwatch. Maybe I will try Cyberpunk once again.",pcmasterrace,2026-01-04 07:48:29,1
AMD,nxqghs7,"3x 8-pin, 12VHP is an unneeded risk. Just grab the cheapest 9070XT.",pcmasterrace,2026-01-05 01:15:06,1
AMD,nxqmoqx,"As a Sapphire Nitro+ 9070 XT user who has the 12VHPWR cable. I recommend to VOID AWAY FROM THE 12VHPWR.  I got the Sapphire Nitro+ 9070 XT, thinking it's no big deal since it's way under the 600W limit of the 12VHPWR that melts on Nvidia, so no big issue for AMD, right?  ... I was wrong. My shit melted after 4 months, and I had to get it RMA'd. It was the 2nd ever recorded case of the GPU melting. A few months pass by, and another 3 have melted. Just don't give yourself the headache and get a 3x 8 9070 XT.  I recommend looking into the Sapphire Pulse or Pure!",pcmasterrace,2026-01-05 01:48:08,1
AMD,nxr4dt9,Asus also has a 3×8 option.,pcmasterrace,2026-01-05 03:23:28,1
AMD,nww8h4o,honestly save a bit more and try get the 9060XT 16G. better long term buy and will save yourself from having to upgrade again sooner than later,pcmasterrace,2025-12-31 10:47:42,2
AMD,nww8r00,"9060xt, 16GB if you can 8GB is fine if not. Lower CPU overhead + PCIE X16 on AMD help a lot.",pcmasterrace,2025-12-31 10:50:17,2
AMD,nww8yr6,the 9060xt,pcmasterrace,2025-12-31 10:52:14,2
AMD,nwwajza,"The 9060xt has twice the pcie bandwidth, meaning in vram limited scenarios it’ll have less issues. Also the 9060xt is closer to a 5060ti so I don’t fucking now why it’s still a comparison, it’s cheaper and it also has less cpu overhead! If you say you want the DLSS for upscaling to 4k just get a stronger/16gb card cuz both ain’t doing that really well and FSR looks great at 1440p/4k. And if you only play 1080p and that’s why you hadn’t upgraded yet I highly doubt you’ll ever need to use upscaling in the near future as both cards here are easily doing 1080p if not vram limited",pcmasterrace,2025-12-31 11:06:50,1
AMD,nwway6s,The 5060 is not a competitor to the 9060XT - That's the 5060 Ti.,pcmasterrace,2025-12-31 11:10:28,1
AMD,nwwek05,"Before buying any of the two, I gotta point out that your rig is... dated, for lack of a better word.  Any of the two will fit and function. The question is: how much will your CPU limit the performance of your new GPU (aka bottlenecking). If you don't care about this or do not plan to go for an entirely new rig, then go AMD.",pcmasterrace,2025-12-31 11:42:47,1
AMD,nwwfahu,"99% of people will tell you to get the 9060xt 16gb..  My personal experience woth the card (and its rma replacement) have been less than good.  Constant freezing, random crashes, reboots, bizzarley high temps and wildly spining fans, drivers delete themselves -- anything that could go wrong went wrong.  First gpu and replacement were tested in 6 different PC's between my house and a few friends and all yielded the same results...unfortunate because ive been team red moatly for 25 years now.  Nvidia is overpriced...and tend to like to catch on fire in some cases these days... but id still look into one of their offerings.  But",pcmasterrace,2025-12-31 11:49:10,1
AMD,nwwfhab,Think that these Cards doesn't exist and try to buy the 16GB VRAM Versions.,pcmasterrace,2025-12-31 11:50:45,1
AMD,nwwao31,"AMD, always! leave NVIDIA in the past.",pcmasterrace,2025-12-31 11:07:52,1
AMD,nwwy56b,"Look outside of “ray tracing” nvidea has proven and stated they are a technology company that happens to make gpus, they show no love or support for it like they did 10+ years ago, AMD is that crazy cousin who has been telling you for years what could be possible for possibles sake, not money, and honestly it shows! I have a full amd build and honestly could not be happier with it!",pcmasterrace,2025-12-31 14:01:53,1
AMD,nwx3fvg,With the bottle neck how does that work and how does it show up ?,pcmasterrace,2025-12-31 14:32:38,1
AMD,nwx81lh,"It means that if the CPU is too slow to fully keep up with your GPU, then you are not getting the most out of it or, in extreme cases, it may even cause stutters. I don't know exactly which CPU you have (you may have a typo there), but if it's the same generation as the GTX980 GPU you currently have, I'd definitely watch out for that.",pcmasterrace,2025-12-31 14:58:03,1
AMD,nwyh6vn,Does overclock the CPU make a difference ?,pcmasterrace,2025-12-31 18:43:46,1
AMD,nwy8zqz,what is your wattage when you're running like this at peak,pcmasterrace,2025-12-31 18:02:30,47
AMD,nwy575j,"What kind of performance increase do you see? I was told that when using a secondary GPU for frame generation and Upscaling, you don’t get any of the artifacts or other performance issues normally tied to LS. Is that true?",pcmasterrace,2025-12-31 17:44:15,31
AMD,nwy51hx,How many games with no FSR support is 9070 XT struggling with?,pcmasterrace,2025-12-31 17:43:30,10
AMD,nx0blzd,"If I see a Fractal case, I upvote 👍",pcmasterrace,2026-01-01 00:57:49,4
AMD,nwz2llj,"Congrats on the build, this looks amazing.  I have heard about Lossless Scaling but I didn't know you could do it with dual GPUs like this. I have recently built a new PC myself (got lucky with the RAM prices in Brazil and bought it two weeks before the prices exploded here too) and I'm running an XFX Mercury 9070XT.  While it would be prohibitively expensive to do something like this now, you've given me an idea of what to do 5 or so years down the line when it might be worth buying a new GPU and using my current one as the extra.  Edit: typos",pcmasterrace,2025-12-31 20:36:37,3
AMD,nx0psu7,What's the game in picture 4? Thanks.,pcmasterrace,2026-01-01 02:30:08,3
AMD,nwymag6,I’m curious about that too! Gotta be pushing some serious watts with that setup!!,pcmasterrace,2025-12-31 19:09:34,2
AMD,nwzcha2,my PC does not have close to enough space for two gpus (at least ones I would use for this kind of thing),pcmasterrace,2025-12-31 21:30:13,2
AMD,nwzdlb6,Whats the performance difference compared to just running everything on 9070xt including loseless scaling?,pcmasterrace,2025-12-31 21:36:21,2
AMD,nwzqgw6,I'm debating if it's worth getting a rx 6400 low profile to upscale/framegen so I can run games easier with my 3080ti currently running a 4k oled 165hz monitor.   I can't go with a better gpu than a rx 6400 low profile because there's not enough space between my watercooled 3080ti and my bottom 360 rad + fans.   Would it be worth it op ?   (I don't want to spend 1000€ on a new 5070ti/5080 with just a 20/30% raw performance boost + getting a waterblock for it),pcmasterrace,2025-12-31 22:49:09,2
AMD,nx3btri,inb4 future dual gpu cards that handle this inherently so you dont have to fiddle with settings,pcmasterrace,2026-01-01 15:41:07,2
AMD,nx7hm3f,Hey OP. Are those extensions in your GPU 8pin slots and mobo 24pin? Or did your psu come with those? They look awesome,pcmasterrace,2026-01-02 05:37:34,2
AMD,nwy7h4t,"I'm going to do this too, but with a 6600XT and a R9 390 that I got off marketplace.",pcmasterrace,2025-12-31 17:55:10,3
AMD,nwy3fur,Whats the purpose?,pcmasterrace,2025-12-31 17:35:37,2
AMD,nwyb0p4,"Dear OP , I have a question as I've never tried LS, would you expect much of an improvement with a 9070 XT and a RX570? I have both cards and I just wanted to know if it's worth bothering?",pcmasterrace,2025-12-31 18:12:28,1
AMD,nwycb5e,Do they make PSUs big enough to handle two red devils? 🤔,pcmasterrace,2025-12-31 18:18:55,1
AMD,nwycoh6,Is there a tutorial you used to help you set this up? It seems really cool,pcmasterrace,2025-12-31 18:20:46,1
AMD,nwyi1m0,Do you think a 7900xtx with a 5700xt slave would be worth implementing? I saw the guide and I'm just not sure it would be worth it for my setup. I'm on the AM4 platform still with a 5800x CPU. I can outperform what my 2k 165 monitor can display most games so unless it can enable 4k scaled to 2k seamlessly without additional latency maybe. Has anyone tried this on a similar setup? Could you tell?,pcmasterrace,2025-12-31 18:48:04,1
AMD,nwysyku,Would this work with a Rx 6800 Xt paired with a Radeon Vega 64? The vega doesnt even have FSR.,pcmasterrace,2025-12-31 19:44:35,1
AMD,nx0jit0,So how does this work? Do you get more performance like with Crossfire?,pcmasterrace,2026-01-01 01:48:47,1
AMD,nx0r4qn,"Lmao, all this to try mimicking what a simple Ada GPU can do with MFG.",pcmasterrace,2026-01-01 02:39:12,0
AMD,nwzvi1t,This is so trash,pcmasterrace,2025-12-31 23:19:36,-2
AMD,nwypdlq,"I don't get it . Can't think a practical scenario using LS on second gpu , especially when old titles are super light, games with dlss2+/fsr2+ can use optiFG (optiscaler using the upscale inputs for FG) and modern games have FG baked in ....   maybe on laptops with weak gpu and LS using the igpu ....",pcmasterrace,2025-12-31 19:25:38,0
AMD,nwyyi5f,"I guess it's 300W + 150W + 100W, add 50W and it would be around 600W total consumption, on regular use. I took a 1000W PSU cause I didn't trust my 6 year old 850W to hold the extra power there.",pcmasterrace,2025-12-31 20:14:15,22
AMD,nwy6oxq,"True, ""performance"" it's the same if you meet the PCI bandwith requirements on both cards, but the perceived fluidity it's huge.  The X2 scaling on lossless works incredibly well, even maybe X3, everything starts to show heavy artifacting after that.  The best thing is that you allow to juice all the 9070XT power, while the 6600XT deals with scaling and framegen, and even with windows and extra app graphic usage (like recording or streaming).  The only issue is increased latency, but let me tell you it's almost non-perciabable, compared to higher latency provoqued by AMD or NVIDIA standard single card frame gen.",pcmasterrace,2025-12-31 17:51:29,49
AMD,nwy5upt,I came here to ask this.,pcmasterrace,2025-12-31 17:47:26,-1
AMD,nwybvem,"It's quite noticeable, actually. If your CPU comes with an integrated graphics unit (iGPU/APU), it also works great.",pcmasterrace,2025-12-31 18:16:42,-1
AMD,nwy5pp9,"None, but I prefer pure power with FSR set, only, Native full quality or completely disabled.  Still, I want to run later a 4K 144hz TV setup, and there I will need to mix both. Still, LossLess allows me to run FSR on the second GPU too.",pcmasterrace,2025-12-31 17:46:46,8
AMD,nwyde0h,Helldivers 2,pcmasterrace,2025-12-31 18:24:21,1
AMD,nx0pxd6,Lords of the Fallen. You welcome!,pcmasterrace,2026-01-01 02:31:00,3
AMD,nwze4e3,"Real performance it's the same, 9070XT runs well on PCI 5.0 x8. Lossless-wise you are able to run X2 flawlessly, with reduced lag and not losing 10% performance, generating 240fps from 120-190 (depending on games). Also the 9800x3D helps a lot preventing any kind of % lows.",pcmasterrace,2025-12-31 21:39:13,1
AMD,nwzqtfg,"No it won't be, 4k 165hz it's very demanding, for example, my 6600XT would be right on its limit on 4k 120hz, and right now it goes 70-80% on 2k 240hz HDR. Try looking for a 9060 or an Intel arc.  Still, you can try.",pcmasterrace,2025-12-31 22:51:09,2
AMD,nx7tbpt,"Thanks. They are 180° adapters, it helps a lot to deal with double cards and cable management. The Mobo also has one of 90°",pcmasterrace,2026-01-02 07:14:51,1
AMD,nwy7rxj,"Nice! Just beware the PCI bandwith, but depending on your desired frames it should work fine for fullhd-2k 144hz (applying fsr for 2k).",pcmasterrace,2025-12-31 17:56:37,0
AMD,nwy50nm,"Well, the purpose of the dual GPU it's to let the second GPU deal with the frame generation, reduces latency by quite compared to AMD or Nvidia frame gen solutions. There's no way an 9070XT can go at Max settings 2K beyond the 100-180fps on demanding titles, so the secondary GPU interpolates generated frames to ""fill"" the video output and show fake 240fps.  Because lossless frametime it's stick to original frames, but if 9070XT frametime it's already good (100-180fps, for example), the difference between the 240hz perceived video signal and the already good frametime it's almost unnoticeable.  Also, it allows me to run LLMs better once a change to a stronger secondary GPU, and, if I needed more GPU strength on professional applications, use double GPUs without buying a 5090 for the price of this whole PC.",pcmasterrace,2025-12-31 17:43:24,5
AMD,nwy43ki,"Reading can be difficult, but it’s possible.",pcmasterrace,2025-12-31 17:38:53,11
AMD,nwyblk7,"Depends on your final framerate goal. It should work for FullHD and FSR 2K, I can´t tell you the max framerate, but there´s more qualified people than me on the lossless scaling subreddit with several bechmarks and secondary gpu´s lists.",pcmasterrace,2025-12-31 18:15:20,2
AMD,nwydk9w,"Sure they can, I believe that something like a good 1000W or an standard 1200W PSU should handle both GPUs at max power limit and OC.",pcmasterrace,2025-12-31 18:25:13,3
AMD,nwydhzi,"Get 2 GPUs, get the app, run app and configure.",pcmasterrace,2025-12-31 18:24:54,3
AMD,nwyti3o,"It should work, lossless has FSR software integrated.",pcmasterrace,2025-12-31 19:47:26,2
AMD,nx0c4jw,have the same setup its great up to 1440 p 240 hz but you cant go higher than that because of the pcie 3.0 x8 bandwidth and the displayport of the vega.,pcmasterrace,2026-01-01 01:01:05,1
AMD,nwzu0ew,u/bot-sleuth-bot,pcmasterrace,2025-12-31 23:10:35,10
AMD,nx0opx9,Ur not OP blud,pcmasterrace,2026-01-01 02:22:50,4
AMD,nwyfhmb,"Non-perciabable, good to know",pcmasterrace,2025-12-31 18:35:04,16
AMD,nwz4qr1,Gamers nexus did a deep dive video on it recently. It also studied the latency aspect if you're interested,pcmasterrace,2025-12-31 20:48:27,4
AMD,nwyhd8f,Thank you for letting us know you came here to ask this,pcmasterrace,2025-12-31 18:44:39,8
AMD,nwyd20y,"That will depend on the iGPU, tried on the 9800x3D iGPU and works terribly. Maybe an AMD laptop with a good Vega Apu might squeeze well. But can't assure it.",pcmasterrace,2025-12-31 18:22:40,10
AMD,nwz2m26,Most igpus are not powerful enough to run LS. Only top end apus like a 890m will do ok,pcmasterrace,2025-12-31 20:36:41,2
AMD,nwz4nnl,"I used it on my ally X, and was not a fan of",pcmasterrace,2025-12-31 20:47:59,1
AMD,nx0kj8e,Maybe at 1080p30 lol,pcmasterrace,2026-01-01 01:55:19,1
AMD,nwzrjhq,Thanks for the heads up ! Issue is I can't find an intel arc gpu or an nvidia gpu powerful enough and as low profile as the rx 6400... Well I guess I'll stick to play games on 1440p on my 4k monitor for the time being 😅,pcmasterrace,2025-12-31 22:55:24,2
AMD,nwyibqj,how to check the pci bandwidth? what should i check before doing this,pcmasterrace,2025-12-31 18:49:30,1
AMD,nwyl1n8,"Nah, 1080p 180hz",pcmasterrace,2025-12-31 19:03:13,0
AMD,nwy6yle,How do you set that up software-wise?,pcmasterrace,2025-12-31 17:52:44,3
AMD,nwylwxx,"The tests I saw with frame gen using LLS and A 2 GPU combo made the timings worse, not better.",pcmasterrace,2025-12-31 19:07:38,2
AMD,nwyfqav,"Kinda silly to downvote the guy. There's a ton of tech jargon in the description and if you don't already know what it is, it can be very difficult to parse whats actually going on.",pcmasterrace,2025-12-31 18:36:19,10
AMD,nx3nrop,"Its ok i wasn't familiar with LS, may use it one day when i go to 4k,  letting 6900xt scale my 1440 9070xt",pcmasterrace,2026-01-01 16:45:22,1
AMD,nwyehju,"Cool, may have to look into that a bit more, cheers dude.",pcmasterrace,2025-12-31 18:29:53,2
AMD,nwyh9i7,It would be my luck that both would surge at the same time lol,pcmasterrace,2025-12-31 18:44:08,1
AMD,nwytv7p,Would i get that much gain though? Vega i believe is an 8gb card. Although it was fast for it's day.,pcmasterrace,2025-12-31 19:49:22,1
AMD,nx0enj6,For real??? You're running the vega and the rx 6800x??,pcmasterrace,2026-01-01 01:17:20,1
AMD,nwyg3bn,It don' even perciate!,pcmasterrace,2025-12-31 18:38:11,8
AMD,nwzpco4,I'd watch that. This thread is first time I've heard of this,pcmasterrace,2025-12-31 22:42:48,2
AMD,nwylaaj,Thank you for thanking him!,pcmasterrace,2025-12-31 19:04:26,7
AMD,nx0cf8p,You're welcome.,pcmasterrace,2026-01-01 01:02:58,1
AMD,nwyygd4,I have a laptop with an hx 370 and Rtx 4050 and the Radeon 890m is GREAT with Lossless Scaling.,pcmasterrace,2025-12-31 20:13:59,2
AMD,nxmr79j,"AllyX only have an iGPU. Means you sacrifice main GPU power to run LS. People here talking about running scaling on iGPU, while game runs on main GPU. Dunno to be honest, sounds like it should still be better, maybe it wouldn't be able to push above 60fps, but iGPU should be enough to x2 if you have 30 fps. What will feel terrible tho.",pcmasterrace,2026-01-04 14:46:14,1
AMD,nx004mv,"There are some LP 5050s, they are rather expensive for what they are though. You could also get a PCIe riser and just put the GPU anywhere inside or outside of your case.  Here's a spreadsheet with some real and estimated performance data if you want to look into it further, in case you didn't already know about it: https://docs.google.com/spreadsheets/u/1/d/17MIWgCOcvIbezflIzTVX0yfMiPA_nQtHroeXB1eXEfI/htmlview#gid=1980287470",pcmasterrace,2025-12-31 23:47:32,3
AMD,nwykwrp,Your motherboard should list the bandwidth of all PCIe slots in the specs.,pcmasterrace,2025-12-31 19:02:31,2
AMD,nwyl3gf,"Completely pointless, I know",pcmasterrace,2025-12-31 19:03:28,2
AMD,nwy7ddh,"First enable PCI bifurcation on your Mobo if you have that technology on your motherboard, connect HDMI/DP to secondary gpu, set primary GPU to render 3D apps on windows and start lossless scaling app while playing.",pcmasterrace,2025-12-31 17:54:41,5
AMD,nwymg9h,"Timings will be worse than no framegen at all, but better than any other type of single card framegen.",pcmasterrace,2025-12-31 19:10:22,1
AMD,nx11k70,https://preview.redd.it/mpo8z2wrtnag1.jpeg?width=4096&format=pjpg&auto=webp&s=bb4edc812bf04eca76be598a598e022006eda672,pcmasterrace,2026-01-01 03:49:50,1
AMD,nwz28lj,Can u get preciate by throw gpu at birthcontrol?,pcmasterrace,2025-12-31 20:34:37,3
AMD,nx0fb19,"I could buy a lp 5050, found it ""cheap"" enough but my motherboard only does 8x and 4x when using dual gpus (both connectors are pcie gen 5 tho).   Would it be a huge bottleneck ?",pcmasterrace,2026-01-01 01:21:33,1
AMD,nwym9ww,"Well, if you dont have to buy a new mobo, I would definetly try at least, the good think of the lossless app its that you can toogle a lot of stuff to lower the framegen stress on the secondary GPU and it may work with the 390. But I would consider to use at least a 580.",pcmasterrace,2025-12-31 19:09:29,1
AMD,nx7ydqz,"No, but you can get preganté",pcmasterrace,2026-01-02 08:01:53,1
AMD,nwz0kf4,"Wym, on PCI Gen5 motherboards the PCI lanes share the Gen5 lanes through the CPU. Other motherboards don't have bifurcation cause they don't have more than one PCI x16 that goes 5.0, usually PCI x16 X4 if your lucky, and goes through the mobo controller.  Modern motherboards not dedicated to AI take the PCI Gen5 lanes, besides one single slot for a GPU, to the M.2 lanes.",pcmasterrace,2025-12-31 20:25:25,-1
AMD,nwzx5mh,"The Gigabyte B850 AI TOP, which OP uses in his PC, can run two PCIe slots in x8 mode or one in x16. That is bifurcation.",pcmasterrace,2025-12-31 23:29:42,0
AMD,nx06ta9,"'PCIe Bifurcation is, as the name suggests, the halving of available lanes. This is very typical on consumer boards by taking one GPU and one slot at x16 and allowing for two slots and GPUs with each running at x8.'  https://sabrent.com/blogs/storage/pcie-bifurcation-lanes  Both what you mentioned and what is being done on this motherboard would be considered bifurcation. Just think about it, the second slot on that board cannot function without taking lanes from the first slot, you have to bifurcate to make use of it. The fact that the second slot is present on the same pcb and this process doesn't require additional hardware doesn't change that.",pcmasterrace,2026-01-01 00:28:27,0
AMD,nxhm5ma,Wow. I didn't know people still do this,pcmasterrace,2026-01-03 19:12:48,7
AMD,nxgfx5v,Can we use it for RAM?,pcmasterrace,2026-01-03 15:58:07,-30
AMD,nxhxsq4,X3d chips legit reduce the need for fast ram,pcmasterrace,2026-01-03 20:08:14,10
AMD,nxgx8up,Have ya tried updating the BIOS?,pcmasterrace,2026-01-03 17:19:48,1
AMD,nxhucll,Yeah I did that as well. Also tried to update the chipset and rolled back to an previously working driver version.,pcmasterrace,2026-01-03 19:51:26,1
AMD,nxhuxon,"Well, guess the cards shot then :/",pcmasterrace,2026-01-03 19:54:15,1
AMD,nxb5vub,https://preview.redd.it/z2rww5liqzag1.jpeg?width=1320&format=pjpg&auto=webp&s=05727ff13d09b2c865f5446650a5d7b45fdee24f,pcmasterrace,2026-01-02 19:51:31,10
AMD,nxb2plj,are the cpu's good for gaming? and productivity?,pcmasterrace,2026-01-02 19:36:16,4
AMD,nxc0b6k,Nice!,pcmasterrace,2026-01-02 22:20:38,3
AMD,nxcx96s,"She’s a beaut, Clark!",pcmasterrace,2026-01-03 01:20:19,2
AMD,nxdphk2,Congratz!,pcmasterrace,2026-01-03 04:08:50,2
AMD,nxdvg3s,That's a Beefy boy!,pcmasterrace,2026-01-03 04:48:15,2
AMD,nxfbelo,"Fuck these comments, my friend. Enjoy it! You earned it so enjoy it",pcmasterrace,2026-01-03 11:56:17,2
AMD,nxqwy6b,Godly cpu. I bought the 9950x before the x3d release and love it. Very nice sir.,pcmasterrace,2026-01-05 02:42:40,1
AMD,nxb2xks,"Yep, pretty much the best overall consumer CPU.",pcmasterrace,2026-01-02 19:37:20,8
AMD,nxd680c,"To actually answer the question, since it's kind of devolved.   The 9950x3d uses 8 cores with x3d cache. In most games, this puts it basically even with the 9800x3d, with some differences going each way depending on title.   For example, Microsoft flight simulator benefits from the extra cores of the 9950 greatly. Believe cities skylines 2 is another one I've seen used as an example. This can also work the other way if the game has no use for more than 8 cores in some situations. (Sometimes game bar doesn't work as well as it should. Was actually what caused me to ""downgrade"")  Where the 9950x3d truly shines over the 9800x3d is in productivity. In terms of gaming, they are more or less identical, with some weird quirks with both. Most benchmarks have it within margin of error on most tests.",pcmasterrace,2026-01-03 02:12:22,5
AMD,nxeqb0f,"for pure gaming, 9950x3d is waste of money. If you go for both gaming and productivity then yeah, go for 9950x3d",pcmasterrace,2026-01-03 08:57:32,0
AMD,nxkkdh3,Thanks!,pcmasterrace,2026-01-04 04:33:06,1
AMD,nxb6588,Recently upgraded to this as well. Thing rips.,pcmasterrace,2026-01-02 19:52:46,2
AMD,nxddace,"Performs the same as my 9800X3D, but whatever makes you feel better about spending more money",pcmasterrace,2026-01-03 02:53:37,-7
AMD,nxcsm7w,It gets regularly shithoused by single CCD cpus in most benchmarks.,pcmasterrace,2026-01-03 00:53:49,-11
AMD,nxf1noo,Lol,pcmasterrace,2026-01-03 10:35:02,0
AMD,nxdnj2y,But doesn’t perform as well for video editing and other productivity tasks. 🙃😎,pcmasterrace,2026-01-03 03:56:29,5
AMD,nxek89l,"People use their PCs for more than just gaming. The 9950 does better in productivity work, game engines, video editing etc. and in basically any game that is built around multi-threading.",pcmasterrace,2026-01-03 08:04:40,6
AMD,nxf1p6o,Lmao,pcmasterrace,2026-01-03 10:35:24,3
AMD,nxfz5f3,Half this sub doesn’t need to spend what they spend.  It’s part of the hobby.  Some spend money on a other cpu some on aesthetics,pcmasterrace,2026-01-03 14:31:50,2
AMD,nxizvx6,You’re the one blowing money on super expensive cpus in the first place but mad someone spent slightly more on a better one lmao sorry CHUMP,pcmasterrace,2026-01-03 23:18:26,1
AMD,nxl9hzv,Hi I feel great - how about you?,pcmasterrace,2026-01-04 07:46:05,1
AMD,nxcxg1l,No it doesn’t 🤣. What a fucking insane thing to actually post for public consumption.,pcmasterrace,2026-01-03 01:21:26,8
AMD,nxf2xyf,This is the shit I see in TikTok comments,pcmasterrace,2026-01-03 10:45:57,2
AMD,nxcstcf,False. It’s as powerful if not more than a 9950X in productive tasks or a 9850X3D in gaming.,pcmasterrace,2026-01-03 00:54:56,-1
AMD,nxf2p8f,lol? Value for money in terms of 9950x3d is literally atrocious if you take in mind that 9800x3d has basically the same performance. It's funny whe  yall see the most expensive option and blindly go for it no matter the price even though most of yall don't even do heavy productivity related stuff,pcmasterrace,2026-01-03 10:43:55,0
AMD,nxf06er,"Not really consumer tasks. Plenty of folk do them, some even well, but where you say ""consumer CPU"", ""intensive Blender and DaVinci Resolve on 8K content"" isn't what anyone thinks.",pcmasterrace,2026-01-03 10:22:22,2
AMD,nxj2i2q,I keep my CPU's for 7-8 years before my 9800X3D i had a 8700k. It makes sense to get the best of the best.,pcmasterrace,2026-01-03 23:32:07,1
AMD,nxcyeqb,https://www.tomshardware.com/pc-components/cpus/amd-ryzen-9-9950x3d-vs-amd-ryzen-7-9800x3d-faceoff#section-gaming-benchmarks-and-performance-amd-ryzen-9-9950x3d-vs-amd-ryzen-7-9800x3d,pcmasterrace,2026-01-03 01:27:01,-11
AMD,nxj9jca,But you didn’t lol you got the second best of the best and you’re mad that somebody else DID. Better luck next time CHUMP,pcmasterrace,2026-01-04 00:08:57,1
AMD,nxda1hm,0.8fps is within margin of error you fucking NPC.,pcmasterrace,2026-01-03 02:34:44,12
AMD,nwhvwjg,"Disconnect the igpu (the cable from the motherboard) , and connect only the 9070xt to the monitor .    Do you get signal output ?",pcmasterrace,2025-12-29 05:15:50,1
AMD,nwhwmny,"No, no signal output from the GPU",pcmasterrace,2025-12-29 05:21:06,1
AMD,nwhyupk,"I think 90% the gpu is dead , but doesn't hurt to try :   Go to Bios ->IO Ports -> Initial Display Output -> PCIe 1 Slot  and -> Integrated Graphics -> Set to Auto\*  \*I usually suggest disabling the igpu since it can cause conflicts , but if the 9070 is dead , you will have to clear cmos (reset bios) and apply your previous settings again .",pcmasterrace,2025-12-29 05:37:43,1
AMD,nwi2oul,"Tried that, sadly still the same outcome",pcmasterrace,2025-12-29 06:07:56,1
AMD,nwi3txx,"Well , unless you can find a second working PSU and a gpu card , I guess you start the RMA process . ( it happens on all devices , I know it sucks but most all other stuff are more important :) )",pcmasterrace,2025-12-29 06:17:13,1
AMD,nx93hpk,I am 14 years old,pcmasterrace,2026-01-02 13:47:43,2
AMD,nx93vbe,"Your top fans are pulling air into the case and your front fans are pulling it out. You should turn them both around for good airflow management.  Looks pretty though, and good cable management.",pcmasterrace,2026-01-02 13:49:56,2
AMD,nx953ig,Red.,pcmasterrace,2026-01-02 13:57:14,1
AMD,nx95exv,Thank you,pcmasterrace,2026-01-02 13:59:08,4
AMD,nx95itl,I will Turn the Fans arround right now i dont have time cuz I wont ne at Home for 3 days,pcmasterrace,2026-01-02 13:59:45,1
AMD,nx9doqb,Team red ist the goat.Sadly the am5 CPUs only got a ddr5 Controller and not ddr5 and ddr4,pcmasterrace,2026-01-02 14:46:21,2
AMD,nx9dtr5,I will try to oc the ram from cl44 to cl42,pcmasterrace,2026-01-02 14:47:06,2
AMD,nx9aq0o,Save your money. Get the Intel.,pcmasterrace,2026-01-02 14:29:46,3
AMD,nxakkg9,14900k or 14700k. Will perform better than the 9950x for much less. No brainer.,pcmasterrace,2026-01-02 18:11:54,1
AMD,nxblgkm,"14700k is the best option, just update ur bios to prevent any problem",pcmasterrace,2026-01-02 21:07:46,1
AMD,nxdunof,"https://preview.redd.it/du810op5d2bg1.jpeg?width=1085&format=pjpg&auto=webp&s=4d1861a014a21bb0720851f377ee7af60d0b4856  I know intel will be better becasue only 1 thing to swap. However 9950x should be better in futher, also price is same as 14900.",pcmasterrace,2026-01-03 04:42:58,1
AMD,nxfz51a,Have u tried jesus?,pcmasterrace,2026-01-03 14:31:46,1
AMD,nxm3nvg,"""If you can't beat them, join them.""",pcmasterrace,2026-01-04 12:14:00,5
AMD,nxn1a74,the famous Intel multicores,pcmasterrace,2026-01-04 15:38:33,3
AMD,nxo4ey9,"Yeah the laptop CPU naming is kinda annoying.  But honestly good price performance, IMO.  Got my laptop 2yrs ago, with R9 7940HS. Insane performance for the price.   With the 4060 and 32gb, I'll play any game 1080p >90fps, but usually plug into my 32"" 1440p, >60fps any game.   And just general computing is great.   I really want to build a desktop again but to beat this performance on my laptop by >30%, that means spending close to $4K NZD, and bugger that haha. Miss my desktop, but LAN parties with the lads is so much easier now.",pcmasterrace,2026-01-04 18:37:35,0
AMD,nx57gio,Air cooler like a chad good job op,pcmasterrace,2026-01-01 21:28:30,49
AMD,nx5vvl5,"https://preview.redd.it/mc2ftspaqtag1.jpeg?width=4032&format=pjpg&auto=webp&s=f42c101b163bf3676ce78dd8d46cc6ec4eb42b4d  This was mine 2 months ago, same case but with a 9800X3D and a 5070ti. Actually a goated case, fairly compact but so easy to build in. Enjoy your pc !",pcmasterrace,2026-01-01 23:39:35,12
AMD,nx5by47,"Nice OP, congratulations",pcmasterrace,2026-01-01 21:51:34,3
AMD,nx5glqv,Love that case so much,pcmasterrace,2026-01-01 22:15:33,3
AMD,nx5ry9i,Nice clean and compact build,pcmasterrace,2026-01-01 23:17:21,3
AMD,nx509qd,"I bought a Dell G5 gaming pre-built back in 2019 (i7-9700k / 1660 Ti), but it had a number of issues on its own.  It had some pretty severe cooling issues until I swapped the proprietary CPU cooler and added another fan.  There was also the issue of a proprietary motherboard and PSU, which prevented me from upgrading easily.  I decided that it would be a good time to start building a new PC sometime in the summer of 25, as the 9060 XT had just been released and Microcenter was selling them at MSRP.  I also had some extra money to pick up a RAM kit while I was there, but decided to put off buying the rest of the parts until Christmas.  I think I got lucky with my timing because I was recently able to pick up my motherboard and CPU at a discount.  I did have to overpay a bit for my storage, but you can't win em all I guess.  I only play on 1080P, so I think this setup will last maybe another 7-10 yrs, depending on what new games come out.  The build process was pretty straightforward, and I managed to complete it without much issue.  My GPU is pretty short, so I had a whole bunch of space for all my cables to fit under the PSU.  The only issue I ran into was that I incorrectly plugged the front USB-C thingy into my MOBO and had to dig around in my case to fix it.  Overall, I'm pretty happy with the way this build turned out.",pcmasterrace,2026-01-01 20:50:42,7
AMD,nx5c2m0,Hell yeah,pcmasterrace,2026-01-01 21:52:12,2
AMD,nx5j992,The A3 looks sooo good,pcmasterrace,2026-01-01 22:29:41,2
AMD,nx5nyiw,">G.Skill Flare X5 (32GB, DDR5-6000, CL30) - $100  Sounds like it took you 3 months to assemble.",pcmasterrace,2026-01-01 22:55:09,2
AMD,nx5yfje,Well done and elite taste,pcmasterrace,2026-01-01 23:53:45,2
AMD,nx612p3,Baller case. I got the same,pcmasterrace,2026-01-02 00:08:35,2
AMD,nx64uvb,Now that is a pcmaster *case* - wow.,pcmasterrace,2026-01-02 00:29:24,2
AMD,nx6wmjy,RAM for $100? You definitely bought that a while back lol,pcmasterrace,2026-01-02 03:17:11,2
AMD,nx8i6e0,Beautiful case. Im looking for a similar case (compact with black and wood parts) but for an atx mobo. I excluded fractal design since it costs a bit too much and it feels empty with all that space,pcmasterrace,2026-01-02 11:08:15,2
AMD,nx5vvoc,Dude. Clean up that cabling. Awesome computer. Great that you got it working. Now make it look all pretty and stuff. Have fun with it. Be well,pcmasterrace,2026-01-01 23:39:36,6
AMD,nx5a3i6,Was the 7600X3D from Microcenter? Asking bc I'm overseas 🥲,pcmasterrace,2026-01-01 21:42:03,1
AMD,nx5mdga,That case is sick,pcmasterrace,2026-01-01 22:46:22,1
AMD,nx5zu7e,Yet,pcmasterrace,2026-01-02 00:01:35,1
AMD,nx6rfva,"Challenging case for a first build too, nice job!",pcmasterrace,2026-01-02 02:44:59,1
AMD,nx70zlz,"Just FYI my century psu blew up and triggered the circuit breaker. I replaced with a corsair one. I would most likely looking to change the psu from more reputed brands (corsair, seasonic etc)",pcmasterrace,2026-01-02 03:45:10,1
AMD,nx67qi1,Did I see MSI put the CPU power under the PCIE slot? Weird design but ok,pcmasterrace,2026-01-02 00:45:40,0
AMD,nx5h6ct,"7600X3D is like 65W TDP, I think this air cooler might even be overkill for the chip lol",pcmasterrace,2026-01-01 22:18:39,15
AMD,nx5ykxy,Ty and sweet build! I was looking at something smaller (fractal ridge) initially but now I realize this case is the perfect size for me.  I don’t see myself buying any case bigger than this in the future tbh,pcmasterrace,2026-01-01 23:54:35,1
AMD,nx5xnpq,Ty!,pcmasterrace,2026-01-01 23:49:36,2
AMD,nx565lk,"Very nice, and yeah you dodged a bullet with RAM prices.   Your exhuast fan is fighting your CPU cooler, so you might want to swap that around. But aside of that, sff m-atx builds tend to be a pain to make tidy (all sff really) so your lucky that card was short. my AP201 build, with a strix 3080, was... cramped. Especially with the dual 360 rads.",pcmasterrace,2026-01-01 21:21:49,8
AMD,nx5xdty,"Took me 3-4 months to get all the parts but only a night to assemble lol  I wish it took a little bit longer to put together though, it’s kinda fun (other than the cable management)",pcmasterrace,2026-01-01 23:48:03,2
AMD,nx6mdga,Not a glass panel case so all good.,pcmasterrace,2026-01-02 02:14:27,3
AMD,nx5hvgx,"Yeah, it came from microcenter... I know there are some other sites (re)selling these chips, but they're like $50-100 more than what microcenter is selling them for.  I have no idea why AMD would restrict these chips as distributor exclusives...",pcmasterrace,2026-01-01 22:22:18,2
AMD,nx5xt8l,Ty!,pcmasterrace,2026-01-01 23:50:27,1
AMD,nx73yfu,The Gold II/2025 version?  I looked it up on the PSU tier list and they put this at an A- rating.  Some of the other century PSUs have lower ratings but I think this one is fine?,pcmasterrace,2026-01-02 04:04:24,1
AMD,nx6bp9l,Nah it’s on the top but just hidden by the cpu cooler  I have no idea what that thing on the bottom is for tho lol,pcmasterrace,2026-01-02 01:09:06,1
AMD,nx5a5vw,Happy New Year 🤡,pcmasterrace,2026-01-01 21:42:24,4
AMD,nx5uczy,"Oh yeah, it's rated for 200w+ lol     Great chip though",pcmasterrace,2026-01-01 23:30:51,8
AMD,nx5v0fq,"An air cooler is an investment IMO. No pump to fail, and if a fan goes you can replace the fan easily and keep the rest! A good cooler will last you a long time!",pcmasterrace,2026-01-01 23:34:38,2
AMD,nx5yyii,"For sure especially with how cheap mATX boards are, there is no real reason to get a bigger case and with a smaller case you'll need an SFX PSU and low profile cooling. Imo this case is quite perfect 👌",pcmasterrace,2026-01-01 23:56:40,3
AMD,nx5gvsi,"I actually flipped the fans on my CPU cooler to account for that b/c of the front mounted PSU.  Air comes in from the top and leaves through the back and bottom.  My temps are looking alright so far, so I think I set up the fans right.  I was originally gonna try and do an ITX build with a Fractal Ridge, but decided against it just b/c I knew it was gonna be hell to build in it (not to mention it probably would've added like $2-300 to my cost).  There's quite a bit of empty space in my case, but I guess I don't mind considering how easy it was to fit it all inside.",pcmasterrace,2026-01-01 22:17:03,1
AMD,nx92g3h,"Not a glass case isn't the point. Proper cable management means cleaner airflow across your cooling system and lower temperatures while pushing the PC.   It isn't just a looks thing. Plus it's a pride thing. You just kicked ass building it. Don't fumble at the finish line. You do you, bub. Be well",pcmasterrace,2026-01-02 13:41:29,1
AMD,nx7znai,You’ll be fine. Some people are just unlucky with flukes,pcmasterrace,2026-01-02 08:13:42,1
AMD,nx9mul5,Custom loop cooling is also an investment; you can re-use 90% of it when you swap computers. A good custom loop can last a long time!  AIO's are a bit of a waste.,pcmasterrace,2026-01-02 15:33:49,1
AMD,nx6db7h,">I actually flipped the fans on my CPU cooler to account for that  I thought that might have been the case, hindsight 20/20 its obviously so...",pcmasterrace,2026-01-02 01:18:56,2
AMD,nx97ej7,There's no airflow over the cables due to the exhaust being on top and no front intake. I'm sure he's plenty proud as is.,pcmasterrace,2026-01-02 14:10:48,1
AMD,nxooxqm,"Here's another outlandish discovery: pre-built PCs with 32gb of ddr5, 2tb nvme, a 5070 or better, and current gen higher end cpu SKUs are selling for $1800 or less.  Remember when people bought pre-builts for gpus? Well now you can buy them for most of the parts on an alright deal.  ![gif](giphy|lXu72d4iKwqek)",pcmasterrace,2026-01-04 20:08:37,3
AMD,nxocy5b,How much is the license for 2 users per year? RAM isn't that expensive after all.,pcmasterrace,2026-01-04 19:14:45,2
AMD,nxot9jl,Too expensive,pcmasterrace,2026-01-04 20:28:37,0
AMD,nxonb5k,"In this economy? You silly goose.   It's up what 400% in 6 months ?   Companies are canceling orders then reposting stock for 25%-50% more.  At best from past surges, rough est it'll be next fall before prices lower but I'll never return to what it was.",pcmasterrace,2026-01-04 20:01:10,2
AMD,nxot73s,Too bad you cant game on ram sticks. No HDMI port.,pcmasterrace,2026-01-04 20:28:18,1
AMD,nxon74p,"$42 CAD for a year, $190 CAD for lifetime.    32 GB of the same RAM that's in my computer (I paid $125 two years ago) is now $699 CAD.",pcmasterrace,2026-01-04 20:00:40,0
AMD,nxphg39,"For $42 CAD for the year seems like a cheap alternative over a second PC. Plus we already had the monitor, mouse and keyboard in the house.",pcmasterrace,2026-01-04 22:20:55,1
AMD,nx951zj,A bottleneck here would depend on what resolution you're gaming at.  At 1440p or 4k I don't think you'd have any CPU bottlenecks.,pcmasterrace,2026-01-02 13:56:59,3
AMD,nx94lrg,"Personally I think your CPU is the biggest bottleneck. Upgrading that should do you some good. Also before buying the 9070 XT be aware AMD GPUs are currently very unstable in most games, but playing around with some settings should help you with that.",pcmasterrace,2026-01-02 13:54:19,-2
AMD,nx9blho,"I play at 1440p, but with the new GPU maybe on 4k. I Play mostly games like total war Warhammer, Anno 117 and sometimes thinks like space Marine or Stalker 2. But nothing competetiv.",pcmasterrace,2026-01-02 14:34:44,1
AMD,nx9dihh,">Also before buying the 9070 XT be aware AMD GPUs are currently very unstable in most games  Yeah, no. That's not true.",pcmasterrace,2026-01-02 14:45:24,3
AMD,nx9tb69,You have proof to back this bullshit your spreading,pcmasterrace,2026-01-02 16:04:44,2
AMD,nx9twc6,"My own laptop lol. It has a ryzen 7 7735HS and RX680M graphics. I know it’s a APU and not a GPU but I find it funny how it struggles to run GTA V at medium graphics but manages to run RDR2 at ultra graphics, at a constant 30 fps. Maybe it’s a driver issue but from what I’ve seen AMD has improved ALOTTT like a lot. So not saying it’s still unstable but some games are really bad on it. But if I’m wrong, then my bad.",pcmasterrace,2026-01-02 16:07:30,0
AMD,nxb87zq,Fair,pcmasterrace,2026-01-02 20:02:49,1
AMD,nxb7crr,"I own a 9070 XT and my steam survey always fails to register it, results in it determining that I have Radeon graphics….",AMD,2026-01-02 19:58:38,294
AMD,nxb05hf,"congratulations, I guess",AMD,2026-01-02 19:23:58,98
AMD,nxb08ia,"It's common knowledge that the non-XT is just the cut down die from the XT, there should be at least three XT's to every non XT.",AMD,2026-01-02 19:24:22,75
AMD,nxdqjbs,More 5090s than 9070s is diabolical.,AMD,2026-01-03 04:15:33,11
AMD,nxb40f3,I was expecting atleast the 9060XT being there lol.,AMD,2026-01-02 19:42:31,9
AMD,nxaygo1,Yay lol. Where's the XT? ;/,AMD,2026-01-02 19:15:53,17
AMD,nxbtkl3,"It was there for couple months already, under dx12 if you sorted by API",AMD,2026-01-02 21:46:58,4
AMD,nxb1erl,"Why is the less desirable, ""only exists to upsell you to the XT"" card showing up before the 9070XT?",AMD,2026-01-02 19:30:01,19
AMD,nxbbprc,"Why do people wait until they get ripped off to rush out and buy the card? They probably sell more cards at over MSRP because people run out and panic buy cards at 100-200 dollars more than MSRP, like fools. I guess people just wanna give all their money to big corporations. 🤷‍♂️",AMD,2026-01-02 20:19:54,3
AMD,nxb17pa,"I'm still kinda cranky about the ""AMD made a ridiculous number of 9070XTs, and everyone can get it at MSRP at launch!"" messaging leading up to launch.   So I sell my 3070 right before launch. Log into Newegg the instant sales open up, and BAM! All of the MSRP models instantly disappeared.   Then I scramble and eventually get a Hellhound off Amazon. It's a good card, mind you. Having an extra $160 is also good, though.",AMD,2026-01-02 19:29:04,13
AMD,nxb916v,"I've seen lots of 9070 models discounted to less than 550€ (VAT included) last November, while only a couple of 9070 XTs could be found with a similar discount, so that checks out I guess",AMD,2026-01-02 20:06:47,6
AMD,nxbkx08,"I'm on linux mint 22 and steam recognizes my 9060 xt as ""radeon graphics""",AMD,2026-01-02 21:05:06,2
AMD,nxbo3sv,This is probably only visible because the steam survey is broken this month; it shows 131% share for DX12 GPUs.,AMD,2026-01-02 21:20:32,2
AMD,nxcpscv,RDNA4 cards are great,AMD,2026-01-03 00:38:16,2
AMD,nxcuv48,"Hey, I’m part of that .22%",AMD,2026-01-03 01:06:33,2
AMD,nxbj4px,Yall are getting hardware surveys?,AMD,2026-01-02 20:56:25,2
AMD,nxbk2ly,happy to be one of them,AMD,2026-01-02 21:00:58,1
AMD,nxbm6s3,The first AMD GPU in a while to show up during the run of the generation,AMD,2026-01-02 21:11:18,1
AMD,nxd5tsc,I’m have yet to see the survey pop up since I got my 9070 XT. Curious to see that GPU’s percentage in the market.,AMD,2026-01-03 02:10:03,1
AMD,nxggkps,"For the people wondering were the XT is that card has shown up as AMD Radeon(TM) Graphics for most people since launch, same deal with the non XT, so there's also 9070 cards that get lumped in there too",AMD,2026-01-03 16:01:17,1
AMD,nxidm3b,I have 9070xt mobo and rams ive run out of money lmao,AMD,2026-01-03 21:26:43,1
AMD,nxbbqc3,this survey is worthless anyways,AMD,2026-01-02 20:19:59,-5
AMD,nxc3ia1,Because steam surveys are skewed,AMD,2026-01-02 22:37:05,-1
AMD,nxbeczv,"Do you also have the iGPU enabled? My hardware survey on Steam shows the GPU as a RX 9070, RX 9070 XT, RX 9070 GRE, which is what seems to get reported on Linux.  However, the system shows it is a RX 9070 XT (lspci, LACT).",AMD,2026-01-02 20:33:00,63
AMD,nxbo3qm,You have to make PCIEx graphics your primary in BIOS and turn off iGPU (cpu integrated GPU which is identified as amd Radeon graphicsl,AMD,2026-01-02 21:20:32,29
AMD,nxbiady,"I don't know how to ask this in the nicest way, but are you sure you've plugged your cable into the GPU and not the motherboard?",AMD,2026-01-02 20:52:18,10
AMD,nxcz0o4,"Same here , same here.",AMD,2026-01-03 01:30:36,1
AMD,nxjynxs,"When I had old laptop with intel and Nvidia I was getting steam survey regularly, after new pc with amd, almost never.",AMD,2026-01-04 02:25:39,1
AMD,nxpmnd7,I just installed my 9070xt and the os is yet to recognize it and give me proper drivers,AMD,2026-01-04 22:45:31,1
AMD,nxg6rlg,"I have never seen proof of this, even though i have seen this being claimed dozens of times",AMD,2026-01-03 15:12:51,1
AMD,nxehd0q,You know what I find interesting? The last 15 top threads on this sub were all from videocardz and were all posted by RenaltMC lol. *Do they work for the website??*,AMD,2026-01-03 07:40:09,14
AMD,nxb8ny4,"At MSRP, the price-to-performance math on the non-XT just doesn't make much sense to me.",AMD,2026-01-02 20:04:57,27
AMD,nxbfvqw,"Everyone says to go for the xt but if you’re really on a budget trying to stock up before prices explode? The non-xt is perfectly good unless you have a real crazy 4K setup and even then it’ll probably be okay with some tweaking.  Maybe you gotta buy some more ram, or an nvme before they go up too, so your budget’s thin? Don’t spend the extra for the xt you’ll still be happy.",AMD,2026-01-02 20:40:34,2
AMD,nxdba9c,In the real world the 9070 is often significantly cheaper than the 9070 XT which was often selling overpriced nearly the same as 5070 Ti. People often go for the cheaper alternative options and that is exactly what the 9070 non XT offers.,AMD,2026-01-03 02:41:59,2
AMD,nxfln60,"I know AMD dont make as many GPU's as Nvidia does, but I've still felt something has always been off about how AMD GPU's get reported on Steam Surveys.",AMD,2026-01-03 13:11:41,6
AMD,nxaymvt,its still shy,AMD,2026-01-02 19:16:42,21
AMD,nxb6kbg,It gets registered as amd radeon graphics,AMD,2026-01-02 19:54:47,7
AMD,nxctwdv,Still too overpriced,AMD,2026-01-03 01:01:04,-9
AMD,nxb58dc,"Because I bought the 9070 for $580 when the cheapest 9070 XT was still selling for $700+.  The price discrepancy has, historically, been a lot more than $50 for these cards.",AMD,2026-01-02 19:48:22,41
AMD,nxb4vnv,"Because it's cheaper, simple as.",AMD,2026-01-02 19:46:41,20
AMD,nxbdc17,9070 is placed comfortably in the midrange market. 9070 XT is in the awkward $600+ range which is entering enthusiast territory. Once you're spending that much on a GPU more people tend to go Nvidia.,AMD,2026-01-02 20:27:54,11
AMD,nxb2ue6,Still 16gb but less money? People really struggle to understand gpus who don’t spend their time reading/watching reviews and stuff.,AMD,2026-01-02 19:36:54,11
AMD,nxbbnwk,The non XT only loses out on 15% of the performance at most while consuming 27% less power. I guess you could undervolt the XT but why tinker with that if you can spend less money?,AMD,2026-01-02 20:19:39,3
AMD,nxg3obv,"I bought my 9070xt at msrp on launch, they raised it after selling out",AMD,2026-01-03 14:56:31,1
AMD,nxb8h1p,You should know by now to NEVER sell your existing hardware until you have the new hardware in your hands. Yes it means you need more cash flow to do an upgrade but make it a rule to not even consider selling your current hardware to finance an upgrade.,AMD,2026-01-02 20:04:01,32
AMD,nxbldfa,"The fact that AMD is *still* getting a pass for the fake MSRP thing is really disgusting to me.  Yeah GPUs often get marked up and stuff but this gen had an explicitly fake price just to get reviews.  I remember all these reviews for 5070 Ti being like ""An amazing card, but you'll only be able to find it for $1000"" and the same people about 9070 Ti were like, ""almost as good as 5070 Ti, but only $600!""  Then I go to the shop and all the 5070 Ti models are $750 meanwhile the 9070 is $900.",AMD,2026-01-02 21:07:20,10
AMD,nxchjtb,"> I'm still kinda cranky about the ""AMD made a ridiculous number of 9070XTs, and everyone can get it at MSRP at launch!"" messaging leading up to launch.  After RDNA2 launch and the Frank ""$10"" Azor bet debacle I'm surprised anyone still trusts Radeon marketing. Even when they have good products they bungle supply and messaging horrendously.",AMD,2026-01-02 23:53:01,2
AMD,nxei352,"How about the ""AMD is outselling NVIDIA by like 20x"" according to one guy tweeting about one store he gets numbers from in Germany. Surely that would have reflected in sales data by end of year. Oh wait it didn't. Surely it would have shown up on Steam surveys at some point during the last 8 months. Oh wait. Surely all these tech youtubers banging the drum on how AMD finally beat NVIDIA this generation aren't playing up the hype for more views on yet another fleeting video. Oh.",AMD,2026-01-03 07:46:23,2
AMD,nxboyx2,"That's why I went for the regular 9070. $100 cheaper on sale, same amount of vram and having it use less power and run cooler gives me a bit of peace of mind for longevity.",AMD,2026-01-02 21:24:40,3
AMD,nxbl9sr,To you maybe. It does show trends over time.,AMD,2026-01-02 21:06:49,2
AMD,nxg6w4y,Cope,AMD,2026-01-03 15:13:31,0
AMD,nxbojws,Yeah that didn't affect me and the survey but the second I slot in my old 3080 it asks Everytime lol,AMD,2026-01-02 21:22:40,27
AMD,nxcft5f,"It wouldn't actually matter, GPU passthrough is a thing.",AMD,2026-01-02 23:43:21,10
AMD,nxp61l7,It's clearly no longer a community reddit sub. It's been locked down and only a select few approved posts are allowed. Sometimes we don't see a new post here for a couple days. Nothing but news from the approved posters are allowed. Everything else is shoved into that megathread that no one even reads.  They've damn near killed the sub and it's ridiculous. It's just news articles with a comment section in another format now.,AMD,2026-01-04 21:27:58,1
AMD,nxban45,The TDP is 80W less which might be important to some people.,AMD,2026-01-02 20:14:36,37
AMD,nxbcwq8,"It's likely by design. If they're having a high yield for the XT die, then there aren't many rejects to cut down for the 9070. If they had a lot of rejects it would be cheaper to move it, if they don't have a lot of rejects then it exists to upsell you to the XT.",AMD,2026-01-02 20:25:48,7
AMD,nxc2qqp,"As someone who bought an RX 9070 a week ago, I didn't feel like buying a new power supply and a new case.",AMD,2026-01-02 22:33:08,3
AMD,nxbw5cc,Even where you live matters. In eastern Europe XT is 100-150$ more expensive.,AMD,2026-01-02 21:59:36,5
AMD,nxf3lpv,atm I can get a 9070xt cheaper than the 9070 in my usual shop,AMD,2026-01-03 10:51:31,1
AMD,nxbtapu,"There are plenty of arguments to buy the non XT, it's a good card but my point is that the XT greatly outnumbers the non XT while only one shows on the survey. Makes the survey seem less accurate",AMD,2026-01-02 21:45:37,1
AMD,nxhat94,The dGPUs shipment numbers tell the same story.,AMD,2026-01-03 18:21:22,2
AMD,nxkthw3,">but I've still felt something has always been off about how AMD GPU's get reported on Steam    Surveys.  I've felt the same thing, especially in recent years and sale data. I just asked Steam to compare my system specs.  A lot of people are buying AMD cards right now, so we'll see what's going on by the end of the year.",AMD,2026-01-04 05:35:45,0
AMD,nxehfep,Must be the weather,AMD,2026-01-03 07:40:44,3
AMD,nxbxltx,"Not anymore. Nvidia is planning to reduce its production of video cards this year, and its prices have already increased: https://overclock3d.net/news/gpu-displays/nvidia-plans-heavy-cuts-to-gpu-supply-in-early-2026/.",AMD,2026-01-02 22:06:51,-1
AMD,nxbkp5r,Yeah they needed to make the xt fall between 5080 and 5090,AMD,2026-01-02 21:04:02,0
AMD,nxc6f4v,"Cuz undervolting is super easy and you lose hardly any performance, if any at all.  Obviously some people wont bother, but it really is braindead easy and take seconds.",AMD,2026-01-02 22:52:15,2
AMD,nxbarag,"Not just ""by now"", that was the case for multiple years leading up the the release.",AMD,2026-01-02 20:15:11,6
AMD,nxbeqzh,"Apparently, every eight years, the Internet goads me into doing something stupid when I should know better.   In 2017, I pre-ordered Mass Effect Andromeda.   In 2025, I believed when they said this GPU launch would be different.  I can't wait to see what 2033 has in store.",AMD,2026-01-02 20:34:56,5
AMD,nxbyc9y,"Prices on old hardware generally drops when new stuff is released, so I understand someone trying to get the maximum value out of his old GPU might choose to sell a bit early.  Better have a backup card though, or at least an IGP enabled CPU.",AMD,2026-01-02 22:10:33,2
AMD,nxcmt9h,"Radeon prices were much better in Europe, at launch there were a few models at msrp but 90% sold above that which was annoying, but prices dropped significantly over the next few months pretty much down to msrp for some models.  Nvidia cards have been way overpriced the entire time however",AMD,2026-01-03 00:22:02,1
AMD,nxei6lo,"At this point you have to think Frank ""whoopsies"" Azor and Jack ""I meant to do that"" are actually NVIDIA employees in disguise lol.",AMD,2026-01-03 07:47:12,0
AMD,nxbwsdj,This one seems really fucked with stats not making sense at all though.,AMD,2026-01-02 22:02:46,2
AMD,nxfnm1o,"What's very annoying is that they dont allow you to view those longer trends over time.  You only get the past handful of months and that's it.  Trends often need longer to present, especially when things can fluctuate they way they don Steam surveys.",AMD,2026-01-03 13:24:10,1
AMD,nxf05p7,"Dude, I'm 100% convinced steam survey is a scam.",AMD,2026-01-03 10:22:12,17
AMD,nxcmvyf,It does actually matter because a lot of motherboards dont support it and it’s usually not enabled.  Just look at the weekly posts in buildapc of people having poor performance because they plugged their monitor into the motherboard gpu.,AMD,2026-01-03 00:22:26,8
AMD,nxbgvn9,I can set it to consume 70w less and doesn't loose any meaningful performance,AMD,2026-01-02 20:45:26,20
AMD,nxbeohs,"That is huge regardless, mostly when even the XT version can surpass 300w, and if electric bill is something to be concerned about (depending on region), well the price adds up to higher levels...rx 9070 is pretty efficient and the best GPU, XT in case you need more power but just like 10% more.",AMD,2026-01-02 20:34:35,9
AMD,nxc8714,"If you go mid to high end gpu, then you better afford a mid to high end psu. I'd argue Psu is more important than gpu if you want some future proofing",AMD,2026-01-02 23:01:40,4
AMD,nxbyu78,"And there are also two fan versions, which XT does not. I don’t regret it.   Well, given that I have a 1080p machine, I do regret that I bought too much graphics card for my purpose. I could’ve gone with a 9060XT.",AMD,2026-01-02 22:13:04,4
AMD,nxbkg5g,This is the reason I chose the 9070 over the XT. Same reason I chose the 6800 over the 6800XT,AMD,2026-01-02 21:02:50,2
AMD,nxcp6oc,"It almost got me, because I knew I was thermals limited, but I figured an undervolt and power limit would give me enough extra performance and headroom to be worth $50...  ...and then I installed the thing, and it runs cooler than the 200ish watt card I used to have that weighed at least three times as much. So I can easily run it at full temp.  The Powercolor reaper has an incredibly good cooler on it. It may be small, but it is mighty.  ...and probably the entire reason for that is the flow-through design, so, y'know... Apparently my whole case's airflow is just a lot better now, because my CPU temps dropped too.",AMD,2026-01-03 00:34:57,1
AMD,nxbujzw,"It's pretty much the same card, you can flash the 9070 to the xt. Power delivery, heatsinks and boards are all the same.",AMD,2026-01-02 21:51:46,2
AMD,nxeu7tb,I compared non XT and XT prices locally and XT is roughly 50-60€ more expensive than same cards non XT version.,AMD,2026-01-03 09:32:13,0
AMD,nxc62in,AMD has plenty of motivation to make the same kind of allocation changes as Nvidia.  Prices will inevitably go up as well.  Nobody is spared from the RAM problems.,AMD,2026-01-02 22:50:25,6
AMD,nxef2p5,AMD will do the same.,AMD,2026-01-03 07:20:30,1
AMD,nxehpuk,"It says ""reportedly"". They left that shit outta the link but then you see its still a rumor.   Besides, with higher prices on electronics in general, people are going to be buying less because everything else is more expensive.   Both companies will likely cut some amounts....because they cannot ORDER enough memory. It's not really by choice. The market is there with more demand than supply....",AMD,2026-01-03 07:43:12,1
AMD,nxch9f7,"> Obviously some people wont bother, but it really is braindead easy and take seconds.  You haven't met the average end-user if you think it's that simple. Even with Wattman (assuming that's still a thing on Radeon). The average end-user can install an app and maybe a web browser, anything beyond that? Or anything that needs reading? Forget it.",AMD,2026-01-02 23:51:24,2
AMD,nxbf45w,"To be fair Andromeda was a decent game, just much inferior to it's predecessor",AMD,2026-01-02 20:36:45,5
AMD,nxdiub0,> Radeon prices were much better in Europe  Seems to frequently be the story. Any time I've seen someone gushing about the prices it's either been portions of Europe or in range of a microcenter. Otherwise a lot of the time it's frequently too similarly priced to Nvidia to actually choose it unless you're a big time Linux user or specifically protesting Nvidia.,AMD,2026-01-03 03:27:16,2
AMD,nxfnadc,Nvidia cards are overpriced by their MSRP already.  Not by any lies.,AMD,2026-01-03 13:22:09,-2
AMD,nxchyzn,"Valve uses AMD hardware in their products, is pushing Linux, and heavy support of more open software. They don't really have an incentive to fudge it especially in a way that disfavors AMD.",AMD,2026-01-02 23:55:24,4
AMD,nxfvmhf,"But you can check old news articles on the HW survey, for example https://www.techpowerup.com/265526/steam-hardware-survey-march-2020-intel-cpus-nvidia-graphics-cards-rising  It's better than no data at all. :)",AMD,2026-01-03 14:11:50,1
AMD,nxkzn2b,lol i got downvoted for saying its worthless - there's a video out by a youtube right now... think last months numbers are bugged.,AMD,2026-01-04 06:22:18,3
AMD,nxg03kh,"People should always bear in mind that the survey is 100% voluntary. If you do not deliberately opt in to the survey, you are left out of the survey. It's not something that happens automatically when you log on to Steam, as apparently many seem to think. IE, it's not representative of 100% of the people who use Steam, and never has been. Although in recent years, Valve has gotten better about it, the survey used to include any number of obsolete GPUs no longer being sold.",AMD,2026-01-03 14:37:07,3
AMD,nxdh8kf,"The poor performance is due to not setting the correct GPU as the 'high performance' option in Windows. It takes about 10 seconds if you don't know what you're doing.   GPU passthrough is an OS feature, not a motherboard one. It only stops working from the motherboard side if you disable some PCIe slots, or if you don't have an iGPU in the CPU.  And, in fact, GPU passthrough can lead to substantially higher performance in games when the dGPU only has 8GBs of VRAM, since if the display is connected to the iGPU, then Dawn's memory requirements are allocated to system memory, reducing the VRAM usage on the dGPU by about 1 GB.",AMD,2026-01-03 03:17:29,7
AMD,nxcnei7,"It's not about motherboard support. We've had GPU passthrough at least since the Skylake era, in 2015 - if not longer. That's when I personally noticed it worked. Windows also began making better use of multi GPU set ups (integrated plus dedicated) where it would automatically select the correct GPU.   I don't believe *any* current day motherboard wouldn't support GPU passthrough. Nor would Windows 11/10 not support GPU passthrough.",AMD,2026-01-03 00:25:13,5
AMD,nxbocvr,The 9070 is more efficient at default settings.  The XT can be set to the same power settings for equal or better efficiency.,AMD,2026-01-02 21:21:45,10
AMD,nxblplx,It's not huge at all. People worried about the bill only have to downclock to their tastes. At the same power XT will always be faster.,AMD,2026-01-02 21:08:58,0
AMD,nxetro5,"Lets save electricity!  Oh no, why is electricity getting more expensive?  Lmao",AMD,2026-01-03 09:28:19,-7
AMD,nxcgn9j,"PSU and mobo are always undervalued, but the most important parts for system longevity.  Every machine I've ever had to repair for someone where components crapped out (that didn't involve end-users overvolting) involved a mediocre PSU and a cheap board.",AMD,2026-01-02 23:47:59,5
AMD,nxbzfdd,You'll be well situated if you ever want to jump up to 1440p at least. 😎,AMD,2026-01-02 22:16:05,2
AMD,nxfkmef,"Very much depends on the model.  Plenty of 9070XT's use higher end boards and cooling to handle the higher TDP.    This is why I wish these companies would stop using such needlessly high TDP's to begin with.  Card makers simply have to build to handle that minimum, instead of leaving that only for premium/overclocking models.",AMD,2026-01-03 13:04:59,2
AMD,nxfkvow,"Makes sense to splurge the extra if you can, but also, some people have a budget.  And sometimes it will be the case that even the non-XT is pushing at or beyond the limits of somebody's budget already.  It can be dangerous game to play the ""But if I just spend a little more"" game. :p",AMD,2026-01-03 13:06:42,2
AMD,nxcgy47,"Heck AMD already barely allocates production to GPUs, it's last on their list of things to use silicon for.",AMD,2026-01-02 23:49:39,1
AMD,nxd5pym,No “average end user” is buying a Radeon dGPU instead of buying Nvidia or far more likely a prebuilt.,AMD,2026-01-03 02:09:25,0
AMD,nxbfx0q,"It honestly wasn't terrible.   When I eventually came back & finished it a few years later, it felt like a somewhat-better-rendition of a Ubisoft game... in space.",AMD,2026-01-02 20:40:45,3
AMD,nxfmn19,"Had the best combat and exploration of the series.    Also its technical misgivings were hugely blown out of proportion.    Like not having perfect lip sync for an RPG with many thousands of voiced lines, before we had better automated tech for that kind of thing.  It was overall in a much better technical state than previous Mass Effects(especially the first one!). Lastly, the dialogue writing in that game was not actually any worse than in the original games.  People have nostalgia glasses for Bioware writing, especially Mass Effect trilogy-era.  It always had very stilted dialogue in plenty of occasions, and felt very sterile at times, too.  I know this cuz I replayed through the remastered trilogy somewhat recently.   Andromeda didn't capture the same magic for sure, but I dont think anything was ever going to.  I dont think any new one will be able to, either.",AMD,2026-01-03 13:18:05,1
AMD,nxev3ja,Some years ago I had problem where I never got hardware survey even when I changed my AMD GPU from one to other meanwhile my friend got hardware survey on his Nvidia powered laptop every 6 months.,AMD,2026-01-03 09:39:47,2
AMD,nxcuxbx,I don't disagree. Just saying this months data collection is not very good.,AMD,2026-01-03 01:06:54,0
AMD,nxji6di,Unfortunately I am opted in the only want I can get it is to force it to come up  I don't see that as very genuine I haven't ever needed to do that,AMD,2026-01-04 00:54:40,1
AMD,nxg4ehy,">Although in recent years, Valve has gotten better about it, the survey used to include any number of obsolete GPUs no longer being sold.  That means it's worse! None of the data is usable if it doesn't represent anything but cherry picks.",AMD,2026-01-03 15:00:27,-2
AMD,nxdjbs4,"I'm one of those people who cheaped out on the motherboard (but not the power supply after a cheap one blew with a very load bang). I've been using a $75 B350 for about 8 years now.  One thing I've noticed is that the boards VRM isn't very good. It can't supply more than 90 amps to the CPU, and the heatsink is too small to keep the VRM at an optimum working temperature (they get really inefficient when they're too hot).  That was never an issue until I installed CP2077. It literally crashed every time I played it and after a LOT of frustrating troubleshooting I learned it was simply a heat issue, i.e it was solved by positioning a fan over the VRM. So I'm pretty sure if I had spent another $10 on a board with a better heatsink I would not have spent all that time troubleshooting.   You can save a few dollars with a cheap motherboard but I think you have to be more attentive to it's weak points, and that takes time which really just negates the initial savings you made. I don't think most people need $200+ boards, but when it comes to the VRM something in the mid range is a real step above the budget choices.",AMD,2026-01-03 03:30:17,5
AMD,nxddisj,"You can somewhat cheap out on the mobo and be fine, but if you cheap on the psu, it's a bomb waiting to explode.",AMD,2026-01-03 02:54:59,3
AMD,nxbzz8r,That’s true.,AMD,2026-01-02 22:18:56,0
AMD,nxg5ycs,"Random sampling. I've had surveys pop on all types of builds Intel/Nvidia, AMD/AMD, and AMD/Nvidia.",AMD,2026-01-03 15:08:36,-1
AMD,nxe3ymp,"i saw a video that showed there methodology - it wasn't very confidence inspiring... honestly you have all the game data right, like how many people are playing what game etc...  just make everyone opt in with an option to opt out....  maybe they'll include all these bots and farmers too which would probably be embarrassing for valve",AMD,2026-01-03 05:49:43,1
AMD,nxgjqc9,How is it cherry picking? Its just a volunteer system.,AMD,2026-01-03 16:16:20,4
AMD,nxdodga,">(but not the power supply after a cheap one blew with a very load bang).  I've actually had a Seasonic blow up recently. First PSU I've ever had pop, tripped the breaker when it blew too. Shot a massive red ember. But to Seasonic's credit all the rest of the hardware seems to have survived unscathed, which is exactly what you want in that sort of scenario.   >You can save a few dollars with a cheap motherboard but I think you have to be more attentive to it's weak points, and that takes time which really just negates the initial savings you made.  I look at it like plumbing. Sometimes saving a couple bucks isn't worth the headaches later. Don't have to spend a fortune, but you want to hit that baseline quality where you don't have to worry about edge cases.   > I don't think most people need $200+ boards, but when it comes to the VRM something in the mid range is a real step above the budget choices.  Yeah for most people I'd say sub $200 is plenty, varying some with needs. I have bought pricier 300-500 boards before, but that was always for special setups. Either HEDT CPUs when that was still a thing, or just needing a lot of expansion options. My next board is going to be on the pricier end (for me anyway boards that cost what GPUs do is insanity) simply because I need a lot of m.2 slots.",AMD,2026-01-03 04:01:45,3
AMD,nxfjtyk,"Motherboard and PSU are two things people should never totally cheap out on.  Find a good deal sure, but dont just buy the cheapest thing.  Motherboards are almost assuredly the most common failure point for PC's.  So much on them that can fail.  And you really dont need to spend much more to get something decent quality.  Power supplies similarly aren't too expensive for something good.  And if they fail, they can fry your whole PC in an instant.  A good motherboard and power supply can last a long time, so it's a very good investment for what will probably only be like $40-50 higher total cost combined.  Feel like this should sort of notice should be a sticky on top of every PC building forum/subreddit. lol",AMD,2026-01-03 12:59:38,2
AMD,nxdhagd,"Realllllly depends on how the person defines ""cheaping out on the mobo"". Some people look at it as getting an matx with a budget chipset. Some look at it as getting the cheapest board they can find on aliexpress or ebay or whatever that may have some really subpar caps and iffy VRM configs.",AMD,2026-01-03 03:17:48,2
AMD,nxhmvwv,"Literally what I quoted. If they are omitting old cards you won't get a real picture. You actually never will, as it is voluntary, and someone said opt-in as well.",AMD,2026-01-03 19:16:11,1
AMD,nxih4ea,I dont think you understand what cherry-picking means.,AMD,2026-01-03 21:43:46,2
AMD,nwsqn80,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-30 20:59:51,1
AMD,nwstlkq,"Wouldn't be surprising if AMD treats this as a ""stepping"" and just silently discontinues the 9800x3d, unless yield/binning still leaves a lot of lesser chips",AMD,2025-12-30 21:13:44,155
AMD,nwu95kn,Bring back 5800x3d!,AMD,2025-12-31 01:45:41,33
AMD,nwzwsg3,The gaming market's going to be s*** when everything is completely overpriced for the next 5 years years. So but they can pric  this at whatever they want but it's not going to matter much if you can't afford ram or storage.,AMD,2025-12-31 23:27:28,3
AMD,nx5f0g2,Will upgrade my 7800x3d and give it to my son,AMD,2026-01-01 22:07:09,3
AMD,nwvdg51,I just got a 9800x3d and it runs at 5400 with PBO I wonder what this will do,AMD,2025-12-31 06:04:31,1
AMD,nx73efp,"With RAM, storage and GPU prices skyrocketing, these are going to be a tough sell.",AMD,2026-01-02 04:00:48,1
AMD,nxmq1ez,"I suspect this is trying to aim at those still on 7000-chips, because no one on a 9000 will get this.  The RAM pricing is gonna make this a very tough sell to anyone on anything older.",AMD,2026-01-04 14:39:47,1
AMD,nwxo1x0,I want to know when this is launching...upgrading my GF to my 7800x3d from her 3700x and i want the 9850x3d,AMD,2025-12-31 16:19:07,1
AMD,nwumi1u,"as much as i want this to be true, there a part of me saying i hope not, because i just bought this shit. 😝",AMD,2025-12-31 03:03:17,0
AMD,nwv3net,"9850x3D is 108mb 3D cache not 96 !! this chart is wrong, videocardz should check twice before posting their crap.",AMD,2025-12-31 04:52:50,-6
AMD,nwuepjz,I shouldn’t have to get a 9950 to max the amount of cores of a 5 year old intel processor. Gimme cores AND power,AMD,2025-12-31 02:17:44,-9
AMD,nwt84ne,"They will release it. Then after a month or two the 9800x3d gets a little cheaper to help push it out of channel, then this completely replaces it.   The 9850x3d and 9950x3d2 are basically all about pushing everything else of the channel preparing for Zen6.   Zen6 cometh.   With DRAM as high as it is, they are hoping people will upgrade.",AMD,2025-12-30 22:22:49,68
AMD,nwtepzy,"Its basically a guarantee that this is the plan, that's a common business practice and it wouldn't be the first time AMD has done this. But its not some underhanded tactic to trick consumers, its just a logistical choice to move old stock.",AMD,2025-12-30 22:56:42,5
AMD,nwsx5ov,"As long as the price is right, there is no problem. Especially because 9800x3d is overkill for most gamers",AMD,2025-12-30 21:30:26,20
AMD,nwuxqdj,"I’m still rocking one, such a good chip. Gonna keep it and throw it in secondary build",AMD,2025-12-31 04:13:23,9
AMD,nwuedf7,I really wish they would have brought back production too given the RAM crisis. Would be a good upgrade for my old 5600x system.,AMD,2025-12-31 02:15:46,8
AMD,nwx6tjh,"Couldn't have happened at a worst time, especially since DDR4 is still relatively cheap and easy to get. Built a starter PC for my little cousin's christmas present and was hoping to use a 5800x3d since I had leftover DDR4, but they were impossible to find for a reasonable price.",AMD,2025-12-31 14:51:24,3
AMD,nx21cxa,"You had infinite opportunities to buy 5800x3d or 5700x3d, you literally slept on it",AMD,2026-01-01 09:13:47,3
AMD,nwugtlt,Id actually like to see some 6 core x3d parts.   A 9600x3d would be a killer gaming cpu for a budget limited buyer,AMD,2025-12-31 02:30:13,0
AMD,nx0sa6k,How many times do you need to buy RAM per platform?  Just once or at most twice right if you want to upgrade later.  You can go through multiple CPUs on the same RAM if you want to with DDR5 if you buy a good kit to begin with.  By the time you feel like upgrading again you will need DDR6 anyways.,AMD,2026-01-01 02:46:54,1
AMD,nxbw4h3,"6ghz with PBO  edit: Actually, more like 5.8ghz",AMD,2026-01-02 21:59:29,1
AMD,nxpawx3,I was thinking the same. Since everything else is going up in price they had to release something to bring in income from the gaming sector. Not really a real upgrade for 9000 users.,AMD,2026-01-04 21:50:26,1
AMD,nwv6xlk,Toms hardware says 96,AMD,2025-12-31 05:15:45,3
AMD,nwx2vd0,"Single CCD Zen 5 X3D CPUs have 640 KB (L1) + 8 MB (L2) + 32 MB (L3) + 64 MB (v-cache L3) = ~104 MB total cache. Probably what you saw stated somewhere.  108 MB L3 cache makes no sense, where would the additional 12 MB (over 96 MB L3) come from? Another layer of V-cache (several layers would be possible according to AMD) would be another 64 MB. All the other cache is just the maximum in one regular Zen 5 CCD.",AMD,2025-12-31 14:29:22,2
AMD,nwwdq31,These cores are not directly comparable.,AMD,2025-12-31 11:35:31,4
AMD,nwtcb74,"With all the recent holiday sales, the 9800X3D has already been sitting steadily at around $100 less than the original price where I live. I can see this being the new normal price in the next while, then the 9850X3D comes out at around the same MSRP.",AMD,2025-12-30 22:44:05,22
AMD,nwuzvmw,"Ah you *think* it’s about pushing the last of Zen 5 out of the ecosystem before Zen 6.   In a “finewine” power move, AMD releases Zen 5-D4 AM4 alongside Zen6.   This move keeps existing contracts alive, though sadly this comes at the expense of the removal of any remaining Bristol Ridge support motherboards may have had remaining.   If you have an AM4 Bristol Ridge chip, don’t update to Zen 5 DDR4 Edition BIOS.   /s",AMD,2025-12-31 04:27:21,-1
AMD,nwvh208,Unfortunately a 9950X3D2 does not actually exist. The latency between CCD's makes it not viable and the benchmarks that were made seem to be fake.,AMD,2025-12-31 06:33:49,-5
AMD,nwv7sef,"this overkill is nonsense clearly u dont play any high refresh rate games, mmos or any games",AMD,2025-12-31 05:21:54,7
AMD,nwvpc3a,I guess that’s true since they are still selling the 7800x3d,AMD,2025-12-31 07:46:51,1
AMD,nwuzzkk,Happy 4 u man. I still am on my 3600. And it seems i will be using this untill my next upgrade whenever it will be (ram pricing allowed). I need desperate to upgrade to a 5800x3d/5700x3d but there is none out there. Only in 2nd hand market for stupid price like €350 for 5700x3d used no warantee. Which is no no....,AMD,2025-12-31 04:28:05,2
AMD,nwurq80,The 5800xt is a pretty solid upgrade too,AMD,2025-12-31 03:35:16,1
AMD,nx2dsit,I didnt slept on it. I was planning to upgrade my cpu with the next Gpu genaration. But Ramocalypse changed the plans.,AMD,2026-01-01 11:25:32,3
AMD,nwur2kc,"It’s coming for sure, they’re stockpiling the lower binned dies and release them when it’s plenty enough or might end up as another microcenter exclusive",AMD,2025-12-31 03:31:08,-1
AMD,nx0ttp0,And if your cou is $200 cheaper but your ram is $500 more you're not really getting ahead now are you?,AMD,2026-01-01 02:57:32,2
AMD,nxcpblh,"lol no, not with 1.3v max",AMD,2026-01-03 00:35:42,1
AMD,nwvaqh1,Article on TechPowerUp from Dec. 17th says the same—96MB.    www.techpowerup.com/344188/amd-ryzen-7-9850x3d-listed-at-usd-553-slightly-above-ryzen-7-9800x3d,AMD,2025-12-31 05:43:32,2
AMD,nwxacf1,"I dunno what's a reasonable low for the 9800x3d, but I would consider moving to one if I found one for under $300. I have a 5800x3d now and game at 4k so it's not pertinent, but I wouldn't mind catching it at a great price. If I could get a decent mobo and the 9800x3d for $400 I'd be really tempted.",AMD,2025-12-31 15:10:18,3
AMD,nwv783h,"Zen5 would be a whole new microcode. That would be a pretty big ask. Jumping two generations on am4.  AMD would do it if it believed that say high profile markets like servers were at risk. IF mobo makers thought it would sell more mobos. Maybe. If they were like it has to be a x570/B550 board and they were making those and needed to sell them. People could keep their ram, and just update their mobo and CPU.   Once AM5 - Zen 6 is released with its IPC/Clocks and more cores, Zen 4 is less of a threat. If AMD wanted to put out 7800x3D onto AM4, they would be hugely popular.   They would just need to fit the Zen4 core with the older memory controller. This might be cheaper than restarting zen3 x3d production. It would still be significantly slower than a 7800x3d, but would be significantly faster than a 5800x3D and anything else on DDR4.",AMD,2025-12-31 05:17:51,2
AMD,nwvya4l,"Eh? AMD literally makes Eypc CPUS with up to 8 CCDS with V-Cache.  Its just not faster for normal desktop workloads like gaming or microsoft word or web browsing. Its been possible since March 2022 retail.  [https://www.servethehome.com/amd-milan-x-delivers-amd-epyc-caches-to-the-gb-era/](https://www.servethehome.com/amd-milan-x-delivers-amd-epyc-caches-to-the-gb-era/)   What changed is Registered DDR5 is unobtainable and costs between a car and a house. AI workloads can use V-Cache particularly in training or processes out side of LLMs. CFD, compression, also use V-cache across two dies with many cores.   AMD Is likely to launch V-cache on multiple CCD on AM5, and V-cache for Thread ripper.",AMD,2025-12-31 09:11:22,11
AMD,nwtsk12,Guild Wars 2 has never played so gloriously during raids I cannot accept this slander. I went from 80fps to...92fps!,AMD,2025-12-31 00:12:15,5
AMD,nwu2vhu,Cs2 and Tarkov would like to talk,AMD,2025-12-31 01:09:14,4
AMD,nww3iuc,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-12-31 10:01:29,1
AMD,nwvatk7,"You played wow with a potato back in my day. You dont need a nasa pc. And at a point, not even pros see the refresh rate difference",AMD,2025-12-31 05:44:11,6
AMD,nwv1py7,"Thanks man, and You could grab a 5800xt that’s an awesome chip. And would a big upgrade from your 3600, also I actually got this 5800x3d chip used but there was warranty because it was sold and shipped from Amazon and have had 0 issues with it. Used CPUs are more reliable parts to buy used I would say especially with am4 if the pins aren’t bent I would put down a lot money that it would work. So it either works or it doesn’t not much room for error but I get it.",AMD,2025-12-31 04:39:48,1
AMD,nx3k2u0,"Hope you learnt a lesson here, 7600X3D, 7600X, 7600, 7500f, 7400f were available more than 2 years and all these times you could upgrade.  Here's another tip I give you, upgrade your CPU to 5800 XT, 5800X or 5700X right now if you don't want to regret it next year.",AMD,2026-01-01 16:25:39,1
AMD,nx4kklo,"Ahead of what?  What is the baseline we are referring to in this scenario?  If the new CPU and new RAM aren't significantly faster than what you had for the money it would cost, don't upgrade obviously.  There are multiple generations of people that were already on DDR5 already.  It's not new technology.  AMD moved to DDR5 over three years ago, and Intel earlier than that.  There are a lot of people in the world that can upgrade to the newest AMD CPUs without buying a new kit of RAM and there's no reason for AMD to stop selling new SKUs in the meantime while they have quite literally no control over the DRAM manufacturing market and the problems it faces today.",AMD,2026-01-01 19:30:35,0
AMD,nwvce24,"Yeah its just 9800x3d but boosts to 5600 mhz instead of 5200, thats it... btw u should remove the amp link, use ublock origin it does that auto...",AMD,2025-12-31 05:56:15,5
AMD,nwy2c6h,Unfortunately not worth it at the moment with DDR5 prices the way they are.  The time to upgrade was when the 9800 first launched.,AMD,2025-12-31 17:30:09,6
AMD,nwwdjv1,Interestingly amd still hasn’t released zen 5 epyc with 3d vcache,AMD,2025-12-31 11:33:58,4
AMD,nwyap07,Ah yes EPYC = Ryzen yeah? You game on EPYC CPU's? Actually make a proper comparison and not this PC vs Server stuff.  And yes AMD will most likely release V-Cache on multiple CCD's for Ryzen when they have resolved the latency problems which is not until Zen 6. So a 9950X3D2 makes no sense at all.,AMD,2025-12-31 18:10:50,3
AMD,nx0q31u,"Enterprise LLMs arent using V-cache.  Maybe finance could use it for lower latency transactions, or some special scientific workload that I have no idea",AMD,2026-01-01 02:32:04,3
AMD,nwultco,My framerates in cyberpunk 2077 on my i7 9700k at 1440p was around 60fps but with my 9800x3d it skyrocketed to over 100fps in the benchmark alone!,AMD,2025-12-31 02:59:11,6
AMD,nwx6goj,"idk about recently now that they've sent a bunch of addons to the shadowrealm, but retail wow with a bunch of addons is an absolute PC destroyer. going from 5600x to a 9800x3d quadrupled my FPS just sitting idle in main city.",AMD,2025-12-31 14:49:29,5
AMD,nwv5qfs,I am gaming on 4k with my 7800xt. So 5600x wouldnt be.much of an upgrade cause i need to increase the minimum 1% low fps. 3600 generaly is holding strong for my games usage. As fir the second hand picks i was on about i mean markets like ebay aliexpress allibaba. Etc... If i could find that cpu to a logical price from a normal market or a trustworthy 2nd hand retailer i would pull the triger.,AMD,2025-12-31 05:07:15,1
AMD,nx4ff4b,I ordered 2day a 5700x3d (tray) for 310 Euros from a vendor from Aliexpress. I will receive it 1st week of February...,AMD,2026-01-01 19:05:01,1
AMD,nwvjim2,"Sorry, I was on mobile. I always use Firefox + uBlock on desktop. I have FF + uB on mobile but I kept chrome as default for no good reason. I'm going to switch my home screen browser shortcut to Firefox now.",AMD,2025-12-31 06:54:24,2
AMD,nx03cvh,"honestly, this is already a hot chip, I don't think the extra 400mhz will be worth the extra heat but that's just my take (I am going off based on how the 5800x3d temps jumped compared to the original 5800x)",AMD,2026-01-01 00:07:17,2
AMD,nwzftcj,They said they don't have any plans too either btw,AMD,2025-12-31 21:48:33,2
AMD,nwv6ggu,At 4k I definitely get that brotha good luck with the build watever you do  🤜,AMD,2025-12-31 05:12:22,1
AMD,nx4gxec,Even better,AMD,2026-01-01 19:12:27,2
AMD,nx1gob3,"Even right now the extra 200  mhz isnt worth the extra power, at stock turbo boost with pbo 2 it uses like 1.1-1.15 voltage or smth like that? And then at +200 frequency (the max) uses like 1.3v, it's just not worth it unless u have the cooling to spare, which arguably this cpu is easy to cool regardless if it uses 100 or 150 watts, but in the latter case ur fans have to spin faster which make more noise, and as a speaker use, i hate noise. And i hate watercooling cuz pump cant be turned off and it has a coil whine.",AMD,2026-01-01 05:48:27,2
AMD,nx1i8c6,Milan X and Genoa X were probably niche tbh.,AMD,2026-01-01 06:01:59,1
AMD,nwv84d5,Thanx bro 🤜,AMD,2025-12-31 05:24:20,1
AMD,nx4ld5c,i just hope it will be legit. Because i am always stressed when i shop from the Chinese....Also it is tray so it will not have AMD warranty...,AMD,2026-01-01 19:34:34,1
AMD,nx2cgt6,"I agree, I have a noctua g2 on mine currently but my case fans drown it out, I just wish that pumpless thermosyphon which was teased last year makes it to the market eventually, I could replace the g2 and give more breathing room for my 5090 for cooling",AMD,2026-01-01 11:11:44,1
AMD,nx2d6hp,Coil whine is not what you think it is.,AMD,2026-01-01 11:19:07,1
AMD,nx2fdh5,What do u think i think it is?,AMD,2026-01-01 11:42:00,1
AMD,nwir9qe,When they are going to release the 8 core version of HX3D for mobile?,AMD,2025-12-29 09:50:04,7
AMD,nwpix6j,They don’t even make a standard 8 core non X3D desktop replacement cpu anymore (no 7745HX or 7645HX successor).,AMD,2025-12-30 10:24:15,3
AMD,nwnjd9j,Why is this being covered in the news as if it's a specific RDNA2 problem? It sounds like it's isolated to specific cards and people who let their GPU sag. It's almost 1:1 tied to heavy cooler cards.   I haven't had as much of an artifact from my 6900xt from xfx with a z support bar all these years later and that thing is massive.,AMD,2025-12-30 01:43:30,15
AMD,nwndav3,well that's good to hear they still honor cracked gpus,AMD,2025-12-30 01:09:15,3
AMD,nwpemdr,Is it still possible to send my AMD GPU in for out-of-warranty service? I feel like my 6900 XT is on its last legs.,AMD,2025-12-30 09:44:59,2
AMD,nxg5bbe,"Yes, I bought the reference AMD 6900XT...10.5""...because of an old case I was using at the time that wouldn't support the longer, heavier cards. It's now running in the wife's machine, still running fine.",AMD,2026-01-03 15:05:14,1
AMD,nwe24vp,Looking forward to seeing the Crosshair X870E Hero NEO board (just for curiosity since I already have the non-NEO version),AMD,2025-12-28 17:05:58,29
AMD,nwforp0,"> ROG  I can't wait for them to cost more than the CPUs going into them./s  Seriously though, I feel the chipset offerings for higher-end systems is lacklustre, especially considering the insane prices some of these boards go for.  X870 is just a B650E in fancier terms and X870E is basically X670E but with USB 4. I feel X870E in particular is bad as it takes up 4 PCIe lanes and provides less options to the user (you could always add USB 4 through a PCIe expansion card and basically have an X870E, if needed).  X870 may also be seen as a downgrade on X670 as it offers less M.2 slots due to having only one BIOS chip instead of two, like X670, X670E and X870E. Some X670 boards also received unofficial PCIe 5.0 support on the main x16 slot, which brings it on par with X670E.  I feel AMD should've changed the chipset uplink to PCIe 5.0 x4 instead of PCIe 4.0 x4 for X870 and X870E, as this would bring the bandwidth between the chipset and the CPU to parity with Intel's Z890 (runs at PCIe 4.0 x8). This would also enable the use of an extra PCIe 5.0 NVMe SSD without leaving too much performance on the table as it will no longer be bottlenecked by PCIe 4.0 between the chipset and the CPU.",AMD,2025-12-28 21:48:20,21
AMD,nwfafra,"Well, just like Gigabyte released their (Elite, Pro, Master, Extreme) ""X3D"" refreshed versions... sound like Asus wants to do the same.  I got the Aorus Elite X3D board and im super happy with it. Curious what the NEO boards have to offer.",AMD,2025-12-28 20:38:18,9
AMD,nwepsqm,What does neo mean? Did they say anything about that?,AMD,2025-12-28 18:58:59,12
AMD,nwe6968,Will there be more workstation boards or is it just Pro WS B850M-ACE SE?,AMD,2025-12-28 17:26:26,5
AMD,nweqmg6,"Of course, right after I buy a new X870E board...",AMD,2025-12-28 19:02:46,4
AMD,nwj3rx8,"My preinstalled io shield on the B450-F is so loose that I can barely plug in a usb connector anymore. Can I fix it? Nope, a giant plastic shroud is blocking access. I used to swear by Asus, but wow their products are trash these days. I can't even control the LED's anymore because I need something called ""armory crate"" and not just the standalone ""aura sync"". Fuck asus and their crap software, which btw wont even install anymore either.",AMD,2025-12-29 11:42:37,3
AMD,nwfsiwd,"Finally, will be keeping eye on what these revised boards do.  MSI got the Max, Gigabyte has X3D, Asus has NEO (lol) and dunno ASRock yet but I'm guessing people have trust issues. MSI messed up with my godlike I got last year so they replacing it with a godlike x. Was buggy as hell so if the moment my replacement acts up I'm replacing it quick with gigabyte or this asus.",AMD,2025-12-28 22:06:56,2
AMD,nwfy7js,"I wonder if there is a new chipset for Z6 next year. Seems awfully unnecessary with a whole new lineup now, mid-generation. The 8xx boards were already an opportunity to improve designs and add slightly newer 3rd party chips like Wifi+BT along with USB4, two years after 6xx and Z4. And the 8xx boards were the *same* chipset hardware. What the hell is this then supposed to offer?",AMD,2025-12-28 22:35:49,2
AMD,nwi9fnk,Thunderbold 5 (80gpbs USB) or it didn't happen.,AMD,2025-12-29 07:04:44,2
AMD,nwr6g6u,there will be alot of expensive stuff announced at CES,AMD,2025-12-30 16:35:42,1
AMD,nwuavrc,"Btw, why there are no available X570 around? All I see B550… maybe there are no huge difference maybe but just my curiosity.",AMD,2025-12-31 01:55:34,1
AMD,nxc286v,"I wonder if they'll refresh the Apex, I want to get the next iteration of the Apex if they do since I want 2DIMM boards.",AMD,2026-01-02 22:30:27,1
AMD,nweqi35,"I really wanted the X870E Crosshair Hero, but the rear I/O was so disappointing at that price.",AMD,2025-12-28 19:02:11,7
AMD,nwgxy2x,there are several x870e boards which allow for the USB4 ports to be disabled (or shared with an m.2 or pcie slot). For whatever reason ASUS doesn't allow for this yet. If AMD somehow got their hands on some USB4v2 chips it wouldn't matter at all since those essentially operate at pcie 5.0 output anyways.,AMD,2025-12-29 01:49:56,6
AMD,nwhuans,At least you get a faulty PCIE slot and a warranty not worth the paper it’s printed on.,AMD,2025-12-29 05:04:31,1
AMD,nwnerfe,what did people expect when it comes to pricing?   Companies are for profit and not defending them only explaining the natural course of whats happened.   AMD took Intel's 2 generations per socket and kicked it to the curve.   Companies are selling 2-4times fewer boards since people upgrading chips aren't upgrading there boards anymore.  The only way to make the same profit was to price boards higher. People though they could have there cake and eat it to.   That isn't even taking into account inflation and all that.,AMD,2025-12-30 01:17:33,1
AMD,nwtmowt,PCH link is because the controller doesn't nearly saturate 4.0 bandwidth; in fact neither do the 5090 or 9070. The board already has 5.0 PCB traces so it wasn't just for RF or VRM reasons; they likely saved money on substrate design..  The other stuff is mATX and ATX problems  Anyone who cares to argue go look at benchmarks of a 9070 XT running on A620 next to 9070 XT running on X870E on OEM clock and voltage.. A620 is hard-locked x8 4.0 so they can save money on PCB shielding.. Widely-demonstrated negligible FPS avg. difference.......,AMD,2025-12-30 23:40:00,1
AMD,nwfoqzi,"Have you had any problems with the bios? I was thinking about picking up the Elite, but I haven't seen many reviews of the X3D version yet.",AMD,2025-12-28 21:48:14,5
AMD,nweyagy,It means 20% higher cost.,AMD,2025-12-28 19:39:01,43
AMD,nwezs4b,Neoprice. Basically they are increasing price to account for inflation until 2030,AMD,2025-12-28 19:46:11,18
AMD,nwf79ri,Small improvements/changes just to refresh the lineup so they can ask for MSRP prices again.,AMD,2025-12-28 20:22:28,10
AMD,nwg9kx6,"maybe new things like more usb in the io shield, better vrm (maybe +16 phases or more), led debug display, clear cmos button, better pcie button and maybe rgb logo plate  oh! and a +10%-20% price increase .\_.",AMD,2025-12-28 23:36:32,4
AMD,nwehhri,"I don’t think so, workstation motherboards usually takes longer. The Pro WS B850m just launched last month.   Saying this as I just bought the Pro WS B850m last month, but I picked it because of mATX & 10GbE. So unless the new range will have these I won’t be interested.",AMD,2025-12-28 18:20:32,4
AMD,nwettvr,workstation board with 24pcie lanes?,AMD,2025-12-28 19:17:39,3
AMD,nwf1ccn,Me2 :D Last week godlike edition x.,AMD,2025-12-28 19:53:38,2
AMD,nwfqkw7,"Well, my board came with bios version F2 and had a couple of random reboots in the first few days and then did some RAM tests. Turns out after updating to F4 it solved my RAM stability issues. Right now its great.  Just forget the base Elite and get the Elite X3D. It has better VRM, metal backplate, more ports, more headers, full bandwidth wifi 7, 5G Lan (instead of 2.5G).",AMD,2025-12-28 21:57:12,3
AMD,nwfcg3u,The X is MSi’s GodLike refresh. You’re good.,AMD,2025-12-28 20:48:00,1
AMD,nwfr950,"Good to know, yeah the x3D seems just objectively better, and it's PCIE lane sharing split makes way more sense than the original board. I wonder if these new Asus boards will also get better lane sharing",AMD,2025-12-28 22:00:32,4
AMD,nwfkvcn,"It's identical to the Godlike, even. Only differences are cosmetic",AMD,2025-12-28 21:29:14,2
AMD,nwfsv98,Check [this review](https://youtu.be/DCzTATSHVMM) of the Aorus Pro X3D (which is basically the same as the Elite X3D just with a bit better VRM and a tiny RAM fan included). The X3D turbo mode 2.0 is very good. Gigabyte really did some magic with these X3D cpus.,AMD,2025-12-28 22:08:41,3
AMD,nwba5fg,"Interesting but not my fav stock cooler design, so no big loss honestly. The stock coolers on 6000 went pretty hard though",AMD,2025-12-28 04:38:02,3
AMD,nwowhta,"That red PCB takes me back, I miss when Radeon cards were all red.",AMD,2025-12-30 06:57:57,1
AMD,nw9u9t0,"Your post has been removed because the site you submitted has been blacklisted, likely because this site is known for spam (including blog spam), content theft or is otherwise inappropriate, such as containing porn or soliciting sales. If your post contains original content, please message the moderators for approval.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-27 23:28:21,1
AMD,nwt1fbc,"Oh yes, I definitely remember my old Radeon 9800 XT or it was PRO version. It was my first GPU. I remember watching a friend play Prey (2006) on that GPU and being amazed.",AMD,2025-12-30 21:50:36,1
AMD,nxaveof,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-01-02 19:01:31,1
AMD,nxqzm72,I'd wait a few weeks before using this.  MSI has had some bugged BIOS lately.  Which is why they pulled them not long after release.    I think the last stable BIOS they had for most of their boards was from Sept 2024.,AMD,2026-01-05 02:56:52,0
AMD,nwzvru2,Won't be long before the fan blades are LCD screens themselves.,AMD,2025-12-31 23:21:15,36
AMD,nwzy1r9,Another brick in the bin... This shitty fired power connector... Who want to have his gpu burned with this power connector? Never me. Only eight-pci-e-connectors for me.,AMD,2025-12-31 23:35:00,27
AMD,nx031yo,🤔 that’s like not the worst idea tho,AMD,2026-01-01 00:05:24,8
AMD,nx68l2u,"I mean... with massive heatsinks and zero RPM designs, this is viable! (With the caveat that you can never run any games or compute workloads on it. But who needs that anyways? /s)",AMD,2026-01-02 00:50:33,1
AMD,nx3g6y3,"I like when it's offered, Sapphire does this where some cards have it and others don't. It's good to have options. And on a moderate power GPU like this it makes a lot more sense. You aren't anywhere near the electrical limit that caused issues with these powering 90 series cards.",AMD,2026-01-01 16:04:49,2
AMD,nx064cc,You need to get over this connector honestly. At this point you are just spreading flames for nothing. 3x 8 pins PCIe cables are out. The number of problematic installation with the 12x-2v6 is very very low compared to the number of these GPU’s installed in gaming PC’s.   I just ditched a bricked 7900xtx with 3x 8pins due to saging (it has a support bracket too) in favor of a nitro+ 9070XT with the 12v-2x6 connector. Im happy to say goodbye to a bunch of wires tangling on the side of the GPU.,AMD,2026-01-01 00:24:11,-29
AMD,nx6u9j2,Or what about something like those lian li fans that have screens in the motor hubs,AMD,2026-01-02 03:02:15,1
AMD,nx3jzam,"Sapphire cards with it have had melted power connectors tho. It's not just the amount of power used, it's inherently flawed.",AMD,2026-01-01 16:25:09,4
AMD,nx33hv2,"Very low, yet still more than the 2x or 3x 8pins.  Why even take the chance?",AMD,2026-01-01 14:51:43,5
AMD,nx0dbbe,Corporate simp. Imagine defending a fire hazard. 😂,AMD,2026-01-01 01:08:42,19
AMD,nx7ysrw,"Or like one of those ""hologram fans"" that just have a single line of LEDs that sync with the fan speed to produce an image. That'd actually be pretty cool with a vertical-mounted GPU",AMD,2026-01-02 08:05:47,2
AMD,nx3sq8x,3 reports in 9 months? And covered under warranty? I mean personally I'd take that risk as I would say that is borderline negligible. When I think the 4090 launched and we were seeing like a dozen reports in the first few weeks thats something I would stay away from.,AMD,2026-01-01 17:11:31,0
AMD,nxax1zg,"it's less the cable itself, and more that the cable HAPPENS to also be timed up with the fact that gpu companies removed the load balancing circuit off gpus after Ampere.  The Sapphire cards also lack load balancing hence why its also affected by the same problems. The coincidential combination of the 12vhwpr connector and the removal of load balancing together was a terrible combination.",AMD,2026-01-02 19:09:17,1
AMD,nx3gmmb,Easier cable management especially compared to 3x 8pin. It's pretty huge in SFF builds.   On a card like this where you don't get anywhere near the power levels it's preferable IMO. It's safe and you get one neat cable as opposed to 2 or 3. The issues only happen when you try and push insanely high levels of power through one cable.,AMD,2026-01-01 16:07:09,-4
AMD,nx0dhj1,Nothing will ever please you I say. Complain about this and that. Just noise,AMD,2026-01-01 01:09:50,-21
AMD,nx3tsyz,Fair enough point but I'd still rather take the 8 pins myself :),AMD,2026-01-01 17:17:08,3
AMD,nx65a45,People are not worried about warranty. People are worried about the fire hazard.,AMD,2026-01-02 00:31:49,2
AMD,nx49ebq,"I'd take not setting my PC on fire instead of 'better cable management' any time of the day, lol!",AMD,2026-01-01 18:35:17,8
AMD,nx4gi85,Imo that’s such a bad trade off and I can’t see how any rational person would disagree.,AMD,2026-01-01 19:10:21,1
AMD,nx0dptq,How dare I not want my house burned down while gaming! I'm so spoiled! 😞,AMD,2026-01-01 01:11:19,16
AMD,nx53147,I think *most* people would prefer the 8x pin but there are definitely a sizable amount of people who prefer this connector and its good there are options. An actually irrational person would be upset at consumers having a choice.,AMD,2026-01-01 21:05:26,-1
AMD,nx0dvb0,You speak like its a certainty but the reality is no where near as bad a you describe you drama queen.,AMD,2026-01-01 01:12:17,-18
AMD,nx57s95,Not upset. Just befuddled by the cost/benefit risk assessment.,AMD,2026-01-01 21:30:13,3
AMD,nx0e0u1,Oh sure. There's only a small probability that you'll end up homeless. It's all good! 😎,AMD,2026-01-01 01:13:15,12
AMD,nx2bx78,"Are all people who know Kirchhoff's current law drama queens, in your opinion?",AMD,2026-01-01 11:06:06,1
AMD,nx6aw3x,"I mean if you're splitting hairs about small risks, all electrical in a house carries a risk. My PSU blew up a few weeks back and shot a big ember at me while at idle.   There are tons and tons and tons of these connectors and cables out there. And scant few reports of anything beyond some people melting some GPUs.   Dislike the connector, campaign for better safety margins, but don't exaggerate things either.   At any given moment there's a chance a drunk driver in an SUV could plow through the building you're in... do you wax poetic about that risk too?",AMD,2026-01-02 01:04:14,1
AMD,nwztzbz,An article about a reddit post. Posted on reddit. Infinite recursion!  https://www.reddit.com/r/ASUS/s/r7r4H74Ulu,AMD,2025-12-31 23:10:24,622
AMD,nx00z39,"I love reading suspicious posts from a brand new account, with no proof and then reading the same post, linked from a website onto a different post to make it seem more legit.",AMD,2025-12-31 23:52:38,288
AMD,nwzsxwb,What the heck is a huntkey PSU? Don't cheap out on the PSU LOL.,AMD,2025-12-31 23:04:02,260
AMD,nwzxi3u,"Unverified rumor, not news",AMD,2025-12-31 23:31:46,76
AMD,nx03qn9,Pairing a premium 9800x3d with a CHEAP/Budget $89 motherboard and a knock off power supply = blame the person who made that shitty decision.,AMD,2026-01-01 00:09:38,89
AMD,nx0qmy4,Tl;dr is the guy decided use F tier rated PSUs with his 9800X3Ds.,AMD,2026-01-01 02:35:50,15
AMD,nwzz0ai,Why would an internet cafe need 150 Ryzen 9800X3D CPUs? Why would an internet cafe have the budget for 150 9800X3D CPUs? Read the og post this article refers to and have doubts this is real ... dodgy PSU or not.,AMD,2025-12-31 23:40:46,49
AMD,nx16l2y,Huntkey psu sounds sus,AMD,2026-01-01 04:25:43,3
AMD,nx07916,The last intel employee,AMD,2026-01-01 00:31:04,10
AMD,nx008fo,"What's the point of buying a bottom of the barrel board and PSU to plop in a nearly $500 CPU? Let alone doing so ""150 times"".  Hardly passes the sniff test.",AMD,2025-12-31 23:48:10,7
AMD,nx21zz0,All OP does is recycle shit article references with click bait titles. 100% waste of time.,AMD,2026-01-01 09:20:46,3
AMD,nx29oxs,"If it’s all in the same cafe, the CPU type is only 1 common denominator, so are environment, installation process, coolers, …",AMD,2026-01-01 10:42:55,3
AMD,nx2gnmp,and the owner's name is u/Distinct-Race-2471,AMD,2026-01-01 11:54:51,3
AMD,nx2vx49,![gif](giphy|ANbD1CCdA3iI8),AMD,2026-01-01 14:01:04,3
AMD,nx2ko70,Dogshit mainboard?  Check.   PSU that is known for blowing up and/or killing stuff due no protection features?  Check.    ItS AmDs fAulT!! 1!1!!!!   Check.    Normal Intel fanboy behavior.,AMD,2026-01-01 12:32:45,6
AMD,nwzywcc,"I know its not the main discussion, but why an Internet cafe needs 9800X3D's??? A 9600X for gaming tasks on this kind of systems is more than e enough.",AMD,2025-12-31 23:40:06,14
AMD,nx08r7y,"Complete nothingburger.  Buys 150 9800X3D's, 150 bottom of the barrel ASUS motherboards and 150 shit tier no-name PSUs....  From the start... a recipe for disaster and to that add dirty AC power since probably the power system is as shit  as the PSUs (because OP clearly doesn't care about the power that he feeds the components) and you get fried components.     That if the story is real, the only ""proof"" is a screenshot from zenmaster...",AMD,2026-01-01 00:40:08,11
AMD,nx14eho,fake news,AMD,2026-01-01 04:09:31,6
AMD,nx003yr,"Wtf, why even give an audience to such bullshit. The rma rate is 0.64% (source: https://www.mindfactory.de/product_info.php/AMD-Ryzen-7-9800X3D-8x-4-70GHz-So-AM5-WOF_1595711.html)",AMD,2025-12-31 23:47:25,14
AMD,nx0a97o,"15 bad CPU's or 1 bad mobo, short, PSU, etc?  I've dealt/did builds/OEM's/etc with over 100 CPU's, and had one bad CPU out of a batch of half dozen I picked up from MC once. Didn't realize it was bad because it was the last one to get used months later, but Intel took care of me.  Out of all the components, I'd had worse luck with RAM, and motherboards than CPU's, and components like PSU's have worse QC than CPU's.",AMD,2026-01-01 00:49:18,4
AMD,nx1v42j,"I actually saw the reddit post when it got posted. At some point, someone asked what PSU they used in the builds, and the guy answered they use some no-name brand gold rated. So, even if it is true, it can be related to bad power delivery.",AMD,2026-01-01 08:07:09,2
AMD,nx2gnrt,"Key points to a good pc.   buy quality psu and mboard.  next, ensure the article is legit and the owner knows what he or she is doing.   First, whoever else runs a coffee internet had similar things happen to them?  none?  Ok, article must be legit...  I had a asus b650board for years with 7800x3d then currently 9800x3d and has worked flawlessly.",AMD,2026-01-01 11:54:54,2
AMD,nx2k7ox,"Imagine being like the cheap CHINA guy who ran the internet cafe.  \-He buys 150 9800X3D's and then matches them with F tier PSU's.  I mean, how much money did he save from each garbage tier PSU vs a B tier PSU? $30 per unit? What's $30 per unit to a guy that casually goes out and buys 150 9800X3D's?",AMD,2026-01-01 12:28:33,2
AMD,nx2u21v,"""Huntkey Power Supply"" LOL",AMD,2026-01-01 13:47:53,2
AMD,nx7w9me,That's what they get for pairing the cheapest PSU and motherboard with the highest performance gaming CPU and expecting no issues.,AMD,2026-01-02 07:42:09,2
AMD,nx8k3ko,"If I was running an internet cafe I would not be wasting money on CPUs massively over-specced and over-priced for the actual task, I would be using something like a 5600G",AMD,2026-01-02 11:25:22,2
AMD,nxa0x45,"Guy needs to clean up his power mess, then wait and see what happens.  Power filters, then wait and see if the situation improves.  If it doesn't, change the PSUs.  If it doesn't, change the motherboards.  If at that point their failure rate is still 10% instead of the average 0.64% for everyone else's 9800X3D, something majorly strange is going on.",AMD,2026-01-02 16:40:26,2
AMD,nx0bsku,Internet cafes aren't rocking 9800x3ds lmao,AMD,2026-01-01 00:58:59,5
AMD,nx0eg0h,150 9800x3ds at internet Cafe. Source: Trust me bro.,AMD,2026-01-01 01:15:58,4
AMD,nx15u7x,"He mentioned he baught cpu trays in bulk, might be engineering samples, very common for people to sell these. That is why we are told to always verify serial numbers while purchasing trays",AMD,2026-01-01 04:20:07,1
AMD,nx2b3o2,"Its because of how they fail, intel cpus will have boot and bluescreen issues,  am4 amd cpus are the same but newer amd cpus just completely die. Failure rate is similar, about 2%. And the reason why icafes use low end parts for the rest of the system is because they are easy to find and buy in bulk and heavily discounted. Costumers only care about cpu and gpu anyway. But these failure rates are nothing compared to the failure rates of gpu fans, those starts with grinding noise and will eventually stop spinning.",AMD,2026-01-01 10:57:30,1
AMD,nx57mje,There are still internet cafes? Thought this was a 200x thing.,AMD,2026-01-01 21:29:23,1
AMD,nx7u6wy,Were they using Asrock murderboards?,AMD,2026-01-02 07:22:44,1
AMD,nxdcle1,"I knew a guy that told me about another guy that he bought 5000Ryzen processors and didn’t have one reject amazing, isn’t it?",AMD,2026-01-03 02:49:34,1
AMD,nxhir9f,100% cooked rate!,AMD,2026-01-03 18:57:18,1
AMD,nx1xxdm,15 out of 15 died in shitty asrock mobos :D,AMD,2026-01-01 08:36:57,1
AMD,nx029s7,are they using those frying asrock motherboards? I had zero problems on my msi (but I also don't skimp on psu) lol,AMD,2026-01-01 00:00:28,0
AMD,nx0zahe,I'm going to guess that cafe used ASRock motherboards.,AMD,2026-01-01 03:34:37,0
AMD,nx7cdco,"AMD has always had stability & problems with their CPU's or chipsets that's why I am sad that Intel are becoming way less competitive, I've always liked the CPU's and platforms more.  even if this is a hoax or real, I believe that Intel and the chipsets are usually more stable & have less failures throughout time it's really only been gen 13/14 that have had serious flaws in recent times.",AMD,2026-01-02 04:59:08,0
AMD,nx7fwbk,"I speculated that the Zen 5 situation could potentially escalate into the Raptor Lake situation, let's see with the development of this one. We didn't label Raptor Lake failures fake news back then, don't we?",AMD,2026-01-02 05:24:28,0
AMD,nxba8js,It’s funny how everything can be at fault for this issue except the cpu itself. At some point reality has to hit and people should consider that the AMD CPUs might have an issue,AMD,2026-01-02 20:12:38,0
AMD,nwzxj5d,Isn't a 10% failure rate pretty normal?,AMD,2025-12-31 23:31:57,-19
AMD,nx0bs4j,* 1- Post on Reddit with a new account. * 2- Make an article on the best website ever about the post. * 3- Make a post on reddit about the article. * 4- Profits!!!,AMD,2026-01-01 00:58:54,147
AMD,nx06vqj,paraphrasing a reddit post isn't journalism,AMD,2026-01-01 00:28:50,110
AMD,nx0mq3w,It's been deleted?,AMD,2026-01-01 02:09:41,15
AMD,nx011ss,Brand new account posted it too.. very weird.,AMD,2025-12-31 23:53:05,48
AMD,nx1aoee,what do you expect... there are people reporting that the rumor that Nvidia is raising the cost of 5090 to 5k. Which I won't believe until it happens. I am sure some major youtuber will report the price hike how horrible it is then at the end go its a rumor.,AMD,2026-01-01 04:57:21,3
AMD,nxc1455,And the AIs will eat it up!,AMD,2026-01-02 22:24:46,1
AMD,nxgdmfw,Stack overflow,AMD,2026-01-03 15:46:58,1
AMD,nx0q60k,Circular reporting is a hell of a drug.,AMD,2026-01-01 02:32:38,42
AMD,nx021n0,Probably the same person who designed Gigabyte PSUs,AMD,2025-12-31 23:59:06,61
AMD,nx0l5lr,"They're an ODM, and not a good one. Out of all of their units that SPL tested, most of them are F tier, and a lot of the units they produced for other brands, also shit.  Basically the only units with their brand name that are actually good, they didn't even make themselves, they got sourced by SeaSonic.",AMD,2026-01-01 01:59:26,51
AMD,nx0jpyj,they're in the SPL Tier list! They have a couple of good units but most of them look like low grade garbage. I doubt he sprung for the good stuff.,AMD,2026-01-01 01:50:04,12
AMD,nwzy26y,I compare it to buying a Ferrari to put 87 octane in the tank,AMD,2025-12-31 23:35:04,49
AMD,nx1cqci,Pretty sure I remember Jonnyguru reviewing one of their PSU's back in the day and it imploded in his testing.,AMD,2026-01-01 05:15:16,10
AMD,nx2w3lh,"Huntkey is one of the largest Chinese PSU OEMs, and obviously not the best one. They make PSUs for desktop PCs and laptops.   For instance, the MSI MAG A750GLS is made by Huntkey, but there several others which are made by them from ""better brands"".",AMD,2026-01-01 14:02:19,7
AMD,nx03s8f,And a $89 cheap motherboard,AMD,2026-01-01 00:09:55,24
AMD,nwzu1o8,cpus (and only cpus) dying doesn't seem like it'd be the failure mode of a bad psu,AMD,2025-12-31 23:10:47,0
AMD,nxba284,Considering it's sourced from a reddit comment it's far more likely to be untrue. The vast majority of stories on this website are false. Especially something as attention-seeking as some rando nobody knows claiming to have a 10% failure rate of 9800X3D CPUs.,AMD,2026-01-02 20:11:47,1
AMD,nx6xn92,cheap Mobo with 9800x3d is not a problem  but bad psu/dirty power are,AMD,2026-01-02 03:23:42,7
AMD,nx2wjnv,"These guys made PSUs for tons of brands, such as MSI, Corsair... they were even in business with Greatwall for a while but not long enough to be decent.",AMD,2026-01-01 14:05:24,0
AMD,nx7kqez,"> Pairing a premium 9800x3d with a CHEAP/Budget $89 motherboard and a knock off power supply   There's nothing relevant that a $400 mainboard does better.   And huntskey is a really big PSU brand, it doesn't matter you never heard of them before.",AMD,2026-01-02 06:02:00,-4
AMD,nx073ga,"Answer: if your competitor down the street has 140 9800X3Ds...These are gaming cafes, not for simply checking email or whatever.",AMD,2026-01-01 00:30:08,20
AMD,nx02hoh,"I'm guessing it's more of a ""PC gaming cafe""? Or more generally a place you can go if you need to rent access to a powerful PC for stuff like video editing? Which... might be a thing? Somewhere in the world? Maybe? To say the article is short on details would be an understatement--best I can tell they're just taken some anonymous reddit comment, slapped it on a web page, and called it a day.",AMD,2026-01-01 00:01:51,3
AMD,nx214mb,"Huntkey is a known (mostly known for making mediocre and cheap stuff) OEM manufacturer and has been around for a while. if anyone remembers BFG, some of their cheaper models of PSUs were OEM'd by Huntkey",AMD,2026-01-01 09:11:17,3
AMD,nx0xdou,More like the one benchmark site that shall not be named.,AMD,2026-01-01 03:21:41,9
AMD,nx2nn1k,I would never buy an AMD. One dies every week for this poor owner. So sad,AMD,2026-01-01 12:58:35,-4
AMD,nx6zjfz,"Because gaming cafes in South Korea, China, etc are extremely competitive. The market is alive there. You’re going to lose your customers if you have a 9600X and the other one down the shop has 9800X3Ds with better 1% lows.",AMD,2026-01-02 03:35:52,0
AMD,nx6zswd,"I wouldn’t be surprised if the mobo and PSU weren’t the root cause of the issues, but rather the mains is just so ridiculously dirty and out of spec. Like delivering 270V at 40-70Hz lol with a big vdroop.",AMD,2026-01-02 03:37:31,1
AMD,nx0678s,"Yeah even the local biggest eshop in my country has 10000+ sold with around 1% RMA rate, when industry standard is sub 3%  https://m.alza.sk/EN/amd-ryzen-7-9800x3d-d12671823.htm",AMD,2026-01-01 00:24:41,8
AMD,nxhu1z5,Doing exactly what they accused Intel fanboys during the Raptor Lake issues,AMD,2026-01-03 19:50:01,0
AMD,nwzymj8,"It isn’t. Even at the peak of 13th to 14th gen catastrophe, I don’t think it reached a 10% failure rate.  That being said, this is also a terribly small sample, so don’t know how relevant it is",AMD,2025-12-31 23:38:26,11
AMD,nx009rr,"No, immo it's 0.64% for this cpu (https://www.mindfactory.de/product_info.php/AMD-Ryzen-7-9800X3D-8x-4-70GHz-So-AM5-WOF_1595711.html)",AMD,2025-12-31 23:48:24,7
AMD,nx1dx0h,And it’s probably paraphrased with AI too.,AMD,2026-01-01 05:25:11,29
AMD,nx20b75,"I hate to break it to reddit, but to the rest of the world, it is lol. It's like saying people posting on Twitter ain't news, but it sometimes is the direct source of news. Twitter users just don't see it that way out of normality.  Reddit at least reports good research and typically has data to back up claims from time to time.",AMD,2026-01-01 09:02:23,7
AMD,nx4ske1,Copy pasting a link to an article is a low-effort post.,AMD,2026-01-01 20:10:34,1
AMD,nxj63px,We are a society of middlemen,AMD,2026-01-03 23:50:51,1
AMD,nx2t9zj,"The whole thing might be a hoax? The cafe photo used by videocardz matches facebook photos of Kasirga Internet Cafe in Turkiye, but the place doesn't seem large enough to be hosting 150 seats (or high-end enough to be using 9800X3Ds).  Not sure if facebook links are allowed so I'll put the link in a reply to this comment.  Edit: so yeah my post with the link got deleted by AutoMod. Anyway it's @KasirgaInternet on FB.",AMD,2026-01-01 13:42:16,18
AMD,nx0tb4k,comments seem to agree the issue is either dirty power or cheap PSUs + overvolted RAM,AMD,2026-01-01 02:53:55,24
AMD,nx96vc0,"""This is Rich of ReviewTechUSA"" 🤣",AMD,2026-01-02 14:07:41,5
AMD,nx18jee,Probably an intern who watched the Gigabyte PSU guy and did his own.,AMD,2026-01-01 04:40:34,11
AMD,nx06x47,I replaced the engine with vtec bro its just as good,AMD,2026-01-01 00:29:04,25
AMD,nx30wfy,Doesn't gas at least have fairly strict quality standards?,AMD,2026-01-01 14:34:58,1
AMD,nx7ukzm,Buying a Tesla and putting a 1500cc 4 cylinder engine in it.,AMD,2026-01-02 07:26:20,1
AMD,nx6wsyd,"A brand can make great PSUs and improvised home explosives across their range.   For example, Great Wall is behind some of the dodgiest no-name explosive PSUs, and some of the best PSUs for workstations and servers. Just like how a butcher sells different grades of meat.",AMD,2026-01-02 03:18:19,2
AMD,nx2hj17,"That is... perfectly fine, if it isn't then it should be, because it's always been fine.",AMD,2026-01-01 12:03:15,3
AMD,nwzviyx,"What do you mean by this?  Unstable voltage and surges will 100% kill a cpu in the blink of an eye.1.25 volts is great, 1.35 volts can completely destroy the 9800x3d. 0.1 volts of fluctuation is completely expected from a cheap temu shitty power supply. The only place that thing belongs is in the trash, even brand new.",AMD,2025-12-31 23:19:46,48
AMD,nx04t8i,"A PSU should always die first, otherwise it didn't do a good job protecting the hardware.",AMD,2026-01-01 00:16:15,6
AMD,nx6ykem,"Yeah and you get what you pay for. Plenty of low end MSI, Corsair PSUs are not good.   It’s all about the tier.",AMD,2026-01-02 03:29:36,5
AMD,nx914kp,There are differences between a high end motherboard and a low end MB.  There are differences between a high end PSU and a low end PSU.  Suggesting otherwise is ignorant.,AMD,2026-01-02 13:33:26,3
AMD,nx0x7lr,It still seems unbelievable that someone would drop like 60-70 grand on CPUs and pair it with nearly fire-starter tier PSUs and the cheapest boards you can probably find with wifi support.,AMD,2026-01-01 03:20:33,29
AMD,nx0bx65,Gaming cafes don't use these types of high end hardware lmao,AMD,2026-01-01 00:59:47,10
AMD,nx04042,"Like i said i read the og post before i saw that they wrote an article on it ... there was barely any info or context from the OG poster (the account was new too). Publishing a article about something like this is at best hear say... not journalism.  Anyways, happy new year!",AMD,2026-01-01 00:11:16,13
AMD,nx6ze4j,"Many people in China don’t have their own computer, just a smartphone, and game in internet cafes which can be quite high end — clean, comfy, food and beverage options, and also not a bad social hangout spot.",AMD,2026-01-02 03:34:54,2
AMD,nx0au58,"yeah, esports cafes and gaming centers are definitely a real thing.  edit: [found a wikipedia page on the korean scene](https://en.wikipedia.org/wiki/PC_bang)",AMD,2026-01-01 00:52:59,2
AMD,nx02rp3,Those arent things.,AMD,2026-01-01 00:03:35,-10
AMD,nx1sghh,"Literally any mid range system will get you way beyond 144hz in competitive games at 1080p. I can't think of a single popular competitive title that's actually demanding on hardware, especially in 1080p.",AMD,2026-01-01 07:39:23,3
AMD,nx52699,"It is ok to use reddit as *a* source, but if you use reddit as the *only* source and don't bother to crosscheck with other sources (like Mindfactory) then that is hardly journalism  In this case Mindfactory numbers show the following RMA rates:  * 9800X3D 0.64% * 9950X3D 0.30% * 285K 0.34%  and for comparison from last generation:  * 7800X3D 0.49% * 14900K 4.63% * 13900K 2.61%  So this report is off by more than an order of magnitude, which should raise eyebrows.",AMD,2026-01-01 21:00:52,36
AMD,nxa09gl,"Reddit doesn't report anything, its just links to the actual reports.",AMD,2026-01-02 16:37:21,2
AMD,nx4xa0v,"It wasn’t a hoax, the OP got fucking cooked in the comments because when asked what PSU he used, it was an F-tier from a brand nobody’s ever heard of, and it became obvious the PSUs were killing his CPUs.",AMD,2026-01-01 20:34:47,20
AMD,nx6q3wn,"Yeah, in order to fit the analogy; it should be something like buying a ferrari then digging up dirt and putting it into your gas tank and calling it fossil fuel.   Its trash tier fuel that will actively harm the car.   Just like an F tier PSU will actively harm the PC.  Octane has nothing to do with gas quality (can have crappy 93 octane or race gas with contamination and poor additives). Octane only measures one thing: Resistance to ignition when compressed. That's it.",AMD,2026-01-02 02:37:05,2
AMD,nx2l4qj,If only said board wouldn't overheat the VRMs with a 7500f.,AMD,2026-01-01 12:36:53,7
AMD,nx5d1fg,I remember a few cases of motherboards messing up CPU voltages due to some weird auto-overclock fuckery over the years. Not sure how common it is but I would still prefer more data from different motherboards.,AMD,2026-01-01 21:57:04,1
AMD,nwzwc28,"unstable 12V wouldn't show that much on cpu voltages as it has to go through the VRMs, and if it did other stuff on the 12V rail would likely die too.   From a PSU I'd also more expect voltages to fall too much instead of overshooting",AMD,2025-12-31 23:24:39,36
AMD,nx07aaa,"It’s the motherboards job to maintain the precise voltage needs of a CPU. Think of it like this: the PSU supplies your water main to your house, but the individual faucets (VRM’s) determine how much water comes out of your fixtures into your sink (CPU in this case).   Those VRMs are capable of handling the power spikes a PSU could create, unless there is a catastrophic failure of the PSU. Even during a catastrophic PSU failure the VRMs have MOSFETS as well as chokes and capacitors that have protections in place for over current, they would trip before the power fried the CPU, unless it was like a lightning strike or something.",AMD,2026-01-01 00:31:17,18
AMD,nwzxuj9,I’ve got mine on -20 on my Curve Optimizer so I keep as much voltage off it as possible!,AMD,2025-12-31 23:33:50,7
AMD,nx1vap9,That's entirely dependent on the issue.,AMD,2026-01-01 08:09:06,3
AMD,nx6ybja,Absolutely not. In many cases PSUs are supposed to die first. In other cases it is safer and reliable for the mobo to die first.  What is correct is the CPU should be the LAST to die.,AMD,2026-01-02 03:28:01,1
AMD,nx9c9jp,"Differences, of course. But note that I said nothing relevant, all of them can be expected to be very well 'good enough' if you keep it within spec. Nobody is going to do LN2 overclocking on a budget board, and nobody would expect that to work too terribly well to begin with.  There have been plenty of PSU vendors/models that are fairly described as ""chinacrackers"", but a PSU being low end doesn't make it a chinacracker in itself.",AMD,2026-01-02 14:38:32,0
AMD,nxb3rd1,"The motherboard differences are irrelevant to a CPU's lifespan.  The failure rate of a CPU on a bottom of the barrel motherboard, the ones even cheaper than what consumers can buy and what OEMs use, is < 0.1%/year.  If CPUs are failing at even a tiny fraction the issue is the motherboard is not in spec or the CPUs would have died in any motherboard.",AMD,2026-01-02 19:41:19,0
AMD,nx30uhw,Internet cafe/ mining operation? Bot farm?,AMD,2026-01-01 14:34:36,3
AMD,nx6z48z,"Umm yes they do. Gaming cafe culture is very different in the east, it’s still alive, it’s still a place to hang out, and the cafes are very competitive in terms of hardware, service, food and beverage, price, etc.  Many people don’t have a computer and just have their phone as their primary computing device, and game in internet cafes.",AMD,2026-01-02 03:33:08,1
AMD,nx0eneo,they do in the East,AMD,2026-01-01 01:17:19,1
AMD,nx0andl,">Those arent things.  confidently incorrect.  board game cafes, internet cafes, esports cafes, these have all been a thing for years. if you can think of an activity theres a place that'll serve you drinks while you do it.",AMD,2026-01-01 00:51:48,4
AMD,nx5li7f,thanks for the information,AMD,2026-01-01 22:41:40,4
AMD,nxav07l,You're joking right? Sometimes redditors will write a whole thesis for a game for their own niche communities lol,AMD,2026-01-02 18:59:39,1
AMD,nxjnkvj,That would've been my 1st guess.  2nd would be terrible wiring or other heavy equipment in/around establishment causing heavy ripple in the electrical input... although GOOD power supplies should mitigate that somewhat or take themselves out without destroying the electronics to which they are feeding power.  3rd Shitty cheap mobos overvolting said CPUs.,AMD,2026-01-04 01:24:18,2
AMD,nx2tafx,"Your post has been removed because the site you submitted has been blacklisted, likely because this site is known for spam (including blog spam), content theft or is otherwise inappropriate, such as containing porn or soliciting sales. If your post contains original content, please message the moderators for approval.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-01-01 13:42:22,0
AMD,nx3czd5,That wouldn't kill the CPU,AMD,2026-01-01 15:47:30,-1
AMD,nx5zq74,That happened more on expensive boards not cheap ones lol,AMD,2026-01-02 00:00:58,3
AMD,nwzywzb,"Hmm yeah maybe you're right; the VRMs would overheat and die first before it bricks a cpu. But he would have also seen a lot more blue screens and crashes before the VRMs and mosfets kicked out. He doesn't report any of that, especially across so many failed CPUs. A mosfet failure on a cheap board, which I don't consider the AYW boards to be, would cause voltage overshooting to compensate though and would instantly fry a cpu.   I don't know I'm still skeptical of the entire story with a huge lack of proof from the OP that he has an internet cafe with 150 PCs rocking that chip.",AMD,2025-12-31 23:40:12,6
AMD,nx6xhe6,"VRMs operate within certain tolerances and OP paired this with an entry level motherboard.   It’s highly likely the problem is a result of all 3 factors:  • Unstable power delivery from the wall (I bet it’s cheap electrical work, or extension boards and extension boards), AND  • Dodgy low end PSU, AND  • Low end motherboard.   At no point would I suspect the 9800X3D, although IF the batch numbers and production runs are the same, it’s rare but possible to be a bad batch. Not impossible.   I believe gamers nexus quoted a source at a hyperscaler that says for every 100 CPUs, 0-2 tend to fail their (extensive) validation and gets replaced by AMD.",AMD,2026-01-02 03:22:39,1
AMD,nx0b0xv,Yeah. The mobo should in theory die before the CPU does. This is what happened to mines a week ago.,AMD,2026-01-01 00:54:10,6
AMD,nx6y5o0,"Do you have core shaper? I’ve been able to get to -27 on some cores, with more than a week of stability testing, thanks to not being as aggressive with the curve on the lower utilisation end.",AMD,2026-01-02 03:26:58,2
AMD,nwzzhkt,Nah man I keep mine cranked all the way so I have an excuse to upgrade once it dies haha,AMD,2025-12-31 23:43:39,4
AMD,nx8gr9k,Im assuming it's from external sources through the power cord - like a surge. If that hits the motherboard or CPU then the PSU didn't do it's job      If it's an internal short circuit that's ofc a different matter,AMD,2026-01-02 10:55:31,1
AMD,nxb2sy9,"I would argue that: ""Power delivery"" High-quality chokes, MOSFETs, and heatsinks ensure clean, cool power for stable CPU performance, crucial for overclocking and longevity ARE relevant.",AMD,2026-01-02 19:36:43,1
AMD,nx330j9,"Maybe, but still seems dumb as hell buying a lot of really sketchy PSUs. The ridiculous ways people approach electricity though it's no wonder places running halfbaked setups show up in the news now and then as cinders, ash, and rubble.",AMD,2026-01-01 14:48:42,2
AMD,nx0fi4h,"Not really. For example PC bangs in Korea use -60 to -70 level GPUs and intel 13/14th gen cpus from the -400k to -700k range. There was even a story last year about how these gaming centers were complaining of high rma rates for these cpus. If they switched over to AMD they would most certainly be using 7600x-9700x type cpus, maybe they'd have a higher tier of pc with an x3d processor but it wouldn't be 150 of them, it'd be like 10-20.    Here I'll give an [example from Taipei](https://www.lhhcybercafe.com/about.html)  They have 193 pcs, it says the cpus involved are 65 r7 9700s, 20 i9 9900ks, 108 i7 8700s. Only 5 of them have 5080s, the rest are older or weaker cards.",AMD,2026-01-01 01:22:48,17
AMD,nx12zwl,Yep totally not real: [https://www.youtube.com/watch?v=veLSexqLtYo&pp=ygUVa29yZWFuIGludGVybmV0IGNhZmVz](https://www.youtube.com/watch?v=veLSexqLtYo&pp=ygUVa29yZWFuIGludGVybmV0IGNhZmVz)  [https://www.youtube.com/watch?v=PaKEkH0YJUU&pp=ygUVa29yZWFuIGludGVybmV0IGNhZmVz](https://www.youtube.com/watch?v=PaKEkH0YJUU&pp=ygUVa29yZWFuIGludGVybmV0IGNhZmVz),AMD,2026-01-01 03:59:27,2
AMD,nx3na46,"You make a lot of long shot assumptions here in defense of this post. It just doesn't make much business sense to have such expensive hardware in an Internet cafe, especially in most Asian countries where Hardware prices are often a lot higher than the US or Europe and most of your core customer base won't even notice the difference between 160hz and 240 or 360.  As others on here have pointed out anyways the source for this ""news"" is just a reddit post so I agree with the guys on here that are sceptical of the veracity of this post.",AMD,2026-01-01 16:42:46,2
AMD,nxaymgl,How is that reporting news? Like the fuck are you saying to me right now.,AMD,2026-01-02 19:16:39,2
AMD,nx3dmj4,"No, but the used PSU with +/-1V swings will do that for you",AMD,2026-01-01 15:51:01,5
AMD,nx6wcyp,Yeah there’s been plenty of enthusiast motherboards with stories of frying CPUs for decades of PC building I remember. The key is they usually have good support/RMA since you pay a premium for the board; or at least from my experience.  I’ve had a few “one time goodwill” repairs when I fried a chip or VRMs; was upfront and honest and offered to pay for repairs both times.,AMD,2026-01-02 03:15:31,1
AMD,nx08t4q,"> I don't know I'm still skeptical of the entire story with a huge lack of proof from the OP that he has an internet cafe with 150 PCs rocking that chip.  Forget that, the mere idea 150 of those PSU going sounds relatively terrifying.",AMD,2026-01-01 00:40:28,6
AMD,nx6xzt0,"In theory. The problem is if the OP is going for top-end hardware on rock bottom PSU and mobos, it’s a reasonable assumption they probably got the cheapest guy to do their electrical wiring.   I wouldn’t be surprised if there’s huge voltage drops, or the mains is delivering 260V instead of the typical (acceptable tolerance) 200-240V for China’s 220.",AMD,2026-01-02 03:25:55,1
AMD,nx6yyl8,"I could dig into it more, but I got it low enough and haven’t crashed that I’m pretty happy.  My super tweaking days are behind me.",AMD,2026-01-02 03:32:08,1
AMD,nxbwrar,Relevant for overclocking? Sure.   Relevant for running 150 systems as cheaply as possible to get them working reliably? Most certainly not. Even the cheap boards run the stuff they are built for just fine. And like the burnt sockets have shown even expensive boards can blow up your system perfectly fine.,AMD,2026-01-02 22:02:37,1
AMD,nx6ynyz,We don’t even know how his electrical is wired. It’s probably done the cheapest way possible and the wiring is probably a fire waiting to happen.  Your mains power is important. Hire licensed electricians with good reputation / reviews; don’t get your mates’ mate to do a cashie.,AMD,2026-01-02 03:30:14,2
AMD,nx1yt22,"i9 9900K once upon a time was a top tier gaming chip. So it depends when they bought those systems. But I will tell you this depending on the region where OP is, Asian CS2 players probably want an X3D system for maximum FPS when playing, it's the most optimal chip for that game. Also this cafe could be new and trying to set themselves apart from the others in the region by giving you higher end systems all round.  As for GPUs, yes I agree that generally Internet Cafes and PC Bangs generally use 60 and 70 series cards for cost and scale. I mean the most popular games played at these places are LoL, CoD, PUBG, CS2 etc thats more than sufficient hardware. But the guy in the post never mentioned GPU anyways and its not a factor in this discussion really.  That being said I actually think the guy in the original post is telling the truth for two reasons:  1. The board reportedly used is NOT ASRock, where most issues occur. This tells me that they're probably not lying as it would be far more believeable to lie and pretend it was an ASRock board rather than an ASUS board and no one would question it.  2. He told us the exact BIOS version, kind of weird to talk about BIOS version especially an old one if you were lying, if someone were trying to astroturf some kind of issue with AMD they would probably say ""I updated the BIOS' to the latest and still had issues!"".  The fact they're not using usual suspect information makes this a far more believeable case IMO.",AMD,2026-01-01 08:46:18,4
AMD,nxazsc4,"It is reporting data, which is a form of news. A thesis is the basis of research.  Here is an [example](https://www.reddit.com/r/GlobalOffensive/comments/1oiq4a7/networking_architecture_cs2_vs_csgo/). This is the original source of data that a news article could write about. You're not gonna find this anywhere else on the internet. Don't say redditors never report anything when there's some real autistic fucks on this website.  These kinds of things happen all the time across all communities. Does that fuck make more sense now?",AMD,2026-01-02 19:22:12,1
AMD,nx6w0y4,"A combination of a dodgy, Temu-quality PSU as well as a low end motherboard.   If you have good VRMs it should smooth out PSU fluctuations at least to the point of not burning the chip.  If you have great PSU, you’re okay with a lower end VRM.  Go bargain basement for both and you have a problem.",AMD,2026-01-02 03:13:24,5
AMD,nx3c2n1,"These gaming cafes aren't rocking 500 hz monitors bro. Go look up the benchmarks, even modest old cpus can max out a 144hz monitor easily",AMD,2026-01-01 15:42:30,1
AMD,nxj6dmc,"But that's not reporting news. And if it isn't news, it isn't news.",AMD,2026-01-03 23:52:17,2
AMD,nxb21js,No one was talking about some autistic kids and their DnD subreddit. We were talking about News.,AMD,2026-01-02 19:33:05,0
AMD,nx3o98r,"Where in any of what I said did I say they're using 500Hz monitors? No where. You just made up some phantom argument in your head.  People just want a consistent 240 FPS in CS2 when they play it at PC Bangs and Netcafes. CS2 can dip pretty hard at times and really only a 7800X3D or 9800X3D gives consistent performance across all maps and scenarios or really any X3D CPU. Thats incredily important too just for overall smoothness and is vital for hitting bhops, flicks etc.  [Look here at 1080p with a 7700 or 9700X how hard it dips once you start sitting in smokes, we're easily below 240 FPS at points.](https://youtu.be/DcM8bKHHB8E?t=159) Mind you this is Dust2 which isn't even the mot demanding map in CS2, something like Ancient or Train is more demanding, you lose like 60-150 FPS on those maps compared to Dust2 depending on the scenario.  Before you say it. Yes, of course the ultra high tier tryhard eSports guys will play with all low settings, 4:3 res like 800x600, but most people going to a PC Bang will rock with native res and high settings or whatever the game defaults to with their system which will be definitely be native res and medium/high settings.",AMD,2026-01-01 16:47:57,0
AMD,nxkkzbj,"We're not even talking about news. Buddy originally said ""reddit doesn't report anything"".",AMD,2026-01-04 04:37:08,0
AMD,nxbcfnf,"That is news. The News is just new information on whatever. People report their own findings and research all the time to reddit, which you claim doesn't exist. What do you classify as news?",AMD,2026-01-02 20:23:27,2
AMD,nxi9u1m,"Why not? The 9070 and 9070 XT are very good products.  The base 9070 is just straight up better than the base 5070 for the same price. The 9070XT is significantly cheaper than the 5070TI. It competes well from a price/performance  perspective. Although, the 5070ti is of course more powerful.",AMD,2026-01-03 21:08:12,40
AMD,nxhe52y,I mean they've been aiming to capture that in the rest of the world for god knows how long and never done it. Do the Chinese like inferior products at $50 less?,AMD,2026-01-03 18:36:28,17
AMD,nxoheh5,For that you need to you know ... actually produce gpus,AMD,2026-01-04 19:34:25,1
AMD,nxq95t8,the problem with amd is Valorant a very popular game has a shader issue that causes stutters in game  this is an amd issue not encoutering it on nvidia gpus.,AMD,2026-01-05 00:36:39,1
AMD,nxl1esz,Also AMD about to raise prices come on AMD you cannot do both.,AMD,2026-01-04 06:36:33,0
AMD,nxlkjys,"The only thing stopping them is the inconsistency of their own driver. Currently they have issues with flipping/vsync for a lot of games under 25H2, and I’ve seen issues I wouldn’t even have to consider with NVIDIA.   They’re not a plucky underdog anymore, so expecting their stuff to work without hours of figuring out settings for popular titles on high refresh displays, should not be a thing.",AMD,2026-01-04 09:26:03,-5
AMD,nxlix1d,"I think key part is that 5070ti isn't more powerful enough to consider it a no brain choice. It's better, yes, but kinda close to 9070xt, so you have to look at price tag. Previously, for lots of people it was a big difference in, let's say, ray tracing and Dlss, now it's path tracing and only if price is 50$ or less",AMD,2026-01-04 09:11:10,16
AMD,nxrlmjp,"It really depends on the efficiency is whatever ai stuff they need or whatever coin they can mine. Gamers can make do with anything, the big players could and would pay for whatever they need to increase electricity efficiency.  There were pictures circulating of crypto mining in abandoned office units during the pandemic. 3060s in mining rigs working away along with mobile 3060s. It's absolutely bonkers, when the pandemic broke, lots of these mining cards ended up on eBay.",AMD,2026-01-05 05:04:30,1
AMD,nxi4qny,"Well never is not true, they used to have around that about a decade ago.    But nvidia is partly banned there so I guess they're going for the ease of access play.   Weird though considering China just hit a massive improvement in GPU performance and Huawei is reportedly about to jump to 2nm. Wouldn't be surprised to see them eventually ban American cards entirely.",AMD,2026-01-03 20:42:59,11
AMD,nxki8ws,>$200 less  FTFY,AMD,2026-01-04 04:19:17,2
AMD,nxhqesr,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-01-03 19:32:36,1
AMD,nxlmufw,"The last 12 months of nvidia desktop GPU drivers have been a rolling garbage fire that brought it's children to work. They were not even trying when blackwell launched.  While that is abnormally bad, Nvidia drivers being flawless and reliable is pure myth and always has been. They have random bugs and compatibility regressions as much as any similar company, and have left some severe issues sitting unfixed for multiple hardware gens in the past, like broken dithering across all of Maxwell 1/2 and into Pascal.  From what i have seen in long tech support experience, it mostly comes down to preconception; any problems on an AMD GPU are always because AMD is buggy. Problems on a Nvidia GPU are because the game/OS/weather/AMD is buggy. Timmy User does not diagnose any further than that.  The TL;DR is we will never see a perfectly reliable graphics driver in the wild, and anyone claiming to have photographic evidence of such is highly suspicious and probably trying to sell you something.",AMD,2026-01-04 09:47:14,14
AMD,nxn5dgo,Even rt isnt worth for most. A lot of games dont even suppirt it or run vert poorly,AMD,2026-01-04 15:58:05,4
AMD,nxjikgj,"Where on Earth did you get Huawei is going to jump to 2nm?  It wasn't because of the ""prototype lithography"" machine they cobbled together from spare ASML parts, was it?",AMD,2026-01-04 00:56:50,13
AMD,nxlglpd,Nvidia is “partially banned there” as much as amd is. MI308. The only Nvidia card “banned there” in the consumer segment has no amd equivalent in any case.,AMD,2026-01-04 08:50:09,5
AMD,nxir7u7,"why would they do that? everyone will flock to domestic if it's better. they want competition to drum up their domestic options, if they don't, they will never be better. EV industry in China is super competitive, and they have not banned Tesla, they've surpassed Tesla in many ways.",AMD,2026-01-03 22:33:45,2
AMD,nxlhw4u,What products are you comparing specifically?,AMD,2026-01-04 09:01:50,2
AMD,nxlwhmd,"Do you even own a recent NVIDIA card? I setup a 5070Ti for my brother and it was basically effortless. My 3090 was the same.   I spent two hours yesterday trying to make games not stutter with high refresh rate VRR displays.  It’s not even close, stop giving them excuses, they’re not struggling.",AMD,2026-01-04 11:13:05,-9
AMD,nxpumpb,"I refuse to play cyberpunk without path tracing, thus amd is completely out of the picture, in games like star citizen 9070xt performance is literally half of 5070ti so idk",AMD,2026-01-04 23:25:09,0
AMD,nxmjk3x,"It's easy to fall for China's bullshit propaganda when you're unaware of how EUV machines are practically black magic assembled with perfection in an artisanal manner. Making rough copies is one thing, but precision engineering really isn't China's strength - they only figured out ball point pens in 2017.",AMD,2026-01-04 14:02:11,7
AMD,nxiu7dj,"Security, get away from Palantir   They already banned it for government devices.",AMD,2026-01-03 22:48:49,5
AMD,nxpohun,"I dont own any recent desktop card, and personal anecdotes are worthless samples. I do pretty broad tech support and pay attention to development circles, it is pretty easy to stay aware of and encounter every standing issue, and encounter a lot of much more niche bugs.  It has not been niche. Nvidia's drivers over 2024 were so terrible it has actually percolated into the awareness of the PCRM-type kids. Stuff that actually impacts normal users, not situational high-end issues.",AMD,2026-01-04 22:54:38,1
AMD,nxqq5tv,"On the other hand, Monster Hunter Wilds AMD smokes Nvidia like 9070xt can keep up with 4090. You will always find some games optimize more on one side.",AMD,2026-01-05 02:06:31,4
AMD,nxr9crn,"They're essentially equivalent in most popular titles, this isn't really a debate.  Multiple third party reviewers have already tested games",AMD,2026-01-05 03:51:47,1
AMD,nxqc9m0,"I’m talking about specific stuff here, for which I never get specific answers.  AMD has issues with vsync/flip with high refresh rate/hdr/vrr displays under 25H2.  With NVIDIA and the same class of GPUs there were no issues, WITH THE SAME TITLES.  Riddle me the settings I need not to have stutter or magically halved performance for Oblivion Remastered, Silent Hill 2 Remake and Mortal Kombat 1.  Each one of them has a different issue, and with each one of them I need to do a different workaround.  Oblivion needs to be run in full screen with AntiLag on, but will lose performance if I make the terrible mistake of alt tabbing.   Silent Hill only runs at full performance in Windowed mode. Borderless and Full screen cause the frame rate and GPU utilisation to halve. No internal or via the driver setting for vsync, esync, any sort of frame limiting, full screen optimisation, performance profile, queue management (AntiLag), does anything.   The only way for it to run is in Windowed mode, and that means no HDR, only Windows AutoHDR. In Windowed mode, the in game frame generation causes horrible stuttering, and the AFMF will crash the game after a while, so no frame generation and no proper HDR, and a ton of time to figure all this out, while people here tell me to format or the usual cope de jure.  In Mortal Kombat, I need to run the game in Borderless with Enhanced Sync on, and the game movies still stutter sometimes. At least it took me only 20 minutes to figure out.   With NVIDIA I had Low Latency set to Ultra and Vsync set to On, globally and 99% of games (including these, in the same system with 25H2, before formatting it to install the 9070XT), just worked. The maximum thing I ever did was disable the in game vsync.  Also, I’ve been doing tech support for twenty years, so there’s that. No driver is perfect, but on the specific issue of playing games optimally under the latest version of Windows, AMD is a roulette, and it’s starting to feel it’s not worth the time.",AMD,2026-01-05 00:52:23,1
AMD,nw17tf6,"Hey, first time running this benchmark, or any 3DMark stuff, but this is what I got from it.  [https://www.3dmark.com/3dm/148400061](https://www.3dmark.com/3dm/148400061)  >Difference: +14.7%  >RX 9070  >Adrenalin 25.12.1",AMD,2025-12-26 15:14:00,3
AMD,nw3h0bf,I've got an old run here on RDNA2 from 2021:  https://www.3dmark.com/sf/19382  >Difference: +4.7%  >6900XT  >Adrenalin 21.10.2,AMD,2025-12-26 22:30:40,2
AMD,nw65v42,[https://www.3dmark.com/sf/149812](https://www.3dmark.com/sf/149812)  AMD Radeon RX 7900 XTX(1x) and AMD Ryzen 9 9950X3D  Sampler Feedback off 840.74 FPS  Sampler Feedback on 777.98 FPS  Difference-7.5 %  Driver version32.0.22029.9039   Adrenalin 25.12.1,AMD,2025-12-27 10:24:25,2
AMD,nwkljib,"https://www.3dmark.com/3dm/148655489  Difference +13,5 %  RX 9070 XT  Adrenalin 25.12.1",AMD,2025-12-29 16:53:21,2
AMD,nwiugt9,Thank you! Interesting,AMD,2025-12-29 10:19:44,2
AMD,nwiublg,Thanks for sharing it! Could you share a recent one? It doesn't matter if it's with a different GPU. All setups are welcome for the research.,AMD,2025-12-29 10:18:23,1
AMD,nwitvsk,"Thank you! It's curious that, so far, only the shared results from RX 7900 XTX users have notable negative performance differences in this test, indicating significantly worse performance when using the Sampler Feedback feature ...  Mine is from the post example (RX 7900 XTX, Adrenalin 25.11.1):  [https://www.3dmark.com/sf/149548](https://www.3dmark.com/sf/149548)  Difference: **-7.9%**",AMD,2025-12-29 10:14:17,2
AMD,nwmuvpk,Just reran it on my 9070XT:  https://www.3dmark.com/3dm/148680595?  >Difference: +13.1%  >9070 XT  >Adrenalin 25.12.1,AMD,2025-12-29 23:28:44,2
AMD,nwpnwnp,"Thank you. Yes, RDNA4 shows significant positive performance differences. The only users experiencing negative results thus far are those with RDNA3 boards (RX 7900 XTX). The problem appears to be located there.",AMD,2025-12-30 11:09:33,1
AMD,nx1ocju,*croaks* back in my day you couldn’t even overclock x3d chips!,AMD,2026-01-01 06:58:29,47
AMD,nx4x31f,Finally a CPU with Tarkov can run steady 60 FPS.. /s,AMD,2026-01-01 20:33:46,10
AMD,nx3zxca,why it doesn't show AVX 512?,AMD,2026-01-01 17:48:47,2
AMD,nxh2nzn,"If the 9850x3d has similar frequencies and the infinity fabric remains the same, it definitely isn't worth it.What is needed is a lower core and soc voltage.",AMD,2026-01-03 17:45:00,0
AMD,nx1xghr,Which is not even that long ago,AMD,2026-01-01 08:31:58,14
AMD,nx4tgxw,"You could through BCLK overclocking. It wasn't worth it though as it needed an external clock generator (not using one would fuck up the clocks of all the other stuff such as the infinity fabric) and motherboards with one are crazy expensive. Also, any gains were minimal as you were still limited by tjmax).",AMD,2026-01-01 20:15:09,2
AMD,nw0h938,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-26 12:07:50,1
AMD,nw0qwpn,This will actually run Windows 98 without additional expensive DDR5 ram :),AMD,2025-12-26 13:25:41,368
AMD,nw0j156,I really wish people would benchmark modern use cases like having your game on one monitor and on the second monitor a browser open with 12 tabs one of which a video you are looking at in between stuff ingame. Then possibly also being connected to voice chat in discord the whole time? I can’t be the only one that has this scenario a lot of the time.,AMD,2025-12-26 12:23:35,391
AMD,nw0im8q,Will this be faster and more performant than 9800X3D in gaming at 0.1 & 1% lows?  192MB of L3 cache is MONSTER,AMD,2025-12-26 12:19:56,71
AMD,nw0tcol,If X3D is so good where’s X3D2??,AMD,2025-12-26 13:42:38,82
AMD,nw0ikib,Except for very rare workloads I still don't see the benefit. If you only game then it will be zero difference there.   Most production workloads benefits more from the higher clocks.,AMD,2025-12-26 12:19:31,26
AMD,nw0lkej,"I am not sure of the purpose of this CPU, Infinity Fabric isn't facts enough to not cause stuttering in latency sensitive operations (gaming being one of them).   Is it meant for other use cases, i would guess?",AMD,2025-12-26 12:45:10,17
AMD,nw20d9x,"I don't see how this symmetrical design will improve performance in a meaningful way, given the interCCD latency penalty still exists.",AMD,2025-12-26 17:47:32,6
AMD,nw15mvp,I'm really excited about the price in comparison with the regular x3D model.,AMD,2025-12-26 15:01:11,3
AMD,nw5s3md,Of course they waited until we're right in the middle of a RAM apocalypse to release it.,AMD,2025-12-27 08:09:28,3
AMD,nw0hlmt,Took them long enough,AMD,2025-12-26 12:10:58,10
AMD,nw1pst2,Well now… let’s see how the 9950X3D2 Electric Boogaloo compares to Core Ultra 5 245K.,AMD,2025-12-26 16:51:37,6
AMD,nw6yega,"Weird product. Pointless for 99% of games as inter CCD latency will still ruin performance when using more than 8 cores (so you'd still want to make sure games stay on one CCD... just like the 9950X3D now).  The number of games that scale to more than 8c/16t can be counted on one hand, pretty much. For me it's just M&B Bannerlord where I saw small but consistent gains by using both CCDs of a 9950X3D (but since I'm fully GPU bound with my regular settings on 1 CCD already... it hardly matters!).  Might be a nice product for professional apps that need cache, though.",AMD,2025-12-27 14:15:13,4
AMD,nw27a35,"Finally, something that might be able to run cities skylines ii",AMD,2025-12-26 18:23:21,2
AMD,nw0jlq4,Would this mean that each CCD has cache directly attached now?,AMD,2025-12-26 12:28:36,3
AMD,nw10ueq,"Of only i got some ram a few months ago... Though about getting the next generation. Then there was a sale on 9800x3d and i almost jumped on it.... Bah just my luck in all things in life, late to the party.",AMD,2025-12-26 14:31:43,2
AMD,nwb8unc,"What's the point of this CPU, isn't Zen 6 around the corner?",AMD,2025-12-28 04:28:59,1
AMD,nwhgwdm,Still waiting for more than 24 pcie lanes on desktop,AMD,2025-12-29 03:39:59,1
AMD,nwpm5nv,"The only major upside is that it kinda declares they can do X3D without substantial clock speed hit now, so maybe when Zen 6 launches, we get full X3D dual CCD models out of the gate.   And I guess this might be nice if you still use very old system and are looking to upgrade right now - even if RAM prices say ""no you don't"".   But as someone with 9800X3D (because 9950X3D was still too janky and the use is 90% gaming anyway), this is kinda too little too late. Except as a proof-of-concept that hopefully gets repeated next generation.",AMD,2025-12-30 10:53:56,1
AMD,nwt4h47,I waited this time for the x3D version only to be punished as they release the X3D2 version 😑 i guess nextgen they will release the x3D version right away then a x3D2 followed by x3D3 and this their future plan against Intel when they catch up.,AMD,2025-12-30 22:05:03,1
AMD,nw0wuy8,How much better do think X3D2 will be over X3D for MMO’s?,AMD,2025-12-26 14:06:03,1
AMD,nw2i7xw,Is this worth upgrading from a 9950x3d?,AMD,2025-12-26 19:20:24,0
AMD,nw2w4ll,Time to sell my 9950x3d for 1% better performance.  Isn't that what CPU buyers do these days!?,AMD,2025-12-26 20:36:25,0
AMD,nw5ojai,x3d2 is such a stupid name.  anyone else remember that one time a company fucked up and listed a 'ryzen x4d'? use that instead.,AMD,2025-12-27 07:35:21,0
AMD,nw0uaqp,So the question is - wait for Zen 6 or go with this? Coming from 7950X3D.,AMD,2025-12-26 13:49:01,-3
AMD,nw1lb7y,"Full gigabyte or bust, DOA  /s",AMD,2025-12-26 16:27:53,-2
AMD,nw0k83i,Names are too god damn long,AMD,2025-12-26 12:33:54,-9
AMD,nw1abne,I wonder if it needs ram at all.,AMD,2025-12-26 15:28:17,69
AMD,nw3wvps,windows 98 SE was the goat,AMD,2025-12-27 00:04:47,3
AMD,nw88con,"Be careful, I was a Beta Tester of MS-DOS 1.0 - I was a part of the original Tiger Team if my memory is right :)",AMD,2025-12-27 18:18:20,2
AMD,nwgawet,"I'll go the Linux way....Just Crunchbang, TinyCore or Puppy Linux will run entirely on Cache.",AMD,2025-12-28 23:43:34,2
AMD,nw0jsfa,"It's a few years old now, but can't see how it would have changed much - hardware unboxed tested this.  https://youtu.be/Nd9-OtzzFxs?si=exfpdvK7cM0GHpdx",AMD,2025-12-26 12:30:12,110
AMD,nw0kecu,"Benchmark has to be reproducible, that kind of load is far too finicky to be a reliable benchmark.",AMD,2025-12-26 12:35:24,35
AMD,nw0ndo7,"All this stuff isn't really resource intensive, any somewhat modern CPU shouldn't have any issues handling that amount of multi tasking. I constantly have edge open with several tabs active and 100+ suspended, Spotify, discord and I don't notice any slowdown on an 11yo 5960x, which is significantly slower compared to basically any modern CPU.   You don't need a 16 core 9950x if all you do while playing is discord and YouTube, a 6 core 9600x will be perfectly fine.",AMD,2025-12-26 12:59:34,17
AMD,nw0kmuh,"I find myself in this exact situation 90% of the time I'm playing a game. Discord open, maybe not in a call but its there, firefox with a bunch of tabs open, steam running in the background, whatever game I'm actively playing and often a YouTube video or streaming platform playing.  I'm curious, do you also get video playback stutters when watching something on one monitor with a game running on the main monitor? I've been trying to figure out whether its inevitable due to the game hogging resources or if there's some configuration I can do to stop or at least reduce it.  I'm running a 9800x3d, rtx 4080 until a couple days ago - now running a 5090 (yay!), 64GB DDR5-6000, Windows 11 all running on nvme ssds. Also MSI MPG X870E Carbon Wifi mobo. Monitors are currently 2x 2560x1440 144hz.",AMD,2025-12-26 12:37:24,10
AMD,nw0k7pe,Surely out of all the CPUs out there this will be one of the worst for your use case /s,AMD,2025-12-26 12:33:48,2
AMD,nw25jzn,"It's almost impossible to do repeatable benchmarks in that manner, it's why basically no one does it. You would probably have to run every test a dozen times to get a good average result.",AMD,2025-12-26 18:14:28,2
AMD,nw0pwsw,I do all of that at the same time with a 5900x from 2021 AM4 has been good to me. Makes me consider skipping AM5 all together.,AMD,2025-12-26 13:18:33,3
AMD,nw1sr57,"I actually ended up getting a huge ultrawide monitor to replace dual monitors. It helped reduce the adhd task switching a little bit lol. With the Windows game mode, those other tasks go into low priority mode. Discord and chat is light enough to not affect much.",AMD,2025-12-26 17:07:18,1
AMD,nw2gspk,While it's older - I love my 5950X which is heavily OC'd for exactly this. I often have virtual machines running in the background then decide hey - it's time for a game. I'll upgrade some day but my glob RAM prices lmao,AMD,2025-12-26 19:12:46,1
AMD,nw44xi5,Those sort of applications have essentially zero CPU cycle hits. Video will be GPU and browsers/electron apps will be RAM.,AMD,2025-12-27 00:52:46,1
AMD,nw621cg,"Video and discord barely require CPU time, video is decoded by gpu and discord runs on other cores.",AMD,2025-12-27 09:46:45,1
AMD,nw6qs9r,So basically watching p*rn on the 2nd screen while leveling your paladin in Classic WoW.,AMD,2025-12-27 13:25:48,1
AMD,nwgxcuc,"Most people have one 1080p monitor. That is not true of extremely high-end users, though..  Additional monitors and browsers tend to have negligible effect on performance on modern hardware, unless you are hitting bus or memory bandwidth limits.",AMD,2025-12-29 01:46:30,1
AMD,nxgaegd,It's important to be downloading terabytes of gooning material while benchmarking gaming also.,AMD,2026-01-03 15:31:10,1
AMD,nw2qwt9,"lmao or you could like, not do that.",AMD,2025-12-26 20:07:38,0
AMD,nw24zp2,Why does this matter at all? This is not what benchmarks measure - they measure relative hardware performance.,AMD,2025-12-26 18:11:32,-1
AMD,nw0jb4u,"In gaming applications you're probably going to see very comparable results to the 9800X3D, since games will still only use one CCD. This chip is basically just two 9800X3D's stuck together, but actually using both CCD's in gaming is still limited by the Infinity Fabric connection, which right now makes that not worth it (though AMD is rumored to be working on exactly that for Zen6 and beyond)",AMD,2025-12-26 12:26:00,84
AMD,nw0ov2d,its 96mb per ccd so performance will be the same,AMD,2025-12-26 13:10:49,13
AMD,nw45fa8,It's 192Mb across 2 CCD so unless your game spills across both CCD and are both somehow used identically it will not affect anything,AMD,2025-12-27 00:55:46,4
AMD,nw7qnx0,Purely speculative guess: games that noticeably benefit from more cache see ~5% uplift.    Price will be $200 more than 9800X3D (another guess).,AMD,2025-12-27 16:48:52,1
AMD,nw0uisw,The 2025 fashion: 0.1% and 1% paranoia,AMD,2025-12-26 13:50:32,-15
AMD,nw0y7sh,"They're currently working on X4D, the tesseract-stacked cache packaging",AMD,2025-12-26 14:14:55,70
AMD,nw24pa7,Could you kick up the 4d3d3d3?  4d3d3d3 Engaged.,AMD,2025-12-26 18:10:02,2
AMD,nwcepi8,"This. This processor is intended for only three types of people:  1. Those running multithreaded scientific workloads (like climate modeling) which want all the fast cache they can get and are sometimes limited by the slower single thread performance (and cost) of Threadripper or Epyc. 2. Those with more money than sense who don't understand how multi-CCD Ryzen CPUs work and are easily manipulated by ""bigger number + X3D = better"" marketing. 3. The ""simulation gamer"" who only plays niche and highly CPU bound games that can scale with 16+ threads.  As soon as a game causes one CCD to reach across the IF to access code assets residing in the 2nd CCD's cache, the majority of the benefit of 3D cache vanishes.  But, maybe the 9950X3D was a testbed with a new hardware thread scheduling workaround that AMD ended up releasing because it was deemed successful. I doubt it, but I'd be thrilled to be wrong.",AMD,2025-12-28 10:36:19,6
AMD,nw2371h,You can now play two games at the same time.   You're welcome.,AMD,2025-12-26 18:02:13,17
AMD,nw16i7v,"Only benefit I see is if you have a use for 16 cores and you want the 2nd half to be more efficient compared to a regular 9950x3d. Also for 9950x3d users, often workloads that would benefit from vcache end up the cores without. Either way I expect the benefit to be very small.",AMD,2025-12-26 15:06:19,8
AMD,nw3ilra,"aren't there benchmarks out there where it shows the 3d cache really helps ray&path traced games? maybe what AMD has been cooking lately in that area has got something to do with it. not to mention we don't know the config of the next consoles, maybe it's time for a 16 cores PS6",AMD,2025-12-26 22:39:45,2
AMD,nwbj44t,It will mean that those of us who have use cases that can be niced to run on a single CCD can leverage these CPUs to full effect. There was little reason to purchase them instead of a 9800 X3D because the second CCD was useless. It may be moot for the layperson though.,AMD,2025-12-28 05:43:50,1
AMD,nw25ki0,"Yes, each of the two CCDs has the usual 32MB of L3 cache plus 64MB stacked below (above for Zen 3 and 4, below for Zen 5) for a total of 2x96MB.",AMD,2025-12-26 18:14:32,4
AMD,nw0pz62,"Yes, each CCD has it's own cache",AMD,2025-12-26 13:19:02,2
AMD,nw0un98,"No, each has its 32 + 64 on a different stack",AMD,2025-12-26 13:51:21,-1
AMD,nwlo2d7,you wont be missing out on much. theres not many games that are not affected by the ccd to ccd latency,AMD,2025-12-29 19:53:41,1
AMD,nw3ktrc,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q4 2025 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-12-26 22:52:27,1
AMD,nw172x3,"Will be the same, no MMO uses more than 4 cores. All popular MMOs run on very old game engines.",AMD,2025-12-26 15:09:43,9
AMD,nw2r3u0,probably not,AMD,2025-12-26 20:08:43,5
AMD,nwd52zr,Hell no,AMD,2025-12-28 14:07:52,1
AMD,nw0vq9x,Wait for zen 6,AMD,2025-12-26 13:58:32,26
AMD,nw25pj3,Is there anything your 7950X3D can't do that you need it to?,AMD,2025-12-26 18:15:16,4
AMD,nw1qrku,Also wondering. I got a 5700X3D but haven’t pulled the trigger yet on an AM5 build.,AMD,2025-12-26 16:56:44,3
AMD,nw2d9b4,Your cpu is already insanely good and the 9950x3d is not a whole lot better.,AMD,2025-12-26 18:54:10,2
AMD,nw34gup,"I hope you realize that AMD releasing this now means Zen 6 X3D CPUs are not coming anytime soon.  You will have to wait for these to sell for months before the first Zen 6 chips arrive, then you will have wait months more for X3D SKUs to be launched.",AMD,2025-12-26 21:22:00,2
AMD,nw0urhc,Wait for 11850x3d2 that will be Zen 5 compatibile,AMD,2025-12-26 13:52:08,-3
AMD,nw4s8zn,why is he kind of correct though,AMD,2025-12-27 03:22:33,3
AMD,nw1qvl5,"Fun fact: during start up, your CPU actually use cache as ram(CAR). Dram is init after SEC phase.",AMD,2025-12-26 16:57:20,60
AMD,nw1x39k,"Been a while since I've studied computer architecture but unless the cache is fully associative, which it isn't, then certain addresses with the same index bits will overlap with each other in the cache, and so certain accesses to cache will end up evicting other cache lines back to main memory, even if the cache has enough space to store all of them. So you would need some amount of DRAM at the very least. That's also not counting the fact that your CPU probably won't even boot without RAM and other technicalities.  On Linux on my 7800x3d I can check the cache details using *dmidecode -t cache* and the L3 is 16-way set-associative. Someone else should probably verify that what I'm saying is correct, but even in theory it isn't possible to run windows 98 entirely off of the L3 cache",AMD,2025-12-26 17:30:16,23
AMD,nw1yv82,I assume most operating systems are written to expect there to be some RAM in the system. It would certainly be possible to create a Linux that could run without RAM. You may need a custom BIOS as well to boot the computer.,AMD,2025-12-26 17:39:44,3
AMD,nw0lvuu,This is why I have discord and the browser configured to run in the non-vcache cores on my 7950x3D.,AMD,2025-12-26 12:47:45,54
AMD,nw0l75v,Thanks a lot I wasn’t aware of this video by them. Interesting though 10% is no small difference in 1% lows id say. I wonder what the difference between a 9800x3d and 9950x3d would be in that scenario where its literally a second ccd available,AMD,2025-12-26 12:42:05,16
AMD,nw27px2,We need a test benchmarking running wsl with dev servers alongside games lol,AMD,2025-12-26 18:25:40,1
AMD,nw1g7uw,"I have a 5800x3d and I constantly run into issues with a game on one screen and streaming something on the 2nd.  Some streams run fine on the 2nd monitor when not full screen, but if both screens are full screen the entire system lags.  I would very willingly pay for a 16 core x3d processor if I knew it would improve my issues, but I never see anyone running these kinds of tests.",AMD,2025-12-26 16:00:39,1
AMD,nw0pgzj,Ive found its due to hardware acceleration being on for your browser. Hardware acceleration uses the gpu for video and if you’re maxing out your graphics card it causes the video to stutter. Turing it off uses the cpu so as long as your cpu is not capped out it should be fine. I put my browser on my non v-cache cores and have no issues that way.,AMD,2025-12-26 13:15:18,8
AMD,nw0w3sz,"This is often due to differences in refresh rates. Believe it or not, a not so talked about thing is that synchronization and multiples of numbers matters too. 60 to 120 to 180.to.240 to 300 to 360 for example..you will stutter more with 60.and 144hz for example as they are not multiples and do not synch. The same can be said for polling rates too. This is more talked about among esports and competitive gaming and among hardcore audio and more latency based communities. And yes, video playback on a separate monitor still can cause stutters while gaming on another. Its the animations and movement of stuff open on the second monitor and thus another latency conversation too.",AMD,2025-12-26 14:01:02,7
AMD,nw0pyod,I was having a similar stream stuttering issue on Firefox. Its been a while but I believe it was an issue with hardware acceleration and a mismatched resolution or refresh rate.  That second part doesnt apply to you but I had a 1440p 240hz and a 1080p 144hz monitor.   I havent had this issue in over a year before I switched browsers.,AMD,2025-12-26 13:18:56,3
AMD,nw1xu8r,You watch Youtube videos 90% of the time you're playing a game?  How do you focus on either? :/  Does the overlapping audio not drive you mad?,AMD,2025-12-26 17:34:19,3
AMD,nw0mlak,"For me, I have a 7800x3d and a 7900xtx on 32gb ram. I almost always have YouTube streaming something on my separate 4ktv while gaming at 7680*2160p (both at 120hz) and rarely get stutters. I've never been able to pin the stutters to anything in particular like loading into levels or anything, it just happens on and off but goes away very quickly. But overall pretty smooth. I would have figured it would be vram related if anything but with your 5090 that makes it even less likely. I've never adjusted any configurations for it either.",AMD,2025-12-26 12:53:22,1
AMD,nw16wtd,"I used to get video playback stutters by the Spotofy client (lol) if it was open and visible next to a windowed game. It stopped after my latest upgrade though and I don't know if it was due to going to a X3D, or because I reinstalled Windows (on a new NVMe)",AMD,2025-12-26 15:08:43,1
AMD,nw1azje,Maybe im a super nerd but I've always had old laptops around that can do secondary stuff on without gumming up the gaming pc. Fixes the stutters right up.,AMD,2025-12-26 15:32:00,1
AMD,nw3q2jd,"Yeah. I think that it might something to do with resource allocation and maybe how Windows handles focused/unfocused windows? That's the best guess I have. That being said, I **do** have a solution that did fix it for me: if you have an iGPU (which you do, since you're on a 9800X3D--make sure to set the iGPU to enabled or auto in the BIOS), **plug your secondary monitor into the iGPU.**  I drove myself insane trying every fix under the sun because the issue was *so* goddamn annoying. Yes, hardware acceleration was the *first* thing that I turned off. Wasn't the refresh rate, since both of my monitors at the time were the exact same model, DDU didn't fix it, and I'd had the issue on 2 different builds over the span of like 5 years.  So I plugged secondary display into the motherboard to use the iGPU on my 7800X3D and haven't had the stuttering issue ever since. The thought process was like: ""well, if it's a resource/utilization thing, what if they both used a different GPU?. I only run games on my primary monitor, and I only use my secondary display for my browser, watching videos, Spotify, Discord, etc., so they don't *need* to be running off the same GPU. It's worth a try.""  Anyway, I'll get ahead of some things people might ask me about:  - Thermals: No significant/noticeable increase in temperature in everyday usage that I can't attribute to other causes, like measurement error, ambient temperature in my room, dust, etc. No thermal throttling issues, no issues hitting the advertised boost clock. My disclaimer is that I do have a modest (-20mV) undervolt running *now*, but my system was not undervolted until a while after I tried the iGPU thing.  - Performance: Negligible, if any, difference. I didn't really feel like running experiments after all this troubleshooting, to be honest.  - Power draw: Negligible.   - Jank when switching windows/mouse/etc. between monitors: Nothing noticeable.",AMD,2025-12-26 23:23:33,1
AMD,nx0eka9,"I was experiencing stutters on my secondary monitor. The typical advice I've seen given is turning off the hardware acceleration in your browser, but I found out turning off the hardware acceleration in your display settings on windows eliminates the issue (I still leave the hardware acceleration on in my Edge browser).",AMD,2026-01-01 01:16:46,1
AMD,nw0mhtb,Ain't no way a large steam game with a lot of decompression uses 1-2%. Linus even had a video about it utilizing processors with more threads.,AMD,2025-12-26 12:52:35,7
AMD,nw265vd,Yes but look at the video comparing a 6core vs 8core. It makes a difference in 1% lows when multitasking that was not there „just“ running the game and to me that matters. Hence why I wish gaming+some mentioned usual task would be tested as well. It’s not hard to repeat a discord chat and a specific video running for the duration of the benchmark just like they did 3yrs ago I think,AMD,2025-12-26 18:17:37,3
AMD,nw1a6td,"Compared to the 9950x3d, u dont have to worry where u place ur game on (the 3d cores) since all cores have 3d, so thats nice.",AMD,2025-12-26 15:27:31,5
AMD,nw0qaz8,bit faster but same,AMD,2025-12-26 13:21:21,5
AMD,nw177i5,"If by paranoia you mean actually useful metrics, then sure",AMD,2025-12-26 15:10:29,17
AMD,nw21i7i,I heard the X4D cache actually sends data forward and backward in time.,AMD,2025-12-26 17:53:26,11
AMD,nw2vzt5,I’m waiting for 5D. I can’t legitimize upgrading every D.,AMD,2025-12-26 20:35:40,2
AMD,nw2u8ny,Can I see a hat wobble?,AMD,2025-12-26 20:25:57,3
AMD,nw247bx,"You could before with the non-vcache cores, it just wouldn't perform as well.",AMD,2025-12-26 18:07:27,1
AMD,nw18sa4,"Yeah, i had a suspicion about that to be honest.",AMD,2025-12-26 15:19:33,1
AMD,nwgoeix,"Now that you said, I guess it offers the convenience of not having to manually set core affinity to the 3D V-Cache CCD with Process Lasso, as both CCDs have it, or install the chipset driver that automatically does that.",AMD,2025-12-29 00:54:59,1
AMD,nw1aczf,"Yeah wow fps is garbage in raids, but x3d significantly improves it.",AMD,2025-12-26 15:28:30,2
AMD,nw4wbmh,Not having to worry about games running on the non-cache cores.,AMD,2025-12-27 03:49:53,1
AMD,nw2qxsk,I'd wait,AMD,2025-12-26 20:07:47,2
AMD,nw1qtjv,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-26 16:57:01,0
AMD,nw35f8f,"With the exception of modern Zen CPUs, where the whole upbringing is done by the PSP...  Sadly, it doesn't work without RAM, 2x the cache seems to be the minimum needed, as the PSP also initializes memory encryption and other features before the entry address is loaded.",AMD,2025-12-26 21:27:07,28
AMD,nw24tzv,"> Been a while since I've studied computer architecture but unless the cache is fully associative, which it isn't, then certain addresses with the same index bits will overlap with each other in the cache, and so certain accesses to cache will end up evicting other cache lines back to main memory, even if the cache has enough space to store all of them.  That only happens when RAM is larger than cache. If you don't have RAM and only use the cache then direct-mapped is good enough because there aren't any other addresses that can alias a cacheline!",AMD,2025-12-26 18:10:43,17
AMD,nw5v7ua,"I can verify you are correct and the other guy contradicting you probably didn't understand what you said. The cache info doesn't have the full address, just parts of it. The cache architecture would have to be completely redesigned to support a ramless mode. If this was the case it would be fully associative as you rightfully pointed out.  Source: What Every Programner Should Know About Memory (Ulrich Drepper)",AMD,2025-12-27 08:39:54,3
AMD,nw3gs8g,this makes me wonder if it's possible to do this while also load up a video game only in VRAM,AMD,2025-12-26 22:29:22,1
AMD,nw0okbp,Did u use process lasso?,AMD,2025-12-26 13:08:36,7
AMD,nw0m15i,Run the game on one CCD and everything else on the other feelsgood,AMD,2025-12-26 12:48:56,12
AMD,nw1xdje,"Sounds like a hardware acceleration issue to me. Tinker around with the hardware acceleration settings in your browser, discord, game clients etc.",AMD,2025-12-26 17:31:48,13
AMD,nw1li3v,Is hardware decoding enabled on your browser?,AMD,2025-12-26 16:28:54,9
AMD,nw453jq,That is almost certainly not a CPU issue,AMD,2025-12-27 00:53:48,4
AMD,nw1gl1s,"Sounds more like a software issue since it only happens when putting the stream in full screen, and that doesn't make it heavier on resources. You won't solve the problem by throwing more cores at it.",AMD,2025-12-26 16:02:37,2
AMD,nwartlx,"I run 3x 4k monitors at 144hz. Typically playing BF6 and afking on OSRS, discord open/multiple tabs/etc. Originally had the 7800X3D. I'd see a noticeable drop in 1% lows as I was pretty much maxing it out when doing the above. I bought an 9950X3D and 1% lows are much better/not seeing high peaks on CPU util. Extra 8 cores make a noticeable difference if you are multitasking. Just wish I would have waited for this new variant for the extra cache.",AMD,2025-12-28 02:43:08,1
AMD,nw14h61,How do u do the manual v vache thing,AMD,2025-12-26 14:54:17,2
AMD,nw1zc7a,"That's why I set my main monitor to 120Hz, even though it supports up to 165Hz. It's a multiple of common framerates like 24, 30, 40 and 60 Hz, and I can't notice much difference to 165Hz anyway.",AMD,2025-12-26 17:42:11,1
AMD,nw2udoi,"Well 90% is an exaggeration. Really it depends on what I'm playing. If its something chill that doesn't require much focus I like to play a movie or something. Partly for background noise partly for entertainment if I'm grinding something repetitive. For example, if I was cargo hauling or mining in elite dangerous I'd be watching something but if I was playing Battlefield or maybe a story game like BG3 I wouldn't be watching anything cause I need to focus on the game. Other times I might be watching a tutorial for the game I'm playing.",AMD,2025-12-26 20:26:43,1
AMD,nw0vcwt,I'll try this on my 9950x3d and 1000/1000 connection when I get back home. Can't remember how much CPU power it uses. But iirc it's not more than perhaps 3%?  !remindme 5h,AMD,2025-12-26 13:56:04,1
AMD,nw25tob,You haven't had to worry about scheduling these CPUs for years now if ever.,AMD,2025-12-26 18:15:52,19
AMD,nw6ww1g,"But you still have to worry about the game using both CCDs at the same time.  As in, some threads on one CCD, some on the other. Most games will fit into one single CCD (8 cores) but if you have other things running you might accidentally see unwanted thread migration.   This is a really weird product honestly, that makes more sense for productivity apps that need cache rather than gamers.",AMD,2025-12-27 14:05:42,2
AMD,nw255lh,"For most games you still want all the threads to run on the same CCD, compared to the 9950X3D you just don't care which one.",AMD,2025-12-26 18:12:23,1
AMD,nw183nq,"You are all silly. Just about lows you have ""knowledge""",AMD,2025-12-26 15:15:38,-17
AMD,nwg0af1,Imagine your computer next year using all its cache to super charge your game this year… then Jan 1 you can’t use your computer because you’re feeding last years games.,AMD,2025-12-28 22:46:39,2
AMD,nwshdhu,Yes but they had to turn it off since there was an exploit with remote past code execution.,AMD,2025-12-30 20:15:12,2
AMD,nw25wv0,"Well, sure. You could do the same on a 1600x if you really wanted to. The experience would be less than stellar, though.",AMD,2025-12-26 18:16:20,3
AMD,nwhro13,"I wrote my own program in go that nices processes according to what I need and know is optimal for the programs I run. Same concept as process lasso, just much more effective because it does exactly what I need.",AMD,2025-12-29 04:47:03,1
AMD,nwbtmh0,"The exact same problem from your current CPU would exist on this hypothetical CPU, because a game running on both CCDs would be getting fucked by latency trying to communicate over the infinity fabric.",AMD,2025-12-28 07:13:26,1
AMD,nw2g129,"Imagine the blazing fast speed the OS would run, if loaded to the L3! I bet something like tinycore would work, give the low ram req.",AMD,2025-12-26 19:08:42,8
AMD,nw2an75,Challenge accepted,AMD,2025-12-26 18:40:43,2
AMD,nw925kf,"... And I can verify that you need to reread your source.  Cache as ram is a normal thing that a lot of platforms support. Cache tags need to contain the entire address, or they could not be used to disambiguate accesses.  The way car works is that you turn off writeback, and choose to only use a contiguous linear range of addresses smaller than your largest cache. This cannot cause any conflicts.",AMD,2025-12-27 20:54:56,3
AMD,nw616lm,In theory it should be possible yes. You may need a custom graphics card driver.,AMD,2025-12-27 09:38:15,3
AMD,nw1kg6j,There are two CCDs and each has a V-cache tile,AMD,2025-12-26 16:23:14,10
AMD,nw0tkyr,"No because it causes a bug that locks up the entire system after hours, i just use a bat file to start certain programs like a browser and discord, never failed for a year + now, unlike Process Lasso.",AMD,2025-12-26 13:44:13,20
AMD,nw48bit,"I went from a from a 5900x/5700xt without this issue to a 5800x3d/6700xt with it.  The 5800x3d/6700xt is much better overall, but I occasionally run into hiccups that I did not have with more cores and a weaker gpu.  Whatever the cause is, I would still appreciate benchmarks with things happening on a 2nd monitor to get a better idea of what is worth spending money on to maximize performance.  I mainly play strategy/4x games that push the cpu much harder than the gpu.",AMD,2025-12-27 01:13:49,1
AMD,nw1hm1b,"Which is why I have as of yet not just thrown money at the problem.  I would still like to know for sure.  Even it is an issue caused by software, does not mean hardware can't fix it.  I have read some people use core parking to keep gaming on the main ccd and everything on the 2nd monitor on the other one.  That could conceivably solve the problem. Or not.",AMD,2025-12-26 16:08:05,0
AMD,nw1fihw,"I have a 7950x3d. 8 of the 16 cores have the 3d v-cache for gaming. The other 8 cores are just regular cores. I use an app called process lasso that I can use to manually force applications onto the gaming cores or the regular cores. I force my games onto the v-cache and all other apps like my browser, discord, and Spotify onto my regular cores. Sorry if its over explained I’m just not sure where your knowledge on this starts or ends.",AMD,2025-12-26 15:56:52,3
AMD,nw0vhpt,I will be messaging you in 5 hours on [**2025-12-26 18:56:04 UTC**](http://www.wolframalpha.com/input/?i=2025-12-26%2018:56:04%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/1pw23gu/amd_ryzen_9_9950x3d2_16core_cpu_with_192mb_l3/nw0vcwt/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F1pw23gu%2Famd_ryzen_9_9950x3d2_16core_cpu_with_192mb_l3%2Fnw0vcwt%2F%5D%0A%0ARemindMe%21%202025-12-26%2018%3A56%3A04%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201pw23gu)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,AMD,2025-12-26 13:56:58,1
AMD,nw29ppk,No? U need process lasso,AMD,2025-12-26 18:35:56,-26
AMD,nw68dk4,"I am wondering if ""running the game on cores all on the same CCD"" might still require parking the cores​ of the other CCD via ""game bar"".  So, even if both CCDs have X3D, it might not help that ""one does not have to care which CCD the game will be running on"". The ​other CCD might still need to be parked via game bar during games to avoid the game running across CCDs.",AMD,2025-12-27 10:48:59,3
AMD,nw19tnn,Ok have fun with a 60 average stutter fest,AMD,2025-12-26 15:25:26,6
AMD,nwc7z9q,"Isnt it still two cores X3D cores. I would assume they would still either run on CCD1 or CCD0 most of the time, the time they spill over to both and the impact on performance, well lets wait for reviews if the cpu ever comes to market.",AMD,2025-12-28 09:31:12,1
AMD,nw1payv,"Tiles aside, it's a chiplet, this. ^^  TomAto tomaato though they are basically the same thing.",AMD,2025-12-26 16:49:00,7
AMD,nw1t1ku,"it's not a ""tile"". tiling is what Intel calls their multiple-chip designs, AMD called them ""chiplets"" for their GPUs. the 3d cache is stacked on top of the same die, it's not a separate tile like intel's CPU tiles or AMDs gpu chiplets",AMD,2025-12-26 17:08:50,8
AMD,nw179lf,Interesting. I have never heard about this bug. Could you please tell me more about the bug?,AMD,2025-12-26 15:10:49,8
AMD,nw0vo6v,"start """" /affinity 1 /LOW ""C:\Games\something.exe""  Something like that?",AMD,2025-12-26 13:58:09,10
AMD,nw1a744,It doesn't cause that bug by default because I use it in background for years to launch games on 3d cores and it works. What's that bug again? You can probably fix whatever is the real problem,AMD,2025-12-26 15:27:34,4
AMD,nw25p2q,"Never had process Lasso fail me, been using it for 7 years now.",AMD,2025-12-26 18:15:12,2
AMD,nx1enrd,"You are most likely setting up process lasso wrong.  I only use process lasso and game mode off and use ultimate performance power with cores unpacked.   Never had an issue  Process lasso, you cannot set cou affinities or I’m will have issues.     So basically leave affinity alone, and set all the stuff via cou sets .   That will stop all the freezing problem  I’m on a 9950x3d",AMD,2026-01-01 05:31:22,0
AMD,nw1ifzy,"You could try splitting 4 cores on the game and 4 cores on the background stuff to see if anything changes, but ofc try a lighter game that can run fine on just 4 cores. Or split 6 - 2 cores.  Might also be a GPU related issue, maybe the driver doesn't handle multiple full screen apps correctly. Are you using gsync/freesync?",AMD,2025-12-26 16:12:33,1
AMD,nw2edcx,Do you even own the hardware or are you just repeating stuff you read on reddit 2+ years ago?,AMD,2025-12-26 18:59:56,22
AMD,nw2w6r5,No you don’t. That hasn’t been necessary for years now.,AMD,2025-12-26 20:36:45,7
AMD,nw2vn25,"As long as you install AMD's chipset drivers, and have an updated version of Windows, you don't need to do anything regarding the cores being used for games. The Windows scheduler handles that all properly, and has for a while.   Source: Literally running a 9800X3D rn",AMD,2025-12-26 20:33:43,0
AMD,nw1d8v5,You poor soul...,AMD,2025-12-26 15:44:35,-15
AMD,nw2cpb4,"Stacked under the cores on the 9000 series, that's how they got higher clock speeds out of the x3d chips",AMD,2025-12-26 18:51:18,2
AMD,nw2ex75,"You can give the information you wanted to provide without having to pull a ""Um akshually"". Just because a multi-billion dollar tech company has a process called 'tiling' doesn't change what the word means in the broader societal lexicon. It's a thin rectangular slab of L3. Thus it's a tile.",AMD,2025-12-26 19:02:49,-7
AMD,nw1abw6,"After closing games (like a 2-3 games closed later, there was no consistency/pattern to be found), with delayed CPU affinity assignment (usually 15 seconds, because some games have an issue with instantly assigning core affinities at launch, example, Baldur's Gate 3, if you start it with a specific CCD affinity manually, it will not be able to have produce any sound for some reason), the PC would lock up completely when that happened, and it only happened with if Process Lasso tried to do the core assignment (the amount of delay i set didn't seem to matter, i tried 5 minutes tops), the PC is perfectly stable otherwise for years now (i build it, i tested it, with so many tools and configured for 24/7 stability). Haven't used Process Lasso since then, due to afformentioned issue.  Edit: I forgot to mention that, i always had and updated the AMD cache driver and Driver set to BIOS (Auto produced the same exact behavior).",AMD,2025-12-26 15:28:19,10
AMD,nw14cx3,"This places Discord on the second CCD (frequency CCD if you want it on the other CCD, just change the FFFF0000 to 0000FFFF), there are better ways to do this affinity handling but it's a decent one if you want something simple.  cmd.exe /c start ""Discord"" /affinity FFFF0000 ""C:\Users\User\AppData\Local\Discord\app-1.0.9218\Discord.exe""  The only issue with this is due to Discord changing the versions on the file name and i have to change the number in the end (haven't bothered finding a workaround, probably exists though), but for other programs i never had to do this.  Edit: Well, a better workaround  @echo off set ""discordPath=%LocalAppData%\Discord"" set ""latestApp=""  for /f ""delims="" %%i in ('dir /b /ad /on ""%discordPath%\app-*"" 2^>nul') do (     set ""latestApp=%%i"" )  start ""Discord"" /affinity FFFF0000 ""%discordPath%\%latestApp%\Discord.exe""",AMD,2025-12-26 14:53:34,22
AMD,nw162ac,"Added a workaround in my other reply, cheers.",AMD,2025-12-26 15:03:43,1
AMD,nx1sbhq,If that was the case I would have issues with setting affinities manually as well but I don't.,AMD,2026-01-01 07:37:57,1
AMD,nw2o74s,U sound frustrated,AMD,2025-12-26 19:52:48,-25
AMD,nw3jfpt,"9800X3D doesn't have two CCDs, 7950X3D user here and when processes jump between CCDs it creates latency issues which is why project lasso is a thing. I have learned about this recently, I wasn't using my CPU to it's full potential.",AMD,2025-12-26 22:44:28,2
AMD,nw37enm,"I was talking about the dual ccd variants, im pretty sure i read that u have to uninstall game bar because otherwise amd's software that decides where to put the load on the cores (i forgot its name) doesnt work properly, but that means u lose game mode which is pretty bad. So yeah, u have to use process lasso. I dont own the cpu but either way its a good practice to do manually than relying on some software that god knows what its doing.",AMD,2025-12-26 21:37:56,1
AMD,nw2u29t,Nope. They're chiplets.,AMD,2025-12-26 20:25:01,6
AMD,nw1bs9e,Ohh I see. Is it the same case with 9950x3d?,AMD,2025-12-26 15:36:28,1
AMD,nx2e5ha,"You're not supposed to use affinities, you're supposed to use CPU sets...",AMD,2026-01-01 11:29:24,1
AMD,nw1euar,Thank you kindly,AMD,2025-12-26 15:53:14,8
AMD,nw1so1t,"batch scripts are a scourge, a blight on humanity",AMD,2025-12-26 17:06:50,3
AMD,nw2pvt4,I hate it when people spread misinformation online.,AMD,2025-12-26 20:01:59,16
AMD,nw67jc4,"I have a 7950x3d… I can’t remember the last time I had trouble gaming due to the unbalanced CCDs. Everything just works now without much thought. When it first came out yeah, there were issues where games tried to use both CCDs with threads hoooing between CCdS and had wild stuttering and performance issues.",AMD,2025-12-27 10:40:52,2
AMD,nw4c5rq,I'm just going to call it rectangular thinking rock,AMD,2025-12-27 01:38:26,1
AMD,nw5ccqc,"Are we talking about CCDs or the L3 layer? One would constitute a chiplet the other doesn't have a ""chip"" it's just a slab of storage medium. Unless we're just calling anything square shaped that ends up on the die chiplets now",AMD,2025-12-27 05:47:11,-1
AMD,nw1erj2,"I don't own that CPU so i can't test it for that specific issue to be honest, someone with it might be interested to do it though, maybe ask around (i would like to know too by the way), cheers!",AMD,2025-12-26 15:52:50,2
AMD,nx2rkqa,Is this a new thing for Process Lasso? It's been quite a while since i've used it as i said.,AMD,2026-01-01 13:29:42,1
AMD,nw3lemx,They can be but it depends on a lot of things.,AMD,2025-12-26 22:55:49,4
AMD,nw9m6mc,blight.bat,AMD,2025-12-27 22:43:09,3
AMD,nw2qgim,Me too,AMD,2025-12-26 20:05:10,-17
AMD,nw6bmhq,"No im talking about amd software, the 3d v cache optimizer or smth like that, the  same one that decides to place ur game on the 3d ccd and the other stuff on the non-3d ccd. Last i read it wasnt very good and it had big conflicts with game bar / game mode. So its best to not use it and just use process lasso. No idea how it's on the 9950x3d, but thats why people recommended avoiding the 7950x3d for gaming and just getting the 7800x3d. Now with the 9th gen it seems like most of these issues are fixed from what ive ""heard"".",AMD,2025-12-27 11:20:26,1
AMD,nw64wwo,"As long as you don't call it a tile, we're good.",AMD,2025-12-27 10:15:04,2
AMD,nx2s1fn,"It's been there for a very long time.  CPU sets is what you use. Google the difference between each to understand why.  Once you move to CPU sets, everything works smoothly. You can do a wildcard config to put everything on the non-cache cores, then run things like games on the cache cores, and run things that benefit with all cores.  Game mod and all that off.",AMD,2026-01-01 13:33:11,1
AMD,nw2rwe4,Lol,AMD,2025-12-26 20:13:04,13
AMD,nw7cvol,"Yes, that’s exactly what I’m talking about. For x3d chips you rarely have to worry about it anymore. The chipset driver takes care of it, including the older generations. It basically modifies the windows scheduler which is what lasso did.",AMD,2025-12-27 15:39:13,2
AMD,nv4c3lv,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-21 00:36:26,1
AMD,num7lk2,"I don’t understand why they won’t make one with 6 cores and 16cus with no npu for gaming handhelds. We don’t need 12 freaking cores in our handhelds, it’s actually detrimental considering it pulls power from the tdp starved gpu.",AMD,2025-12-18 01:46:36,58
AMD,nunw8sf,Rebadged ryzen 5 340.,AMD,2025-12-18 10:07:13,4
AMD,nunyoel,"This doesn't deserve the AI7 naming with 2+4 cpu cores and 4 cu graphics.  I think this is the uncut Krackan Point 2 die. If it's Gorgon Point, then they could have given it 3+3 cores, just like the 340 predecessor, and not make it worse than it.",AMD,2025-12-18 10:31:34,4
AMD,num0j11,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-18 01:01:45,3
AMD,numf02f,Soo rdna3.5? How much the improvement from 740m?,AMD,2025-12-18 02:32:57,2
AMD,numb80h,a 6/16 with RDNA 4 (for the FSR4) would make a fantastic successor Steam Deck 2,AMD,2025-12-18 02:09:26,31
AMD,nuncidy,"Isn't this the Z2E? It's an 8 core IIRC, but 5/8 are Zen 5c cores (granted, it's cut down from Strix Point).",AMD,2025-12-18 06:45:12,1
AMD,numpkn5,"Not a ton, but measurable. Per-CU rdna3.5 isn't much better in typical gaming, but it is better. Check out 880M vs 780M benchmarks to see the top end of each.",AMD,2025-12-18 03:41:59,2
AMD,numbul6,They will def skip rdna4 for some stupid reason.  I get it’s a stop gap before rdna 5…but it’s also a 2 year gap.  Maybe longer now with memory prices.  I think steam deck 2 will be 6 zen 6 cores and 12-16 rdna 5 CUs in 2027.  But again…memory :/,AMD,2025-12-18 02:13:17,12
AMD,nun4rl5,In 2028.,AMD,2025-12-18 05:36:52,2
AMD,nuvuc7h,"Z2E is rdna 3.5, which is just a refined rdna 3.  Rdna 4 is significantly more efficient than rdna 3.  Example: the 9070xt is a 64CU chip that outperforms the 84cu 7900xt at similar TDP (~300w)  If that holds true at lower tdp’s, then an RDNA 4 16cu chip would perform around 25% better than the Z2E, which is already basically 80% better than the steamdecks apu.  So a hypothetical rdna4+ 6core 16cu handheld would have >2x performance to the OG steamdeck.",AMD,2025-12-19 16:20:16,1
AMD,nun2edd,"AFAIK Valve said they're not going to make Steam Deck 2 until there's a very noticeable advance in APU technology. IIRC even +50% performance is not enough for them which is a shame on one hand, but on the other, I totally get it.",AMD,2025-12-18 05:17:20,12
AMD,nuvumpq,My comment was not about graphics.,AMD,2025-12-19 16:21:40,1
AMD,nun2jsv,Rdna 5 would be well beyond 50% considering rdna 4 would deliver almost 50% uplift.,AMD,2025-12-18 05:18:34,1
AMD,nuv5u7h,"You can't push for devs to make console-like optimization for your device if you keep releasing new devices everytime there is +10% power available !  SD2 should be like Switch 2 vs Switch 1, something that really feels like a different generation, that will run games that the first one can't even hope of launching.",AMD,2025-12-19 14:14:30,1
AMD,nunfrvo,"Isn't it like there would be no RDNA 5? AMD is planning to merge RDNA (consumer branch) with CDNA (professional branch) into UDNA. Nonetheless, the rumors suggest that APU based on Zen 6 (which would be the next generation) could power the Steam Deck 2 with its launch in 2028 ([article](https://www.linuxjournal.com/content/steam-deck-2-rumors-ignite-new-era-linux-gaming)). Fingers crossed for Valve",AMD,2025-12-18 07:15:50,2
AMD,nuvt7wt,">You can't push for devs to make console-like optimization for your device if you keep releasing new devices everytime there is +10% power available !  The problem with Steam Deck is that it's just a PC packed into a handheld. It uses the same games library as your ""normal"" PC, someone's else ""normal"" PC, my ""normal"" PC, so there's no chance we'll get the console-like optimization since it's impossible. PCs are just too different.   Although I totally agree that there's no point in releasing a new handheld every year or two because there's a new APU with +15% more performance.",AMD,2025-12-19 16:14:53,2
AMD,nuoh920,I read that as zen6 being 2028 and got a little concerned. I’m planning to upgrade to zen6 when “AM6” drops and that timeline was putting it at 2032 lol.,AMD,2025-12-18 13:07:28,1
AMD,nuolk58,"Yes it will likely not be called rdna 5, rumors have said udna for a while.  I think 2028 is way too late.  Next gen consoles and PlayStations portable will be 2027 which will all be zen 6 and udna.  Unless memory pricing pushes that back.",AMD,2025-12-18 13:35:12,1
AMD,nvimelc,"Devs can create specific settings to target 30/40/60 fps, just like on console, by tweaking all graphic parameters, NPC density and various effects.  It works on Steam Deck because everyone using one have the same hardware and same small screen so devs can adjust based on that.",AMD,2025-12-23 09:09:52,1
AMD,nuoljt7,"I think AM6 will be revealed in late 2027 or in 2028 as it'll be around 5 years since AM5 release, so it's at least 2 years of waiting.  I also wait for AM6 to upgrade my PC, but if the current situation will stay with us for longer (or, hopefully not, become even worse) mg rig will have to stay with me for a while.",AMD,2025-12-18 13:35:08,1
AMD,nuonk0w,I’m rocking an AM5 platform now and my plan is to upgrade when they EoL the socket to an end stage processor.,AMD,2025-12-18 13:47:23,1
AMD,nx0l0h0,Nintendo's lawyers are in position,AMD,2026-01-01 01:58:30,22
AMD,nx6v3rw,They are going to get sued royally.,AMD,2026-01-02 03:07:34,6
AMD,nx9eez4,That'll be expensive. Looks cool though,AMD,2026-01-02 14:50:18,7
AMD,nx16n2g,So cute,AMD,2026-01-01 04:26:08,4
AMD,nxps8ms,They going to release new spyware along with it?,AMD,2026-01-04 23:13:16,1
AMD,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
AMD,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
AMD,nw3e1uz,that is the most non-descript render of a laptop possible,Intel,2025-12-26 22:13:56,5
AMD,nw638sa,So light it visibly doesn't have any ports?,Intel,2025-12-27 09:58:34,2
AMD,nur68kw,"Does intel 10A still come out as scheduled in 2027? I googled it and found out intel said the 10A will come out in 2027, but this was old news in 2024.",Intel,2025-12-18 21:37:42,16
AMD,nuu5n9y,I wonder how intel and other companies are going to manage for next year? Prices for memory and SSD’s are predicted to go even higher putting off many buyers from getting a new PC build or laptop.   This makes me concerned Nova Lake won’t sell as well because of this.,Intel,2025-12-19 09:41:00,5
AMD,nuthq66,It's shameful to see LBT posing with 14A wafers when all the groundwork for this was setup by Pat Gelsinger. The entire Intel board should have been sacked instead of Pat.,Intel,2025-12-19 05:59:22,12
AMD,nur0ojq,"GFHK also has 14a for Razor and Coral Rapids in 2H 2027, so I'm taking what they are saying with very little credibility.   Plus, we had very similar rumors during 18A, and that went nowhere. Fool me once...",Intel,2025-12-18 21:10:09,11
AMD,nutolrl,Unbelievable till official announcement,Intel,2025-12-19 06:56:47,2
AMD,nusrcmh,can't they use it to make more ram ?,Intel,2025-12-19 03:04:41,4
AMD,nur9juo,good news,Intel,2025-12-18 21:54:21,2
AMD,nvzjgd1,They can't even sell 18A to NVDA what are they doing on 14A really ?,Intel,2025-12-26 06:29:30,1
AMD,nur0nvy,"Lisa So Sue Me wants a taste of the Lip? Am I living in a different dimension? I callled out So Sue Me on X, is she jumping on Big Blue’s Back?  Is anyone Dollar Cost Averaging INTC? It will still be awhile before IFS is firing on all cylinders. The Lip said he would stop high end chip production for external customers (If No One Took A Byte) in order to get $$$ to build out Ohio Fab.   Let’s get it done. I’m driving distance from the Ohio Fab, any chance Intel will give me a tour?",Intel,2025-12-18 21:10:03,-18
AMD,nusksfh,"If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  [https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots](https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots)  *""Intel's previously-unannounced Intel 10A (analogous to 1nm) will enter production/development in late 2027, marking the arrival of the company's first 1nm node, and its 14A (1.4nm) node will enter production in 2026.*  ***\[Edit: to be clear, this means 10A is beginning development, not entering high volume manufacturing, in 2027\]*** *The company is also working to create fully autonomous AI-powered fabs in the future.""*",Intel,2025-12-19 02:26:13,12
AMD,nur6gwd,"14A probably won't be ready for 2027, much less 10A.",Intel,2025-12-18 21:38:53,13
AMD,nutent6,10A & 7A are in R&D phase,Intel,2025-12-19 05:35:17,3
AMD,nurn23y,It's gonna be 2026 soon and 18A is launching at the very start of 2026. A double node shrink in like 2 years doesn't exactly sound very possible.,Intel,2025-12-18 23:07:17,4
AMD,nuu144l,"Remember, these are just names/nicknames. 10A? The difference between 14A and 10A is probably equivalent to the difference between 14nm and 14nm+",Intel,2025-12-19 08:55:31,2
AMD,nurpt6m,And yet here you are.,Intel,2025-12-18 23:23:22,10
AMD,nutpnod,"Brother, don't hint at your place of employment when you have your full face in your profile as well as you commenting in NSFW subs.",Intel,2025-12-19 07:06:02,3
AMD,nuteqb3,There will probably still be another of layoffs next month 😂,Intel,2025-12-19 05:35:49,2
AMD,nutezdb,"Yes, perhaps it’s better if you post it on the r/intelstock subreddit instead 🤪",Intel,2025-12-19 05:37:45,1
AMD,nv78q7z,"Ram should be at a more reasonable price in 2027 according to Moores Law is Dead. Maybe not $100 for 32GBs, but maybe below $200 🤞",Intel,2025-12-21 14:18:36,2
AMD,nvi0fpp,They have contract.,Intel,2025-12-23 05:47:11,1
AMD,nw3qq9x,"TBH I feel LBT is doing a good job. I was hesitant at first, but he's making a lot more sense than Pat's crazy descent into spending crazy amount of cash with no business in sight.  Speaking as a shareholder.",Intel,2025-12-26 23:27:31,2
AMD,nutoo6g,"The entire Intel board probably should have been sacked, but Gelsinger as well. He failed at his main mission and drove the company into a crisis. That kind of thing should have consequences.",Intel,2025-12-19 06:57:22,2
AMD,nuv6jd0,Who was it that decided to exit the SSD business.  They sold off a cash cow for pennies on the dollar.,Intel,2025-12-19 14:18:21,0
AMD,nutu4bu,Nvidia is at least some what believable. AMD though?,Intel,2025-12-19 07:47:24,4
AMD,nuu5f18,"I thought that too. At least they'd have some money coming in. But apparently it takes years to rejig the plants to churn out RAM instead of CPUs. And they're heavily invested in getting the next gen CPU fabs working.   Pivoting to RAM just doesn't make sense, unless they magic'd up a new type of RAM that's cheap to make and has super low latency - which is one thing I've always thought they ought to do.   Imagine if external RAM ran with super low latencies like CL1 or CL2 or something. You wouldn't even need branch prediction and prefetch and massive caches in the CPUs.",Intel,2025-12-19 09:38:44,3
AMD,nutophj,"""news"" needs a lot of quotes around it...",Intel,2025-12-19 06:57:41,1
AMD,nuuzsz6,This isn't wallstreetbets. We don't talk like that here.,Intel,2025-12-19 13:40:01,4
AMD,nusti41,">If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  Yup, and to make it even more obvious, the same graph also has Intel 14A showing up early 2026, and 20/18A showing up at the start of 2023*,* so clearly it's not the date of when the node is going to come out (or even start HVM).",Intel,2025-12-19 03:17:42,7
AMD,nurmeyo,"Dunno why this is being downvoted, the CEO of Intel himself said that 14A is a 28-29 node in the Q2 2025 earnings call.",Intel,2025-12-18 23:03:37,10
AMD,nuy09wl,enough info about how intel names products exists to know. if it didn't increase in transistor density per mm it would not be called 10A.,Intel,2025-12-19 23:07:36,4
AMD,nutpt6n,"I think everyone knows there will be continued Q1 and possibly Q2 layoffs.   Return to office didn't lead to enough voluntary attrition. Leadership wants to hit a magic number which sounds good for financial reports, not what is actually viable to run things.",Intel,2025-12-19 07:07:25,5
AMD,nw3rzlk,That crazy amount of cash being spent by Pat is what enabled 18A and 14A. They HAD to buy multuple $250 Litho machines from ASML in order to make that possible. Pat was playing catch up after years of under-investment by Swan and Krzanich. It was necessary and LBT is getting the credit. You don't appear to understand the lead times required in the semi industry. Pat understood that. The mistakes Pat made were trying to build a fab in Ohio and not cutting headcount and getting rid of dead weight sooner.,Intel,2025-12-26 23:35:12,1
AMD,nuuu28f,The thing intel is doing rn is literally pat's groundwork isn't it?,Intel,2025-12-19 13:04:50,7
AMD,nutv81y,Still a tall order imo unless it's some defense chip for RAMP-C,Intel,2025-12-19 07:57:50,1
AMD,nv0hjyu,"If they're following industry standards I'd say it depends on how good AMD's next gen is. Intel doesn't need direct access to AMD designs to etch chips for them, and designers make way more than per wafer than foundries do.  If AMD has superior designs to intel again they could finally ship out some damn chips for laptop OEMs. It would hurt intel more than the revenue would benefit them imo since client has really been carrying intel for the last six years and demand for AMD chips has been high despite the drip feed of strix chips. honestly I'm considering an AIO/NUC/whatever the new name is with strix halo and unified LPDDR5 to upscale old footage without having to use my daily desktop. imagine if it was available at scale.",Intel,2025-12-20 10:33:05,1
AMD,nuu642g,"they don't have to make faster ram, just make it, right now, some ppl don't really care about speed",Intel,2025-12-19 09:45:35,2
AMD,nuu0o0x,"So, risk production in late 27/early 28 and HVM in 2029 I suppose?",Intel,2025-12-19 08:51:11,2
AMD,nuvsda6,YEs it is. He did make mistakes. He was hiring like crazy at the beginning of his term. And he should have started cutting sooner. But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.,Intel,2025-12-19 16:10:45,3
AMD,nuwuwrt,"Nothing they're doing *right now* is a success story. Remember that they don't actually have customers, and that is first and foremost what got Gelsinger fired. As things stand, the foundry as a whole is a failure. If things turn around, that will have to be under Lip Bu.",Intel,2025-12-19 19:20:39,1
AMD,nuu5jf7,I wasn’t aware 14A is part of the RAMP-C initiative. I thought it was only Intel 16 & 18A that are currently covered by RAMP-C?,Intel,2025-12-19 09:39:58,1
AMD,nuubzhq,I think so. Maybe optimistically we see a 14A product in late 28'.,Intel,2025-12-19 10:41:56,4
AMD,nuwv4b8,"> But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.  No, that was just more wasted money. 18A doesn't even use the high-NA machines Intel bragged so much about. It seems they tried blaming their struggles in foundry on the equipment instead of the broader org culture and talent.",Intel,2025-12-19 19:21:43,-1
AMD,nv78ydu,"As much as I hate to say it, Intel arc was also a mistake.",Intel,2025-12-21 14:20:00,0
AMD,nvl276b,get out of here with your sensable comments. we only circle jerk on this sub,Intel,2025-12-23 18:22:21,0
AMD,nuu6whm,It can expand in future ? My point is how can we believe such stuff at face value without actual proof.,Intel,2025-12-19 09:53:22,1
AMD,nuzuyhs,14A does use the High-NA machines. They didn't buy them with no plan to use them That would be stupid.,Intel,2025-12-20 06:44:37,4
AMD,nvl210i,"no it wasnt. GPU's are surpassing cpu's eventually if not now.  a major part of amds success  was buying radeon all those years ago. when intel realized how utterly shortsighted they had been, they pushed arc heavy even though it wasnt going to succeed that well.  this was the right choice, as otherwise they would look like a dinosaur.",Intel,2025-12-23 18:21:32,1
AMD,nuu7dr5,"It can expand in the future but this is a trial, it’s not yet a long term commitment until the outcome of the project is known (final evaluation won’t be until 2026/2027). 14A is not part of RAMP-C, it’s still in phase III trial with 18A. There’s been no additional RAMP-C design calls via NSTXL that I’m aware of",Intel,2025-12-19 09:58:04,1
AMD,nv05iea,"> 14A does use the High-NA machines. They didn't buy them with no plan to use them  They bought the very first high-NA machines, claiming it was for 18A. Now they won't be used until a node that hits volume in '28/'29, by which point TSMC will have (or rather, already has) much better machines. So what exactly was the point?  > That would be stupid.  Is that not a perfectly apt description for Intel's foundry strategy in recent years? It sounds like they really drunk the coolaid with their attempts to blame the 10nm failures on the lack of EUV.",Intel,2025-12-20 08:28:08,2
AMD,nvl37xc,Yeah. The real mistake was LBT and the other Intel board members nerfing the r&d budget.,Intel,2025-12-23 18:27:21,1
AMD,nv074kj,14A will have volume production in 2027.,Intel,2025-12-20 08:44:44,3
AMD,nv63f2d,Didn't Intel say in a presentation that 2027 is risk production for 14A? https://www.techspot.com/news/107736-intel-doubles-down-foundry-ambitions-unveils-18a-14a.html  https://www.youtube.com/watch?v=5Jbj4RQBXbo&t=818s,Intel,2025-12-21 08:18:24,1
AMD,nv088jg,"Lip Bu himself is saying '28-'29. At this point, there isn't a chance in hell it's ready for volume in '27.",Intel,2025-12-20 08:56:04,0
AMD,nvqcqv3,I just wanted arc to succeed 😔,Intel,2025-12-24 15:43:21,1
AMD,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,52
AMD,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
AMD,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
AMD,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
AMD,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,8
AMD,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,10
AMD,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,5
AMD,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,4
AMD,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,4
AMD,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
AMD,nspzeik,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,5
AMD,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
AMD,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,3
AMD,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
AMD,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,3
AMD,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,5
AMD,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,2
AMD,nsv64t7,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,2
AMD,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
AMD,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
AMD,nsyv727,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
AMD,ntimkr9,Back in the day you could overclock a 2600k from 3.4Ghz to 4.5Ghz on a $25 Hyper212 cooler. The performance gains were incredible as Sandy Bridge scaled very well at higher clocks.   Now days CPUs come overclocked already.,Intel,2025-12-11 19:25:58,29
AMD,ntifd3j,"""It's crazy to think that a cpu from 2009 can be easily overclocked.. 2.9Ghz to 4.1Ghz is crazy !""  You could overclock huge amounts on earlier generations - I used to run Pentium 4 1.6GHz chips at 3.2GHz on air-cooling, more on phase-change cooling.",Intel,2025-12-11 18:50:27,27
AMD,ntikk9s,"I ran my i5 750 2.67Ghz for years at 4Ghz without any issues. I benched it some at 4.2Ghz even, but it was not fully stable.  The X58 CPU are even better tho. And even if you had insane OC potential back in the days it was not as good as it sounds, since the turboboost was higher than the stock frequency that is listed.",Intel,2025-12-11 19:15:50,9
AMD,ntjwvoj,"Lol a 15 year old computer running Windows 11, meanwhile Microsoft telling people to upgrade 5 year old laptops for win10 being EOL.",Intel,2025-12-11 23:27:29,7
AMD,ntivoqo,X5690@4.6GHz on Rampage III Extreme 😘,Intel,2025-12-11 20:12:02,7
AMD,ntj26xa,it is crazy that intel sold you same technology at downclocked speeds to make a nice model range with different prices.,Intel,2025-12-11 20:45:29,5
AMD,ntiq1p0,Sick stuff. I still got my i7 930 at 4.2Ghz running just fine. These types of chips overclock like crazy.,Intel,2025-12-11 19:43:28,3
AMD,ntkwagl,Be nice. Give it another stick of ram!,Intel,2025-12-12 02:58:53,3
AMD,ntoxmhs,Q6600 G0,Intel,2025-12-12 19:02:37,3
AMD,ntiqlci,"Cool. Glad it worked for you. I have dual xeon server, maybe i should try it. But its production server dont wana break my apps. Lol",Intel,2025-12-11 19:46:15,2
AMD,ntjb06t,My 2500k did ~4.8 ghz and my 6950x did 5.2 ghz. Its base clock was like 3.2ghz and this was using 128GB of quad channel DDR4.  It was “stable”,Intel,2025-12-11 21:29:41,2
AMD,ntkz9ut,45nm is crazy in 2025,Intel,2025-12-12 03:16:12,2
AMD,ntosqvz,500W power draw when,Intel,2025-12-12 18:38:31,2
AMD,nthl3mn,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-12-11 16:23:16,1
AMD,ntk2ims,"I used to run my i3-540 at 4.2GHz, air cooled on what is effectively worse than a Hyper 212 Evo. I miss the old days when I could overclock the snot out of them. These days I guess they're binned to almost their max potential out of the factory so most of the time I'm undervolting them.",Intel,2025-12-12 00:01:05,1
AMD,ntk9tcx,Well done. Still using two H55m machines with OC (x3450 and i5 661).  They also OC decently at stock voltage keeping turbo and all power savings. My X3450 does 2.6 -> 3.3Ghz(3.8 turbo). The advantage is that it idles quite low at 50-60W.   But for gaming and rendering it's better to go all in as you did. Most chips can do anywhere from 3.8 to 4.2 all cores IME.,Intel,2025-12-12 00:44:00,1
AMD,ntl2d0n,nah my 40 logical processors would smash through it all  x2 xeon e5-2680 v2,Intel,2025-12-12 03:34:33,1
AMD,ntk4sw2,"is that better? I dont need to dive into setting anymore, the CPU maker do it for me with warranty.",Intel,2025-12-12 00:14:36,3
AMD,ntsgvaj,"There is still more to work with, especially if one does not fossilize on static all core OC, but does 2-step TVB fueled dynamic OC, Ecores are Aldo the source of much happiness on arrow",Intel,2025-12-13 09:30:18,1
AMD,ntja1e8,I miss overclocking. Felt like you were getting a bargain. Now I don’t even try.,Intel,2025-12-11 21:24:52,14
AMD,ntjnkj4,"Not as big an OC as yours, but I had a pre-built from FutureShop.  It was their home brand name.  Found a BIOS for the board that wasn’t theirs.  Managed to get 3.2GHz out of a 2.4GHz Pentium 4 on pre-built from FutureShop cooling.",Intel,2025-12-11 22:34:57,3
AMD,ntihjuu,"Wow, soo cool",Intel,2025-12-11 19:00:59,1
AMD,ntnudso,The motherboard doesn't accept other stick of ram. Only my corsair ram work,Intel,2025-12-12 15:49:11,1
AMD,ntmrk4p,How did you even get a 6950x to boot at 5.2ghz? Most of them hit a wall around 4.3ghz,Intel,2025-12-12 12:03:28,3
AMD,ntkag8u,"Has its ups and downs. Now that I'm older and have less time to tweak things and mostly just want shit to be stable, I see ""pre-overclocked with maybe 5% performance left on the table"" as a pro. The con is that chipmakers just jam a ton of power through it to make it happen, and the option of buying a half-price chip and spending an entire sleepless weekend tweaking it yourself to get 95% of the more expensive chip's performance is gone.",Intel,2025-12-12 00:47:48,3
AMD,ntjl0xd,"Same - the complexity and heat rose a lot and the gains because less significant - with multi-core chips and turbo frequencies there just isn't much headroom in them.  That and I work fixing issues with computers all day, I just want my own PC to work.",Intel,2025-12-11 22:21:04,7
AMD,ntlnjst,"Thats because these older CPUs were surprisingly energy efficient. Also mostly because now modern CPUs are powerful enough where overclocking is pointless. Even my i3-12100 being overclocked would be pointless, even if its only a 80 watt CPU",Intel,2025-12-12 05:56:24,2
AMD,ntwqjex,He couldn't without LN2.,Intel,2025-12-14 01:36:27,2
AMD,ntnxj8h,"It was short lived, over ~7 years I had to pull back the multiplier from 52 to 44 to keep it stable.  I retired the system this year.  It was a full open loop from EK.  2x Pascal Titan X in SLi",Intel,2025-12-12 16:04:28,0
AMD,ntmp3kk,"I used to overclock everything, now I undervolt everything lol",Intel,2025-12-12 11:44:00,2
AMD,nonhxk4,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is “slow” is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already “beating” AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Intel,2025-11-13 16:13:13,52
AMD,nootzxi,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Intel,2025-11-13 20:08:27,10
AMD,nrd4uj2,"I've gotten one and honestly it's amazing, easily the best laptop I've ever used so far.   I was skeptical about the battery life claims but I've genuinely found that using it for about 8 hours straight for coding, only drains the battery maybe 50%.  I've set it to only charge up to 80% max for battery health conservation, and I've regularly coded for 12 hours straight on the medium performance profile and haven't needed to charge until I got back home.  (This is for the Ultra 7 258v cpu variant btw)  Also this is while running Fedora with KDE Plasma which makes the battery life even more impressive as it's one of the heavier distros running cutting edge hardware and I've heard that Linux has less battery optimization compared to windows.    Screen isn't anything to write home about but the 100% srgb one looks good enough and is bright, 60hz looks kind of bad but I know that it saves a lot on battery.   Keyboard feels very nice as far as laptop keyboards go, having it be easily swappable is lovely as I wore out the keys on my old laptop, and I want this thing to last.   Linux hardware compatibility is perfect so far, even the fingerprint sensor works out of the box on fedora.   My only real complaint is that the plastic it is made out of is a major grease magnet and if I touch it without having immediately washed my hands, even if my hands weren't dirty, it'll leave dark patches from oils. Also it would be nice to have swappable RAM but I think 32gb ought to last a very long time anyway.   Genuinely seems like arguably one of the, if not the, best laptops for actually getting work done. Maybe it's not as fancy or sleek, but it just works. It's like the 2001 Toyota of the laptop world, it's not winning prizes for looks, but it'll never die, gets good mileage (battery life), and is easily repairable. Maybe not the laptop you want, but definitely the one you need (excluding people who need something like a dedicated GPU or really need super high CPU performance).",Intel,2025-11-29 11:19:13,5
AMD,noon81e,"Intel beats AMD in software (drivers, firmware) … I got think pad 780M laptop by company I work for. Randomly display won’t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Intel,2025-11-13 19:34:25,12
AMD,non7ozt,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Intel,2025-11-13 15:23:02,7
AMD,nom9a0s,lol. Even in the cons it says weaker multicore than AMD….?   This article seems like AI wrote it,Intel,2025-11-13 11:53:59,0
AMD,nr8651t,"Unfortunately Intel abandoned the on-package RAM after Lunar Lake again, which is the primary reason for the great efficiency and low power usage. I kind of understand why, it's expensive and not very flexible, plus apparently the market doesn't actually care that much about long battery runtimes. Only a small minority of people are ready to pay premium for this.",Intel,2025-11-28 15:11:41,1
AMD,nopvuqn,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Intel,2025-11-13 23:25:31,0
AMD,nonh9ew,Suck at gaming.,Intel,2025-11-13 16:09:54,-15
AMD,notgml4,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Intel,2025-11-14 15:00:36,8
AMD,noy6f36,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Intel,2025-11-15 08:22:42,7
AMD,npap4we,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Intel,2025-11-17 10:42:37,3
AMD,ntz6joo,Isn’t the keyboard one of the most important characteristics?,Intel,2025-12-14 13:39:49,1
AMD,nopn323,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Intel,2025-11-13 22:36:30,-1
AMD,ntz6z28,"Hey I’m looking at the exact same laptop that you have. Can you tell me about the build quality and if there’s any keyboard flex when pressing down on it? Please tell me. I’m going to use it for word, excel, reading lots of pdf files and ebooks and watch movies. Will it be enough for that?",Intel,2025-12-14 13:42:38,1
AMD,nop3ehp,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Intel,2025-11-13 20:56:10,4
AMD,nongtqn,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Intel,2025-11-13 16:07:47,4
AMD,nousjfs,Soldered ram is a lot faster. So no.,Intel,2025-11-14 18:58:54,2
AMD,noniq16,Yes but now RAM costs a ton of money,Intel,2025-11-13 16:17:05,0
AMD,nomcmkj,Is multicore performance the only consideration when buying a laptop?,Intel,2025-11-13 12:19:56,27
AMD,nqyoc8i,What kind of issues?,Intel,2025-11-26 23:00:59,2
AMD,ntz73fr,What kind of issues with Intel? I thought it was the AMD that had tons of issues,Intel,2025-12-14 13:43:27,1
AMD,nonhqb5,It is not a gaming laptop,Intel,2025-11-13 16:12:13,18
AMD,np9o16h,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Intel,2025-11-17 04:44:52,2
AMD,np3siex,It's an enterprise grade product you buffoon.,Intel,2025-11-16 06:03:41,5
AMD,npd9987,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Intel,2025-11-17 19:32:27,2
AMD,np8gg6z,Build quality.    Thinkpads are solid machines that are easy to fix.    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Intel,2025-11-17 00:13:54,1
AMD,ntzg92j,"So build quality will be subjective, from what I can tell, it's got very good build quality in terms of ""real"" factors such as durability. But it definitely feels less ""premium"" than similarly priced consumer grade laptops. The plastic is plastic so it will flex a little bit, but the parts all seem very well put together and it does feel ""solid"" overall.   I haven't really noticed keyboard flex, but I have noticed a slight amount of flex where my palms rest, particularly on the right side, where the smart card reader is, which makes sense as it is just a big hole in the side of the laptop. I plan on getting a dummy smart card to fill the gap and hopefully that should reduce it.   Overall whilst the internal chassis is metal, the outside is just plastic. I imagine that is good for durability, as it ought to be able to absorb shocks, but, as I said, it definitely makes it feel less ""premium"". They key press feel of the keyboard definitely does feel very nice as far as laptops go though. Obviously it's still nothing compared to a good mechanical keyboard but for a laptop it's very nice.   I bought this laptop for longevity and durability, so given that It's only just come out, I can't really say much about that, but the prestige of thinkpads of previous generations kind of speaks to their reliability. Plus it's apparent that they are still quite easy to repair and Lenovo has video guides on replacing loads of the parts.   And for your use case the battery life should be very good. It seems the Intel chip was designed to be very efficient during periods of downtime and something like viewing a PDF or editing a document has a LOT of downtime for the CPU",Intel,2025-12-14 14:40:19,2
AMD,norwnxs,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Intel,2025-11-14 07:46:09,5
AMD,noruygl,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Intel,2025-11-14 07:29:24,3
AMD,noxc6wn,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Intel,2025-11-15 03:57:11,2
AMD,nop71cl,All the more reason to make it upgradable,Intel,2025-11-13 21:14:31,-2
AMD,nomhldl,Lunar Lake isn’t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Intel,2025-11-13 12:55:02,15
AMD,nomp84g,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Intel,2025-11-13 13:42:02,5
AMD,nu0g7o7,This was back when the 14th generation were having issues.,Intel,2025-12-14 17:47:28,2
AMD,nonivqb,"It's $2,000 so no excuse.",Intel,2025-11-13 16:17:52,-10
AMD,nu6fuik,Thank you so much for this valuable and comprehensive information! I really appreciate it:),Intel,2025-12-15 16:29:10,1
AMD,npd8o9g,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Intel,2025-11-17 19:29:33,3
AMD,nomwgxm,"Lunar Lake already beat AMD, nobody buys AMD laptops",Intel,2025-11-13 14:23:36,19
AMD,non5ael,i stopped at $2100 for a Thinkpad T14,Intel,2025-11-13 15:10:49,1
AMD,nu0kv9z,"Ah okay, got it thanks.",Intel,2025-12-14 18:10:16,1
AMD,nowos5a,wrong  Nobody Supply AMD laptop     There fixed for u,Intel,2025-11-15 01:22:32,2
AMD,nov79aa,"I do, and many of the people I know do.",Intel,2025-11-14 20:14:59,-1
AMD,nonhh1g,Nobody pays that much.,Intel,2025-11-13 16:10:57,7
AMD,np79214,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Intel,2025-11-16 20:19:32,-1
AMD,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,12
AMD,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,4
AMD,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
AMD,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,2
AMD,nn1205o,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Intel,2025-11-04 08:46:02,2
AMD,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
AMD,np6680l,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Intel,2025-11-16 17:04:39,1
AMD,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
AMD,nmg20dw,Oh... for sure 😁,Intel,2025-10-31 22:15:08,3
AMD,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
AMD,nn1h3l3,Great work dude! Only 200MHz to go 😉,Intel,2025-11-04 11:15:21,2
AMD,nmilk0q,Car coolant in the freezer 😁,Intel,2025-11-01 11:12:18,2
AMD,np782zx,That's the way! Let us all know the results.,Intel,2025-11-16 20:14:39,1
AMD,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,2
AMD,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
AMD,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
AMD,np7d3w5,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Intel,2025-11-16 20:40:17,1
AMD,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
AMD,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
AMD,npa5wyd,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Intel,2025-11-17 07:21:57,1
AMD,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
AMD,ngieos7,And largely against the non-x3d lmfao.,Intel,2025-09-27 17:21:03,81
AMD,ngif1q6,Aren't they just showing that AMDs CPUs are better for gaming?,Intel,2025-09-27 17:22:52,30
AMD,ngmmadi,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Intel,2025-09-28 10:33:19,2
AMD,ngiqxv3,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Intel,2025-09-27 18:23:57,-16
AMD,ngp4a4b,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Intel,2025-09-28 18:56:21,0
AMD,nglqard,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Intel,2025-09-28 05:26:07,33
AMD,ngiw9gz,I assume they compared with CPUs in a similar price range,Intel,2025-09-27 18:52:05,34
AMD,ngl774g,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Intel,2025-09-28 02:59:30,6
AMD,ngj2d2a,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Intel,2025-09-27 19:24:33,52
AMD,ngmt8qo,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Intel,2025-09-28 11:35:30,10
AMD,ngn0xy1,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Intel,2025-09-28 12:33:09,2
AMD,ngix2qg,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Intel,2025-09-27 18:56:22,32
AMD,ngiy5wa,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Intel,2025-09-27 19:02:07,9
AMD,ngir4n9,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Intel,2025-09-27 18:24:57,4
AMD,ngj6exq,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Intel,2025-09-27 19:46:01,3
AMD,nhi1lee,"Now install windows 11, lol",Intel,2025-10-03 06:08:42,1
AMD,nglqeum,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Intel,2025-09-28 05:27:08,1
AMD,ngkj8l9,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Intel,2025-09-28 00:25:26,13
AMD,ngkwq9d,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Intel,2025-09-28 01:51:37,19
AMD,ngmy2qz,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Intel,2025-09-28 12:12:58,1
AMD,ngmx0bu,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Intel,2025-09-28 12:05:06,1
AMD,ngn1g9m,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Intel,2025-09-28 12:36:42,2
AMD,ngiyobq,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Intel,2025-09-27 19:04:51,-5
AMD,ngizavn,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Intel,2025-09-27 19:08:11,1
AMD,ngjh5f6,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Intel,2025-09-27 20:42:11,3
AMD,nhk7a3s,I did same performance on all processors.,Intel,2025-10-03 15:24:53,1
AMD,ngm7i8q,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Intel,2025-09-28 08:05:53,5
AMD,ngkykmi,That sounds like an AMD Stan argument circa 2020,Intel,2025-09-28 02:03:12,19
AMD,nh261w2,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Intel,2025-09-30 19:23:27,1
AMD,ngn25ws,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Intel,2025-09-28 12:41:35,0
AMD,ngjdprn,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Intel,2025-09-27 20:24:13,11
AMD,ngj1pgu,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Intel,2025-09-27 19:21:01,5
AMD,ngkqt0z,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Intel,2025-09-28 01:12:59,1
AMD,ngj49y7,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Intel,2025-09-27 19:34:44,3
AMD,ngk1b4v,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Intel,2025-09-27 22:35:49,3
AMD,nhka12e,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Intel,2025-10-03 15:38:08,1
AMD,ngmkwf8,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Intel,2025-09-28 10:19:51,10
AMD,ngnw7zu,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Intel,2025-09-28 15:28:44,5
AMD,ngl05xq,Expand ?,Intel,2025-09-28 02:13:17,-2
AMD,nh27g09,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Intel,2025-09-30 19:30:10,1
AMD,ngtvgdv,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Intel,2025-09-29 14:03:26,6
AMD,ngw45vk,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Intel,2025-09-29 20:34:34,0
AMD,ngjg69y,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Intel,2025-09-27 20:37:03,4
AMD,ngqfmrw,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Intel,2025-09-28 22:53:49,1
AMD,ngk26op,did you do it,Intel,2025-09-27 22:41:09,1
AMD,ngmi765,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Intel,2025-09-28 09:53:01,1
AMD,ngk22ke,can you reset settings then choose ray tracing ultra preset.,Intel,2025-09-27 22:40:28,2
AMD,nhle3iw,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Intel,2025-10-03 18:52:42,1
AMD,ngnx0bd,because they exclusively exist in DIY build your pc enthusiast bubble,Intel,2025-09-28 15:32:30,4
AMD,nhvyteo,Pricing was aggressive. A 12 core 3900x was 400 usd.,Intel,2025-10-05 13:26:56,1
AMD,ngl3zfu,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Intel,2025-09-28 02:38:06,14
AMD,nh2848m,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Intel,2025-09-30 19:33:29,1
AMD,ngtxj1d,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Intel,2025-09-29 14:14:15,3
AMD,ngmif3t,"Okay, I did it",Intel,2025-09-28 09:55:13,2
AMD,ngmglra,"No, I didn’t remember good",Intel,2025-09-28 09:37:10,1
AMD,ngk41a9,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Intel,2025-09-27 22:52:22,2
AMD,nhp053x,Thanks for solidifying opinion that your benchmarks are fake,Intel,2025-10-04 10:05:07,1
AMD,nh2a34y,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Intel,2025-09-30 19:43:15,1
AMD,ngk5zrq,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Intel,2025-09-27 23:04:17,2
AMD,nhp2exv,"What’s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If you’re comparing score screenshots to OBS-recorded videos, you shouldn’t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. I’ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think they’d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since I’ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Intel,2025-10-04 10:27:16,1
AMD,ngbpsza,Cam someone confirm or is this gas lighting?,Intel,2025-09-26 15:51:04,21
AMD,ngbym0c,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Intel,2025-09-26 16:33:40,16
AMD,nghesqk,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Intel,2025-09-27 14:17:43,4
AMD,ngc7w1b,Intel comeback real?,Intel,2025-09-26 17:18:16,7
AMD,ngf1ik5,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Intel,2025-09-27 02:43:51,2
AMD,ngbsck8,3D v-cache has entered the chat.,Intel,2025-09-26 16:04:06,7
AMD,ngbr9eb,Take it as a grain of salt. Intel marketing LOL,Intel,2025-09-26 15:58:30,2
AMD,nh5ixeo,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Intel,2025-10-01 08:33:03,1
AMD,ngfguk7,Thats cool ...but lets talk about better pricing.,Intel,2025-09-27 04:36:12,1
AMD,ngfrla1,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Intel,2025-09-27 06:09:56,0
AMD,ngc573e,Tech Jesus has entered chat :).,Intel,2025-09-26 17:05:31,-10
AMD,ngcjbbq,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Intel,2025-09-26 18:13:09,-11
AMD,ngbqjhe,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Intel,2025-09-26 15:54:51,39
AMD,ngdvx9l,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Intel,2025-09-26 22:23:43,12
AMD,nge3sfi,What do you mean by gaslighting in this case?,Intel,2025-09-26 23:10:51,4
AMD,ngcf9aj,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Intel,2025-09-26 17:53:19,1
AMD,ngcutw5,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Intel,2025-09-26 19:10:03,1
AMD,ngl3adb,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Intel,2025-09-28 02:33:31,1
AMD,ngdfut5,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Intel,2025-09-26 20:55:42,1
AMD,ngbxbws,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Intel,2025-09-26 16:27:28,-6
AMD,ngfqkoh,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Intel,2025-09-27 06:00:33,2
AMD,ngfebe1,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Intel,2025-09-27 04:16:03,2
AMD,ngfqbry,Nova Lake bLLC about to ruin Amd X3D party.,Intel,2025-09-27 05:58:18,2
AMD,ngc2ju0,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Intel,2025-09-26 16:52:57,17
AMD,ngc2czl,I always wondered if Intel marketing budget is higher than the R&D budget,Intel,2025-09-26 16:52:01,-6
AMD,ngfrkpn,Intel Arrow Lake is much cheaper than Amd Zen 5.,Intel,2025-09-27 06:09:47,3
AMD,ngemp1j,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Intel,2025-09-27 01:07:45,7
AMD,ngeo8em,only an AMD fan would worry about replacing their shit CPUs under 3 years,Intel,2025-09-27 01:17:36,0
AMD,ngbzwzr,Hardware unboxed isn't a reliable source.,Intel,2025-09-26 16:40:06,8
AMD,ngf1ob8,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Intel,2025-09-27 02:44:54,10
AMD,ngealuz,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Intel,2025-09-26 23:51:44,1
AMD,nge8xbh,Telling people that its performance is better than it actually is?,Intel,2025-09-26 23:41:39,3
AMD,ngca7el,The ones with similar pricing not performance,Intel,2025-09-26 17:29:11,8
AMD,ngigkrj,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Intel,2025-09-27 17:30:41,3
AMD,ngfrgqn,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Intel,2025-09-27 06:08:45,0
AMD,ngerbdr,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Intel,2025-09-27 01:37:32,2
AMD,ngezf04,Quite common for AM4 in my experience.,Intel,2025-09-27 02:29:45,1
AMD,ngihhii,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Intel,2025-09-27 17:35:17,1
AMD,ngerg53,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Intel,2025-09-27 01:38:23,0
AMD,nggftxh,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Intel,2025-09-27 10:08:25,-1
AMD,ngeozwu,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Intel,2025-09-27 01:22:31,-1
AMD,ngg1fuo,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Intel,2025-09-27 07:42:27,-1
AMD,ngerrz8,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Intel,2025-09-27 01:40:28,10
AMD,ngeao9a,Sooo they are in the YouTube space for the money not for the love of tech,Intel,2025-09-26 23:52:08,5
AMD,ngfq1bg,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Intel,2025-09-27 05:55:39,2
AMD,ngdp9bd,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Intel,2025-09-26 21:45:51,-2
AMD,ngc0yus,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Intel,2025-09-26 16:45:15,11
AMD,ngeb3z7,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Intel,2025-09-26 23:54:48,7
AMD,ngmlzg8,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2025-09-28 10:30:22,1
AMD,ngtp3t3,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Intel,2025-09-29 13:28:22,1
AMD,nh5i8gf,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Intel,2025-10-01 08:25:38,1
AMD,nh5il7n,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Intel,2025-10-01 08:29:26,1
AMD,ngc36bx,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Intel,2025-09-26 16:55:54,4
AMD,ngcbde9,Sure but charts seem about right to me,Intel,2025-09-26 17:34:45,1
AMD,ngcxbac,APO is game specific. I'm referring to what has changed overall.,Intel,2025-09-26 19:22:34,2
AMD,nil3hc6,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Intel,2025-10-09 12:28:52,2
AMD,nimk9vp,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Intel,2025-10-09 17:05:36,2
AMD,njlc3cu,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Intel,2025-10-15 09:13:26,1
AMD,nkp7gzk,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Intel,2025-10-22 00:53:17,1
AMD,nksv7pa,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Intel,2025-10-22 16:17:48,1
AMD,nm887uc,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Intel,2025-10-30 17:38:48,1
AMD,nmj83z4,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Intel,2025-11-01 13:55:37,1
AMD,nnc1z1l,"Hi, I’m at my wit's end with my build and would really appreciate some advice.  My PC has been plagued by random crashes, CRC errors, and installation failures for months:  - Random application crashes, often citing `KERNELBASE.dll`, `ntdll.dll`, or `ucrtbase.dll`. - Frequent CRC errors when extracting large ZIP or RAR files. Retrying usually works. - Software installations fail with data corruption or unpacking errors, only to succeed when I try again. - Games crash or stutter randomly. - Very rare BSODs.  Specs:  - **CPU**: Intel Core i9-13900K - **Motherboard**: ASUS ROG STRIX Z790-H GAMING WIFI - **RAM**: 32GB (2x16GB) G.Skill Trident Z5 RGB (DDR5-6400 CL32, Model: F5-6400J3239G16GX) - **GPU**: MSI RTX 4090 Gaming X Trio - **Storage (OS Drive)**: Crucial P5 Plus 2TB NVMe SSD - **OS**: Windows 11   This is what I've already tried (everything passed):  *  **Memtest86:** Completed multiple passes with no errors. *  **Prime95 & Intel XTU:** Seems to be stable. *  **FurMark:** GPU stress test is stable. *  **Storage Health:** All drives pass SMART and manufacturer-specific self-tests. *  **System Integrity:** `sfc /scannow` and `DISM /RestoreHealth` complete successfully. *  **Updates:** All drivers, firmware, BIOS, and Windows are fully up-to-date. *  **Physical:** Cleaned the case, re-seated components, and replaced thermal paste. * **XMP**: I've disabled/re-enabled XMP multiple times, doesn't make a difference.  I have trouble finding a consistent way of reproducing the issue. Today I tried a 7-Zip benchmark which failed once with a ""decoding error"", but I wasn't able to reproduce it afterwards. I couldn't get Intel Processor Diagnostic Tool to fail after multiple hours. So...  *   **Is this a memory instability issue?** Could the RAM be faulty?  *   **Or, could this be a faulty CPU core?** I found [this post](https://www.reddit.com/r/intel/comments/15mflva/tech_support13900k_problems_when_using_multiple/) where a user had identical symptoms (CRC/7-Zip errors) that were only resolved by replacing a faulty 13900K, even with XMP off.  Thanks in advance for any help.",Intel,2025-11-06 00:15:38,1
AMD,noppamh,"# Alienware 34 - AW3425DWM resolution issues  [](https://www.reddit.com/r/ultrawidemasterrace/?f=flair_name%3A%22Tech%20Support%22)  I just got the AW3425DWM, and my laptop is a Dell Inspiron 15 5510, which is not a gaming laptop. I'm not a gamer.  When connecting through the HDMI port on the laptop and monitor, I can't set the resolution to 3440x1440; I can go up to 3840x2160, but not the monitor's native resolution. However, when I connect through the USB-C port on the laptop to the DisplayPort on the monitor, I can set the resolution to 3440x1440 without any issues. The downside is that I've lost the only Thunderbolt port I have available.  Is there a workaround for this issue? If I use an HDMI to DisplayPort adapter, will I be able to set the resolution to 3440x1440?  I understand that the HDMI 1.4 port usually can handle 21:9 resolutions, but with the latest Intel drivers, it isn't giving me the option for 3440x1440",Intel,2025-11-13 22:48:26,1
AMD,noqvatt,"Hi, can someone help me please? I'm trying to generate a video on AI playground after installing it but it just keeps loading for videos and images don't show up. That's my it it'll specs:   Processor: Intel(R) N95 (1.70 GHz) Installed RAM: 32.0 GB (31.7 GB usable) System Type: 64-bit operating system, x64-based processor Graphics Card: Intel(R) UHD Graphics   -That's a part of what's in the console:    No key found for setting negativePrompt. Stopping generation oa @ index-fOc02QH8.js:61 w @ index-fOc02QH8.js:61 await in w V @ index-fOc02QH8.js:22 pt @ index-fOc02QH8.js:61 await in pt V @ index-fOc02QH8.js:22 c @ index-fOc02QH8.js:61 await in c Il @ index-fOc02QH8.js:14 Sr @ index-fOc02QH8.js:14 n @ index-fOc02QH8.js:18 index-fOc02QH8.js:61 uploadImageName b99fb28ea4440a2f51ce53cd5c529554e5965b66f5f5a8506b9b973b66e754bd.png index-fOc02QH8.js:251 [comfyui-backend] got prompt (anonymous) @ index-fOc02QH8.js:251 (anonymous) @ VM5:2 emit @ VM4 sandbox_bundle:2 onMessage @ VM4 sandbox_bundle:2 index-fOc02QH8.js:61 updating image {id: '6eb88f1b-f433-4541-a421-40619ac9fdc2', imageUrl: 'data:image/svg+xml,%3C%2Fpath%3E%3C%2Fsvg%3E', state: 'generating', settings: {…}, dynamicSettings: Array(3)}       With: RuntimeError: UR error     (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:251 [comfyui-backend] Prompt executed in 116.86 seconds   (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:61 executing Object",Intel,2025-11-14 02:55:16,1
AMD,nq4im71,Will XeSS 3 and Intel multi framegen be available for Iris xe graphics igpus?,Intel,2025-11-22 01:57:26,1
AMD,nq9ltvl,"Putting together a 4k gaming 5090 machine, deciding between the 285k or 265k. Does the extra L3 cache of the 285k make any difference, or does the ~5ns less latency of the 265k make more of a difference?   I plan on a 2dimm board to push memory and OC slightly, but nothing crazy.",Intel,2025-11-22 22:42:16,1
AMD,nqoo97z,"Hi all,  I ordered a contact frame from Thermal Grizzly because the temperature of my CPU wasn’t great, and I was tired of the fan noise.   I installed the contact frame in August, and everything was fine until two weeks ago, when the desktop suddenly stopped booting. The fans were spinning, but there was no POST, no debug LED — nothing. I thought the motherboard was dead.   However, when I removed the AIO head and tried to boot again, it worked!  Since then, I’ve been experiencing intermittent no-POST issues, especially after gaming sessions.   To get it to boot, I always have to adjust the screws on the AIO head. If it’s too tight: no POST. If it’s too loose: CPU temperatures are high. For example, in Hogwarts Legacy, I get around 60 FPS with an RTX 4090, while the CPU runs at around 18% usage @ 85°C. Which is not normal at all, as I used to run the game at around 100–110 FPS.  I tightened the screws on the contact frame using my thumb and index fingers.   I’m looking for advice from anyone who has encountered a similar issue, because at this point my only idea is to remove the contact frame and reinstall the original one.  For context, my setup is from November 2022, and I installed the contact frame in August 2025. I have never removed the CPU from the socket since the first installation.  Thank you all.",Intel,2025-11-25 10:53:08,1
AMD,nr6yiuz,Anyone know why the performance of my 13700k is so much better when using a pre micro code bios on a z790 board? This even after applying the latest micro code update through windows,Intel,2025-11-28 09:31:06,1
AMD,nsz9e66,"My hp elitebook 830 g8 notebook, had some problems booting up and it shows blinking lights (caps key) and it keeps trying to boot up without success, but i after getting it repaired now it boots up but sometimes it still does the same thing and boots up after awhile  I was already using the latest versions of the BIOS and ME firmware, and trying to download and update them again did nothing. However, avter updating the BIOS (to the same version), it showed a message saying ""HP Sure Start detected that the Intel Management Engine Firmware is corrupted"", but it only did so once.  What it does consistently is it restarts once or twice when i boot it. It shows the logo, then turns off, and then boots normally.  In the BIOS, it shows in system information that ""ME Firmware Mode: Recovery Mode"".  Windows also takes much longer to boot than usual, can take up to a full minute.  I can recall that the whole system was super slow at some point, but that was before I started diagnosing any of these details. It works fine now after boot.  Tried all sorts of things, installing ME drivers doesnt seem to do anything, and IDK what to do now.  help",Intel,2025-12-08 18:41:34,1
AMD,ntcs1o8,"My processor with integrated 11th Generation Intel graphics (Intel Core i3-1115G4) completely lost Vulkan support after installing the latest version, 7080 (coming directly from the previously installed version, 6987). I'm testing the drivers one by one, but it looks like Vulkan support has been completely removed. Did this only happen with my processor, or with all 11th Generation processors?",Intel,2025-12-10 21:00:34,1
AMD,ntn8xto,"I'm on Win11 with Killer WiFi 7 BE1750x 320MHz Wireless Network Adapter. PC keeps crashing/restarting. Here is an error:  The computer has rebooted from a bugcheck.  The bugcheck was: 0x00020001 (0x0000000000000011, 0x0000000000210720, 0x0000000000001005, 0xffffe700010059a0). A dump was saved in: C:\\WINDOWS\\Minidump\\121225-18625-01.dmp. Report Id: 189a9fba-5969-4b6c-8199-d8b6a03a1a34.  Copilot had me disable all Killer services except Killer Network Service, but the issue persists. Now it tells me to uninstall the drivers and block them from reinstalling and just use the generic drivers.",Intel,2025-12-12 13:56:20,1
AMD,ntoe22w,"I recently bought a 2025 LG Gram 17 (Intel 258V + 140V). Great laptop for productivity tasks which is its primary use for me. I might occasionally game on it if I'm traveling. I only really play Final Fantasy XI these days which was released 2003 so this processor has no problem running it at max settings and \~60 FPS even in Silent mode.  That's when it is plugged in though.  As soon as it is unplugged the performance drops like 80%. I don't mind the performance hit when I'm doing productivity tasks, but would like control over it when I want.  I've tried various things to improve this while on battery:  * Making sure it is designated as ""High Performance"" in Windows. * Making sure Advanced Power settings are the same between plugged-in and battery. * Making sure in Intel Graphics Command Center that Display Power Savings are off. * Making sure in My gram that performance is set to high. * Changing anything anywhere I can find that might be limiting performance on battery.  I haven't started digging through the BIOS yet and don't know that I will. At that point it is too much of a hassle as opposed to some quick toggle(s) in the OS.  Any other places I should be checking or suggestions to address this related to the CPU itself?",Intel,2025-12-12 17:25:39,1
AMD,nttrt4h,"Hey everyone, looking for second opinions because this behavior doesn’t seem normal.  Specs:  CPU: Intel i5-14600KF  Cooler: Scythe Fuma 2 (dual tower, dual fan) with oficial LGA 1700 mounting kit.  Motherboard: ASUS TUF Gaming B760M-PLUS WiFi II  Case airflow is fine.  Darkflash DLX case with 9 fans.  Ambient temp \~25–30°C (Brazil)  Before this build I had a Ryzen 7 5800X on an ASUS TUF board with the same case and airflow, and temps were completely normal. No overheating issues like this.  but now, I'm facing the following:  Idle temps sit around 60–75°C  Any moderate load (opening anything, even the bios) causes instant spikes to 95–100°C. When acessing the Bios, the temp shown is 78-88°C  CPU thermal throttles immediately  Heatsink stays barely warm, even when CPU reports 95–100°C  Fans ramp up correctly  I’ve tried:  Re-mounted the Scythe Fuma 2  3 times (check [https://imgur.com/a/ehBnxLn](https://imgur.com/a/ehBnxLn) for how the termal paste was when I remounted  the last time)  Re-mounted the CPU.  Checked power limits  Limited PL1/PL2 to 65W → temps drop but CPU becomes extremely slow, 1˜3GHZ, but still hitting 50-60ºC on idle and 80-90ºC on the rest, 88 on the bios as well.  Undervolted –0.06V, didn't solve.  Has anyone else seen this behavior with 14600KF + ASUS B760 TUF boards?  My friend (chatgpt) recommended a contact frame and said that it would fix it. What do you think about this?  Any input appreciated. Thanks!",Intel,2025-12-13 15:34:32,1
AMD,nupcpsu,"This isn't tech support, but my thread got locked and the mods said to post it here.  My 14700k is starting to give instability symptoms, so I opened a ticket with Intel.  The next day, I got an email asking for more information and was told they would call yesterday if they hadn’t received it. I replied to the email with information and my order confirmation, but as of 4:00 yesterday, I didn’t get a reply.  So, I went into the ticketing system and didn’t see my reply. I added a new comment with the information from my email and received almost an immediate email from the support person granting the RMA.  He asked for shipping information and if I’d choose option 1 or 2. For option 2, he asked for my agreement to the process and my billing address, name on the credit card, and expiration. He said he’d call to get the credit card number.  I replied to the email and pasted the information in my ticket. Oddly enough, I didn’t see his second response in the ticket.  Is that really how this goes down? There’s not an order system where I can input my credit card information? I have to wait for a call from some random support person and give him my credit card number over the phone?",Intel,2025-12-18 16:06:09,1
AMD,nv6u3b9,"Hi,  I’m trying to register my account for claiming master game key card, but when i put my phone number to complete registration it always “phone number unreachable”.   I already submitted the ticket for support, i worry about not get any replies because of xmast and new year holiday.   Can you help me to complete my registration, i just want to claim the game and play in peace.   I tried to look for solution but nothing have same problem with me ? Is there any possibilities that on weekend the system can’t use A2P SMS for Indonesia Region",Intel,2025-12-21 12:36:51,1
AMD,nvbb6wv,"Intel ARC b580 apparently unable to run games in the snowdrop engine, but in such a way that it's almost impossible for benchmarkers/testers to catch.  I've been trying to run Avatar Frontiers of Pandora on my arc b580 for over a year now, and I keep having the same extremely unusual crash.  If I freshly reinstall the game, or change my operating system, or switch arc driver version, I can play the game normally, at regular framerates, with no issues, for around an hour. MAYBE two if I'm lucky.   The game then crashes, with seemingly no trigger. No thermal issues, no unusual resource usage, no memory leak, no framerate issues, no stuttering, not even a crash report. The game will just close as if I had closed it manually.   I don't crash in the same location every time, it crashes with different game settings, xess enabled/disabled, different monitors, different ram configurations. I can have the framerate capped or uncapped. GPU maxed out or not, power draw high or low. I can't consistently recreate the crash in the same way every time. The only certainty is that it WILL crash eventually.  After crashing once, It will then crash on game launch, every single time. The game opens, the epilepsy warning flashes up for a second or two, and then the game crashes in the same way, with no freeze, no crash report, just the game closing.  Since the game runs fine for an hour or so after I first install it, any benchmarker testing the gpu will likely never see the crash, because they install the game, run benchmarks, do their tests, and then close it before seeing anything wrong.  The following are fixes that I have tried, and have NOT worked.  * Updating graphics drivers (to both the latest absolute driver, and latest WHQL certified driver). * Downgrading graphics drivers (going all the way back to the first game ready drivers for the b580). * Updating windows (latest windows 10 and latest windows 11 versions). * Running a memory diagnostic * Enabling/disabling XMP. * Verifying integrity of game files. * Forcing the game to run in the Vulkan engine instead of snowdrop. * Performing a complete reinstall of the game. * Replacing one of the .dll files with a ""repaired"" one. (this apparently fixed a crash for some people). * Forcing the game to boot in dx11 instead of dx12.  The following are fixes that I have tried, and HAVE worked.  * Running the game on my AMD integrated graphics.  Obviously running the game on an igpu doesn't offer playable performance, but the game will run, and I've yet to find any crash, even leaving the game open for several hours.  Due to this issue, I'm on the verge of selling my b580 and replacing it with a different card from another manufacturer that hopefully won't have the same issue.  Has anyone had a similar issue and knows the fix? Although I suspect the issue has to be resolved with a driver update.",Intel,2025-12-22 03:46:16,1
AMD,nw4g1zt,VR Support for ARC GPU? :),Intel,2025-12-27 02:03:19,1
AMD,nwenzma,"Hello, I'm experiencing an intermittent but severe system hard freezes on a prebuilt PowerSpec G441 desktop purchased from Micro Center 2 years ago. The freezes occur during gaming, mixed workloads (gaming + video playback), remote desktop usage (TeamViewer), and occasionally shortly after logging into Windows with no workload running.  When my PC hard freezes:  \- Mouse is unresponsive  \- Keyboard is unresponsive  \- No audio sound  \- No BSOD or error message     My PC does not recover and requires a manual power button reboot.  After a forced reboot, Windows loads normally and allows me to log in but the system may freeze again within 1–2 minutes, even at idle. Other times, the system may run normally for days before the issue reappears.  No relevant error logs are generated at the time of the freezes. Event Viewer only shows Kernel Power events related to the forced shutdown.  So far I've tried all possible solutions to resolve my issue:  \- Updated motherboard BIOS to the latest available version  \- Updated Windows 11 fully and also reinstalled Windows 11 through USB  \- Performed clean GPU driver reinstallations  \- Verified CPU and GPU temperatures (normal under load)  \- Ran hardware stress tests and diagnostics (CPU, GPU, memory, storage) and no failures detected  \- Tested each monitor individually (dual monitor setup)  \- Swapped HDMI/DisplayPort cables and ports  \- Tested my PC without background applications  \- Returned my PC to stock settings (no overclocks, no undervolts, no XMP changes beyond default)  I've also took my pc to a local repair shop, which they could not reproduce the issue but suggested it may be CPU or PSU related issue. However, when I took my PC to Micro Center, technicians ran similar diagnostics and stress tests but were also unable to reproduce the freezes. They have also tested the PSU voltages and it passed. Despite extensive testing, the issue remains unresolved and continues to occur consistently now.  Specs:  Operating System: Windows 11 Pro 25H2 (Build 26200.7462)  CPU: Intel Core i7-13700KF (stock settings)  GPU: NVIDIA GeForce RTX 3070 Ti 8GB (stock)  Motherboard: MSI PRO Z690-A WIFI (MS-7D25), BIOS v5.32  RAM: 32GB (2×16GB) DDR5-5600 (G.Skill)  Storage: WD Blue SN570 1TB NVMe SSD  Cooling: NZXT Kraken 240mm AIO liquid CPU cooler  PSU: 750W 80+ Gold (PowerSpec OEM, included with prebuilt)  Display: Dual monitor setup, tested individually  I gave a call to Intel and now waiting for a call back. Thank you and I appreciate any guidance!",Intel,2025-12-28 18:50:36,1
AMD,nwjovjk,"Does my 12700f support TME?  In specs, there's no single line about TME. 12700's specs declare TME support. Processor Identification Utility does not mention TME, `hwinfo64` shows TME in gray, and `cpuid -1 | rg TME` prints `TME: Total Memory Encryption = false`. There can i check whether my cpu support TME or not?",Intel,2025-12-29 14:08:35,1
AMD,nwkfoww,"I have an i5-13600kf and a asus strix b760-i motherboard and mostly it runs fine but in specific games I get random shutdowns. Most commonly hell divers and now expedition 33.  I found the vmin instability issue and patched my bios to the latest version now but I still get it happening. Is there any suggested settings changes that have been found to help or is the only course to look at an RMA? I’d rather not be out of a PC for a while so trying to seek help to avoid that if possible.  What’s I find odd is I never get any errors or BSOD. Just the monitor goes black and the system locks up, the fans are still running but it has to be shutdown by holding the power button.",Intel,2025-12-29 16:25:47,1
AMD,niosucw,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-10 00:13:23,1
AMD,niowtna,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-10-10 00:37:02,1
AMD,nicof1i,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-08 01:46:19,2
AMD,njq2uvf,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Intel,2025-10-16 01:31:17,1
AMD,nkq87np,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Intel,2025-10-22 04:59:31,1
AMD,nkxufey,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Intel,2025-10-23 11:35:54,2
AMD,nkwr1mw,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Intel,2025-10-23 05:27:23,1
AMD,nmj7ctn,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Intel,2025-11-01 13:51:07,1
AMD,nmsah2r,"u/vincococka ,Yes you can use that memory kit safely as long as you don’t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of Intel® Core™ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Intel,2025-11-02 22:47:21,1
AMD,nnd464j,"u/SuperV1234 If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor  need a replacement.",Intel,2025-11-06 04:10:00,1
AMD,np77ndt,"u/triptoasturias Before I share any recommendations, could you confirm if this is your exact system-[Inspiron 15 5510 Setup and Specifications | Dell](https://www.dell.com/support/manuals/en-my/inspiron-15-5510-laptop/inspiron-5510-setup-and-specifications/specifications-of-inspiron-15-5510?guid=guid-7c9f07ce-626e-44ca-be3a-a1fb036413f9&lang=en-us)? Also, may I know which driver version you’re using and where you downloaded it from—was it from Dell or the Intel Download Center?",Intel,2025-11-16 20:12:25,1
AMD,np8jpxo,"u/triptoasturias Your concern is related to **port and bandwidth limitations**. Most Inspiron models only support **HDMI 1.4**, which is limited to 4K at 30Hz or 2560×1440 at 60Hz. Ultra-wide resolutions like 3440×1440 often aren’t exposed because they’re outside the standard HDMI 1.4 spec. **USB-C to DisplayPort**: This works because DisplayPort has much higher bandwidth and supports ultra-wide resolutions natively. HDMI-to-DisplayPort Adapter, Unfortunately, **passive adapters won’t work** because HDMI and DisplayPort use different signaling. You’d need an **active HDMI-to-DisplayPort converter**, but even then It will still be limited by the HDMI 1.4 bandwidth from your laptop. So, you likely **won’t get 3440×1440 at 144Hz,** maybe 3440×1440 at 30Hz or 50Hz at best.",Intel,2025-11-17 00:32:25,1
AMD,np8oqxv,"u/mercurianbrat This spec can run **basic image generation workflows** (CPU mode or lightweight models), but **video generation and heavy diffusion models will struggle or fail** on this setup. AI Playground’s minimum requirements are currently tied to Intel Arc GPUs with 8GB or more of allocated VRAM.  Currently you can download the installer for discrete GPUs.  We will also publish an installer that will run on Intel Core Ultra-H with built-in Intel Arc GPU (please keep in mind that [Windows allocates half of the system RAM as VRAM](https://www.intel.com/content/www/us/en/support/articles/000020962/graphics.html) for integrated GPUs, so 16GB or more of system RAM are required)  and Intel Arc GPU discrete add in cards with 8GB or more of memory. AI Playground takes up 8GBs of  HDD/SDD requirements: 8GB w/o models,  \~50GB with all models installed.",Intel,2025-11-17 01:00:59,1
AMD,nqg4c7m,"u/mano109 As a general corporate policy, Intel Support does not comment on information about products that have not been released yet.  **Visit** our [Newsroom](https://newsroom.intel.com/) for the most recent announcements and news releases.",Intel,2025-11-24 00:10:02,1
AMD,nqg5tvg,"**265K all the way.** At 4K with a 5090, you're GPU-bound anyway. The 265K's lower memory latency beats the 285K's extra cache at that resolution, plus you save money for better RAM or cooling.",Intel,2025-11-24 00:18:36,1
AMD,nqry4gw,"u/hus1030  The mounting pressure from your AIO cooler can directly affect whether the system successfully completes POST. When the cooler is tightened too much, it can cause the CPU or motherboard to bend slightly, which may lead to poor or lost contact between the CPU and the socket pins. This prevents the processor from initializing properly, resulting in a no-POST condition. Installing a contact frame changes the pressure distribution compared to the stock retention mechanism, so overtightening the AIO screws can amplify this issue. On the other hand, if the screws are too loose, the CPU temperatures will rise because the cooler is not making proper thermal contact. To avoid these problems, ensure the AIO screws are tightened evenly in a cross pattern and do not exceed the manufacturer’s torque specifications. If the issue persists, you may need to verify that the contact frame is correctly installed or temporarily revert to the original retention bracket to rule out pressure-related problems.",Intel,2025-11-25 21:43:16,1
AMD,nrmm6fq,"u/BudgetPractical8748   Intel Default Settings may impact system performance in certain workloads as compared to unlocked or overclocked settings.  As always, system performance is dependent on configuration and several other factors.",Intel,2025-12-01 00:19:07,1
AMD,ns3syx7,Nope got cash back,Intel,2025-12-03 18:08:27,1
AMD,nt12wk5,"u/Any_Information429 Your HP EliteBook 830 G8 is experiencing boot issues due to corrupted Intel Management Engine (ME) firmware, which is a critical low-level system component that manages hardware initialization. This corruption is causing the blinking caps lock light, multiple restart attempts before successful boot, and the extended Windows startup times you've been experiencing. The BIOS showing ""ME Firmware Mode: Recovery Mode"" confirms this diagnosis. Since these issues began after your recent repair, it's likely that the Management Engine chip connections were disturbed or the firmware became corrupted during the service process.  To resolve this, you need to perform a forced recovery of the ME firmware by downloading the specific firmware version for your EliteBook model from HP's support website-[HP EliteBook 830 G8 Notebook PC Software and Driver Downloads | HP® Support](https://support.hp.com/us-en/drivers/hp-elitebook-830-g8-notebook-pc/38216726) and using specialized recovery tools to reflash the Management Engine. You also have check BIOS settings to ensure proper ME configuration and temporarily disable fast boot to allow complete initialization. If the firmware recovery doesn't resolve the issue, this may indicate hardware-level damage to the ME controller that occurred during the previous repair, which would require professional chip-level service or potentially warranty coverage since the problem originated after authorized service work. The good news is that once the ME firmware is properly restored, your system should return to normal boot times and eliminate the restart cycles you're currently experiencing.  USB flash recovery method is definitely worth trying first - it's designed specifically for these types of firmware corruption issues and should get your laptop back to normal boot times without all those frustrating restarts. Check here: [Support Search Results | HP®️ Support](https://support.hp.com/us-en/search/videos?q=BIOS) BIOS Videos",Intel,2025-12-09 00:22:32,1
AMD,ntk9thw,"Hi @[Content\_Magician51](https://www.reddit.com/user/Content_Magician51/) Upon checking, there is a new driver version available which is 32.0.101.7082. You may try this and use DDU method to make sure that you performed a clean driver installation. Here are the links of the latest driver and the steps on how to perform DDU.  [Intel® 11th – 14th Gen Processor Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/864990/intel-11th-14th-gen-processor-graphics-windows.html)  [How to Use the Display Driver Uninstaller (DDU) to Uninstall a...](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)",Intel,2025-12-12 00:44:01,1
AMD,ntyyo2o,"u/MISINFORMEDDNA  I took a look at your crash error, and here's what's going on. That error code you're seeing (0x00020001) is actually what's called a ""hypervisor error,"" which basically means it's a problem with Windows' virtualization stuff rather than directly being caused by your WiFi drivers.  The real culprits are more likely things like memory issues, BIOS problems, or conflicts with virtualization features like Hyper-V. I'd suggest running a memory test first (just search for ""Windows Memory Diagnostic"" in your start menu), and if you have any virtual machines or Docker running, try shutting those down temporarily, we'll probably need to dig deeper into hardware or system-level issues to really fix this one.",Intel,2025-12-14 12:43:17,1
AMD,ntz1is6,"u/strumpystrudel So what you're experiencing is actually pretty normal behavior for your laptop when it's unplugged - that 80% performance drop is totally expected and here's why. When your laptop is plugged into the wall, your CPU can run at much higher power levels (probably around 28W or more), but when you switch to battery, it gets severely limited to maybe 8-15W to preserve battery life. This is especially true for ultrabooks like the Gram that prioritize being thin and light over raw performance. The thing is, a lot of this power management happens at the hardware level with Intel's built-in systems, which is why all those Windows power settings you tweaked aren't really making a difference - the CPU is basically ignoring them and doing its own thing to save battery.  Now, for a game like Final Fantasy XI, you should still be able to get it running decently on battery with some tweaks, but expecting that same smooth 60 FPS at max settings is probably unrealistic given the fundamental power constraints of ultrabook design. Most ultrabooks see this kind of 60-80% performance hit on battery for any sustained workload, so you're definitely not alone in this.   But honestly, this is just how these thin and light laptops are designed to work - they're amazing when plugged in, but they have to make compromises when running on battery to actually give you decent battery life.",Intel,2025-12-14 13:04:59,1
AMD,ntz4fy4,"u/Aggravating_Gap_203 I'd recommend running Intel's Processor Diagnostic Tool first to rule out any hardware defects with the CPU itself. Just download it from Intel's website, run the test, and let us know if it passes or fails. While you're at it, try loading your BIOS defaults and make sure your power settings are at Intel's recommended specs - PL1 should be around 125W and PL2 around 181W for your 14600KF-[Intel® Core™ i5 processor 14600KF](https://www.intel.com/content/www/us/en/products/sku/236778/intel-core-i5-processor-14600kf-24m-cache-up-to-5-30-ghz/specifications.html)  [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html)  Your friend is actually spot on about the contact frame recommendation. Before you buy one though, try remounting your cooler one more time - make sure you're tightening the screws in an X-pattern and that everything is perfectly aligned. Sometimes it just takes that perfect mount to get things working right. Let us know what the Intel diagnostic shows and we can go from there!",Intel,2025-12-14 13:25:40,1
AMD,nurwhxx,Hi [RadioFr33Europe](https://www.reddit.com/user/RadioFr33Europe/) I sent a direct message to gather more details for me to review the case and check the status of your replacement request.,Intel,2025-12-19 00:03:08,1
AMD,nvbnrfy,"Hi [Designer-Let-7867](https://www.reddit.com/user/Designer-Let-7867/) For issues related to game bundles and how to claim it, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry/issues/error message during claiming.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-12-22 05:14:39,1
AMD,nvfrjth,"u/earwig2000 Let me check this internally. From what I see, you’ve already tried a lot of steps to address the game crash issue. I’ll share an update here as soon as I have more details, and I might need to collect some info from you for further analysis.",Intel,2025-12-22 21:29:55,1
AMD,nvmgqub,"u/earwig2000 Have you tried doing a clean installation of the graphics driver using [DDU ](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)and then installing the latest version from our [download center](https://www.intel.com/content/www/us/en/download/785597/intel-arc-graphics-windows.html)? After that, please retest the game. If the issue still persists, could you share your PSU make, model, and wattage? Also, check if Resizable BAR (ReBAR) is enabled in your BIOS. Finally, review the Event Viewer for any error messages or crash-related events, this will help us determine whether the problem is driver-level or application-related.",Intel,2025-12-23 22:44:39,1
AMD,nwglbda,u/outlander94 Kindly check this article: [Is Virtual Reality (VR) supported on Intel® Arc™ A-Series and...](https://www.intel.com/content/www/us/en/support/articles/000093024/graphics.html),Intel,2025-12-29 00:38:16,1
AMD,nwgmhqt,"u/Kai-juu We trust the technician’s diagnosis of the system. However, since the unit is a prebuilt, it is likely to have a tray processor. Based on our warranty terms and conditions, we can only replace boxed processors. For a faster turnaround time, please first verify whether the processor is tray or boxed using our website. Once you confirm the type, I can guide you through the next steps.  [Warranty Information](https://supporttickets.intel.com/s/warrantyinfo?language=en_US)  [Where to Find Intel® Boxed Processor Serial Numbers (FPO and ATPO)...](https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html)  [Warranty Policy for Intel® Boxed and Tray Processors](https://www.intel.com/content/www/us/en/support/articles/000024255/processors.html)",Intel,2025-12-29 00:44:27,1
AMD,nwohvpq,"u/Sk7Str1p3 I can see you've already tried several tools and are getting mixed results, which is definitely frustrating.  To help you out better, could you share what's driving this inquiry? Are you working on a security project, dealing with compliance requirements, or troubleshooting a specific feature? Understanding your use case will help me point you toward the right verification methods or suggest alternatives if needed. The more context you can provide, the better I can assist you!",Intel,2025-12-30 05:04:29,1
AMD,nwvjztq,"u/pheoxs you may try to follow this article [Computer Randomly Reboots or Shuts Down](https://www.intel.com/content/www/us/en/support/articles/000035903/processors.html) if the issue persist, disable Turbo boost If the motherboard BIOS allows and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor is affected and need a replacement.",Intel,2025-12-31 06:58:32,1
AMD,nktqa25,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Intel,2025-10-22 18:45:49,2
AMD,nmk5tv3,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Intel,2025-11-01 16:57:33,1
AMD,nnel13c,"Hi, I've tested disabling turbo and I still experienced issues. I've created a support request, case number 06728608, please take a look ASAP.",Intel,2025-11-06 12:05:51,1
AMD,np8oivj,"Actually, this is not the case. After spending hours on this issue, I've finally found the 31.0.101.4502\_A14 drive at Dell's website that is working without any problems with the 3440x1440. So your claim that HDMI 1.4 is limited to 2560x1440 is false. HDMI 1.4 can go further to 3840 x 2160 without any problems. It's unacceptable the lack of support for UWM from Intel graphics cards and drives. Many previous drivers let you choose 3440X1440 resolution. Why did Intel stop supporting this resolution in the last graphics drivers?",Intel,2025-11-17 00:59:39,1
AMD,nqkt81n,I was going with the 265K over the 9800X3D since the Intel stuff seems to get better 1% lows and smoother experience at 4k and above. But does DLSS change that? Does DLSS lowering the render resolution push the 9800X3D back into the lead?,Intel,2025-11-24 19:16:09,1
AMD,nvmmo42,"Tried running a clean driver reinstall using DDU. (I'm pretty sure I did this last install too but did it again to double check) and that didn't fix the issue.  ReBAR is enabled, it was on by default.  My PSU is the [Thermaltake Toughpower 650W Gold](https://www.thermaltake.com/toughpower-650w-gold-modular.html)  Windows event viewer did pick up the crash [(imgur link)](https://imgur.com/NhS5e6l), not sure what to make of it though.",Intel,2025-12-23 23:18:45,1
AMD,nwgvzmr,"u/Intel_Support Thank you for your guidance. My system is a prebuilt, so the processor never came in a box and is likely a tray CPU. Micro Center told me to reach out to Intel for warranty support, so I just want to confirm, should I be working with Intel directly or do you recommend contacting PowerSpec/Micro Center for the RMA? Thank you again, I look forward for the next steps!",Intel,2025-12-29 01:38:35,1
AMD,nwr1jrz,"Yes, I am information security hobbyist, I have a research project on countering attackers with physical access to the machines. Unfortunately, my current hardware is average gaming PC - i7 12700f with h670 motherboard and I cannot update hardware.",Intel,2025-12-30 16:12:50,1
AMD,nkvsdop,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Intel,2025-10-23 01:24:29,1
AMD,nnh8tyn,"u/SuperV1234 Hi, thanks for the update. I’ve reviewed case number and confirmed that it’s currently being handled by our **warranty team** for replacement. You should be receiving further instructions from them shortly.     [Guide to pack your faulty CPU](https://www.intel.com/content/www/us/en/content-details/841997/guide-to-pack-your-faulty-cpu.html)",Intel,2025-11-06 20:19:42,1
AMD,np8q866,"u/triptoasturias this explains, The generic Intel® driver provides users with the latest and greatest feature enhancements and bug fixes that computer manufacturers (OEMs) might not have customized yet. OEM drivers are handpicked and include customized features and solutions to platform-specific needs. Installing Intel generic graphics driver will overwrite your handpicked OEM graphics driver (in your case Dell driver). Users can check for matching OEM driver versions at OEM websites. For more information on how the installation of this driver may impact your OEM customizations, see this [article.](https://www.intel.com/content/www/us/en/support/articles/000096252/graphics.html)",Intel,2025-11-17 01:09:54,1
AMD,nvz4tef,"u/earwig2000 ﻿Thank you for sharing this information. I will begin investigating the issue and attempt to replicate it on our end. I'll post an update here or notify you directly once there are any developments. If I need further details, I'll reach out to you here. I appreciate your patience as I work on this matter.   [](javascript:void(0);)",Intel,2025-12-26 04:28:47,1
AMD,nwnx92g,"u/Kai-juu According to our warranty policy, RMAs for tray processors must be handled by the original place of purchase, as clearly stated in the article I referenced.",Intel,2025-12-30 02:59:10,1
AMD,nwvn4dm,"u/Sk7Str1p3 I see, let me loop in our product support engineer on this one and dig into it a bit more. I'll circle back with you once I have a clearer picture on the TME support situation.",Intel,2025-12-31 07:26:19,2
AMD,nnd9ral,Unfortunately my Dell warranty support has ended and so far the forum there has not been able to help either. The removal tool works but Killer just keeps coming back.,Intel,2025-11-06 04:50:53,1
AMD,nnd9w3h,"I also went to Dell, typed in my service tag # and cant find any other ethernet drivers.",Intel,2025-11-06 04:51:52,1
AMD,nvz6y15,"If it helps, I'm also using   CPU - Ryzen 5 7600   RAM - Corsair Vengeance 32gb DDR5 6000mhz cl36  SSD - Crucial p3 plus 1tb  Motherboard - MSI b650m-a",Intel,2025-12-26 04:44:54,1
AMD,nx27aj0,"Hello, have you received a reply yet?",Intel,2026-01-01 10:17:28,1
AMD,nwgjs6p,u/earwig2000 Please check your inbox; I’ve sent you a message.,Intel,2025-12-29 00:30:28,1
AMD,nx7bu85,u/Sk7Str1p3 ﻿I’m still reviewing the details to ensure I provide you with the most accurate information. I’ll update you as soon as I have a clear answer.  [](javascript:void(0);),Intel,2026-01-02 04:55:46,1
AMD,nfolbmr,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Intel,2025-09-22 23:20:45,55
AMD,nflslxh,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Intel,2025-09-22 14:31:25,47
AMD,nfm76rf,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Intel,2025-09-22 15:42:11,33
AMD,nfnaznn,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Intel,2025-09-22 18:52:47,10
AMD,nfpb0n3,"i think it's bad for us, consumers",Intel,2025-09-23 02:12:30,4
AMD,nflx3jg,Was the team up really to crush AMD or Nvidia's answer to enter China?,Intel,2025-09-22 14:53:26,8
AMD,ng4ik7u,AMDware unboxed only cares about AMD anyway,Intel,2025-09-25 13:37:58,3
AMD,nfm1wz0,This hurts the arc division way more than this could ever hurt amd.,Intel,2025-09-22 15:16:45,15
AMD,nfoivfo,They will crush user's wallet,Intel,2025-09-22 23:04:45,2
AMD,nftk1b6,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Intel,2025-09-23 19:23:17,2
AMD,nfv8a1x,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Intel,2025-09-24 00:40:38,1
AMD,ng0xe2q,Remember Kaby Lake G? No? This will also be forgotten soon.,Intel,2025-09-24 22:04:55,1
AMD,ng2ck7r,Yes.,Intel,2025-09-25 03:11:04,1
AMD,ngzb138,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Intel,2025-09-30 09:53:37,1
AMD,nhckqoj,Foveros baby!,Intel,2025-10-02 11:51:31,1
AMD,nfmdikv,AMDUnboxed on suicide watch.,Intel,2025-09-22 16:12:26,1
AMD,nfn7em1,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Intel,2025-09-22 18:34:17,1
AMD,nfmh1rz,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Intel,2025-09-22 16:29:30,1
AMD,nflsfzz,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Intel,2025-09-22 14:30:36,0
AMD,nfmihzp,Ohh noooerrrrrrrrr,Intel,2025-09-22 16:36:30,0
AMD,nfvhp2d,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Intel,2025-09-24 01:35:50,0
AMD,ngcdph8,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Intel,2025-09-26 17:45:54,0
AMD,nfnsqrd,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Intel,2025-09-22 20:29:14,-3
AMD,nfmr987,welcome to the Nvidia and amd duopoly,Intel,2025-09-22 17:17:38,-4
AMD,nfmy4sf,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Intel,2025-09-22 17:49:51,3
AMD,nfn29jk,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Intel,2025-09-22 18:09:19,-8
AMD,nfma1mz,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Intel,2025-09-22 15:55:41,4
AMD,ng59q6y,a partnership doesnt mean they get free reign over license lol,Intel,2025-09-25 15:49:13,2
AMD,nfodhll,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Intel,2025-09-22 22:29:59,-2
AMD,nfm0v8n,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Intel,2025-09-22 15:11:42,21
AMD,nfm237h,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Intel,2025-09-22 15:17:35,6
AMD,nfm66b6,Why would it?,Intel,2025-09-22 15:37:21,8
AMD,nfm5ru9,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Intel,2025-09-22 15:35:26,12
AMD,nfmevnp,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Intel,2025-09-22 16:19:02,3
AMD,nhzfr23,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-10-05 23:57:54,1
AMD,nfm4tdl,Past != Future,Intel,2025-09-22 15:30:48,5
AMD,nfnb9ui,"nvidia also used to make motherboard chipset, with mixed success.",Intel,2025-09-22 18:54:18,2
AMD,nfoib8h,FSR 4 looks like the later versions of Dlss 2 did,Intel,2025-09-22 23:01:07,7
AMD,nfo2bk7,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Intel,2025-09-22 21:21:54,2
AMD,ng9kwgg,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Intel,2025-09-26 07:01:28,0
AMD,nfmqv0q,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Intel,2025-09-22 17:15:46,5
AMD,nfoz7q1,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Intel,2025-09-23 00:52:30,0
AMD,ng6jjhd,Never said that.,Intel,2025-09-25 19:28:08,-1
AMD,nfm1xh4,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Intel,2025-09-22 15:16:49,13
AMD,nfm6de6,Why do so many people think that this will kill ARC?,Intel,2025-09-22 15:38:18,11
AMD,nfm8iqb,The market for Arc is the same as for Nvidia.,Intel,2025-09-22 15:48:31,9
AMD,nflxpfk,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Intel,2025-09-22 14:56:21,3
AMD,nfmpyxd,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Intel,2025-09-22 17:11:35,4
AMD,nfncgcw,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Intel,2025-09-22 19:00:37,2
AMD,nfpd2vm,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Intel,2025-09-23 02:26:29,1
AMD,nfq8t4o,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Intel,2025-09-23 07:05:35,0
AMD,ng2cqnz,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Intel,2025-09-25 03:12:17,1
AMD,ng2dv4q,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Intel,2025-09-25 03:20:00,0
AMD,nfm6s3o,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Intel,2025-09-22 15:40:15,5
AMD,nfmtpsf,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Intel,2025-09-22 17:29:18,14
AMD,nfm7mbh,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Intel,2025-09-22 15:44:16,0
AMD,nfm91t2,Nvidia does not have an A310 competitor.,Intel,2025-09-22 15:51:01,-1
AMD,nftpodz,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Intel,2025-09-23 19:50:16,2
AMD,nfq8uoi,I have not lied,Intel,2025-09-23 07:06:03,1
AMD,ng2iom6,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Intel,2025-09-25 03:54:05,1
AMD,ng17you,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Intel,2025-09-24 23:06:01,2
AMD,nfonq2p,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Intel,2025-09-22 23:36:31,3
AMD,nfm9qnn,Intel doesn't have a current gen A310 competitor either.,Intel,2025-09-22 15:54:15,10
AMD,nfma2t0,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Intel,2025-09-22 15:55:51,1
AMD,nfm0jnd,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Intel,2025-09-22 15:10:08,2
AMD,nftspxa,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Intel,2025-09-23 20:04:43,2
AMD,nfrlmki,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Intel,2025-09-23 13:46:08,0
AMD,nfu9p3r,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Intel,2025-09-23 21:25:34,2
AMD,nfpz6gm,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Intel,2025-09-23 05:25:09,3
AMD,nfqz1sj,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Intel,2025-09-23 11:29:37,2
AMD,nfrltqc,The later versions of Dlss 2 look like Dlss 3,Intel,2025-09-23 13:47:11,2
AMD,nfq8fx8,Nvidia probably feels the same about their low end SKUs.,Intel,2025-09-23 07:01:31,1
AMD,ngn9bw4,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Intel,2025-09-28 13:27:33,1
AMD,nfrmbyw,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Intel,2025-09-23 13:49:53,0
AMD,nfq8hgn,Yeah lol,Intel,2025-09-23 07:02:00,1
AMD,nfrmiwb,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Intel,2025-09-23 13:50:54,2
AMD,nfrmnqg,"""Everyone I don't like is biased""-ass answer",Intel,2025-09-23 13:51:36,1
AMD,naz5fcr,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Intel,2025-08-27 16:46:20,49
AMD,nazj7k0,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Intel,2025-08-27 17:50:18,15
AMD,ncitd5t,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Intel,2025-09-05 08:24:57,1
AMD,naz95i7,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Intel,2025-08-27 17:04:24,1
AMD,naz6p5r,Just hodl until you get the biscuits,Intel,2025-08-27 16:52:25,38
AMD,naz7hoi,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Intel,2025-08-27 16:56:15,7
AMD,nazkvn7,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Intel,2025-08-27 17:57:46,3
AMD,nazb0q3,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Intel,2025-08-27 17:13:24,3
AMD,nazazfk,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Intel,2025-08-27 17:13:14,32
AMD,nb330i7,I thought Arrow Lake refresh was in the cards for 2025.,Intel,2025-08-28 05:57:34,1
AMD,nb04wuk,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Intel,2025-08-27 19:33:00,-1
AMD,nazl60x,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Intel,2025-08-27 17:59:03,3
AMD,nb8e4jm,This entire thing is a mobile roadmap so why are you here?,Intel,2025-08-29 00:35:26,2
AMD,nb1w9ji,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Intel,2025-08-28 01:01:42,3
AMD,nazbb9i,"Yeah if it's Surface roadmap, it's a nothing burger.",Intel,2025-08-27 17:14:47,5
AMD,nb02q5l,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Intel,2025-08-27 19:22:51,1
AMD,nb171ho,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Intel,2025-08-27 22:38:31,3
AMD,nb1xsvd,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Intel,2025-08-28 01:10:35,1
AMD,nb1vmub,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Intel,2025-08-28 00:58:05,3
AMD,n73y5u9,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Intel,2025-08-05 20:10:52,33
AMD,n73iurw,"I like how they just throw random words around to pad their ""article"".",Intel,2025-08-05 18:43:24,63
AMD,n73t59w,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Intel,2025-08-05 19:42:58,27
AMD,n741jok,"""leaks""",Intel,2025-08-05 20:28:54,7
AMD,n74lp88,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Intel,2025-08-05 22:16:47,5
AMD,n77fxeb,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Intel,2025-08-06 10:20:26,1
AMD,n7koh28,Scared of their rumor?  Lets release our rumor!,Intel,2025-08-08 09:59:01,1
AMD,n758aa3,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Intel,2025-08-06 00:22:30,1
AMD,n76sf5r,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Intel,2025-08-06 06:42:22,-2
AMD,n74sw92,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Intel,2025-08-05 22:56:59,-6
AMD,n75af32,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Intel,2025-08-06 00:34:46,5
AMD,n754qqc,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Intel,2025-08-06 00:02:20,8
AMD,n7582e3,bLLC is not stacked cache,Intel,2025-08-06 00:21:15,4
AMD,n767h6n,What do you consider random? The article was perfectly clear.,Intel,2025-08-06 03:54:03,10
AMD,n748zl3,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Intel,2025-08-05 21:08:09,14
AMD,n741l8k,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Intel,2025-08-05 20:29:08,2
AMD,npq13eo,"Isn't the latency similar? The advantage of the 3d cache is an even larger cache size, while the advantage of what intel is going to do with bllc is that there is less of a thermal issue so the clocks can be higher.",Intel,2025-11-19 19:51:12,1
AMD,npq0u97,Probably only on the skus with less cores.,Intel,2025-11-19 19:49:56,1
AMD,n76jhxw,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Intel,2025-08-06 05:24:22,1
AMD,n7s4doe,Yeah. Definitely just you,Intel,2025-08-09 14:25:48,4
AMD,n757qwt,You could literally make that claim with any CPU performance increase.,Intel,2025-08-06 00:19:24,10
AMD,n7615sm,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Intel,2025-08-06 03:11:36,7
AMD,n77a60y,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Intel,2025-08-06 09:28:27,2
AMD,n76ahn8,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Intel,2025-08-06 04:15:00,5
AMD,n757yiy,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Intel,2025-08-06 00:20:37,1
AMD,n749ios,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Intel,2025-08-05 21:11:01,16
AMD,npq1inb,"No, because it would be the same die. Just won't fit.",Intel,2025-11-19 19:53:18,1
AMD,n76z9ct,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Intel,2025-08-06 07:45:28,2
AMD,n77cfm2,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Intel,2025-08-06 09:49:46,6
AMD,n77cdot,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Intel,2025-08-06 09:49:17,5
AMD,nam3zf1,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Intel,2025-08-25 17:06:43,0
AMD,n75cvtk,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Intel,2025-08-06 00:48:56,2
AMD,n7ddfzp,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Intel,2025-08-07 06:10:13,1
AMD,n7dcz9u,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Intel,2025-08-07 06:06:08,1
AMD,nax8p6j,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Intel,2025-08-27 10:26:21,1
AMD,n7ddyd1,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Intel,2025-08-07 06:14:41,1
AMD,n7deaek,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Intel,2025-08-07 06:17:37,1
AMD,n9hwp92,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Intel,2025-08-19 08:52:24,62
AMD,n9hupxb,It's not sorcery. Its just Intel doing the game developers work.,Intel,2025-08-19 08:32:04,79
AMD,n9i0i3y,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Intel,2025-08-19 09:30:05,8
AMD,n9ic94k,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Intel,2025-08-19 11:11:41,11
AMD,n9k23nq,Never count out Intel. They have some very talented people over there.,Intel,2025-08-19 16:43:12,7
AMD,n9hygne,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Intel,2025-08-19 09:09:52,6
AMD,n9qyqfh,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Intel,2025-08-20 17:38:05,2
AMD,n9tmgam,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Intel,2025-08-21 02:04:20,1
AMD,nab3aup,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Intel,2025-08-23 20:50:51,1
AMD,naohj1m,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Intel,2025-08-26 00:28:42,1
AMD,nattrzq,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Intel,2025-08-26 20:25:28,1
AMD,nd42fep,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Intel,2025-09-08 16:53:22,1
AMD,ndg3xrf,I will follow all!,Intel,2025-09-10 13:49:20,1
AMD,n9j60kj,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Intel,2025-08-19 14:10:38,-7
AMD,n9iw99o,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Intel,2025-08-19 13:18:38,11
AMD,n9ihm3d,How is this doing the game developers work?,Intel,2025-08-19 11:50:25,-6
AMD,n9imvpt,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Intel,2025-08-19 12:24:19,5
AMD,n9ipgu8,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Intel,2025-08-19 12:39:59,5
AMD,n9ktypv,The intel software team is pure black magic when they allowed to work on crack.,Intel,2025-08-19 18:54:19,3
AMD,n9iml8f,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Intel,2025-08-19 12:22:32,4
AMD,n9s6nj5,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Intel,2025-08-20 21:12:41,5
AMD,n9iyl6t,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Intel,2025-08-19 13:31:21,7
AMD,n9u57ww,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Intel,2025-08-21 04:07:48,3
AMD,na9spi1,Balanced in full load will just do the same thing as High Performance.,Intel,2025-08-23 16:43:30,1
AMD,n9lhg6u,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Intel,2025-08-19 20:46:34,20
AMD,nah6zpc,Or... Just process lasso.,Intel,2025-08-24 21:05:07,1
AMD,n9ij60y,Because it’s optimizations on how it can efficiently use the cpu.,Intel,2025-08-19 12:00:43,22
AMD,n9iv66n,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Intel,2025-08-19 13:12:40,1
AMD,n9iwird,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Intel,2025-08-19 13:20:05,3
AMD,n9mv7uv,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Intel,2025-08-20 01:17:18,4
AMD,n9iojt9,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Intel,2025-08-19 12:34:27,1
AMD,n9y05el,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Intel,2025-08-21 19:06:28,1
AMD,n9lzy5i,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Intel,2025-08-19 22:19:43,3
AMD,n9mhmhz,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Intel,2025-08-19 23:59:05,1
AMD,nah75n6,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Intel,2025-08-24 21:06:00,1
AMD,n9kb8x1,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Intel,2025-08-19 17:25:29,-5
AMD,n9ivrcr,You need to download the Intel Application Optimization app from the Windows store,Intel,2025-08-19 13:15:54,1
AMD,n9iy6ma,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Intel,2025-08-19 13:29:09,7
AMD,n9t6inr,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Intel,2025-08-21 00:31:46,1
AMD,n9xzseo,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Intel,2025-08-21 19:04:43,1
AMD,n9ipdm4,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Intel,2025-08-19 12:39:26,4
AMD,n9opxwe,Where to download this APO,Intel,2025-08-20 10:12:53,1
AMD,n9ktn8y,Except its THE game devs job to optimize games for multiple cpus and gpus.,Intel,2025-08-19 18:52:47,7
AMD,n9ks0ao,It’s literally their job to do so? wtf you talking about?,Intel,2025-08-19 18:45:02,4
AMD,n9mt1qu,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Intel,2025-08-20 01:04:38,1
AMD,n9iw056,"Will do, I got a 265K. Performance is already great tbh.",Intel,2025-08-19 13:17:15,1
AMD,n9iz3ih,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Intel,2025-08-19 13:34:10,3
AMD,n9irrhk,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Intel,2025-08-19 12:53:21,1
AMD,n9q1a42,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Intel,2025-08-20 14:59:31,2
AMD,n9kvap0,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Intel,2025-08-19 19:00:36,-3
AMD,n9p3czn,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Intel,2025-08-20 11:55:36,1
AMD,n9kt4hv,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Intel,2025-08-19 18:50:20,-2
AMD,n9mx3nm,That's not simply what APO does.,Intel,2025-08-20 01:28:12,1
AMD,n9iwijq,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Intel,2025-08-19 13:20:03,2
AMD,n9j8cqp,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Intel,2025-08-19 14:22:18,3
AMD,n9iublo,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Intel,2025-08-19 13:07:57,4
AMD,n9pa899,i am not talking about older games. i am talking about newer games.... really dude?,Intel,2025-08-20 12:37:54,0
AMD,n9l8ne6,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Intel,2025-08-19 20:05:16,4
AMD,n9iyd7e,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Intel,2025-08-19 13:30:09,6
AMD,n9iuumd,Thanks a lot.,Intel,2025-08-19 13:10:53,5
AMD,n9pc7ju,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Intel,2025-08-20 12:49:28,2
AMD,n9lby71,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Intel,2025-08-19 20:20:50,3
AMD,n9pd885,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Intel,2025-08-20 12:55:15,0
AMD,n9lgtiq,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Intel,2025-08-19 20:43:35,2
AMD,nadrsxq,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Intel,2025-08-24 08:29:02,0
AMD,n9li080,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Intel,2025-08-19 20:49:11,0
AMD,n6bumwm,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Intel,2025-08-01 11:02:22,63
AMD,n6bnc4i,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Intel,2025-08-01 10:01:27,35
AMD,n6chmtn,Did the DP4A version also improve from 1.3 to 2.0?,Intel,2025-08-01 13:26:20,4
AMD,n6i4k9c,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Intel,2025-08-02 10:05:35,1
AMD,nu3t3jt,"i don't get the performance i got with FSR, but damn, FSR looks like crap in every aspect, i got less performance with xess in comparison, but at least got better framerats than native and get a better image quality",Intel,2025-12-15 04:45:45,1
AMD,n6bj9ci,Okay but why would I want to use that instead of NVIDIA DLSS?,Intel,2025-08-01 09:23:32,-22
AMD,n6bwcya,It’s the least you should get after not getting FSR4.,Intel,2025-08-01 11:15:16,19
AMD,n6ck8qn,You'd use it over FSR if that's available too?,Intel,2025-08-01 13:40:07,3
AMD,n6i0iq2,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Intel,2025-08-02 09:25:17,1
AMD,n6c91ju,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Intel,2025-08-01 12:38:17,12
AMD,n6btrfp,And they expect people who bought previous RDNA to buy more RDNA,Intel,2025-08-01 10:55:34,4
AMD,n6nl7sh,Not by a significant amount.,Intel,2025-08-03 06:43:34,1
AMD,n6i0oud,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Intel,2025-08-02 09:27:02,0
AMD,n6bp1iu,for the games that dont support DLSS,Intel,2025-08-01 10:16:31,30
AMD,n6bul99,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Intel,2025-08-01 11:02:02,22
AMD,n6bqgih,10 series cards will benefit from this,Intel,2025-08-01 10:28:41,13
AMD,n6bnz56,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Intel,2025-08-01 10:07:08,1
AMD,n6cmxm3,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Intel,2025-08-01 13:54:01,18
AMD,n6d0adu,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Intel,2025-08-01 14:58:55,7
AMD,n6i1gij,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Intel,2025-08-02 09:34:56,2
AMD,n6emb0z,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Intel,2025-08-01 19:37:45,1
AMD,n6guokn,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Intel,2025-08-02 03:16:52,1
AMD,n6nl9o1,DP4a is cross vendor.   XMX is Arc only.,Intel,2025-08-03 06:44:04,3
AMD,n6bqj7p,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Intel,2025-08-01 10:29:20,-10
AMD,n6cx6cb,1080ti heard no bell,Intel,2025-08-01 14:44:10,3
AMD,n6i2ufc,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Intel,2025-08-02 09:48:51,1
AMD,n6r59ev,where did amd touch you bud?,Intel,2025-08-03 20:25:24,2
AMD,n6d2yn8,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Intel,2025-08-01 15:11:40,-3
AMD,n6f09jv,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Intel,2025-08-01 20:47:10,3
AMD,n6iu4q0,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Intel,2025-08-02 13:21:11,2
