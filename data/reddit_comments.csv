brand,comment_id,text,subreddit,created_utc,score
Intel,ngzq6md,nice to see that Intel is actually putting resources into fixing cpu overhead,hardware,2025-09-30 11:55:39,63
Intel,ngznsb2,"Fixed/improved in some cases, on a game-by-game basis.  Better than nothing, but far from actually *fixed* fixed. Steve says he'll be testing more games and resolutions shortly, but I'd also like to see direct comparisons between a pre-fix driver and the newest.",hardware,2025-09-30 11:39:12,138
Intel,ngzp1cb,"While it's great to see improvements, those still seem to be on a case by case basis.   The root cause is still there and if there are more powerful cards coming (big if), then the issue will shift up again.   But hey maybe Nvidia can start prioritising the overhead issue as well while we're at it.",hardware,2025-09-30 11:47:53,80
Intel,ngzzyle,"Huh, didn't expect that. Good job, Intel.  B580s mostly aren't super-worth buying atm, but this is a really good sign for future Intel products (assuming there will be any of course) as well as obviously good news for people who bought B580s.",hardware,2025-09-30 12:57:10,10
Intel,nh0sgz2,How is idle power usage?,hardware,2025-09-30 15:24:40,11
Intel,ngzn9wl,"What I took away from the video: more or less fine with 5600 and up, still struggles with 2600. Intel is doing per game optimizations, so your mileage may vary. Space Marine is still struggling, Spider man 1 is running great, SpiderMan 2 does not.  Drivers are constantly improving, HUB has bright outlook. The bigger issue is 9060XT8 within 10-15% price range, but delivering 30% better perf.     Edit: A lot of people talking about how old or slow 2600 is in replies. And that's part of the point, I obviously didn't get across well. The issue is not the absolute performance, but the relative loss compared to AMD. In CPU bound scenarios they both should be close, since the limiting factor is CPU. But in this scenario 9060XT is even further ahead than when they are unconstrained.",hardware,2025-09-30 11:35:33,44
Intel,ngzlxvl,"Lol, just in time for the Panther Lake announcement. But credit where it's due.",hardware,2025-09-30 11:25:54,14
Intel,nh085od,Mildly interesting B580 news! There seems to be some improvements to the CPU overhead problem,hardware,2025-09-30 13:43:01,5
Intel,nh09giv,Could this have been a bug on Ryzen only? Did they ever test the 12400 or something like it in comparison to a Ryzen 5600?,hardware,2025-09-30 13:50:03,6
Intel,nh121pe,"I thought people were pretty sure it was an entirely hardware problem that can't be alleviated with driver fixes.  Guess they were wrong somewhat. I think the claim that it can't be completely fixed still seems to stand, but it can be alleviated alright.",hardware,2025-09-30 16:11:24,7
Intel,ngzo0zu,"Meanwhile Intel Vulkan drivers on Linux are absolute garbage, they provide less than 50% of the performance of what's available on windows. It's so bad, that usng WineD3D (DirectX to OpenGL) gives better performance than  DXVK (DirectX to Vulkan).",hardware,2025-09-30 11:40:54,13
Intel,ngzu0u6,"I bought an Arc A380 for $100 when it first launched, have been comfy 1080p gaming ever since, graphics / software maturity and improvement has been amazing.  I can't wait to see the B770 (or whatever they decide to call it) and also Arc C580 based on Xe3-HPG (Celestial)",hardware,2025-09-30 12:21:04,7
Intel,ngzxoc3,72 up upvotes on a duplicate post when someone posted it an hour earlier. WTH Reddit.,hardware,2025-09-30 12:43:48,8
Intel,nh0bn6z,"I still remember the day when amd took over ati and finally got to hd 7xxx series aka the first trully new microarch since the purchase and they launched the first multithreaded version of the drivers for the tahiti? One version of the hd 7850xt, then the 7950 and 7970 giving everyone basically a massive perf improvement, less frame to frame latency and like a two digit perf uplift... Sounds a lot like what intel is going through. Who knows maybe in 10 more years intel shows their 1080 or whatever ends up being called and is competing in the high tier. It's been impressive, considering 10 year ago hey were basically on worthless igps only good for quicksync h264 low quality encoding.",hardware,2025-09-30 14:01:29,2
Intel,nh214yf,"> Fixed/improved in some cases, on a game-by-game basis. Better than nothing, but far from actually fixed fixed.  :(",hardware,2025-09-30 18:59:39,2
Intel,nh10x94,unrelated but whats up with the 1488 in your username,hardware,2025-09-30 16:05:50,5
Intel,ngzpti7,I'm noticing the 8gb card has better averages & lows than 12gb,hardware,2025-09-30 11:53:12,4
Intel,nh06tz5,which version is this,hardware,2025-09-30 13:35:46,3
Intel,ngzn8wn,Thats terrific from Intel.  Hopefully they can still improve on it and ofcourse continue releasing & developing GPUs. Maybe two generations down the line we would be considering Intel GPUs over AMD.,hardware,2025-09-30 11:35:21,6
Intel,nh05ke8,"I mean that’s ok, but for Spider-Man, which was the most detailed view, it looked more like the 9060 xt performed inconsistent with the 5600.  20% uplift on lower and higher quality CPUs. I would have expected there to been a consistency across all of them if this was the case.   I would still not even consider recommending a 570/580 to a friend over even the 8 gb models of nvidia and amd.",hardware,2025-09-30 13:28:46,2
Intel,nh2l35g,"TL;DR improved, but far from fixed, clickbait title.",hardware,2025-09-30 20:35:55,2
Intel,nh2f33w,"I can't reccomend Arc to anyone, there's 0% chance the arc driver team still exists in 2 years.",hardware,2025-09-30 20:07:27,1
Intel,ngztn99,"Intel is really turning things around, looking forward to what they have moving into the future",hardware,2025-09-30 12:18:39,0
Intel,nh02xhs,Should've used RTX 4060 instead. Previous tests also showed that AMD GPUs too can suffer from CPU overhead.,hardware,2025-09-30 13:14:02,-6
Intel,nh2b3pc,Intel: lets make amd cpu work slow with code-compiler tricks  Amd cpu: slow and make overhead in games but only for Intel gpu.  Intel: can't sell low-end gpu for high-end cpu  Intel: removes amd-cpu dampener.,hardware,2025-09-30 19:48:08,-2
Intel,nh2ynj9,"this is so sad,  because intel just officially ended arc :/  remember, that nvidia wouldn't have intel use nvidia graphics chiplets in apus, if arc wasn't dead dead.  so you can't suggest arc anymore, because intel sure as shit won't properly support it longterm at all.  and this SUCKS, because the b580 at least had the barest minimum vram, that works rightnow.  in a different timeline arc would still be cooking and the royal core project didn't get nuked by an idiot.  but well it is what it is.",hardware,2025-09-30 21:42:26,-2
Intel,nh08akc,"Probably Nvidia engineers helped them make better drivers, since Nvidia owns them",hardware,2025-09-30 13:43:46,-13
Intel,nh2ftbk,"a lot of people dog on them and spread fear mongering info about their GPU division and ARC support, but they've been doing quite well in terms of support. I got a Claw 8AI+ and they've been pushing updates with performance optimizations for mobile arc as well.",hardware,2025-09-30 20:11:01,22
Intel,ngzyiuq,The CPU bottleneck didn't happen universally either. Some games others tested didn't suffer from it.,hardware,2025-09-30 12:48:52,46
Intel,ngzzff5,There is a direct comparison between driver versions in the video.,hardware,2025-09-30 12:54:07,12
Intel,nh16axh,"Remins me of the first Alchemist GPUs which were abysmally slow in older games and there were headlines of like ""new drivers bring 30% more performance!"" every few months an then it turned out it got that benefit in some DX9 game nobody play anymore than the rest barely improved...",hardware,2025-09-30 16:32:16,6
Intel,nh0wo3q,"Once Intel figures out how to make QuickSync use the full potential of these cards, they’ll be unmatched for anyone that does video work.",hardware,2025-09-30 15:45:12,7
Intel,nh43mkf,They are worth if they're at MSRP. 12GB for $249 is great.,hardware,2025-10-01 01:38:32,3
Intel,ngzr9zq,"Since 2600 is Zen+, I would've been more interested to see where 3600/Zen 2 landed between the Zen+ and Zen 3 CPUs since my old 3600 is in my brother's PC as an upgrade from i5-4460 and his GTX 1060 is starting to fail (one of the vram chips has errors in nvidia mats after running mods so it sometimes runs fine in idle but under load it randomly freezes [can take seconds or an hour] and/or artifacts and then either TDR manages to reset the GPU or not and PC crashes) so he's also now rocking my old GTX 960.",hardware,2025-09-30 12:03:02,14
Intel,ngznzyz,">still struggles with 2600  as someone who upgraded from Ryzen 3800X -> 5600X -> 5800X3D -> 9800X3D, I noticed all upgrades in CPU-heavy games even at 1440p (hello, MMORPGs or Escape from Tarkov) and if you're playing with Ryzen 2600 in 2025(almost 2026) you'll end up with mediocre experience even without considering overhead problem - I agree with your point, I just think that in 2025 Ryzen 2600 is an outdated CPU and considering AM4 upgradability, it should be changed to 5600/5600x3d/5700x3d/5800x3d to get a proper experience without being CPU-limited in some games.  [AMD Ryzen 7 9800X3D Review - The Best Gaming Processor - Game Tests 1080p / RTX 4090 | TechPowerUp](https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/18.html)  If we look at CPU charts at 1080p, Ryzen 2600 is not even on this list because of how bad it is.",hardware,2025-09-30 11:40:42,34
Intel,ngzpwyb,Imagine buying a brand new current gen GPU in 2025 while still gaming on 7 year old Zen 1+ fabbed on that delicious Global Foundries 12nm.,hardware,2025-09-30 11:53:51,17
Intel,nh0lnvm,"Yeah, in EU the 9060XT is 20€ more, and for that you get:  * noticeably more perf. * more mature driver, game support, feature,... * no random issues like this (and several more)  IDK whether anyone would even consider the B580 here.",hardware,2025-09-30 14:51:38,8
Intel,nh0wxxc,2600 is slower than haswell in games,hardware,2025-09-30 15:46:29,5
Intel,nh1ez6e,At 1080p.   Now have HWU go test TLOU2 or any other memory intensive stress test they’ve done in the past with any other 8gb card.   B580 scales excellent up to 1440p.   The 8gb cards are going to fall flat on their face.,hardware,2025-09-30 17:13:53,2
Intel,ngzpyol,Don't know about discrete but integrated graphics are fine on Linux with DXVK.  Played GTA IV with Tiger Lake Iris Xe using Proton and got 40-60 FPS at 1080p High settings with the random FPS drops I get in Windows completely gone.,hardware,2025-09-30 11:54:10,8
Intel,ngzx7if,Good thing the drivers are Open Source so The Community(TM) can improve them.,hardware,2025-09-30 12:40:59,3
Intel,nh01r2x,I’m guessing this is because of the dumpster fire that is Xe drivers,hardware,2025-09-30 13:07:24,-3
Intel,ngzy29f,"Share the post you're talking about, I shared this video in \~5 minutes timeline after it was released on YouTube, and your scenario is not realistic.",hardware,2025-09-30 12:46:08,-1
Intel,nh11qeb,I'm Scottish and it's related to [Battle of Sauchieburn.](https://en.wikipedia.org/wiki/Battle_of_Sauchieburn),hardware,2025-09-30 16:09:51,3
Intel,ngztrfz,"Well, that's to be expected if a game doesn't actually require more than 8GB.  But there's also the second issue (at least in some games) that a game simply reduces texture quality etc. automatically without your consent if it would go above the VRAM limit otherwise or some textures actually take a lot longer to load in properly which is not reflected in the framerate comparison.",hardware,2025-09-30 12:19:23,10
Intel,nh079mx,"He's using 1080p medium to lower the stress on the gpu and better reveal the driver problem.  Use a more demanding setting and maybe the vram becomes relevant, but that's a different video.",hardware,2025-09-30 13:38:11,2
Intel,nh129qi,See for yourself:  https://youtu.be/gfqGqj2bFj8?t=700  7028 and up seem to have that fix.,hardware,2025-09-30 16:12:29,2
Intel,ngznxne,"What do you mean in two generations? Arc at MSRP is a perfectly viable choice. It has pros and cons, but Arc is competitive.",hardware,2025-09-30 11:40:16,20
Intel,nh08r3d,actually previous tests have shown amd to have less overhead than nvidia [https://youtu.be/JLEIJhunaW8](https://youtu.be/JLEIJhunaW8),hardware,2025-09-30 13:46:14,10
Intel,nh12rht,"The change happened in August with the 7028 version driver release, odds are they've been working on this for months.  Intel has really good engineers too. Nvidia has nothing to do with this.",hardware,2025-09-30 16:14:55,6
Intel,nh0zk2v,"I really hope you’re joking here. You know an announcement of future products doesn’t automagically mean what you wrote, right?",hardware,2025-09-30 15:59:02,7
Intel,nh4lxwi,MLID is a clown,hardware,2025-10-01 03:35:50,7
Intel,nh42xjh,8AI+ is an underrated handheld.,hardware,2025-10-01 01:34:22,4
Intel,nh4t382,intel *should* be dominating the handheld space.  their mobile cpus are plenty good and their strategy of offloading gpu work to the cpu makes tons of sense on an igpu.  honestly i was surprised the 1st gen claw wasn't competitive.,hardware,2025-10-01 04:28:29,1
Intel,nh09xbg,"But it still does suffer in many games in the video.  The video title is misleading. This should be celebrated, but not as a ""fix"".",hardware,2025-09-30 13:52:31,7
Intel,nh02rn5,For one game. I'd like to see which games improved and by how much.,hardware,2025-09-30 13:13:08,10
Intel,nh2fll0,"I mean, that's how drivers mature.  Some game saw 30% boost, but everything else using that code maybe saw a 1% boost.  And then you do the same for another game and so on until you've covered the weird cases and everything else gets a stack of little 1% boosts that collectively add up to their own big boost.  It's a bunch of work and doesn't happen overnight.",hardware,2025-09-30 20:09:58,10
Intel,nh3pczp,I think that was just them bolting the open source dxvk into their driver.,hardware,2025-10-01 00:14:43,3
Intel,nh3b3w7,"> abysmally slow in older games  This is still the case in some games. One that Ive had first hand experience with is Resident Evil Revelations 2.  For some reason theres a shader caching issue where for the first 3ish minutes on every level its single digit frame rates. After it ""loads"" everything its perfectly normal.    Never had that issue on my older, far less powerful, system.",hardware,2025-09-30 22:51:42,2
Intel,nh35w7r,"They're already the fastest card for AV1 and other codecs outside of industry-made FPGA/ASICs for television studios. Primere Pro, DaVinci, and other editing software have to support it.",hardware,2025-09-30 22:21:47,6
Intel,nh46bjq,"Maybe it's region-dependent, but where I live the 8Gb 9060XT is 10% more but it's a \*lot\* faster.  I don't think either of those represents great value. One is crippled because of the VRAM, the other one is just slow for the asking price.  IMO the 9060XT 16Gb is the cheapest GPU genuinely worth buying right now. The B580 and the nVidia and AMD options closest to it have too many drawbacks.",hardware,2025-10-01 01:54:44,2
Intel,ngzrzih,"CPU is king if you're looking for a smooth and responsive system - sure, gpu well likely dictate average fps, but fps dips tend to be due to cpu and in the cases where they aren't, you can just adjust graphics settings.",hardware,2025-09-30 12:07:44,17
Intel,ngzs35l,"Hardware Canucks made a whole video on the [5060ti vs 9060xt on multiple CPUs](https://youtu.be/NqRTVzk2PXs)  Even Nvidia and AMD cards to an extent suffer from notable performance losses particularly on more CPU reliant games like spiderman from the 2600's level of performance. The CPU is just too weak to run with midrange and higher cards in 2025. I remember my 2600x struggling to maintain 165fps in valorant of all games. The moment I moved to a 5600 it shot up to 300 fps, and this was just on an rtx 2060.",hardware,2025-09-30 12:08:24,23
Intel,ngzsfqf,"> and if you're playing with Ryzen 2600 in 2025(almost 2026) you'll end up with mediocre experience  If you were playing CPU heavy games, it was a mediocre experience even back when it released.  Anyone who cared about CPU performance when it came to gaming. Didn't eve look at Zen before Zen 2. That's when they started getting some wins in ST heavy games thanks to the cache. But the double CCX layout still limited performance. Then with Zen 3 is when they finally reached parity with Comet Lake.",hardware,2025-09-30 12:10:44,18
Intel,nh4vt3w,"I have a 5600x and plan to get the 5800x3D soon, did you notice much performance gains when you upgraded?",hardware,2025-10-01 04:50:36,1
Intel,nh0jytx,"If you have a limited budget, you would upgrade part of the system at a time and whatever limits you hit.  Not everybody have unlimited money glitch to upgrade everything all at once to the latest and greatest.  I prioritize my upgrades to CPU, so but had to upgrade my RX480 earlier this year when game (e.g. Indiana Jones) start requiring raytracing to run.  I upgraded my CPU after fing a good deal on 5800X and also due to Windows 11 won't official run on my 1700.",hardware,2025-09-30 14:43:20,7
Intel,nh0jawn,"Entirely plausible, if you can't afford a rig overhaul but your GPU shits the bed.",hardware,2025-09-30 14:40:04,1
Intel,nh2qobb,It's a budget GPU; I'm not exactly sure why you think that's not a valid use case.,hardware,2025-09-30 21:02:17,1
Intel,nh1zpz7,>IDK whether anyone would even consider the B580 here.  Media and productivity tasks.  Quicksync is way more faster and more quality especially in AV1. More formats like HEVC 10-bit 4:2:2 are also supported.  Blender score is much higher for B580 than 9060XT.  And 12GB VRAM makes a difference compared to 8GB.,hardware,2025-09-30 18:52:50,8
Intel,nh2w8ol,"It's slower than 6700k and 7700k at max boost frequency in many games but Haswell (core 4000 series) is a stretch. Maybe it's slower than a highly overclocked 4770k in specific titles that are highly frequency dependent. But not overall. There are enough modern games that benefit from 12 threads over 8, back when 2600 launched there definitely were not.",hardware,2025-09-30 21:30:01,6
Intel,nh2qyng,Bullshit. Based on what?,hardware,2025-09-30 21:03:39,2
Intel,nh4lk1r,">B580 scales excellent up to 1440p.   Despite its VRAM deficit, the 9060 XT 8 GB is 23% faster than the B580 at 1440p with maximum settings according to [TechPowerUp](https://tpucdn.com/review/powercolor-radeon-rx-9060-xt-reaper-8-gb/images/relative-performance-2560-1440.png).",hardware,2025-10-01 03:33:07,1
Intel,ngzz3l3,"I have an Intel Arc A380, it's supposed to be more or less equal with Radeon RX 6400, but for example Guid Wars 2 runs at around 80-90 FPS on 6400 in some of the older zones (outside of Lion's Arch), but only 38-39 FPS on Arc A380. It's not a problem for me, because I didn't buy it for its Vulkan performance, I just needed a GPU that can run 3 monitors and ideally with hardware encoding and A380 is great for that. That's not only my observation, benchmarks on Phoronix show the same story.",hardware,2025-09-30 12:52:15,3
Intel,nh01vf6,"Tiger Lake uses i915, Battlemage uses the dumpster fire that is Xe to put it kindly",hardware,2025-09-30 13:08:05,3
Intel,nh1729r,"> Played GTA IV with Tiger Lake Iris Xe using Proton and got 40-60 FPS at 1080p High settings with the random FPS drops I get in Windows completely gone.  Eh, a game made originally for the xbox360 over a deade and a half ago almost getting 60fps in 1080p is not the reassurance you think it is.",hardware,2025-09-30 16:36:03,3
Intel,ngzycwi,https://www.reddit.com/r/hardware/comments/1nua9bm/huge_arc_b580_news_intel_fixes_cpu_overhead/   It's the second one that shows up under new(older).   Maybe it was hidden and a mod approved it?,hardware,2025-09-30 12:47:54,5
Intel,nh04mh7,Is it noticeable by average person? like how switch 2 has bad display? I think more people care about the frame pacing. Wonder why steve doesn't include 0.1% lows,hardware,2025-09-30 13:23:29,5
Intel,nh2uvvj,Sweet then haha,hardware,2025-09-30 21:23:05,1
Intel,ngzp15m,I want more performance.,hardware,2025-09-30 11:47:50,0
Intel,ngzp1uf,I want more performance.,hardware,2025-09-30 11:47:58,-4
Intel,nh0b3u4,It's a Ryzen problem. On way an i3-10100 is supposed to match a R5 3600. They should've included an i5-10400.,hardware,2025-09-30 13:58:42,-1
Intel,nh15ub6,It was previously assumed that overhead was a hardware issue and likely not fixable for this generation. Pretty big development I would say,hardware,2025-09-30 16:30:02,20
Intel,nh496z8,Progress is progress,hardware,2025-10-01 02:11:49,1
Intel,nh37zmt,"Exactly. Once they nail QuickSync on it, it’ll be a no-brainer.",hardware,2025-09-30 22:33:45,2
Intel,nh0ym7f,"> If you have a limited budget, you would upgrade part of the system at a time and whatever limits you hit. Not everybody have unlimited money glitch to upgrade everything all at once to the latest and greatest.  If you have a limited budget, (EDIT ADDED: And you're starting from scratch where you don't have a working system) it’s often best to save up instead, and do a complete build when you can afford it. You often get a lot more for your money that way.",hardware,2025-09-30 15:54:30,5
Intel,nh23ibr,"Welp my comment was about gaming (as the video)  But even if you consider productivity and media, the Arc still got dunked on by Nvidia card.  * The 5050 (240€) is 20€ cheaper, similar gaming performance, 20% faster blender perf. * The 5060 (280€) is 20€ more, 20% more gaming perf, **more than 50%** perf in blender.  You also get the whole DLSS RTX shebangs, for media NVENC is at least equal if not better than QuickSync. The only thing the arc got going for it is the 12GB.  So yeah unless you absolutely needs the 12GB, this card will not be appealing at all at its price. For 200€ then maybe it might make sense I guess.  Source for blender perf [here](https://opendata.blender.org/benchmarks/query/?device_name=Intel%20Arc%20B580%20Graphics&device_name=NVIDIA%20GeForce%20RTX%205050&device_name=AMD%20Radeon%20RX%209060%20XT&device_name=NVIDIA%20GeForce%20RTX%204060&device_name=NVIDIA%20GeForce%20RTX%205060&blender_version=4.5.0&group_by=device_name)",hardware,2025-09-30 19:11:13,6
Intel,nh0dsa9,"I'm pretty sure that i915 supports Arc dGPU as well. You need to use certain kernel flags, similar to how you need to disable nouveau for Nvidia, the details of which vary by distro.  [https://dgpu-docs.intel.com/devices/hardware-table.html](https://dgpu-docs.intel.com/devices/hardware-table.html)",hardware,2025-09-30 14:12:29,4
Intel,nh20flx,"Xe doesn't have poor performance, it's the userspace anv driver to blame. On Arc A series and meteor lake SoCs phoronix reported massive performance gains by switching from i915 to Xe.",hardware,2025-09-30 18:56:16,2
Intel,nh1b3bd,"Play the PC version if you can some time. It causes drops to less than 40 FPS on Windows with its DX9 renderer even on a modern, cheap GPU like the 6500 XT.  Drops which disappear when playing with Proton on Linux.",hardware,2025-09-30 16:55:35,5
Intel,ngzyq65,"I don't know why, but yes, it was hidden/didn't exist back then and when I posted this video there were no other posts visible, which is proven by upvote ratio on that video you shared.  I think it's either mods/Reddit auto-flag system.",hardware,2025-09-30 12:50:03,13
Intel,nh05wc4,"Super noticeable when large textures take a million years to load in, or are constantly swapping from low to high resolution as they keep being evicted from and reloaded into VRAM. If they never load in the first place, I suppose some people might not realize the game shouldn't look like mud.",hardware,2025-09-30 13:30:35,6
Intel,ngzqbso,Isn't that exactly what the driver update is providing?,hardware,2025-09-30 11:56:37,9
Intel,nh0q89t,"It's an nVidia problem, well-known for years, which exists for both Intel and AMD processors.",hardware,2025-09-30 15:13:50,5
Intel,nh38s8l,"Intel has been putting in work on their drivers. They've been playing catch-up with AMD and Nvidias years of experience.    ...but lets not pretend they're making maracles here. Lots of these driver updates claim +200% improvements when in reality that takes a game from literally broken to ""playable""  I say this as an early adopter of an A770 and have been reading driver patch notes every single time.",hardware,2025-09-30 22:38:20,5
Intel,nh1guiz,"Of course, and as I said, it should be celebrated.",hardware,2025-09-30 17:22:35,3
Intel,nh2jgp0,"Not really... If you only game it's worth upgrading the GPU more often, if you mostly work, the GPU can be upgraded every console cycle or less.  Unless you mess up with bad choices, e.g. buying the last 4 threads cpu ever made.",hardware,2025-09-30 20:28:19,1
Intel,nh25nh9,"Quicksync has less artifacts/blockiness in actual AV1 side-by-side comparisons. And encoding speed is faster. And it has two encoders.  Also 8GB is very bad for the price, you could have 8GB RX480 for $220 in 2016. Plenty of games have issues with 8GB already and it will only get worse.",hardware,2025-09-30 19:21:33,3
Intel,nh3w0w3,"I think they mean the range lacks a high end, or even a midrange.",hardware,2025-10-01 00:53:57,2
Intel,nh2ucp0,"Yes but I am specifically looking for a lot more performance, the B580/9060XT isn't enough performance for me.     I'm saying within 2 generations because i'm hoping within those 2 generations they'll be at an affordable price while being similar to a 5090 performance.",hardware,2025-09-30 21:20:24,0
Intel,nh2ncr8,"I thought about it and you're right. I was thinking about it in the context of either not having any system, or having one so old that it's useless for your intended use case.. where you would buy parts every few months before finally having a complete system a year or two later. In that scenario, I'm 100% right. On the other hand, if you have a workable system, then sure, it makes sense to upgrade where needed, and in that context, you're the one that's right. I'll edit my comment.",hardware,2025-09-30 20:46:38,1
Intel,nh3qset,Hardly anyone is encoding video with their PC's the few times they do it the CPU works just fine.  Buying a GPU to stream video no one is watching is daft.  Plex transcoding is fast enough on modern CPUs there really isn't a need for GPU transcoding unless you are streaming to the neighbourhood.,hardware,2025-10-01 00:23:09,1
Intel,nh3v3o0,If you're using it in places where quality matters you're using it wrong.,hardware,2025-10-01 00:48:33,1
Intel,nh2k2zb,"Why even encode to AV1 with quicksync? Streaming? I thought that wasn't supported by major platforms, and on most others it will be reencoded anyway.  For everything else you're going to get higher quality and compression with CPU encoding (svt-av1, x265, whatever)",hardware,2025-09-30 20:31:10,0
Intel,nfxvgt2,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,hardware,2025-09-24 13:00:51,32
Intel,nfxs3mr,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",hardware,2025-09-24 12:41:08,36
Intel,nfxrqaw,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,hardware,2025-09-24 12:38:53,15
Intel,nfy52qu,I do have to wonder how much we are missing out by these features being proprietary rather than having graphics vendors work with other stake holders and each other to make open cross compatible upscaling and frame generation techniques. It's been great for nvidia but bad for the ecosystem as a whole for everything to be so fractured.,hardware,2025-09-24 13:53:35,12
Intel,nfz9psy,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",hardware,2025-09-24 17:11:03,8
Intel,nfyjkgh,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",hardware,2025-09-24 15:06:03,2
Intel,nfxz0x9,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,hardware,2025-09-24 13:20:54,2
Intel,nfz9315,we are witnessing the downfall of pc gaming in real time,hardware,2025-09-24 17:08:01,4
Intel,nfxotxk,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-24 12:20:56,1
Intel,nfxyif8,This could be awesome more completion The better,hardware,2025-09-24 13:18:01,1
Intel,ng021ru,Why should they? They are going to buy NVidia GPUs for everything now.,hardware,2025-09-24 19:27:35,1
Intel,ng09fp7,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,hardware,2025-09-24 20:03:42,0
Intel,nfxyqy3,"Multi-Post News Generation, with three articles interpolated per source.",hardware,2025-09-24 13:19:21,42
Intel,nfxswgf,I'm excited for Intels new GPU,hardware,2025-09-24 12:45:57,7
Intel,ng28c6g,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",hardware,2025-09-25 02:43:17,11
Intel,ng09kyr,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",hardware,2025-09-24 20:04:24,4
Intel,nfy71ww,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,hardware,2025-09-24 14:03:45,-7
Intel,nfxzd6m,Competition *,hardware,2025-09-24 13:22:47,1
Intel,nfydv2b,Obligatory article quoting reddit post quoting another article quoting original reddit post.,hardware,2025-09-24 14:38:06,11
Intel,nfy8e3s,"Fake frames, fake articles! /s",hardware,2025-09-24 14:10:35,6
Intel,ng9nabn,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,hardware,2025-09-26 07:24:34,5
Intel,ng3cb6o,"Yes but FSR support is all over the place. Look at what optiscaler is doing, we could have had an open standard for upscalers that FSR XESS and DLSS could have been built on top of meaning much wider support across games instead of every game needing specific implementation and leaving us with outdated upscalers that we need driver overrides and DLL swaps to get around.  Microsoft is only now working on a directX based upscaler API that solves this problem. We should have had something like that years ago like we did for RT.",hardware,2025-09-25 08:18:28,3
Intel,nfy9ky6,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",hardware,2025-09-24 14:16:39,5
Intel,nfy9qc1,"It makes adoption slower though. Especially for smaller devs and the smaller graphics vendors. Even now with pretty large games we still have software lumen only. If RT was pushed as an open standard earlier we might actually have more games and better implementation across the whole market, not just nvidias pet projects.",hardware,2025-09-24 14:17:24,4
Intel,ngt9vbz,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",hardware,2025-09-29 11:53:54,1
Intel,ngt9z2u,"patents expire after 15 years, they will have to share it then.",hardware,2025-09-29 11:54:37,1
Intel,nfykmyd,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,hardware,2025-09-24 15:11:13,11
Intel,nfyiwdp,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",hardware,2025-09-24 15:02:46,9
Intel,ng9ndqw,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,hardware,2025-09-26 07:25:31,3
Intel,ngta2hh,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,hardware,2025-09-29 11:55:17,1
Intel,ngtlhim,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",hardware,2025-09-29 13:07:31,1
Intel,nfyowfp,"> RT in games is now done through DirectX APIs, which are vendor agnostic.  Now being the key word, Nvidia launched their RTX before DXR was even available.  AMD's slow adoption is absolutely one of the problems which might not have not been so delayed if AMD, Nvidia, console makers and Microsoft worked on RT together from the start.   This is all just musing really. Maybe nvidia did us all a favour by breaking the mold forcing everyone else to play catch up.",hardware,2025-09-24 15:31:47,4
Intel,ngtvb8k,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",hardware,2025-09-29 14:02:39,1
Intel,ng9nh90,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,hardware,2025-09-26 07:26:29,3
Intel,nf42stl,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,hardware,2025-09-19 18:12:40,34
Intel,nf2t8pz,"Okay then, what's the Xe GPU roadmap looking like then?",hardware,2025-09-19 14:34:57,74
Intel,nf43nvh,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",hardware,2025-09-19 18:16:54,12
Intel,nf4q7tq,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,hardware,2025-09-19 20:09:01,12
Intel,nf31f8h,They barely lived on before the deal.,hardware,2025-09-19 15:13:57,35
Intel,nf53r9t,"It'll live on our hearts, yes.",hardware,2025-09-19 21:17:43,10
Intel,nf3f8y9,Lol if you believe that I have a bridge to sell you in Brooklyn,hardware,2025-09-19 16:20:00,15
Intel,nf7xz74,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",hardware,2025-09-20 09:42:21,3
Intel,nf8zcs6,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",hardware,2025-09-20 14:09:11,1
Intel,nfacpei,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,hardware,2025-09-20 18:18:08,1
Intel,nf8psx7,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,hardware,2025-09-20 13:16:21,0
Intel,nf605ki,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",hardware,2025-09-20 00:24:16,8
Intel,nfcc5gm,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",hardware,2025-09-21 00:57:40,3
Intel,nf2xqty,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",hardware,2025-09-19 14:56:28,34
Intel,nf53uu0,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,hardware,2025-09-19 21:18:14,5
Intel,nf69muk,"Always selling out actually, they just can't produce that much",hardware,2025-09-20 01:23:31,11
Intel,nf3gd7v,And I have another if you think nVidia is capable of keeping this deal running for that long...,hardware,2025-09-19 16:25:26,7
Intel,nfho1ld,Intel will hopefully split their fab business from the rest of the company either way.,hardware,2025-09-21 21:10:12,1
Intel,nf3itwc,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",hardware,2025-09-19 16:37:23,-20
Intel,nfe3v2w,> they just can't produce that much  because they are losing money on them,hardware,2025-09-21 09:31:45,4
Intel,nfkj39o,Trade bridges.,hardware,2025-09-22 09:21:12,1
Intel,nf6x83x,Intel's arc is dead with or without nvidia deal.,hardware,2025-09-20 04:05:06,1
Intel,nf3naf4,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",hardware,2025-09-19 16:58:38,23
Intel,nf41fhg,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,hardware,2025-09-19 18:05:57,16
Intel,nf5ky00,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",hardware,2025-09-19 22:54:47,9
Intel,nfjjjlm,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",hardware,2025-09-22 03:50:01,1
Intel,nf72gvm,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",hardware,2025-09-20 04:47:19,1
Intel,nf5bseq,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",hardware,2025-09-19 22:01:34,4
Intel,nf5z77h,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",hardware,2025-09-20 00:18:20,4
Intel,nfjkc77,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,hardware,2025-09-22 03:55:54,2
Intel,nf73hs7,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",hardware,2025-09-20 04:55:37,0
Intel,nf6x2qk,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",hardware,2025-09-20 04:03:55,1
Intel,nflcaft,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",hardware,2025-09-22 13:04:14,2
Intel,nf729ff,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",hardware,2025-09-20 04:45:38,2
Intel,nf74cdn,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",hardware,2025-09-20 05:02:36,3
Intel,nf1rhgr,"> If using NVIDIA RTX iGPU in Intel SoC, that will leave only discrete Intel Arc designs to be sold independently.  Assumes without evidence that Nvidia's GPU will immediately replace Xe in every SKU.",hardware,2025-09-19 10:49:14,174
Intel,nf1yoyh,Oh those sweet promises. How credible it is!,hardware,2025-09-19 11:42:57,39
Intel,nf23nol,I wonder what this will mean for running llms locally. Could I buy a laptop with 128 gb of ram and an Nvidia iGPU and have that memory unified with the GPU to run the models?,hardware,2025-09-19 12:15:26,9
Intel,nf1ucvo,"Sure it does..... Ignore all the signs, we are fine, nothing to see here.",hardware,2025-09-19 11:11:37,70
Intel,nf29be5,If the roadmaps haven't changed does that mean there's still a chance we'll see Celestial gpus in 2024?,hardware,2025-09-19 12:49:48,7
Intel,nf44w50,Rtx “igpus” are probably in a different power envelope to igpus..,hardware,2025-09-19 18:22:55,6
Intel,nf1vsvb,"Xe will live in Desktops and high end HX which uses the same chips. Xe3 is done. Xe4 is probably done too. So i expect them to be used here like HD Graphics and will get very little improvements from Xe5 onwards. Just to keep display and needed functions.  In a way they are still commited to GPUs   H, U and V are almost certainly getting their Arc GPUs replaced with RTX ones.  We all know with RTX tech those Laptop CPUs will sell a whole lot more and make intel way more profit. Anything with Nvidia's tech sells like pancakes now.  So going forward they can't recover Xe development costs through the laptop iGPUs using like they did before. Remember laptop iGPUs are the highest volume ARC sales.  Without those sale it's hard to justify full on Xe spending needed for dGPUs. Especially with Intel's financial situation.",hardware,2025-09-19 11:22:25,22
Intel,nf2yisf,"This deal seems crazy.  For Nvidia that's an easy way to get custom x86 CPUs for datacenters and a way to directly compete with Strix Halo tier products.   If the products suck they can just blame Intel and walk out as if nothing ever happened. They don't really have much to lose here.  Even if they cancel their own CPU cores as a result of this deal, they can still use stock ARM cores in the future when/if the partnership ends...  But for Intel? For Intel this seems like a complete nightmare.  What did it take for them to convince Nvidia? It's hard to believe this won't have any impact on Arc... In the worse case scenario they'll be giving up on high-end graphics, just for Nvidia to abandon them right after when x86 isn't as important anymore...  This sounds like a desperate gamble, and it's difficult to understand where it came from because Intel does have pretty good iGPUs... Why are they so desperate?",hardware,2025-09-19 15:00:08,12
Intel,nf2hoae,Intel's market cap is 141 billion.  Nvidia's 5 billion investment doesn't give Nvidia enough ownership to start calling all the shots.,hardware,2025-09-19 13:35:51,7
Intel,nf4v61k,"I wonder why MLID seems to want so much that ARC be cancelled...  How many times has he brought out ""leaks"" about ARC getting axed in the last few years? I stopped counting.",hardware,2025-09-19 20:33:41,5
Intel,nf4l9hk,Why are we not linking to the original source?  https://www.pcworld.com/article/2913872/intel-nvidia-deal-doesnt-change-its-roadmap.html,hardware,2025-09-19 19:44:36,3
Intel,nf2euhm,Remember how well it went when Intel shipped an AMD iGPU?,hardware,2025-09-19 13:20:36,2
Intel,nf5s2ve,With the announcement I imagined Nvidia branded products. Mini-PCs where the CPU and GPU aren't replaceable. Nvidia having Intel and Mediatek products. The integrated GPUs for Intel have seemed pretty solid for a very long time. The discrete cards are competitive in price at least. I wouldn't bet on them quitting discrete cards. Don't think they'd want to be caught with nothing again if another major novel GPU algorithm pops up and starts a frenzy again,hardware,2025-09-19 23:36:07,2
Intel,nf2qfwi,"Intel iGPUs are equal or better than AMD outside of the one on the 395. 140v is performant and efficient, 140t is on par with the 890m but paired with a better CPU.",hardware,2025-09-19 14:21:12,2
Intel,nf741x9,"Well, I hope they will have them, but I have a feel they will only be available for servers.",hardware,2025-09-20 05:00:11,1
Intel,nf76fkj,Yeah right.. this will age well... Nvidia definitely wants more competition...,hardware,2025-09-20 05:20:07,1
Intel,nflux04,"They just want to clear Arc inventory, development has stopped.",hardware,2025-09-22 14:42:51,1
Intel,nf2ben9,Intel has struggled with GPU for decades which is insane when look what Apple has accomplished with their iGPUs. Intel was decimated from the insides out when the C-suite decided asset stripping via sharebuybacks and gutting employees/R&D etc.. in addition to taking in debt.  Not sure if Intel can recover but interesting to see all this corporate heroic medicine trying to save the patient.,hardware,2025-09-19 13:01:39,-6
Intel,nf1w0x1,Which is also a big reach to assume when for a large portion of intel's market the current intel iGPU is already faster than they need or will need for the forseable future.,hardware,2025-09-19 11:24:06,81
Intel,nf28f98,"Much more likely the intel/Nvidia collab is for a few special new products, not a complete change in how intel has done graphics for decades.",hardware,2025-09-19 12:44:36,10
Intel,nf1waqd,"Not immediately, but Intel knows CPUs with RTX GPUs will sell a whole lot more than CPUs with ARC.. Money is what Intel desperately needs.  So don't be surprised if they do it fast.",hardware,2025-09-19 11:26:06,22
Intel,nf2jdtv,There are definitely more Intel ARC iGPU users than Arc GPU users. This will lead to a cut in workforce (already happened) and more in the future. This also means less investment into things like XeSS.,hardware,2025-09-19 13:44:59,2
Intel,nf748mc,The thing is that Intel barely sells any dedicated ARC cards. Almost all of their graphics userbase comes from Intel integrated graphics with their new XE tiles on them. These will be replaced by the Nvidia tiles.      The result of this is that basically every Intel laptop will now have Nvidia inside.,hardware,2025-09-20 05:01:42,0
Intel,nf7yurd,"Yes, I doubt Nvidia would ""give"" iGPU without any licensing/fees.",hardware,2025-09-20 09:50:56,0
Intel,nf24pgq,If the bandwidth is as low as most PC laptops currently have it will be slow. Even Strix Halo and DGX have this problem and they are better than most laptops.,hardware,2025-09-19 12:22:02,17
Intel,nf4ubpb,Nvidia already has this with the the DGX Spark and that same chip is coming to laptops.,hardware,2025-09-19 20:29:25,3
Intel,nf2nlnf,"That's likely the plan, and they should be able to pull it off. Just a question of whether they can beat AMD and Apple at it.",hardware,2025-09-19 14:06:58,0
Intel,nf75l6u,Intel really needs to use a unified memory design like Apple or they will always look like a joke. They really should be buying a company that makes memory.,hardware,2025-09-20 05:13:02,0
Intel,nf1wpmb,Reminds me of Stadia. All the signs were there that Google would pull the plug but many were optimistic. Google even said they are committed.  Then it got killed.  Edit - found a article that lists how many times Google said they were committed.  https://www.theverge.com/2022/9/30/23378757/google-stadia-commitments-shutdown-rumors,hardware,2025-09-19 11:29:05,55
Intel,nf75ngc,"Yes, I think we'll see them next year.",hardware,2025-09-20 05:13:33,3
Intel,nf23vcu,"Doubt they’ll remove the option for Arc completely from the H, U and V series. NVIDIA is most likely charging a pretty penny for their GPUs, which will make Intel completely non competitive in a lot of markets if they were to just switch to NVIDIA.  At minimum, it will stick around for the H and U series as a complimentary option.  Not everything magically sells better just because it has NVIDIA on it",hardware,2025-09-19 12:16:48,28
Intel,nf25qyb,I don't see how Intel might be getting more profit from these tbh... Revenue? Yes. Profit? From where?  Especially if Nvidia will continue to use exclusively TSMC nodes.,hardware,2025-09-19 12:28:28,12
Intel,nf28eb8,"Killing Xe3 and Xe4 is so weird to me when Panther Lake is supposed to launch this year, and Nova Lake (which is rumored to to use the Xe4 media block and use Xe3 for the graphics/shaders/compute block) is supposed to launch sometime in 2026.   Like, the volume for both ""launches"" is supposed to be low, actual devices probably won't be available to purchase until the next year, but. Companies are supposed to be receiving samples of Panther Lake. Now. Presently.   For them to say their roadmap isn't affected, isn't changed at all. Unless that's a massive lie, that's only possible if samples of integrated Xe3 have already been fabbed, are already in working silicon.   ...Like, not to get too into the rumor mill, but supposedly the early Panther Lake samples aren't doing great. The CPU side isn't very efficient compared to Lunar Lake, and the GPUs sometimes don't work. But, when they do work, supposedly, the performance is really good, and, again, rumors, but the problem is supposedly more about Intel's drivers than the hardware or architecture itself.   ...I can completely believe that Intel would panic and would rather kill their GPU division entirely than. Invest in their software stack. Developing good technology and then abandoning it instead of advancing, because abandoning it is cheaper and easier is an extremely Intel move.   But if nothing else, the sunk costs for Xe3 at least make me feel like. They sorta have to figure it out, if only because they don't have time to replace it with NVIDIA, in the time frame Panther Lake has to come out.",hardware,2025-09-19 12:44:26,12
Intel,nf2l9et,"There is just simply no way Intel is going to rely on Nvidia as the sole supplier of their iGPUs when their product lines. Theres even less chance that the U series, their low cost volume product line, would switch to Nvidia sourcing.   Nvidia iGPUs are going to be their own separate, lower volume product line, with its own designation. Maybe -G, or -N, or -AX",hardware,2025-09-19 13:54:52,8
Intel,nf227rk,I dont think so.  Why would they cut themselves from hundreds of bilions of dollars market this easily?  It doesnt make sense,hardware,2025-09-19 12:06:16,4
Intel,nf33ahx,">We all know with RTX tech those Laptop CPUs will sell a whole lot more and make intel way more profit. Anything with Nvidia's tech sells like pancakes now.  this makes no sense, it will sell more as opposed to what? the igpu needs a cpu anyway and nvidia's gpus mostly get paired with Intel's cpus anyway. Intel controls 80%+ of laptop cpu market and 65% of ALL igpu + dgpu market, why would they give up such a big lead? If anything this has the potential to cannibalize Nvidia's lower end 4050/4060 market since thats probably the performance these RTX SOC's will be",hardware,2025-09-19 15:22:54,1
Intel,nf3z1vb,"Intel has already given up on high end graphics, this deal has no impact on Arc cause Arc already has no future.",hardware,2025-09-19 17:54:21,3
Intel,nf80rkx,Intel will want the nvidia name on everything regardless as they know it will sell,hardware,2025-09-20 10:09:33,2
Intel,nfkg2kz,The actual original source is here: https://events.q4inc.com/attendee/108505485/guest  But youll need to register to see it.,hardware,2025-09-22 08:49:54,2
Intel,nf7628f,A lot of AMD's are using outdated nodes and architecture.,hardware,2025-09-20 05:16:59,4
Intel,nf2n7gn,Intel's biggest struggle with GPUs is gaming compatability and drivers. Apple certainly isnt doing great their either.,hardware,2025-09-19 14:04:56,8
Intel,nf6g09x,"I love my laptop with an Intel iGPU, it can actually last me through a work call compare to my other laptop with an AMD chip plus a damn 4070Ti in it.",hardware,2025-09-20 02:04:17,9
Intel,nf1z591,Sure but a lot of Intel's laptop chip business has no need for (presumably) more expensive integrated RTX does it?  Of course for gamers and maybe even workstation laptops (which is a pretty tiny market) these chips will be very appealing but everything else... whatever flavour of integrated ARC Intel are currently developing will continue to be the norm.,hardware,2025-09-19 11:46:01,47
Intel,nf22yb2,"I would assume that the RTX iGPUs cover all the ""gaming"" SKUs, leaving the intel iGPUs for all the low-end and business ones.",hardware,2025-09-19 12:11:00,16
Intel,nf2j2tp,"Why would they sell more? We already have laptops with RTX dGPUs and they sell less than Intel iGPU laptops.   The Nvidia iGPU versions are going to be more expensive chips and the laptop prices will reflect that just as they currently do. There's going to be a *reason* to pay extra for the Nvidia versions, and thats going to be measurable more GPU performance. Not just low end Nvidia GPUs on-par with Arc iGPUs.",hardware,2025-09-19 13:43:22,13
Intel,nf34ngo,"Eh. 90% of people and especially companies don't have any need for high GPU performance in a laptop, so CPUs with cheaper Xe iGPUs will likely continue to make up the vast majority of their sales.",hardware,2025-09-19 15:29:24,6
Intel,nf2jvhz,"> Not immediately, but Intel knows CPUs with RTX GPUs will sell a whole lot more than CPUs with ARC.. Money is what Intel desperately needs.  Among gamers and people who need the power? Agreed!  Majority of users who don't game or need the power/expense.... nope!  What I like about this is that it pushes innovation further. Hopefully it brings innovation to the sub-$600 laptop market.",hardware,2025-09-19 13:47:35,3
Intel,nf4zm19,Average person doesn't care.    Most people don't play games on their computers all of the time.,hardware,2025-09-19 20:56:05,2
Intel,nf3bhys,Businesses do not care about GPU's in their laptops it will not sell a whole lot more.,hardware,2025-09-19 16:01:51,4
Intel,nf26ffi,Intel iGPUs are drastically better than Nvidia stuff for anything except Halo-type parts.,hardware,2025-09-19 12:32:38,4
Intel,nf2cfkd,"I think it may come down to the fact that the executives within a particular business unit probably ARE very committed to making it succeed and sticking with it, but the ultimate decision to pull the plug comes from higher up",hardware,2025-09-19 13:07:23,16
Intel,nf2ci1h,Ahh yes reminds me of this lovely Google Graveyard.   https://killedbygoogle.com/,hardware,2025-09-19 13:07:46,17
Intel,nf2cz1m,"The reason why Stadia failed was because they never implemented ""negative latency""",hardware,2025-09-19 13:10:22,5
Intel,nf26ssc,"Because a Laptop is a sum of its part.  Lets say for $400 IntelArc CPU and $500 IntelRTX.  When a laptop is built, if the IntelArc costs $1000 the  IntelRTX will cost $1100  The higher you go the closer it gets percentage wise.  So when the difference is less 10% which one do you think sell way more?",hardware,2025-09-19 12:34:55,1
Intel,nf7ph98,Nvidia is always looking for more fabs as they seem to be significantly supply limited at TSMC. If 18A/14A turn out good it's a safe bet they'll at least try it out.  Remember how Ampere customer cards were done on Samsung so they could use all of their TSMC allocation for A100. They would probably have continued that Samsung partnership if internal corruption at Samsung didn't bomb their 4nm process.,hardware,2025-09-20 08:16:33,1
Intel,nf2ltua,"Even **if** Intel were to completely cancel Xe and their entire business becomes dependent on Nvidia selling them iGPU chiplets (for some odd reason), the timeline for these Nvidia chiplet CPUs would be after NVL",hardware,2025-09-19 13:57:47,12
Intel,nf4nwrn,They've spent the past 1-2 years laying off much of their driver team.,hardware,2025-09-19 19:57:38,2
Intel,nf2397d,Excuse me but Hundreds of billions?   Since when has gaming GPUs make 100 of billions.  Nvidia makes just over 10 billion on Gaming GPUs a year.,hardware,2025-09-19 12:12:55,13
Intel,nf23aor,"They don't see themselves having a credible chance of capturing that market (GeForce + CUDA moat is deep) without having to trash their margins, they see short-term gain in being able to have the best gaming laptops on the market for a few generations, and being the choice to provide the CPUs for NVIDIA AI platforms.",hardware,2025-09-19 12:13:10,8
Intel,nf34qm4,If you haven't noticed intel hasnt been doing so hot.  Dell recently added AMD for XPS for the first time. Many OEMs similarly are doing amd versions for the first time. Something unthinkable even 5 years back   This way they losing OEM sales on the laptop space. They are no longer the preferred CPU brand.  So what do you do? Well having their CPUs with RTX would give them a surefire boost,hardware,2025-09-19 15:29:49,4
Intel,nf75btl,Intel is worried that they'll lose this market to the now superior AMD chips.,hardware,2025-09-20 05:10:49,0
Intel,nf75zfy,"Well AMD does make 4 billion each year from their console deals, and Intel cannot enter that space because they don't offer a product that is as compelling as AMD.      With this partnership they could do this.",hardware,2025-09-20 05:16:20,2
Intel,nf7hjtd,Oh we know they are trying to sell us outdated modes and architectures. And they will continue to do so for the entirety of 2026 in mobile.,hardware,2025-09-20 06:59:23,3
Intel,nf2oioo,Apple GPUs are significantly more powerful than any intel iGPU. I can play cyberpunk 2077 at 60fps 4K and nice visuals. Also I can run large AI LLMs as well. I can run heavy AI workloads all day long and total wattage 150-160w for the entire Mac Studio.  Intel iGPUs would melt/die. Its amazing how far Apple has pushed Apple Silicon while Intel got stuck in the mud.  Sad to see how the mercenary C-suite came in like a plague of locusts and gutted Intel leaving a husk that requires intensive care to resuscitate. I hope they do rise back up because competition is good and pushes progress forwards.,hardware,2025-09-19 14:11:39,-6
Intel,nf7famq,"If Optimus is working properly, your AMD laptop should not be consuming any more power than if it were APU-only during normal use.",hardware,2025-09-20 06:39:00,7
Intel,nf2o79q,this is likely more to compete with amd's apu's and less for business,hardware,2025-09-19 14:10:03,8
Intel,nf2lxvf,"I can see a consumer marketing reason and a business professional reason for RTX throughout: driver support. Anything with the RTX branding will get more support on both fronts, especially if the integrated parts default to studio level drivers. A large part of Arc’s inefficiencies are due to the lack of software support, so streamlining this aspect would be a huge boon for the “it just works” marketing coming back online for Intel.",hardware,2025-09-19 13:58:23,6
Intel,nf2mlx9,That could still mean Intel dGPU business will go away.,hardware,2025-09-19 14:01:50,4
Intel,nf74fm1,"It has been talked about a lot but SOCs will be replacing the lower end systems completely. For example, the Strix Halo chip that AMD just released is better than a RTX 4060. That may seem weak, but it's cheaper to provide an APU than a CPU+GPU.",hardware,2025-09-20 05:03:20,1
Intel,nf1zwvp,"I said profit.  Even if RTX CPUS cost more, they result in significantly more sales resulting in greater profits.  We know from the market, that even if its more expensive, Nvidia GPUs will still sell more than equivalent cheaper counterparts",hardware,2025-09-19 11:51:11,-10
Intel,nf247w4,"You mean the business sectors which are most likely to use AI? You know, what Nvidia is famous for.  And low-end don't exist for new CPUs for laptops anymore. Its all older generations rebarded.  Intel use older alderlake and raptor lake CPUs rebarded with UHD graphics and AMD sells Zen 2 CPUs.",hardware,2025-09-19 12:19:01,0
Intel,nf58gpv,Makes me wonder if we will see lower price devices with Nvidia iGPU V vs roughly equivalent dGPU models. There should be some saving I think but will also be interesting to see if there remains a choice or if Nvidia will try and move most of it's mobile lineup to this unified product.  I think on the very highest end like 80 and 90 tier mobile chips they may well be too powerful to adequately cool as part of a combined package.,hardware,2025-09-19 21:43:16,0
Intel,nf6grg6,"I honestly want a work laptop that has a good keyboard, screen, and battery life. (and light!)   I'm mostly doing emails, anything else I'm using a workstation for number crunching.",hardware,2025-09-20 02:09:14,1
Intel,nf26z7k,And how do you know this?  Cause last i remember Nvidia doesn't have iGPUs,hardware,2025-09-19 12:36:00,7
Intel,nf74rq5,"While we have yet to see good comparisons, it's provable that Nvidia produces more efficient graphics cards than Intel does. The die size on the Intel chips is massive compared to the similar performing Nvidia ones. Larger the die the more power it's using. Switching to Nvidia will provide more performance at the same power usage.",hardware,2025-09-20 05:06:08,2
Intel,nf2dntk,Latency on stadia was actually good. I tried it for a bit with with their pro subscription for a month.  But never renewed or bought anything cause of the awful business models.,hardware,2025-09-19 13:14:07,13
Intel,nf2mvv9,"And why would Intel do that? Theyre dropping MoP because they act as a middleman, moving MoP at cost to OEMs.  A $100 BOM increase per unit rarely only comes with a $100 product increase. And even then, it's corporate suicide to make Nvidia a critical sub-supplier when you've dont need them to be. Intel would never go in 100% on Nvidia being the sole iGPU option.   The Nvidia option is going to be a separate, lower volume, more premium product line.",hardware,2025-09-19 14:03:16,8
Intel,nf750dr,"Apparently they have been working on this for a year at this point, so a product with an Nvidia tile is likely going to launch by 2027.",hardware,2025-09-20 05:08:07,3
Intel,nfkfsir,"Nvidia makes over 10 billion on gaming GPUs in a quarter. But yes, not hundreds of billions.",hardware,2025-09-22 08:47:04,1
Intel,nf23n8h,Arc covers also non gaming segments like B50 product,hardware,2025-09-19 12:15:21,1
Intel,nf240dn,"Gaming laptops?  Ive watched the webcast with Jensen and as far as I understand this is about datacenter because customers dont want to switch to arm, so partnership with Intel will allow them to get x86 cpus with features Nvidia needs for their datacenter and hpc customers",hardware,2025-09-19 12:17:41,2
Intel,nf757kz,"Also, the partnership potentially could get Nvidia using their fabs. Nvidia will also now hold a 4% stake in Intel.",hardware,2025-09-20 05:09:49,1
Intel,nf3ij0s,"LNL(Xe2) solos every AMD offering in igpu out there and matches the AI 395 with lower power, even ARL(Xe+ Alchemist) isnt that behind, igpu perf is the last thing Intel is concerned about, these high perf RTX laptop apus will only damage the lower end offering from nvidia  and here look at the laptop market share, Intel's share has been pretty constant between 75-80% for the past 3-4 years, idk where the notion of Intel struggling in laptop space is coming from, they even clawed some back after the launch of Lunar Lake: [https://www.tomshardware.com/pc-components/cpus/amds-desktop-pc-market-share-hits-a-new-high-as-server-gains-slow-down-intel-now-only-outsells-amd-2-1-down-from-9-1-a-few-years-ago](https://www.tomshardware.com/pc-components/cpus/amds-desktop-pc-market-share-hits-a-new-high-as-server-gains-slow-down-intel-now-only-outsells-amd-2-1-down-from-9-1-a-few-years-ago)",hardware,2025-09-19 16:35:54,-1
Intel,nf7h2yf,It’s only been five years since people said it when Renoir APU launched. Maybe in another 5 AMD can cross 30% market share on laptop (it’s been stuck at 25% for about 3 years)?,hardware,2025-09-20 06:54:56,1
Intel,nf7hfqt,That’s really nothing. In Q2 2025 AMD made 1.1B from gaming (which includes semi custom and Radeon) out of 7.7B.,hardware,2025-09-20 06:58:19,0
Intel,nfkeauy,Asuming AMD tech work properly is not a bet you want to take.,hardware,2025-09-22 08:31:33,2
Intel,nf3310d,"I know, it's just that I think we'll keep seeing Arc iGPUs for the business and lower end consumer market primarily. Cheaper or more power efficiency focused laptops mainly.",hardware,2025-09-19 15:21:38,6
Intel,nf7fhq5,They don’t need that to compete with AMD’s regular iGP. 140v/140T do well enough.   Strix Halo is irrelevant as it was near zero mainstream OEM presence and,hardware,2025-09-20 06:40:44,4
Intel,nf33ehv,"It's possible.  Arc might be carving a workstation niche though? their B50 certainly seems like that might be the focus, offers a lot of pro features for a low price.  As far as enthusiast goes I wouldn't be surprised, it's a money sink Intel probably doesn't need even if the payoff might be worth it long term. They say it will make no difference though so who knows.",hardware,2025-09-19 15:23:26,2
Intel,nf7fovq,Demonstrably false in any currently existent device. Any $1500 laptop with a 5070mobile  blow the pants off Strix halo.   Continue your APU delusion in r/amd,hardware,2025-09-20 06:42:30,3
Intel,nf23m60,"Will they really profit more by adding RTX GPUs where there were none previously? Especially considering that NVIDIA will likely not make it any cheaper.  Like ultrabooks aren’t going to magically sell more and more, or more expensive in all categories just because they have NVIDIA GPUs.  Only place I can see it really make a lot of sense and add more profit is in place that already had NVIDIA GPUs, like with gaming laptops or enterprise, or as an option to complement Arc.",hardware,2025-09-19 12:15:10,12
Intel,nf28jld,Are you aware that most enterprise computing is cloud based?,hardware,2025-09-19 12:45:18,18
Intel,nf3gcu1,"I think client inference is kind of stupid. It's limited in capability by your local memory amount, and it's economically inefficient because you can't batch requests.  No AI system that exists is ""there"" yet, all of them can be improved enough that people would switch over to a better one. The current top of the line commercial models are hundreds of GB for the weights. To use client inference today you need to use a severely castrated model. Alternatively, if you ship all those hundreds of GB of DRAM on every device, they will be very inefficiently utilized because a single user rarely has the token flow to keep them working, and when they do it's all serial so you can't even batch, you just have to do a linear read over the whole model to get a single token out. And when there's an improved model next year that takes twice the ram you have to roll out whole new machines to the whole fleet.  In contrast, centralized inference can fit as much memory as you put on it, there are no power constraints, you can batch hundreds of requests in one go, and you can update the whole system much more easily. Client inference won't even win in latency because even though you have to pay for network latency, the centralized solution is probably much faster.  The only real advantage client inference has is privacy, and that's not a problem in business, they just get their own inference server. For office work, that even makes latency very fast.",hardware,2025-09-19 16:25:23,9
Intel,nf5bfo0,"I truly think you'll start to see lower tier offerings, like 50 - 60 class because large iGPUs. Maybe even 70, and a separate, discrete graphics chip become increasingly rare of the next decade.   Nvidia has a similar deal going on with Mediatek to bring their graphics there too.   This is Nvidia shoring up their lower end market in client laptops as the APU wars begin",hardware,2025-09-19 21:59:38,4
Intel,nf7g24e,"You don’t have to wonder, laptops with 7600m are already far better deal for gaming than any equivalent system with Strix halo for gaming.",hardware,2025-09-20 06:45:46,3
Intel,nf28all,"That kind of proves my point. Intel iGPUs exist, and therefore are better.  Trying to scale-down discrete graphics into integrated graphics will take quite a bit of development effort. AMD has done pretty well with it, but it took them years.  PC gaming / PC building hobbyists seem to drastically undervalue how good Intel graphics really are. They're low power, stable, and have sufficient performance to make the entire category of discrete graphics a niche market in PCs. For most applications that currently use discrete graphics, there would be no reason to even consider Nvidia if it were an option.",hardware,2025-09-19 12:43:50,3
Intel,nf27h9g,Quicksync and power management if I were to guess. Spinning up a dGPU uses a lot of power.,hardware,2025-09-19 12:38:59,4
Intel,nf2dk2f,Quick sync on a $150 Intel CPU is better at transcoding then anything Nvidia offers under $600.   A SIGNIFICANT portion of intel sales are because of features like this. Otherwise the entire industry would just switch to AMD. Which is superior in every single way EXCEPT it's IGPU support.,hardware,2025-09-19 13:13:33,0
Intel,nf3c5nn,"Meanwhile back in the real world the switch 2 exists with its nvidia SOC and all those SOC's inside cars exist, all those SOC's in Jetson's etc exist.   Lol literally knows nothing out side of PC gaming hardware.",hardware,2025-09-19 16:05:02,1
Intel,nf7p0vl,Nvidia actually does have iGPUs. Cars that use AGX Drive will have one.   The Volvo EX90 is using [AGX Orin](https://www.techpowerup.com/gpu-specs/jetson-agx-orin-64-gb.c4085) which has an Ampere-based IGPU with 2048 CUDA cores and [owners are being given free a upgrade to a Dual Orin setup](https://arstechnica.com/cars/2025/09/forget-ota-volvo-is-upgrading-the-core-computer-in-the-ex90-for-free/).  The latest AGX Thor has a Blackwell IGPU with 2560 CUDA cores.  Either way it seems easy enough for Nvidia to package Blackwell into a tile that Intel can replace their Arc graphics tile with.,hardware,2025-09-20 08:12:00,1
Intel,nf9dtsx,"Again, you're confusing yourself with current-generation dGPU comparisons. Yes, discrete Arc is behind Nvidia, because it's new.  An iGPU isn't just a dGPU slapped next to a CPU, nor are different process nodes really comparable.",hardware,2025-09-20 15:23:55,0
Intel,nf2tyya,"Pay for a sub, still have to pay again to ""buy"" games that you don't own. What could possibly be unappealing about that?",hardware,2025-09-19 14:38:28,8
Intel,nfkflp0,Latency on stadia could not be good. It would defy laws of physics (namely - the speed of light) for latency on Stadia to be good. This is why streaming gaming services never work and cannot work. We need FTL communication to reduce latency enough and FTL remains strictly fictional.,hardware,2025-09-22 08:45:06,1
Intel,nf2pi2o,You are kidding yourself if you think Nvidia will let their iGPUs be a low volume products.,hardware,2025-09-19 14:16:32,-1
Intel,nf24jgl,I am pretty sure intel made the arc pro at a low price to sell of most remaining stock and capacity bookings.  This Nvidia deal was in talks from a year ago. And isn't it a funny coincidence they announce the deal after the Arc Pro has sold out in a lot places?,hardware,2025-09-19 12:21:02,2
Intel,nf33yoj,"There were two things announced. A client partnership for gaming laptops (primarily), and a datacenter partnership for custom Xeons with NVLink.",hardware,2025-09-19 15:26:08,7
Intel,nf4oai8,"> LNL(Xe2) solos every AMD offering in igpu out there   Except in, you know, actual workloads.",hardware,2025-09-19 19:59:30,3
Intel,nf3ksre,Well intel is and will still be high in volume since they have very low end on lockdown. They maybe high volume but they are the lower margin chips.  Here is whole range of intel core (not ultra) that use older architectures  https://www.intel.com/content/www/us/en/products/details/processors/core.html  They even revived intel 14nm AGAIN  https://www.intel.com/content/www/us/en/products/sku/244818/intel-core-i5110-processor-12m-cache-up-to-4-30-ghz/specifications.html  Amd cant quite match this yet  However the higher margin products is where they getting hurt by AMD. So that's not good for them,hardware,2025-09-19 16:46:56,0
Intel,nf75fzx,Luner Lake is for handhelds are very lower powered laptops. It's not for work.,hardware,2025-09-20 05:11:49,-1
Intel,nfkv6gb,LMAO but tbf this time it’s also nvidia,hardware,2025-09-22 11:09:42,2
Intel,nf5ejqo,"That's likely the case for now, but Intel probably can't fight against the market.   Now that Nvidia iGPU is an option, OEMs are going to express interest, enterprises are going to express interest. It's the AI era, nobody is going to get fired for buying Nvidia.  Intel's CEO is a customer-pleaser, so if there's demand for mainstream SKUs w/ RTX, I can't imagine he'll say no. And Nvidia winning more of Intel's business? Maybe they'll have to port Nvidia IP to Intel foundry to service the higher volume.  I think Intel Xe will stick around for a while as a legacy platform, Intel does have a bit of a software moat, but I think this goes much further than Kaby Lake-G.",hardware,2025-09-19 22:17:19,1
Intel,nf34dp2,> Arc might be carving a workstation niche though  Not enough to keep it alive. Even smaller market than gaming.   > offers a lot of pro features for a low price  Well that's kind of the problem. They're forced to compete on price.,hardware,2025-09-19 15:28:07,2
Intel,nfh9av9,"Thats just more of AMD failing to work with OEMs. Placing a large iGPU tile on a CPU is lower BOM costs than two separate chips with two separate memory pools. And it's easier for OEMs to source a singular chip and cool that single chip.   Strix Halo's failure at using these lower BOM costs to aggressively gain market share, and instead positioning itself as a premium ""high-VRAM"" option is besides the point.   I agree with the poster above the APUs are going to begin cannibalizing the low end dGPU market over the next 10 years.",hardware,2025-09-21 20:02:01,2
Intel,nfkekvs,">Like ultrabooks aren’t going to magically sell more and more, or more expensive in all categories just because they have NVIDIA GPUs.  actually, they will. The mindshare aside, the ability to support things like CUDA or ray tracing will make it a much more desirable product.",hardware,2025-09-22 08:34:32,1
Intel,nf25uxh,"Let me tell you why intel rtx laptops wont be significantly more expensive.  A $400 Intel+ARC and $500 Intel+RTX. Same performance, but Nvidia has their features.  Now the Intel+RTX looks more expensive right by costing 25%? WRONG   Cause if the two laptops are built with exact same components and Intel+ARC laptop costs $1000, the Intel+RTX laptop would cost $1100.  Suddenly the Intel+RTX becomes the obvious choice at only just 10% more money.  It gets worse the higher you. $1500 intel+arc v $1600 Inte+rtx. Or $2000 v $2100  This is also why AMD sucks at OEM and Prebuilts  If a 9060xt built desktop costs $900 the equivalent RTX 5060 Ti desktop would just cost $970.  So suddenly instead of the RTX 5060 TI costing 20% more, it becomes only 8% more expensive.",hardware,2025-09-19 12:29:09,-12
Intel,nfkeodr,Its debatable. There was a big push for putting everything in the cloud. now there is a big push for getting everything back to local because cloud sucks.,hardware,2025-09-22 08:35:32,1
Intel,nf29pfq,Which runs on Nvidia hardware.  Nvidia laptop GPUs dont have enough memory to run ai locally.  However an Nvidia iGPU with access high capacity DDR5 and you see where we are going?,hardware,2025-09-19 12:52:03,-3
Intel,nf5dgpr,"I'm really looking forward to it. I guess ODMs are probably pretty pleased as well, this should simplify laptop motherboard layout, feasibly improve reliability and given it's already so rare for a gaming laptop to have anything other than an Intel CPU + Nvidia GPU they're probably going to find all of this very easy to design around.",hardware,2025-09-19 22:11:06,2
Intel,nf35spi,"Nvidia's been doing iGPUs for *years* in Tegra.  > For most applications that currently use discrete graphics, there would be no reason to even consider Nvidia if it were an option.  Nvidia's IP and drivers are simply better.",hardware,2025-09-19 15:34:52,7
Intel,nf2900x,"You know, i think an iGPU does exist. The switch 2.  And I don't need to tell how efficient that is despite using the awful Samsung node",hardware,2025-09-19 12:47:58,4
Intel,nf27yd0,Intel Xe Media Engine is the SOC tile. Not the GPU tile  So Arc GPU tile etting replaced will not effect that at all.  Edit - here is how it works  https://www.intel.com/content/www/us/en/support/articles/000097683/graphics.html  The display is also there too. So the GPU tile cam be completely idle. Arc or RTX,hardware,2025-09-19 12:41:49,9
Intel,nf34rvq,> Quick sync on a $150 Intel CPU is better at transcoding then anything Nvidia offers under $600.  That's just how they spec the encoders across the lineup.,hardware,2025-09-19 15:30:00,2
Intel,nf2eqdd,Good thing the Arc tile getting replaced wont effect it.  If anything now you get both Qsycn and Nvenc,hardware,2025-09-19 13:19:58,5
Intel,nf3bs97,Intel sales are mostly laptops and most businesses do not give a single shit about iGPU or quicksync.   Hardly anyone actually buys intel for quicksync lol.,hardware,2025-09-19 16:03:14,1
Intel,nf3ckbi,I actually did reply the switch 2 just below because i got tunnelvisioned on windows  He never replied after that because the switch is far more efficient than anything intel has and that too using a worse mode,hardware,2025-09-19 16:07:00,-1
Intel,nf2y54i,Here is the kicker. You didn't need to sub and then buy games.  If you bought a game you can play it without a sub.  But google failed spectacularly to market it that as you didn't even know that.,hardware,2025-09-19 14:58:20,4
Intel,nfl66ce,The sub was optional. You could buy a game and play it indefinitely without any extra costs.,hardware,2025-09-22 12:26:55,2
Intel,nfkg54y,"It was good. Not as good GFN cloud gaming but surprisingly good and very playable.  GFN has lower latency than consoles btw on MnK. That's because consoles rely on Bluetooth controllers which adds more latency.  So if anyone is fine is consoles, they will be more than fine on GFN.",hardware,2025-09-22 08:50:39,2
Intel,nf2qn6t,"well, their dGPUs are a niche product in laptops already. Why would an agreement to co-develop an APU with Intel change that?",hardware,2025-09-19 14:22:13,8
Intel,nf26ud6,"The Arc Pro B stuff hasn't even released to consumers yet.  B50 is shipping the 25th, B60 hopefully not to long after.  If Intel can really ship B60's at MSRP this year they will sell every single one they can make. There's nothing out there that comes even close to being competition for it.",hardware,2025-09-19 12:35:11,8
Intel,nf2m488,"The deal was negotiated for months and finalized / signed on September 13th (per Greg Ernst on LinkedIn). That means they announced the deal just 5 days after making the deal.  And even then, these parts arent coming out for years.",hardware,2025-09-19 13:59:18,4
Intel,nf7a9tl,"what actual workloads? In gaming they are basically tied, and that's what most consumers care about. The workstation laptop market is very small and niche and most people use a discrete gpu there anyway",hardware,2025-09-20 05:53:10,1
Intel,nf7grbi,What “actual workloads” on an integrated graphics would you be referring to?,hardware,2025-09-20 06:51:56,1
Intel,nf7gyiw,"What kind of work are we discussing here you think the bulk of office laptop from dell, hp, and Lenovo (which is the majority of worldwide pc shipment) have to do?",hardware,2025-09-20 06:53:46,3
Intel,nfaoxn2,Whether MT matters or not seems to depend on if Intel is food or not.  Don't people always talk about how gaming is king?,hardware,2025-09-20 19:21:27,0
Intel,nf34sw5,We will see. For the moment all we can go on is what Intel is saying and they're saying it's not going anywhere.,hardware,2025-09-19 15:30:08,1
Intel,nfkegwu,"and yet Strix is the more expensive option out there. Large, performan APUS are not easy or cheap.",hardware,2025-09-22 08:33:22,1
Intel,nfl500h,"Again, no.      First off, Arc already support RT on iGPU. And unless they start shipping more power hungry, larger GPUs in the place of smaller ones for ultrabooks, the performance isn’t going to magically get that much better for RT to be useable.     Second, if CUDA made ultrabooks that much more desirable, you would think we would see a 2050 or even a 3050 be included in a every ultrabook. That’s not exactly the case",hardware,2025-09-22 12:19:25,1
Intel,nf2854w,It's really funny how you assume that OEMs and ODMs won't also charge more on top of the component cost.,hardware,2025-09-19 12:42:57,9
Intel,nf2auox,"No I don't, actually. Why would enterprise customers bother running anything on laptops? There are no reasonable use cases for this. The only ""AI"" thing that needs to exist on a laptop is advanced search and windows recall, which the existing Intel hardware is perfectly capable of handling. Plus, you are aware Microsoft copilot+ requires a separate NPU right? Which makes the whole idea of running ""AI"" on said Nvidia IGP quite redundant.",hardware,2025-09-19 12:58:32,16
Intel,nf3gbr0,"Yea, gamers definitely live in some sort of mirror universe.  From my perspective, Intel drivers are significantly better than AMD drivers, which are worlds ahead of Nvidia drivers.",hardware,2025-09-19 16:25:14,-6
Intel,nf2cmxp,"Samsung's 8LPP was one of their best nodes lmao, what is with this revisionism",hardware,2025-09-19 13:08:31,-4
Intel,nf2e9yj,"90 percent of servers using Intel for Quicksync are not going to waste their time buying an Intel ARC GPU for $300 that they can't even source.   If Intel makes this mistake, literally every single person in the industry would just switch to AMD CPU's. As they are superior in virtually every single aspect of computing. If you are forcing people to buy an Intel ARC GPU, then the CPU they own will not matter.   Contributors to things like Jellyfin already stated they will just spend all their development time optimizing for AMD APU's.   I have worked in Comp Eng for 20 years. I can assure you Intel will not survive if they do this. They will likely be bought like the first company I worked for, ATI.",hardware,2025-09-19 13:17:29,2
Intel,nf2kjcr,"AFAIK, the 265KF for example, does not support quick sync.",hardware,2025-09-19 13:51:05,6
Intel,nfkpbpv,"Well, console controllers add about 50ms latency. However if your stream latency is less than that then you must be lucky and live very close to the server (physically).",hardware,2025-09-22 10:20:57,1
Intel,nf2r2dl,Nvidia laptops are niche products....HUH????  Have you seen the steam charts? Their laptops are big Even rivaling desktop numbers.  https://store.steampowered.com/hwsurvey/videocard/,hardware,2025-09-19 14:24:17,0
Intel,nf2754k,Its bait to sell off remaining stock and bookings.,hardware,2025-09-19 12:36:59,3
Intel,nfabxge,> In gaming they are basically tied  LNL vs Strix Halo? Absolutely not.,hardware,2025-09-20 18:14:10,1
Intel,nf7x9r6,"Gaming, content creation. Your choice, really.",hardware,2025-09-20 09:35:16,0
Intel,nfcz1wz,CPU performance was never discussed in this thread only iGP.,hardware,2025-09-21 03:27:42,1
Intel,nf356mu,"> and they're saying it's not going anywhere  They've said nothing about dGPUs in particular. If anything, likely died before this deal.",hardware,2025-09-19 15:31:57,2
Intel,nflvz26,"Laptops \*With Strix Halo are more expensive.  But I'm failing to see how taking the 40CU's package, making it a separate dGPU chip, giving that chip its own VRAM is cheaper than placing those 40CUs on package.  Strix Halo is low volume with a bespoke 256b motherboard. It's cost structure is negatively impacted by its economies of scale - not because its design is inherently more expensive.",hardware,2025-09-22 14:48:01,1
Intel,nfldrtw,"it may not be desirable enough to include a 3050, but desirable enough to slightly increases sales if its part of the iGPU? There is also a thing that you want to avoid dGPUs in ultrabooks due to battery time.",hardware,2025-09-22 13:12:43,1
Intel,nf28h5z,"Of course they will, Because you and i both know which one will be in more demand and sell more.",hardware,2025-09-19 12:44:55,0
Intel,nf34jjk,"> Plus, you are aware Microsoft copilot+ requires a separate NPU right? Which makes the whole idea of running ""AI"" on said Nvidia IGP quite redundant.  That bit seems to be changing, fwiw.",hardware,2025-09-19 15:28:53,2
Intel,nf2g2rw,"Why are you of the belief that people care about CoPilot+ at scale?  There are plenty of reasons a company would want local compute options instead of compute in the datacenter. Data security, data ingress/egress costs, local AI compute for appliance-type deployments, edge inference, network constraints, etc.  There is not a one-size-fits-all approach to AI compute. NPUs are not sufficiently powerful enough for the workloads today, let alone where AI compute is heading.",hardware,2025-09-19 13:27:16,0
Intel,nf2c1q6,Wouldn't change anything as the NPU is im the SOC tiles. Not the GPU tile.,hardware,2025-09-19 13:05:14,-2
Intel,nf3ghh5,"> Intel drivers are significantly better than AMD drivers, which are worlds ahead of Nvidia drivers  What?",hardware,2025-09-19 16:26:00,6
Intel,nfh2asl,Bro what? In what context?,hardware,2025-09-21 19:30:29,2
Intel,nf360e8,It's not even a current *Samsung* node. It's like 3 full gens behind state of the art.,hardware,2025-09-19 15:35:53,5
Intel,nf3e4jr,It is awful compared to modern tsmc 5nm. Which all CPU and GPUs use,hardware,2025-09-19 16:14:36,3
Intel,nfkf6in,It wasnt good when it was new andt isnt good now.,hardware,2025-09-22 08:40:46,1
Intel,nf2fb5j,I think the smarter move would be to do what amd did. A VPU for servers  https://www.amd.com/en/products/accelerators/alveo/ma35d/a-ma35d-p16g-pq-g.html  Now that will sell. Nvidia isnt competing there either.,hardware,2025-09-19 13:23:06,2
Intel,nf2n0rd,"They aren't saying you'll need a gpu, they are saying the quicksync and other media engine features are on the cpu SOC tile which will still be present in hybrid nvidia igpu tile designs. They could still stuff it up by not supporting it but the hardware will be there.",hardware,2025-09-19 14:03:58,1
Intel,nfkfch9,people doing large scale transcode with Quicksync would never switch to AMD - the worst possible option for transcode.,hardware,2025-09-22 08:42:27,1
Intel,nf2yqpq,F = no IGPU,hardware,2025-09-19 15:01:11,0
Intel,nf2x7cy,Less than 25% of PCs sold have Nvidia graphics at all. Don't care about the Steam hardware survey lol. Intel iGPU outsells Nvidia 2.5 to 1 in client.,hardware,2025-09-19 14:53:55,8
Intel,nf291ep,"Nah.  The AI market is too hype for Intel to just drop it, and Nvidia is exploiting enough market power that there's certainly space to profitably undercut them in that segment.",hardware,2025-09-19 12:48:11,5
Intel,nf8vjwz,What content creation are people buying AMD iGPU laptops for specifically? This is the weirdest lie someone has ever told on this website lmfao,hardware,2025-09-20 13:48:58,1
Intel,nf8648z,"140v does not lose to 890m in either of these, what are you on about?",hardware,2025-09-20 10:58:02,0
Intel,nfqmegv,Ask AMD. They are the ones selling the SOC for 600-1000 dollars to the OEMs. Low volume high RnD ammortization may be the reason.,hardware,2025-09-23 09:40:33,1
Intel,nf28r20,"Yeah, the one that has an almost identical featureset for $500 less",hardware,2025-09-19 12:46:31,9
Intel,nf2k5eq,"Companies are not clamoring to get AI compute more powerful than what the NPU can provide, locally, and at scale across their whole fleet.   If they were, P series and Precisions would've supplanted E/T and Latitudes by now.   Like, what's the usecase of that much local AI on individual workstations? Presumably any data being inferenced will he in some shared location, and with it, so will the inferencing hardware.",hardware,2025-09-19 13:49:02,11
Intel,nf2ilko,"OK, what kind of workload do you think they will actually run on laptops that is too powerful for existing and future Intel hardware, not too powerful for whatever Nvidia IPG will be bundled, and they don't want to do with an on-prem solution?",hardware,2025-09-19 13:40:48,6
Intel,nf2cebu,You still haven't addressed the why,hardware,2025-09-19 13:07:12,7
Intel,nfhqens,"Being a long-time desktop Linux user with a decent memory.  Intel's the only graphics vendor that takes drivers seriously at all. They write and ship them months in advance of product releases, to the point that I could reasonably expect to buy a pre-release B60 today, put it in my desktop PC, and have it just work with already-installed drivers.  AMD and Nvidia treat drivers like video game releases. They ship at the last possible minute and then need patches a week later for bugs, then they kind of get abandoned. And Nvidia never does open source driver releases, which are the only way to have reliable long-term support or any support for configs that differs from the vendor test setup.",hardware,2025-09-21 21:21:59,-2
Intel,nf6fv6h,"No shit, but compared to its contemporaries it did pretty well.",hardware,2025-09-20 02:03:22,-1
Intel,nf8fe8c,"Huh, weird, last time I checked Blackwell, Navi 48 and Zen 5 were all on TSMC N4, Arrow Lake on N3, and none of them on N5. There seems to be an awful lot missing from ""all CPU and GPU""",hardware,2025-09-20 12:09:31,1
Intel,nfknx76,"That's just a lie lmao, the 8LPP was possibly the only good node SF has put out in years. 10LPX and 10LPP were both not great but 8LPP was good.",hardware,2025-09-22 10:08:18,1
Intel,nf3sfl9,"Of course they are. Even upstream FFmpeg couldn't keep up with the demand and they had to write additional code themselves to drive the hardware transcoding.  https://github.com/jellyfin/jellyfin-ffmpeg/wiki  It's not true that they spent all their time on APUs, they did spend some time, but it was much better than their competitors spending almost no time on AMD.",hardware,2025-09-19 17:22:55,2
Intel,nf30g07,Right. The person I'm responding to is saying that Intel CPUs would still support Quicksync if Intel removed the Arc iGPU tile.,hardware,2025-09-19 15:09:19,5
Intel,nf2yrg0,"Nvidia's gaming division makes over 4 billion a quarter. And its a safe bet more than a 1 billion comes from Laptops alone.  That not a niche market at all. Yes, its not market dominant if you include iGPUs which even exists in celerons, but if you call over 1 billion a quarter niche, we need to rewrite the meaning of ""niche""",hardware,2025-09-19 15:01:17,2
Intel,nf2ap8j,Intel is going to get Ai market by being the exclusive x86 supplier of Nvidia.  Both on server and client.,hardware,2025-09-19 12:57:41,3
Intel,nf2of16,"Intel's too late to the AI market, Arc flopped, Gaudi flopped harder, broadcom is doing custom solutions, AMD is number 2. China is all in with full state backing. What is Intel going to do? Nobody needs a third place.",hardware,2025-09-19 14:11:08,3
Intel,nfabrjg,> This is the weirdest lie someone has ever told on this website lmfao  Weirder than claiming LNL outperforms Strix Halo in GPU?,hardware,2025-09-20 18:13:21,0
Intel,nfadt03,They claimed it even beats Strix Halo.,hardware,2025-09-20 18:23:44,2
Intel,nf297x3,"""identical"" lol",hardware,2025-09-19 12:49:15,-2
Intel,nfkeum0,I think you dont realize how many companies both need the local performance and not get it because of budget constraints. Employees having hell of a time trying to deal with that mess? not shareholder headache.,hardware,2025-09-22 08:37:18,1
Intel,nf2nrvo,"I think you're missing the point because you seem to believe that all users or departments have unconstrained budgets and can just buy whatever the ""perfect"" solution is for their workload. If that were the case, all workloads would already be handled in the datacenter, and no one would ever need a local GPU for compute, graphics, or AI workloads, which is not reality.  It's a hell of a lot cheaper to go buy a dozen laptops than to go buy a single B200.  Look at the entire entry laptop workstation market. There's a reason why those products exist, or else OEMs wouldn't make them.",hardware,2025-09-19 14:07:51,-1
Intel,nf2d36f,Won't matter anyway because of how poor NPU support is.  ROCm doesn't support Ryzen NPUs. Don't think Intel OneAPI does either. And CUDA obviously doesn't.  There was good post about NPUs here  https://www.reddit.com/r/hardware/s/JGJ45bpbjN  There is very little reason to support NPUs.,hardware,2025-09-19 13:10:59,0
Intel,nfqvti3,"And yet anyone who does more than basic display and maybe some 3d gaming stuff on their GPUs, uses Nvidia on Linux. Be it GPGPU stuff, cryptographic work, ML training or rendering. Intel ARC had driver issues on both Windows and Linux for a long time. They have a good record on Linux before that yes, but only for their integrated stuff. And intel 7th gen igpus still do not have Vulkan support.   Also we are not talking about Linux users anyways. Like 5% of people use Linux.",hardware,2025-09-23 11:05:23,1
Intel,nfkf4pp,> Being a long-time desktop Linux user with a decent memory.  so completely irrelevant market niche.,hardware,2025-09-22 08:40:15,1
Intel,nf7gd2o,Intel 14nm did not only “pretty well” but amazingly compared to its 2014 contemporaries.,hardware,2025-09-20 06:48:27,1
Intel,nfkf8ni,N4 is a custom N5 version. They are a lot more similar than people think. They are both 5nm nodes.,hardware,2025-09-22 08:41:22,1
Intel,nf3drib,I think the F series has both Display engine and Media Engines disabled cause they assume you are going to be using a requirered dGPU for that.,hardware,2025-09-19 16:12:50,3
Intel,nf302o1,"Intel iGPU outsells Nvidia dGPU 2.5 to 1. There's no need to redefine niche, which means ""a specialized segment of the market for a particular kind of product or service."" I think being a high end upsell product that's found in less than 25% of computers qualifies.    >You are kidding yourself if you think Nvidia will let their iGPUs be a low volume products.  That's what you said. They already *are* a comparatively smaller market than Arc in client. The Nvidia x Intel collab product will be the same: A lower volume specialized part that costs more and performs better, that some people will pay extra to upgrade to. But it will not form the bulk of Intel's volume.",hardware,2025-09-19 15:07:34,6
Intel,nfadkg1,They are very clearly not including Strix Halo. You're somehow the only one who is thinking of Strix Halo.,hardware,2025-09-20 18:22:30,-1
Intel,nfbo7bs,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,hardware,2025-09-20 22:33:22,137
Intel,nfb2xr8,Far more than I expected them to come out at. Damn.,hardware,2025-09-20 20:35:14,16
Intel,nfarr9a,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",hardware,2025-09-20 19:36:31,56
Intel,nfbz1kb,I’m getting one when it releases in Australia,hardware,2025-09-20 23:37:15,5
Intel,nfj2y23,Thats great and all but when will there be stock? (Canada),hardware,2025-09-22 02:00:50,2
Intel,ng8k9a2,Pytorch libs are getting better for intel and amd support but there are still a lot of 3090 cards on the used market and those don’t need hoops to jump through to get compute working.  If I was handed a budget this low by a boss I’d just ask them to buy a lot of 3090 cards.,hardware,2025-09-26 02:15:59,1
Intel,ngid0f0,I'm disappointed.  My order was canceled,hardware,2025-09-27 17:12:34,1
Intel,nfaolmx,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-20 19:19:41,1
Intel,nfbxe5d,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",hardware,2025-09-20 23:27:31,131
Intel,nfc739o,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,hardware,2025-09-21 00:26:10,18
Intel,nfateyp,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",hardware,2025-09-20 19:45:24,200
Intel,nfb479l,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",hardware,2025-09-20 20:41:48,80
Intel,nfb166b,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",hardware,2025-09-20 20:26:03,37
Intel,nfb3pdb,The 3090 does not have ECC on it's VRAM nor certified drivers,hardware,2025-09-20 20:39:13,18
Intel,nfc4ak9,Totally not worth it.,hardware,2025-09-21 00:08:54,-3
Intel,nfb6hdv,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,hardware,2025-09-20 20:53:44,-8
Intel,nfd2k0s,"I'm more looking forward to the B50, but obviously local pricing is everything.",hardware,2025-09-21 03:53:22,6
Intel,nfbgzuw,"It has SR-IOV, certified drivers and other professional features...",hardware,2025-09-20 21:51:30,22
Intel,nfc15d7,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",hardware,2025-09-20 23:49:42,8
Intel,nfdv26x,">The gaming cards require significantly less robust driver and application support  Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  It was like that since A770 release. When some games straight up don't work or have unplayable bugs, you are literally paying for own suffering.  That said, B50/B60 are good, though i wish B50 worked with ""Project Battlematrix"" that is Intels NVLink. B60 was also expected to be for 500$, not 600$, but it is what it is.",hardware,2025-09-21 08:04:03,13
Intel,nfbzlhb,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 1 year!?,hardware,2025-09-20 23:40:29,-14
Intel,nfeqn5s,"...Do integrated graphics support SR-IOV? Do any of Intel's iGPUs support the ""pro"" version of their drivers,   Like, obviously iGPUs have less compute power and less bandwidth than anything dedicated, but. Part of me feels like iGPUs should actually have the features of the ""pro"" cards, given they aren't replaceable, with the actual pro cards providing performance.",hardware,2025-09-21 12:41:54,6
Intel,nfauwnn,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",hardware,2025-09-20 19:53:22,23
Intel,nfau9ib,Do not underestimate the lack of CUDA.,hardware,2025-09-20 19:49:57,33
Intel,nfdmqe2,"Bought my 3090 for MSRP 4.5 years ago and honestly would never have imagined that it would hold 50% of its value.  I want to get another one when they drop to $200 or so, so that I can finally be disappointed by SLI and make 14 year old me happy by finally clicking the bridge onto two GPUs and firing up a benchmark, but who knows if/when that day will come.",hardware,2025-09-21 06:45:03,2
Intel,nfbbfys,A used 3090 is only $100 more.,hardware,2025-09-20 21:20:29,1
Intel,nfauhs7,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",hardware,2025-09-20 19:51:10,-17
Intel,nfc1012,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,hardware,2025-09-20 23:48:50,-4
Intel,nfb5jdp,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,hardware,2025-09-20 20:48:47,28
Intel,nfc0fwm,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",hardware,2025-09-20 23:45:29,18
Intel,nfckjwe,It is an competitive edge and hope Intel pushes on it hard. Same with AMD and its Strix iGPUs. I will buy unified memory on a desktop for a premium too - the M5 chip is going to make all 3 Intel/Nvidia/AMD wake up hard,hardware,2025-09-21 01:50:42,3
Intel,nfbovre,Atlas 300i duo,hardware,2025-09-20 22:37:31,-5
Intel,nfb510h,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",hardware,2025-09-20 20:46:06,28
Intel,nfb3ro9,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",hardware,2025-09-20 20:39:33,6
Intel,nfc234i,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,hardware,2025-09-20 23:55:21,5
Intel,nfcbz1s,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",hardware,2025-09-21 00:56:35,1
Intel,nfc72xn,It was meant to be a joke. Not so funny I guess.,hardware,2025-09-21 00:26:06,6
Intel,nfek88y,"B580 needed to get out very early to beat RTX 5060 and RX 9060 to the market. Nvidia's and AMD's cards outperform it in gaming raster performance thanks to having been in the market for much longer. They have the more efficient architecture and software for gaming. It wouldn't be great for marketing if Arc came out at the same time with similar pricing, yet got beaten in the FPS benchmarks.  But productivity cards could wait. Arc's always been very competitive in productivity tasks. Maybe because Intel has more experience in this department. So reliable driver is the focus here.",hardware,2025-09-21 11:57:29,8
Intel,nfsgdep,">Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  Good driver and application support is not possible to deliver without a significant install base of your product in the wild. PC configurations vary wildly and usually driver issues are about some bad luck combination of hardware, driver versions, and application versions.  The incumbency advantage in this market is immense. Devs will validate that their stuff works on all manner of configurations including an Nvidia GPU, so they get the perception of having bulletproof drivers.",hardware,2025-09-23 16:15:20,2
Intel,nfd2naj,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,hardware,2025-09-21 03:54:03,12
Intel,nfc0nds,Why does this matter?,hardware,2025-09-20 23:46:45,21
Intel,nfv9260,"Some iGPU do support SR-IOV, I think 12th generation and newer. Motherboard, driver, and OS dependent.",hardware,2025-09-24 00:45:10,2
Intel,nfb63fj,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,hardware,2025-09-20 20:51:42,43
Intel,nfbbr35,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",hardware,2025-09-20 21:22:13,8
Intel,nfbedl2,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",hardware,2025-09-20 21:36:51,8
Intel,nfbifxk,Used market is freaking insane. It is better to grab new or open box.,hardware,2025-09-20 21:59:41,5
Intel,nfcx3qi,Over here used 3090s are sold for 500-600€.,hardware,2025-09-21 03:13:43,1
Intel,nfbbgrb,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",hardware,2025-09-20 21:20:36,28
Intel,nfcjowb,I hope this advances well for the future hardware and software releases -[MLIR] (https://www.phoronix.com/news/Intel-XeVM-MLIR-In-LLVM),hardware,2025-09-21 01:45:17,2
Intel,nfhd7ci,Dude that feeling is so worth it though. I did that same with 2 Titan Xps a couple months ago and the build just looks awesome. Great performance on pre 2018 / SLI supported games as well.,hardware,2025-09-21 20:19:33,1
Intel,nfb5o6f,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,hardware,2025-09-20 20:49:29,21
Intel,nfayuhx,the super gpus are not expected to release soon?,hardware,2025-09-20 20:14:00,8
Intel,nfcl0od,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",hardware,2025-09-21 01:53:39,14
Intel,nfcm0px,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",hardware,2025-09-21 02:00:02,9
Intel,nfc9w3x,Used 4070 Supers dont sell for nearly $700+ on the low dude.,hardware,2025-09-21 00:43:40,5
Intel,nfco0nb,Yes running business of used cards is how its done.....,hardware,2025-09-21 02:12:43,3
Intel,nfdimrg,no fee SR-IOV makes this a substantially different offering then AMD or Nvidia.,hardware,2025-09-21 06:07:29,6
Intel,nfbr125,Typically run at 250W though to be fair.,hardware,2025-09-20 22:50:29,4
Intel,nfc1wom,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",hardware,2025-09-20 23:54:16,-7
Intel,nfbgadn,not on the Vram like professional cards,hardware,2025-09-20 21:47:32,5
Intel,nfdjdro,Q4.,hardware,2025-09-21 06:14:16,2
Intel,nfdm9xu,I thought it was funny ¯\_(ツ)_/¯,hardware,2025-09-21 06:40:53,1
Intel,nfjxn7s,The b580 is still good at $250 and it’s only recently in the past month the 5060 and 9060xt 8gb dropped near or at $250 in some instances.  It’s a good card at 1440p aswell and it has 12gb of the other 2 8gb cards. Only problem I have it is the windows 10 drivers being abysmal dog shit if you’d want to play any current game with stuttering and crashes.,hardware,2025-09-22 05:47:02,2
Intel,nfddcxx,"Yeah, apparently a year’s worth of it",hardware,2025-09-21 05:20:13,-5
Intel,nfcew77,Because they're super late to the party.,hardware,2025-09-21 01:14:55,-8
Intel,nfb6fud,"Wow, 800-900USD after 5 years is more than I would have expected.",hardware,2025-09-20 20:53:31,17
Intel,nfdc18j,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",hardware,2025-09-21 05:08:46,6
Intel,nfc1e0n,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,hardware,2025-09-20 23:51:10,-5
Intel,nfc1j2x,For what exactly do they need vram without cuda ?,hardware,2025-09-20 23:52:00,-6
Intel,nfbq925,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",hardware,2025-09-20 22:45:52,-11
Intel,nfksagq,Gamers are not niche. They are 20 billion a year business.,hardware,2025-09-22 10:46:43,-1
Intel,nfbohl3,"False, they’re releasing in december or jan. So about 3 months from now",hardware,2025-09-20 22:35:06,6
Intel,nfbxm75,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",hardware,2025-09-20 23:28:51,15
Intel,nfbksrd,"The 3090 Ti did, but the standard 3090 did not.",hardware,2025-09-20 22:13:21,9
Intel,nfjz0lo,"It's a really good general purpose card. Perhaps not the best card if you want to squeeze out the most fps out of games, but it performs really well in a wide range of tasks including gaming. And yes 12GB VRAM really comes in handy.  It can easily be found for under $250. B580 is still cheaper than 5060 and 9060 XT and even RTX 5050 and RX 7600, especially in the UK where you can find a B570 for only 20 pounds more than RTX 3050. That's way cheaper than RX 6600, it's a no brainer. But the prices could only go down thanks to economy of scale mostly. It's been produced for a while, that's why.  Also one of the big reasons it's still selling now is thanks to good publicity. Imagine if it came out as the same time as RX 9060 XT. HWU would tear a new hole regardless of how much of an improvement it is over the Alchemist cards and the impressive productivity performance. Those guys literally don't care about anything other than pushing a dozen more frames in a dozen personally chosen games. Gamer Nexus and Igor too to some extent. It would be bad publicity.",hardware,2025-09-22 05:59:38,3
Intel,nfdem0c,"If intel felt they could have released this safely earlier, they would have",hardware,2025-09-21 05:31:12,6
Intel,nfb6vit,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,hardware,2025-09-20 20:55:46,23
Intel,nfc975k,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",hardware,2025-09-21 00:39:26,1
Intel,nfg0b8g,3090s will be much less attractive when the 5080 super and 5070ti super launches.  If the 5070ti super is 750- 800 with 24gb for a new faster GPU with a warranty it hel used market will have to shift.  The 3090s have nvlink but I'm unsure how much that matters for most people.,hardware,2025-09-21 16:40:24,3
Intel,nfcmei6,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,hardware,2025-09-21 02:02:24,10
Intel,nfc3jog,"rendering, working on big BIM / CAD models, medical imaging,...",hardware,2025-09-21 00:04:18,17
Intel,nfc3y1b,CUDA isnt the only backend used by AI frameworks,hardware,2025-09-21 00:06:45,23
Intel,nfde0a9,"I use opencl for doing gpgpu simulations, this card would be great for it",hardware,2025-09-21 05:25:54,2
Intel,nfbqi8f,You mean your 0 profile history because you have it private?,hardware,2025-09-20 22:47:22,8
Intel,nfd62l7,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",hardware,2025-09-21 04:20:02,0
Intel,nfbqms5,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,hardware,2025-09-20 22:48:07,-1
Intel,nfksqka,Spoken like a true gamer.,hardware,2025-09-22 10:50:25,1
Intel,nfbt6h6,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,hardware,2025-09-20 23:03:03,2
Intel,nfcz630,You still get about 85% performance compared to stock settings.,hardware,2025-09-21 03:28:33,2
Intel,nfdr09z,"Of course, but that’s the point. It took them an excessive amount of time to “feel” like they could have released this",hardware,2025-09-21 07:24:57,-2
Intel,nfbvai4,What are they doing that's making them money? Or are theu selling the compute somehow?,hardware,2025-09-20 23:15:17,17
Intel,nfc1jx5,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",hardware,2025-09-20 23:52:08,-8
Intel,nfc9f6k,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",hardware,2025-09-21 00:40:47,1
Intel,nfkntvr,">rendering  Rendering what? Because if you're talking from a 3D art perspective, all our software is leaps and bounds better with CUDA or outright requires it. You can't even use Intel GPUs with Arnold or Redshift, so good luck selling your cards to a medium scale VFX house that heavily use those render engines",hardware,2025-09-22 10:07:27,4
Intel,nfcsv0e,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",hardware,2025-09-21 02:44:26,4
Intel,nfdll3m,>CUDA isnt the only backend used by AI frameworks    It is not the only backend for all AI frameworks; it is the only backend for the majority of AI frameworks.,hardware,2025-09-21 06:34:32,1
Intel,nfks66x,It is the only functional one.,hardware,2025-09-22 10:45:44,1
Intel,nfbqlfx,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",hardware,2025-09-20 22:47:54,-7
Intel,nfd6ez2,0 people are using autodesk with an intel arc gpu lmao,hardware,2025-09-21 04:22:47,-1
Intel,nfbriv2,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",hardware,2025-09-20 22:53:25,0
Intel,nfkvz03,"Oh no, im part of 60% of world populatioin. how terrible.",hardware,2025-09-22 11:15:48,0
Intel,nfd2pwi,Which is 36% higher performance per watt.       ... to be fair.,hardware,2025-09-21 03:54:35,3
Intel,nfdsvbs,The reason they didn't is because it would have been worse to release it earlier,hardware,2025-09-21 07:43:03,0
Intel,nfbxbg7,"Gamers Nexus has a documentary on the GPU smuggling business, where even the 3060 12GBs are being used: https://www.youtube.com/watch?v=1H3xQaf7BFI  The 4090s, 3090s and other cards? There's such a demand for those in China that people will buy them up and smuggle them.",hardware,2025-09-20 23:27:04,14
Intel,nfbw0b9,Software Development,hardware,2025-09-20 23:19:26,-6
Intel,nfc44n9,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,hardware,2025-09-21 00:07:53,7
Intel,nfcbku1,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",hardware,2025-09-21 00:54:09,0
Intel,nfcbqrf,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",hardware,2025-09-21 00:55:09,-2
Intel,nfkwldk,"Depends on what software you use, but your right that NVIDIA with Cuda is safe bet when is comes to rendering. I use Enscape and that works great in my AMD gpu",hardware,2025-09-22 11:20:33,1
Intel,nfcv5fj,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",hardware,2025-09-21 03:00:04,7
Intel,nfeidh5,Which ones?,hardware,2025-09-21 11:43:23,1
Intel,nfcjfv3,Private profile = Complete troll.  You're not an exception to this rule.,hardware,2025-09-21 01:43:42,11
Intel,nfbs5c3,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,hardware,2025-09-20 22:57:03,-1
Intel,nfdu2bk,"Right man, my point is that it shouldn’t have been",hardware,2025-09-21 07:54:31,-1
Intel,nfbxo4k,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,hardware,2025-09-20 23:29:11,13
Intel,nfcdfer,"Hell, one of them was just the cooler without the actual GPU.",hardware,2025-09-21 01:05:34,12
Intel,nfcde1y,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",hardware,2025-09-21 01:05:20,0
Intel,nfgcxg6,for example every local image and video generation system I've seen.,hardware,2025-09-21 17:38:32,3
Intel,nfbsbxa,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",hardware,2025-09-20 22:58:06,2
Intel,nfc0g5t,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",hardware,2025-09-20 23:45:32,-1
Intel,nfducqd,According to whom? With what evidence?   Intel spends more on r&d per year than nvidia and amd combined   Do you think they are just being lazy?,hardware,2025-09-21 07:57:16,3
Intel,nfd4nt4,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",hardware,2025-09-21 04:09:11,4
Intel,nfh4t9j,Getting good LLMs crammed onto GPUs for coding is a popular topic on /r/locallama.,hardware,2025-09-21 19:41:40,1
Intel,nfd0war,And the one directly under that was the actual PCB...,hardware,2025-09-21 03:41:08,6
Intel,nfgecr2,Im running Comfy UI + Flux on my Strix Halo right now without issue 🤷‍♀️,hardware,2025-09-21 17:44:40,0
Intel,nfksgbg,Propaganda is 90% of chinas economic output though.,hardware,2025-09-22 10:48:04,0
Intel,ne6ahg8,"> Neweggâs  I know this is mojibake, but this kinda sounds like a Lithuanian versions of Newegg lol",hardware,2025-09-14 14:46:52,183
Intel,ne5xw93,The competing product is sometimes slower while also being twice the price.  If this wasn't a success then Nvidia would be  unbeatable,hardware,2025-09-14 13:39:18,223
Intel,ne6669f,"What's the word on the B60? Even more VRAM (24GB), and double the memory bandwidth. I see it listed as ""released"" in various places, but can't figure out where to actually buy one.",hardware,2025-09-14 14:24:15,62
Intel,ne680ly,"My RTX a4000 doesn't support SR-IOV. I don't know about newer series, but at the time you had to buy the A5000($2500) or A6000 and then there are some crazy licence fees to use it.  For 350 i will buy it when it gets available just for this.",hardware,2025-09-14 14:34:03,58
Intel,ne6hcg6,L1 techs had a great feature on these.,hardware,2025-09-14 15:21:17,21
Intel,ne62jka,"Profitable product for Intel, wouldn't suprise me if Xe3P and onwards for dGPUs happens because stuff like this can do easy returns.",hardware,2025-09-14 14:04:39,30
Intel,ne8mbnv,1:4 ratio of FP64 performance is a pleasant surprise,hardware,2025-09-14 21:19:44,9
Intel,ne6anee,"Honest question here: what makes it a ""workstation gpu"" that does it differently than say like a low end 5060/AMD equivalent?   Iis it just outputting 1080p ""faster""?",hardware,2025-09-14 14:47:44,12
Intel,ne65b1l,"Its also just a whole 95 cards sold. (Past month, I’m unsure if its been up longer)",hardware,2025-09-14 14:19:36,18
Intel,ne6rl68,"It will never be in stock again. It’s good for AI, hosting pass through srvio to VMs without licensing and a number of other things outside of gaming.",hardware,2025-09-14 16:11:13,5
Intel,ne5wmh6,"Hello 79215185-1feb-44c6! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-14 13:32:02,-2
Intel,ne6nm7f,Let me know when it shows up on the steam hardware survey. That's the only barometer for success that true hardware enthusiasts care about.,hardware,2025-09-14 15:52:11,-24
Intel,ne7zdif,what's up my Neweggâs,hardware,2025-09-14 19:32:23,89
Intel,ne95tlq,My favorite game  Fallout Neweggas,hardware,2025-09-14 23:05:48,22
Intel,ne9ba6q,It also sounds like an ikea lamp name or something,hardware,2025-09-14 23:35:36,7
Intel,ne7p3l1,Geriau nei Bilas Gatesas,hardware,2025-09-14 18:44:05,3
Intel,ne6o1us,I gotta spell Neweggâs?!?,hardware,2025-09-14 15:54:14,1
Intel,ne6355p,"The B50 appears to be an decent low-end workstation GPU, at least as long as the intended workloads don't effectively require CUDA in any way, shape, or fashion.    My one lingering question is what use cases actually *require* the certifications of a workstation-class GPU (which would rule out something like a relatively similar consumer-tier RTX 5060 Ti / 16GB) but wouldn't benefit from CUDA?  Then again I'm not exactly an expert in the field, so I could be completely off-base here.",hardware,2025-09-14 14:07:57,70
Intel,ne7314f,"I'm disappointed in Nvidia's inability to put out a stable driver since last December, I'm waiting to see if a competitor card will come out that meets my wants for an upgrade.",hardware,2025-09-14 17:04:11,7
Intel,ne8v3zo,Intel might be using the B50 as a pipe cleaner for the B60's drivers to prepare it for a retail launch in Q1 2026    IF they're doing this then it's a sound strategy,hardware,2025-09-14 22:05:58,15
Intel,ne6kilx,"Double the memory bandwidth of trash is still trash.  Edit: Y'all can downvote me all you want, but 250GB/s is just slightly more than the 200GB/s of my low-profile 70W GTX1650 GDDR6 that I bought for €140 in 2019. Its absolutely pathetic and should be unacceptable for a new product in 2025, let alone a product of $350 !!!. Even double of this (\~500GB/s) of the B60 is less than a RTX3060. Pathetic products.",hardware,2025-09-14 15:37:09,-74
Intel,ne7d8mq,SR-IOV is the selling feature for me and why I have one ordered. Getting a Tesla P4 with nvidias vgpu licensing working is a pain in the ass and expensive.  I'll get it and sit on it until SR-IOV is released in case of scalpers/stock issues. If it doesn't pan out I'll either just sell it on or drop it into my home media server for the AV1 encoding/basic AI stuff.,hardware,2025-09-14 17:49:31,20
Intel,ne6eqfv,"Last time I checked GRID licensing can be faked out, but yes, only Quadro/Tesla and Turing/Pascal(IIRC) through driver mods can use Nvidia's vGPU.",hardware,2025-09-14 15:08:14,-8
Intel,ne70sx3,"The professional market is smaller than gaming and even more slanted towards Nvidia. This might be a nice side business, but can't remotely justify developing these cards.   Not even clear it's profitable either. The numbers here are negligible so far.",hardware,2025-09-14 16:54:03,3
Intel,nealgik,"Do people actually need and use FP64 at all anymore? I've got one or two original Titan cards that I haven't thrown out although I've never used them for this purpose either, because they apparently have very high FP64 numbers and if I recall correctly can operate in ECC mode as well.",hardware,2025-09-15 04:31:20,8
Intel,ne6c0of,"iirc, SR-IOV and VDI support in the coming months, toggleable ECC support, and it is ISV certified",hardware,2025-09-14 14:54:44,45
Intel,ne6dpqm,"So I spec our PCs at work. We do anything from traditional office work, to intense engineering tasks. On our engineering computers we run MatLAB, Ansys, Solidworks, MathCAD, LTSpice, Xilinx, Altium and other such apps. Lots of programming, VMs, design work, simulation testing, number crunching, and on occasion AI work.   This means we spec systems like with RTX Pro 500, RTX Pro 2000, RTX A4000, A4500, A6000s. The reason we have these rather than cheaper GeForce cards is mostly 3 things. Power/form factor, Driver certification, pro GPU features.   So typically Nvidia keeps the top binned chips for their professional cards meaning the power efficiency to performance is top tier. So we can get high performance single slot or low profile cards, or get some serious GPU performance in relatively small laptops. Drivers usually are validated better than the GeForce drivers, so they include better bug testing, and the apps we use validate performance with the cards which helps us evaluate performance. They also have way more vram like the RTX A4000 has 20GB of vram while being just a supped up 4070. Then from a feature perspective they have better VM passthrough support, or you can enable the vram to run in ECC mode for error correction. Very important when running 24-48 hour simulations.",hardware,2025-09-14 15:03:08,23
Intel,ne6yi39,Software support is a thing. CAD applications like solidworks and inventor don’t officially support the GeForce rtx or radeon rx line of gpus and they’re considered untested unsupported options. You can’t get any tech support if you’re using them. For a business that needs those apps you need a workstation gpu. They also come with ECC vram,hardware,2025-09-14 16:43:33,11
Intel,nebc25u,ECC memory.,hardware,2025-09-15 08:46:16,1
Intel,neb16s5,"That kind of puts it into perspective.  Also, let my take a guess:  Newegg sells them well because of how dirt cheap they are, people buying actually expensive Pro cards will more likely do it directly via their system integrator.",hardware,2025-09-15 06:53:42,3
Intel,neaqhtz,Oh when they get enough enterprise customers they will definitely charge licensing fees,hardware,2025-09-15 05:14:05,4
Intel,ne70g18,How many A2000s show on the hw survey? Because that's the Nvidia variant and it has been around for a long time..,hardware,2025-09-14 16:52:26,18
Intel,ne878lu,Lmao,hardware,2025-09-14 20:08:06,12
Intel,nebbyvw,Komentarą skaitai per tapšnoklį ar vaizduoklį?,hardware,2025-09-15 08:45:18,1
Intel,ne66mwj,I am no expert but don't these gpus have ECC vram. That's a enough to get labs/professionals to buy them.   You don't want the headache of lacking error correctiin in a professional environment.,hardware,2025-09-14 14:26:44,76
Intel,ne6xv28,"I seriously considered getting one for my homelab. I would really like some SR-IOV, and giving multiple VMs access to transcoding would be very useful. Ultimately decided against it because at the moment my CPU alone is powerful enough, I have other uses for the PCIe slot, and I would have to import one. But it's something I'm going to check in on whenever I'm browsing for new hardware from now on.",hardware,2025-09-14 16:40:31,5
Intel,ne8a75b,"I know in my field of work, solidworks certified hardware is one such application where certain features are gated behind workstation class cards.",hardware,2025-09-14 20:21:25,3
Intel,ne95ql6,Vulcan does fine with inferencing.,hardware,2025-09-14 23:05:20,3
Intel,nfekr88,"all the professional graphics stuff is where this can matter, i.e. CAD, large display walls, traffic control, video studios etc",hardware,2025-09-21 12:01:22,1
Intel,neavc5w,people are buying B60's  https://www.reddit.com/r/LocalLLaMA/comments/1nesqlt/maxsun_intel_b60s/,hardware,2025-09-15 05:57:49,5
Intel,ne8ur5d,"Most Zen-1 parts had much worse single core performance than Kaby Lake,    People still cheered on the competition anyway despite it's shortcomongs",hardware,2025-09-14 22:03:59,19
Intel,neprgo9,"GTX 1650 has only 4GB of RAM at 128 GB/s; RTX 3060 is only 360 GB/s, and only 12 GB--or maybe just 8 GB for some cards--of RAM. But thanks for playing.   Edit: relevant username. Up voting you for jebaiting the crap out of all of us.",hardware,2025-09-17 14:56:51,0
Intel,ne6jp74,you really dont want to fuck around with software licensing as a business. vendors do inventory audits to ensure nobody's exceeding their license allocations. piracy would automatically invite a lawsuit.,hardware,2025-09-14 15:33:02,37
Intel,ne8fips,Grid licensing can be faked if you depend on a sketchy github driver that only works on Turing GPUs. You certainly don't want to be doing that in a professional setting where licensing costs are not a massive expense anyways.,hardware,2025-09-14 20:46:12,13
Intel,ne8eldw,"I believe mobile is the main reason they continue developing ARC IP, highly integrated SoC are crucial for lower power consumption and performance per watt, as more and more mobile designs are becoming more integrated (see strix halo for example) Intel knows it has to continue developing graphics IP that is competitive with competition. As for discrete cards, this is a battle in the long run to win, but it will take serious investment, we can hope that they won't axe as part of cost cutting measure.",hardware,2025-09-14 20:41:52,10
Intel,ne93icy,"The B50 (16Xe cores) is pretty cut down compared to the full G21 (20Xe)die, it has 2600mhz boost clocks instead of the 2850mhz on the gaming cards, it uses 14GB/s memory (19Gbps on gaming cards) and it has a 128bit bus with 8 memory chips (B580 has 192bit bus with 6 memory chips)  The only costly thing about is the 2 additional memory chips.   I'm not saying it's extremely profitable but it can't be too expensive to make since a portion of the volume is likely faulty G21 dies that can't make a B580 or B580.   If Intel can sell the B580 for $250 without too much pain, then the B50 is probably making a profit",hardware,2025-09-14 22:52:49,3
Intel,nealnf3,"Yes, to the point where I’m considering picking up a Titan V on eBay. It’s a must for scientific computing, single precision floats accumulate errors fast in iterative processes.",hardware,2025-09-15 04:32:57,12
Intel,ne6dmlw,I recognize those as words...,hardware,2025-09-14 15:02:42,6
Intel,ne7cbc2,I think it was obvious I was being facetious.,hardware,2025-09-14 17:45:29,-4
Intel,ne8jx5n,> You don't want the headache of lacking error correctiin in a professional environment.  I think Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU. That was the explanation that I got back in the university years when the IT department would use the cheapest possible professional GPUs instead of high end consumer GPUs.,hardware,2025-09-14 21:07:34,32
Intel,ne6qvax,"Yeah, if ECC is a hard requirement for whatever reason then that would certainly rule out all the GeForce-branded RTX cards.    Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a [RTX PRO 2000 Blackwell](https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-2000/) instead, which fits the same niche as the B50 (i.e. low-profile 2-slot @ <75w with 16GB of VRAM) while being faster and having a far superior feature set.",hardware,2025-09-14 16:07:47,20
Intel,ne6p0f3,I’ve been working in a professional aechtiectural environment for 5 years and haven’t seen the need for ECC once.   Can you explain situations where it’s needed? I’ve always wondered.,hardware,2025-09-14 15:58:50,3
Intel,ne8kqkn,"SR-IOV for (license-free!) vGPU is IMHO the killer feature here, perhaps along with being able to get 16GB of VRAM per card relatively cheaply and without needing auxiliary power.  Both open up interesting server and workstation use cases that can't be had cheaply from the competition.",hardware,2025-09-14 21:11:39,11
Intel,ne95wnj,Have you tried GPU paravirtualization?,hardware,2025-09-14 23:06:16,1
Intel,nebgla3,"You can buy it off AIB Partners but you can't buy it at retail (i.e. microcenter, newegg] and it doesn't have an official MSRP yet.   The prices you see now are what AIB's want to charge in bulk orders.    If you want to know how much let's say 5 B60's cost you have to get a quote from a distributor",hardware,2025-09-15 09:33:31,3
Intel,ne8r5hr,They need GPU IP for two things: client and AI. Anything else is expendable.,hardware,2025-09-14 21:44:37,6
Intel,ne9x6oh,"Yes, my point was *if* they have the gaming cards, they can justify the professional line, but it's not nearly big enough to justify making a dGPU to begin with.",hardware,2025-09-15 01:47:23,6
Intel,ne6f8e1,"SR-IOV is Virtual GPU (SR-IOV is IO Virtualization used to split PCIe lanes into virtual functions so their physical function can be shared between VMs). No consumer cards support Virtual GPU right now besides Pascal/Turing with driver hacks. AMD's SR-IOV offerings are [very limited](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#virtualization-support), [And Nvidia has a bigger selection](https://docs.nvidia.com/vgpu/gpus-supported-by-vgpu.html) but their budget VGPU options are being phased out (P40).  I believe VDI is Microsoft's implementation. I believe I've done VDI on my RTX 2070 before (I have done seamless sharing between host and VM), but I don't know if it's possible with AMD. Someone please correct me if I'm wrong here, I'm more familiar with the Linux side / vGPU than VDI.  ECC is Error Correcting RAM. I generally don't understand the use case for ECC either, but it is ubiquitous in HPC. All server boards support ECC RAM.  In modern environments most of these features need 16GB of VRAM minimum, but if you ever wanted to try it on a consumer card, you could get an old RTX 20 series and try it out with some driver mods. Optionally, the P40 is still pretty cheap ($250 used) and doesn't need those hacks at the cost of drawing a lot of power, which Intel has solved with their Battlemage Pro platform (by far the cheapest VRAM/$/W you can get).",hardware,2025-09-14 15:10:44,31
Intel,neanu9y,>Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU.   A380 and A770 is also on the certified gpu list. But otherwise that statement is correct.,hardware,2025-09-15 04:51:08,12
Intel,ne6s65g,Can you buy an rtx pro 2000?  If i had to guess what percentage of wafers are for b200 chips i would say 90%.  I don't think there are enough pro 2000s around. I don't think there are enough gpus around in most cases.,hardware,2025-09-14 16:13:57,6
Intel,neaqr14,"> Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a RTX PRO 2000 Blackwell instead  It is interesting. IDK how common it is, but one of my university labs had computers donated from nvidia with nice quadro GPUs for their time.",hardware,2025-09-15 05:16:19,2
Intel,neb03p4,Pathetic 1:64 ratio of FP64 flops,hardware,2025-09-15 06:43:15,2
Intel,neb28v9,RTX 3090 Ti and RTX 4090 have ecc. Not that they're cheap.,hardware,2025-09-15 07:03:44,2
Intel,ne705od,"It's needed whenever the work you're doing matters and a single-bit error could cause significant harm.  Something like audio or video, a single-bit error probably isn't very noticeable.  Calculations, it absolutely depends on what you're calculating and which bit gets flipped; flipping the low-order bit in a number might not matter much and flipping the high-order bit could cause a big error.  Networking, it depends on whether the protocols you use have more error checks at a higher level (TCP does; UDP does not).  If in doubt, you want ECC, but market segmentation to mostly restrict ECC support to ""server"" chips and boards and charge more for ECC memory means you'll overpay for it.",hardware,2025-09-14 16:51:06,15
Intel,nea091d,"They're so good, I wish there was a single slot variant.   I want to put them in my MS-01s. The Sparkle A310, being the main candidate for deployment in those machines, only has 4GB and its maximum h264 encoding throughput actually drops below the iGPU (although its h265 and AV1 throughput slaps the 12900H/13900H). It's just a little too low to comfortably handle the Plex server usage I have, so the iGPU remains in service until a suitable competitor arrives.",hardware,2025-09-15 02:05:50,3
Intel,nebghm1,"IIRC, that requires a Windows host right? That's a non starter for many people unfortunately",hardware,2025-09-15 09:32:29,3
Intel,necmv9o,my guy sr-iov is a type of gpu paravirtualization.,hardware,2025-09-15 14:23:28,1
Intel,nenukwd,The Asrock b60s are $599,hardware,2025-09-17 06:30:55,1
Intel,ne90ue7,AI doesn't even need a GPU; it can have its own accelerators - see Gaudi.,hardware,2025-09-14 22:37:50,3
Intel,ne8u0oh,"One use case for ECC, is when the data is critical and can’t be lost.",hardware,2025-09-14 21:59:59,10
Intel,nea1lfo,>  I generally don't understand the use case for ECC either  Its for when you don't want errors to just be ignored?   How is that hard to understand?,hardware,2025-09-15 02:14:04,12
Intel,nebc61d,ECC should be in literally all memory.,hardware,2025-09-15 08:47:24,1
Intel,ne70ox2,"\>but their budget VGPU options are being phased out (P40).     I mean, the T4, L4 , and A16 exists...     I'm also not sure why low end workstation GPU needs SRIOV support.",hardware,2025-09-14 16:53:32,-11
Intel,ne6xml5,I see them at least available in my stores. although mostly as backordres via remote warehouses but they seem readily available with some shipment time.,hardware,2025-09-14 16:39:25,8
Intel,nee4dd7,"but you can still do gpu paravirtualization without sr-iov using Mediated Passthrough, API Forwarding (RemoteFX) or Dedicated Device Assignment",hardware,2025-09-15 18:42:34,2
Intel,neozcix,"Where can you get them? And are they for sale yet, or pre-orders, or...?",hardware,2025-09-17 12:27:00,1
Intel,ne9vlfz,"The problem with Gaudi (I know, I've written code and run training runs on it) is simply that the programming model is not oneAPI, or whatever oneAPI becomes. Yes, pytorch works, but people care a lot about software longevity and long term vision when buying $5mm+ of GPUs (and these are the purchases Intel cares about that can actually start to offset the cost of development).   The whole purpose behind Falcon Shores (and now Jaguar Shores, if it will even happen) is to put Gaudi performance (i.e. tensor cores) in an Xe-HPC package. Unifying graphics and compute packages is what NVIDIA was able to achieve but not yet AMD, and it's really great for encouraging ML development in oneAPI.  See this post to see where Intel would like to be: https://pytorch.org/blog/pytorch-2-8-brings-native-xccl-support-to-intel-gpus-case-studies-from-argonne-national-laboratory/ (they don't mention the ""XPU"" because it's Ponte Vecchio, which are iiuc worse than A100s).",hardware,2025-09-15 01:38:00,8
Intel,ne9xfff,"Intel can't get people even in an AI shortage. No one wants to deal with an ASIC. That's why their AI solution is GPUs, starting with (hopefully) Jaguar Shores. So it's that or bust.",hardware,2025-09-15 01:48:50,7
Intel,neav264,I spit my coffee reading that. Gaudi? The platform that nobody uses that Intel has to revise their sales estimates down each half quarter?,hardware,2025-09-15 05:55:13,2
Intel,neaiu6t,"Yup. For example you are doing a structural integrity physics simulation, and a single flipped bit can ruin your 1 week long run (and your liability insurer will reject your claim, a lot of them have standards requiring calculations to be done only on ECC for sensible reasons).",hardware,2025-09-15 04:10:17,11
Intel,ne78lg0,"Great example of why certain people shouldn't reply if they don't have knowledge in the area.  - Tesla T4 is $650 Used and has 16GB of VRAM. - Tesla L4 is $2000 Used and has 24GB of VRAM. - Tesla A16 is $3000 Used and has 64GB of VRAM.  Compared to:  - Arc Pro B50 is $350 new and comes with 16GB of VRAM. - Tesla P40 is $275 used and comes with 24GB of VRAM.  If all you care is vGPU / VDI for a small amount of hosts, then no, you're not getting a Tesla A16. What kind of joke suggestion is that?",hardware,2025-09-14 17:29:09,14
Intel,neea3nk,"Mediated pass though requires big ass license fees vGPU/MxGPU, and isn't FOSS other than Intel's currently broken support that they abandoned for SR-IOV support.  API forwarding only support limited host/guest setups, and even more limited API support. The only FOSS support is VirGL, which only support Linux host/guest and only OpenGL.  Obviously fixed pass though is an option, but even that isn't without issue. NVIDIA only recently removed the driver restriction, they could add it back at any time. Plus you are limited on virtual machine by the physical GPU count. It works with Intel GPUs and is FOSS with them.  SR-IOV on Intel fixes all of that. It works amazingly well with their iGPUs, has no license issues, and is fully FOSS.",hardware,2025-09-15 19:10:25,5
Intel,ne79g56,Hey no need to be aggressive towards the other user. Your comments are very helpful and I appreciated them a lot but keep it constructive please!,hardware,2025-09-14 17:32:53,10
Intel,ne79riz,"LMAO, I actually have quite a bit of knowledge in this area.  If all you care for is VDI for a small number of VMs, then you'd go GPU passthrough. vGPU / MxGPU often requires higher levels of hypervisor software tier (i.e. VMware vSphere Enterprise Plus), requiring more money. For KVM hosts, setting up vGPU is a lot more difficult and time consuming than just straight up GPU passthrough.  Only two groups of people would be interested in GPU virtualization / splitting:  * Enterprise, in which they wouldn't care about the used card prices.  * Enthusiasts, in which they wouldn't want to pay for vGPU prices anyway. So why bother catering to this crowd?",hardware,2025-09-14 17:34:16,-13
Intel,ne8go27,"Full GPU passthrough is not a solution that many people would consider because it is clumsier than using sr-iov (or potentially VirtIO GPU Venus). Plus for each extra passthrough instance I would have to add in another GPU and this greatly increases power consumption, heat output and cooling requirements. The process is not all that much more complicated at least on Turing GPUs with a hacked driver on KVM guests at least. Plus for passthrough, you probably still need an NVIDIA card because last I checked AMD cards still had a random kernel panic issue after being passed through.  My assumption is that sr-iov on the b50 will allow users an affordable way to have multiple guests on one host GPU without increasing power draw and paying for expensive alternatives and expensive vGPU subscriptions.",hardware,2025-09-14 20:51:40,10
Intel,ne92tmq,"...first time I heard people prefer SRIOV over GPU passthrough because it's ""clumsier"" lol. I'm sure setting up mdev devices in KVM, finding the correct corresponding GPU instances, making them persistent through reboot, then edit virsh xml for each individual VM is a lot easier than just doing IOMMU passthrough. /s     Again, enthusiasts don't care about power consumption / heat output / cooling requirements for their lab environment. Enterprise that do care about them are very willing to pay extra cost to get a production ready driver. You're creating a hypothetical situation that simply does not exist in the real world.",hardware,2025-09-14 22:49:00,-6
Intel,nd4rusz,"It's good they launching this, this card adds some competition to the landscape, but before anyone buys it they should figure out if their drivers are lighter weight.  It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  The more ARC cards out there the more developers get familiar with them, the more XeSS v2 gets added to titles, the more the drivers get matured and the better future ARC cards will be.  I'd happily pick up an ARC card... once they've proved themselves in terms of driver maturity and overhead.",hardware,2025-09-08 18:55:24,86
Intel,nd5dm3n,This should be a great upgrade from my A750 if B580 performance is anything to go by. I hope it's under $400.,hardware,2025-09-08 20:41:18,24
Intel,nd8r4il,"Unless it's super cheap, it's not gonna be sold well at all.  Even here with all the ""enthusiast"" and people are saying ""make sure you have this hardware combo, that driver, these settings,..."". The average buyer would just simply pay 50$ more for an nvidia card and not have to worry about all that.",hardware,2025-09-09 10:54:51,10
Intel,nd57awu,I hope they fixed the drivers CPU overhead problem or that GPU's gonna need a 7800X3D or 9800X3D to feed it fully.,hardware,2025-09-08 20:11:04,26
Intel,nd5z0zb,Too late for me I already went with a 9060xt but hell I had dreamt of it!,hardware,2025-09-08 22:32:44,3
Intel,nd4ofdy,"I wish they'd get the drivers past the point of frequently broken, but also they haven't produced enough cards for any previous launch to make any dent in the market regardless.  It's pretty much guaranteed the upcoming super refresh will make much more of a difference in terms of bang for your buck.",hardware,2025-09-08 18:38:32,3
Intel,nd5mkse,The problem with arc is you need the latest and greatest CPU to go with or you lose 1/4 of performance,hardware,2025-09-08 21:25:37,3
Intel,nd8g10o,Intel has the ball in it's court   If you released a New GPU..  that is pretty much a 5070.... add on 24gb of ram...  and price it at 399   u will   make  boatloads.  it will play pretty much any game at max settings at 1440p..  They must really be  hating on turning down sony though at making the SOC for the PS6 cause the margins too low..they really would need that money now lol,hardware,2025-09-09 09:13:47,2
Intel,nd4ety2,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-08 17:51:42,1
Intel,ndaptcd,Depending on the price I might give it a shot.,hardware,2025-09-09 17:16:31,1
Intel,nd7euko,Love it going to get one if I can scratch some money together,hardware,2025-09-09 03:38:15,1
Intel,nd5tii5,Wouldn’t there be a risk that future drivers will not be supported and that it comes with US government back doors?,hardware,2025-09-08 22:02:05,-4
Intel,nd6wdjt,"LOL, preparing ""box"" packaging   I immediately thought of advanced silicon packaging like CoWoS or whatever",hardware,2025-09-09 01:46:51,0
Intel,nd4lt4f,Who is gonna tell them G31 would be celestial die since B580 was G21 and A770 was G10?,hardware,2025-09-08 18:25:36,-12
Intel,nd5ec1f,"Ah yes, finally the 4060ti 16gb/4070 killer, only 1.5 years too late! Ig at least this will force price drops on the rx9070",hardware,2025-09-08 20:44:45,-7
Intel,nd50mkd,Aren't the driver overhead issues really only seen on older processors that are six cores or less?  Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course,hardware,2025-09-08 19:38:23,16
Intel,nd7icfg,"> It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  Is it really though? Powerful CPUs are comparatively cheap, powerful GPUs are expensive. I know plenty of people who went with a 9700X/7700X or even 9800X3D but 'cheaped out' on the GPU because spending $1200 on a 4080 (at the time) was simply too much.",hardware,2025-09-09 04:02:28,-2
Intel,nd91aqv,"I agree with everything you said.  However, I myself will buy one just because I want more competition and so I am just going to give Intel a sale. Sure it doesn't move the needle much, sure Intel's probably not going to make any money out of it and I personally probably won't use it much, but I am just doing it out of principle. I sure am in the minority, but at this point I can't sit idle and allow this duopoly to continue without trying something.",hardware,2025-09-09 12:07:11,7
Intel,nd5nmhj,"It's a hardware problem, not a driver problem. The cards have insufficient DMA capabilities and uploads must be handled by the CPU, no driver will fix it, and as a consequence the B770 will have even worse overhead.",hardware,2025-09-08 21:31:01,22
Intel,nd50bqm,I have a B580 and the driver seems pretty stable to me at this point.,hardware,2025-09-08 19:36:56,35
Intel,nd4xpjv,"The super refresh is only for 5070, 5070ti and 5080. I doubt the B770 will compete with the 5070 to begin with so those cards are more upmarket.",hardware,2025-09-08 19:24:08,9
Intel,nd5kt0f,my b580 has been stable,hardware,2025-09-08 21:16:33,9
Intel,nd7ucd7,"I’ve had a B580 for 6 months and have experienced one game-specific issue with Tarkov. Everything else, old, new or emulated has worked fine.",hardware,2025-09-09 05:39:29,2
Intel,nd86r6j,"I’ve been using Arc on both windows and Linux since alchemist, it’s powering 3 rigs for gaming, transcoding, etc.   Initial few months was rough but drivers are absolutely serviceable and have been for a while, and continue to get better each release.  I play lots of different games on steam btw, very rarely do I have issues.",hardware,2025-09-09 07:37:52,2
Intel,nd7ihdv,to our knowledge... i wonder what kind of uplift we'll see it have with next gen cpus,hardware,2025-09-09 04:03:27,3
Intel,nd4pva0,It's BMG-G31.  https://videocardz.com/newz/intel-confirms-bgm-g31-battlemage-gpu-with-four-variants-in-mesa-update  https://videocardz.com/newz/intel-ships-battlemage-g31-gpus-to-vietnam-labs-behind-past-arc-limited-edition-cards,hardware,2025-09-08 18:45:36,12
Intel,nd4qkrx,"It isn't the number that determines the generation, it's the prefix.  A770 was ACM-G10 (alchemist G10), while the B580 is BMG G21 (Battlemage G21). The shipping manifests that have been floating around for the better part of a year have been for the BMG G31. Unless new leaks I'm not up to date with are discussing a G31 with a different prefix, everything points towards it being battlemage, not celestial.  Now I pray that Intel have found a way to mitigate the driver overhead. If not, the B770 will be utterly useless for gaming. Nvidia is bad in the overhead regard, but the B580 is damn near an order of magnitude worse.",hardware,2025-09-08 18:49:07,17
Intel,nd57ffs,> Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course  Not even close.,hardware,2025-09-08 20:11:40,35
Intel,nd5dyvw,"HUB showed the b580 lost like 20%+ perf between the 9800x3d and the 7600 or 5700x3d and actually fell behind the 4060, as the 4060 lost minimal performance on the weaker cpu vs using the 9800x3d. And the 7600x and 5700x3d can certainly power much stronger gpus like the rtx 5070 without bottleneck.  Edit: my bad, I didn't know it was for only a specific game, though still not a good result for b580 overall",hardware,2025-09-08 20:43:00,0
Intel,nd63tmr,Where do you find such information?,hardware,2025-09-08 23:00:34,16
Intel,nd648nd,Source: I made it the fk up,hardware,2025-09-08 23:03:03,4
Intel,nd55qwm,"I was going to say, I put my sister in a b580 and she has had no driver issues in 6 months.",hardware,2025-09-08 20:03:30,18
Intel,nd4z315,"That's sort of my point, they'll probably still exert more price pressure across the stack than the b770, despite being a totally different segment.",hardware,2025-09-08 19:30:52,10
Intel,nd7nnb6,https://youtu.be/00GmwHIJuJY?si=z4wU05sJx2SeS7K1  How can people get on here and lie and literally no one questions them? The 7600 only lost anywhere near that performance on Spider-Man Remastered,hardware,2025-09-09 04:42:33,45
Intel,nd6d7u0,"Inference, there was a blog post tracing and comparing what the Arc driver does with the Radeon driver. The radeon driver just sends a few pointers to buffers, the Arc driver sends large amounts of data. Assuming the driver programmers at Intel aren't idiots, it's because something is seriously wrong with the cards and DMA.",hardware,2025-09-08 23:55:26,14
Intel,nd6ck88,"No, I inferred it from tracing what the driver does, and assuming the programmers aren't idiots.",hardware,2025-09-08 23:51:40,7
Intel,nd5oln7,Try Mechwarrior 5: Clans on high and say there are no problems again.,hardware,2025-09-08 21:36:04,-12
Intel,nd70uvd,"https://chipsandcheese.com/i/154252057/driver-cpu-usage-vs-application  By the bars, it looks API specific which hints driver problems.  But maybe there is some command protocol inefficiency. I'll certainly hope at least one of the Steves will look into B770 driver overhead.",hardware,2025-09-09 02:12:34,22
Intel,nd6bnfg,"that game runs on my 3080 ti like ass, just like all early UE5 games...  even with RT turned off, to hit solid 4k60 I needed DLSS and if I wanted 90+ fps I need to use DLSS performance / ultra performance.",hardware,2025-09-08 23:46:18,13
Intel,nd89f2e,Steve from HUB and Steve from GB both lack the technical knowledge to look into the underlying issues,hardware,2025-09-09 08:05:10,11
Intel,nd6cup4,"It doesn't even run, it crashes left and right on an Arc.",hardware,2025-09-08 23:53:20,-5
Intel,nd9sk86,"Not looking for a chipsandcheese analysis, just whether the issue exists.",hardware,2025-09-09 14:37:33,4
Intel,nd6drxa,"that I have no idea, on launch I did have crashing issues on my 3080 ti, but they did get resolved over time.  but if you are now seeing it still then welp  PGI is a small team that may not have gotten help / getting to it themeslves to make their game arc capable.",hardware,2025-09-08 23:58:41,4
Intel,nd6en4h,"Yeah, still doesn't work at the latest patch (and latest Arc driver) with anything other than low.",hardware,2025-09-09 00:03:40,-1
Intel,nc8cvxo,SR-IOV at that price. Who cares about anything else.,hardware,2025-09-03 17:55:15,162
Intel,nc9qxn7,Intel would be stupid to axe there Graphic card division if this proves to be successful.,hardware,2025-09-03 22:00:54,90
Intel,nc9seh0,"Single-slot variant or custom cooler please, my MS-A2 running proxmox is demanding this card.",hardware,2025-09-03 22:08:49,20
Intel,ncc2zjm,About 66% overall performance of a B580 it looks like. That's really nice for a 70W card.,hardware,2025-09-04 07:35:40,13
Intel,nc98dp9,"This is exciting, definitely looking forward to the b60 as well.",hardware,2025-09-03 20:28:14,22
Intel,nc8kczs,"Obligatory ""Intel is exiting the GPU business any moment now"".",hardware,2025-09-03 18:32:12,82
Intel,nc8gp00,how hard are tehse to actually buy?,hardware,2025-09-03 18:14:07,18
Intel,ncaibo5,"Buying one, this is impressive",hardware,2025-09-04 00:36:23,14
Intel,nca78up,Its better than a 1.5 year old bottom of the range card....well done i guess.,hardware,2025-09-03 23:32:04,-24
Intel,nca1tmp,Better than NVIDIA? lol .... oooookay,hardware,2025-09-03 23:01:43,-34
Intel,nc8e96z,"Haven't seen the video, but I'm already buying one if that's the case",hardware,2025-09-03 18:01:47,48
Intel,ncafc02,Literally it could be a damn arc a310 or rx6400 and people would buy that card at $350 without licencing bs. For anything VDI related the B50 is huge.,hardware,2025-09-04 00:18:35,32
Intel,ncbdpjm,Intel’s “MOAAAAAR CORES” in the GPU space???,hardware,2025-09-04 03:54:14,7
Intel,ncde8b8,what is that? SR-IOV?,hardware,2025-09-04 13:38:29,6
Intel,nccuj5h,Super interesting!  Wonder how well it would handle AI tasks like Frigate while another VM uses it for inference or a third doing video transcoding with Plex.,hardware,2025-09-04 11:42:43,3
Intel,ncbojo9,16 GB VRAM too,hardware,2025-09-04 05:20:40,5
Intel,nc9ulp7,They will eventually axe it.,hardware,2025-09-03 22:20:53,22
Intel,nccun4t,Instead of axing it maybe spin it off like AMD did with Global Foundries?,hardware,2025-09-04 11:43:28,1
Intel,ncbkppb,And if it isn’t successful?,hardware,2025-09-04 04:48:20,1
Intel,ncbs3pt,They do have 3rd party vendors for ARC PRO Cards this time around so it most likely will happen.,hardware,2025-09-04 05:52:01,11
Intel,nc9di83,"The B60 is more exciting to me just for that 24GB VRAM. Still, at this price point the B50 is a pretty compelling buy tbh.",hardware,2025-09-03 20:52:30,15
Intel,nc95it5,I think it would be really stupid for them to do so.,hardware,2025-09-03 20:14:44,40
Intel,nc8hjt4,You can preorder from newegg now. They ship later this month.,hardware,2025-09-03 18:18:26,33
Intel,nc8xqpv,"One Swedish retailer I checked has them coming into stock next week (10 September) and open to orders, how much stock there will be however, I have no clue.",hardware,2025-09-03 19:37:13,10
Intel,nce6pwg,Same. I put my preorder in. Plan to put it into one of my sff builds.,hardware,2025-09-04 15:55:20,4
Intel,ndhamhf,Why is this impressive for $350 USD? How will this be useful for you? I’m not being sarcastic. I am genuinely curious.,hardware,2025-09-10 17:12:23,0
Intel,ncdy79x,What did bottom of the range cards cost 1.5 years ago?  How much VRAM did they have?  Did they support SR-IOV?   Just think for a bit sometimes.,hardware,2025-09-04 15:15:31,11
Intel,ncdy1t2,It quite literally is. Watch the fucking video.,hardware,2025-09-04 15:14:51,11
Intel,nc8xenf,"Wendell confirmed as much in the comments, looking forward to his future testing of the card.",hardware,2025-09-03 19:35:34,31
Intel,ncc2mdm,What does AMD have in this product segment?,hardware,2025-09-04 07:31:59,2
Intel,nd8jouo,"Ingen av de kortene greier LLM, hvis du tror det.",hardware,2025-09-09 09:50:11,0
Intel,nd8rcrq,"Ingen av de kortene greier LLM, hvis du tror det.",hardware,2025-09-09 10:56:38,0
Intel,ncdf0ln,"I didn't know either so I looked it up.   ""SR-IOV (Single Root Input/Output Virtualization) is a PCI Express technology that allows a single physical device to appear as multiple separate virtual devices, significantly improving I/O performance in virtualized environments by giving virtual machines direct access to hardware. This bypasses the overhead of a software-based virtual switch, resulting in lower latency and higher throughput for demanding applications by dedicating virtual functions (VFs) to guest VMs.""",hardware,2025-09-04 13:42:39,12
Intel,nccz58k,Everyone (including Nvidia) is moving toward APUs with large GPUs onboard. Why would Intel kill their chance at competing in that market?  They've already withstood the most painful part of the transition. There's no point in stopping now.,hardware,2025-09-04 12:12:40,33
Intel,ncegs0k,They are keeping their Fabs which is even more expensive to maintain why would they sell GPU not to mention their iGPUs are pretty Damm good nowdays not like meme in Intel HD4400 even though they could play any game /jk.,hardware,2025-09-04 16:42:52,3
Intel,ncenqdx,Doubt they have the revenue to spin out successfully without significant investment from outside sources.,hardware,2025-09-04 17:15:36,4
Intel,nc9xu5x,Sadly Intel has a recent history of making poor life choices.,hardware,2025-09-03 22:39:04,59
Intel,ncaqsmf,"Maybe it's just me, but this reads as AI generated.",hardware,2025-09-04 01:26:28,12
Intel,ncar7n9,"I dunno man, I was building a PC for work and the 3050 was the cheapest Nvidia card I can get and the 7600 is the cheapest from AMD. Huge price gap between the two, by about 100 USD. AMD really needs to buck up their APUs to render cheap GPUs surplus or have something cheaper than a 7600 to price match the 3050.",hardware,2025-09-04 01:28:53,3
Intel,nce2wy8,"They're comparing it to an entry-level NVIDIA GPU, the A1000. Saying that Intel GPUs are ""better than NVIDIA"" as a universal statement is flat-out wrong. Let's see some competition to the RTX 5070 Ti, 5080, or 5090. NVIDIA has zero competition on mid-range and high-end GPUs.",hardware,2025-09-04 15:37:44,-16
Intel,nceqap4,Radeon Pro V710 and you can't even buy it retail.,hardware,2025-09-04 17:27:25,10
Intel,ncdfanm,thanks 🙏,hardware,2025-09-04 13:44:06,2
Intel,ncd1hnr,Because intel shareholders are super short sighted.,hardware,2025-09-04 12:26:50,29
Intel,nc9zg8o,this comment is the weirdest version of 'corporations are people' that i've encountered,hardware,2025-09-03 22:48:13,13
Intel,ncb4jkb,Lmao seriously the formatting and the amount of bolded words just screams AI,hardware,2025-09-04 02:50:42,6
Intel,ncar9b0,It's AI generated in your mind,hardware,2025-09-04 01:29:09,-9
Intel,ncaz7h1,"because AMD has a bad habit of leaving a bunch of their older cards in the channel and having them become the low end...  CPU and GPU, AM4 lives for so long because there are still piles of the stuff in the channel and just gets slow tiny discounts till its gone in full  its like their demand forecast is too optimistic or something but at this point I think its deliberate",hardware,2025-09-04 02:16:51,2
Intel,ncee75r,Because this is not a gaming GPU and thus the A1000 is the correct card to compare with.,hardware,2025-09-04 16:30:17,12
Intel,ncel88g,Good luck using those super gpus to host multiple gpu accelerated vm with one card. Nvidia won't let you.,hardware,2025-09-04 17:03:58,8
Intel,ncemu0g,"Yes, compare an Arc Pro to a GeForce, totally the same market.",hardware,2025-09-04 17:11:26,7
Intel,nchqdw5,"That seems more mid or high tier rather than these relatively low tier gpus, the b50 is a cut down b580...  Also the v710 seems like the kind of ""passive"" gpu that's ""passive"" as long as it's next to several 7,000 rpm fans.  So it would probably not work very well as a retail car because it's rack server focused.",hardware,2025-09-05 03:02:19,1
Intel,ncdxuek,"I think the really loud short sighted shareholders have quieted down a bit after it became clear they helped drive the company to where they are. Hell, they're probably not even shareholders anymore.",hardware,2025-09-04 15:13:52,11
Intel,nca5gwf,The weirdest version was Citizens United,hardware,2025-09-03 23:22:10,21
Intel,ncbz15z,Did you not figure out why they’re bolded?,hardware,2025-09-04 06:56:47,2
Intel,nci2bk5,I'm aware but it's the only current gen GPU for graphics workloads that has virtualization support from AMD.,hardware,2025-09-05 04:23:02,4
Intel,ncifqdy,"The current Chairman of the board, Frank Yeary, is one of these stupid short sighted people. He REALLY wants to sell the fabs, and is probably the reason Intel went through their latest round of layoffs (Lip-Bu Tan wanted to raise money from Wall Street, Yeary apparently sabotaged it).",hardware,2025-09-05 06:14:10,10
Intel,ncbjmt2,"if corps are people, they should be allowed to vote, right ?",hardware,2025-09-04 04:39:39,2
Intel,ncbjx8i,"Not only that, but because they are people, they should also be able to fund gigantic super PACS to get a candidate into office. I love America!",hardware,2025-09-04 04:41:58,7
Intel,ncd1vg1,"u/michaellarabel It would be super cool to have Molecular Dynamics benchmarks for these kind of cards, since you already use them for CPU testing and a few of them (e.g. GROMACS) support APIs from all three vendors (CUDA, ROCm, SyCL, + OpenCL)",hardware,2025-09-04 12:29:05,4
Intel,ner1h1d,"I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other, the leader can change in a generation or two, which hasn't been the case often in recent history.  Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.",hardware,2025-09-17 18:36:40,49
Intel,ner49pn,"Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.",hardware,2025-09-17 18:50:20,70
Intel,ner8xmt,Youtube link:  [https://www.youtube.com/watch?v=Y9SwluJ9qPI](https://www.youtube.com/watch?v=Y9SwluJ9qPI),hardware,2025-09-17 19:12:31,16
Intel,nerwnif,I cannot wait to see what's in store for 2026 Mac Studios and the M5 CPU. Especially if M5 Ultra makes its debut. AI workloads should see a significant performance boost by 3-4x? I wonder if M5 Ultra will offer 1000GB/s memory bandwidth.,hardware,2025-09-17 21:04:08,37
Intel,ner54hk,6 wide e core holy shit,hardware,2025-09-17 18:54:23,32
Intel,neqzzpm,"A major exciting aspect for me is the massive boost to Raytracing performance. The M4 Max is the closest anyone has ever come to matching Nvidia in 3D Raytraced Rendering, beating out even AMD. In Blender M4 Max performs somewhere in between an RTX 4070M and 4080M.  A 56% leap in RT performance would essentially put an M5 Max closer to a RTX 5090M than anyone before at a fraction of the power.",hardware,2025-09-17 18:29:21,36
Intel,neugekl,"Is there a link for English? If not, can you summarize how they tested sustained performance and how much is the improvement over previous generations?",hardware,2025-09-18 06:51:14,7
Intel,netnyfa,"The E core having more improvements than just 50% larger L2 is a nice surprise, but damn the efficiency and performance of it is insane. 29% and 22% more performance, at the same power draw is insane, clocking like 6.7% higher too. They used to be behind the others in performance with the E cores but had better efficiency but now they both have better performance and efficiency.  As for GPU, I always wanted them to focus on GPU performance next and they finally are doing it. Very nice, the expected 2x FP16 performance, which now matches the M4 which is insane(M5 will be even more insane). Gpu being 50-60% faster is a nice sight to see. For RT performance(I still find it not suited for mobile but M5 will be a separate matter) I’m surprised that the massive increase is just from 2nd gen dynamic caching, the architecture of the RT core is the same, just basically a more efficient scheduler which improves utilization and less waste.  For the phone, vapor chamber is nice, them being conservative on having a low temperature limit can both be a good and bad thing which is shown, the good thing is that it means the surface temperature is lower so the user won’t get burned holding the device, and the bad thing is that it can leave performance off the table which is shown. As that can probably handle like another extra watt of heat and performance. Battery life is very nice, the fact that it can match other phones with like over 1000mAh bigger battery is funny. As people always flexing over how they have like a 4000, 5000mAh+ battery, of course having a bigger capacity is better, but the fact that Apple is more efficient with it and can have the same battery life at a much smaller battery speaks volumes about it.",hardware,2025-09-18 03:04:06,9
Intel,newpjmq,"The occupancy characteristics of A19 Pro are quite incredible. 67% occupancy for a RT workload.  Look at Chips and cheese's SER testing. 36-44% ray occupancy with SER in Cyberpunk 2077 RT Overdrive.     Assuming NVIDIA can get this working on 60 series an effective a 52-86% uplift. After OMM and SER this seems like the third ""low hanging"" RT fruit optimization. Anyone serious about a PT GPU architecture NEEDs dynamic caching like Apple. And no this is not RDNA 4's Dynamic VGPR, it's a much bigger deal. Register file directly in L1$ has unique benefits.",hardware,2025-09-18 15:59:37,4
Intel,nes030p,"Nice been waiting for this. P-core frontend improvements and branch, and a wider E-core with it's newer memory subsystem shows great YoY gains as usual. Though I am not surprised since it's been years leading up to this that Apple has steadily have been increasing power/freq to get the rest of it's performance gains, although IPC YoY is still class leading. The wider e-core takes the stage which is now commonly being focused in the industry (ex. Intel: Skymont etc). Excited for any outlet doing die analysis (I don't know if kurnal has done it yet).  Real generational GPU gains, instead of last year's YoY tick. Supposedly GPU size has not increased and that is impressive. Massive FP16 compute matching the M4, really shows their commitment to ML (as if naming 'tensor cores' wasn't obvious) and this will greatly help with prompt processing if you're into local models. Finally with a vapour chamber in the PRO models, performance overall really stretches it's legs and sustained is really respectable.  Also, since I'm skimming, I'm assuming A19 base like it's predecessor is a different SoC to the Pro. It is also really really refreshing to see the base A19 be better than the 18 Pro, little to no stagnation and a year at that. The base iPhone 17 looks like a reaaly reallly good option, more than ever, wished they didn't drop the Plus model. But man, I feel like waiting another year, hearing rumours about N2 and new packaging tech excites me.  That said, looking forward to QC, MT, and Samsung. SD8EG5 seems to be closing the gap, and that'll be very interesting tho those GB numbers don't tell things like power.",hardware,2025-09-17 21:21:03,9
Intel,nergppp,"""Generations ahead of other ARMs M cores"".   Uhm we are getting the Dimensity 9500 and 8 elite gen 5 next week    The C1 Pro has 20% IPC improvement IRRC, plus this is N3P   Let's not jump to conclusions before seeing the competition    I also wonder if QC made changes to the E cores",hardware,2025-09-17 19:49:30,18
Intel,nf0sz1w,>SLC (Last Level Cache in Apple's chips) has increased from 24MB to 32MB  Really tells you how pathetically stringent AMD has been with cache sizes on their apus (no die size excuse allowed here because they never use leasing nodes specially N4 was already old when Strix point debuted),hardware,2025-09-19 05:27:21,2
Intel,nerudqn,">A19 Pro E core is **generations ahead** of the M cores in competing ARM chips.  >A19 Pro E is 11.5% faster than the Oryon M(8 Elite) and A720M(D9400) while USING 40% less power (0.64 vs 1.07) in SPECint and 8% faster while USING 35% lower power in SPECfp.  >A720L in Xiaomi's X Ring is somewhat more competitive.  No, Apple's 202**6** E-cores are just **+3% (int perf)** and **+1% (fp perf)** vs Arm's 202**5** E-cores, though at **-22% (int)** and **-16% (fp)** less power.  Note: Geekwan's chart is wrong. The Xiaomi O1 does not use the A72**0.** It uses the upgraded A72**5** from the X925 generation. Not sure how Geekerwan got the name wrong, as they recently reviewed it.  Integer  |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%|  Floating point  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%|  I would not call this ""generations"" ahead.",hardware,2025-09-17 20:53:36,0
Intel,nezl76m,I always wonder how they plot the architecture map and figure out such as the depth of LDQ kind of things...Is it public somewhere? That kind of detail won't be able to get via regular benchmark right?,hardware,2025-09-19 00:45:37,1
Intel,nf76uxj,A19 Pro GPU is now only 1.62GHz vs 1.68GHz in A18 Pro while having the same number of ALUs (768). Does that mean the increased performance is basically due to memory bandwidth increase?,hardware,2025-09-20 05:23:42,1
Intel,nftq0t1,"At this point all the flagship phones are great. I have an iPhone 14 pro max and a Samsung Galaxy S23 Ultra. But I’ve had iPhones for years and i couldn’t switch if i wanted too i have way too much invested in ios (games, apps, music etc). But I ordered a 17 Pro Max 1tb thought about 2tb’s but have a 1tb now and still have 450gb’s free and i download everything and never delete anything. Im trading in the Samsung and giving iPhone to my mom. Still think of buying a cheap phone that gives me the $1100 trade in because it’s any condition and I hate trading in a phone thats practically new.",hardware,2025-09-23 19:51:55,1
Intel,ng9npz2,-21 freezer lol https://browser.geekbench.com/v6/cpu/14055289,hardware,2025-09-26 07:28:53,1
Intel,nerrr9i,">Power however has gone up by 16% and 20% in respective tests leading to an overall P/W regression at peak.  That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade in floating point. The 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm:  |SoC / SPEC|Fp Pts|Fp power|Fp Perf / W|Perf / W %| |:-|:-|:-|:-|:-| |A19 Pro P-core|17.37|10.07 W|1.70 Pts / W|84.2%| |A19 P-core|17.13|8.89 W|1.93 Pts / W|95.5%| |A18 Pro P-core|15.93|8.18 W|1.95 Pts / W|96.5%| |A18 P-core|15.61|8.11 W|1.92 Pts / W|95.0%| |A17 Pro P-core|12.92|6.40 W|2.02 Pts / W|100%| |8 Elite L|14.18|7.99 W|1.77 Pts / W|87.6%| |O1 X925|14.46|7.94 W|1.82 Pts / W|90.1%| |D9400 X925|14.18|8.46 W|1.68 Pts / W|83.2%|  These are *phones*. Apple, Arm, Qualcomm, etc. ought to keep max. power in check. This is *o*n par with MediaTek's X925, a bit worse than the 8 Elite, and much worse than Xiaomi's X925.  I would've loved to see efficiency (joules) measured, like AnandTech did. That would show us at least if ""race to idle"" can undo this high 1T power draw or not in terms of battery drain.",hardware,2025-09-17 20:41:23,-7
Intel,nerpzuu,"All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P. Apple buys first dibs on the wafers so they always have that advantage, it isn't always about the architecture itself.  It will be more interesting when there are Qualcomm chips out with their architecture on this N3P node, and the Mediatek chips with the usual off the shelf ARM cores on this node too to compare.",hardware,2025-09-17 20:33:10,-8
Intel,nfc55oq,"Those GPU stats are false. According to Tom's Guide, in 3D Mark Solar Bay Unlimited, the 17 Pro Max is only 10% faster than the s25 ultra https://www.tomsguide.com/phones/iphones/iphone-17-pro-max-review#section-iphone-17-pro-max-performance-cooling",hardware,2025-09-21 00:14:10,-1
Intel,neqz076,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-17 18:24:39,-11
Intel,neqyojo,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-17 18:23:10,-14
Intel,ner4der,">I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Me too. I'd imagine a marginal increase in size over A18 Pro. The P cores have mostly stayed the same. And the E cores despite major changes are unlikely to contribute to a major increase in area (if I'm right, individually they occupy around 0.6-0.7mm2 per core). The extra cache (around 2MB) should increase area slightly. SLC area as well should contribute to that increase.  I'd imagine the GPU with the new RT units, doubled FP16 units, new tensor cores, and general uarch improvements are the major contributor to any notable area increase.  Plus I still don't feel like the approaches these companies are taking are aligned very much in terms of GPU architectures. For eg, Apple's been very focussed on improving compute performance on their GPU. Qualcomm less so.  >Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other,   True that. Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp. But Apple's done extremely well with this upgrade. The E core jump has made them close the nT gap with Qualcomm while using much lower power.  GPU is a case where technically Qualcomm could take raw perf crown. But Apple's RT dominance, Tensor cores and general compute lead might help them in the desktop space.  >Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.  Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs. They consume similar or even more area for their core architecture while lagging in performance while using significantly more power. The x86 ecosystem and compatibility is the only reason they'd survive Oryon.",hardware,2025-09-17 18:50:49,28
Intel,netwfkn,Having so many SoC makers on arm competing against each other by one upping each other yearly is bearing fruit vs x86. i don't know how Intel and AMD long term can fare at this rate. Arm CPUs are still showing double digit gains yearly.,hardware,2025-09-18 04:02:47,16
Intel,neu40ew,Most of apples RT gains are from optmsiing how the GPU deals with divergence.  This is not dedicated RT silicon so much as making the GPU be able to maintain much higher throughput when there is lots of divergence.  RT operations have a shit tone of divergence.,hardware,2025-09-18 05:00:56,6
Intel,nfntq8k,I can't wait to see die shots and measurements for these chips. The A18 Pro and A18 die shots were really interesting to see what was compacted or lost for the base model chip. I have a feeling that there will be bigger differences for the A19 Pro and A19 with that giant SLC on the former. Die areas will also be interesting. Cache isn't cheap for area and I'd also love to see inside the new E-cores and GPU.,hardware,2025-09-22 20:34:30,1
Intel,ner599v,">Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  I wouldn't say its a medium core yet tbh. Apple's E cores are still sub 1mm2 in area usage. Compared to other ""M cores"" they are still relatively small. I imagine the A19 Pro E core is fairly larger but the A18 E core was around 0.6/0.7mm2 in size. I'd imagine its not grown a whole lot.  >They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.  I'd imagine they're saving up these to mention Blender perf etc in M5 keynote.",hardware,2025-09-17 18:55:01,28
Intel,ner97f2,4 minutes ago. Damn. I should have waited. I'll post it!,hardware,2025-09-17 19:13:49,14
Intel,nerym62,"An M5 Ultra would offer 1.23 Tb/sec of bandwidth scaling from the A19 Pro.  M5 (128-bit, LPDDR5X-9600) -> 153.6 GB/s M5 Pro (256-bit, LPDDR5X-9600) -> 307.2 GB/s M5 Max (512-bit, LPDDR5X-9600) -> 614.4 GB/s M5 Ultra (1024-bit, LPDDR5X-9600) ->1228.8 GB/s",hardware,2025-09-17 21:13:39,25
Intel,ner5ngo,"Haha. I mean they are actually pretty late to this tbh. Most ""E/M cores from other competitors"" are similar in size if not bigger. I'd imagine Apple's E core is stuck between a true M core like the A7xx and a true E core like A5xx in terms of area, although it probably leans toward the A7xx in that regard.",hardware,2025-09-17 18:56:53,20
Intel,nesz0ua,What does 6 wide mean? What units?,hardware,2025-09-18 00:36:10,1
Intel,nesjazw,"[https://www.reddit.com/r/hardware/comments/1jcoklb/enable\_rt\_performance\_drop\_amd\_vs\_nvidia\_20202025/](https://www.reddit.com/r/hardware/comments/1jcoklb/enable_rt_performance_drop_amd_vs_nvidia_20202025/)  In gaming RDNA4 RT isn't that far behind Blackwell. Other than that raytraced rendering like in Blender AMD has been for a while far behind. It won't be until Blender 5.0 till we see any improvements to HIPRT. Though for the longest time since following HIP it's been rather mediocre and my expectations are low for next release, though their [PRs](https://projects.blender.org/blender/blender/pulls/145281) make it seem they've been doing some work. It's a low priority for AMD which is unfortunate.",hardware,2025-09-17 23:06:05,7
Intel,ner9dq0,>	beating out even AMD  Was that one really surprising?,hardware,2025-09-17 19:14:39,0
Intel,nes24jj,Is that Metal vs Optix or Metal vs Cuda?,hardware,2025-09-17 21:31:31,1
Intel,neumrwz,"The video has a mostly accurate English captions option. CPU P core is up 10%, E core is up by 25%, GPU perf is up by 40% and sustained performance is up by 50%.",hardware,2025-09-18 07:52:53,10
Intel,neu4z8x,> just basically a more efficient scheduler which improves utilization and less waste.  When you take a look at GPUs doing RT task you see that they tend to be very poorly utilized.  GPUs are not designed for short running diverging workloads. But RT is exactly that. So you end up with a huge amount of divergence and or lots of wave like submissions of very small batches of work (so have a large scheduling overhead).     There is a HUGE amount of perfomance left on the table for this type of task for HW vendors that are able to reduce the impact on GPU utilization that divergence has.,hardware,2025-09-18 05:08:55,3
Intel,newyqf1,Maybe Apple measures occupancy differently in their tools. I wouldn't be too sure comparing these two. But I'd definitely think a combination of SER and Dynamic Caching present in A19 should result in very good utilization compared to other uarchs.,hardware,2025-09-18 16:43:46,5
Intel,nfd8klj,"That is very impressive, RT tends to have very poor occupancy as it is a heavily branching workload!",hardware,2025-09-21 04:40:04,2
Intel,netw9xh,>Supposedly GPU size has not increased and that is impressive.   Important to note that the supporting SLC which is a major reason for improvements om the GPU side has increased from 24Mb to 32Mb. Which would increase area a bit.,hardware,2025-09-18 04:01:38,5
Intel,nerkv5x,">The C1 Pro has 20% IPC improvement IRRC, plus this is N3P  N3P is 5% faster than N3E. By TSMC's own claim..  Also I can't find a source for a 20% IPC improvement. ARM's claim is 16% IPC improvement. And that is not without a power cost since ARM claims that at similar performance, power reduction is only 12%.  https://newsroom.arm.com/blog/arm-c1-cpu-cluster-on-device-ai-performance  >Let's not jump to conclusions before seeing the competition   I mean I agree. But I don't see how the C1 Pro is supposed to cross a 95% P/W disparity. (4.17 points using 0.64W vs 3.57 points using 1.07W) using D9400",hardware,2025-09-17 20:09:09,16
Intel,nf0tzxz,"Tbf, Apple had a 32Mb SLC back in the A15 Bionic. They reduced the size of that afterward to 24Mb. Its not like the size significantly mattered in GPU performance until now.",hardware,2025-09-19 05:35:43,1
Intel,nery3xl,A 30% lead in P/W is a generations ahead in this day and age. Considering the successor (C1 Pro) is stated by ARM to reduce power by just 12% at iso performance leaving Apple with a comfortable lead for a year. Also I specifically was a bit confused by their choice to compare the A725L instead of the M variant.,hardware,2025-09-17 21:11:11,17
Intel,nezsmw1,There's a guy on twitter who does the microbenchmarking for them.,hardware,2025-09-19 01:28:21,2
Intel,nf79l31,"No. I'm positive memory bandwidth offers very little in terms of performance upgrades. If you recall, the A16 was essentially the same GPU architecture as the A15 but used LPDDR5 instead of LPDDR4X, yet there were practically zero performance improvements.  I don't think anyone has investigated the A19's GPU microarchitecture thoroughly. But the main improvements seem to come from the increase in SLC size (System Level Cache which serves the GPU) from 24Mb to 32Mb and the newly improved Dynamic Caching. Its very likely there are a lot more changes responsible for that 40%+ improvement that we don't know about.",hardware,2025-09-20 05:47:18,1
Intel,nfd8co2,With most GPU tasks you are never ALU limited.,hardware,2025-09-21 04:38:19,1
Intel,nerth3n,">That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade and the 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm  Technically we need to compare P/W at similar power or similar performance. Peak power P/W is not a very accurate measure to compare gen on gen. And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.   An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.",hardware,2025-09-17 20:49:22,14
Intel,netxg4n,Im having my doubts here. Howis the A19 p core much better than the A19 pro p-core? Aren’t they exactly the same p cores?,hardware,2025-09-18 04:10:12,3
Intel,nerrcls,">All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P.   N3P is a mere 5% faster than N3E when comparing similar microarchitectures... This is straight from TSMC's marketing slides.  Comparitively, barring the P core (which did see an ok improvement), the E core and the GPU have seen 30%+ improvements. The node has nothing to do with it.  If Qualcomm loses to, matches or exceeds A19 Pro this year it would be because of their updated microarchitectures and have barely anything to do with minor single digit improvements offer by a sub node.",hardware,2025-09-17 20:39:28,16
Intel,neu4jji,in the end what matters is the sicion you can buy so you can compare them.,hardware,2025-09-18 05:05:19,3
Intel,nf0v0x1,Please tell us more about how we can never compare AMD vs Intel chips by your logic,hardware,2025-09-19 05:44:15,2
Intel,nfcfd2t,"Tom's Guide tested basic Solar Bay. This is the older version of the benchmark with less raytraced surfaces.  Geekerwan tested the modern, updated version of Solar Bay referred to as Solar Bay Extreme. This new benchmark has a much higher raytraced load, with far more reflective and transparent surfaces and much more detailed scene with more geometry.  Please kindly read the benchmark title mentioned in the posts. Or atleast watch the videos. Before commenting.",hardware,2025-09-21 01:17:53,1
Intel,nf0v37m,Bad bot,hardware,2025-09-19 05:44:47,1
Intel,ner7ckc,"I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  I just wouldn't call the race so early, but it does seem very likely, that AMD will be behind. I just dont think it is as bad as it seems. AMD was plagued by Zen5% and still on 4nm for client, they might hit heavy with client dedicated improvements and N3, but in the end we have to see, x86 client performance really seems to struggle rn (and whatever intel is doing...).",hardware,2025-09-17 19:04:59,9
Intel,nernd6v,">Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs.  Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.  Also... Let's not forget about ARM. A SoC with ARM CPU cores and an Nvidia/AMD GPU could absolutely ruin Qualcomm's day regardless of how better/worse their custom CPU cores are.",hardware,2025-09-17 20:20:52,6
Intel,nexnqt0,"IMO, Microsoft needs to push hard to switch to Windows-on-ARM, or else they risk an Android-like OS for Laptops swooping in and filling the gap left by those who do not want to go the Apple route. It feels like a crucial moment for both Windows and Intel/AMD, at least in the x86 product space. It retains PC Gaming at this point, but if that nutt is cracked via even half-decent, *compatible* emulation, then ... sayonara!",hardware,2025-09-18 18:41:54,4
Intel,ner8bhp,"Yeah, the E core isn't quite there yet, but it's good to know they're moving in that direction with now stubborn they are with P cores, refusing to include more than what can be run at the equivalent of all core turbo.",hardware,2025-09-17 19:09:33,8
Intel,nes3cj4,"Wow that will be insane and adding the new ""Tensor"" elements added to the GPU cores will make it a formidable AI workstation.  Especially when NVIDIA monopoly is only offering small VRAM GPU cards at absurd prices.",hardware,2025-09-17 21:37:52,16
Intel,neu4a08,That would be amazing. Id love to see them put some hbm there too,hardware,2025-09-18 05:03:07,1
Intel,neuqao2,It’s highly unlikely they will go past a 256 bit bus. You run out of pins and layers to route a bus that wide. Gets extremely expensive. Still neat bandwidth!,hardware,2025-09-18 08:28:29,1
Intel,nfjr9p6,The m5 ultra should be on par with a 4090?,hardware,2025-09-22 04:50:52,1
Intel,nerh62x,Skymont (Intel’s E core) is 8 wide (9 wide at first stage then bottlenecks to 8 I think iiuc),hardware,2025-09-17 19:51:36,12
Intel,nevbky1,"According to AnandTech, Apple's E cores were based on Apple's Swift core, back when they had a single core type  Previous die shots show Apple's E cores were close to Arm's A5xx in core area (larger, but far smaller than A7xx core only). But in terms of core+L2 Apple's E cores are similar to Arm's A7xx in core+L2 area  I'd argue it's the other way around, Apple's E cores are the true E cores  Whereas Arm's A7xx were stuck between being an M core and an E core  Now Arm has split their A7xx core to Cx Premium (M core) & Cx Pro (true E core)  Arm's A5xx/Cx Nano are a very odd core, almost no else makes a similar in-order core. Arm's A5xx/Cx Nano are more like Intel's LP E cores, instead of Apple's E cores",hardware,2025-09-18 11:35:22,5
Intel,nesz8nh,The decoder,hardware,2025-09-18 00:37:26,11
Intel,neuonvq,"Amd is very far behind in rt.  You re linking gaming benchmarks, thats not rt thats mixed use.    Just look at path tracing results for a more representative comparison",hardware,2025-09-18 08:11:51,11
Intel,ner9wud,"Hey, they made an effort with RDNA 4. I think that should surpass the M4 Max. I just can't find any proper scores for it.",hardware,2025-09-17 19:17:13,19
Intel,nes2dxt,"Metal vs Optix.  https://youtu.be/0bZO1gbAc6Y?feature=shared  https://youtu.be/B528kGH_xww?feature=shared This is a more detailed video with individual comparisions and a lot more GPUs.  Its a lot more varied. In Lone Monk, it hangs with a desktop class 5070. In Classroom, it hangs neck to neck with a 4060Ti. In Barbershop, it falls behind a desktop 4060Ti. In scanlands, it falls behind a 4060.   If we consider Classroom as a baseline average, a theoretical 60% faster M5 Max, like the jump we saw in Solar Bay, would land hot on the heels of a desktop class 5070Ti, a 300W card. Competing with a 65W laptop GPU.  Edit; The Youtuber is using the binned 32C variant. A 40C variant would surpass the 5070ti.",hardware,2025-09-17 21:32:51,6
Intel,neuej06,"Yeah I forgot what was the term before but I remember, it’s just like Nvidia’s Shader Execution Reordering introduced in Ada Lovelace.",hardware,2025-09-18 06:33:54,1
Intel,newy5n1,I have a query regarding RT workloads. Would offsetting RT performance to the CPU with the help of accelerators help? Or is that not the case and it would be even worser on CPUs.,hardware,2025-09-18 16:41:01,1
Intel,nexio2d,"Sure it might be different, but I doubt it. Occupancy is just threads used/total threads.  It's interesting how first gen dynamic caching + SER (apple equivalent) is hardly better than NVIDIA in terms of occupancy. Yet only 44%. So only slightly better than NVIDIA (upper end of range). Seems like first gen was more about laying the groundwork while second gen is really about pushing dynamic caching allocation granularity and efficiency. At least so it seems.  Oh for sure. That occupancy is incredibly high. \~1.5x uplift vs A18 Pro. Getting 70% occupancy in RT workload is really unheard of. Apple engineers did a fine job.  AMD might opt for this nextgen if they're serious, but it's a massive undertaking in terms of R&D, but could massively benefit PT and branch code, ideal for GPU work graphs.",hardware,2025-09-18 18:17:21,4
Intel,ngmjqfa,Agreed and you can see that by comparing with occupancy numbers for competitors.       Anyone who's serious about RT needs to copy whatever Apple is doing xD,hardware,2025-09-28 10:08:16,1
Intel,nez7dv6,"C1 Pro is two generations ahead of the A720 in the 9400. Also, Xiaomi demostrated a much more efficient implementation of the A720 cores in their O1 chip (4.06 points at 0.82 W).  Edit: actually, it seems like the O1 uses A725 cores. Perhaps that is what they are referring to in the video as ""A720 L""",hardware,2025-09-18 23:23:49,2
Intel,nf0vdfo,It’s been a BIG bottleneck in AMD’s apus since Vega 11 back in 2018. Doubling from 8CU to 16CU in 860m vs 890m gets you only +30%.   AMD is just so damn stringent with area despite jacking up the price on Strix point massively on an old ass node.,hardware,2025-09-19 05:47:13,2
Intel,nes028x,"You ought to do the math first. Power is the denominator. 12% reduction in power is *substantial*.  Integer: A19 Pro E-core is 3% faster at 12% less power vs claimed C1-Pro.   |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%| |\-12% power|4.06|100%|0.72W|5.64|114%|  Floating point: A19 Pro E-core is 1% faster at 4% less power vs claimed C1-Pro.  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%| |\-12% power|6.07|100%|0.96 W|6.32|113%|  Hardly ""generations ahead"".",hardware,2025-09-17 21:20:56,-2
Intel,nf7bmfd,Thanks for your reply.  CPU monkey reported 2GHz instead of 1.62GHz. So maybe that's where most of the gain comes from.  I suppose the tensor core like matmul units also boost performance for graphics and AI.,hardware,2025-09-20 06:05:07,1
Intel,nfdgjzr,Maybe the improvement is from the new matmul units?,hardware,2025-09-21 05:48:36,1
Intel,nerzs8z,"I am deeply suspicious of all of these power measurements. Separating p core power usage from other aspects of the soc is difficult on iOS. Which is not a criticism of your summary, but I’d be wary of drawing anything definitive about the efficiency of the p cores.",hardware,2025-09-17 21:19:32,2
Intel,nesrzwb,"It's not a knock on the design, it's how apple is configuring the CPU. It doesn't matter that performance at the same power is improved if the default clocks on the real product put it way past the sweet spot into diminishing returns so bad it regresses efficiency.   On one hand, the power inflation isn't causing problems if the 23% increased battery life per Wh is anything to go by, but on the other, what's the point of chasing peak performance like this if your boost/scheduling algorithms never allow that speed to make an impact on responsiveness?",hardware,2025-09-17 23:55:18,1
Intel,nery8a6,"Geekerwan's results are ***average*** power, not peak power, IIRC. These are real, actual frequency bins that Apple has allowed.  These frequency bins *will* be hit by some workloads, but just not nearly as long as SPEC & active cooling will allow. It would be good to revisit Apple's boosting algorithms, but IIRC, they hit 100% of Apple's design frequency in normal usage.  It's not like users have a choice here; we can't say, ""Please throttle my A19 Pro to the same power draw as the A18 Pro."" Low power mode neuters much of the phone, so it's rarely used all the time.  //  I find avgerage power useful two reasons:  1. *How* performance vs power were balanced; here, performance took precedence while not keeping power stable.  2. It also shows, when nodes are not the same, where the node's improvements went. Here, an N3P core delivers notably worse perf / W versus an N3E core. TSMC claims [up to 5% to 10% less power](https://www.tomshardware.com/tech-industry/tsmcs-3nm-update-n3p-in-production-n3x-on-track) on N3P vs N3E.  I agree 10W is not common and SPEC is a severe test, but it's more the *pattern* that has emerged on Apple's fp power and whether it's worth it:  2023 - A17 Pro P-Core: 6.40W  2025 - A19 Pro P-Core: 10.07W  Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   >And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.  I agree it's rare, but why would Apple allow 10W? Were many workloads ***lacking*** fp performance that users prefer a bit less battery life for +9% fp perf vs the A18 Pro?  Of course, to most, battery life is more important, IMO, which is why core energy is most crucial, but missing here.  //  >An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.  So the question becomes: do users want slightly better perf and 5% more power? On a phone, I'm of the opinion that power is paramount and should be forced to lower levels.",hardware,2025-09-17 21:11:46,3
Intel,neu0m6n,">Aren’t they exactly the same p cores?  Definitely not. See the SPEC perf / GHz: A19 is nearly just the A18 Pro. Thus, this seems to be the final picture:  A19 Pro = new uarch, 32MB SLC, 12GB LPDDR5X-9600  A19 = binned OC of last year's A1**8** Pro, w/ faster LPDDR5X-8533, but smaller SLC (12MB)  New uArch clearly didn't pan out as expected in fp efficiency. A19 Pro may sit at a flatter part of the freq / power curve, A19 Pro may have more leakage, A19 Pro's [faster RAM](https://en.wikipedia.org/wiki/Apple_A19) may eat *a lot* power (Geekerwant tests mainboard power, not purely CPU power), etc.",hardware,2025-09-18 04:34:04,0
Intel,nfcgk7w,I'm sorry you had to take time away from ripping off others' content to correct my mistake,hardware,2025-09-21 01:25:24,-1
Intel,ner89fq,">I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  M2 and Zen 4 launched around the same period. The desktop chips score around 5-10% faster in Geekbench while using 20W per core power and 30-40W more for the I/O die. Taking the ST crown by a hair's width while using 5-10x more power isn't a win at all imo.",hardware,2025-09-17 19:09:17,7
Intel,nerolok,">Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.   I'd agree if they performed as well as their clocks suggest. Whats the point of clocking to 5.7Ghz if a mobile CPU clocked at 4.4Ghz leads you in absolute performance by 15% (Geekbench) while using a tenth of the total power.",hardware,2025-09-17 20:26:36,9
Intel,nezdr68,This snapdragon era is Microsoft's third time trying arm (windows rt and surface pro x). Hopefully third times the charm,hardware,2025-09-19 00:01:23,6
Intel,nes52e4,Their E-core is faster than something like Tiger lake per clock. It’s about 60-70% as fast as the best desktop CPU from 2020 and probably as fast as a U-series chip from then under sustained loads.  The real takeaway for me is that the rumored A19 MacBook is going to dominate the $600 laptop market segment if it releases before Christmas.,hardware,2025-09-17 21:46:45,13
Intel,ner8qa3,"Weirdly enough, I feel like them not letting the E core balloon in size has helped them in the long run. Seems they're focussed on maximizing performance within a limited area. Their E cores have had exceptional jumps in performance since the A12 almost every generation being 25%+ in improvements.  I'd wager that E core dominance is the primary reason why the iPhones are able to match Android manufacturers using super large and dense batteries in terms of endurance.",hardware,2025-09-17 19:11:31,15
Intel,neu2ajk,Doesn't mac studio already 500gb vs nvidia workstations of 96gbs?,hardware,2025-09-18 04:47:07,4
Intel,neskg6l,NVIDIA currently fist pumping the air over their failed ARM acquisition rn,hardware,2025-09-17 23:12:34,5
Intel,neun1xa,Its a bit unlikely. Maybe for a version of the M series dedicated for a Mac Pro. But one of the main reasons they can get away with this design is because its very scalable. All the way from A series to Mx Max series. Adding HBM would probably require a dedicated SoC redesign for a very niche product segment in Macs.,hardware,2025-09-18 07:55:36,4
Intel,neuqz81,The M4 Max already uses a 512 bit bus. Does it not?,hardware,2025-09-18 08:35:23,4
Intel,neri14i,Its technically not a true 9 wide core. I think its 3+3+3.,hardware,2025-09-17 19:55:40,13
Intel,nes6z1y,E-Core is relative. Skymont is more of a C-core (area optimized) than what we typically think of as an E-core (energy optimized).,hardware,2025-09-17 21:56:51,12
Intel,neu4ctl,with a viable width ISA it is better to look at the typcile throughput not the peak throuput as you very rarely are able to decode 8 instructions per clock cycle.,hardware,2025-09-18 05:03:46,2
Intel,nerdt8m,"With the 9070? I don’t think I’ve seen any results showing that either, however all I’ve looked at is the blender benchmark charts",hardware,2025-09-17 19:35:44,1
Intel,neuqhpy,"the shader re-ordering is different. (apple also do this).  Even with shader re-ordering you have the issue that you're still issuing 1000s of very small jobs.    GPUs cant do lots of small jobs, they are designed to do the same task to 1000s of pixels all at once.     If  you instead give them 1000 tasks were each pixel does something differnt the GPU cant run that all at the same time... in addition the overhead for setup and teardown of each of these adds even more void space between them.   So apple are doing a hybrid approach, for large groups of function calls they do re-ordering (like NV) but for the functions were there is not enough work to justify a seperate dispatch they do function calling.    This is were dynamic coaching jumpstart in.      Typical when you compile your shader for the GPU the driver figures out the widest point within that shader (the point in time were it need the most FP64 units at once, and register count).  Using this it figures out how to spread the shader out over the GPU.  Eg a given shader might need at its peak 30 floating pointer registers.   But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time.   If you have a shader with lots of branching logic (like function calls to other embedded shaders) the driver typically needs to figure out the absolute max for registers and fp units etc.  (the worst permutation of function branches that could have been taken).  Often this results in a very large occupancy footprint that means only a very small number of instances of this shader can run at once on your GPU.  But in realty since most of these branches are optional when running it will never use all these resources.  The dynamic cache system apple has been building is all about enabling these reassures to be provided to shaders at runtime in a smarter way so that you can run these supper large shader blocks with high occupancy as the registers and local memory needed can be dynamically allocated to each thread depending on the branch it takes",hardware,2025-09-18 08:30:29,6
Intel,ney7oxo,"While RT does have lots of branching logic (and CPUs are much better at dealign with this) you also want to shader the result when a ray intercepts, and this is stuff GPUs are rather good at (if enough rays hit that martial)   We have had CPU RT for a long time and so long as you constrain the martial space a little GPUs these days, even with all the efficiency  loss are still better at it.   (there are still high end films that opt for final render on cpu as it gives them more flexibility in the shaders they use) but for a game were it is all about fudging it GPUs are orders of magnitude faster, you just have so much more fp compute throughput on the GPU even if it is running at 20% utilization that is still way faster than any cpu.",hardware,2025-09-18 20:16:47,5
Intel,ngmxbr9,"Its not just useful for RT but also for a lot of other situations, being able to just call out to functions on the GPU and not pay a HUGE divergence penalty (you still pay some) opens up GPU compute to a load more cases were we currently might not bother.",hardware,2025-09-28 12:07:27,2
Intel,nes14p7,I'm a bit confused. You think lagging 15% behind in P/W the competition for an entire year is not being a generation behind?  ARM themselves have managed only a 15% jump this year. So it will essentially be 2 years before we get an E core that matches the A19 pro. And this is just considering the Xiomi's SoC. Mediatek's and Qualcomm's which dominate the majority of the market lag even further behind.,hardware,2025-09-17 21:26:24,10
Intel,nf7c0uo,I'd advise against using CPUmonkey as a reliable source. They're known to make up numbers. (Reported M1 Pro/Max Cinebench scores 6 months before they launched based on predictions),hardware,2025-09-20 06:08:45,1
Intel,nes4cv8,"I agree to some degree; Geekerwan notes they are testing mainboard power, not core power (if you Google Translate their legend).  For me, I assume all the major power draws on the motherboard *are* contributing to the overall SPEC performance, too.   If the faster LPDDR5X-9600 in the A19 Pro eats more power, it's fair to include that power because ***all*** A19 Pros will ship with LPDDR5X-9600. That was Apple's choice to upgrade to this faster memory.    Now, you're very right: we can't purely blame the uArch at Apple. It may well be the DRAM or the boost algorithms (like we saw at the A18 Pro launch last year) and—at Apple specifically—even the marketing overlords.  It's also why I'm a big proponent of JEDEC speeds & timings & volts in desktop CPU tests, much to the chagrin of a few commenters.",hardware,2025-09-17 21:43:06,9
Intel,nes0tlj,Why is separating p-core power usage from SOC power uniquely difficult on iOS?,hardware,2025-09-17 21:24:50,1
Intel,nes018z,I understand your reasonings. But its the only semblance of comparison we have to date between different SoC. I've learned not to look a gift horse in the mouth.,hardware,2025-09-17 21:20:48,1
Intel,nerz3p1,">Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   >That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   I mean this has been a trend long before the A17. Apple has been increasing peak power little by little since the A12 Bionic.  I remember reading Anandtech articles about it.",hardware,2025-09-17 21:16:05,5
Intel,nfcj3et,Don't be a sour puss now because you didn't check your sources before commenting. Mistakes happen.  >ripping off others' content   Eh? Are you stupid? You're pissed that someone posted a hardware review ON A hardware sub? The entire sub exists to discuss hardware bozo.,hardware,2025-09-21 01:41:28,2
Intel,nerbuxh,"While the efficiency for this is BAD, I dont think its 20W per core.  When we look at results by [Phoronix](https://www.phoronix.com/review/amd-zen4-zen4c-scaling/2) we can see \~7-8W per core for this (not great numbers, because its a weird chip and different node), which is still very bad. AMD certainly has some power issues, but many of which, i.e. inefficient I/O dies, are not really dependent on the CPU uArch and could switch at any moment. They certainly have much more inefficient chips at the moment than both Apple and Qualcomm. For Zen 6 we expect a major update to the desktop chiplet architectur which could bring some much needed improvements in terms of I/O though.  They have reasonably fast cores, and I think they are not in a terrible position, even though it is far from good. I think what is interesting for AMD to look out for is that they keep moving fast, instead of intel who didnt move fast since like 14nm, and AMD has strong cores. Additionally AMD has (including from the datacenter) an enterprise need to make the CPUs more efficient.  So yeah, very bad CPUs efficiency wise. A bit behind, but not terribly on perf per node wise, efficiency on the desktop is an afterthought for AMD, clearly, but they are moving constantly and are improving. It might be AMD Laptop Zen 6 has again like 35W TDP, for 3000 Geekbench and be dead, but with some client oriented tweaks I see chances (maybe just from the patterns in the tea leaves in my mug)",hardware,2025-09-17 19:26:30,5
Intel,nerrlad,"I originally saw this on Phoronix' forums, but I can't find the link to the comment so I'll send this one instead: [https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png](https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png)  Zen 5 is now behind, yes, but it isn't really that bad.",hardware,2025-09-17 20:40:36,3
Intel,nf8dztb,"Per clock, the A19 Pro E core is competitive with Golden Cove. Atleast in SPECint.  M4 E core-> 3.53 points at 2.88Ghz (1.23 p/Ghz) or IPC i9 14900k-> 9.93 points at 6.00Ghz (1.65 p/Ghz) or IPC  A19 Pro has a 22% jump in IPC in SPECint (might be some variations due to time difference and lack of knowledge about the subtests ran, but still gives a good picture)  22% IPC jump over M4/A18 E core = 122% of 1.23 = 1.50 A19 P core = 1.50 p/Ghz  Golden Cove in i9 14900K has a mere 10% lead over A19 Pro in perf/clock in SPECint.  https://youtu.be/EbDPvcbilCs?feature=shared  Source: I9 14900K compared with M4 in this review.",hardware,2025-09-20 11:59:43,3
Intel,neu482t,It does but the Macs are limited in other ways (memory speed among other things),hardware,2025-09-18 05:02:41,3
Intel,netuj31,It wasn't for a lack of trying.,hardware,2025-09-18 03:49:13,8
Intel,nf2fm06,Yeah there was some rumors of a server version and thy have a patent for a multi level cache setup but they also patent and prototype plenty of things that never get released.   https://www.techpowerup.com/277760/apple-patents-multi-level-hybrid-memory-subsystem,hardware,2025-09-19 13:24:45,2
Intel,neuu5xp,"Oh huh, it does but it’s 128 bit per channel. So memory on 4 sides of the die. Wild, don’t see that normally except in FPGA for data logging (or GPUs)",hardware,2025-09-18 09:07:21,2
Intel,netvlsr,"The difference seems a bit drastic in open data benchmarks.   https://youtu.be/B528kGH_xww?feature=shared  Testing individual scenes, the 9070xt and M4 Max seem neck and neck.  The M4 Max at best (in Lone Monk) is 5070 desktop class and at worst (in Scanlands) is 4060 desktop class. On average, I'd say in Blender, it is neck and neck with an RTX 4060Ti desktop card. I think a theoretical M5 Max should be on par with a 5070Ti if we see the same 60% bump in RT performance.",hardware,2025-09-18 03:56:47,3
Intel,nerhwya,Apparently Cinebench 2024 GPU is not compatible with RDNA4 cards lol. So I can't find any scores to compare.,hardware,2025-09-17 19:55:07,1
Intel,newfwdv,"So does dynamic caching ensure that the total size will ""always"" be the same as whats being called? As in certain cases it is still possible that there can be wastage like for the example you said ""Eg a given shader might need at its peak 30 floating pointer registers. But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time."" on that, there would be 10 registers wasted doing nothing, if it cant find any else thats <10 registers to fit in that.",hardware,2025-09-18 15:14:05,2
Intel,ngnf1x5,Yeah you're right. Should've said branchy or low occupancy code.  Combining such a HW side change with change on SW side with GPU work graphs API could indeed open up many usecases and new possibilities that are well beyond the current way of doing things. I can't wait to see what Apple does when Work Graphs arrives in a future Metal release.,hardware,2025-09-28 14:00:56,2
Intel,nesav03,"Well when you put it like that (13-14% behind in efficiency) saying ""generations behind"" certainly sounds misleading.  All it would take for another arm vendor to beat that is jumping one node(let) earlier than Apple, which is certainly doable given the lower volumes, although traditionally it rarely happens. Or jumping earlier to anything that might give them an advantage really, e.g. lpddr6.",hardware,2025-09-17 22:18:15,4
Intel,nesu2qs,"It's not ""generation**s**"" behind as you originally wrote. It's being compared to cores from **a year ago** already, mate.  I fully expect MediaTek to adopt C1-Pro and Qualcomm for sure will also update.  Apple's E-cores are simply nowhere near as dominant as they used to be in the A55 era.  EDIT: before we speculate more using marketing for Arm and hard results for Apple, let's check back in a few months to see how C1-Pro  *actually* performs and how Qualcomm's smaller cores actually perform.",hardware,2025-09-18 00:07:30,2
Intel,netwip5,Its a bit harder since we don't have any A19 Pro die shots yet. But Apple's E cores have always been sub 1mm2 compared to A7xx cores.,hardware,2025-09-18 04:03:25,2
Intel,nes5hmu,Yeah. All fair points. I don’t disagree. It’s still fun to speculate!,hardware,2025-09-17 21:48:58,3
Intel,nes2dlk,"Because we don’t have the tools. On macOS Apple provides powermetrics but Apple states that the figures can’t necessarily be relied on. On some very specific tests you can narrow down power to a cpu core, kinda. Spec tests often stress other aspects like memory, so I would use the provided figures as a guide. Either as a “p core bad” or “p core good” conclusion.",hardware,2025-09-17 21:32:48,1
Intel,nes1r2v,"Oh for sure. It’s not a criticism of either Geekerwan or yourself. They are doing a great job with the available options and I appreciate your summary. I just find it a little amusing when people dissect milliwatt differences as absolutely accurate. We just don’t have the tools, and people are keen to jump on the “p core doomed” bandwagon.",hardware,2025-09-17 21:29:35,1
Intel,nes2xxk,"The power increaseas are definitively (and definitely) accelerating, though, worse than it used to be.  [**Geekerwan P-core power**](https://youtu.be/iSCTlB1dhO0?t=422) **| SPECint2017 floating point**  *% is from the previous generation*  A14: 5.54W  A15: 5.54W (+0% power YoY)  A16: 6.06W (+9% power YoY)  A17 Pro: 6.74W (+11% power YoY)  A18 Pro: 8.18 W (+21% power YoY)  A19 Pro: 10.07 W (+23% power YoY)  //  AnandTech did the *great* work of measuring energy consumption / joules. That really proved that race to idle was working; more instanteous power under load, less overall energy   [**AnandTech P-core Power | SPECint2017 floating point**](https://web.archive.org/web/20240907062349/https://www.anandtech.com/Show/Index/16983?cPage=15&all=False&sort=0&page=2&slug=the-apple-a15-soc-performance-review-faster-more-efficient)  A14: 4.72W, 6,753 joules  A15: 4.77W, 6,043 joules (+1% power YoY, -11% energy YoY)  Average power went up, but energy consumption went down.",hardware,2025-09-17 21:35:43,2
Intel,nfcjm81,You copied and pasted someone else's data and now you're acting like a hero,hardware,2025-09-21 01:44:49,-1
Intel,net2bpw,"Still very bad used to work in their IO team, and the idle eatt performance is still very bad. Their idle core loading is also not as good as I would expect for their next generation, and its sad to see the future generation lose to even on M3 on benchmarks. The LP core was supposed to help in this situation, but I just can't see past the fact on why the gap is actually widening over time. I hope Intel can come in and close this gap.",hardware,2025-09-18 00:55:13,3
Intel,neru5qx,"The 9950x lags by 8% behind the M4. M5 is another 10% on top of this lead. M series chips use around 8W of power in total to achieve this perf including memory and what not. 9950x is like 20W per core, another 40W for the I/O and another unknown amount for mempry.",hardware,2025-09-17 20:52:33,4
Intel,nf8k821,I was comparing overall score.  Geekerwan got 4.17 at 2.58GHz which is ~1.62pt/GHz which is a little higher than your speculation and a mere 1.8% IPC difference from that Golden Cove number.  The fact that it did this at 0.64w in such a tiny core is absolutely incredible. It once again raises the question about how much ISA matters. I don't see any x86 E/C-cores getting anywhere close to those numbers.  https://youtu.be/Y9SwluJ9qPI?t=260,hardware,2025-09-20 12:42:15,1
Intel,neu5yj1,one will think the massive vram capacity just override the disadvantages.,hardware,2025-09-18 05:17:08,1
Intel,ney6vfr,"dynamic caching would let more copies of the shader run given that is knows the chances that every copy hits that point were it needs 30 registers is very low.   If that happens then one of those threads is then stalled but the other thing it can do is dynamicly at runtime convert cache, and thread local memroy to registers and vice versa.   So what will happen first is some data will be evicted from cache and those bits will be used as registers.   maybe that shader has a typical width of just 5 registers and only in some strange edge case goes all the way up to 30.   With a width of 5 it can run 20 copies on a GPU core that has a peak 100 registers.",hardware,2025-09-18 20:12:58,4
Intel,ngpzxjn,"Metal has supported GPU side dispatch for a long time, (long before work graphs were a thing in DX) Barries and fences in metal are used by the GPU to create a dependency graph between passes and this is resolved GPU side (not CPU side).   I don't see the explicit need for some extra feature here as we already have \*and have had for a long time since metal 2.1\*",hardware,2025-09-28 21:29:23,2
Intel,nevepm7,"Historically, the core only area for Apple's E cores have always been between Arm's A5xx & A7xx cores, but closer to Arm's A5xx  But for core+L2 (using sL2/4), Apple's E cores have always been similar to A7xx cores in mid config",hardware,2025-09-18 11:56:17,1
Intel,nes29nv,"I agree that the presence of inaccuracies is very likely. And I certainly don't think the P core is doomed for a 10% jump in what is essentially a a very minor node upgrade.  But considering the video does go into the P core's architecture where the only substantial changes were the size of the Reorder Buffer and a marginally better branch predictor, the performance numbers make sense.",hardware,2025-09-17 21:32:15,3
Intel,netbw6q,This is a combination of the meaningless smartphone benchmark game (95% of users would be perfectly fine with 6 of Apple's latest E-cores) and the need to have a powerful chip in laptops/desktops all sharing the same core design.,hardware,2025-09-18 01:50:19,0
Intel,nfckcjc,"I'm confused. And legitimately concerned about your mental faculties.  Literally in your previous comment, you posted Tom's Guide data, data thats not ""yours"" to try and discredit my post. And checking your post history, you also posted a Mrwhosetheboss video to discuss battery life comparison in an another subReddit.   So are you a hypocrite since you're ""stealing"" data as well? I'm not stealing anyone's data. I'm correcting your stupidly incorrect conclusion with a source to back it up. Just like you attempted to lol.",hardware,2025-09-21 01:49:24,2
Intel,neuiclx,"You see 7-8W increase going from 1 Core to 2 Core, which indicates that AMD has a huge base overhead but the core doesn't use 20W.   So its a problem with the SoC design not with the CPU design that it uses 20W for ST.  Its more like running a ST workload draws 20W SoC + 8W CPU stuff. So if they had better SoC design they could substantially boost efficiency even with the same cores.",hardware,2025-09-18 07:09:23,2
Intel,nescl7s,"Comparing the 9950X to the M4/M5 is a bit of a stretch... I'm not saying AMD is as good, but if they did a ""Lunar Lake"" with Zen 5C + on package memory they wouldn't really be that far off.  I want x86 to belong in the museum too, but sadly the ISA doesn't really matter (that much) and AMD isn't exactly incompetent... EPYC CPUs are still dominant and this is what they're truly targeting...",hardware,2025-09-17 22:28:00,1
Intel,nf8puxr,"Oh I was referring to this statement in your previous comment.  >Their E-core is faster than something like Tiger lake per clock.  By ""per clock"" I assumed you mentioned IPC.",hardware,2025-09-20 13:16:40,2
Intel,nfb2xd7,"I wouldn't recommend comparing IPC from two different runs of SPEC2017. There could be numerous different changes or subtests we don't know about. For eg, compare IPC numbers here vs the test used in A18 review. There are some notable differences in scores and IPC.",hardware,2025-09-20 20:35:11,1
Intel,neu6scx,Oh it's a huge benefit (I have a M2 Ultra at work) but we still use Nvidia. The cuda ecosystem is far more mature and widely supported with better support for embedded and datacenter scale compute.,hardware,2025-09-18 05:24:15,5
Intel,nf3x91p,"I see, so dynamic caching can make it so a shader doesnt have to be 30 registers wide if it doesnt have to do 30 often so it doesnt have to reserve that much space and waste it(such as in conventional cases, if its 5 registers and 30 peak, it will still reserve 30 registers despite it being at 5, which then would waste 25 doing nothing)  Also SER happens first right?",hardware,2025-09-19 17:45:47,1
Intel,ngq1nfa,Impressive and thanks for enlightening me.,hardware,2025-09-28 21:38:03,1
Intel,nes2pt3,I don’t disagree. The performance figures seem good. The power figures may or may not be. I’m just nitpicking.  Edit: just noticed that they show the A19 having 99% P-core FP performance path 11% less power. That is weird and get’s to my point about power measurement strangeness.,hardware,2025-09-17 21:34:33,3
Intel,nete2h5,"The needlessly high clocks, agreed: Apple could've still improved perf with lower clocks.  >the need to have a powerful chip in laptops/desktops all sharing the same core design.  They previously kept this in check on the A16 and even A17 Pro, both sharing the same core designs as the M2 and M3 respectively. That doesn't seem too related, as every uArch should scale a few hundred MHz either down or up.",hardware,2025-09-18 02:02:53,1
Intel,nfckhxr,I posted links to reviews. I didn't copy the entire data for my own post,hardware,2025-09-21 01:50:21,0
Intel,net132z,"ISA matters much as well, or mostly the implementation of it. X86 and AMD dont go well together, and AMD is the very definition of incompetency. Both deserve to be sunseted by now. The fact that there is no proper ARM support on consumer platform is the only reason why X86 on consumer still exists. For servers, an ARM server is more power efficient, and only those really legacy stuff requires X86. Companies would really appreciate the cost savings and the ARM ecosystem more than the clusterfk of X86.",hardware,2025-09-18 00:48:07,2
Intel,nfb1vqa,"Yeah, I guess that wasn't clear. What I meant is that the IPC of the E-core is so much higher than Tiger Lake that it's performance at 2.6GHz is surprisingly close to desktop Tiger Lake at 5GHz.  I know it's not fair to compare with Intel 10nm (later Intel 7) with TSMC N3P as it's 2 major nodes apart, but this goes way beyond that because these chips are using only around 2.5w for just the E-cores. TSMC claimed 25-30% power savings from N7 to N5 and another 30-35% from N5 to N3.  Using these numbers, we get these 4 E-cores using somewhere around 6-7w PEAK, but this is all-core turbo. Intel's CPU at it's most efficient 12w configuration won't hit it's 4.4GHz turbo required to win in single-core performance and is going to hit closer to its 1.2GHz base clock in all-core sustained performance at which point the E-cores not only have their giant IPC advantage, but double the clockspeed too.  All this again to make the point that a macbook with this will be blazing fast for consumer workloads and might finally be the laptop to usher in multi-day battery life.",hardware,2025-09-20 20:29:44,1
Intel,nfbgkad,"This is true, but there aren't a ton of alternatives short of taking a ton of time to compile/run everything myself. I don't think it's worth it for a hypothetical IPC comparison that it sees nobody else would even be that interested in seeing.",hardware,2025-09-20 21:49:05,1
Intel,nfd97ff,"Reordering of shaders has a cost, if for a given martial you just hit 10 rays you will not want to dispatch that shader with just 10 instances as the cost of dispatch and scdulaing will be higher than just inlining the evaluation, so you will merge together the low frequency hits into a single wave were you then use branching/fuction point calls.    You will also use this mixed martial uber shader to use up all the dregs that do not fit within a SMD group.      Eg you might have 104 rays hit a martial but that martial shader can only fit 96 threads into a SIMD group so has 8 remaining thread, you don't want to just dispatch these on there own as that will have very poor occupancy (with 88 threads worth of compute ideal) so you instead inline them within a uber shader along with a load of other overflow.",hardware,2025-09-21 04:45:15,1
Intel,nfclcjn,"Are you perhaps blind? The youtube link to the review is at the top of the post? What is wrong with you friend? Feeling a bit under the weather?  The video is in Chinese and before I updated the post with a youtube link, the previous source was from Bilbili, a platform that doesn't even work in most countries.   So I summarised the important points in it for people who didn't understand Chinese and couldn't infer anything from the graphs. No one's gonna ignore the link that is at the top of the post and read my summary before seeing that a link is available.   You're fighting imaginary demons here. Go to your samsung subreddit and whine about Geekbench or something.",hardware,2025-09-21 01:55:46,2
Intel,nev2rja,"But we were discussing CPU uArches, and on the question of ""Does AMDs single core suck up 31W"" the answer is ""NO"", the SoC power is 31W, and that is an issue that can be solved independently of the CPU uArch.  This just means that the massive inefficiencies are not inherint to the CPU uArch, but are a Problem of SoC design.  7W on a single core isnt super efficient, but its far lower than 20-31W. The question is if AMD is able to strip away a large part of those ~20W overhead, but that is not contingent on questions about CPU uArches.  And thats the point of the 1 Core vs 2 Core comparison. To demonstrate that it isnt the CPU that sucks up 20W, but the SoC, which can be solved much differently.",hardware,2025-09-18 10:28:02,4
Intel,nfcmlir,Enjoy your scratch-prone iPhone!,hardware,2025-09-21 02:03:39,1
Intel,ng0a1u9,Your one brain cell is trying really hard. See if you can wake up its buddy.,hardware,2025-09-24 20:06:41,1
Intel,n133hqd,"Same price as B580 with lower performance, 4GB less vram and 128 bit bus.  A round of applause for Nvidia",hardware,2025-07-03 06:56:28,389
Intel,n14594r,"GB207 being slower than AD107 is pathetic, what's the point of these x07 dies again? They're not thst mich smaller than recent x06 dies.  They're spaffing design cost on these barely different dues.",hardware,2025-07-03 12:22:36,30
Intel,n133i3r,"Now that we have a third-party review, it pretty much confirms what Inno3D said the other day, it's definitively slower than the RTX 4060 by about 5-7%.",hardware,2025-07-03 06:56:35,39
Intel,n13b3g4,The RTX 5050 is 2.5% slower than the Arc B580.   It's also a 50 series card that costs $250.,hardware,2025-07-03 08:09:36,32
Intel,n146i3h,> the system used a Ryzen 7 9800X3D  If the B580 only wins by 2.6% with this CPU then it's going to lose when you use something weaker because of that CPU overhead problem with Intel.,hardware,2025-07-03 12:30:30,28
Intel,n13bh59,It's actually surprising that it's that close to a 4060 considering the 5050 only has 2 GPCs as opposed to the 4060's 3,hardware,2025-07-03 08:13:18,17
Intel,n14m3pz,IDK but that performance is actually not bad. It should've just been cheaper.,hardware,2025-07-03 13:58:15,11
Intel,n13fsw1,The only positive about this is they're likely going to make a 5040 with a cut down chip that should pretty easily fit <75W.,hardware,2025-07-03 08:55:13,8
Intel,n17bpa4,"Honestly for people who don't need a lot of GPU power, not the worse.   I have two work computers, one with a 4090 and an old one with a 3090Ti.  These GPUs sit idle just taking up space, would have made more sense to get the something less performant.",hardware,2025-07-03 21:51:56,3
Intel,n13ktwe,272mm\^2 for B580 vs \~150 or less for RTX 5050 and perf within 5% lmao,hardware,2025-07-03 09:44:20,11
Intel,n150h4y,"I think to lose in sales, it would have had to lose a little more convincingly.  Yes, it's worse. But not enough to make up for features and brand-loyalty / nvidias massive reputation with gamers who are not into tech.  This thing will sell. It will sell A LOT. Because Average-Kevin who just wants to play Fortnite and some League will have a great time with it. His YT videos will get upscaling, his shitty mic-quality will get fixed (mostly) by Nvidia Broadcast... and he DGAF why we think it's the wrong move.",hardware,2025-07-03 15:08:37,2
Intel,n1t7zvj,"Does it actually fall behind the b580 or is it due to vram bottleneck in certain instances?  Either way, this should open up people’s eyes about the b580. The b580 is just marginally above the 5050, which is a terrible product, with 4gb more vram… even at its 250 dollar price point, it’s never been a good deal. Not to mention how it’s going for 400 these days.",hardware,2025-07-07 14:18:35,2
Intel,n148e4b,"People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.  Edit: r/hardware is dumb CPU overhead is a thing and budget GPU buyers need to know which of these two cards is effected the most by it. Everyone already knows that you are going to be GPU limited on top tier CPU's its not valuable information for a budget card.",hardware,2025-07-03 12:42:05,8
Intel,n13o0yi,Looks like people already forgot that testing the B580 with a top end CPU gives unrealistic results for actual budget buyers.  Every trick in the book for a nvidia sucks article.,hardware,2025-07-03 10:13:39,7
Intel,n13vnqm,Don’t make Jensen sad,hardware,2025-07-03 11:16:41,2
Intel,n132z0y,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-07-03 06:51:39,1
Intel,n147qg1,Will it also fall behind them in the Steam hardware survey?,hardware,2025-07-03 12:38:08,1
Intel,n15wi0t,the 3060 is still a better value than this card bc its about same speed but 12gb vram and higher bus,hardware,2025-07-03 17:38:26,1
Intel,n16kpem,"Maybe a product for non-gamers who want/need multiple display setup, like me. Even then, if you bother to spend that amount of money for 5050, better add some more to get 5060, and if bother to get 5060 just better get 16GB one. Furthermore, you may feel why not add bit more to get 5070 series rather than 5060 16GB. That's how these things are priced.",hardware,2025-07-03 19:35:55,1
Intel,n1at07e,Value engineering to extract as much wealth as possible while unable to match performance conditions.,hardware,2025-07-04 13:00:03,1
Intel,nbicg0f,When you gimp the memory bus of course it is not as good,hardware,2025-08-30 15:52:42,1
Intel,n13okls,"No 75 watt gpu, pin connectior, yeah this is death of arrival.",hardware,2025-07-03 10:18:30,-2
Intel,n153l0f,"Trying to convince people to buy a 8gb card in 2025 should be illegal. Yet, people will line up to buy it because of the Nvidia logo...",hardware,2025-07-03 15:23:12,-2
Intel,n135r6a,"It's a Nvidia Rx 7600, actually good recommendation over that card if it released for the same price in India. Rx 7600 is awful compared to rtx 5050 in pretty much everything.",hardware,2025-07-03 07:17:43,-11
Intel,n163uxd,Humiliation for the RTX 5050 is well deserved. It's good this gets curb stomped by the Intel B580.  More market share for Intel because AMD also introduced a useless 8 GB card.,hardware,2025-07-03 18:13:13,-1
Intel,n1481lj,">128 bit bus  Seems like this isn't the main issue with 5050's performance. 4060 has lowest memory bandwidth out of all 3, yet it's the fastest card. The die is just too cut down.",hardware,2025-07-03 12:39:59,49
Intel,n137c11,Unfortunately it will still sell like hotcakes in pre-builts to people who don't know much about computers.,hardware,2025-07-03 07:32:56,93
Intel,n13dk9q,"Their brand name is so strong that despite being worse in basically every way, including having poor drivers which was one of their major selling points in the past, they will still sell super well",hardware,2025-07-03 08:33:33,29
Intel,n139o7n,"I just looked at PCPartPicker and the cheapest B580 on there, in the US, is $299 (and the brand is Onix). The Intel Limited Edition is $340. If you can get the 5050 at MSRP it would have a decent price advantage.  I mean, I wouldn't get a 5050 but in the US at least, the $249 price for the B580 isn't real.",hardware,2025-07-03 07:55:44,35
Intel,n147v98,"Where I live none of the main sellers stock the B580, a round of applause for Intel.  The B580 isn't faster on the CPU's owned by the people who are in the market of a budget GPU.  People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.",hardware,2025-07-03 12:38:56,8
Intel,n13ev3s,Intel apparently hasn't shipped any GPUs last quarter. The shortage is already been seen as prices of b580 are going up.,hardware,2025-07-03 08:46:08,6
Intel,n13nzct,They're really pushing what they can get away with these days. Less VRAM and narrower bus but want the same money... bold strategy.,hardware,2025-07-03 10:13:15,1
Intel,n169a4v,"> with lower performance,  even on lower end cpu? Holy crap nvidia wtf",hardware,2025-07-03 18:39:52,1
Intel,n1521le,Because it will sell a shit load. Probably will be one of the highest selling Nvidia GPUs this generation,hardware,2025-07-03 15:15:58,17
Intel,n1sr0i1,"At these small die sizes, yields can be really high. High enough that if they wanted to have supply for a 5050, they might not have enough GB206 dies that can be cut down to 20/36 SMs.  The 5060M is probably catching the bulk of the bottom-level GB206 dies at 26/36.  There's also the memory difference, with the 5050 having gddr6 while the rest of the lineup uses gddr7. With dies this small, that cost difference in memory could actually make it worth taping out a new chip with controllers for the cheaper stuff. Gddr6 is dirt cheap at this point.  You could absolutely then argue that there's no reason for the 5050 to be so cut down that it needs a new, even smaller die, and that Nvidia can eat some margins on a budget card to give it better memory. And I'd agree. But I'm sadly not anybody who can make that decision.  Personally, I can't wait to see what becomes of the cut-down GB207s. The 5050 is a ""golden"" fully-enabled die. There will be some with only 18, or 16, or even fewer working SMs. Hell it could lose an entire GPC and be down to half-working. Those will be some absolutely sad little GPUs.",hardware,2025-07-07 12:41:55,3
Intel,n13lkxr,> It's also a 50 series card that costs $250.  I'll believe that when I see it in stock for 250. MSRP is meaningless if the product isn't available for that price.,hardware,2025-07-03 09:51:23,30
Intel,n13m4a0,And will be even worse since it's vram is 8gb. In many many games the b580 would Perform even better,hardware,2025-07-03 09:56:22,1
Intel,n14glix,Yeah it would be interesting to test with a more real-world CPU pairing.,hardware,2025-07-03 13:29:04,13
Intel,n151kyi,"Yeah, that was my question, did that ever get fixed? It seems like nobody remembers that anymore",hardware,2025-07-03 15:13:49,5
Intel,n152cbk,"TBH, they should have made THIS card a 75W SKU. It probably easily can be, without losing much performance. That would make it a very viable upgrade path for low-end systems. And kind of make the price more attractive, because it would require zero additional changes.",hardware,2025-07-03 15:17:22,7
Intel,n13qh8j,Margins go crazy,hardware,2025-07-03 10:35:09,18
Intel,n15brqh,4x the performance with MFG though.,hardware,2025-07-03 16:01:21,-1
Intel,n14yvrm,In mkst cases comparison would make sense since you testing gpu's not anything else. But when you toss in b580 complaints about using too powerfull cpu make much more sense,hardware,2025-07-03 15:01:03,1
Intel,n15bkf0,I have a system with a 9950x3d and a 1060. I’m thinking of getting a 5050 to go with it.,hardware,2025-07-03 16:00:24,-1
Intel,n13wkrr,> Every trick in the book for a nvidia sucks article.  That's just a standardised testing suite.   Someone will have low-end CPU testing once this gets any distribution.,hardware,2025-07-03 11:23:28,6
Intel,n13qrff,They're using the same cpu for the 5050 too. What a weird comment to make.,hardware,2025-07-03 10:37:36,2
Intel,n1t9745,"Don’t forget how everyone is shitting on the 5050, calling it the worst nvidia card ever, but were praising intel non stop for the b580.  Even using purely stats from the article, b580 is 3% better with 4gb more VRAM than “worst gpu ever”, so I don’t see how this translates to a win for the b580. Being better than a terrible product by a hair doesn’t exactly make it good.",hardware,2025-07-07 14:24:40,1
Intel,n13s2zk,Same can be said for the 5050 no? Both have CPU overheads unlike AMD which any mid-range AMD GPU can easily outperform Nvidia's 5090 at 1080p.   The 12GB VRAM however are better suited for people who are stuck at older Gen3/Gen4 PCIE speeds.,hardware,2025-07-03 10:48:35,-3
Intel,n18avh8,"It'll run nearly every game in existence at 1080p with playable framerates with the right settings. IDK why it would be a product for non-gamers. Calling it that means the B580, 4060 and 7600 are products for non-gamers, too.  Only game I could think of it wouldn't run properly would be oblivion remastered, which has terrible performance on anything.",hardware,2025-07-04 01:18:02,2
Intel,n14lclc,3050 released with the exact same specs and msrp. The 70w variant only came out 2 years after. The first variant was in top 10 of steam's hardware chart.,hardware,2025-07-03 13:54:24,9
Intel,n17ei31,"This is not a card for 1440p, as it would have issues with getting even 60 fps there, and in 1080p 8gb is mostly enough.  There is a difference between choking on VRAM with 80 base fps (5060ti) and 50 base fps (5050). Yes, second case is still annoying, but you might want to dial down the settings anyway, and then you would be fine.",hardware,2025-07-03 22:07:05,4
Intel,n139moa,Nvidia RX7600? That a new GPU? 😂,hardware,2025-07-03 07:55:19,0
Intel,n18a77s,"Funnily enough the 8gb 9060 XT is the best brand-new alternative at $300 and below, hands-down, and that includes the B580. It's x16 interface means it's less restricted by the vram compared to the 5060/Ti 8gb. The B580 still has overhead issues, VR issues, game issues and is rarely available at MSRP.  But people trash it anyway and praise the B580 without actually looking at its performance. In the benchmark linked by the OP alone it loses to the 4060 lol",hardware,2025-07-04 01:13:47,1
Intel,n14h57q,"Performance is fine I'm sure it'll perform just as good as the 3060 maybe slightly if you lower the power to 75W, call it a 5030 and sell it for $150. But these are different times and Nvidia doesn't do reasonable things like that anymore.  Bottom line, it's just the price and TDP that sucks.",hardware,2025-07-03 13:32:03,0
Intel,n13vxvy,TBF a pre-built you're just looking at:   * What's the price?  * Does it have a graphics card or not?  * Overall package/size,hardware,2025-07-03 11:18:47,21
Intel,n15185y,"or even people who have a baseline knowledge, but think ""dlss and framegen"" will make this abomination better than its competition",hardware,2025-07-03 15:12:09,1
Intel,n14gfbw,"It's not so much the brand name but their near monopoly in the prebuilt market. There will be 1 prebuilt with a 9060XT 8 or 16GB for every 20 prebuilts with a 5050/5060. It's so bad that even the prebuilts with the 16GB 9060XT will be outsold by prebuilts with a 5050 at the same price range, as companies like to pair trash GPUs with ""high"" end CPUs like a 13700F to clear stock but the ""i7"" allows them to mark up the price.",hardware,2025-07-03 13:28:07,15
Intel,n13pika,"It was in stock at newegg monday for 3+ hours at $249.99, I picked one up after seeing a post that was 3 hours old on /r/buildapcsales   I've stopped into microcenter a couple of times and they said they get them, but I just haven't been lucky enough to be there when they were restocked.  EDIT: it is back in stock as of 11:40 EST at Newegg",hardware,2025-07-03 10:26:45,28
Intel,n13wagd,"In Germany you can find Arc B580 for €269-283 low-end, and B570 for €218-235.",hardware,2025-07-03 11:21:22,9
Intel,n13m7wm,Her in Canada they are $360 CAD. Cheap cheap. And ways to find. Might be a issue in your country perhaps,hardware,2025-07-03 09:57:18,7
Intel,n13rke5,"Went on Shopee and the lowest I could find is $280, which is not bad considering there's usually taxes which increases prices of GPUs by 10-20% from global USD MSRP.",hardware,2025-07-03 10:44:24,3
Intel,n14na3u,">the $249 price for the B580 isn't real.  Yes it is, it just gets sold out as soon as it's restocked - https://www.nowinstock.net/computers/videocards/intel/arcb580/",hardware,2025-07-03 14:04:18,2
Intel,n14edfd,Doesn't it just need a 7600?,hardware,2025-07-03 13:16:41,4
Intel,n13m638,Not in Canada. Prices are exactly as launch. Could be your countries problem,hardware,2025-07-03 09:56:50,14
Intel,n146tdx,"Eh, they recently shipped a new batch at MSRP to Newegg.  Don’t know what’s going on with the partner cards. Probably the typical shenanigans",hardware,2025-07-03 12:32:28,2
Intel,n14t8qw,You forgot about the smaller dies.,hardware,2025-07-03 14:33:57,0
Intel,n151f6c,"The chip is tiny, so a single wafer -> more chips. The demands on power delivery, cooling etc. are nothing to speak of. And the GDDR6 memory is cheap and readily available.  If anything will ever be in stock, it's this card.",hardware,2025-07-03 15:13:04,8
Intel,n1krj48,5060 has been in stock at MSRP everyday since launch. This should be the same. Even the 5060ti 8gb and 5070 are at MSRP.,hardware,2025-07-06 03:12:56,4
Intel,n144vvx,MSRP has always been meaningless because it stand for suggested retail price.,hardware,2025-07-03 12:20:16,3
Intel,n151x7q,"But if the recent LTT video is halfway accurate, AMD would murder both of them in that scenario. The 9060 XT 8GB might want to have a word with those two cards.",hardware,2025-07-03 15:15:24,-1
Intel,n14n60t,negative margins for intel,hardware,2025-07-03 14:03:43,8
Intel,n15ha0f,The B580 showed we need to test in the systems the cards will end up being used in. The restricted PCIe lines on the lower tier cards i.e. only use 8 also means they need to be tested on older systems.,hardware,2025-07-03 16:27:40,7
Intel,n15gzg8,You really believe you are an average 9950x3D owner? Really?,hardware,2025-07-03 16:26:15,4
Intel,n19kdp4,"Out of curiosity, what is your use case in such a setup? I find myself cpu bottlenecked quite often on a 7800x3D but i use a 4070S, when i used 1070 GPU was bottlenecking me.",hardware,2025-07-04 06:45:36,1
Intel,n14a1ho,"But Intel Arc drivers have a really bad overhead, while nvidia does not, so in gpu bound games the 5050 will have around the same performance but the b580 will lose a lot of it, so it's not a weird comment, it's a very important point.",hardware,2025-07-03 12:51:45,26
Intel,n188xgl,>in 1080p 8gb is mostly enough.  The narrative was pushed so much that this fact became an unpopular opinion.,hardware,2025-07-04 01:05:47,4
Intel,n13aat8,nvidia Rx 7600 equivalent or Rx 7600 alternative by nvidia. Ig people can't even get a silly joke nowadays,hardware,2025-07-03 08:01:46,-9
Intel,n18u6nu,The 12 GB of RAM in the B580 will age more gracefully for a truly low end gamers. On purpose of 8 gb is retro gaming.,hardware,2025-07-04 03:21:07,0
Intel,n15lctf,"Hate to be that guy but to call this a 30-class card is insane. The 1030 was 25% the performance of the 1060, and the 1630 was around 45% the performance of the 1660. This card is 80% the performance of a 5060, which should probably be called the 5050.  To take it a step further, the GT 1030 was 13% of the 1080ti, the 5050 is 20% of the 5090. The 1050 was 25% of the 1080ti, so it would be reasonable to call this a 5040 or I could even be satisfied with 5040ti.",hardware,2025-07-03 16:47:05,30
Intel,n15k9s6,That would be an interesting card,hardware,2025-07-03 16:42:01,1
Intel,n14itr9,I think a big thing about prebuilts are the aesthetics. Does it look good? is the cables managed? Is the RGB controllable?  Slapping a PC together is easy but building something that looks beautiful through the side panel is not as easy.,hardware,2025-07-03 13:41:11,6
Intel,n1kqz69,The 4060 is 2% and DLSS did plenty of work for the even slower 3060. DLSS is going to be great for this card. Frame gen will be good in games this card can run at good base framerates.,hardware,2025-07-06 03:08:56,2
Intel,n15e4ii,"cool, it's in stock now",hardware,2025-07-03 16:12:35,1
Intel,n16ve1o,A 2-year old CPU ushering a new platform at the time of B580's launch. Far from a budget user's upgrade timeline-wise.,hardware,2025-07-03 20:28:40,5
Intel,n1881kg,"For a brand-new truly budget build, AM5 is still restrictive pricewise to people eyeing GPUs of this level. The 3050 and 6600 were well-known inclusions in $500 builds, building with a 7500F is going to cost $280-300 just for the cpu, mb and ram alone. AM4 or LGA1700 otoh can be as low as $180-200.",hardware,2025-07-04 01:00:16,2
Intel,n17vhk5,Depends on the game  The 7600 can see some serious performance deficits in some games like Spider Man  https://youtu.be/00GmwHIJuJY&t=521  and Hogwarts' minimum FPS is affected  https://youtu.be/00GmwHIJuJY&t=468,hardware,2025-07-03 23:42:58,1
Intel,n1454en,I mean tariffs in the US are a thing...,hardware,2025-07-03 12:21:46,5
Intel,n1595nj,"It wasn't always meaningless. Overall deviation from MSRP used to be smaller, and it wasn't hard to find the basic models at MSRP once upon a time.",hardware,2025-07-03 15:49:09,9
Intel,n15k42w,more like we need to test with different processors. Like with high-end to see absolute maximum and some lower grade to see how good/bad driver overhead is,hardware,2025-07-03 16:41:16,1
Intel,n16tjab,I never said I was,hardware,2025-07-03 20:19:36,2
Intel,n14c896,"I'm looking into this now, and you are correct. I am just reading this for the first time.",hardware,2025-07-03 13:04:21,7
Intel,n195igm,Lol no. For truly low-end gamers the B580 will perform worse because of its CPU overhead that still hasn't been fixed more than half a year after release.,hardware,2025-07-04 04:42:40,6
Intel,n4q5tdd,"I mean not really. The b580 is low end enough to the point where it'll become obsolete in performance long before it runs out of VRAM.  It's the equivalent of saying buying a 16gb 7600xt in 2025 is ""more futureproof"" than buying a 12gb 3060 in 2025.",hardware,2025-07-23 15:14:46,1
Intel,n53yfmn,an 8 GB card is obsolete at day one for modern games.,hardware,2025-07-25 16:12:47,1
Intel,n15s14a,"It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup. You can find those numbers on Gamer's nexus latest shrinkflation video on this topic (https://www.youtube.com/watch?v=caU0RG0mNHg). Considering those things, the 5060 is essentially a 5050 and the 5050 is a 5030.  The GT 730 had 13.3% as many cores as the full GK110 die, with a 50W TDP.  The GT 1030 had around 10.3% as many cores as the full GP102 die, with a 30W TDP though this one was impressive.  The GXT 1630 had around 11% as many cores as full TU102 die, with exactly 75W TDP.  The RTX 5050 has 10.7% as many cores as the full GB202 die (RTX PRO 6000 Blackwell), and around 11.7% vs the 5090 (which is the number you'll se on Gamer's Nexus video, he rounds it up to 12%, I think that was a mistake on his part? I'm not gonna try and correct him though), but it has a TDP of 130W somehow, just 15W less than the 5060, that's absurd.  Hence why I'm saying it SHOULD be a sub 75W TDP card, and priced way below that. My theory is that it was OC'ed past its efficiency curve to make it at least moderately better than the 3060 and still be able to call it a XX50 card.",hardware,2025-07-03 17:17:52,1
Intel,n9995zt,Nah most people who buy prebuilts just want a good working PC with minimum effort,hardware,2025-08-17 22:53:54,1
Intel,n176oya,Yep. That's a problem for that country. Countries with normal leadership have much more viable options,hardware,2025-07-03 21:25:30,6
Intel,n17ibhv,Price is only driven by demand and supply. See how the crazy expensive GPU still sell like a hot cake during the COVID lockdown period?,hardware,2025-07-03 22:28:19,1
Intel,n14d3md,It's well known by Arc owners.,hardware,2025-07-03 13:09:23,11
Intel,n53mmej,"The 16gb 7600xt  is a giant pile of crap even with 16 GB , so a 12 GB 3060 is the better choice.  If you go to techpowerup you'll see that the b580 is 17% in relative performance than the ""12 GB 3060 """,hardware,2025-07-25 15:18:15,1
Intel,n84aj5f,What if you’re playing at 720p?,hardware,2025-08-11 14:46:01,1
Intel,n17rxfu,"> It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup.  That is a absolute dog shit methodology to use.   The 1080 Ti was 471 mm², 5090 is 750 mm².  Those two products are not the same tier. And can therefor not be used as equal reference points. When it comes to die size the top Pascal card is closer to the 5080 than 5090.",hardware,2025-07-03 23:22:30,10
Intel,n53r1fq,The point is more VRAM =/= aging better. The b580 having 12gb isn't saving it from becoming obsolete in a few years.,hardware,2025-07-25 15:38:30,1
Intel,n1glwbn,"They re not the same price tier either bro... They could have called the 5090 a 5095ti if that makes you happier.  The guy you quoted is right.   Also comparing the performance diff between the 1030 vs 1060 because guess what the 5060 should have been the 5050 looking at die size and memory bus.   Not sure why people keep defending these naming schemes, do you think the engineers use them internally lol? Its bullshit that marketing comes up with",hardware,2025-07-05 12:34:50,2
Intel,n17vpx6,"Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍 Doesn't matter if die sizes get bigger, that's up to Nvidia. Just because the halo product got bigger doesn't mean they can get away with moving their product stack (below the XX90) one tier up in prices.   I guess it is a dog shit methodology to use, only Gamer's Nexus uses it and I'm sure #1 Nvidia apologist u/Alive_Worth_2032 knows more than him.   It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree.  And it's a shit methodology to use if you're an Nvidia executive.  If you can't see that Nvidia is putting all the effort into binning high end chips for AI cause it's 90% of their income, then I don't know what to tell you. Obviously it's a good move for them, and it works, but there's 0 reason for a gaming customer to defend them for it. We used to get much more out of their chips in the gaming cards for more reasonable prices, why's it wrong to want that?   You can't just draw a conclusion based on relative performance between 2 underpowered products, you compare them to the ""best"" Nvidia COULD give us (which is the full top die for each generation) and go down from there.  Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm^2. That's not even an excuse.",hardware,2025-07-03 23:44:19,-1
Intel,n180tf6,"> It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree. And it's a shit methodology to use if you're an Nvidia executive.  You can chose one methodology.   You can decide on comparing what hardware you get at a certain price point.   Or you can ignore price/bom and look only at arbitrary model numbers as if they mean something.  You cannot do both at the same time.  Personally I prefer to look at die area. The 5050 today is roughly comparable to the 1050 Ti, comparing it to the top cards that are not comparable is irrelevant.  Pascal did not have a analogue for the 5090, period.  >Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍  That changes nothing. The Pascal Titan are using the same die as the 1080 Ti. The whole tier of die that is used in today's consumer top SKUs and for the RTX 6000 Blackwell, DID NOT EXIST back then.  >Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm2. That's not even an excuse.  What are you even trying to say? Ampere also did not have a analogue to the 5090. The 3090 Ti, even it is sitting almost a tier below the 5090. The 2080 Ti and 5090 are unmatched in other generations.",hardware,2025-07-04 00:15:04,10
Intel,n18i4wk,"You’re still missing the point by focusing purely on die area and pretending model numbers are arbitrary. Model numbers mean something, nvidia knows that so that’s why they kept them consistent for so long (unlike AMD that's hella inconsistent). Because they rely on the perception of tier consistency from generation to generation, even if they’ve worsened the specs behind the scenes.  You say you “prefer to look at die area,” but that’s irrelevant unless you’re building the chips yourself. Customers don’t game on silicon real estate they game on actual performance and hardware capabilities. And the cuda core count + bandwidth vs top-die approach directly shows how much nvidia is offering relative to what they could offer if they weren't prioritizing AI margins.  So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  > Pascal did not have a 5090 analogue  That’s just semantics. Every generation has had a full-fat top die, whether branded as “Titan,” “RTX 6000,” or whatever, which where I drew the comparison. Comparing lower-tier cards to the top die is how you reveal how much of the architecture's potential is being offered to gamers.  And yes, 3090 and 3090 Ti still preserved proportionally higher specs vs top-tier dies. What changed now is not just die sizes, it’s NVIDIA reserving most of the silicon for AI and throwing scraps to gaming.  > You can choose one methodology...  Sure, and I chose one: compare the lowest-tier GPU to the top die, which accurately shows how nvidia has been shrinkflating consumer value over time. You're welcome to look at BOM and TDP too the 5050 still loses. It should be sub 75W and priced accordingly.  I think it's okay to call it an XX40 series card even if we keep the TDP at 75W but the XX30 series would need to disappear. What I don't agree with is using the die area excuse to justify shrinkflation just because the 5090 is such a halo product.",hardware,2025-07-04 02:03:25,1
Intel,n18jqvu,"> So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  Do you even listen to your own madness?  You realize that what you are saying. Is that the lower end 5000 series would be ""better"". If Nvidia removed the 5090 entirely. And renamed the 5080 the 5080 Ti, and made the 5070 Ti into the new 5080.  Suddenly, you would praise the lower end cards for being ""better"". Purely because the high end is less powerful. Nothing else changed, no one is getting more hardware for their money.  Jesus Christ the mental gymnastics you people go trough.   The hardware Nvidia gives you for your money, is all that matters. How large the top die is and what core configuration it has. Is irrelevant for the value for cards further down the stack.",hardware,2025-07-04 02:13:15,9
Intel,mxmawxn,End of Life is not the correct term for this. End of manufacturing is,hardware,2025-06-13 19:40:01,130
Intel,mxm2d1j,Was about to shit on Intel for such a terrible product lifecycle time and how its GPU division was not going to do well if a GPU only has a ~2 years of updates until I read the article...,hardware,2025-06-13 18:57:57,114
Intel,mxmb1bk,"End of life typically means end of support, not end of manufacture, or am I wrong?  Anyhoo I blame the article for bad wording",hardware,2025-06-13 19:40:37,27
Intel,mxm07wf,"> This announcement marks the beginning of the end for a model that arrived just two and a half years ago, and it offers partners a clear timetable for winding down orders and shipments. Customers should mark June 27, 2025, as their final opportunity to submit discontinuance orders for the Arc A750.",hardware,2025-06-13 18:47:32,14
Intel,mxmcw6w,"looks like Intel didnt fire enough of its incompetent staff. EOL usually means end of support/drivers/Developmemt.  [Intel® Arc™ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",hardware,2025-06-13 19:49:53,8
Intel,mxwxs3r,"Why did anyone buy these garbage Arc cards to begin with? The performance/$ was never ever ever ever ever not even for a single second better than comparable Nvidia and AMD cards.  I've never understood the point of the Arc cards. It was like a company in 2020 saying ""we're officially into console gaming and just created the PlayStation 2 for only $399!""",hardware,2025-06-15 14:35:14,2
Intel,mxsyrub,But did it really even live to begin with? 🤔,hardware,2025-06-14 21:08:51,1
Intel,mxm95g9,is this the shortest life cycle of recent GPUs? AMD was notorious for that... wasn't expecting Intel to top that....,hardware,2025-06-13 19:31:14,-8
Intel,mxpuga3,"So, for the distribution process this is also known as End-of-Sale. Since this is a B2C business, they can't really control when their resellers reach the end of their stock, but at that point Intel has stopped selling the product.  EoM and EoL are both dates that usually do not coincide with the End of Sales.",hardware,2025-06-14 10:09:38,10
Intel,mxmbvmp,"The article is poorly written, and the headline misleading.   The card is discontinued meaning no more orders will be accepted. It says nothing about software support.   Admittedly the miscommunication is Intel's fault because they specifically use EOL in their notification, but I also put this somewhat on the article writers because they didn't do a good job of clarifying that intel meant discontinued. They could have used more appropriate wording in the headline but instead chose to follow intel's lead likely knowing it would sow confusion, but lead to more clicks.",hardware,2025-06-13 19:44:48,116
Intel,mxm3esb,"Least likely reddit user behavior, actually reading the article probably puts you in the top 5 :)",hardware,2025-06-13 19:03:03,50
Intel,mxml6tk,"Don't worry, Iris Xe GPUs are still ""supported by the driver"", but their last fix was in 2023.  Intel doesn't notify when an architecture is actually dead, you're just left stranded for years until they make it finally official.",hardware,2025-06-13 20:31:06,13
Intel,mxmdlmm,"The incompetence source is actually intel themselves ...  [Intel® Arc™ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",hardware,2025-06-13 19:53:22,23
Intel,mxn4bgv,> Anyhoo I blame the article for bad wording   Intel chose the wording in its own notice.,hardware,2025-06-13 22:09:46,9
Intel,nbpqm8r,"Bruh did you even read the article?  Btw it costs half of a 3060 and it outperforms it, no idea why you are saying it is a garbage product lmao shitty redditors don't know the difference between end of manufacturing and end of life/service.  The article is garbage, but it was talking about the end of manufacturing, not the end of the service/life, meaning it will still het driver support.",hardware,2025-08-31 19:30:36,1
Intel,mxm9ioh,"No because they are still supporting it, just ending manufacturing of new cards.",hardware,2025-06-13 19:33:05,23
Intel,mxn1rnm,"For a website that's predominantly text-based, a shocking amount of its users can't read for shit",hardware,2025-06-13 21:55:47,11
Intel,mxmkvoz,"The job of a journalist is to provide the translation and context for their readers, not copypasta and regurgitate headlines that laypeople will immediately misunderstand.",hardware,2025-06-13 20:29:35,30
Intel,mxmj7un,That's just for the hardware manufacturing.,hardware,2025-06-13 20:21:23,17
Intel,mxmaq6o,"ah, i thought it meant end of driver support.",hardware,2025-06-13 19:39:05,6
Intel,mxmevd8,We all did. unconventional use of EOL,hardware,2025-06-13 19:59:39,13
Intel,mxnw8ja,"Unfortunately it’s been like that for decades with Intel, I can count numerous times in the past decade we’ve had this same conversation about their processor lines being EOL.",hardware,2025-06-14 00:53:32,4
Intel,mxn4nhz,End of Sale,hardware,2025-06-13 22:11:38,5
Intel,mzf0gr9,"Tbf arc pro battlemage gpus are supposedly going to be fairly cheap, so its not that out of reach for consumers",hardware,2025-06-23 23:03:20,54
Intel,mzfz70f,"Imo, the lockdown of virtualization started before AI, as Nvidia and AMD didn't want people buying cheap GPUs to virtualize corporate environments.      At least that's the context that I'm most familiar with for this. A lot of small to midsize companies barely need the GPU for their individual VMs so being able to split a single GPU against a beefy CPU would be a good cost saver to avoid the professional GPU tax.",hardware,2025-06-24 02:23:09,32
Intel,mzf114m,"Intel has said that SR-IOV will come to consumer. Even if it doesn’t, their GPUs aren’t super expensive.",hardware,2025-06-23 23:06:27,30
Intel,mzg3xoe,"Splitting a GPU is so that you can run multiple VMs with each GPU. Any AI training and any inference on LLMs or similarly large models wants to run *one* application across many GPUs. There's nothing in common between those use cases. The market for GPU virtualization for remote desktop, etc. kind of stuff is still there, but it never was a big chunk of the datacenter GPU market and still isn't.",hardware,2025-06-24 02:51:39,19
Intel,mzf8eno,"Yep, I have to use Intel for my virtualization server because it's impossible to get anything to work the way I want on more powerful AMD APUs. I can easily have a VM using part of the iGPU while the host shares it.  Also kind of sad that a pretty old Intel iGPU still has more video transcoding features in QuickSync like HDR tone mapping and better stability than modern APUs.    I don't think it has anything to do with AI it's been like this  forever.  AMD is nice consumer hardware that they just don't provide the tools to use in any advanced capacity.",hardware,2025-06-23 23:46:58,9
Intel,mzezrml,"i thought nvidia grid got semi replaced by MIG. then again, MIG is only supported only on a specific subset of datacentre cards.",hardware,2025-06-23 22:59:26,5
Intel,mzg00hx,"Hyper-V supports GPU partitioning without the need for SR-IOV, which is almost perfect on nvidia cards (at least for what I need). It doesn't support VRAM partitioning though, so every VM has full access to all of the VRAM. It also disables checkpoints for the VM (or at least says it does, apparently there's a way around this), and doesn't support dynamic RAM allocation.  In my limited use of other brands of GPU, it has some infrequent but major bugs with AMD IGPUs and frequent but minor bugs with Intel IGPUs (I've never tried a dedicated card from either brand with Hyper-V).",hardware,2025-06-24 02:27:59,3
Intel,mzhzus4,"I think the future way around is to make the vm look like any other application from the gpu drivers point of view. On Linux (both host and guest for now) there exists virtio native, but for now it only works on Amd and is still in Beta and not yet really well documented how to get it working.",hardware,2025-06-24 12:15:24,3
Intel,mzf3egk,"I know AMD GPU passthrough is supported by HyperV virtualization and LXC containers without MxGPU, but that might be Linux-specific.",hardware,2025-06-23 23:19:19,4
Intel,mzfq3xm,"AMD appears to be heading towards allowing SR-IOV on consumer Radeon soon, but we'll see if it actually happens.  https://www.phoronix.com/news/AMD-GIM-Open-Source",hardware,2025-06-24 01:30:18,3
Intel,n01czo2,"I'm a bit late to the conversation, but I have more faith in Virtio-GPU Native Context landing than whatever the big three are planning  But VirtioGPU NC is, at the moment, Linux specific.",hardware,2025-06-27 09:48:18,1
Intel,mzfqyea,"I think the real question is availability and drivers. If it was like the initial Intel Arc GPUs launched in 1 laptop shop in S. Korea, good luck trying to expand market share.",hardware,2025-06-24 01:35:15,9
Intel,mzh6dpj,"Yeah, i'd reckon this mostly. Initially, more than a decade ago by now i guess, we were seeing this, with consumer grade GPU's being used or tried to power workloads on virtual desktops, RDS deployments and even research clusters.",hardware,2025-06-24 08:04:43,4
Intel,mzg1tp0,Surprised AMD just acted like Nvidia while Intel is the one actually doing this for so much GPU stuff.,hardware,2025-06-24 02:38:50,8
Intel,mzh08sa,The problem is they're not super available either,hardware,2025-06-24 07:04:06,2
Intel,mzf5byu,"PCIe Passthrough is not the same as GPU splitting. All 3 have support for PCIe Passthrough. I use it all of the time. The main goal of SR-IOV is to remove the need for multi-GPU setups in virtualization which from a market segmentation point would be detrimental to profits at the consumer level because a host may only need 1-4GB of VRAM and the rest can be allocated to the guests. This would be very impactful in ""Prosumer"" markets where someone may need more than 2 GPU outputs (typical for iGPUs) but may not want a multi-GPU setup as a multi-GPU setup has higher power draw.",hardware,2025-06-23 23:29:54,20
Intel,mzg0701,Intel Linux drivers are solid for everything except games,hardware,2025-06-24 02:29:04,5
Intel,mzgni4h,"Its an incredibly small market for this among consumers.  Most people asking for this is in a professional environment, and everyone wants to sell pro cards.",hardware,2025-06-24 05:10:46,22
Intel,mx1cb0m,TLDR:    Geomean: RX 9060 XT 16GB is 46% faster (mixed settings).     Raw performance closer to 30-40% faster.          Cheapest RX 9060 XT 16GB in EU is ~400 euros (VAT included)     Cheapest Arc B580 12GB in EU is ~294 euros (VAT included),hardware,2025-06-10 15:58:32,37
Intel,mx1kt2f,Does the B580 still suffer from the massive CPU overhead or was is ever fixed?  Considering that noone reasonable will pair something like a 9800X3D with a B580 something like a 7600 or even 5600 or 5700X3D this is something a lot of people would have to keep in mind when it comes significantly worse on the kind of CPUs you would actually see in a budget build.,hardware,2025-06-10 16:38:47,24
Intel,mx29c7r,average 58fps 45fps 1% lows with 9800x3d system will be unplayable for most people with zen3 era speeds.   Turn off rt- use fsr/xess/dlss + fg for 90fps+,hardware,2025-06-10 18:31:26,9
Intel,mx30dvt,"Since 9060 xt is $390, cheapest 5060 Ti 16gb at $430 might as well be included in this calculation",hardware,2025-06-10 20:39:15,10
Intel,mx5audi,I hope B770 delivers at a price that old-heads like me can appreciate,hardware,2025-06-11 04:36:47,3
Intel,mxrir8n,I own both. The 9060xt is better use of money.,hardware,2025-06-14 16:33:12,1
Intel,n5b0wbx,hot take   intel arc b580 is a better value then the 9060xt 16gb reason   your going entry level gpu   9060xt is a weird spot because if you can find at retail 9070 smashes it at 100 150$ more it makes it EXTREMELY HARD TO SUGGEST YES some people supposendly cant afford the extra 100 150 (although its very very likely not off the bad just a extra month of work or so) especially when your going entry you can argue fps is higher but my argument is 250$ for a 12gb gpu that competes properly against what its trying to compete against the 4060 while the 9060xt is a messy pricing and i also argue no one wants to give intel a realistic chance which is another issue if more people gave intel a chance i fondly believe it would be great for most people,hardware,2025-07-26 18:30:42,1
Intel,n9s1258,"Question is:  They have a deal in few days where i can get BF6 free with intel GPU. It's a game i would have bought anyway so it's like getting a 90$CA off the GPU price    Currently, i can get a Arc B580 for 360$CA which would mean an effective price of 270$CA. A 9060 XT, the cheapest comes at 490$CA (but it's on sale right now so the real cheapest would be 540$CA).   At this theorical price, i guess i can't go wrong with the intel one right? I will pair it with a Ryzen 7 5800x probably.   I'm currently running a Ryzen 7 1800x and a GTX 1070ti and playing at 1440p and i really don't care about running 4K ultra. BF6 was working incredibly well at low on my Nvidia so if graphics are higher i can only be happy but again, dont need ultra and fancy graphics that make real world look dull.    What's your thought people?",hardware,2025-08-20 20:44:46,1
Intel,mx2m202,"Prices differ between EU countries, in Germany the cheapest B580 is currently 270€ and 369€ for the 9060 XT.  For the US it is 310$/370$ currently.",hardware,2025-06-10 19:31:49,16
Intel,mx1d5bz,"Yes, but as he says in the video, there are some major outliers in the results due to things like Vram going past 12GB, and certain games just not liking Arc GPUs. Looks like a more ""typical result"" is 30% to 40% faster when not going over 12GB VRAM and not in a game that Arc has issues with. It's always hard to sum up the difference with a percentage, but especially hard with Arc GPUs, upscaler differences, etc.",hardware,2025-06-10 16:02:27,25
Intel,mx1ibks,"Australia pricing (AUD, GST incl.):   9060 XT 16GB: $629   B580 12GB: $449",hardware,2025-06-10 16:26:55,8
Intel,mx52fap,What's VAT?,hardware,2025-06-11 03:35:35,1
Intel,mx25eua,Never heard of any more news about this either,hardware,2025-06-10 18:13:02,10
Intel,mx1lidh,"Yes, but the conclusion was that even in this best case scenario for the B580 it offers worse value. So not looking good for the B580, especially when it's way above MSRP.",hardware,2025-06-10 16:42:08,21
Intel,mx2fuv9,I don't think it can be fixed. The card seems to have been designed to utilize extra CPU resources as much as it can.,hardware,2025-06-10 19:02:13,15
Intel,mxnt59g,"have a b580 and 9800x3d combo, and am perfectly happy with it. it even does some 4k gaming stuff just fine.",hardware,2025-06-14 00:34:48,1
Intel,mx29ssu,"As long as the zen3 era CPU can hit 58gps average and 46fps 1% lows in whatever game you are referring to, then it would make no difference vs having the 9800x3d. You only see a difference if the CPU frame times take longer to compute than the GPU frame times.",hardware,2025-06-10 18:33:38,1
Intel,mx5wf17,"Yeah, as someone trying to build a new PC for 1080p/1440p it makes no sense to go for the 9060xt since the 5060 Ti is only slightly more expensive and gets me the new DLSS, better ray tracing, etc. I guess for pure raster performance one could also include the 7800xt but it's not as ""future proof"" that's for sure.",hardware,2025-06-11 07:50:30,5
Intel,mxfi19s,"Got a 3080 ti for 550$ yesterday, man that hits hard for value against 7900 xt (750$) and 5060 ti   A 5060 ti, 4070 here costs 600 used.",hardware,2025-06-12 18:55:28,1
Intel,nfcwc0b,I’m in your shoes. I just bought my B580 with the Intel bundle for BF6. I have a Ryzen 5 3700X and tbh i’m not super super impressed with how much tinkering and adjustment is needed to get the Arc card to run “properly”. I’ve only had mine for a few days so hard to justify but i’m on edge of returning it possibly.   It seems every game is a hit or miss on how it runs and by saving only $100ish dollars i’m starting to think it’s not worth it. Don’t get me wrong majority of my games are running fine but the few that are inconsistent it’s like was this really worth it?  TLDR; Card is a hit or miss with the game you play. IMO spend the extra $ and get a reliable card from NVIDIA or AMD and feel secure,hardware,2025-09-21 03:08:19,1
Intel,n2dal8f,260€ for the B580 in july...,hardware,2025-07-10 14:24:59,1
Intel,mx2qzeo,"Also, has Intel fixed the driver over head issue? People buying lower end gpus probably dont have top of the line CPUs to play with.",hardware,2025-06-10 19:55:27,14
Intel,mx5lvvg,Value Added tax. Think a better version of sales tax.,hardware,2025-06-11 06:09:55,1
Intel,mx1wp8z,"It's better then *any* 8 gig model, but that is the definition of damnation with faint praise.",hardware,2025-06-10 17:33:13,2
Intel,mx2aejk,"Hell no, the 9800x3d is doing a lot of heavy lifting with 1% lows and 0.1% lows. If you ran same tests with 3600-5700x. It wont be same experience at all especially on B580",hardware,2025-06-10 18:36:31,12
Intel,mx7xjln,dlss is only dealbreaker if they're extremely similarly priced otherwise I'm not really sold on the difference just for the features    especially where i live,hardware,2025-06-11 15:58:48,3
Intel,mx8d89v,"I don't consider a 25% price difference to be ""slightly"" more expensive.",hardware,2025-06-11 17:13:00,1
Intel,mxpjr2z,Also hits hard on the power draw,hardware,2025-06-14 08:21:05,2
Intel,mx3r68w,Is it fully a overhead issue or were tests done on CPUs that predate RE-BAR support? It looks like Arc is basically have RE-BAR and it performs close to expectation or its in the dumpster without it,hardware,2025-06-10 22:56:15,0
Intel,mx6a6s1,Oh,hardware,2025-06-11 10:07:04,1
Intel,mx25q1a,"debatable. 4060/5060, 9600xt on 3600x-5600x are faster than b580 on similar stepup",hardware,2025-06-10 18:14:29,16
Intel,mx2bmqq,"I'd rather have a 5060 ti 8GB than a B580 if you offered me them for the same price. It'd be way faster as long as I tune my settings. That being said, id never buy the 5060 ti 8GB.",hardware,2025-06-10 18:42:19,9
Intel,mx4s3ko,I would rather take the 5060 and play with lower textures than the b580 and be cpu bottlenecked with driver issues and no DLSS.,hardware,2025-06-11 02:28:26,3
Intel,mx2b549,"You still aren't saying which test you are referring to, so my answer has to say general rather than specific, but you seem to not fully understand how this works. If the CPU computes the frame faster than the GPU, then an even faster CPU will not improve the frame time. If in this particular test a lower end CPU could not compute the frame faster than the GPU, then the faster CPU is helping. But a faster CPU does not always make a difference. It only matters if the weaker one is actually limiting the performance, which is often not the case.",hardware,2025-06-10 18:40:00,-2
Intel,mx40jbk,"HUB tested very thoroughly and even did a follow up video about it because some russian tech site called HUB liars. TLDW of it is, yes rebar enabled, any game with decent CPU usage seriously hampered GPU performance, not all games but enough that it is something that anybody reviewing an Intel GPU should test for and advise the viewer about.",hardware,2025-06-10 23:47:39,15
Intel,mx86fb1,"Running a B580 with a 5700X3D, rebar enabled, tweaking a few settings, and it runs well for what I need it to do.  Basically iRacing and other sim racing titles.",hardware,2025-06-11 16:41:21,1
Intel,mx2cnqd,"Nah theres definitely a drop if you pair 9800x3d with b580 and 5600 & other gpus as well. You'll see lower average & 1% and 0.1% lows.  Setups with b580, 4060s, 9600xt could see averages of: 52fps 40fps 1% lows. Just turn off the rt, optimize and run fg for \~100fps",hardware,2025-06-10 18:47:12,2
Intel,mx2d7x5,"That is not true as a blanket statement. It is situationally true in the cases where the lower end CPU  can't keep up with the GPU frame times, but it is not true when it can.",hardware,2025-06-10 18:49:48,0
Intel,mx2fl7u,... its disingenuous to only test with 9800x3d on b580. Average person wont get those perf. Adequate step-up for me would've been a midrange cpu,hardware,2025-06-10 19:00:56,7
Intel,mx2gl9d,The conclusion was that the b580 is still worse performance and value even in this best case scenario,hardware,2025-06-10 19:05:44,1
Intel,mx2i9bj,Intel isnt serious about dgpu anyways. arc still has less than 0.15% marketshare on steam and the competitors are thriving...well only Nvidia. B580 probably 0.04%,hardware,2025-06-10 19:13:39,1
Intel,mt5nyr2,"I will say that Intel's naming is incredible. Now we have *Battlematrix*, what a badass name. Can we, as a society, *please* stop pretending that the rule of cool is ""cringe""?",hardware,2025-05-19 17:41:16,74
Intel,mt8ck76,700$ for the B60 is pretty cheap,hardware,2025-05-20 02:30:19,11
Intel,mt5olpn,"Intel badly needs a presence in enterprise or the data center, so these Pro Arc GPUs are to be expected.  Hopefully these cards can act as a lifeline for the struggling Arc division, but I hope that data center revenue doesn’t compel Intel to abandon the consumer segment altogether.",hardware,2025-05-19 17:44:16,20
Intel,mt8miq9,The passively cooled one looks interesting.,hardware,2025-05-20 03:40:15,6
Intel,mt5btpw,"Original title: THIS is the Most Important GPU of 2025  Genuinely could be exciting stuff, both in terms of AI/pro use cases, and in terms of giving Intel's discrete GPU arm more sales / runway.  Also includes commitments from Intel towards the gaming side of GPUs, so all in all pretty good news, if this doesn't bomb.",hardware,2025-05-19 16:42:59,42
Intel,mta0joa,Interesting video downvoted to oblivion  GN spam upvoted endlessly  Make it make sense...,hardware,2025-05-20 11:25:20,16
Intel,mtc38v0,It’s nice that Intel will let people run the gaming drivers on these instead of artificially segmenting the consumer and professional products. Could see it being a cheap 24GB people’s champion once games start requiring obscene amounts of VRAM and companies offload their old stuff.  I want to see how the 70w model compares to the 6GB 3050.,hardware,2025-05-20 18:04:32,3
Intel,mteku0z,The problem is it is a bit slow even compared with 5070 sigh,hardware,2025-05-21 02:05:33,1
Intel,mtvlu4c,"But think of all those nVidia cards that could have been sold to gamers instead being sold for AI...  And think about what will happen when there is a $500 24GB AI capable board the AI guys can buy instead!   Or a dual GPU 48GB!  There are many AI cases where the 48GB card is going to trounce any offering from nVidia, and every card Intel sells to AI folk is one more nVidia card available to gamers.   So whether or not you are personally interested in buying a B60, it IS going to be a crazy important card to gamers because this is going to permanently shift the supply/demand for nVidia cards as well as force nVidia to start increasing vram to compete.  Bonus, because Intel has their own foundries, the upcoming celestial line will not be competing with nVidia for foundry time, so this will also increase the total number of video cards produced!",hardware,2025-05-23 18:12:43,1
Intel,muoy0wo,"This is legit, I can see a lot of companies adopting this to use for local AI. Now if only Ollama IPEX can update their support and not lag behind for Intel ARC graphics, compared to the regular build for AMD and Nvidia.",hardware,2025-05-28 13:01:36,1
Intel,mxakky4,I will be watching out for B50 though.,hardware,2025-06-11 23:51:22,1
Intel,mt5cn8v,"Great troll clickbait title tbh, the same day that 5060 hits shelves. People will of course think this is a 5060 review and LTT knelt the knee on Nvidia if they don't watch the video.",hardware,2025-05-19 16:47:02,-35
Intel,mtbffx9,TL:DR ITS ALL DATACENTER GAMERS MOVE ALONG.,hardware,2025-05-20 16:10:32,0
Intel,mt6bkc8,"No, we gotta have AI Pro Max Ultra+   /s",hardware,2025-05-19 19:37:00,46
Intel,mt70lbr,I still feel their architectures called after D&D things is so cool.,hardware,2025-05-19 21:43:09,15
Intel,mt86tqz,I'm not a fan of *Sparkle*,hardware,2025-05-20 01:53:51,3
Intel,mt6lxvx,"What data center revenue, they have pretty much none. Their roadmap also slid, they won't have anything competitive for data center till 2027.",hardware,2025-05-19 20:28:50,6
Intel,mtn2amf,"Why is Arc 'struggling'? Every release so far, going back to the first one, have sold out very quickly. B580s haven't been in retailer stock much at all since it came out, until the new AMD GPUs that recently released. But, whether it's scalpers or consumers, Arc never stays in stock for long.",hardware,2025-05-22 11:57:12,2
Intel,mt601wk,"I mean I think that's what the dGPU for Intel is changing for Xe3P and onwards; Intel's fabs producing these GPUs which make money with AI/Workstation. I mean the original Xe produced on TSMC under Raja is certainly gone but feels like they've re-calibated with Xe3P quite a bit.  Honestly if Xe3P performs \*very\* well in labs I'd do a N44>N48 style die upscaling of a probable 256-bit part they're working on and just double the spec of it. If it still struggles in gaming Vs RTX 60 & UDNA/RDNA5/GFX14 PPA wise then they can just reuse said dies for AI easily. Especially could fill in the gap left by Falcon Shores to an extent.  An ""affordable"" 96GB Clamshell card for AI/Professional Vs Nvidia sounds pretty tempting.",hardware,2025-05-19 18:39:44,6
Intel,mtfh7k0,"> Original title: THIS is the Most Important GPU of 2025  seeing a title like that would instantly get me to click ""not interested"" in youtube.",hardware,2025-05-21 06:04:23,1
Intel,mtn1uxy,It's Reddit. How often does it really make sense? XD,hardware,2025-05-22 11:54:15,3
Intel,mur9wzh,I NEED to see this in gaming xD  I don't think gamers should be walled off from having a GPU that can do ai stuff and gaming as well. The GPU should be able to do both IMO,hardware,2025-05-28 19:50:44,2
Intel,mt7xyra,No one will actually think that 5060 is important,hardware,2025-05-20 00:58:52,20
Intel,mt7klij,No they won't?,hardware,2025-05-19 23:37:46,9
Intel,mti2kqi,"not all datacenter, but prosumers and businesses that need to leverage stuff for workstations also have a space here. plus, while nowhere near as performant as a full fledged gaming GPU, i bet the B60 with gaming drivers can be a respectable gaming card for those off work hours gaming sessions.",hardware,2025-05-21 16:50:10,4
Intel,mt5c5ii,"It was clickbait, I editorialized the best I could. Although I do kind of buy their reasoning for why it _could_ be a very significant GPU overall - if it lands well.",hardware,2025-05-19 16:44:37,11
Intel,mt7r22d,Where's my Chaotic Neutral Rogue GPU Intel?  WHERE!?,hardware,2025-05-20 00:16:20,6
Intel,muysqk8,Sparkle is an AIB who makes GPU boards. Intel didn't pick that name.,hardware,2025-05-29 22:25:09,2
Intel,mtqrh4t,Their first generation had some issues and was not great even for the low price but in the past year they have been killing it in the budget card space. Its really nice to have some options on the lower end again.,hardware,2025-05-22 23:17:55,2
Intel,mti3ac2,"VR development could be an interesting topic here, assuming one GPU die can not access the other GPU's VRAM, it could still be a decent option for development of VR games on the internal testing front. VR is very much staying, along with AI so i can see this being a nice tool for both, just not all at once.",hardware,2025-05-21 16:53:31,1
Intel,mtfh9nf,you underestimate human stupidity and overestimate their ability to read.,hardware,2025-05-21 06:04:57,1
Intel,mtfh2of,Youll have to wait for R generation for the Rogue. Alignment optional.,hardware,2025-05-21 06:03:05,2
Intel,mthkmay,Chaotic Neural Rogue.,hardware,2025-05-21 15:24:05,1
Intel,muz3jnb,"I see, ya they do have cool codenames. Also TIL Intel Arc GPUs are named after character classes from D&D. Alchemist and Battlemage for example, no idea where Battlematrix comes from though",hardware,2025-05-29 23:24:21,1
Intel,mtqzhgz,"$299 and I can do Forza high settings 120fps at 1440p? Sold. ...don't care about RT, I turn it off anyway.",hardware,2025-05-23 00:04:10,2
Intel,mtpd95g,I'm just trying to find an affordable card that will run 4k vr at 60 to 90 fps without any dlss; so many cards don't have enough vram for vr. Excited to see if the B60 will be able to do this.,hardware,2025-05-22 18:58:57,1
Intel,nex3oxj,"You can reasonably assume that there will be driver support for a significant amount of time for the B580, honestly it'll probably be supported for the entire time that you have the card. It's unfortunate in that this probably kills future products in the Arc line, but given Intel's need to shed costs, it's also not all that surprising.",buildapc,2025-09-18 17:07:02,27
Intel,nex5ljh,Intel releases a competitive GPU   Nvidia instantly proceeds to buy a part of Intel...,buildapc,2025-09-18 17:16:00,25
Intel,nex40td,Seems to me like they'll be building SoC hardware for like mini-PCs or laptops.  Maybe integrated graphics for desktop CPUs.  It doesn't seem like it's competing with discrete GPUs.,buildapc,2025-09-18 17:08:34,11
Intel,nexi8um,This is shit. Hopefully they still make the mid range 700 series. I was sort of holding out for that but it's taking too long.,buildapc,2025-09-18 18:15:20,5
Intel,nex7uaa,"This does suggest that Intel GPU unit will likely get the axe.  It also suggests that ARM doesn't have the HPC ability of x86.  ARM was half as fast, at the same power, as AMD EPYC.  So, Nvidia is likely admitting that ARM isn't close enough yet.  Real question is how long this takes to materialize?  Further, this could violate x86 agreement.",buildapc,2025-09-18 17:26:32,3
Intel,nexirdo,They aren't going to sabotage a division of a company they just bought for 5 billion. I think it's a problem for the new year release of the 7 series cards though. Those were supposed to challenge some of the mid range cards.   They aren't dropping driver support for a LONG time. By the time you have to worry about it you wont own it.,buildapc,2025-09-18 18:17:48,2
Intel,nf18ub4,Wasn't arc already on its way out?,buildapc,2025-09-19 07:51:44,1
Intel,nex49b1,"Alright, good to hear. I’ve been pretty happy with it so far otherwise!",buildapc,2025-09-18 17:09:41,1
Intel,nexxb1j,"Swatch owns all the watch brands. Meta, alphabet and Microsoft own all the tech brands. Nestle, Unilever own all the commercial food products etc etc",buildapc,2025-09-18 19:27:47,7
Intel,nex641q,Lol seriously. This is how capitalism in america works these days!,buildapc,2025-09-18 17:18:25,13
Intel,nex5bea,"True, my first thought was consoles and handhelds (Steamdeck).",buildapc,2025-09-18 17:14:40,1
Intel,nex5jyl,I really hope this is the case because I love the idea and technology behind arc and really want it to become more competitive.,buildapc,2025-09-18 17:15:47,1
Intel,nexjii9,"Also consider that even for Nvidia spending 5 billion to just terminate a division of intel isn't why they did it.  They'll probably slap their brand on it and rake in the profits. I'd expect a 10% jump in msrp but still.. They aren't in business to lose money. I think they'll keep production going personally, put their stamp on it, sell it for a bit more and keep the status quo.   I think they were worried about what the new intel card was going to do to their low/mid range lineup. The dev cost for the 700 series has already been spent. They will release it.. Just at what price point?",buildapc,2025-09-18 18:21:26,3
Intel,nexps44,5 billions is a small price to pay to eliminate a competitor in their cash cow almost monopoly segment.,buildapc,2025-09-18 18:51:37,4
Intel,nf3spi2,b series is less than a year old,buildapc,2025-09-19 17:24:13,1
Intel,nex6wep,it's always worked that way.,buildapc,2025-09-18 17:22:07,6
Intel,nex7cv4,Thats true i guess if were being honest,buildapc,2025-09-18 17:24:16,1
Intel,ngtmchu,Save a little more and buy 9060 XT 16 Gb to serve long life without pain in the ass because of potential Intel discontinuing of support or lack of VRAM - this is the best solution.,buildapc,2025-09-29 13:12:34,10
Intel,ngtlag8,"B580, the 8gb 9060xt isnt a super attractive option. The 16gb variant is a better card but its usually above ~40% more than the b580",buildapc,2025-09-29 13:06:21,5
Intel,ngtmy0n,"If you are not going to be buying a new gpu for a good few years I would go with the 9060xt. There is no telling how long Intel is going to maintain driver support, and how much improvement there will be with the driver support that is left. B580 still suffers with issues from launch like cpu overhead, if there are game specific bugs or optimizations that they need to do for in drivers for new titles you might just be shit out of luck.",buildapc,2025-09-29 13:16:03,2
Intel,ngtqdud,B580 is significantly slower on weaker cpus like your i5. Get a 9060xt,buildapc,2025-09-29 13:35:41,0
Intel,ngu9z2l,Neither. Get used if those are your only new options. 8gb is not enough for a new card and Intel GPUs are e-waste and are plagued with driver issues. I also doubt Intel is going to be pumping resources into improving that situation because dedicated intel graphics are effectively canceled.  9060 XT 16GB is the cheapest GPU worth buying new.,buildapc,2025-09-29 15:16:49,0
Intel,nguhzf9,That 9060XT will become obsolete by its own VRAM very very soon. Either save for the 16GB version or go with the B580. Run away from any 8GB GPU if you want it to last more than a year.,buildapc,2025-09-29 15:56:00,0
Intel,ne1d063,"Sure. B580 is best value GPU on the market right now, as it comes with BF6",buildapc,2025-09-13 18:38:41,2
Intel,ng387ko,Used or new?,buildapc,2025-09-25 07:36:42,1
Intel,ng38hkg,"7700 xt, you can find them new at around 320$",buildapc,2025-09-25 07:39:31,1
Intel,ng3asfg,Look around Jawa:   Gigabyte Gaming OC RX 7600XT | Jawa https://share.google/drh4RK5YDMkwiXxLS,buildapc,2025-09-25 08:02:44,1
Intel,ng3f80v,Used 3080 go for about $350 and under now a days,buildapc,2025-09-25 08:48:48,1
Intel,ng3hxv4,Try a used or lesser brand 3070. If you're in usa theyre at newegg for less than 270usd,buildapc,2025-09-25 09:16:29,1
Intel,ng3jn52,"9060XT with 16GB of VRAM, the only budget card worth buying. Friends don't let friends buy 8GB VRAM cards in 2025.  FSR4 is a game changer and this upscaling and frame gen tech will give you added performance with improved visuals and is quite frankly the future of gaming, and will be implemented into the PS6. Sony helped AMD develop FSR4, or should I say, they co-developed it.  Here you are, by the way....  https://www.youtube.com/watch?v=ApAOw29_QCE",buildapc,2025-09-25 09:33:47,1
Intel,nh4kk31,"7500F + motherboard + DDR5 for £324:  https://uk.pcpartpicker.com/list/gbYMsp  9600X + motherboard + DDR5 for £354:  https://uk.pcpartpicker.com/list/WzrdLc  I'd go for the 9600X personally, for that motherboard you may need a BIOS update. You could sell your 5600X + mobo + DDR4 as a bundle on FB Marketplace or something. Ali Express may have cheaper stuff if you trust them",buildapc,2025-10-01 03:25:58,1
Intel,nh4lvvh,"Based on what games you say you play, I'm not sure a platform upgrade is required at this time.    What might be the easiest, cheapest, and most prudent option is to just upgrade your GPU. Then, if you're not getting the performance you desire, upgrade the platform.    What budget do you have? What resolution and refresh rate do you use?",buildapc,2025-10-01 03:35:26,1
Intel,nfqnj5q,The ARC card will probably be ur best bet but AMD and Nvidia have better features in general amd the 9060xt 8gb isnt that bad. It jsut kinda sucks that the lower end of hardware hasn't moved all that much in years .  As for a cooler mean for $20 or less u cant beat the Thermalright Burst Assassin its a single tower air cooler but its build really well and can handle a ton of heat  https://pcpartpicker.com/list/8zXRcx,buildapc,2025-09-23 09:52:41,2
Intel,nfqdviq,"Atleast the 12gb option, probably pick the 12gh nvidia over the arc",buildapc,2025-09-23 08:02:57,1
Intel,nfqied9,"this would be my game plan - 9060XT 8gb, 5060 8gb, Arc b580, 3060 12gb  for a tower cooler I personally like the ID-Cooling SE214XT   I added a PCPartPicker list to give some ideas of better PSUs to get over what you listed  [https://pcpartpicker.com/list/MyWcwY](https://pcpartpicker.com/list/MyWcwY)",buildapc,2025-09-23 08:55:15,1
Intel,nfqks1r,"Dont overpay for your usage. Compare benchmarks between b580,  3060 12 gb, 5060 8 gb and 9060 8 gb and buy what's best for your use (1080p competitive games). It will probably be the 5060.",buildapc,2025-09-23 09:22:31,1
Intel,nex191d,"First, your cpu is weak enough to still limit a 5060, and if you dont have a frame cap it will cause stuttering and frame drops. You're likely running into a ram limit being on 16gb as well, which again causes stuttering and frame drops, since once the ram is full it starts to use system storage as ram.    Id suggest saving some money up and going for a 9600x/7600x mobo and ram. If you need a cheaper option, id buy a used 5000x3d cpu with 32gb ddr4.",buildapc,2025-09-18 16:55:35,2
Intel,nex0xur,>\- Turn on resizeable BAR  Turning that off actually helped me with FPS drops in games.  Did you test it being turned off with your new GPU?,buildapc,2025-09-18 16:54:08,1
Intel,nex2hzp,"turn off ReBAR and check GPU temps, trust me on this one.  if your RAM is healthy and CPU also, with good temps, it could be the ReBAR and temps on ur gpu. check for airflow in ur case",buildapc,2025-09-18 17:01:27,1
Intel,neyx3y3,See if your motherboard is outputting the proper pcie signal to your GPU. It might be trying to output the wrong signal. It happened to me because I had an old vertical GPU mount that had an older cable. It couldn’t handle the 4.0 signal and I manually dropped it to 3 in the bios.,buildapc,2025-09-18 22:25:02,1
Intel,nf1dzyq,"Seems like you tried basically everything, do you have the same stutters with your 1080p monitor as well?",buildapc,2025-09-19 08:43:55,1
Intel,nex5vpt,"Before I got rid of my 1060 everything ran just fine. I am willing to upgrade, but I mostly play older games anyways. Many of these games use little ram and leave plenty of cpu headspace.",buildapc,2025-09-18 17:17:19,1
Intel,nf1dkuo,I have the 3600 paired with 16gb of ram and I experience no stutters. Pretty sure the problem is something else.,buildapc,2025-09-19 08:39:46,1
Intel,nex5owa,"Thanks for the tip, just did. It sadly did not help",buildapc,2025-09-18 17:16:26,1
Intel,nex46ca,"> turn off ReBAR and check GPU temps  Given that they're probably also using the Wraith Stealth that came with the 3600, they should probably check CPU temps as well. If the install is old enough, it's quite possible that the thermal paste has dried up and the CPU is throttling. Even more likely if this is a chassis something like a non-Flow variant of the NZXT H510.",buildapc,2025-09-18 17:09:17,1
Intel,nex6drb,Did you run DDU yet?,buildapc,2025-09-18 17:19:41,1
Intel,nf20kmu,Pretty sure people play a different list of games and have different expectations for different settings,buildapc,2025-09-19 11:55:33,1
Intel,nex65pj,"Thanks both for the tips. Good point on the cpu paste. However, the temps are nothing out of the ordinary. As far as turning of ReBAR goes I also have no succes :(",buildapc,2025-09-18 17:18:38,1
Intel,nex7p4w,I did!,buildapc,2025-09-18 17:25:51,1
Intel,nex8i3c,"Then my best answer is the initial comment I gave, sorry I dont have anything else to offer.",buildapc,2025-09-18 17:29:35,1
Intel,nb8jouv,I'd personally suggest the 9060xt.  The 9000 series are spectacular tbh...,buildapc,2025-08-29 01:08:31,12
Intel,nb8khvg,If it's the 16gb 9060 xt I'd say the upgrade is worth it,buildapc,2025-08-29 01:13:15,1
Intel,nb8tz26,If its the 16gb 9060xt then absolutely that. Thats the one i just got for my first pc,buildapc,2025-08-29 02:11:04,1
Intel,nb8z9g2,"It honestly depends on what your needs are... Fps, solo campaign, rpg, streaming, vid rec., etc ... Do u have an interest in A.I. or M.L.? RX is amazing for raw frames n streaming. ARC is amazing for AI n ML so it depends on what your use cases are",buildapc,2025-08-29 02:43:36,1
Intel,nb9it9e,9060 XT and get the 16GB version.,buildapc,2025-08-29 05:05:01,1
Intel,nb8jzrh,is the current price still good with it?,buildapc,2025-08-29 01:10:18,1
Intel,nb8lgjv,the pricing would be my real question as it seems to be too much for both,buildapc,2025-08-29 01:19:03,1
Intel,nb8kctz,"they're both overpriced, is this USD?",buildapc,2025-08-29 01:12:27,2
Intel,nb8kk4l,Nah. MSRP is 350. +/-50 is what I'd accept. 100+ more is crazy. That's 30% upcharge.,buildapc,2025-08-29 01:13:38,1
Intel,nb8ohbu,What country are you in? It would depend on local pricing and what alternatives are priced at. Sometimes countries are just expensive unfortunately,buildapc,2025-08-29 01:37:21,2
Intel,nb8kjao,usd equivalent of php,buildapc,2025-08-29 01:13:30,1
Intel,nb8lbsr,where can we get msrp ones im from the philippines.,buildapc,2025-08-29 01:18:15,1
Intel,nb8l2bd,"if I had to overpay for something, it'd be amd over Intel.  Especially because their new CEO has been threatening to discontinue their GPUs",buildapc,2025-08-29 01:16:41,4
Intel,nb8lyby,I see. Are the 9060 xt around those price?,buildapc,2025-08-29 01:22:02,1
Intel,nb8lddy,thanks for that info ill keep that one in mind,buildapc,2025-08-29 01:18:31,1
Intel,nb8m7z8,on stores here yeah around those price,buildapc,2025-08-29 01:23:36,1
Intel,nb8nu3n,I see. Then I guess it's fine to buy it. It's a much better card that the b580 by 25%.,buildapc,2025-08-29 01:33:25,2
Intel,nd0u6iz,"If you go with a smaller case you can get a better GPU.  [PCPartPicker Part List](https://pcpartpicker.com/list/RDQQv4)  Type|Item|Price :----|:----|:---- **CPU** | [\*AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $178.51 @ Amazon  **CPU Cooler** | [Thermalright Assassin King SE ARGB 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/9Gstt6/thermalright-assassin-king-se-argb-6617-cfm-cpu-cooler-ak120-se-argb-d6) | $18.59 @ Amazon  **Motherboard** | [\*Gigabyte B650M GAMING PLUS WIFI Micro ATX AM5 Motherboard](https://pcpartpicker.com/product/9HTZxr/gigabyte-b650m-gaming-plus-wifi-micro-atx-am5-motherboard-b650m-gaming-plus-wf) | $125.00 @ Amazon  **Memory** | [\*Silicon Power Value Gaming 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/cCKscf/silicon-power-value-gaming-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-sp032gxlwu60afdeae) | $85.97 @ Silicon Power  **Storage** | [TEAMGROUP MP44L 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/2x4Ycf/teamgroup-mp44l-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-tm8fpk001t0c101) | $54.99 @ Newegg  **Video Card** | [\*PowerColor Reaper Radeon RX 9060 XT 16 GB Video Card](https://pcpartpicker.com/product/fzh2FT/powercolor-reaper-radeon-rx-9060-xt-16-gb-video-card-rx9060xt-16g-a) | $379.98 @ Newegg  **Case** | [Cooler Master MasterBox Q300L MicroATX Mini Tower Case](https://pcpartpicker.com/product/rnGxFT/cooler-master-masterbox-q300l-microatx-mini-tower-case-mcb-q300l-kann-s00) | $39.99 @ Amazon  **Power Supply** | [\*Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $89.90 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$972.93**  | \*Lowest price parts chosen from parametric criteria |  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-09-07 23:06 EDT-0400 |",buildapc,2025-09-08 03:06:31,6
Intel,nd0uz9i,"Looks like you hit the nail on the head for a budget ddr5 gaming system, maximising performance per dollar IMO.   You could try a 9060 xt, to get 16gb of vram to help give your GPU more legs to play longer since a 7500f should still be well suited for that GPU. It is likely about $100 more though.   I see nothing wrong with this build for what your assumed goal is, you could probably squeeze more out of your budget with a different mobo, case, and cooler and upgrade the GPU at most and stay in budget.",buildapc,2025-09-08 03:11:48,2
Intel,nd0vafv,Bit over budget  https://pcpartpicker.com/list/HfKWh7   In budget  https://pcpartpicker.com/list/jJCGC8,buildapc,2025-09-08 03:13:52,2
Intel,nd0v587,"you can shave of like $25 off the SSD with a 2tb $100 SDD instead. You can also shave 17$ off the cooler by getting a regular assassin instead of the double radiator version, since you don't need a double radiator to cool the 7500f. And you can save money on the power supply by opting for a 650w supply instead. Gives you plenty of wattage overhead as well. All those savings can allow you to get a better GPU like the Radeon RX 9060XT 16gb",buildapc,2025-09-08 03:12:54,1
Intel,nd0vhi0,"Could also do a 7700xt, slightly worse performance and 4gb less vram, $50 cheaper  https://pcpartpicker.com/product/97kH99/xfx-speedster-swft-210-core-radeon-rx-7700-xt-12-gb-video-card-rx-77tswftfp",buildapc,2025-09-08 03:15:11,1
Intel,ncr9put,"buy a better gpu because it sounds like you're really only interested in playing games at max fidelity, so a good GPU will take you way further than any monitor currently can. IMO, a good/great monitor should be the last thing that you get because if max frames is the goal, it's smarter to see where your personal cutoff is (in regards to resolution/fidelity with your current setup) and whether or not you're really wanting to buy pricier hardware to max out your current monitor or upgrade to a monitor with higher specs or a different panel type like W/QD-OLED",buildapc,2025-09-06 16:46:19,3
Intel,ncrcdin,I mean in short i have 2 choises intel arc B580 and QD/W-Oled monitor or Rtx 5070/5060 ti and a normal not good not bad monitor but is the monitor worth it? cause im not sure is an OLEd that big of an diffrence,buildapc,2025-09-06 16:59:50,1
Intel,ncrdoxa,"There is definitely a noticeable difference between a nice IPS panel monitor and a QD/W-OLED monitor, there's absolutely no debating this. But where objectivity ends, subjection begins and this is where your personal standards come into play. For me personally, growing up playing games on low resolution CRT monitors, I only care that I can play my games of choice, so I would personally be happy with a decent IPS monitor because the games won't care if I'm playing on a nice monitor or not, but they will care what hardware I'm playing on. So I would go with the 5070/60 Ti and a decent monitor. The B580 will struggle more with a nicer OLED panel anyway due to the more demanding specs.",buildapc,2025-09-06 17:06:32,1
Intel,ncrdtnj,A B580 won't be able to properly show off such a nice monitor because it will run so slowly and you won't be able to turn the graphics settings up.,buildapc,2025-09-06 17:07:13,1
Intel,ncre5wd,I mean i can test the oled and refund it if its not good enough,buildapc,2025-09-06 17:08:56,1
Intel,ncredaf,I mean with some ai FG i have 80 frames with high/cinematic settings in BMW its not that bad,buildapc,2025-09-06 17:09:59,1
Intel,ncreq00,"You can, but you'll end up with initial disappointment because you don't know what you want yet. I'm giving you advice that will help you figure out what you want. But I guess I can't force you to take it. Good luck on your search.",buildapc,2025-09-06 17:11:44,1
Intel,ncrezdr,Thanks for Gl ill propably test it and refund if its not good enough,buildapc,2025-09-06 17:13:03,1
Intel,n92pgwe,"Everybody was talking about it when it came out. At MSRP, it’s a good value   It was just a bitch to get because it sold out instantly everywhere. I’m sure that’s changed now. It’s a good purchase   It also requires a decent CPU, that’s the only issue",buildapc,2025-08-16 21:29:37,85
Intel,n92s3u3,They make up something like 2% of the total GPU market.  You don't see a lot of talk about them because so few have them.  They are ok for what they are.,buildapc,2025-08-16 21:44:51,14
Intel,n92oj13,They’re really good for what they are and are pretty universally well received.   They’re budget tier cards with a limited lineup that simply don’t have the same recognition and acclaim as Nvidia or AMD.,buildapc,2025-08-16 21:24:13,12
Intel,n92qk9a,"The Intel arc GPUs used to be avoided because of the drivers. Some games straight up don't work with them, some had way worst performance than the competitions. Intel's driver has improved leaps and bounds, it's a great one to get unless you have a really dated CPU.",buildapc,2025-08-16 21:35:56,31
Intel,n93o943,"I just got one and it’s awesome, $395 AU and I got a nice increase at 1080p. Made it very cheap to upgrade!   I think my end game is to try get the next line of graphics card at early retail price, or just hop on the next Intel… tbh done me great. The only problem I have had so far was tabbing out of helldivers caused a few glitches on my mates end for the 15 seconds I was gone.",buildapc,2025-08-17 01:03:11,4
Intel,n935ovh,"Their A series gpus had some major issues alongside drivers still being pretty raw. The a310/380/580 have some niche uses in video encoding or for driving 1080p displays.   The B series cards have turned out to be really good budget gpus for 1080/1440p. The drivers have come a long way since launch, theyre still not as refined as Nvidia but theyre mostly stable now. They both require more modern platforms to perform well but not the top end hardware alot of reviews claim they need. The b580 has had alot of issues staying in stock since it offers 12gb of vram at a price point that almost exclusively offers 8gb cards.  Ive had my b580 since around launch. I got it on launch day for MSRP and aside from an unstable driver update i had to roll back its been a great upgrade from the 1070 i had previously. If you play on 1080/1440p displays at high settings theyre good cards",buildapc,2025-08-16 23:07:06,9
Intel,n947yxo,"Intel arc a310 is one of the best cards for transcoding videos, can handle multiple 4k streams. I love mine, using it for plex server and immich",buildapc,2025-08-17 03:09:07,3
Intel,n92ogae,It's good,buildapc,2025-08-16 21:23:47,5
Intel,n92oiup,No they aren’t. The b580 in particular is praised for its great value at its price range,buildapc,2025-08-16 21:24:11,2
Intel,n930cbr,As long as your CPU and motherboard supports RE-Bar it's a great option for 1080p/1440p for the price,buildapc,2025-08-16 22:34:04,3
Intel,n93bvyj,Idk but I take it as a good sign because people will only post if there is something wrong with the product. This also applies to other subreddits.,buildapc,2025-08-16 23:45:13,1
Intel,n93txb5,"They're good and I have one that I use for a rig that only plays games like esport titles and multiplayer steam games. They aren't the best for any type of AAA game.  The bigger deal is that intel has announced some pretty heavy changes and cuts coming and there is good speculation that intel's gpu sector might be cut. If it doesn't happen, it's no big deal, but if it does get cut or restricted, any type of updates or driver updates that are needed for future releases will probably be unlikely to come out. That's probably the only reason I wouldn't recommend it to anyone, but for the price its great for someone who only enjoys current multiplayer games or indie games.",buildapc,2025-08-17 01:38:44,1
Intel,n953vyb,"Intel Arc has physical hardware that works ok, but it's not high performance yet.  But MOST importantly, this is a new company in terms of graphics drivers and industry cooperation in software development. So what? NVIDIA and AMD partner with game developers to optimize graphics, sample and super-sample textures in AI for optimization for best performance. This is an ongoing development of specialized software and tools to improve performance to the maximum with the given hardware. This software is NOT available to Intel and they will have to spend years catching up to their competition.  In short, Intel graphics cards will work fine for low demand popular programs. They may eventually get there for high end performance, but it will take years for them to catch up.",buildapc,2025-08-17 07:29:05,1
Intel,n93yzaj,"Have you considered 9060 xt at all? There has been a price decrease recently, you might be able to get one for the same price as a b580 (8gb model 9060 xt).  Except for the vram department, the 9060 xt is pretty much better in every way and performs better too.  Was on the same boat with you. The b580 was $360 CAD, and the 9060 xt 8gb was $380. After going through countless benchmarks (especially at 1440p), I eventually decided to pick the 9060 xt sapphire.   In only like 1 or 2 games the b580 scored higher, and everything else was in 9060 xt’s favour. Idk much about the driver issues on the b580, but at least the 9060 xt seemed like the safer bet. I wish it was at least 10gb though…. Amd really missed an opportunity. Otherwise this gpu is a beast! I’ve never seen an 8gb gpu do so well at 1440p. Amd did score with that.",buildapc,2025-08-17 02:09:47,1
Intel,n93udpj,They are good for 1080p gaming also if you have a really low budget and want to game. For slightly more money you can pick up a 9060xt 16gb which is good for 1440p and has better long term returns.,buildapc,2025-08-17 01:41:28,1
Intel,n93qv6o,"They're okay, if you hate yourself get an Intel Arc GPU otherwise get an AMD Radeon GPU or if you have the money to burn literally and figuratively speaking an Nvidia GPU.",buildapc,2025-08-17 01:19:51,0
Intel,n92t8e9,I think better price to performance can be found on a used card.,buildapc,2025-08-16 21:51:23,-1
Intel,n93bgu5,"Has driver issues with more dated and/or less popular games, but they've made it a point that major AAA titles and esports games usually have good drivers. It also basically requires a fairly beefy CPU and it will at least slightly negatively impact the CPU performance too. But when I say fairly beefy I basically just mean AM4 5000 series or basically any AM5 CPU even the 7500f, I personally don't know the Intel comparison but I imagine any Core Ultra or 12th gen would absolutely crush it and be fine.  But the B580 is basically competing in the 60 series tier and is alright, but definitely if you can get it for MSRP or within I'd say even $30 over MSRP it's a steal. Something of note too is at least from my experience and seeing benchmarks, the Arc GPUs normally do this split the middle kind of thing, where they tend to outraster/render Nvidia GPUs of their tier, but get outrastered/rendered by AMD GPUs of their tier, but on the other end XeSS upscaling/frame gen performance and raytracing usually outperforms the comparable AMD GPUs, but loses to the comparable Nvidia GPUs. Although FSR4 and the better raytracing on the 9000 series might've changed that, a lot of those comparisons were from when we were still comparing to the RTX4000 and RX7000 cards.",buildapc,2025-08-16 23:42:39,0
Intel,n93dl7l,B series is getting a bit old now. Eagerly awaiting what the C series will bring.,buildapc,2025-08-16 23:55:45,0
Intel,n94a4jq,No idea about the drivers but B570 and B580 which released some months ago are very good value and also very efficient.,buildapc,2025-08-17 03:24:16,0
Intel,n94duvw,"The B580 is actually very good for a $250 card as it performs similarly to the 4060 and more importantly has 12GB memory instead of 8GB. Even today it's still better value compared to the 5050 at the same price.  The main issue with the card is availability in certain regions, which impacts it's price. Where I'm from (Bangladesh) availability is actually pretty decent and so most B580s can be had for at or near the $250 msrp. The other issue with the card is it's driver overhead, which mostly shows up on the AM4 platform where the performance delta can sometimes be so massive that the card ends up slower than the 3060 12GB.  The issue is far less rampant on AM5. It performs around the 4060/5050 on a cheap R5 7500F, and is already beating the Nvidia cards in terms of value. It can still go faster if you pair it with the fastest CPU on the planet, but not by much and I personally don't think it's that big of a deal.   Alternatively you can play at 1440p and eliminate the driver overhead; the card has the performance and memory for it anyway, so why not. You can even go all the way upto 4k Balanced with XeSS 2.1 which is actually pretty good and supports frame gen. Kinda insane for a $250 card if you ask me.  Driver support and game compatibility is nowhere near as bad as Arc Alchemist, and people have even played the Battlefield 6 Beta on day 1. Older games such as those on DX9 or DX10 may still struggle compared to a 4060/5050, but the card is so fast you should still get highly playable framerates. We're talking 150 fps instead of 300fps, big deal.",buildapc,2025-08-17 03:50:52,0
Intel,n92yskg,No one buys them because people truth internet myths even from years ago but the reality is that those are very good gpus and near to MSRP are a great value.,buildapc,2025-08-16 22:24:43,-1
Intel,n939hns,"They're decent GPUs but for gaming yo'ull want something a lot more powerful, like an RTX 5070 or RTX 5070 Ti.  [https://www.youtube.com/watch?v=VcKPJvGNr0c](https://www.youtube.com/watch?v=VcKPJvGNr0c)",buildapc,2025-08-16 23:30:30,-2
Intel,n93cwvi,Nobody talk about the A series anymore. Is it still worth to buy?,buildapc,2025-08-16 23:51:32,3
Intel,n9ay9g8,"You must think all of us can afford Nvidia. In Europe you guys probably can, but in America all of us have millions in debt from medical bills and we make minimum wage!",buildapc,2025-08-18 05:56:13,0
Intel,n92ra8x,So for exemple an 7500f is ok ?,buildapc,2025-08-16 21:40:06,8
Intel,n93y470,"> unless you have a really dated CPU.  Read: more than a generation or two old. So, not actually dated at all, which is why the overhead issues with Battlemage were so criticized.",buildapc,2025-08-17 02:04:19,5
Intel,n944jl6,"I ran an a750 in Linux until Monday. It worked well.  Rarely had problems.  I just wanted more vram so I got a 9060xt. If Intel had an upgrade path for the a750, I would have bought one.",buildapc,2025-08-17 02:46:03,1
Intel,n94qd93,Idk if I'd call Nvidia refined either.   I was still using the 566.36 driver until the Battlefield 6 beta made me update and since then I've encountered the flickering display and sudden black screen issues a few times that have been going on with pretty much every driver version since the 50 series came out.,buildapc,2025-08-17 05:25:48,1
Intel,n99smmg,"Rebar isn't all it needs. It needs a good CPU. I bought one to replace a 7600, paired with an i5-10400. After extensive testing it didn't really offer any improvement, and even performed worse in CPU-heavy/multiplayer titles. I thought the overhead issue was overblown, it isn't.",buildapc,2025-08-18 00:52:07,1
Intel,n935f0h,It really cant,buildapc,2025-08-16 23:05:24,3
Intel,n92v3ab,Like which one ?,buildapc,2025-08-16 22:02:20,1
Intel,n93y7iv,B series launched literally under a year ago.,buildapc,2025-08-17 02:04:56,2
Intel,n93dshg,Depends. For gaming? Not really. Better bargains. But for a home server the a310 is the champ of media encoding,buildapc,2025-08-16 23:57:00,26
Intel,n9xi0z3,"In gaming, the B series will generally be better in pretty much every way. It just runs faster, cooler, and cheaper. Outside gaming it gets a bit weird, but actually favors A series. The 310s are *very* good at media encoding for the price. The A770 can also outperform the B series in some non gaming tasks (generally its inferior, but it can get 4gb more vram and has a wider bus, which can make it better, or sometimes close enough that it being older and thus cheaper can make up the difference.) For gaming, B series is strictly better",buildapc,2025-08-21 17:40:26,1
Intel,n9449nv,The b580 is close to the a750 in performance. Not worth buying that. The a770 is better but I wouldn’t recommend them now with the amd 9060xt out.,buildapc,2025-08-17 02:44:14,-1
Intel,n9azast,"The median net worth in the US is 200k and the US accounts for more of Nvidia’s revenue than the rest of the world. Speak for yourself, big bro.   Regardless, what even prompted you to say this? I’m just answering OP’s question about Intel’s popularity lmao",buildapc,2025-08-18 06:05:56,1
Intel,n92rprb,Totally fine.,buildapc,2025-08-16 21:42:35,16
Intel,n94srky,"Given you dont need to run DDU or (in some cases) do a fresh install of Windows to get the cards to run decently id argue theyre pretty well refined.   Even compared to AMDs drivers Nvidias just tend to work without much effort, from what ive been seeing it sounds like bf6 has had a ton of issues regardless of which gpu youve got. Having a set of rough updates off the back of 20+ years of development doesnt discount the amount of time theyve had to work on getting things working well. Intels 1st dgpus came out in 2022 IIRC",buildapc,2025-08-17 05:46:16,0
Intel,n937vuc,Sure it can. You can get a 3070 at the cost or a 3070 Ti for a little more and it beats the B580.  I just sold my 3070 Ti for $290 a month ago because that's all people were willing to buy at. Some sold the card for even less.,buildapc,2025-08-16 23:20:40,-2
Intel,n96o38v,"A used b580 is better price to performance, they’re just kinda speaking arbitrarily",buildapc,2025-08-17 14:48:12,1
Intel,n9381sa,3070 or 3070 Ti. One may cost more. I sold my 3070 Ti for $290 and others are selling for less.,buildapc,2025-08-16 23:21:41,0
Intel,ncc73oq,"Wrong, the B580 outperforms the A770 in every case i know.",buildapc,2025-09-04 08:17:13,0
Intel,n93t9zb,You’re comparing the used market to the new market. You can get a used B580 for $170 on ebay as the first listing I see.,buildapc,2025-08-17 01:34:47,3
Intel,n981kxq,"Not arbitrary with my assumption they want to spend $250 on a new B580. I don't know for sure they wanted to go new, but in case it was their plan, I'm saying instead of spending $250 on a new one if that's what their plan is, just spend $250 on a used card which will have better performance. It's really common sensical as to what I am saying. Don't understand what is arbitrary about that.",buildapc,2025-08-17 18:56:23,1
Intel,ncczq47,"Wrong. The A770 has more VRAM and works better for AI/ML workloads.  That's why they're releasing other cards to help with loading models.    As for gaming performance, the A770 and B580 are very close in most benchmarks.  It's game dependent which one does better, but in scenarios more vram is needed the A770 wins.  12GB isn't terrible for 1080p/1440p workloads but it's just OK.",buildapc,2025-09-04 12:16:14,1
Intel,n944bb3,"Indeed, that was my intent and my assumption that OP was going to go for new. $250 spent on a used card is spent better than on a new B580.  I don't think this point is invalid to the point where it should be downvoted.",buildapc,2025-08-17 02:44:33,1
Intel,n949c44,"Ok… but if OP is willing to buy used cards, $170 is better spent on a used B580 than $250 on a 3070 as you can spend the extra $80 to get other components that are better as well",buildapc,2025-08-17 03:18:40,2
Intel,n94a2m5,"Sure this is true, but I was only talking about going for a brand new B580 versus a used GPU. I naturally assume at such a low price point that people will go brand new on that GPU rather than used.  You can argue different scenarios with a different good answer, but what you stated wasn't exactly relevant to my assumption.",buildapc,2025-08-17 03:23:52,1
Intel,n94b4w6,"Yes, but your assumption is inherently flawed is all I’m pointing out.",buildapc,2025-08-17 03:31:29,1
Intel,n94g29o,"No, my assumption isn't flawed when I had no evidence to suggest that OP was going used on that card. How can I possibly know what OP intends on doing without it having been explicitly stated? I would personally expect them to go for a new card and not used as the card is already cheap enough to get it new (without considering budget constraints, but they also never stated any details on a budget and other essential info).   Hindsight and what comes after the comment was made is different, but my comment was based entirely on a very specific scenario. I think it's clear from my comment that I was specifically discussing differences between a new B580 and a different used card. I wasn't talking about anything else.",buildapc,2025-08-17 04:06:31,1
Intel,n94iyut,"Your comment here already dismantles the scenario you thought of. You claimed you assumed they would be going for a new card. This assumption automatically means that going for a used card is out of the question, so comparing the new card to the used card doesn’t make sense. If OP were to be considering going used on cards, why would they not consider going used on the B580? The problem is that you didn’t think to expand the logic you presented beyond one specific card, which is inherently flawed.",buildapc,2025-08-17 04:28:42,1
Intel,ncbqor8,For what purpose? Because if gaming is your goal this is not the play unless you’re refusing both AMD and Nvidia graphics for ideological reasons.,buildapc,2025-09-04 05:39:25,2
Intel,ncbrhxz,"So my goal is to get you to a RTX 5070 which requires $280 of reallocation.  - Downgrade the CPU to a 9600X. $160 saved  - You have three M.2 slots on your motherboard, you don’t need 4TB to start. Drop to 2TB $130 saved.  - Replace the Arc with an RTX 5070  Alternatively you can make a less severe CPU cut with the 7700X ($60 saved) and get the other $100 from your peripheral budget because frankly you don’t need $500 for all that. You could also downgrade the case to something with less fancy rgb (Montech XR, Phanteks XT View) but I understand some folks are very passionate with their case selection.",buildapc,2025-09-04 05:46:38,0
Intel,ncbtxjb,It is a little cheaper and I do a little bit of video editing on the side!,buildapc,2025-09-04 06:08:42,1
Intel,ncbu0ld,"This is honestly really good advice, I might make the jump to a 5070! Thank you!!",buildapc,2025-09-04 06:09:30,1
Intel,n7vint8,"The first release might be sold with a new PC. If you live in US, you might as well go to China and buy one there (or one of their online stores). Tarif and scalpers might jack up the price by a lot. This is not Intel, but he went to Hong Kong to buy the rare 5050 and found a few of them easily. https://youtu.be/-jXMaS8e8oo?si=6M1XiK54SrVDI3h9",buildapc,2025-08-10 02:11:28,2
Intel,n7xtw3n,"For the 48GB Maxsun dual variant of the Pro B60, I was quoted $3,300 New Zealand Dollars (NZD) per GPU. Not available in retailers, only to business customers through limited suppliers. The supplier who services my area doesn't even have a website.   For comparison, the RTX PRO 5000 with 48GB is $8,800 NZD, and the cheapest RTX 5090 is around $4,650 NZD at the moment.   I was interested initially because of the RAM in the dual Maxsun card, but was hoping for a price closer to $2,000 NZD.",buildapc,2025-08-10 13:44:43,2
Intel,n7f7mhv,Why specifically the Intel when plenty of GPUs aren't going to interfere with AI.,buildapc,2025-08-07 14:16:23,1
Intel,n7xqndu,That's an interesting idea :) I'm not in the US but going to China to buy some hardware sounds very cool! Maybe I'll do it some day. Thank you.,buildapc,2025-08-10 13:24:52,1
Intel,n7xxnpt,"My idea was to buy a singular one now and another one later when I have the money for it. But dual variants look cool too if you can afford it. $2000 USD sounds too much though. I feel like the supplier in your area is jacking up the prices since it is difficult to find them.  I asked a friend of mine who does business hardware deals. He couldn't even get a quote. I guess it is not even available on many regions now.  Anyway, thanks for the info :)",buildapc,2025-08-10 14:06:54,1
Intel,ndm8q50,"I was waiting on this, meanwhile bought 5070Ti Vanguard - with vapor chamber and I am really surprised how Nvidia moved with AI workload on this gen, so Intel is no go, It's even better for me to have Scout locally in VRAM and RAM when 5070Ti has so much performance - in FP4 it's lighting speed. Waiting on 5070Ti super - 24GB card with almost 1TBps VRAM bandwidth.",buildapc,2025-09-11 11:35:53,1
Intel,n7fl1xe,24GB VRAM for 500$ sounds super nice and not offered by any other GPU manufacturer. Plus they advertised a card with dual B60s (48GB VRAM total) sharing one PCI-E x16 slot (x8 each).,buildapc,2025-08-07 15:20:46,1
Intel,ndmcddo,"I went down a similar route and ended up with a 5090. Less RAM than the Maxsun card, but the performance is insane.",buildapc,2025-09-11 12:00:47,2
Intel,n7fs5xm,"Then price the GPUs that have Tensor cores. I'm just wondering because Intel GPUs aren't traditionally for gaming, though that is slowly changing",buildapc,2025-08-07 15:54:41,0
Intel,n7hhge2,"Tensor cores is not the only thing that matters when it comes to performance. B60 performance was looking pretty good in the demos but now there is nothing on the internet, no benchmarks, no prices, no people with crazy connections showing off a hardware others cannot buy, nothing.",buildapc,2025-08-07 20:46:19,1
Intel,n7ismbe,You have a very narrow vision of speed. What you are saying is like your truck is faster than my audi because it has more horsepower. Nvidia is the top dog right now but that doesn't mean you get more from nvidia for 200$. VRAM size and speed is something you cannot ignore. If you have to load half of the model to system memory it is gonna decrease your speed significantly. And guess what? If you can't fit the model into your memory you won't be running it even if you have a billion cuda cores. This is why intel is offering 24GB VRAM for 500$ while you need to pay 2500$ for the same VRAM size in nvidia (rtx a5000 for example).,buildapc,2025-08-08 01:06:31,0
Intel,neyo3le,"B60 is similar performance like 5060 in language of HW numbers. In language of TOPS it's slow, because CUDA... and FP4 optimisation of 5xxx cards... No, you really don't want to screw around for days to make OpenML or how is theirs ""engine"" called working...",buildapc,2025-09-18 21:36:43,1
Intel,n7ifcir,"But compared head to head with an nVidia or AMD/ATI of the same price and the Intel is not the top performer, last I checked.   I looked into this recently because I was thinking about upgrading.  $200 for that versus $200 for an nVidia/AMD and the non-Intel GPU was a slightly better performer.",buildapc,2025-08-07 23:48:23,0
Intel,n7ifjl8,"But compared head to head with an nVidia or AMD/ATI of the same price and the Intel is not the top performer, last I checked.   I looked into this recently because I was thinking about upgrading.  $200 for that versus $200 for an nVidia/AMD and the non-Intel GPU was a slightly better performer.",buildapc,2025-08-07 23:49:35,0
Intel,n7j6hep,"No i don't have a narrow vision of speed. Maybe ask questions first. Like i said, i read reviews, Intel didn't break through. Maybe I'm incorrect, but i could care less. Reviews change daily. And i could find a dozen that support my opinion. Not like i care. Intel said it wasn't primarily intended for gaming.  What's worse is your rudeness.",buildapc,2025-08-08 02:30:55,1
Intel,n7jo11a,"Yeah, sure mate. You do you.",buildapc,2025-08-08 04:31:45,1
Intel,nh466vt,"If you can afford to step up to the 9060XT 16GB, it's much better. It's about 30-40% faster on average and has 4GB more VRAM. Not to mention better driver support and access to a better upscaler with FSR 4.   Why are you considering going down to the B580?",pcmasterrace,2025-10-01 01:53:58,2
Intel,nh4blra,The b580 is a great tinkering machine or amazing if you're building for video editing first but for a PC gamer who has to ask I'd avoid it. It's fine if you literally only play new releases a month after release but for freshly released stuff or anything before like 2022 it can get rough. It's kind of like being in a really cool beta nobody's sure if Intel will commit to,pcmasterrace,2025-10-01 02:26:44,2
Intel,nh4kj5l,The B580 is great if your CPU can handle it. Judging by the pricing you gave in that other comment i'd say it could be worth it. That all depends on your CPU though.,pcmasterrace,2025-10-01 03:25:47,1
Intel,nh4woga,"i will go with B580, save your money to buy something else, since you are playing FHD then yes B580 is a better value",pcmasterrace,2025-10-01 04:57:53,1
Intel,nh49boe,"The biggest reason was the price, I saw an arc b580 for R$1800, compared to the 16g 9060xt which I was only finding for around R$2800",pcmasterrace,2025-10-01 02:12:37,1
Intel,ngzu7py,i'd go with the RX 9070 more VRAM better drivers better performance its more money but should last you longer,pcmasterrace,2025-09-30 12:22:18,15
Intel,ngzv59y,Bro that thing looks less like a PC and more like it’s about to open a portal to hell,pcmasterrace,2025-09-30 12:28:12,5
Intel,ngzu1xj,9070!!,pcmasterrace,2025-09-30 12:21:16,3
Intel,ngzvcow,"If you can afford it, the 9070 is way faster than the B580. It's not even close. And I think pairing it with an 11700K is totally fine. That's still a decently fast CPU.",pcmasterrace,2025-09-30 12:29:29,2
Intel,ngzvtsp,"That is legitimately one of the coolest PCs I've seen, such a shame it's nearing the end of it's life...  You BETTER build a worthy successor of it!!!",pcmasterrace,2025-09-30 12:32:28,2
Intel,nh0hyaz,man i playing on 1440p and 4k on a 9070. get that and thank yourself.,pcmasterrace,2025-09-30 14:33:21,2
Intel,nh00tsp,The 9070 XT is 1000 times better... Intel is dead 5 years ago...,pcmasterrace,2025-09-30 13:02:05,1
Intel,ngzvdkc,"So, put it in my current build first then build the other other one and swap over?",pcmasterrace,2025-09-30 12:29:38,3
Intel,ngzv973,Sick right? 😁 lmao,pcmasterrace,2025-09-30 12:28:52,2
Intel,ngzvfss,"Thanks, that's what I was concerned about",pcmasterrace,2025-09-30 12:30:02,1
Intel,ngzvy82,"Yes I know, I'm very easy to please just with Star wars stuff",pcmasterrace,2025-09-30 12:33:15,3
Intel,nh0ixmz,Think my current system can handle it?,pcmasterrace,2025-09-30 14:38:14,1
Intel,nh01b5l,Rip 🙏,pcmasterrace,2025-09-30 13:04:52,1
Intel,ngzvmir,yeah i know some people still rocking 11th gen intel you should be fine with the new GPU now and then a full upgrade later,pcmasterrace,2025-09-30 12:31:12,5
Intel,ngzxgty,You and me both,pcmasterrace,2025-09-30 12:42:34,3
Intel,nh1ezbz,with the 1050? fuck no,pcmasterrace,2025-09-30 17:13:54,1
Intel,ngzxdww,"Awesome good to know, thanks 👍",pcmasterrace,2025-09-30 12:42:04,2
Intel,nh1fzme,No I'm replacing it. The 1050. With the 9070. Can my 11700K and ddr4 ram work with the 9070?,pcmasterrace,2025-09-30 17:18:35,1
Intel,nh2oe2u,"that should be good depending on the game. cpu intensive games can be tough, but if youre planning on 1440p and/or 4k the gpu gets taxed alot more anyway. the 9070 is a great card, ive owned one for about 4 months now and im very impressed with it. theres some OC headroom too.",pcmasterrace,2025-09-30 20:51:28,2
Intel,nfp0bny,When they inevitably do a refresh they better call it necromancer,pcmasterrace,2025-09-23 00:59:54,64
Intel,nfs63wp,"Assuming the thing competes with a 5070 it's going to be bigger than a 5080 (the 4060 and B580 trade blows  the Arc is 70% bigger).  There's no way they can get any decent profit selling a 400+ mm2 die for 500$ (probably even less once the 5070 super launches). It will probably end up like the B580, a paper launch with most of the cards actually selling for 650$.  They're better just selling those dies to professionals with 32GB of VRAM.",pcmasterrace,2025-09-23 15:26:37,-4
Intel,nfp0j9c,Born from the bin of nvidia igpu deal resurrected to kick ass,pcmasterrace,2025-09-23 01:01:18,13
Intel,nfve4m2,"If Arc isn’t shelved by N maybe they will. Next up are Celestial and Druid, hopefully.",pcmasterrace,2025-09-24 01:14:39,1
Intel,neybsse,Rip gpu market,pcmasterrace,2025-09-18 20:35:52,405
Intel,neyh6sc,https://preview.redd.it/b50eap3fmzpf1.jpeg?width=1078&format=pjpg&auto=webp&s=aebc215f68b6c6444a8e247dfed48d9eed711dd0,pcmasterrace,2025-09-18 21:01:43,104
Intel,nexs2wg,"That’s the last fucking thing I want, an Intel cpu with an integrated Nvidia gpu. Horrible.",pcmasterrace,2025-09-18 19:02:35,480
Intel,neymuhj,"[Intel and AMD have done this in a NUC before.](https://www.tomshardware.com/reviews/intel-hades-canyon-nuc-vr,5536.html)  Edit: My educated guess is that this comes from Nvidia's ARM APU, and ARM Windows just isn't there yet. So, to move it to an x86 platform Intel would be the logical choice to partner with to move the APU to x86 Windows.",pcmasterrace,2025-09-18 21:30:13,88
Intel,neyk0pn,All I know is we gonna lose,pcmasterrace,2025-09-18 21:15:48,25
Intel,neyns8w,I have been saying for quite some time Intel will abandon Arc. They abandoned the GPU market in the past.  It is past embarrassing at this point what Apple offers with the integrated GPU in the M series of cpus and what Intel delivers.,pcmasterrace,2025-09-18 21:35:05,18
Intel,nexbwg5,Let Intel Suffer for all the BS they did with their CPU division while they were a Monopoly and even after they declined still continued their BS  But i like Intel GPU division,pcmasterrace,2025-09-18 17:45:22,203
Intel,neyzaxk,Even before this announcement Intel Arc faced an uncertain future. Even their most successful GPU the B580 at the very least had razor thin margins if not outright being a loss leader.  Intel simply ran out of both time and money and they can't keep bleeding money on their GPU division with the company's current situation. If Intel actually managed to execute and follow their initial roadmap and release Alchemist GPUs during the COVID pandemic we'd probably be looking at a very different Intel GPU division but here we are now.,pcmasterrace,2025-09-18 22:37:35,7
Intel,nezo4ea,I don’t necessarily think that it’s that dramatic. The Arc GPUs are aimed at a market that Nvidia no longer competes in (the budget GPU space). If anything you’d think Nvidia would be more supportive of the Arc stuff getting off the ground because it means they can dedicate more fab space to producing the higher end stuff that’s in demand for AI and industry use and commands better margins.,pcmasterrace,2025-09-19 01:02:23,5
Intel,nez5cy1,I wonder if this is specifically to go after the handheld pc gaming space where amd basically has a monopoly.,pcmasterrace,2025-09-18 23:11:58,3
Intel,neyukuo,"This is great for people who just want a gaming system, no AI. Cheap motherboards with decent integrated graphics would be great for a lower income. (Most of the gamers I know)",pcmasterrace,2025-09-18 22:10:58,7
Intel,nf006uq,Someone is cashing out for both INTC and NVDA.  Intel traded a shitty incompetent CEO for a conniving one.,pcmasterrace,2025-09-19 02:11:15,2
Intel,nf1d7ot,I truly hope Intel won't drop their GPU division.,pcmasterrace,2025-09-19 08:36:01,2
Intel,nf12j87,"Such a weird thing going on in this thread. Did people forget when AMD acquired ATI it was to make a complete package APUs and for both respective companies to compete with Intel and Nvidia.   Now Nvidia probably wants to do same thing in the CPU market and intel gets the benefits of being with Nvidia in the GPU market.  Only reason it did not happen sooner was because Intel was still king marketwise then.  Intel could have easily acquired Nvidia in 2006 when AMD acquired ATI, or even in 2011 when AMD stopped using ATI branding.",pcmasterrace,2025-09-19 06:50:39,3
Intel,nezkod7,NOOOOOOOOOOOOOOOOOOOO!   Whatever,pcmasterrace,2025-09-19 00:42:39,1
Intel,nf0r6sm,:(,pcmasterrace,2025-09-19 05:12:45,1
Intel,nf0xt8w,Low end vs high end. I think it will be fine.,pcmasterrace,2025-09-19 06:08:17,1
Intel,nf3m939,If only intel aggressively sell their GPU in third world countries but no they only focussed in America where except 10 redditors everybody buys nvidia,pcmasterrace,2025-09-19 16:53:47,1
Intel,neygu5y,Should have done it with amd instead of this dying pig,pcmasterrace,2025-09-18 21:00:02,-13
Intel,nez4q8d,Arc was never even a player.,pcmasterrace,2025-09-18 23:08:22,-87
Intel,ney9a0z,Then surely no one will buy it.... right?,pcmasterrace,2025-09-18 20:24:04,107
Intel,neybvtp,"I think it would be pretty sweet, if they could cook up something like the apples M series.",pcmasterrace,2025-09-18 20:36:16,39
Intel,neynzmg,Uh...why?  Nvidia have consistently had the most power efficient GPUs on the market for years now. That technology in an APU? That's promising stuff.  It wouldn't surprise me if we start to see 4050/4060 performance on a single chip. Imagine what that means for handhelds.,pcmasterrace,2025-09-18 21:36:09,20
Intel,neyj252,"Speak for yourself, that sounds fucking great.",pcmasterrace,2025-09-18 21:10:58,15
Intel,nf2b476,That actually sounds amazing for handhelds,pcmasterrace,2025-09-19 13:00:01,1
Intel,nf1pl8t,"its for ai accelerators with built in CPUs, not apus",pcmasterrace,2025-09-19 10:33:33,0
Intel,nf1qc1k,Unfortunately a majority of gamers/pc users in general don't give two shits about the specifics of their hardware.,pcmasterrace,2025-09-19 10:39:46,0
Intel,nez9kqc,"Why? AMD makes inferior products in every way compared to Nvidia, except for GPU power connectors.",pcmasterrace,2025-09-18 23:36:47,-6
Intel,nf3qid6,"kinda, Kaby Lake G wans't really an APU, it was just a CPU and a GPU on the same package.",pcmasterrace,2025-09-19 17:13:52,1
Intel,nez1fht,Imagine if Apple suddenly turned round and decided to make a Mac for gaming 🤣,pcmasterrace,2025-09-18 22:49:48,4
Intel,nexy0is,"Companies aren't your friends, AMD being on top means they can pull the exact same shit that Intel did in the past.",pcmasterrace,2025-09-18 19:31:12,180
Intel,nf07qv4,"How would they suffer? Intel isn't a living entity, and the executives that led the downfall of Intel left, who is gonna suffer except intel's employees and us?",pcmasterrace,2025-09-19 02:55:31,3
Intel,nezbneq,"> But i like Intel GPU division    The problem with the Intel GPUs is they are clearly struggling with the technology. The B580 needs a much bigger die than equivalent Nvidia cards, meaning it must be expensive for Intel to make the chips and uncomfortable to sell them at such a low price.   That might be ok initially to get a foot in the door for their GPU brand, but if they can't get the efficiency up then from a business point of view they just can't continue it.",pcmasterrace,2025-09-18 23:48:57,6
Intel,nf0eju6,Nvidia still sells gt710 and and rtx3050 for the low end,pcmasterrace,2025-09-19 03:39:22,2
Intel,neyx0sw,"Make no mistake, this will be marketed for AI before gaming",pcmasterrace,2025-09-18 22:24:34,21
Intel,nf1r5wt,"Yes but AMD didn't have a GPU division when they bought ATI last I checked, what became of ATI is AMD's Radeon division, CPU or GPU both remained with two major players. The problem here have here is that Intel IS a player in the GPU space, they are the opportunity to have a 3rd company for extra competition but Arc is still in a vulnerable stage where it could easily get axed if it isn't bringing in more money than what they spend. Nvidia investing and putting in RTX iGPUs is the sort of thing that could devalue Arc iGPU investment.",pcmasterrace,2025-09-19 10:46:39,2
Intel,nf1l84f,Maybe they want competition. Unlike NVIDIA now wants to have 100% share to everything. Monoply at its finest,pcmasterrace,2025-09-19 09:54:46,1
Intel,nezeg4u,"People may not like it but you're right, they essentially had a 0% market share the quarter following the release of their Battlemage GPU's",pcmasterrace,2025-09-19 00:05:34,48
Intel,nf06vjg,"whats with the downvotes lol, dudes right, fellas time to step outside yer echo-chamber & touch some grass",pcmasterrace,2025-09-19 02:50:27,6
Intel,nfkejum,They could have been.,pcmasterrace,2025-09-22 08:34:14,1
Intel,neycvkr,Already sold out,pcmasterrace,2025-09-18 20:41:04,176
Intel,nezjoun,"Woops, it's in every laptop ever.",pcmasterrace,2025-09-19 00:37:03,15
Intel,nf07b8t,"Intel's mobile CPUs are alright, maybe this could be a responsive to AMD's gigantic APU.",pcmasterrace,2025-09-19 02:53:06,5
Intel,neyhvny,Nvidtel: best we can do is Kaby Lake-G.,pcmasterrace,2025-09-18 21:05:07,31
Intel,nezjnpq,"Never gonna happen. NVIDIA will put that kind of effort into their own CPU but not an Intel one. This is doomed from the start, we’ve seen it before several times when big tech companies “collaborate” and the product just languishes and dies. The PowerPC CPU architecture and OS/2 leap to mind.",pcmasterrace,2025-09-19 00:36:52,3
Intel,nf0lpxi,"AMD fanboy - thats why. Doesn't like the ""enemies"" joining forces.",pcmasterrace,2025-09-19 04:30:26,-2
Intel,nezg9jy,"That's roughly the power on the amd ai 395+ max whatever the fuck that chip is called. It's mostly ai oriented, not gaming, but the apu has 4060 or above performance in most games.  The real question is whether nvidia would make such an investment for handheld devices which they dont seem to care for all that much, or mabw they just want their foot in the ai tablet market where amd has taken the lead.",pcmasterrace,2025-09-19 00:16:30,0
Intel,nezkfmt,"I was obviously speaking for myself. In English, we use the “I” to refer to ourselves. I would have used the word “you” if I were talking about you.",pcmasterrace,2025-09-19 00:41:19,8
Intel,nezx94z,Lol why? Intel sucks at making CPUs now.,pcmasterrace,2025-09-19 01:54:37,0
Intel,nf6d95r,"It's both, [per Nvidia and Intel's own joint press release](https://nvidianews.nvidia.com/news/nvidia-and-intel-to-develop-ai-infrastructure-and-personal-computing-products?ncid=so-twit-672238):  >For data centers, Intel will build NVIDIA-custom x86 CPUs that NVIDIA will integrate into its AI infrastructure platforms and offer to the market.  >**For personal computing, Intel will build and offer to the market x86 system-on-chips (SOCs) that integrate NVIDIA RTX GPU chiplets. These new x86 RTX SOCs will power a wide range of PCs that demand integration of world-class CPUs and GPUs.**  Which, personally, I think would be an interesting product line.",pcmasterrace,2025-09-20 01:46:50,2
Intel,nf2cs1e,He said with his 3080 Ti that draws 100 W more than a 6900 XT for the same performance,pcmasterrace,2025-09-19 13:09:18,1
Intel,nf4jh28,"Yeah, I was just saying Intel has worked with direct competitors before.",pcmasterrace,2025-09-19 19:35:34,2
Intel,nf1vah7,"Their computers can already technically game, just at worse performance than expected. Most developers don't want to develop their games to include direct support for Metal in addition to Vulkan, Directx and/or whatever system consoles use, and Apple doesn't want to directly open their OS to Vulkan or Directx. Everything needs to go through metal in some way.",pcmasterrace,2025-09-19 11:18:39,2
Intel,nexzrd1,"True like now that they are at the Top where are Ryzen 3? Where are CPUs for the Budget Gamers? Im have money so i dont need ryzen 3.  But still Intel deserves what they did and even AMD isnt as dumb to do this:  \-Force Mobo Upgrade every CPU Gen   \-Limit Users too 4 Cores 8 Threads   \-Deliver minimal Generational Upgradea while doubling the Price   \-Rebrand Old CPUs with new names and increase Power draw so much it destroys itself   \-Limit/Exclude Tech like ECC, PCI-E Lanes for CPUs under 500€   \-Israel",pcmasterrace,2025-09-18 19:39:40,62
Intel,ney6d8w,"Yup, I'm someone with an all AMD build, and Intel going down the shitter and the possibility of Arc being axed is not something I'm happy about at all.   Even if I was one of those weirdos with diehard brand loyalty and didn't care about a monopoly/duopoly being solidified, it's still going to suck for me as an RDNA3 user because Intel is still the only only making an (officially supported) upscaler for my card that isn't completely dogshit.  There's still no guarantee for official FSR 4 support on RDNA 3, and if that never happens and XeSS gets axed, I'll effectively be stuck with the god awful FSR 3 for any multiplayer games I can't use Optiscaler on.",pcmasterrace,2025-09-18 20:10:34,6
Intel,nez81nn,like when was the last time AMD increased the core count on their desktop lineup y.y  they kept R5/R7/R9 at the same core counts since introduction,pcmasterrace,2025-09-18 23:27:45,1
Intel,nf2213q,They already are. zen 3-4's uplift came almost all from node change,pcmasterrace,2025-09-19 12:05:04,1
Intel,neyokn0,"I know American companies like to shit the bed, does Chinese tech companies have the same history?",pcmasterrace,2025-09-18 21:39:09,1
Intel,nf1zwq4,tell me how Intels Numbers are doing. Why do they need the state too pump money into them now?,pcmasterrace,2025-09-19 11:51:09,1
Intel,nezgjz1,They made enough profit while delivering the most minimal Generational upgrade back when they were the Monopoly. They should have invested that money into the GPUs instead of a Paycheck for the CEO and A Paycheck for Israel and building a Plant in Israel.  Let them suffer,pcmasterrace,2025-09-19 00:18:17,7
Intel,nf1knns,Once those are gone they’re gone.  Nvidia won’t waste TSMCs limited wafer capacity on budget chips at this point.,pcmasterrace,2025-09-19 09:49:31,1
Intel,nf0e9tp,Yeah Nvidia needs a cpu for their AI servers that’s better than their own arm chips. That’s probably what this is about,pcmasterrace,2025-09-19 03:37:29,4
Intel,nf2rido,Why would any company want competition?  Competition is only beneficial for the consumer.,pcmasterrace,2025-09-19 14:26:28,1
Intel,nf075l5,"They were actually eating some of AMD's shares, Nvidia didn't budge",pcmasterrace,2025-09-19 02:52:09,24
Intel,nf0qyve,"You’re not wrong, but that’s not entirely the point. It was a sign that a big company like Intel was willing to dump significant resources into R&D in a new and heavily dominated market segment. The A series was a good if not sloppy start (general driver & performance issues, many were fixed over time), and the B580 was an amazing bang for your buck card. They were improving fast in the technology, and while they are still on larger dies than NVDA & AMD they were offering cost-performance in the budget/mid range sector that the others weren’t.  TLDR more competition is pretty much always good for the market",pcmasterrace,2025-09-19 05:10:58,21
Intel,neymm62,Site crashed,pcmasterrace,2025-09-18 21:29:01,47
Intel,neypejf,Already at 37% user base in steam charts topping any and all AMD gpus somehow.,pcmasterrace,2025-09-18 21:43:24,46
Intel,neyydt9,14nm++++++++++++++++++,pcmasterrace,2025-09-18 22:32:17,14
Intel,nez24ga,I've been calling those builds (IRL) as Intel-Vidia,pcmasterrace,2025-09-18 22:53:41,5
Intel,neyw2r8,![gif](giphy|fngnOeui0oSdSwjZ9K|downsized)  Infidel,pcmasterrace,2025-09-18 22:19:15,1
Intel,nezlw1o,"""I"" could tell you were speaking for yourself. In English, ""speak for yourself"" is an idiomatic expression meaning that the person  replying disagrees with the prior statement, typically appended with their point of view.  EDIT: OP of the comment can't take a joke and delete everything.",pcmasterrace,2025-09-19 00:49:36,2
Intel,nf2fgni,"Yes, they have done for a few years. It all started with the 14+++++++ fiasco. Their fabs are in a bad way and their CPUs are not where they should be.",pcmasterrace,2025-09-19 13:23:56,1
Intel,nf2v71n,"Who really cares about power consumption? It doesn’t matter unless you need to upgrade your power supply. Especially if you have enough money to buy a high end gpu, an extra 100 W during the few hours a day max that you’re gaming isn’t going to matter at all. 0.1 kWh * 3 hours * $0.16 = $0.048 extra per day. Oh no!!!  Also, even if the 6900 XT had better performance, I wouldn’t have bought it still because of the extras that Nvidia has. Things like Nvidia broadcast, CUDA support, and DLSS, just to name a few.",pcmasterrace,2025-09-19 14:44:21,1
Intel,nf80q76,">  and Apple doesn't want to directly open their OS to Vulkan or Directx. Everything needs to go through metal in some way.  Without buying out MS apple cant support DX. (and even if they did the flavor of DX used by PC titles is not compatible with apples GPUs).  And while apple could support VK, remember VK is not HW agnostic, a PC VK backend (the small number to titles that have these) will be written expliclty to target AMD/NV GPUs and would require rather large changes to run well (or even at all) on a VK driver for apples GPUs on macOS.    The work needed to add a metal backend to a modern engine (if it does not already have one... most do) is not that large.  All modern engines are written to be mutli backend enabled.  The amount of a games engine that directly calls a graphics apis is intently tiny since we need this to run very fast within the Redner loop, idealizing keeping the entire loop within L1 Cpu cache.     Adding metal support to an engine is not were the work is.  All the other changes, user input, file system, Audio, networking, digital signatures, color spaces (Mac users expect proper HDR)... this is all way more work than adding metal.    (also most engines already have metal backends).     from a graphics side of things there is a LOT of work but this work is API agnostic and relates to making changes to target the HW (you would need to do this regardless of API as all the modern apis want to remove runtime overhead of the driver reputedly making these optimizations over and over again on each frame).",pcmasterrace,2025-09-20 10:09:11,1
Intel,ney7fp8,barely anyone bought the ryzen 3s when they were a thing. budget gamers just bought last gen ryzen 5s because of the higher core/thread counts and better performance.,pcmasterrace,2025-09-18 20:15:36,45
Intel,ney6v2b,"Both are publicly traded companies and their sole purpose is pumping up share price up as high as it will go. AMD will definitely take the foot off the gas with Intel continuing to crumble, keep sales up and fatten margins. Intel had to push power draw to try and keep up on performance as they were stuck on an inferior node. Everything else is well on the table for AMD until another company challenges their lead.",pcmasterrace,2025-09-18 20:12:55,18
Intel,ney9a25,We dont need r3s for budget cpus when older amd r5s are just as cheap with similar performance.,pcmasterrace,2025-09-18 20:24:04,3
Intel,ney4sv9,"You must not remembered AMD’s socket A, socket 754, socket 939, and socket 940…  And AMD was limiting users to dual cores before Intel came out with C2Q, and selling dual cores at ridiculous prices before Conroe.  If you’re looking for morally better companies, AMD ain’t it.",pcmasterrace,2025-09-18 20:03:10,11
Intel,nez5iqi,what did intel do in Israel?,pcmasterrace,2025-09-18 23:12:54,2
Intel,neyml53,didn't intel just come out with the Core Ultra 110 which is literally just a 10th gen cpu that they are trying to trick us into thinking is new?,pcmasterrace,2025-09-18 21:28:52,1
Intel,nf22w93,Don't need a new ryzen 3 model when an older gen r5 is dirt cheap.  And who is in the market for a €70 cpu on a €200 motherboard? I like AMD's approach of keeping AM4 for systems with low requirements and AM5 for the high-end stuff.,pcmasterrace,2025-09-19 12:10:38,0
Intel,nf288rr,Mayhaps they'll open source XeSS if/when that happens,pcmasterrace,2025-09-19 12:43:31,1
Intel,nf1t6d0,"Nope, you’re thinking of the highly sought after 4 and 5 nm TSMC fabs. The 3050 is build on Samsung 8nm and gt710 is build on 28nm.  Which does not have high demand and thus is also much cheaper to manufacture on",pcmasterrace,2025-09-19 11:02:28,1
Intel,nf3ohut,"To deliver that price/performance they had to sell at cost, the B580's die is 20% bigger than a 5070 and Intel is selling them for less than half the price. Heck, the thing competes with a 4060 and it's 2x the size.  As long as they can't get even remotely competitive on die size they'll be stuck in this situation and personally I'd rather have them spending money on figuring their shit out on the CPU side to avoid  a monopoly instead of selling at cost low end GPUs than don't work well on low end CPUs.",pcmasterrace,2025-09-19 17:04:20,-1
Intel,nf0r7m9,Already reselling at 160% MSRP,pcmasterrace,2025-09-19 05:12:56,9
Intel,nez47dp,I like that!,pcmasterrace,2025-09-18 23:05:23,2
Intel,nf4av3i,"He didn't delete anything, he just blocked you. Comments will be shown as deleted to you.",pcmasterrace,2025-09-19 18:52:38,1
Intel,neznjo8,Yeah idioms like that are for speaking in person not typewritten on social media. It’s a meme to make that mistake.,pcmasterrace,2025-09-19 00:59:02,-5
Intel,nf3902e,"> Who really cares about power consumption?  Hey, you're the one who said ""*every* way"" 😎  > Things like Nvidia broadcast, CUDA support, and DLSS, just to name a few.  DLSS for sure, it's black magic and it might even be worth the $200 premium over the 6900 XT to some. I have a 4080 that I got used, but even at MSRP vs MSRP I would have probably picked it over a 7900 XTX just for DLSS.  The other stuff though, do you actually use it?  ^((also as someone who's owned a 4080 and a 6800 XT within the last 2 years, AMD drivers and auxiliary software are *way* better... but DLSS is so freaking good man)^)",pcmasterrace,2025-09-19 15:49:56,2
Intel,ney9cev,"Oh i did not know this. I thought most of them were normies who had no idea what, what meant so they just bought the cheapest one that works",pcmasterrace,2025-09-18 20:24:23,3
Intel,ney8x2n,I do love AliExpress,pcmasterrace,2025-09-18 20:22:25,3
Intel,ney9n26,AMD has nothing to do with Israel but Intel does. So already a MASSIV plus for AMD,pcmasterrace,2025-09-18 20:25:47,7
Intel,ney9vnc,Yeah i didnt think people would do this. But this is good,pcmasterrace,2025-09-18 20:26:55,1
Intel,nf0a4vi,"I remember having the motherboards exploding, just in time for the cpu upgrade anyway during the early 64 bit transition. Damn those crappy capacitors!",pcmasterrace,2025-09-19 03:10:07,2
Intel,ney7hyp,difference is AMD has nothing to do with Israel,pcmasterrace,2025-09-18 20:15:53,-17
Intel,nezg82i,"many things. Including investing in Israel with and Plant and manufacturing Chips, GPS and other stuff for B\*mbs",pcmasterrace,2025-09-19 00:16:15,1
Intel,neyojha,They did that more than once. 7th Gen Intel and 14th Gen Intel are the once i can quickly think of,pcmasterrace,2025-09-18 21:39:00,2
Intel,nf24zx8,"You do know you can get AM5 Motherboard for as low as 70€ new? And a 8400F for 120. Not even 200€ for AM5 Platform.   Thats the dumbest take i have ever heard. ""AM5 only for High end stuff""",pcmasterrace,2025-09-19 12:23:50,2
Intel,nf2060z,They’re no longer being manufactured which is the point.  All 40 and 50 series GPUs are manufactured by TSMC and they won’t waste TSMC capacity on budget GPUs with low margins.  It remains to be seen what will happen with Nvidia’s recent investment in Intel.,pcmasterrace,2025-09-19 11:52:52,1
Intel,nf4l9m1,"272 mm² B580   263 mm² 5070   According to tech power up.   Die size is not really that important at this stage, they need a better architecture and that will lead to better performance for the die size.   Let's also not pretend like NVIDIA is selling at thin margins, they're profiteering. And were is your source they sell at a loss?",pcmasterrace,2025-09-19 19:44:37,4
Intel,nf0wbbv,That's a bargain!,pcmasterrace,2025-09-19 05:55:17,1
Intel,ney9vye,yep. amd has pretty much replaced the ryzen 3 tier with their last gen ryzen 5 cpus because of how cost effective they are.,pcmasterrace,2025-09-18 20:26:58,17
Intel,neybfcy,AMD has an R&D facility in Israel. Every big company has something to do with Israel.,pcmasterrace,2025-09-18 20:34:05,17
Intel,neyg8tb,AMD sold their fabs to a Saudi consortium. I’m sure that’s way better.,pcmasterrace,2025-09-18 20:57:08,11
Intel,nf3df4u,"I can’t find any sources online saying the 3050 is discontinued. They discontinued the 8gb and replaced it with a 5th revision 6GB some time ago? which appears to be current and still manufactured. There’s never been a time in the last 20 years I’ve been watching the gpu market Nvidia has not filled the lowest tier with older generations of cards. If there’s a shortage of manufacturing capacity at the bleeding edge, they can just use older manufacturing processes so there’s no reason not to fill lower tiers if people are buying them. This is what AMD does for their lowest tier CPUs, they use older manufacturing processes, like 7nm and 14nm",pcmasterrace,2025-09-19 16:11:11,1
Intel,nf4qrbm,"Die size is important, it's the main driver of the cost. Also the point is not the size itself, it's the fact that it performs like a 150mm die from the competition so it has to be priced accordingly.  Also I never said they're selling at a loss, I said at cost. It's sensible to assume that at 250$ the B580 is at cost or at a really thin margin, the VRAM alone is 80-100$.",pcmasterrace,2025-09-19 20:11:41,0
Intel,neyhkbx,Now that Israel attacked Qatar they have been talking with the Middle East talking about arming themselfs against Israel. The Saudis have joined that talk. Things are changing,pcmasterrace,2025-09-18 21:03:34,-7
Intel,nf4rmzj,"Sure but Intel isn't losing because of die size, they're losing because theyre 4 years behind AMD and Intel   It's not sensible because we have nearly zero insider info on the modern silicon market operates.",pcmasterrace,2025-09-19 20:16:04,0
Intel,nf0twdp,"TLDR:  > ""We’re not discussing specific roadmaps at this time, but the collaboration is complementary to Intel’s roadmap and Intel will continue to have GPU product offerings,"" Intel said in an emailed statement to HotHardware.",pcmasterrace,2025-09-19 05:34:54,14
Intel,nf0dm6z,wouldn't it be something if we ended up with a hybrid arc/rtx card.   I could see Nvidia throwing some of the tech at Intel helping them build things up so that they don't get slapped with a monopoly,pcmasterrace,2025-09-19 03:32:58,23
Intel,nf18g0t,"intell will make next 60 series with 12gb vram, lol.",pcmasterrace,2025-09-19 07:47:52,4
Intel,nf1z9p8,"Honestly, I don’t see much reason for them to abandon the discreet GPU market. Most of these collaborations are mutually beneficial to both companies but I would suspect the Intel is still going to try and muscle into the AI accelerator GPU market which also means relatively easy to keep doing graphics cards for gaming just like Nvidia is not going to give up their ARM based APU ambitions.   Now I think Intel may shutter the GPU division anyway because they are basically rounding error on the current discrete GPU market, but I do not think it’s a foregone conclusion based on this collaboration with Nvidia.",pcmasterrace,2025-09-19 11:46:50,3
Intel,nf052sz,But the dooomers!!!! what will they be mad about now? considering they didn't even read any of the articles on this colab/plan....what will they be upset about now? Back to Borderlands 4 memes I guess.,pcmasterrace,2025-09-19 02:39:47,-30
Intel,nf0lex4,I could see they just letting Intel run DLSS,pcmasterrace,2025-09-19 04:28:06,8
Intel,nf0la9m,"the article says nothing useful tho, just that they will continue to provide GPUs for now.",pcmasterrace,2025-09-19 04:27:08,13
Intel,nf1kvr6,"What did you expect to hear from Intel?   ""Oh yes, we're now abandoning the dGPU sector. Those architectures that we've promised and are almost finished, they're not coming. Also, if you bought an Intel dGPU, well, too bad for you.""  Instead they gave a vague statement, saying they will continue with their current roadmap, which means that yes, we should probably see Xe3 and maybe Xe4... But what after it?",pcmasterrace,2025-09-19 09:51:36,1
Intel,nf0ufz6,I am ok with anything at this point. I want to see AMD/Intel be competitive against Nvidia competition is always a good thing.,pcmasterrace,2025-09-19 05:39:25,3
Intel,nf0nxdr,"They said it's complimentary to their roadmap and they'll continue to offer GPUs. ARC isn't going away because this partnership on SoCs isn't in direct conflict discrete graphics cards. It's targeting products that utilize SoCs, not dGPUs.  But I guess people gotta dig deep into what was said to extract what they want to believe in. But as is tradition here, we can already presume that whatever 'hot take' this sub has, the reality will be complete opposite.",pcmasterrace,2025-09-19 04:47:03,-10
Intel,nf1haqy,"This would be kinda the opposite of that though… it would be basically NVIDIA reskinned.  Edit: by “this” I meant the situation if NVIDIA were to simply supply Intel with chip or chip designs. Not the article in the post, that instead would be the best for the industry.",pcmasterrace,2025-09-19 09:16:37,6
Intel,nh413l5,"I've just known that both ARM and AMD use ""Gill Sans Std"" for brand logo, ""Ryzen/RyzeUP"" for ryzen and radeon logo.",pcmasterrace,2025-10-01 01:23:29,1
Intel,nh42yb8,There are some websites that can scan images and identify fonts for you. But maybe some of these are bespoke,pcmasterrace,2025-10-01 01:34:29,1
Intel,neidjom,Isn't the B580 just straight up faster? It'll also be supported for longer and should have the major teething issues solved being a second gen product versus first gen.,pcmasterrace,2025-09-16 12:02:25,18
Intel,neiyrxz,The B580 is about 10% better than the A770 for any game in which VRAM is not the bottleneck.,pcmasterrace,2025-09-16 14:03:11,6
Intel,nejilno,"Get the b580, I have a a770 and I want to upgrade, 12gb vram enough for 1080p but 8 is not.its also a bit faster.",pcmasterrace,2025-09-16 15:39:46,3
Intel,neitadj,"Just a side note here, you may want to opt for the G.Skill Flare instead of Ripjaws as it's EXPO for AMD CPUs.",pcmasterrace,2025-09-16 13:34:33,2
Intel,nen6azd,B580 is more mature. I just got one and so far it's decent.,pcmasterrace,2025-09-17 03:15:43,2
Intel,neqnti8,I'd go with the B580. 12GB vram is sufficient.,pcmasterrace,2025-09-17 17:31:56,2
Intel,nek0rln,"The answer is pretty simple, do you want more VRAM or a stronger card?",pcmasterrace,2025-09-16 17:07:16,1
Intel,neiedyt,"The A770 is 16gb VRAM but the B580 is 12gb, isn't that something to consider?",pcmasterrace,2025-09-16 12:07:58,-2
Intel,neiua3b,"Actually It's a gift with the motherboard, i was planning to sell it anyway :)",pcmasterrace,2025-09-16 13:39:53,3
Intel,nenz85t,Makes no difference. Xmp can be used on amd boards and vice versa with zero downsides. You will even find xmp only kits in QVL lists for amd mobos.,pcmasterrace,2025-09-17 07:15:17,1
Intel,nekpkcs,Obviously stronger.,pcmasterrace,2025-09-16 19:05:26,1
Intel,neif5d7,"You're not gonna be playing at resolutions and texture qualities that would need more than 12GB of VRAM with either of these cards.  But yes, if you're a fiend for max texture quality and 4K, the A770 would probably be able to run the latest and greatest broken UE5 games at a steady 20fps, while the B580 would run out of memory and drop to 10fps.",pcmasterrace,2025-09-16 12:12:52,12
Intel,nekpzwg,Then the answer is obvious,pcmasterrace,2025-09-16 19:07:31,2
Intel,neigl78,"I don't play to go crazy right now just because I saw the Witcher 4 trailer or GTA 6. There are tons of games I haven't played and I kinda wanna play those games before considering, to recap I could buy the B580 now and upgrade to something better in 2-3 years.   Thanks for the insight!",pcmasterrace,2025-09-16 12:21:54,3
Intel,neio28l,">to recap I could buy the B580 now and upgrade to something better in 2-3 years.  Absolutely. You can direct also upgrade the CPU without swapping the board or RAM.   Just don't cheap out on the PSU, get something that's at least B tier and at the very least 650W capacity (though 850W units aren't significantly more expensive and you'll be able to keep using that for multiple generations of your PC)  [https://docs.google.com/spreadsheets/d/1akCHL7Vhzk\_EhrpIGkz8zTEvYfLDcaSpZRB6Xt6JWkc/edit?gid=1973454078#gid=1973454078](https://docs.google.com/spreadsheets/d/1akCHL7Vhzk_EhrpIGkz8zTEvYfLDcaSpZRB6Xt6JWkc/edit?gid=1973454078#gid=1973454078)",pcmasterrace,2025-09-16 13:05:51,5
Intel,neir4i1,"Absolutely! I am planning to get an MSI MAG 750W 80+ Gold, although I am seeing Be Quiet as an alternative option.",pcmasterrace,2025-09-16 13:22:46,2
Intel,ngu0ubf,That 68 fps on the rtx 4080 super is just upsetting for me.  Tbf i was just in the first two borderlands for Handsome Jack... The others didn't hit the same...,pcmasterrace,2025-09-29 14:31:14,37
Intel,ngv4sr2,Why is the 7900 XTX getting stomped by the 9070? They are usually neck and neck? This have heavy RT?,pcmasterrace,2025-09-29 17:44:58,9
Intel,ngz1bt2,"The game is most definitely not a graphical showcase, where the hell is the performance going ?",pcmasterrace,2025-09-30 08:14:28,3
Intel,ngvkb2a,"I'm playing with a 9070XT in 1440p with FSR Quality and getting \~80-90 FPS on average.  The setting you want to reduce is ""Lighting quality"" from Very High (badass) to Medium and you [save more than 25% FPS](https://youtu.be/11XZL0VinHY?t=799) for almost no discernable difference. They really messed up with how balanced some settings are.  It's one of the most stable game I've played this year, no stutter at all (shaders are compiled at first start). But I've seen people say they have a different experience.  I feel like a lot of people are overreacting because Randy is an idiot XD.",pcmasterrace,2025-09-29 18:58:28,10
Intel,ngx6tok,"Those B580 numbers are whack. I’ll check my settings but I get 80FPS at 1440p with XeSS quality. (Edit: I double checked, I get 80FPS at 1440p High, XeSS Quality)",pcmasterrace,2025-09-30 00:01:43,2
Intel,ngwi6u3,"For what it's worth my 4070 system can generally hit 4k 60fps (w vsync) with dlss performance and settings on medium with some on high. No FG.   The 4080 I can get 4k 100-110fps with dlss balanced and mostly high settings w FG.   The 4090 rig I hit 140fps at 3440x1440 with dlss quality w everything maxed except volumetric fog/cloud, direction shadow quality and foilage density are on high, the 4090 is also undervolted.  Game is pretty playable overall, performance is okay not great but no where near as bad as reddit would make you think.",pcmasterrace,2025-09-29 21:43:49,2
Intel,ngw4p1r,"Crazy that the Twitter account had the B580 as recommended.  Other than that, what's considered playable nowadays? Usually I expect 60fps for single player or PvE games. Which doesn't seem hard to get based on these metrics.  Doesn't anyone remember that new games always tax the best hardware? Like, I remember when Crysis came out and it was nearly impossible to play at max settings without the best of the best.  Randy is an idiot and ass, and the game needs to be optimized better, but this doesn't seem like the huge deal people are making it out to be.",pcmasterrace,2025-09-29 20:37:01,3
Intel,ngvkxdx,Does anyone know where the 3090 sits on this chart?,pcmasterrace,2025-09-29 19:01:26,1
Intel,ngw1m6s,Okay? But what are the in game graphics settings at?,pcmasterrace,2025-09-29 20:22:42,1
Intel,ngxis0t,Those frame rates look okay to me.,pcmasterrace,2025-09-30 01:11:32,1
Intel,ngxtmpt,I get better performance in bl4 than I do in aw2 or other demanding titles from 2 years ago.,pcmasterrace,2025-09-30 02:15:28,1
Intel,ngxvo5l,"Brutal, but not brutal enough to stop people from forking over money.",pcmasterrace,2025-09-30 02:27:58,1
Intel,ngxxw82,Yikes so best I can expect 5090 on a 5k2k is probably in the 80s. Wow.,pcmasterrace,2025-09-30 02:42:05,1
Intel,ngzk207,UE5 is CPU heavy lags and stutter varies of CPU performance. larger stutter spikes on slower CPUs.,pcmasterrace,2025-09-30 11:11:48,1
Intel,ngvu848,"BL4 loves RDNA4, well ""loves"" is *slightly* more playable lmao.",pcmasterrace,2025-09-29 19:46:44,1
Intel,ngw2wtq,"the insanity is still that it is with ALL the upscaling possible even in performance mode, so this 2560x1440 just plain lying. it is more like 960p or less. so yeah.. we can no on a 5090 run games in 960p at decent framerates..  saw the 3 vs 4 compare, and the game looks the same, just sucks performance wise..",pcmasterrace,2025-09-29 20:28:43,1
Intel,ngx4h8n,Thanks. Now can I see results for graphics cards people actually have? Only one 30 series on here and its the 3060 Ti? That is not the most common...,pcmasterrace,2025-09-29 23:48:12,1
Intel,ngw2z06,Gross looks like my card would be getting like 50-60fps with 3440x1440. Think I will keep waiting.,pcmasterrace,2025-09-29 20:29:01,0
Intel,ngvzo98,This is a computerbase.de bench not mine. Thanks,pcmasterrace,2025-09-29 20:13:19,-1
Intel,ngvw6i3,"Normally the 3090 has performance above the 9070 non xt and close to 9070xt (I think) but with unreal engine doing better with Radeon, I don't know.",pcmasterrace,2025-09-29 19:56:12,-2
Intel,ngue3uw,You might like the Pre-Sequel then! It goes in depth in explaining Jack's origin story.   And has lots of cool stuff. I liked the game a lot honestly.,pcmasterrace,2025-09-29 15:37:15,11
Intel,ngvu33g,But that’s at 4K? Sounds about right to me,pcmasterrace,2025-09-29 19:46:03,1
Intel,ngu1qci,"I really like BL4, and I wish it ran faster. I have to use framegen with balanced FSR",pcmasterrace,2025-09-29 14:35:50,-4
Intel,ngvps8b,Maybe newer drivers or specific optimization? I think it does have somekind of lumen rt?,pcmasterrace,2025-09-29 19:25:08,2
Intel,nh1fw8t,Could be FSR4 vs FSR3,pcmasterrace,2025-09-30 17:18:09,1
Intel,nh3hbas,my guess is it loads in the entire region all at once and all the background effects & shit probably chugs the UE5 engine. other UE5 games ive had doesnt have as much of a hit than this game,pcmasterrace,2025-09-30 23:27:34,1
Intel,nh1vb48,There are a lot of particle effects and visual clutter everywhere. I play on my 3070 with DLSS performance 1440p with roughly 60fps. Absolutely playable and enjoyable.,pcmasterrace,2025-09-30 18:31:22,-1
Intel,ngvu1me,"This is it, I also have an XTX and it applies to the older generation too.",pcmasterrace,2025-09-29 19:45:52,4
Intel,ngwa7ra,"Seconding this. 6700XT on 1440p ultrawide, getting stable 60 fps (locked fps in settings) just by doing this plus turning volumetric fog and cloud down to the lowest. I also threw FSR on since everyone's recommending DLSS for Nvidia cards.   Having a blast. Only very rare stutters and those only started with the latest patch.",pcmasterrace,2025-09-29 21:03:13,1
Intel,ngyyha4,All of my Nvidia friends keep saying when they load somewhere it's like 50/50 they have to compile shaders for some reason. I've only had to do it once at the start on my 9070xt.,pcmasterrace,2025-09-30 07:45:30,1
Intel,ngvuy0g,"No discernible difference is definitely not true. In any environment consisting of a low light area with light sources such as wall lights, floor lamps, neon signs, etc, there is very noticeable grainy and bouncy lighting effects emitted from these sources that look horrible when the setting is not maxed. The game was built around it being maxed unfortunately. I still would drop it to medium but you do lose a large amount of visual fidelity.",pcmasterrace,2025-09-29 19:50:14,-3
Intel,ngvv282,Randy is an idiot. I agree that the gameplay is very smooth on 9070XT. People saying different might be on Nvidia cards. This is very well documented.,pcmasterrace,2025-09-29 19:50:48,-3
Intel,ngvu723,Yeah people are really overreacting to this lol do people not have gsync displays??,pcmasterrace,2025-09-29 19:46:35,-6
Intel,ngwbmxd,No people dont remember they also dont remember that there were games released that were buggy as fuck and there was no way to patch it on old consoles.   Or all the Movie License crap Games.   They only remember the 10 games that they loved as children and therefore Gaming has gone to shit.  There has always been slop there always will be.  In 20 or so years people will post on whatever new site there will be and praise the 2020s for their banger releases and again forget the shit ones.,pcmasterrace,2025-09-29 21:10:16,2
Intel,ngw4kbs,[https://www.computerbase.de/artikel/gaming/borderlands-4-benchmark-test.94463/seite-2](https://www.computerbase.de/artikel/gaming/borderlands-4-benchmark-test.94463/seite-2),pcmasterrace,2025-09-29 20:36:24,1
Intel,ngylwvo,True,pcmasterrace,2025-09-30 05:45:48,1
Intel,ngvxm1z,"I have a theory that might be because the drivers for AMD are less processor heavy than Nvidia's. So a game like this, which is very heavy on the processor, punishes AMD less on that front.",pcmasterrace,2025-09-29 20:03:11,0
Intel,ngwntch,No. The 3090 is usually around 4070 super/5070 type performance. 9070 is usually around a 4070ti super/7900xt type performance. The 9070xt is usually around a 5070ti/7900xtx/4080 and sometimes 5080 type performance.,pcmasterrace,2025-09-29 22:13:37,1
Intel,ngukpbz,It uses 15gb max at 4k native,pcmasterrace,2025-09-29 16:09:09,6
Intel,ngvuu0z,Not going to do a thing. Learn what memory is and why it matters.,pcmasterrace,2025-09-29 19:49:42,2
Intel,ngvws7k,3440x1440,pcmasterrace,2025-09-29 19:59:07,-14
Intel,ngw46ea,IIRC it uses Lumen everywhere because there's no baked lighting fallback (same as Doom Dark Ages and mandatory RTGI).,pcmasterrace,2025-09-29 20:34:38,5
Intel,ngvxucl,"Didn't notice grainy/bouncy(?)/flickery lighting effects, which I've been seeing in a lot of Lumen stuff. I'll try to compare High vs Medium tonight but as far as I can see in Hub's video, medium is only darker because there's less GI bounces.",pcmasterrace,2025-09-29 20:04:19,1
Intel,ngw1e9y,"It’s likely downstream of differences in shader compute - UE5 moves many (but not all) visual features over to running entirely on compute, rather than raster/RT pathways, and a huge portion of frametime is now spent here on stuff like nanite, VSMs and software lumen. This is quite different from how most other engines have historically behaved frame-to-frame. It creates an opportunity for divergent performance characteristics from what we might ordinarily expect between IHVs; RDNA is often ahead in graphics-concurrent async compute, which could be contributing here to overall performance and frame health, but there are a lot of architectural differences potentially at play.  We see this behavior still at very high resolutions and lower framerates, which indicates it’s not generally a driver overhead thing (as a primary cause - it definitely contributes though, increasingly so with higher framerates)",pcmasterrace,2025-09-29 20:21:40,1
Intel,ngvzch7,"Possibly, and compared to RDNA3 it can show that a lot more due to the RT improvements.",pcmasterrace,2025-09-29 20:11:43,0
Intel,ngwu84n,You are right,pcmasterrace,2025-09-29 22:50:00,0
Intel,ngvxafk,It’s your own chart lol it has 67.6 average FPS for the 4080 super at 4K,pcmasterrace,2025-09-29 20:01:36,4
Intel,ngvy6n5,Go to zadras hideout and walk out toward the threshers. There are a lot of areas here where it is more noticeable,pcmasterrace,2025-09-29 20:06:00,1
Intel,ngxu4v4,"Yh, not sure why you get downvoted, mistakes do happened.",pcmasterrace,2025-09-30 02:18:30,2
Intel,nguzi3b,"There is more to performance than VRAM, you’ve been deceived by the shitposting",pcmasterrace,2025-09-29 17:20:08,9
Intel,ngvze19,It's computerbase.de chart. I should have put the link,pcmasterrace,2025-09-29 20:11:56,-10
Intel,neopavx,"consider a platform that is alive or get a used mobo + cpu if on a strained budget, while there is stronger cpus on lga1700 they are plagued by degradation issues. Highly depends on use case too, but a budget and country you're planning to build in would make it easier to give advice.",pcmasterrace,2025-09-17 11:19:48,3
Intel,neomnrn,"Not terrible for a low-end gaming PC. But don't cheap out too much on the motherboard; in general if it doesn't have any radiators on VRMs around the CPU socket, it will struggle to run anything hotter than a 12400 in the future.",pcmasterrace,2025-09-17 10:59:57,1
Intel,neosbji,"A series intel GPUs aren't good for someone starting out, they're a first generation product with first generation issues. B series have solved a lot of the issues, but I wouldn't really recommend that either.  12100F is a pretty slow chip for any serious gaming. You have some upgrade potential, but it's limited to chips that are already now 2 years old and have serious degradation issues. And you're even more limited if the motherboard is of the same budget range as the CPU, as it won't run any seriously powerful 12-14th gen i7.",pcmasterrace,2025-09-17 11:41:26,1
Intel,ngo8wyv,Its obvious. Learn ho to manage the cables,pcmasterrace,2025-09-28 16:29:29,2
Intel,ngrcm71,Must cable manage.,pcmasterrace,2025-09-29 02:08:49,1
Intel,nebegcv,"Could just be a typo, BMG-G21 die vs BMG-321 die",pcmasterrace,2025-09-15 09:11:23,1
Intel,ndefpms,you get free bf6 code if you buy intel 14th gen right? or is it just select retailers/online sellers,pcmasterrace,2025-09-10 05:57:47,3
Intel,ndequu8,"B580's completely fine. I throw them out in the <£700 budget builds. They don't tend to come back, they pass 48 hr burn ins fine, drivers are solid. I'm sure 99% of them spend their entire lives playing Fortnite, Roblox, and YouTube, mind.  I don't know if there's anything different on an Intel CPU, nobody really uses or wants those old things anymore, almost every build this year has been a genuine AMD except some nutcase who wanted a 265K.",pcmasterrace,2025-09-10 07:42:53,3
Intel,ndesefm,"They are good GPUs, but you can get them in the £220-250 range. If you are looking at £300-320 options you will be able to find better GPUs.",pcmasterrace,2025-09-10 07:58:10,2
Intel,ndefw6r,"I believe so, with overclockers UK you get BF6 with the 14600k and the B580",pcmasterrace,2025-09-10 05:59:23,2
Intel,mz2hn4c,"What a disgusting build, I love it",AMD,2025-06-21 23:44:28,160
Intel,mz2c56w,the content we crave,AMD,2025-06-21 23:11:17,81
Intel,mz2taf0,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",AMD,2025-06-22 00:56:32,48
Intel,mz35qhi,What GPU are you using in your build?  All of them,AMD,2025-06-22 02:15:29,16
Intel,mz34fmt,you're one hell of a doctor. mad setup!,AMD,2025-06-22 02:07:07,5
Intel,mz38u8t,The amount of blaspheming on display is worthy of praise.,AMD,2025-06-22 02:35:37,3
Intel,mz4f388,Brother collecting them like infinity stones lmao,AMD,2025-06-22 08:29:44,5
Intel,mz4ibrt,I'm sure those GPUs fight each others at night,AMD,2025-06-22 09:02:18,4
Intel,mz4o6eq,Bro unlocked the forbidden RGB gpus combo,AMD,2025-06-22 10:01:39,4
Intel,mz3lb45,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,AMD,2025-06-22 04:02:59,3
Intel,mz419ab,What the fuck,AMD,2025-06-22 06:15:48,3
Intel,mz520aa,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,AMD,2025-06-22 12:03:18,3
Intel,mz8w6af,Yuck,AMD,2025-06-23 00:36:46,3
Intel,mz3q5i1,Wait until you discover lossless scaling,AMD,2025-06-22 04:40:21,2
Intel,mz4pnpm,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,AMD,2025-06-22 10:16:23,2
Intel,mz4vx72,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",AMD,2025-06-22 11:13:47,2
Intel,mz57f8x,Now you just need to buy one of those ARM workstations to get the quad setup,AMD,2025-06-22 12:42:21,2
Intel,mz5dj5p,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,AMD,2025-06-22 13:22:04,2
Intel,mz65vu4,Love it lol. How do the fucking drivers work? Haha,AMD,2025-06-22 15:55:37,2
Intel,mz6knzs,What an amazing build,AMD,2025-06-22 17:11:07,2
Intel,mza30vq,wtf is that build man xdd bro collected all the infinity stones of gpu world.,AMD,2025-06-23 05:11:08,2
Intel,mzdg22n,You’re a psychopath. I love it,AMD,2025-06-23 18:23:11,2
Intel,mzeff3z,This gpu looks clean asf😭,AMD,2025-06-23 21:12:27,2
Intel,mzf9oh7,The only setup where RGB gives more performance. :D,AMD,2025-06-23 23:54:00,2
Intel,mzgj5a3,Now you need a dual cpu mobo.,AMD,2025-06-24 04:36:20,2
Intel,mzjl4ek,Placona! I've been happy with a 6700xt for years.,AMD,2025-06-24 17:04:15,2
Intel,ng0v4qd,absolute cinema,AMD,2025-09-24 21:52:34,2
Intel,mzaqf4v,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",AMD,2025-06-23 08:51:27,1
Intel,mz3qf7i,"Brawndo has electrolytes, that's what plants crave!",AMD,2025-06-22 04:42:29,47
Intel,mz2vfon,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",AMD,2025-06-22 01:09:53,20
Intel,mz3a7jh,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",AMD,2025-06-22 02:44:38,15
Intel,mz3f8hm,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",AMD,2025-06-22 03:18:58,3
Intel,n031c2v,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",AMD,2025-06-27 15:50:28,1
Intel,mz3fahp,Team RGB,AMD,2025-06-22 03:19:20,16
Intel,mz775k1,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",AMD,2025-06-22 19:03:06,5
Intel,mz3q4dh,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",AMD,2025-06-22 04:40:06,12
Intel,mz5nt69,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",AMD,2025-06-22 14:21:37,3
Intel,mz4qjhz,"OpenCL works on all of them at once, and is just as fast as CUDA!",AMD,2025-06-22 10:25:02,3
Intel,mz5onps,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",AMD,2025-06-22 14:26:11,2
Intel,mz5oxpc,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,AMD,2025-06-22 14:27:41,2
Intel,mz737je,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",AMD,2025-06-22 18:42:52,2
Intel,mzavujs,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",AMD,2025-06-23 09:45:37,1
Intel,mz3m009,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),AMD,2025-06-22 04:08:09,8
Intel,mz57a7w,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,AMD,2025-06-22 12:41:24,5
Intel,mz3lspz,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,AMD,2025-06-22 04:06:39,4
Intel,mz3kt6w,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",AMD,2025-06-22 03:59:14,2
Intel,mz3l3jt,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",AMD,2025-06-22 04:01:24,7
Intel,mz3qt8d,Thank you so much for the very detailed response!,AMD,2025-06-22 04:45:35,3
Intel,mz5oyvv,Well worth it!,AMD,2025-06-22 14:27:51,3
Intel,mz5zat7,Thank you my man!! Looking forward to run some tests once I get home.,AMD,2025-06-22 15:21:59,2
Intel,mz74o6f,That's awesome!,AMD,2025-06-22 18:50:23,2
Intel,mzbns72,"Yes, but SLI is a bad description for it.",AMD,2025-06-23 13:13:43,1
Intel,mz3s5tj,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",AMD,2025-06-22 04:56:27,18
Intel,mz4kejl,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",AMD,2025-06-22 09:23:30,8
Intel,mz64tvp,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",AMD,2025-06-22 15:50:15,3
Intel,mz3smwy,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",AMD,2025-06-22 05:00:24,3
Intel,mz40qgf,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",AMD,2025-06-22 06:11:00,3
Intel,mz56bwd,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",AMD,2025-06-22 12:34:46,4
Intel,mz4wpgy,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",AMD,2025-06-22 11:20:29,4
Intel,mzffsev,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",AMD,2025-06-24 00:29:44,2
Intel,mz4ih7t,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",AMD,2025-06-22 09:03:49,1
Intel,mz4olvb,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",AMD,2025-06-22 10:05:55,2
Intel,mz4mwra,Why are you connecting the monitor to the gpu and not the mobo?,AMD,2025-06-22 09:49:01,0
Intel,mzeajzd,"👍   thanks for the info, this'll definitely come in handy eventually.",AMD,2025-06-23 20:49:01,1
Intel,mz4oaqj,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,AMD,2025-06-22 10:02:50,2
Intel,mzehy8b,No worries mate. Good luck,AMD,2025-06-23 21:25:07,2
Intel,mz4zjpa,"For some reason I switched up, connecting to the gpu is the way to go. I derped",AMD,2025-06-22 11:44:11,3
Intel,ms6f1il,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-05-13 23:11:19,1
Intel,ms76zj5,It's alive. Rejoice.,AMD,2025-05-14 01:54:03,3
Intel,m84i6ct,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",AMD,2025-01-20 06:59:20,21
Intel,m84uer1,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",AMD,2025-01-20 09:01:59,15
Intel,m8861s4,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,AMD,2025-01-20 20:45:52,5
Intel,m80r0p3,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,AMD,2025-01-19 18:16:28,35
Intel,m8efiwt,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,AMD,2025-01-21 19:23:32,2
Intel,m84nhes,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,AMD,2025-01-20 07:50:12,2
Intel,m83he9u,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",AMD,2025-01-20 02:32:38,-8
Intel,m862icn,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",AMD,2025-01-20 14:58:27,7
Intel,m84neo0,I'm fairly sure they use dxvk for d3d9 to 11.,AMD,2025-01-20 07:49:28,5
Intel,m872p8h,Could just be a cache issue,AMD,2025-01-20 17:49:03,2
Intel,m8c5h0v,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,AMD,2025-01-21 12:24:17,1
Intel,m85qkad,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,AMD,2025-01-20 13:49:31,3
Intel,m80ufhx,"According to the graphs, AMD has slightly less overhead than NVIDIA.",AMD,2025-01-19 18:32:18,80
Intel,m8290el,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",AMD,2025-01-19 22:33:50,10
Intel,m874iee,"Lowest with DX11 and older, but not with the newer APIs",AMD,2025-01-20 17:56:51,1
Intel,m81i5d3,And when is the last time HUB did a dedicated video showing the improvement in overhead?,AMD,2025-01-19 20:25:39,0
Intel,m873isl,or it's just a cache/memory access issue,AMD,2025-01-20 17:52:35,1
Intel,m83l8d5,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",AMD,2025-01-20 02:54:04,25
Intel,m83sg28,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",AMD,2025-01-20 03:39:34,15
Intel,m83s1d0,"Intel uses software translation for DX11 and lower, so it does matter for them.",AMD,2025-01-20 03:36:52,0
Intel,m82afin,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",AMD,2025-01-19 22:40:55,-15
Intel,m82o5am,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",AMD,2025-01-19 23:53:09,0
Intel,m862pny,That's not true. Intel's issue is being too verbose in commands/calls.,AMD,2025-01-20 14:59:30,0
Intel,m83h5jp,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",AMD,2025-01-20 02:31:29,-15
Intel,m83sird,HUB used DX12 games that also showed the issue.  It's something else.,AMD,2025-01-20 03:40:04,6
Intel,m87xk13,"The comment to which I am replying is talking about nVidia, not Intel.",AMD,2025-01-20 20:07:14,5
Intel,m84dadg,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",AMD,2025-01-20 06:15:54,10
Intel,m83slz3,That's actually... just worse news.,AMD,2025-01-20 03:40:39,4
Intel,lfjff1l,I always dreamt of the day APUs become power houses.,AMD,2024-07-29 19:57:14,56
Intel,lfj5g73,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",AMD,2024-07-29 19:03:41,21
Intel,lfltm14,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",AMD,2024-07-30 05:13:45,2
Intel,lfqfwra,Damn Why is AMD even involved in iGPU,AMD,2024-07-30 23:50:46,1
Intel,lfjm4t2,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",AMD,2024-07-29 20:32:18,-13
Intel,lfjhomu,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",AMD,2024-07-29 20:09:09,49
Intel,lfjtsec,almost there,AMD,2024-07-29 21:13:13,3
Intel,lfkaj8b,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",AMD,2024-07-29 22:50:53,1
Intel,lfkuvgo,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",AMD,2024-07-30 00:57:59,0
Intel,lfkjnlw,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,AMD,2024-07-29 23:47:05,-3
Intel,lfjfk07,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",AMD,2024-07-29 19:57:57,23
Intel,lfkemqm,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,AMD,2024-07-29 23:15:53,2
Intel,lfjlhvn,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",AMD,2024-07-29 20:28:55,2
Intel,lgze3vw,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",AMD,2024-08-07 18:47:35,1
Intel,lfjrf1q,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",AMD,2024-07-29 21:00:13,1
Intel,lfjr0pr,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",AMD,2024-07-29 20:58:06,-8
Intel,lfjet3n,yes its so bad. better go buy some steam deck or ally x,AMD,2024-07-29 19:54:02,-10
Intel,lfjomos,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,AMD,2024-07-29 20:45:29,11
Intel,lfji4cg,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",AMD,2024-07-29 20:11:25,16
Intel,lfk18sm,How are they going to feed all those CUs? Quad-channel LPDDR5X?,AMD,2024-07-29 21:55:13,5
Intel,lfkuy27,That's considerably faster than an XSX.,AMD,2024-07-30 00:58:27,2
Intel,lfkvkit,>That's tapping on 4070/7800 levels of performance.  What is?,AMD,2024-07-30 01:02:29,3
Intel,lfmp8zh,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",AMD,2024-07-30 10:56:08,3
Intel,lfjj0he,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",AMD,2024-07-29 20:15:59,5
Intel,lfm3fxr,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,AMD,2024-07-30 06:54:17,1
Intel,lfkw2is,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,AMD,2024-07-30 01:05:44,2
Intel,lflubg9,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",AMD,2024-07-30 05:20:30,2
Intel,lfjw9yq,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",AMD,2024-07-29 21:27:05,4
Intel,lfkbfbe,It's called satire. You're just salty because you're the butt of the joke.,AMD,2024-07-29 22:56:19,-3
Intel,lfkw8g2,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,AMD,2024-07-30 01:06:50,6
Intel,lflsl6l,Praying the blade16 gets it.,AMD,2024-07-30 05:04:09,1
Intel,lfk3os9,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",AMD,2024-07-29 22:09:30,11
Intel,lfk4vp7,256 bit bus + infinity cache.,AMD,2024-07-29 22:16:36,11
Intel,lfkfxeg,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,AMD,2024-07-29 23:23:53,2
Intel,lfl3c3y,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",AMD,2024-07-30 01:53:05,1
Intel,lfl04sh,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",AMD,2024-07-30 01:32:08,1
Intel,lfovbfq,The rumored 40CU strix halo chip. Not the actual chips released this week.,AMD,2024-07-30 18:37:40,1
Intel,lfkzt9q,7500mhz ram and the 780m,AMD,2024-07-30 01:30:05,2
Intel,lflujq4,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",AMD,2024-07-30 05:22:43,2
Intel,lfm7511,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",AMD,2024-07-30 07:34:59,1
Intel,lfk4w6h,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",AMD,2024-07-29 22:16:41,9
Intel,lfkvrtv,Literally where did you see 40-60% uplift at half the power?,AMD,2024-07-30 01:03:49,4
Intel,lfnnej3,> 40-60% performance uplift at half the power  Source?,AMD,2024-07-30 14:48:25,1
Intel,lfm3q9d,"i chuckled, then again im not a fanboy of anything",AMD,2024-07-30 06:57:22,-1
Intel,lflvl1g,Dont expect 40CUs in a handheld anytime soon,AMD,2024-07-30 05:32:53,8
Intel,lfmyyqu,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",AMD,2024-07-30 12:16:43,1
Intel,lg35wq0,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",AMD,2024-08-02 03:44:51,1
Intel,lgyqo0o,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",AMD,2024-08-07 16:49:14,1
Intel,lfp60n3,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,AMD,2024-07-30 19:33:48,1
Intel,lfql0n0,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",AMD,2024-07-31 00:22:30,3
Intel,lfo4zrj,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,AMD,2024-07-30 16:22:11,1
Intel,lfoeo9v,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,AMD,2024-07-30 17:12:32,0
Intel,lukc8v1,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",AMD,2024-10-30 18:32:02,1
Intel,lukp0ww,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",AMD,2024-10-30 19:35:13,1
Intel,lukywwo,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",AMD,2024-10-30 20:22:39,1
Intel,ldaak7j,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2024-07-15 13:10:50,1
Intel,leiilpv,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2024-07-23 08:23:24,1
Intel,lekd2f5,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,AMD,2024-07-23 16:24:13,31
Intel,lejyiil,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",AMD,2024-07-23 15:07:15,19
Intel,lelur0p,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",AMD,2024-07-23 21:04:22,7
Intel,lek4mor,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,AMD,2024-07-23 15:39:41,2
Intel,leouddh,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",AMD,2024-07-24 11:04:39,1
Intel,lep6hwc,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",AMD,2024-07-24 12:39:31,1
Intel,leufb7c,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",AMD,2024-07-25 09:17:02,1
Intel,lehh8b4,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-07-23 02:41:24,1
Intel,len76ez,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,AMD,2024-07-24 01:57:07,1
Intel,lelfwyp,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,AMD,2024-07-23 19:47:16,0
Intel,lelodyi,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",AMD,2024-07-23 20:31:10,0
Intel,leki2kn,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",AMD,2024-07-23 16:50:30,4
Intel,lemusx8,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",AMD,2024-07-24 00:37:13,1
Intel,lenkqpy,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,AMD,2024-07-24 03:30:22,0
Intel,lem1iup,"Installs beta software, proceeds to complain about it",AMD,2024-07-23 21:41:28,1
Intel,lenbfz4,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,AMD,2024-07-24 02:25:00,1
Intel,lem77tu,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",AMD,2024-07-23 22:13:57,0
Intel,lelhk36,What Ghost of Tsushima issue?,AMD,2024-07-23 19:55:44,1
Intel,lelridi,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",AMD,2024-07-23 20:47:19,9
Intel,lf385p0,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",AMD,2024-07-26 20:25:40,1
Intel,leorvpo,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",AMD,2024-07-24 10:41:40,4
Intel,lelhp6y,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,AMD,2024-07-23 19:56:28,0
Intel,lem0nam,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",AMD,2024-07-23 21:36:35,-1
Intel,lf3gd3s,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",AMD,2024-07-26 21:11:19,0
Intel,lf88lah,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,AMD,2024-07-27 19:04:01,0
Intel,lezwia9,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",AMD,2024-07-26 06:45:51,1
Intel,lem6kr4,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,AMD,2024-07-23 22:10:14,9
Intel,lf1fo06,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,AMD,2024-07-26 14:36:17,2
Intel,lenktr1,The documentation for it would still be in their archives,AMD,2024-07-24 03:31:01,-2
Intel,lep98lz,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",AMD,2024-07-24 12:57:51,3
Intel,ky7tcb2,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",AMD,2024-04-05 19:25:59,22
Intel,ky7p0fb,Wish Arc cards were better. They look so pretty in comparison to their peers,AMD,2024-04-05 19:01:17,13
Intel,ky7t8hc,Thats actually a pretty solid and accurate breakdown.,AMD,2024-04-05 19:25:23,4
Intel,ky7m91o,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,AMD,2024-04-05 18:45:54,11
Intel,kyooqk9,3080 still looking good too,AMD,2024-04-08 22:34:34,2
Intel,kyakde9,What they have peaceful then 4k series?,AMD,2024-04-06 07:27:42,1
Intel,kyjljxe,Just get a 4090. I will never regret getting mine.,AMD,2024-04-07 23:42:07,1
Intel,kys0jes,i miss old good times where radeon HD 7970 as best single core card cost around 400$,AMD,2024-04-09 15:02:55,1
Intel,kzdsbrd,"Damn, the A770 is still so uncompetitive...",AMD,2024-04-13 13:49:40,1
Intel,kybklob,"It's like the free market priced cards according to their relative performance. How weird, right?",AMD,2024-04-06 13:42:41,-1
Intel,kyjjx67,How is that possibly annoying,AMD,2024-04-07 23:31:52,0
Intel,kya236v,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,AMD,2024-04-06 04:17:14,3
Intel,kyaw0hp,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",AMD,2024-04-06 09:51:52,1
Intel,kybpb3p,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",AMD,2024-04-06 14:15:00,2
Intel,kygdnfc,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,AMD,2024-04-07 11:17:10,1
Intel,kys12cm,8gb perfectly fine today :),AMD,2024-04-09 15:06:00,1
Intel,l9ad3sk,"Ah yes sure, now where did I leave my 1500 euros?",AMD,2024-06-19 10:11:00,2
Intel,kybkrrc,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",AMD,2024-04-06 13:43:53,11
Intel,kymgwzk,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,AMD,2024-04-08 14:36:56,1
Intel,kya4qoq,"Yeah, i like the black super series.",AMD,2024-04-06 04:40:54,1
Intel,kyw7k0z,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",AMD,2024-04-10 08:27:23,0
Intel,kybtcsj,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",AMD,2024-04-06 14:41:11,2
Intel,kxhli0e,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,AMD,2024-04-01 02:17:59,225
Intel,kxl9t8e,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",AMD,2024-04-01 19:43:02,24
Intel,kxiush3,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",AMD,2024-04-01 10:12:15,111
Intel,kxrny0e,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",AMD,2024-04-02 22:36:02,18
Intel,kxkeqm3,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",AMD,2024-04-01 16:50:42,29
Intel,kxhn7gu,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",AMD,2024-04-01 02:30:21,121
Intel,kxi9i5m,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",AMD,2024-04-01 05:48:52,65
Intel,kxpi7rl,"Yo, I saw the title and thought this gotta be Gnif2.",AMD,2024-04-02 15:15:20,7
Intel,kxhii78,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",AMD,2024-04-01 01:56:41,36
Intel,kxisjb3,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",AMD,2024-04-01 09:45:49,36
Intel,kxhfw6h,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",AMD,2024-04-01 01:38:50,62
Intel,kxiukyk,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",AMD,2024-04-01 10:09:50,13
Intel,kxiah6c,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",AMD,2024-04-01 05:59:50,26
Intel,kxlnigb,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",AMD,2024-04-01 20:59:38,22
Intel,ky0wzku,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",AMD,2024-04-04 15:28:04,3
Intel,ky567n0,Long but worth it read; Well Done!,AMD,2024-04-05 08:38:06,4
Intel,kxnqc72,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",AMD,2024-04-02 05:31:11,3
Intel,ky1f7to,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,AMD,2024-04-04 17:07:58,3
Intel,l012ykv,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",AMD,2024-04-17 19:05:55,3
Intel,kxitz3a,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",AMD,2024-04-01 10:02:50,24
Intel,kxmpmyk,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,AMD,2024-04-02 00:54:21,5
Intel,kxp7mvs,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,AMD,2024-04-02 14:13:09,5
Intel,kxq8p0p,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",AMD,2024-04-02 17:41:45,5
Intel,kxr0ydr,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,AMD,2024-04-02 20:16:04,6
Intel,kxtpd72,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",AMD,2024-04-03 08:19:54,6
Intel,kxj7ncd,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",AMD,2024-04-01 12:18:50,10
Intel,kxirbw1,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",AMD,2024-04-01 09:31:11,14
Intel,kxnysdb,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,AMD,2024-04-02 07:08:39,4
Intel,kxi4dih,100% all of this...  Love looking glass by the by,AMD,2024-04-01 04:54:44,7
Intel,kxt140w,How does say VMware handle this? Does it kind of just restart shit as needed?,AMD,2024-04-03 04:01:28,2
Intel,kxibc53,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",AMD,2024-04-01 06:09:51,16
Intel,kxizp6h,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",AMD,2024-04-01 11:05:58,3
Intel,kxju0p0,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",AMD,2024-04-01 14:52:01,3
Intel,kxjywwd,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,AMD,2024-04-01 15:20:47,2
Intel,kxkj3fj,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",AMD,2024-04-01 17:15:05,4
Intel,kxilacf,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",AMD,2024-04-01 08:13:50,4
Intel,kxikwgx,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",AMD,2024-04-01 08:08:54,1
Intel,kxnag16,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",AMD,2024-04-02 03:12:09,1
Intel,kxqkz3h,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",AMD,2024-04-02 18:48:54,1
Intel,kxk4suo,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",AMD,2024-04-01 15:54:31,0
Intel,kxjykgb,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,AMD,2024-04-01 15:18:47,0
Intel,kxnctg8,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",AMD,2024-04-02 03:30:01,0
Intel,kxierbw,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",AMD,2024-04-01 06:50:41,-8
Intel,kxxhwq9,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",AMD,2024-04-03 23:01:13,-1
Intel,kxip0e1,TL;DR. **PEBKAC**.,AMD,2024-04-01 09:01:51,-23
Intel,kxk9iir,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-01 16:21:24,-3
Intel,kxksj8e,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",AMD,2024-04-01 18:06:47,-3
Intel,kxo5btd,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,AMD,2024-04-02 08:32:08,-5
Intel,kxiw3lo,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",AMD,2024-04-01 10:27:10,48
Intel,ky1fyc2,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,AMD,2024-04-04 17:12:00,3
Intel,kxjwsde,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",AMD,2024-04-01 15:08:22,28
Intel,kxte67y,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,AMD,2024-04-03 06:04:47,2
Intel,kxkf630,"Thanks mate I appreciate it, glad to see you here :)",AMD,2024-04-01 16:53:06,14
Intel,kxtip4r,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",AMD,2024-04-03 06:57:08,5
Intel,ll8wytp,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",AMD,2024-09-03 02:42:30,1
Intel,kxhow6p,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",AMD,2024-04-01 02:42:51,33
Intel,kxhpa3h,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",AMD,2024-04-01 02:45:39,14
Intel,kxjf8yq,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,AMD,2024-04-01 13:17:38,12
Intel,kxpa05g,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,AMD,2024-04-02 14:27:21,-3
Intel,kxiv9ac,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",AMD,2024-04-01 10:17:32,19
Intel,kxp8y84,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,AMD,2024-04-02 14:21:05,8
Intel,kxjfdjy,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",AMD,2024-04-01 13:18:34,3
Intel,kxj3tba,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",AMD,2024-04-01 11:45:39,7
Intel,kxjhcp0,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",AMD,2024-04-01 13:32:25,5
Intel,kxjknpx,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",AMD,2024-04-01 13:54:35,3
Intel,kxtwy1v,"Funny, I saw the title and thought the same too!",AMD,2024-04-03 09:54:20,6
Intel,kxhlmwx,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",AMD,2024-04-01 02:18:57,30
Intel,kxn102r,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",AMD,2024-04-02 02:07:08,-1
Intel,kxnsbw0,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,AMD,2024-04-02 05:52:30,7
Intel,kxjj86s,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",AMD,2024-04-01 13:45:07,8
Intel,kxjs7vy,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",AMD,2024-04-01 14:41:18,-6
Intel,kxi3d8c,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",AMD,2024-04-01 04:44:52,13
Intel,kxvte63,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",AMD,2024-04-03 17:32:25,3
Intel,kxmufyt,ursohot !  back to discord rants...,AMD,2024-04-02 01:24:48,-5
Intel,kxix377,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,AMD,2024-04-01 10:38:16,24
Intel,kxmy36x,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",AMD,2024-04-02 01:48:12,7
Intel,kxjbu8k,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",AMD,2024-04-01 12:52:07,5
Intel,kxlfj2c,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,AMD,2024-04-01 20:14:49,2
Intel,kxnky9y,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",AMD,2024-04-02 04:38:17,0
Intel,kxj2kjm,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",AMD,2024-04-01 11:34:09,5
Intel,kxta6ee,"It doesn't handle it, it has the same issue.",AMD,2024-04-03 05:22:41,2
Intel,kxj4eg4,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2024-04-01 11:50:55,12
Intel,kxj38ou,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",AMD,2024-04-01 11:40:25,6
Intel,kxiu2ph,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",AMD,2024-04-01 10:03:58,3
Intel,kxidqq0,Me neither. I use a RX580 8GB since launch and not a single problem.,AMD,2024-04-01 06:38:22,3
Intel,kxie3oi,Because they're talking absolute rubbish that's why.,AMD,2024-04-01 06:42:43,-15
Intel,kxj72uk,You are one of the lucky ones!,AMD,2024-04-01 12:14:06,9
Intel,kxue41z,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",AMD,2024-04-03 12:32:07,2
Intel,kximvz5,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",AMD,2024-04-01 08:34:35,11
Intel,kxjkdyv,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",AMD,2024-04-01 13:52:49,1
Intel,kxi3fxr,lol your flair is Please search before asking,AMD,2024-04-01 04:45:36,-2
Intel,kyy38w2,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-10 17:04:31,1
Intel,kxipuql,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,AMD,2024-04-01 09:12:36,-1
Intel,kxt2f9e,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,AMD,2024-04-03 04:12:16,1
Intel,kxiexwv,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",AMD,2024-04-01 06:52:56,31
Intel,kxxifs5,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",AMD,2024-04-03 23:04:27,5
Intel,kxkxwhq,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,AMD,2024-04-01 18:36:38,7
Intel,kxo5nh7,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,AMD,2024-04-02 08:36:20,6
Intel,kxmvpp1,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",AMD,2024-04-02 01:33:01,43
Intel,ky1ipao,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",AMD,2024-04-04 17:26:58,2
Intel,kxkcepy,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",AMD,2024-04-01 16:37:46,29
Intel,kxs8nai,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",AMD,2024-04-03 00:45:36,5
Intel,kxk4crx,"""NVIDIA, it just works""",AMD,2024-04-01 15:51:58,13
Intel,kxncqt4,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,AMD,2024-04-02 03:29:27,4
Intel,kxof5tw,What is the AMD Vanguard?,AMD,2024-04-02 10:31:39,7
Intel,kxtr5do,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",AMD,2024-04-03 08:42:33,10
Intel,kxnum1q,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,AMD,2024-04-02 06:18:22,6
Intel,kxjkmnv,You misspelled $2.3T market cap....,AMD,2024-04-01 13:54:24,11
Intel,kxjp8qb,"Okay yeah fair enough, hadn't considered this. Removed it from my post",AMD,2024-04-01 14:23:19,2
Intel,kxxn4fl,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",AMD,2024-04-03 23:33:02,2
Intel,kxpe18q,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",AMD,2024-04-02 14:51:06,1
Intel,kxlmn5s,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",AMD,2024-04-01 20:54:42,2
Intel,kxjv1e3,This is not a fix. It's a compromise.,AMD,2024-04-01 14:58:00,13
Intel,kxjpkam,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",AMD,2024-04-01 14:25:16,2
Intel,kxtj7av,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,AMD,2024-04-03 07:03:13,1
Intel,kxmam0y,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",AMD,2024-04-01 23:20:26,10
Intel,kxxefr8,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,AMD,2024-04-03 22:40:23,1
Intel,kxpad65,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,AMD,2024-04-02 14:29:30,-1
Intel,kxjq477,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",AMD,2024-04-01 14:28:37,23
Intel,kxi6i64,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",AMD,2024-04-01 05:16:16,21
Intel,kxllisv,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",AMD,2024-04-01 20:48:17,3
Intel,kxoidrh,The comment I quoted was talking about people playing games having issues.,AMD,2024-04-02 11:05:13,6
Intel,kxoc6dt,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,AMD,2024-04-02 09:57:53,3
Intel,kxoib9e,The thing I quoted was talking about people playing games though.,AMD,2024-04-02 11:04:33,2
Intel,kxjibo8,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",AMD,2024-04-01 13:38:59,5
Intel,kxj9jkm,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",AMD,2024-04-01 12:34:08,5
Intel,kxjdtt9,"Idk, I don't use Linux",AMD,2024-04-01 13:07:13,-12
Intel,kxjdrs5,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",AMD,2024-04-01 13:06:49,0
Intel,kxigqbh,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",AMD,2024-04-01 07:15:19,29
Intel,kxj2oqt,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",AMD,2024-04-01 11:35:13,1
Intel,kxj4abt,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:53,-1
Intel,kxih6b1,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),AMD,2024-04-01 07:20:59,29
Intel,kxm7xhx,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",AMD,2024-04-01 23:03:36,1
Intel,kxuiptm,Because adding a feature for a product literally gives users more control for that product.,AMD,2024-04-03 13:05:04,1
Intel,kxine7u,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,AMD,2024-04-01 08:41:11,-1
Intel,kxis9nq,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",AMD,2024-04-01 09:42:40,5
Intel,kyhsjnw,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-07 17:08:48,1
Intel,kxjqk3k,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",AMD,2024-04-01 14:31:18,-3
Intel,kxzlw7y,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,AMD,2024-04-04 09:36:41,1
Intel,kxmwxwt,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",AMD,2024-04-02 01:40:54,8
Intel,kxj49ms,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:43,-2
Intel,kxs4to2,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",AMD,2024-04-03 00:21:22,6
Intel,ky39ja5,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,AMD,2024-04-04 23:11:22,5
Intel,ky4zrtz,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",AMD,2024-04-05 07:20:00,3
Intel,kxldpfb,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",AMD,2024-04-01 20:04:38,14
Intel,kxp3oh8,*wayland users have joined the chat,AMD,2024-04-02 13:48:33,10
Intel,kxm4qt3,You're falling for slogans.,AMD,2024-04-01 22:43:30,-2
Intel,kxobyv3,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",AMD,2024-04-02 09:55:25,13
Intel,kxpaw46,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,AMD,2024-04-02 14:32:39,11
Intel,kxojs3c,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),AMD,2024-04-02 11:18:39,5
Intel,kxtnu71,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",AMD,2024-04-03 08:00:44,2
Intel,kxjpcl3,Honestly after a trillion I kinda stop counting 😂🤣,AMD,2024-04-01 14:23:58,2
Intel,kxjvfz1,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",AMD,2024-04-01 15:00:22,9
Intel,kxpf9fv,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",AMD,2024-04-02 14:58:15,9
Intel,kxodaii,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",AMD,2024-04-02 10:10:50,6
Intel,kxjvmo3,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",AMD,2024-04-01 15:01:28,7
Intel,kxpamp2,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,AMD,2024-04-02 14:31:05,-2
Intel,kxy4p6p,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",AMD,2024-04-04 01:24:00,1
Intel,kxpia4a,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",AMD,2024-04-02 15:15:42,3
Intel,kxjr4lw,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",AMD,2024-04-01 14:34:44,8
Intel,kxp7oc3,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",AMD,2024-04-02 14:13:24,-2
Intel,kxi7ym2,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",AMD,2024-04-01 05:31:48,1
Intel,kxm9n9f,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",AMD,2024-04-01 23:14:25,3
Intel,kxk5inl,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",AMD,2024-04-01 15:58:39,0
Intel,kxiim2c,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",AMD,2024-04-01 07:39:33,-7
Intel,kxih401,Oh then just ignore my comment 😅,AMD,2024-04-01 07:20:10,-1
Intel,kxjfryq,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",AMD,2024-04-01 13:21:24,3
Intel,kxiojjd,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,AMD,2024-04-01 08:55:52,7
Intel,kxiiqcv,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",AMD,2024-04-01 07:41:05,-12
Intel,kxin4tk,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",AMD,2024-04-01 08:37:50,-12
Intel,kxmwd7i,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",AMD,2024-04-02 01:37:14,1
Intel,kxioc93,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,AMD,2024-04-01 08:53:17,5
Intel,kxiqori,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",AMD,2024-04-01 09:23:10,0
Intel,kxiuak1,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,AMD,2024-04-01 10:06:29,1
Intel,kxit1y6,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,AMD,2024-04-01 09:52:00,-2
Intel,kxjg5xf,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",AMD,2024-04-01 13:24:09,-2
Intel,kxjr7cc,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",AMD,2024-04-01 14:35:12,8
Intel,kxzn1iw,"Too soon to tell, but hopes are high.",AMD,2024-04-04 09:50:05,2
Intel,kxo5u7w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-02 08:38:44,1
Intel,kxoprjw,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",AMD,2024-04-02 12:09:39,12
Intel,kxm2qa6,"Agreed, they cannot rest on their laurels.",AMD,2024-04-01 22:30:48,4
Intel,kxn01lt,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",AMD,2024-04-02 02:00:52,27
Intel,kxnsapp,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",AMD,2024-04-02 05:52:08,3
Intel,kxpuexg,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,AMD,2024-04-02 16:23:44,2
Intel,kxpwkoo,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,AMD,2024-04-02 16:35:41,1
Intel,kxk96s0,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",AMD,2024-04-01 16:19:33,5
Intel,kxpcxh7,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",AMD,2024-04-02 14:44:41,6
Intel,kxiic2i,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",AMD,2024-04-01 07:35:56,37
Intel,kxi921e,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",AMD,2024-04-01 05:43:54,11
Intel,kxijoyb,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",AMD,2024-04-01 07:53:26,24
Intel,kxiqghx,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,AMD,2024-04-01 09:20:14,16
Intel,kxiitb5,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,AMD,2024-04-01 07:42:10,18
Intel,kxj5139,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,AMD,2024-04-01 11:56:29,7
Intel,kxio9nt,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",AMD,2024-04-01 08:52:23,12
Intel,kxn5a9z,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",AMD,2024-04-02 02:35:33,2
Intel,kxjrku0,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,AMD,2024-04-01 14:37:29,7
Intel,kxipvh2,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",AMD,2024-04-01 09:12:52,14
Intel,kxjy6gb,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,AMD,2024-04-01 15:16:31,-2
Intel,kxp15kv,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",AMD,2024-04-02 13:32:11,6
Intel,kxn7ur7,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,AMD,2024-04-02 02:53:24,2
Intel,kxq0m39,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",AMD,2024-04-02 16:57:48,0
Intel,kxq98bx,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",AMD,2024-04-02 17:44:39,3
Intel,kxm4q67,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",AMD,2024-04-01 22:43:23,5
Intel,kxq0fuf,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",AMD,2024-04-02 16:56:51,2
Intel,kxpfg1v,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,AMD,2024-04-02 14:59:19,0
Intel,kxin2k0,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",AMD,2024-04-01 08:37:02,15
Intel,kxj2kf3,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",AMD,2024-04-01 11:34:06,0
Intel,kxnjdov,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",AMD,2024-04-02 04:23:59,-2
Intel,kxisrca,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",AMD,2024-04-01 09:48:29,7
Intel,kxs5a0e,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",AMD,2024-04-03 00:24:15,2
Intel,kxj34w0,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",AMD,2024-04-01 11:39:28,-7
Intel,kxindr9,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",AMD,2024-04-01 08:41:01,-8
Intel,kxiniuo,Oh and XE also have bug feature reporting.  Omfg!!!!,AMD,2024-04-01 08:42:51,-2
Intel,kxl4asu,Nobody is 100% right ;),AMD,2024-04-01 19:12:15,-5
Intel,kxta5m0,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),AMD,2024-04-03 05:22:28,2
Intel,kxiq2zk,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",AMD,2024-04-01 09:15:31,-5
Intel,kxjix5f,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",AMD,2024-04-01 13:43:03,-1
Intel,kxjz1ko,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",AMD,2024-04-01 15:21:32,6
Intel,kxthgxe,What about using a DP to HDMI 2.1 adapter for that situation?,AMD,2024-04-03 06:42:39,2
Intel,kxnvnrf,"2021 my guy, it's right there on the date of the article.",AMD,2024-04-02 06:30:33,7
Intel,kxqftwv,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,AMD,2024-04-02 18:20:45,-1
Intel,kxp8mfb,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,AMD,2024-04-02 14:19:07,2
Intel,kxipvcp,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",AMD,2024-04-01 09:12:49,17
Intel,kxj4mkp,And I guess infallible game developers too then. /s,AMD,2024-04-01 11:52:55,6
Intel,kxjlszk,So you decide what criticism is valid and what not? lol,AMD,2024-04-01 14:01:58,6
Intel,kxio3k4,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,AMD,2024-04-01 08:50:13,7
Intel,kxioj2i,"Yup, but do you see them making a big press release about it?",AMD,2024-04-01 08:55:43,8
Intel,kxno85r,that is not how it works but sure,AMD,2024-04-02 05:09:33,2
Intel,kxtv199,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,AMD,2024-04-03 09:31:19,2
Intel,kxjk8f2,>whine about Redditors.  The irony.,AMD,2024-04-01 13:51:48,-2
Intel,kxu2whw,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",AMD,2024-04-03 10:58:25,0
Intel,kxqg0v8,learn to comprehend.,AMD,2024-04-02 18:21:49,3
Intel,kxiqgpx,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,AMD,2024-04-01 09:20:19,8
Intel,kxj4whx,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",AMD,2024-04-01 11:55:21,4
Intel,kxnjs9x,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",AMD,2024-04-02 04:27:38,0
Intel,kxl4djq,"No, that would be you obviously /s",AMD,2024-04-01 19:12:41,-2
Intel,kxivsl5,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,AMD,2024-04-01 10:23:43,-3
Intel,kxivodj,"Yea, given the state of XE drivers every major update has come with significant PR.",AMD,2024-04-01 10:22:23,-2
Intel,kxnxxva,Why not ;),AMD,2024-04-02 06:58:11,0
Intel,kxqg47j,Go word salad elsewhere.,AMD,2024-04-02 18:22:19,-1
Intel,kxnwc84,"I have replicated the issue reliably yes, and across two different systems.",AMD,2024-04-02 06:38:43,5
Intel,kxjrbmq,If discord crashes my drivers.. once every few hours. I have to reboot,AMD,2024-04-01 14:35:55,6
Intel,kxo4jke,Discord doesn't crash my drivers  I don't have to reboot.,AMD,2024-04-02 08:22:06,0
Intel,kpp4kwl,Really love how the 6000 series radeons look.,AMD,2024-02-09 21:57:31,13
Intel,kpqv9od,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",AMD,2024-02-10 05:25:10,5
Intel,kpougfk,That's a good looking line up,AMD,2024-02-09 20:58:04,3
Intel,kps7pkq,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",AMD,2024-02-10 14:18:43,3
Intel,kpr86tx,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",AMD,2024-02-10 07:45:28,4
Intel,kpq3r57,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",AMD,2024-02-10 01:49:13,3
Intel,kptibdx,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,AMD,2024-02-10 19:15:04,-1
Intel,kptwmeu,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",AMD,2024-02-10 20:44:28,3
Intel,kpv2g8f,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,AMD,2024-02-11 01:23:45,1
Intel,kpv5euk,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",AMD,2024-02-11 01:44:32,3
Intel,kpvwyyr,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",AMD,2024-02-11 05:16:13,2
Intel,kcvx2pq,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,AMD,2023-12-11 10:20:41,10
Intel,kcvsq1w,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",AMD,2023-12-11 09:20:24,15
Intel,kcvzwca,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,AMD,2023-12-11 10:58:26,2
Intel,kcyc7u2,That 7900xtx sale number is insane,AMD,2023-12-11 21:59:22,2
Intel,kcytq9l,That just shows that most people that buy GPU's don't know a thing about them.,AMD,2023-12-11 23:54:41,1
Intel,kcwedyi,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",AMD,2023-12-11 13:30:14,16
Intel,kcvzjgq,best discounts were 6750xt 6800 and 7800xt,AMD,2023-12-11 10:53:41,1
Intel,kdazjvv,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,AMD,2023-12-14 10:36:15,1
Intel,kcvv71l,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",AMD,2023-12-11 09:54:52,6
Intel,kcwe3k6,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",AMD,2023-12-11 13:27:46,4
Intel,kcw3vwl,"They're not out of stock there, duh",AMD,2023-12-11 11:47:20,7
Intel,kcyhmsr,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,AMD,2023-12-11 22:33:46,3
Intel,kd0h0lm,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-12-12 07:54:01,1
Intel,kcxlwiu,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",AMD,2023-12-11 19:18:18,3
Intel,kcxu0yw,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",AMD,2023-12-11 20:09:16,1
Intel,kcx65jb,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",AMD,2023-12-11 16:46:01,2
Intel,kcw55l4,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",AMD,2023-12-11 12:01:31,1
Intel,kcy5bwz,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",AMD,2023-12-11 21:17:17,1
Intel,keln136,the card is pretty bad if you missed that somehow,AMD,2023-12-23 12:50:02,1
Intel,kcw5qf2,AMD probably ships leftover to countries in which they know it will sell,AMD,2023-12-11 12:07:48,3
Intel,kdhrs0l,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",AMD,2023-12-15 17:44:32,1
Intel,kemomla,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",AMD,2023-12-23 17:01:05,1
Intel,kb07k5w,"As you notice the photoshop version differs, so you can't compare them really",AMD,2023-11-27 18:28:41,4
Intel,kao517l,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),AMD,2023-11-25 07:14:52,2
Intel,k74d0ev,So basically PC games are never going to tell us what the specs are to run the game native ever again.,AMD,2023-10-30 18:21:26,536
Intel,k751wfu,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",AMD,2023-10-30 20:52:50,48
Intel,k748hf1,The true crime here is needing FSR to reach these requirements.,AMD,2023-10-30 17:53:41,190
Intel,k74fig7,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",AMD,2023-10-30 18:36:48,35
Intel,k74fyp7,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),AMD,2023-10-30 18:39:33,15
Intel,k74c3qr,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,AMD,2023-10-30 18:15:55,65
Intel,k7407mp,"well, at least the chart is easy to read, not a complete mess",AMD,2023-10-30 17:03:14,17
Intel,k7466bs,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,AMD,2023-10-30 17:39:45,10
Intel,k741zrd,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,AMD,2023-10-30 17:14:11,57
Intel,k748t1r,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,AMD,2023-10-30 17:55:39,54
Intel,k76jevo,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",AMD,2023-10-31 02:51:26,6
Intel,k76we4z,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",AMD,2023-10-31 04:46:34,7
Intel,k75qe4v,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,AMD,2023-10-30 23:33:24,5
Intel,k74d5ad,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",AMD,2023-10-30 18:22:16,25
Intel,k748nsg,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",AMD,2023-10-30 17:54:45,12
Intel,k74kjmk,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",AMD,2023-10-30 19:07:25,19
Intel,k7533wp,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",AMD,2023-10-30 21:00:16,3
Intel,k77mpgx,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,AMD,2023-10-31 10:35:56,3
Intel,k74hphk,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,AMD,2023-10-30 18:50:08,12
Intel,k773lbl,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",AMD,2023-10-31 06:11:46,6
Intel,k74eavi,I hope they will bundle this game with CPUs/GPUs,AMD,2023-10-30 18:29:20,2
Intel,k74fxtg,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,AMD,2023-10-30 18:39:23,2
Intel,k77022h,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,AMD,2023-10-31 05:27:42,2
Intel,k78gl5o,This game better look like real life with those specs. I does looks beautiful!,AMD,2023-10-31 14:43:10,2
Intel,k75as90,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",AMD,2023-10-30 21:48:53,5
Intel,k748ctr,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,AMD,2023-10-30 17:52:56,3
Intel,k74mjvt,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,AMD,2023-10-30 19:19:43,2
Intel,k74oqng,I love it how absurd these things are these days.,AMD,2023-10-30 19:33:13,2
Intel,k74awed,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,AMD,2023-10-30 18:08:29,2
Intel,l06s6dc,Does anyone know if it supports SLI or crossfire?,AMD,2024-04-18 19:16:23,1
Intel,k73yt0i,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,AMD,2023-10-30 16:54:36,3
Intel,k74kcre,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",AMD,2023-10-30 19:06:17,1
Intel,k74mb9y,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,AMD,2023-10-30 19:18:13,1
Intel,k74oabx,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,AMD,2023-10-30 19:30:24,1
Intel,k74sd5w,Why in the f*ck is upscaling included on a specs page?,AMD,2023-10-30 19:55:26,1
Intel,k77tobm,no more software optimization and full upscaling  bleah,AMD,2023-10-31 11:50:04,1
Intel,k749m34,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,AMD,2023-10-30 18:00:34,-2
Intel,k74d7ll,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",AMD,2023-10-30 18:22:41,0
Intel,k74phj0,"I've never even heard of this game, nor care about it, but these system requirements offend me.",AMD,2023-10-30 19:37:49,1
Intel,k79vqgg,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,AMD,2023-10-31 20:00:03,0
Intel,k744khv,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",AMD,2023-10-30 17:29:53,-2
Intel,k79zrsa,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",AMD,2023-10-31 20:25:07,0
Intel,k7c9frd,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,AMD,2023-11-01 08:20:56,0
Intel,k75roib,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,AMD,2023-10-30 23:42:07,-2
Intel,k741y61,Nice,AMD,2023-10-30 17:13:54,-1
Intel,k765iq2,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",AMD,2023-10-31 01:14:49,0
Intel,k77fyxm,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",AMD,2023-10-31 09:08:01,0
Intel,k77zvvn,Native gaming died or what ? Wtf they turning pc gaming into console gaming,AMD,2023-10-31 12:44:58,0
Intel,k76r1ve,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",AMD,2023-10-31 03:53:47,-1
Intel,k74pme9,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,AMD,2023-10-30 19:38:39,1
Intel,k74qrim,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,AMD,2023-10-30 19:45:38,1
Intel,k74t7ty,"Omg, it would be a graphic master piece or  bad optimized thing.",AMD,2023-10-30 20:00:39,1
Intel,k74ybxr,First time I see matches recommendations for nv and amd GPUs...,AMD,2023-10-30 20:31:22,1
Intel,k756ha4,Rip laptop rtx 3060 6gb,AMD,2023-10-30 21:21:24,1
Intel,k759rkx,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",AMD,2023-10-30 21:42:14,1
Intel,k75e3m3,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,AMD,2023-10-30 22:10:21,1
Intel,k75ej2l,*NATIVE* resolution gang ftw!,AMD,2023-10-30 22:13:11,1
Intel,k75k1fl,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",AMD,2023-10-30 22:50:05,1
Intel,k75n4rl,Looks capped at 60fps?,AMD,2023-10-30 23:11:06,1
Intel,k75ops0,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",AMD,2023-10-30 23:21:57,1
Intel,k75qeie,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,AMD,2023-10-30 23:33:28,1
Intel,k760or5,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",AMD,2023-10-31 00:42:35,1
Intel,k77465m,Is it using UE5?,AMD,2023-10-31 06:19:31,1
Intel,k77bz45,"Ubisoft, rip on launch.",AMD,2023-10-31 08:09:13,1
Intel,k77htgp,guessing no DLSS3 then?,AMD,2023-10-31 09:33:36,1
Intel,k77l01d,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,AMD,2023-10-31 10:15:17,1
Intel,k77twmu,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",AMD,2023-10-31 11:52:12,1
Intel,k77x5ay,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",AMD,2023-10-31 12:21:45,1
Intel,k77z2e6,Farewell 1660ti… looks like it’s time for an upgrade,AMD,2023-10-31 12:38:12,1
Intel,k78p32u,well my 3300x is now obsolete for these new AAA games...,AMD,2023-10-31 15:37:57,1
Intel,k798boq,4k ultra right up my alley 😏,AMD,2023-10-31 17:36:35,1
Intel,k799x2x,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,AMD,2023-10-31 17:46:17,1
Intel,k7ec5fz,7900xtx will do 4k 120fps with FSR 3 then I guess?,AMD,2023-11-01 18:22:39,1
Intel,k7fkb1o,What must one do to achieve a higher rank than an enthusiast? A demi-god?,AMD,2023-11-01 22:56:10,1
Intel,k7gz4pf,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",AMD,2023-11-02 05:23:56,1
Intel,k7k2rqe,Is vrr fixed with frame gen then?,AMD,2023-11-02 20:32:54,1
Intel,k859q58,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",AMD,2023-11-07 00:16:09,1
Intel,k8kv1d8,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",AMD,2023-11-10 00:27:13,1
Intel,kaeixym,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,AMD,2023-11-23 05:18:15,1
Intel,k74h55l,They recommend upscaling even at 1080p.. disgusting ew,AMD,2023-10-30 18:46:41,219
Intel,k75hnu4,pretty much this we all knew they would start using upscaling as a crutch.,AMD,2023-10-30 22:34:04,14
Intel,k76ze7a,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,AMD,2023-10-31 05:19:49,4
Intel,k74f7tr,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",AMD,2023-10-30 18:35:00,26
Intel,k74e6u4,My thoughts too…,AMD,2023-10-30 18:28:39,6
Intel,k778snb,Thanks to all of you that were screaming dlss looks better than native lmao.,AMD,2023-10-31 07:23:19,4
Intel,k79niu3,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",AMD,2023-10-31 19:09:38,1
Intel,k77adqn,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,AMD,2023-10-31 07:46:18,1
Intel,k74x8ab,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",AMD,2023-10-30 20:24:48,-1
Intel,k74zbiw,Nope we as a community abused a nice thing,AMD,2023-10-30 20:37:19,1
Intel,k755j7t,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",AMD,2023-10-30 21:15:25,-5
Intel,k75o59y,Didn't take them long to make upscaling worthless.,AMD,2023-10-30 23:18:03,-1
Intel,k78qsdu,i smell a burgeoning cottage industry of game spec reviewers!,AMD,2023-10-31 15:48:28,0
Intel,k7hhgqi,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",AMD,2023-11-02 09:36:54,0
Intel,k7k4fzn,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",AMD,2023-11-02 20:43:01,0
Intel,k74js59,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",AMD,2023-10-30 19:02:44,-2
Intel,k78m5tc,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,AMD,2023-10-31 15:19:10,-1
Intel,k74pa1g,yeah this is the new standard,AMD,2023-10-30 19:36:32,1
Intel,k77dlbv,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",AMD,2023-10-31 08:33:14,25
Intel,k78n190,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,AMD,2023-10-31 15:24:48,1
Intel,k77nqtn,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",AMD,2023-10-31 10:47:59,-6
Intel,k74z5qq,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",AMD,2023-10-30 20:36:21,14
Intel,k76jxv9,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",AMD,2023-10-31 02:55:18,2
Intel,k76rqab,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,AMD,2023-10-31 04:00:06,2
Intel,k75qnpb,The actual true crime here is even having fsr to begin with. It should just have dlss,AMD,2023-10-30 23:35:12,-6
Intel,k76ju0a,Seems like they tried to cover every basis with these.,AMD,2023-10-31 02:54:33,7
Intel,k75ao0c,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",AMD,2023-10-30 21:48:08,10
Intel,k74htwr,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",AMD,2023-10-30 18:50:52,16
Intel,k74jsww,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,AMD,2023-10-30 19:02:51,3
Intel,k760d0a,"GCN support is over, RDNA1 is the lowest currently supported arch.",AMD,2023-10-31 00:40:29,5
Intel,k74k4hi,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",AMD,2023-10-30 19:04:51,3
Intel,k77v39i,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",AMD,2023-10-31 12:03:13,0
Intel,k74efc2,What do you mean forced raytracing?,AMD,2023-10-30 18:30:05,10
Intel,k77dids,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,AMD,2023-10-31 08:31:59,7
Intel,k74fapr,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",AMD,2023-10-30 18:35:29,14
Intel,k7gd959,It's actually 960p :(,AMD,2023-11-02 02:12:51,2
Intel,k78nhhp,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",AMD,2023-10-31 15:27:45,0
Intel,k74go33,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,AMD,2023-10-30 18:43:49,6
Intel,k78s16k,We'll see in a year.,AMD,2023-10-31 15:56:02,-1
Intel,k744q6u,AFAIK  &#x200B;  It is with RT,AMD,2023-10-30 17:30:51,21
Intel,k745n5v,Seems pretty good to me given it is at 4K with RT,AMD,2023-10-30 17:36:31,17
Intel,k743fzm,"Likely includes the RT features , its also 4k",AMD,2023-10-30 17:23:02,8
Intel,k743sfv,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",AMD,2023-10-30 17:25:06,-15
Intel,k74aacw,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",AMD,2023-10-30 18:04:43,32
Intel,k74c9q7,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",AMD,2023-10-30 18:16:57,26
Intel,k78pyma,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,AMD,2023-10-31 15:43:22,1
Intel,k771jw5,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",AMD,2023-10-31 05:46:01,1
Intel,k7aj8ac,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",AMD,2023-10-31 22:34:23,2
Intel,k77bs4b,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,AMD,2023-10-31 08:06:28,1
Intel,k75qm0a,Exactly! It even got xess so Intel users also can use xess,AMD,2023-10-30 23:34:53,2
Intel,k75k4vk,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",AMD,2023-10-30 22:50:44,3
Intel,k74wzhj,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",AMD,2023-10-30 20:23:22,4
Intel,k7708p1,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,AMD,2023-10-31 05:29:56,5
Intel,k77n6u7,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,AMD,2023-10-31 10:41:33,3
Intel,k75ya7o,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",AMD,2023-10-31 00:26:50,3
Intel,k7a5u7g,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",AMD,2023-10-31 21:03:27,0
Intel,k77o6pg,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",AMD,2023-10-31 10:53:04,1
Intel,k755rf9,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",AMD,2023-10-30 21:16:52,5
Intel,k75jput,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",AMD,2023-10-30 22:47:54,4
Intel,k761p0l,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",AMD,2023-10-31 00:49:12,1
Intel,k75i4pq,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,AMD,2023-10-30 22:37:13,2
Intel,k77cvge,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",AMD,2023-10-31 08:22:22,-1
Intel,k755zaf,"Mirage is PS4 game, Avatar is PS5",AMD,2023-10-30 21:18:15,5
Intel,k74a84y,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",AMD,2023-10-30 18:04:20,5
Intel,k75qyxk,Timed epic exclusivity? Aww man.,AMD,2023-10-30 23:37:17,3
Intel,k74vid8,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,AMD,2023-10-30 20:14:32,0
Intel,k74dkaf,You... are.. joking... right..?,AMD,2023-10-30 18:24:49,3
Intel,k770p8h,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,AMD,2023-10-31 05:35:28,2
Intel,k7ftdl4,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,AMD,2023-11-01 23:57:41,3
Intel,k74azia,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,AMD,2023-10-30 18:09:00,0
Intel,k749l36,Try ubisoft achievements :),AMD,2023-10-30 18:00:24,-1
Intel,k7a8c78,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",AMD,2023-10-31 21:19:36,1
Intel,k767klo,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",AMD,2023-10-31 01:28:47,1
Intel,k783twl,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,AMD,2023-10-31 13:15:34,0
Intel,k76sj05,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),AMD,2023-10-31 04:07:38,3
Intel,k76mhsf,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",AMD,2023-10-31 03:14:45,4
Intel,k747o6w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2023-10-30 17:48:49,1
Intel,k74w3s3,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",AMD,2023-10-30 20:18:07,0
Intel,k75if4b,yup that is why I will play 1440 UW native with a 7900XTX.,AMD,2023-10-30 22:39:08,1
Intel,k74vwla,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,AMD,2023-10-30 20:16:56,1
Intel,k75qeu1,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",AMD,2023-10-30 23:33:32,2
Intel,k75p425,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",AMD,2023-10-30 23:24:38,1
Intel,k75qigc,Very similar performance I guess,AMD,2023-10-30 23:34:13,1
Intel,k77bjls,Ye,AMD,2023-10-31 08:03:05,0
Intel,k7813wv,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,AMD,2023-10-31 12:54:43,0
Intel,k77obao,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",AMD,2023-10-31 10:54:29,1
Intel,k77o9a4,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",AMD,2023-10-31 10:53:51,1
Intel,k77udwp,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2023-10-31 11:56:37,1
Intel,k78piiq,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,AMD,2023-10-31 15:40:37,1
Intel,k7e7bz2,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,AMD,2023-11-01 17:53:25,1
Intel,k7eem4z,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,AMD,2023-11-01 18:37:35,0
Intel,k7fs4ss,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",AMD,2023-11-01 23:49:18,0
Intel,k7k3mjx,"with AFMF it works , didnt test with FSR3 now.",AMD,2023-11-02 20:38:03,1
Intel,k8l2h6h,Reading before raging :),AMD,2023-11-10 01:18:48,1
Intel,k74j51i,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",AMD,2023-10-30 18:58:47,67
Intel,k774q01,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,AMD,2023-10-31 06:26:48,4
Intel,k74rg7l,For 30fps even lol,AMD,2023-10-30 19:49:50,1
Intel,k74tk9h,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",AMD,2023-10-30 20:02:46,1
Intel,k779kpw,Or be happy your shitty video card is still supported.,AMD,2023-10-31 07:34:39,0
Intel,k78mpu0,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,AMD,2023-10-31 15:22:45,1
Intel,k77deta,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,AMD,2023-10-31 08:30:28,6
Intel,k77kqzi,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,AMD,2023-10-31 10:12:09,1
Intel,k74w3jk,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,AMD,2023-10-30 20:18:05,12
Intel,k74frkt,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,AMD,2023-10-30 18:38:21,11
Intel,k74k70d,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",AMD,2023-10-30 19:05:17,1
Intel,k7a00mh,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",AMD,2023-10-31 20:26:38,1
Intel,k74w35a,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",AMD,2023-10-30 20:18:01,0
Intel,k77ddfz,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,AMD,2023-10-31 08:29:53,7
Intel,k79hiyj,But it does in many ways.,AMD,2023-10-31 18:33:00,7
Intel,k77ktee,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",AMD,2023-10-31 10:13:00,2
Intel,k78mx3z,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",AMD,2023-10-31 15:24:03,-2
Intel,k792qlt,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,AMD,2023-10-31 17:02:03,6
Intel,k77jb2i,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",AMD,2023-10-31 09:53:38,4
Intel,k776y63,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",AMD,2023-10-31 06:57:06,10
Intel,k75f8fi,speaking facts my guy,AMD,2023-10-30 22:17:50,1
Intel,k77ezjy,You shouldn't have downvote dood wtf redditors ?,AMD,2023-10-31 08:54:03,0
Intel,k79hyhx,People with AMD cards dislike upscaling more because FSR sucks ass lol.,AMD,2023-10-31 18:35:41,4
Intel,k77ebzi,"Uhm, me btw...",AMD,2023-10-31 08:44:25,3
Intel,k77kv35,"> Who actually plays games at native these days, if it has upscaling?  I do.",AMD,2023-10-31 10:13:35,3
Intel,k75rfzg,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",AMD,2023-10-30 23:40:30,2
Intel,k7kxjkt,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",AMD,2023-11-02 23:49:47,3
Intel,k7xpols,onto absolutely nothing.,AMD,2023-11-05 15:20:55,1
Intel,k785u5t,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",AMD,2023-10-31 13:30:13,11
Intel,k75w2l1,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,AMD,2023-10-31 00:12:04,6
Intel,k76yitl,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,AMD,2023-10-31 05:09:41,1
Intel,k77agw9,The game is raytracing only with a ridiculous ammount of foooliage.,AMD,2023-10-31 07:47:35,6
Intel,k77h1xr,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",AMD,2023-10-31 09:23:11,5
Intel,k78n7c0,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,AMD,2023-10-31 15:25:54,3
Intel,k74jbjo,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,AMD,2023-10-30 18:59:54,3
Intel,k77uxf6,False.  guy blocked me lmao,AMD,2023-10-31 12:01:42,1
Intel,k770n5h,"I thought that was on Linux, though I might be wrong",AMD,2023-10-31 05:34:47,0
Intel,k75wi30,1070 doesn't have hardware RT though.,AMD,2023-10-31 00:14:59,3
Intel,k74g227,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),AMD,2023-10-30 18:40:06,26
Intel,k78amg9,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,AMD,2023-10-31 14:03:39,0
Intel,k74qlaz,Completely agree.,AMD,2023-10-30 19:44:35,3
Intel,k7hr3n6,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,AMD,2023-11-02 11:28:02,1
Intel,k7476pu,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),AMD,2023-10-30 17:45:53,2
Intel,k75aybf,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",AMD,2023-10-30 21:50:00,8
Intel,k74gjdn,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",AMD,2023-10-30 18:43:00,9
Intel,k74gjiu,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,AMD,2023-10-30 18:43:01,1
Intel,k74d1es,"Derp, derp.",AMD,2023-10-30 18:21:37,0
Intel,k7gqdq3,Sounds like John on Direct Foundry Direct every week.,AMD,2023-11-02 03:55:28,1
Intel,k7721zf,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",AMD,2023-10-31 05:52:19,3
Intel,k77id8y,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",AMD,2023-10-31 09:41:01,6
Intel,k75quo4,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,AMD,2023-10-30 23:36:30,3
Intel,k75wl3m,Avatar is RT only. There is no non RT mode.,AMD,2023-10-31 00:15:31,7
Intel,k77ar4b,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,AMD,2023-10-31 07:51:38,4
Intel,k76ly3i,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",AMD,2023-10-31 03:10:28,2
Intel,k78qssi,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",AMD,2023-10-31 15:48:32,3
Intel,k75m1nd,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,AMD,2023-10-30 23:03:44,6
Intel,k75t1dz,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,AMD,2023-10-30 23:51:23,1
Intel,k74c5po,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",AMD,2023-10-30 18:16:15,13
Intel,k74ejc7,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",AMD,2023-10-30 18:30:46,5
Intel,k74f5gw,"Next gen seems promising so far, low native rez hidden by upscaling, 60 FPS target on +500$ GPUs, traversal stuttering thanks to the glorious UE5. But hey, we have great reflections on puddles so at least we can take good looking screenshots.",AMD,2023-10-30 18:34:36,6
Intel,k762gt6,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,AMD,2023-10-31 00:54:19,0
Intel,k7795r7,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,AMD,2023-10-31 07:28:35,6
Intel,k8xdnpw,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,AMD,2023-11-12 13:53:39,1
Intel,k77azgf,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",AMD,2023-10-31 07:54:57,0
Intel,k7ad42q,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",AMD,2023-10-31 21:51:09,-1
Intel,k771qsz,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",AMD,2023-10-31 05:48:28,1
Intel,k7baowx,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",AMD,2023-11-01 01:58:28,1
Intel,k76iowi,This is what I am thinking too.,AMD,2023-10-31 02:46:08,1
Intel,k7al3b9,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",AMD,2023-10-31 22:48:01,1
Intel,k78r7ik,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",AMD,2023-10-31 15:51:02,0
Intel,k7eeuc1,Let's hope so. 30fps is far from recommended for FSR 3,AMD,2023-11-01 18:38:59,1
Intel,k7kbe2u,Yeah I heard it works with that hoping it works with fsr3 now,AMD,2023-11-02 21:25:15,1
Intel,k74jv5m,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",AMD,2023-10-30 19:03:14,38
Intel,k770h31,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",AMD,2023-10-31 05:32:44,0
Intel,k74va9v,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",AMD,2023-10-30 20:13:10,-9
Intel,k78mlfi,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,AMD,2023-10-31 15:21:57,5
Intel,k776zix,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,AMD,2023-10-31 06:57:37,3
Intel,k77ebsi,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",AMD,2023-10-31 08:44:20,-1
Intel,k82kpsv,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",AMD,2023-11-06 14:19:02,1
Intel,k792d5j,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",AMD,2023-10-31 16:59:46,2
Intel,k77dtn2,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",AMD,2023-10-31 08:36:45,3
Intel,k77togb,Assassins creed origins and odyssey side quests/collectibles oh my god,AMD,2023-10-31 11:50:06,2
Intel,k74jzj8,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,AMD,2023-10-30 19:04:00,0
Intel,k74m5ig,I appreciate your insights and opinions. Thank you.,AMD,2023-10-30 19:17:14,-1
Intel,k775lx7,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",AMD,2023-10-31 06:38:51,1
Intel,k77sn6f,console games started upscaling way before PCs .,AMD,2023-10-31 11:40:07,5
Intel,k79hu3v,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,AMD,2023-10-31 18:34:55,4
Intel,k79vmwl,you forgot who owns one of the most popular engines out there?,AMD,2023-10-31 19:59:27,-1
Intel,k7abo7k,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",AMD,2023-10-31 21:41:33,-5
Intel,k77b8ol,downvoted by devs lol,AMD,2023-10-31 07:58:40,5
Intel,k79ib8i,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,AMD,2023-10-31 18:37:52,0
Intel,k78h0ux,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",AMD,2023-10-31 14:46:05,-1
Intel,k792yre,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,AMD,2023-10-31 17:03:29,0
Intel,k79ll9w,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,AMD,2023-10-31 18:57:46,2
Intel,k78xgtm,Why is there a dialog message about unsupported hardware when you try and run a 390X?,AMD,2023-10-31 16:29:37,3
Intel,k77blcl,It can do software based RT just like every other modern GPU out there.,AMD,2023-10-31 08:03:49,2
Intel,k74gdb3,Well then no wonder rx 5700 can't manage 30 fps lol,AMD,2023-10-30 18:41:59,28
Intel,k74m0rw,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",AMD,2023-10-30 19:16:25,11
Intel,k74ihd8,Avatars uses a Lumen like software RT solution.,AMD,2023-10-30 18:54:49,19
Intel,k78qd8n,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",AMD,2023-10-31 15:45:52,2
Intel,k74lfw8,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",AMD,2023-10-30 19:12:53,14
Intel,k75797r,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),AMD,2023-10-30 21:26:16,2
Intel,k74khy7,"Yeah, thatsl happened.",AMD,2023-10-30 19:07:09,0
Intel,k7a6im2,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",AMD,2023-10-31 21:07:50,1
Intel,k75wfsk,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",AMD,2023-10-31 00:14:32,1
Intel,k77chzy,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",AMD,2023-10-31 08:16:52,2
Intel,k77pinh,You didn't understand. It's another Swiss knife engine,AMD,2023-10-31 11:07:56,0
Intel,k76pny2,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",AMD,2023-10-31 03:41:22,1
Intel,k75nvkv,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,AMD,2023-10-30 23:16:13,-1
Intel,k77ap82,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,AMD,2023-10-31 07:50:56,-2
Intel,k78n2dv,The PS5 has a 6700.,AMD,2023-10-31 15:25:00,1
Intel,k7i9v2e,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",AMD,2023-11-02 13:58:02,1
Intel,k74jbph,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",AMD,2023-10-30 18:59:55,1
Intel,k8xdwme,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,AMD,2023-11-12 13:55:46,1
Intel,k77fln1,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,AMD,2023-10-31 09:02:53,0
Intel,k7ahuzb,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,AMD,2023-10-31 22:24:31,1
Intel,k78sexp,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,AMD,2023-10-31 15:58:21,1
Intel,k74o0rt,TAA,AMD,2023-10-30 19:28:44,53
Intel,k74rhkm,Temporal anti aliasing.,AMD,2023-10-30 19:50:04,7
Intel,k78mepd,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",AMD,2023-10-31 15:20:46,4
Intel,k78zoea,"Ahh, welcome to r/FuckTAA",AMD,2023-10-31 16:43:15,1
Intel,k7fo2qt,Even 4K looks blurry with some implementations of TAA,AMD,2023-11-01 23:21:50,1
Intel,k775ulz,they're giving you the bare minimum until your upgrade!,AMD,2023-10-31 06:42:13,0
Intel,k74lgqm,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,AMD,2023-10-30 19:13:01,-6
Intel,k7551yk,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",AMD,2023-10-30 21:12:24,10
Intel,k74xjh2,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",AMD,2023-10-30 20:26:40,12
Intel,k7549zz,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",AMD,2023-10-30 21:07:34,3
Intel,k78jvij,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",AMD,2023-10-31 15:04:19,4
Intel,k79h6tb,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,AMD,2023-10-31 18:30:53,2
Intel,k77gqlw,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",AMD,2023-10-31 09:18:51,2
Intel,k78mn3v,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",AMD,2023-10-31 15:22:16,4
Intel,k7kwbcl,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,AMD,2023-11-02 23:41:35,1
Intel,k783kme,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",AMD,2023-10-31 13:13:39,-1
Intel,k7adm22,No I havent. AMD is bigger than Epic.,AMD,2023-10-31 21:54:30,2
Intel,k7aqfmz,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",AMD,2023-10-31 23:27:21,5
Intel,k77exua,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",AMD,2023-10-31 08:53:22,0
Intel,k79j5la,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",AMD,2023-10-31 18:43:01,6
Intel,k78wlj1,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,AMD,2023-10-31 16:24:13,6
Intel,k79ipwc,It's software ray tracing which isn't accelerated by hardware.,AMD,2023-10-31 18:40:20,2
Intel,k79sb21,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,AMD,2023-10-31 19:39:06,1
Intel,k79a73q,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,AMD,2023-10-31 17:47:59,2
Intel,k74o26m,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,AMD,2023-10-30 19:28:58,19
Intel,k778s32,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",AMD,2023-10-31 07:23:06,1
Intel,k75bea7,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,AMD,2023-10-30 21:52:53,5
Intel,k75j5le,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,AMD,2023-10-30 22:44:05,6
Intel,k74nnyo,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",AMD,2023-10-30 19:26:34,3
Intel,k7a88v5,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",AMD,2023-10-31 21:19:00,1
Intel,k77pvj3,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,AMD,2023-10-31 11:11:45,4
Intel,k78lq5t,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",AMD,2023-10-31 15:16:17,3
Intel,k76zwtm,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,AMD,2023-10-31 05:25:55,2
Intel,k77ddap,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",AMD,2023-10-31 08:29:49,7
Intel,k77dhd7,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",AMD,2023-10-31 08:31:33,6
Intel,k77h10q,Both excellent 👌 Down the Rabbit Hole was another solid one.,AMD,2023-10-31 09:22:50,1
Intel,k7b776f,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",AMD,2023-11-01 01:31:29,1
Intel,k78ut1x,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",AMD,2023-10-31 16:13:03,0
Intel,k74xlfo,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,AMD,2023-10-30 20:27:00,27
Intel,k74uq1m,I have to turn it off in borderlands 3. Ugh.,AMD,2023-10-30 20:09:47,1
Intel,k78zmsb,r/FuckTAA,AMD,2023-10-31 16:42:58,0
Intel,k7n69qz,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",AMD,2023-11-03 12:46:56,1
Intel,k7gyunv,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",AMD,2023-11-02 05:20:45,1
Intel,k7bw0wl,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,AMD,2023-11-01 05:13:29,-1
Intel,k78p5rj,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",AMD,2023-10-31 15:38:25,2
Intel,k7kx549,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,AMD,2023-11-02 23:47:05,1
Intel,k7c5sa3,epic is tencent...,AMD,2023-11-01 07:26:14,-1
Intel,k79evb5,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",AMD,2023-10-31 18:16:40,0
Intel,k79vqds,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",AMD,2023-10-31 20:00:02,1
Intel,k79dbl9,You don't need RT hardware to do software RT. That's what I'm saying.,AMD,2023-10-31 18:07:06,3
Intel,k74wirt,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",AMD,2023-10-30 20:20:36,20
Intel,k755cwh,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",AMD,2023-10-30 21:14:19,20
Intel,k74ogkv,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),AMD,2023-10-30 19:31:29,16
Intel,k751p3z,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,AMD,2023-10-30 20:51:38,10
Intel,k75za9c,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",AMD,2023-10-31 00:33:28,10
Intel,k76k5hm,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,AMD,2023-10-31 02:56:53,3
Intel,k77u7zx,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,AMD,2023-10-31 11:55:06,2
Intel,k77anl2,Yes hah,AMD,2023-10-31 07:50:17,1
Intel,k78kxsq,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",AMD,2023-10-31 15:11:02,1
Intel,k75bm3k,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",AMD,2023-10-30 21:54:17,1
Intel,k75jy4k,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,AMD,2023-10-30 22:49:28,7
Intel,k75cf9l,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",AMD,2023-10-30 21:59:30,11
Intel,k74s4b6,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",AMD,2023-10-30 19:53:57,5
Intel,k78qh5r,Oh man the hair... its so crazy how much upscaling kills the hair..,AMD,2023-10-31 15:46:32,0
Intel,k74vpzd,i mean they already arent the smartest bulbs considering they went team green.,AMD,2023-10-30 20:15:50,-8
Intel,k7bwwni,Reading comrehension dude.,AMD,2023-11-01 05:24:02,1
Intel,k77rq8u,"I do, but thanks for your interest.",AMD,2023-10-31 11:31:01,0
Intel,k781oqh,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,AMD,2023-10-31 12:59:12,1
Intel,k77gylo,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",AMD,2023-10-31 09:21:55,0
Intel,k77prws,Nice. It’s on the list. Thanks man,AMD,2023-10-31 11:10:40,0
Intel,k79111l,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,AMD,2023-10-31 16:51:38,1
Intel,k74y62b,Probably because TAA is (unfortunately) more prevalent than it ever was.,AMD,2023-10-30 20:30:23,13
Intel,k77crxd,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",AMD,2023-10-31 08:20:57,5
Intel,k7ky7yi,Oh my bad if I read that wrong,AMD,2023-11-02 23:54:13,1
Intel,k79nw3z,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,AMD,2023-10-31 19:11:53,1
Intel,k74xr8d,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,AMD,2023-10-30 20:27:57,-11
Intel,k78nlj0,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",AMD,2023-10-31 15:28:29,1
Intel,k78nx4p,imagine being mad that 4 year old cards arent high end,AMD,2023-10-31 15:30:32,2
Intel,k78qsw0,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,AMD,2023-10-31 15:48:33,0
Intel,k75i9u4,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",AMD,2023-10-30 22:38:10,1
Intel,k77gx0m,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",AMD,2023-10-31 09:21:18,1
Intel,k74smha,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",AMD,2023-10-30 19:57:03,1
Intel,k78qwba,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",AMD,2023-10-31 15:49:08,0
Intel,k74wqog,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",AMD,2023-10-30 20:21:55,2
Intel,k7bxbe9,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",AMD,2023-11-01 05:29:04,1
Intel,k77y4x2,"It's for textures, not object edges from FSR use, lol. Two completely different things",AMD,2023-10-31 12:30:16,5
Intel,k793lf6,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",AMD,2023-10-31 17:07:23,-1
Intel,k77g0i6,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",AMD,2023-10-31 09:08:37,12
Intel,k77ubrs,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",AMD,2023-10-31 11:56:04,-1
Intel,k8f5yw4,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",AMD,2023-11-08 22:27:09,1
Intel,k79oe6d,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,AMD,2023-10-31 19:14:56,1
Intel,k753gel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",AMD,2023-10-30 21:02:23,12
Intel,k78lli0,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,AMD,2023-10-31 15:15:24,4
Intel,k7au2y4,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",AMD,2023-10-31 23:54:23,1
Intel,k75wc3v,5700 was probably the lowest AMD card they had to test with.,AMD,2023-10-31 00:13:51,1
Intel,k74tcrl,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",AMD,2023-10-30 20:01:29,4
Intel,k7byrit,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,AMD,2023-11-01 05:47:35,1
Intel,k7a8jw6,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,AMD,2023-10-31 21:20:59,1
Intel,k7kn3dl,AMD CAS is aimed to restore detail,AMD,2023-11-02 22:40:21,1
Intel,k7k5dha,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",AMD,2023-11-02 20:48:40,1
Intel,k7cl7iw,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,AMD,2023-11-01 10:59:12,2
Intel,k74ugbm,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",AMD,2023-10-30 20:08:12,0
Intel,k7bzcr6,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",AMD,2023-11-01 05:55:17,1
Intel,k7cntlm,Yeah but a 3060ti didn't,AMD,2023-11-01 11:27:22,1
Intel,k78mcrt,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",AMD,2023-10-31 15:20:25,2
Intel,k74zgnn,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",AMD,2023-10-30 20:38:09,3
Intel,k1rzmke,Confirmed Can it run CP2077 4k is the new Can it run Crysis,AMD,2023-09-22 22:22:20,588
Intel,k1s9f6d,It'll be crazy seeing this chart in 5-10 years with new gpu's pushing 60-120 fps with no problem.,AMD,2023-09-22 23:31:10,584
Intel,k1rtgmz,In 1440p with 7900 xtx with fsr 2.1 quality it doesn’t even get 30fps,AMD,2023-09-22 21:41:53,306
Intel,k1tezss,"Makes sense. Is almost full path tracing, it’s insane it’s even running.",AMD,2023-09-23 05:11:42,24
Intel,k1spmgk,good. this is how gaming should be. something to go back to before the next crysis shows its teeth,AMD,2023-09-23 01:30:07,20
Intel,k1ryede,3080 falling behind a 3060? what is this data?,AMD,2023-09-22 22:14:11,125
Intel,k1say8p,wake me up when we have a card that can run this at 40 without needing its own psu.,AMD,2023-09-22 23:42:17,47
Intel,k1seqym,5090 here I come!,AMD,2023-09-23 00:09:51,26
Intel,k1sf8id,"Playing the game maxed out with path tracing, FG, RR and DLSS set to balanced at 1440p. Over 100fps 90% of the time. Incredible experience.  *With a 4070 ti and 13600k",AMD,2023-09-23 00:13:23,38
Intel,k1samn8,"I mean, Path tracing is to Ray tracing, what Ray tracing is too rasterization.",AMD,2023-09-22 23:39:57,29
Intel,k1se7w7,When Cyberpunk first came out the 3090 only got 20 fps in RT Psycho mode.  https://tpucdn.com/review/nvidia-geforce-rtx-4090-founders-edition/images/cyberpunk-2077-rt-3840-2160.png  Still does  Fast forward just one gem and you don't see anyone saying its demanding with many able to get RT Psycho on thier cards as new cards got faster.  Give it 2 gens and you are going to get 4k60 here.  Gpu will improve and get faster.,AMD,2023-09-23 00:06:00,28
Intel,k1tcwd4,"Look at that, my 6700XT is just 19fps slower than the RTX 4090 in this title.",AMD,2023-09-23 04:49:38,6
Intel,k1swxw6,It's a good thing nobody has to actually play it native.,AMD,2023-09-23 02:26:41,18
Intel,k1sd0w5,Well good thing literally nobody is doing that…,AMD,2023-09-22 23:57:14,9
Intel,k1tv5m1,The future is not in native resolution so this is really pointless information.  Cyberpunk looks absolutelt mindblowlingy insane with all the extra graphical bells and whistles it has gotten over the years and with Nvidia’s technology it runs so damn smooth as well.,AMD,2023-09-23 08:24:44,11
Intel,k1svrss,4.3fps lol. No amount of upscaling is going to fix that and make it playable. People were saying the 7900xtx had 3090ti levels of RT when it launched. A 4060 ti is 50% faster then it.,AMD,2023-09-23 02:17:32,16
Intel,k1u5n8e,That's actually pretty amazing for any GPU to get a metric in frames per second instead of minuites per frame.,AMD,2023-09-23 10:40:09,4
Intel,k1t06iw,"Meh I get like 90+ fps with everything cranked with RR, FG and DLSS(Balanced) toggled on with my 4090 @ 4k.  Path tracing does introduce ghosting which is annoying buts not really noticeable most times but at the same time with RR enabled it removes shimmering on 99% of objects that is normally introduced with DLSS so I am willing to compromise.   Honestly as someone who used to have a 7900XTX I am disappointed with AMD its clear that AI tech in gaming is the way forward and they just seem so far behind Nvidia now and even Intel(going by some preview stuff). FSR is just not even comparable anymore.",AMD,2023-09-23 02:52:43,11
Intel,k1sta67,Who cares when it looks worse than with dlss and ray reconstruction on top of running a lot worse? Native res 4k is pointless.,AMD,2023-09-23 01:58:23,15
Intel,k1ru0dy,"Well the upscaling in this game is really good, DLSS with ray reconstructions AI accelerated denoiser provides better RT effects than the game at native with its native Denoiser.  Also Path tracing scales perfectly with resolution so upscaling provides massive gains. using DLSS quality doubles performance to 40fps, and dlss balanced gives 60fps on average, performance about 70-80 or 4x native 4K, that includes the 10-15% performance gain RR gives as well over the native denoiser as well.  I've been playing with about 60fps on average 50-70. with DLSS Balanced and RR and its been amazing. I don't like frame gen tho since it causes VRR flickers on my screen",AMD,2023-09-22 21:45:23,26
Intel,k1sh29m,"Running Ray Tracing or Path Tracing without DLSS or Ray Reconstruction is like intentionally going into a battlefield without any gear whatsoever, it's absolutely pointless and suicidal, what we can clearly see here though is top of the line 7900 XTX losing against already mediocre mid-range 4060 Ti by over 50%, which is just beyond embarrassing for AMD Radeon.  All this says to me is AMD Radeon need to get their shit together and improve their RT / PT performance, otherwise they will continue to lose more Marketshare on GPU department no matter how hard their fanboys thinks that they are pointless features, just like DLSS was back on 2020 right?  Also, with my 4070 Ti OC i can run it at average of over 60 FPS at 1440p DF optimized settings with DLSS Balanced + Ray Reconstruction without even using DLSS Frame Gen, with it on i can get over 80+ FPS",AMD,2023-09-23 00:26:40,18
Intel,k1rwvmw,Running this way means you lose RR…why in the world would you run at native 4k?  It’s completely pointless now with RR.,AMD,2023-09-22 22:04:05,22
Intel,k1s5ads,Im having 80 fps  thanks to dlss 3.5  and its looking better than ever,AMD,2023-09-22 23:01:20,9
Intel,k1t5pl6,"Why would I run native? I have amazing DLSS, ray reconstruction for huge image gains and frame gen. Nvidia offering all the goodies.",AMD,2023-09-23 03:40:25,7
Intel,k1sde74,"I have a 13900KF-4090 rig and a 7800X3D-7900XTX rig. They are connected to a C2 OLED and to a Neo G9.  I've been holding on to play the game till 2.0 update. I've tried many times with different ray-tracing options and they all look good and all. But in the end I closed them all, turned back to Ultra Quality without ray-tracing and started playing the game over 120FPS.  This is a good action fps game now. I need high fps with as low latency as possible. So who cares about ray-tracing and path-tracing.  Yeah ray-tracing and path-tracing are good. But we are at least 2-3 generations away for them to become mainstream. When they are easily enabled on mid-range gpus with high-refresh rate monitors, they will be good and usable then :)",AMD,2023-09-22 23:59:57,18
Intel,k1sbtem,"If you have the HW, go all out. That's why we spend on these things.  I'm getting the best experience you can get in the premiere visual showcase of a good game.   It's path tracing. It isn't cheap but the fact that we have DLSS and FG with Ray reconstruction is a Godsend. It looks stunning and it's still early in development.",AMD,2023-09-22 23:48:27,5
Intel,k1sr9s2,This doesn’t seem right…. 3080 performing worse than a 2080TI?,AMD,2023-09-23 01:42:55,2
Intel,k1t2kii,How tf is a 2080 ti getting more fps than a 3080?!?,AMD,2023-09-23 03:12:42,2
Intel,k1t66da,I'm not seeing the RTX A6000 on here...,AMD,2023-09-23 03:44:41,2
Intel,k1tglq7,4 fps for 7900 XTX  Oof. Shows you how much of a gap their is between AMD & Nvidia with pure ray tracing.,AMD,2023-09-23 05:29:29,2
Intel,k1u9v0r,"Everything maxed, PT, 1440p, DLSS+RR+FG, input feels good, game looks and runs great, 100+ fps",AMD,2023-09-23 11:28:08,2
Intel,k1ufe40,"""Get Nvidia if you want a good ray tracing experience""  Yes Nvidia GPUs give a better ray tracing experience but is it really worth it if you are required to turn on DLSS? Imo, the more AI-upscaling you have to turn on, the worse the Nvidia purchase is.   I have a 6900XT and I will readily admit that the RT experience (ultra RT, no path tracing) at native 1080p resolution is ok, like 30-45 FPS (around 50 on medium RT settings), but if I turn RT lighting off (so RT shadows, sunlight, and reflections are still present) suddenly I get pretty consistent 60 FPS (i left my frames tied to monitor refresh rate, so 60 FPS is my max) and i can't tell the damn difference at native 1080p compared to RT medium or Ultra.   So would I spend another $400-$1000 to get an imperceptible outcome (imperceptible to me that is)? Most definitely not.",AMD,2023-09-23 12:22:22,2
Intel,k1witvs,Does anyone actually play this game? Its more of a meme game imho,AMD,2023-09-23 20:46:05,2
Intel,k1xd78i,4.3 fps 😂😂😂,AMD,2023-09-24 00:13:50,2
Intel,k1sczh9,"RTX 4090 DESTROYS 7900XTX with over 400% increased FPS, coming in at an astounding…. 19.5FPS 😫😂",AMD,2023-09-22 23:56:57,10
Intel,k1rumnu,"Overclocked RTX 4090 can (there are factory OC models that run up to 8% faster than stock). A measly 2.5% overclock would put that at 20 FPS.  Native is irrelevant though, DLSS Quality runs much faster with similar image quality.",AMD,2023-09-22 21:49:26,11
Intel,k1shlno,[85-115fps (115fps cap) in 4k dlss balanced](https://media.discordapp.net/attachments/347847286824370187/1154937439354368090/image.png) ultra settings,AMD,2023-09-23 00:30:36,3
Intel,k1t2byj,Lol the 3060ti is better than a 7900xtx...crazy,AMD,2023-09-23 03:10:40,2
Intel,k1vuadd,Why would you use it without DLSS? Upscaling is the way of the future and AMD better improve their software to compete. Future GPU's aren't going to be able to brute force their way to high frames.,AMD,2023-09-23 18:12:06,3
Intel,k1s8b2u,Who cares about native though.  Cyberpunk PT at 4k DLSS Balanced + FG is graphically so far ahead of any other game ever released that it literally doesn't even matter if other games are run at native or even 8K. Cyberpunk is just in league of its own.,AMD,2023-09-22 23:23:07,10
Intel,k1t61f2,ThE gAMe iS PoOrLY OpTImiSed!!!,AMD,2023-09-23 03:43:26,2
Intel,k1ttlda,Who cares about 20 fps native? Use the tools and tricks provided by the game and card manufacturer and you are good. The whole native rendering argument can be thrown in the garbage bin.,AMD,2023-09-23 08:04:31,2
Intel,k1sgjyj,"The 7900xtx losing NATIVELY to the 4060ti 16gb is embarrassing, that card is half the price and overpriced, nevermind Nvidia's far superior upscalers. AMD falls further and further behind in RT, sad.",AMD,2023-09-23 00:22:53,3
Intel,k1sxb06,"99.7fps  with everything maxed out at 4k balanced with frame gen.   64.3fps with everything maxed out at 4k balanced w/out frame gen.   29.4fps with everything maxed out at 4k native.   That’s a big difference from this chart what I’m getting vs what they’re getting. For record, I’m using a 4090 suprim liquid undervolted to 950mv at 2850mhz. Stock is 2830mhz for my card.",AMD,2023-09-23 02:29:34,2
Intel,k1sydqu,"Even with frame gen and DLSS quality, Ultra+Path Tracing looks incredible. I get solid 90 FPS. Funny thing is I can't get into playing CP 2077, even with incredible graphics and a QD-OLED monitor. It's just not my type of game :-D",AMD,2023-09-23 02:38:06,2
Intel,k1tbdbm,I was wondering why a thread like this with the name like this is in this sub. And the I see the name of the OP. And it all makes sense,AMD,2023-09-23 04:34:09,2
Intel,k1twz2u,"""4090 is 4 times faster than 7900XTX""",AMD,2023-09-23 08:48:39,2
Intel,k1uouoc,"So? Native 4k is such a waste of compute hardware, only an idiot would use it.",AMD,2023-09-23 13:40:46,2
Intel,k1wadup,People need to change their mind frame. AI is here to stay and will be a major part of graphics rendering going forward.,AMD,2023-09-23 19:52:54,2
Intel,k1tl3qo,I played 2.0 yesterday with 2k ultra and path tracing with 155fps (DLSS3.5),AMD,2023-09-23 06:20:29,1
Intel,k1s8g0f,"The 4060 Ti literally better than the 7900 XTX in RT, cope snowflakes!111!11! /s",AMD,2023-09-22 23:24:08,2
Intel,k1s8o3g,"So Nvidia only needs to increase their RT performance 3x while AMD needs to do 14x. We are not seeing that within a decade, if that.",AMD,2023-09-22 23:25:45,2
Intel,k1sg5kw,Sounds like Ray tracing isn’t ready for prime if nothing can run it and you have to use AI smoke and mirrors to fake it playable. I’ll worry about Ray tracing when it’s a toggle on or off like shadows and there is no major performance hit. See you in the 9000-10000 series of Nvidia or 12000-13000 series for AMD.,AMD,2023-09-23 00:20:00,2
Intel,k1skakr,Native resolution is a thing of the past. DLSS looks better than native anyway.,AMD,2023-09-23 00:50:07,1
Intel,k1txff8,Is this with DLSS + FG?,AMD,2023-09-23 08:54:33,1
Intel,k1smbmr,"Ray tracing already wasn't worth it imo, that's why I saved money buying an AMD card. It's not worth the cut in frame rate that then has to be filled with upscaling that makes frames look blurry anyway, so what's the point of even using it?",AMD,2023-09-23 01:05:06,-1
Intel,k1s3u4i,"Meanwhile me using medium settings + performance dlss on 3080 ti to get that locked 120fps. I just can't go back to 60fps. The game still looks amazing, especially on blackstrobed oled.",AMD,2023-09-22 22:51:01,1
Intel,k1t2qkw,What is even with with all these path traced reconstruction bullshit lately? Ray tracing itself isn't even that mainstream yet.,AMD,2023-09-23 03:14:12,2
Intel,k1sgzlr,No one is using these settings.,AMD,2023-09-23 00:26:07,1
Intel,k1spqtc,"Ray tracing is fun and all, but games have really improve much.. Same animations.. Same type of quest... Same listening to notes.. There have to be better ways....",AMD,2023-09-23 01:31:01,1
Intel,k1shcs4,"4K rendering for games like this is pretty pointless unless you're upscaling it to a massive 8K display via DLSS Performance.  Ultimately, the final image quality is going to be more affected by how clean the Ray / Path Tracing is than whether you're rendering at Native 4K or not.",AMD,2023-09-23 00:28:49,1
Intel,k1sj5mo,"I'm honestly kind of shocked my 4070 is that far above the 7900XTX. I'd have thought it would be no where near close, regardless of ray tracing.",AMD,2023-09-23 00:41:56,1
Intel,k1ulwx6,cyberpunk sucks don't worry about it,AMD,2023-09-23 13:17:47,1
Intel,k1rvukw,3080ti?,AMD,2023-09-22 21:57:22,1
Intel,k1sljwz,"Yes, that is true, but what the numbers do not tell you is that if you tweak some settings, you are able to get 90 fps on that resolution with path tracing with no problem.",AMD,2023-09-23 00:59:20,1
Intel,k1sqr9g,I feel like the 4k resolution is here to stay for a couple years boys. Get yourself a decent 4k monitor or OLED TV and chill.,AMD,2023-09-23 01:38:55,1
Intel,k1tt1b6,"This is why DLSS3 and frame generation are so important. 4x the performance at over 80FPS with a relatively minor amount of silicon dedicated to AI, with only minor visual degradation.",AMD,2023-09-23 07:57:19,1
Intel,k1tuc3v,"It's way too early for using ray / path tracing in games comfortably, maybe in 10 years it'll be different, but for the foreseeable future I'm more than happy with normal rasterization.",AMD,2023-09-23 08:14:11,1
Intel,k1uq4ce,Who cares tho? Its not even the way it's meant to be ran. This tech is is a synergy. No point in trying to 'flex' native at this point. A 4090 gets very high FPS and amazing visuals when doing things right.,AMD,2023-09-23 13:50:25,1
Intel,k1s9ert,can wait in 2030 with the 8030RTX low profile 30 watt card run this game with full path tracing in 4k 120fps lol (in 2030 a low end card will run this like butter don't down vote me because you didn't get it lol),AMD,2023-09-22 23:31:05,-1
Intel,k1rzvbb,"Seems like this tech is only around to give Nvidia a win. Also path and ray tracing doesn’t always look better. If the one true God card can’t run it, is it reasonable tech or is it a gimmick? Especially seeing how like no one has a 4090. Like a very small percentage of PC gamers have one. Most aren’t even spending $1600 on their whole build!",AMD,2023-09-22 22:23:58,-17
Intel,k1sfn3w,What body part do you think will get me a 5090?,AMD,2023-09-23 00:16:18,0
Intel,k1txo0m,"Once they introduced this dlss upscaling anti-lag all these gimmicks I don't think we've seen true 4K native and almost anything lately.  This is maybe one game that does look that good but many of the games don't look that great and have some really serious system requirements when there are games that look much better and are much better optimized.  Companies are taking shortcuts, instead of optimizing they give you suboptimal and use FSR dlss all these gimmicks.  Don't get me wrong I always thought it was great for people who wanted to reach a certain frames per second someone with a weaker system to allow them to play at least the minimal at a steady frame rate.  It just seems like this is the normal now non-optimized games.  The 4090 is a beast if you could only get about 20 frames native with Ray tracing it tells you everything you need to know about Ray tracing.  It needs to be reworked that needs to be a new system I believe one real engine has it built in in a certain way probably not as good as Ray tracing directly.  To me it seems like ray tracing is hampering many games that tried to add it, too much focus on that instead of actual making a good game.  Let's daisy chain 4 4090s ..gets 60 fps.. and then house goes on fire!  Obviously I'm being sarcastic but realistically it's actually pretty sad a 4090 20 frames per second.",AMD,2023-09-23 08:57:42,0
Intel,k1u7ngf,"Its not really surprising, raytracing is decades away from being truly mainstream. I applaud the misguided pioneers who betatest the technology, their sacrifice will be honoured",AMD,2023-09-23 11:04:00,0
Intel,k1uxont,"bro, for who are they making such intensive/badly optimised games? for aliens with more powerful pcs? i trully dont get it.",AMD,2023-09-23 14:44:13,0
Intel,k1s5mej,That not even proper path tracing.,AMD,2023-09-22 23:03:45,-1
Intel,k1s6qt1,And I thought that ray tracing was a needlessly huge performance drop for a barely noticeable difference...,AMD,2023-09-22 23:11:53,-6
Intel,k1sb5wk,"Can't wait to buy a $1,600 GPU to run a game at 20fps!!!  &#x200B;  Nvidia fans: i cAnT wAiT 🤑🤑🤑",AMD,2023-09-22 23:43:51,-8
Intel,k1svwfa,Native is a thing of the past when dlss produces better looking image,AMD,2023-09-23 02:18:33,-3
Intel,k1sdfih,Lmfao ... where are all those nvidia fan boys who prefer eye-candy path trace over 60fps?,AMD,2023-09-23 00:00:13,-4
Intel,k1s7xzc,Sounds like some efficient tech. Lmao. What's the fucking point if it makes the game unplayable? RT just washes out the image and looks like shit. Kills the contrast. Can't wait for this fad to end,AMD,2023-09-22 23:20:30,-9
Intel,k1slr2s,In my opinion it's too early for Ray / path tracing it's computationally expensive and if you are using rasterization at ultra settings not only does look nearly identical but it's easier to do the work for the graphics and that gives you more fps...,AMD,2023-09-23 01:00:47,-1
Intel,k1sfvki,"Rather than having 2-3fps, I would go and see a video of path tracing.",AMD,2023-09-23 00:17:59,0
Intel,k1sihxn,"Does this fact really matter tho? Dlss in cyberpunk is known the look on par or even better than native, frame generation will further smooth it out. I can definitely see the 7900xtx being able to do path tracing at 1440p at least once amd release fsr 3 and better once they release something to now match ray reconstruction which also give a small boost in fps making path tracing even more doable.",AMD,2023-09-23 00:37:09,0
Intel,k1su9sh,cool idc i wont play that shit at such shit settings,AMD,2023-09-23 02:05:55,0
Intel,k1syk9g,Do we need this BS? Seriously? Gpu’s will end-up costing over 2 grand soon,AMD,2023-09-23 02:39:33,0
Intel,k1tpxos,I don't even get the hype around ray tracing most games don't have it and when they do it either makes the game look blurry or isn't noticeable unless you're in a weirdly lit area or looking at a reflection,AMD,2023-09-23 07:18:17,0
Intel,k1tvoqm,"Am replaying it with a 6950xt and a 7700k(defo bottleneck).  All maxed out(no RT) at 1440p. If I wanted 4k I would have to enable FSR.  Its completely playable imo.   Mostly 50-60fps.  For me RT vs off vs path tracing.   The game doesn't look more real or better, it just looks different.  But ignorance is bliss.",AMD,2023-09-23 08:31:41,0
Intel,k1tytwk,This chart is bull. Since when is a RTX 3060 faster than a 3080. Someone needs to seriously question the source of the chart,AMD,2023-09-23 09:12:53,0
Intel,k1tzugl,Just goes to prove once again that Raytracing was pushed at least 4-5 gens too early as a pure marketing gimmick by Nvidia,AMD,2023-09-23 09:26:15,0
Intel,k1uhusa,"The Tech Power Up article says:   >Instead of the in-game benchmark we used our own test scene that's within the Phantom Liberty expansion area, to get a proper feel for the hardware requirements of the expansion  So it's very hard to reproduce their data at the moment, but I've run 3 tests with the built in benchmark (run to run variance below 1%, so 3 runs are enough) and my config is averaging 25.7 fps at 4K native with Path Tracing, so the title's premise is not correct. Still, the game is not playable without upscaling at 4K with Path Tracing, so the basis of the sentiment remains correct. Benchmark data available [here](https://capframex.com/api/SessionCollections/279f9216-bdb8-4986-a504-91363328adbe).",AMD,2023-09-23 12:44:21,0
Intel,k1v2ghf,RTX 4090 1600€ 19FPS 👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼,AMD,2023-09-23 15:16:08,0
Intel,k1v8o7m,"Yay i will get 13,8 FPS with my 4080 👍😂 Someone want to borrow some frames? I don't need them all!",AMD,2023-09-23 15:55:59,0
Intel,k1vfxae,The first card on this list a normal person would consider buying is a 4070. 8.5 fps lol.   Even with upscaling that's basically unplayable.   Then you have all the UE5 and path trace type demos where a 4090 is only getting 60fps with upscaling.. at 1080p lol.   We are not remotely ready for these technologies yet. They're at least one if not two console generations away. So multiple years.,AMD,2023-09-23 16:41:57,0
Intel,k1vjs6i,"Looks like 4K is mainly just for videos, not gaming for the time being.",AMD,2023-09-23 17:06:09,0
Intel,k1vnxlu,How can you bring out such technology and no hardware can process it properly.  an impudence,AMD,2023-09-23 17:32:09,0
Intel,k1wm3gk,the question is who need path traycing? i mean that cool and beautiful but 20 fps with a 1600dollar price tag that wild.,AMD,2023-09-23 21:06:13,0
Intel,k1ysyyz,You could just you know... disable path tracing and get 70 fps or go below 4k,AMD,2023-09-24 08:12:13,0
Intel,k1z173o,"That what I expect when a game gets built specifically to market Nvidia, but Nvidia over estimated their technology.",AMD,2023-09-24 09:58:57,0
Intel,k25idkz,"um...im running cp 2077 on a 4090 at 70-90fps, 5k monitor with path tracing/ray reconsturction on and most settings maxed out...",AMD,2023-09-25 16:06:18,0
Intel,k1rv0mr,Yo guys let's make this really unoptimized game then rely on hardware manufacturers to use AI to upscale the resolution and insert extra made-up frames to make it playable on a $1600 GPU lmao.,AMD,2023-09-22 21:51:56,-33
Intel,k1sbmt5,"so it is settled, Devs have zero optimization in games.",AMD,2023-09-22 23:47:08,-12
Intel,k1t5ilx,"Love how shit the 3080 was lol, even when it came out it was shit. But somehow everyone loved it",AMD,2023-09-23 03:38:40,-2
Intel,k1t8qzc,"Or, hear me out, developers could just do better. I've seen far better looking games without the constant frame hit.   Cyberpunk hasn't looked good enough to justify the frame rates from the get go.",AMD,2023-09-23 04:08:39,-1
Intel,k1syqs1,Once again proving RT is useless,AMD,2023-09-23 02:41:00,-6
Intel,k1sfuie,Cyberpunk w/ path tracing at max settings seems even more demanding than Crysis was at launch.   20fps with a 4090 is insane.,AMD,2023-09-23 00:17:45,325
Intel,k1sw4a8,"""only gamers get this joke""",AMD,2023-09-23 02:20:15,2
Intel,k1tqj3o,"People have forgotten or are too young to remember when Nvidia had hardware tessellation features when that was first emerging. They had tessellation maps under the ground and water in Crysis that were doing nothing other than tanking AMD (ATi) performance. I am not saying this whole Cyberpunk thing is exactly the same, but it's essentially similar. In all honesty, turning everything up to 11 on my 3090 doesn't net much visual gain beyond some mirror like reflections and nothing I really care about when actually playing the game (compared to standard High/Ultra) and not pixel peeping, which seems to be the new game now for people buying expensive GPUs, so they can justify their purchase. Meanwhile, everyone else just gets on with enjoying video games instead of living vicariously through someone's 4090 at LOL 1080p",AMD,2023-09-23 07:25:29,-3
Intel,k1uey5o,Starfield is peaking around the corner.,AMD,2023-09-23 12:18:19,0
Intel,k1ume3r,starfield as well,AMD,2023-09-23 13:21:33,0
Intel,k1smpf3,I remember when physx was so demanding people had a dedicated 2nd Nvidia graphics card just to turn the setting on in Arkham City. Now it's considered so cheap to calculate that we just do it on the CPU lmao,AMD,2023-09-23 01:07:57,413
Intel,k1sm1vi,The new Crysis,AMD,2023-09-23 01:03:02,18
Intel,k1t68ad,"I wouldn’t be so optimistic. Transistor shrinking is crazy hard now, and TSMC is asking everyone to mortgage their house to afford it.",AMD,2023-09-23 03:45:09,36
Intel,k1tduwg,How long until a $200 card can do that?,AMD,2023-09-23 04:59:33,12
Intel,k1tl6bz,Most improvement is probably going to AI software more than hardware in the next few years.,AMD,2023-09-23 06:21:18,8
Intel,k1sui5w,"8800GT giving advice to 4090:  “I used to be 'with it. ' But then they changed what 'it' was. Now what I'm with isn't 'it' and what's 'it' seems weird and scary to me. It'll happen to you!""",AMD,2023-09-23 02:07:40,32
Intel,k1tggmb,GPU need to have its own garage by then,AMD,2023-09-23 05:27:53,6
Intel,k1sjd2a,Yep! Insane how fast things change.,AMD,2023-09-23 00:43:26,17
Intel,k1th7kz,"Most likely there will be no gpu that supports path tracing and gives you native 4k 120fps in 5 years, maybe even in 10 years.  The technology has slowed down a bit. It’s increasingly more challenging to make more dense chips.  That’s why Intel has been struggling for many years already and every iteration of their cpus gives only minor improvements. AMD went with chiplets but this approach has its own problems.  Nvidia stands out only because of AI. Raw performance increase is still not enough to play native 4k even without ray tracing.",AMD,2023-09-23 05:36:13,25
Intel,k1tksc7,Yeah rtx 12060 with 9.5 gb Vram will be a monster,AMD,2023-09-23 06:16:50,14
Intel,k1toc97,I'm willing to bet hardware improvement will come to a halt before that.,AMD,2023-09-23 06:59:10,3
Intel,k1tp7w0,"In 10 years AI based upscaling will be so good, no one will want to natively render unless they are generating training data",AMD,2023-09-23 07:09:42,9
Intel,k1tk7k9,10 is too much. Give it 5.,AMD,2023-09-23 06:10:19,2
Intel,k1u5t5c,Transistor density advancements have been declining for a good while now. We can't expect hardware performance gains of old to continue into the future,AMD,2023-09-23 10:42:10,2
Intel,k1u9trt,Like the 1080 barely doing 4k30 and now we have gpus that do 4k120 id way heavier games.  Its still weord to me to see 4k120,AMD,2023-09-23 11:27:46,2
Intel,k1umihs,But then the current gen games of that era will run like this. The cycle continues,AMD,2023-09-23 13:22:31,2
Intel,k1vcazz,TRUE! i think 5-10 years was the actual point in time where anybody should have paid their hard earned dollar for raytracing gpus. instead ppl dished out $1000s for the RTX2080/Ti and now are sitting on them waiting for raytracing to happen for them xD,AMD,2023-09-23 16:19:04,2
Intel,k1t8b6k,Who knows what new tech will be out in even 4 years lol,AMD,2023-09-23 04:04:25,2
Intel,k1tl2z8,"And that’s when I’ll turn on this setting in my games. Fuck having to pay $1600 for a sub 30fps experience. Path tracing is far from being a viable option, it’s more like a proof of concept at this point.",AMD,2023-09-23 06:20:14,1
Intel,k1u0yj3,"You wish, GPU makers have stagnated like it's crazy, even the graphics have peaked. I expected to see CGI level graphics like Transformers now",AMD,2023-09-23 09:40:52,1
Intel,k1u6uez,"...but will cost your everlasting soul, body, firstborn and parents. If it's on sale.",AMD,2023-09-23 10:54:31,1
Intel,k1sby3m,It is path trancing though. The technology used by Pixar to make Toy Story 4 (though they spent up to 1200 CPU hours for one frame) Path tracing used to take up to a day per frame for films like the original Toy Story. And they had their supercomputers working on it. It is a miracle of modern technology that it even runs real time.,AMD,2023-09-22 23:49:24,306
Intel,k1rw4fg,"You basically need a 4090 to crack 60 fps at 1440p w/ dlss on quality without frame gen. It looks good, but not good enough to run out and buy a 4090.",AMD,2023-09-22 21:59:09,101
Intel,k1sbwfh,"Same card, I just turned off RT at 4K. 75-120fps is better than 40 with muddy but accurate reflections",AMD,2023-09-22 23:49:03,12
Intel,k1t5qx0,"The 7900xtx is one of the worst gpus iv ever had lol on paper it looks so so good, In games it just play isn’t. Fsr is garbage, 4080 mops the floor with it :( plus the drivers from amd besides starfield have been awful this generation",AMD,2023-09-23 03:40:46,-3
Intel,k1tf1z9,I think that most of the progress will go together with software tricks and upscalers.,AMD,2023-09-23 05:12:21,4
Intel,k1s07rb,"lol. And you missed the 2080Ti.   Every result under 10 fps is just to be ignored, it isn't representing anything outside of ""woot, the card managed to chuck a frame our way"".",AMD,2023-09-22 22:26:17,123
Intel,k1rz345,That's VRAM for you,AMD,2023-09-22 22:18:45,134
Intel,k1tc7vb,we are counting decimals of fps its just all margin of error.,AMD,2023-09-23 04:42:40,12
Intel,k1s03of,If you buy a card with low ram that card is for right now only lol,AMD,2023-09-22 22:25:32,4
Intel,k1v5pv8,Wake me up when a $250 GPU can run this at 1080p.,AMD,2023-09-23 15:37:12,9
Intel,k1uiun2,Well DLSS isn't best. DLAA is,AMD,2023-09-23 12:52:43,7
Intel,k1svcks,Reviews have been saying that the game looks better with DLSS than native. Not to mention runs extremely better.,AMD,2023-09-23 02:14:15,24
Intel,k1si3ng,Can’t you just disable TAA? Or do you just have to live with the ghosting?,AMD,2023-09-23 00:34:15,5
Intel,k1v2dce,TAA is garbage in everything. TAA and FSR can both get fucked,AMD,2023-09-23 15:15:33,1
Intel,k1svesl,"Yeah. 70 to 100fps on a 4080, but with 3440x1440 and DLSS-quality-FG-RR (nvidia needs new nomenclature....)",AMD,2023-09-23 02:14:43,7
Intel,k1v301u,"same, high refreshrate at 4k with optimized settings + PT + FG. With a 4080 of course, it's insane that it can look and run this great.",AMD,2023-09-23 15:19:41,2
Intel,k1v2lmw,"Not exactly. Path tracing is just ray tracing, but cranked up to 99% with the least amount of rasterization possible.",AMD,2023-09-23 15:17:04,2
Intel,k1sv2p6,"> Give it 2 gens and you are going to get 4k60 here.  Assuming the 5090 literally doubles a 4090 (unlikely), that only gets us to 4K 40hz.  Assuming a 6090 doubles that, 80. which won't be bad.  Going with more conservative 50% boosts. 5090 will give 30. 6090 will give 45.  And i feel like 50% is being very generous, as nvidia have claimed moores law is dead and they can't advance beyond a 4090 by much. I'd guess we get 30% uplift in 5090 and maybe 10-15% uplift in 6090. So we'd still be under 4K30.",AMD,2023-09-23 02:12:07,8
Intel,k1vdbm6,Imagine buying a 4090 and then using upscaling.,AMD,2023-09-23 16:25:29,-4
Intel,k1scblp,Path tracing is so much more demanding than ray tracing due to light scattering being modelled. It is a marvel it even runs.,AMD,2023-09-22 23:52:08,17
Intel,k1sfzkt,PC gaming is in Crysis.,AMD,2023-09-23 00:18:47,5
Intel,k1y7rss,Well it's a good thing 99.999999% of PC gamers don't give a shit about ray or path tracing.,AMD,2023-09-24 04:15:05,-1
Intel,k1t8lmy,You're the first other person other than myself I've run into that had an XTX but ended up with a 4090 in the end.,AMD,2023-09-23 04:07:13,2
Intel,k1rxtnn,"Plus frame generation works very well in Cyberpunk in terms of image quality. In some games, you need to get closer to ~80 fps output for an acceptable image quality with FG. But the FG in CP2077 is decent with 60 fps output, ~~and I get ~65-70 fps output with quality DLSS + FG at 4k on a 4090.~~ EDIT: I misremembered what I was getting. With path tracing, DLSS quality, frame generation, and ray reconstruction, I got 80.1 fps with the benchmark!  Of course there's the matter of latency, and the latency of CP2077 with FG output of ~65-70 fps isn't great. So I'll often use DLSS balanced + FG. Thanks to ray reconstruction, this now looks very close enough native 4k (to my eyes), with acceptable latency (to me), at a high framerate output.",AMD,2023-09-22 22:10:20,9
Intel,k1u60lu,"Nvidia features are always useless until AMD copies them a year or two later, only then they become great features 😁",AMD,2023-09-23 10:44:39,14
Intel,k1sizpa,"Tbh, list the games that have ray tracing right now. Pretty few.  It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it). I would personally go amd over nvidia because those 50 or so euros more that nvidia has against the amd counter parts simply are too much for just better rt and  dlss. I could spend those for a better amd card with generally better performance.  Regarding dlss, personally I would just lower the graphics before resorting to kinda bad generated frames, fsr even worse. But that's my point of view.  I find that amd still has to fix some driver issues but other than that, they are a fine brand. (so is nvidia)",AMD,2023-09-23 00:40:43,-6
Intel,k1u5vqx,Yeh because native 4k looks worse than dlss + RR 4k,AMD,2023-09-23 10:43:02,10
Intel,k1xi8dd,"Yeah the game with DLSS, RR and path tracing at 1440p looks amazing and with very high fps",AMD,2023-09-24 00:50:15,2
Intel,k1u5mhq,What's the point of having $5000 PC when you're still gonna have literally the same graphics as $1000 PC then?,AMD,2023-09-23 10:39:55,10
Intel,k1sshhs,"This is a tech demo. That's the whole point. It's not really playable yet, but the game really is meant to showcase what is possible in the future and how close we are getting. That's what Nvidia is doing here by funding this whole project.  Crysis who many people are comparing this to, was in itself quite revolutionary for its time. The destructible environment in Crysis to this day holds up, and that was it's killer feature really.  You're gonna have swings at the future that miss as well, and that's ok.",AMD,2023-09-23 01:52:13,10
Intel,k1sr5lk,"Did you try DLSS?  Imagine getting down voted for asking if he was using DLSS. What a fragile community. Sometimes it's okay to use DLSS, other times you better not or people will come to your house with torches",AMD,2023-09-23 01:42:00,1
Intel,k1t41gi,VRAM,AMD,2023-09-23 03:25:30,3
Intel,k1uiuuv,"Depends on what you are after i guess, is it worth it for me? Absolutely, DLSS+FG+RR with PT is a great experience.",AMD,2023-09-23 12:52:46,1
Intel,k1zcvhv,still better than 90% of games in 2023,AMD,2023-09-24 12:08:19,0
Intel,k1rvtrp,Arguably better now Ray Reconstruction has been added. It's quite a big image quality upgrade.,AMD,2023-09-22 21:57:13,19
Intel,k1s0kpr,"…no it does not, unfortunately. Have you seen the artifacting in Cyberpunk?",AMD,2023-09-22 22:28:43,-2
Intel,k1sbnjt,"Someone ate up the marketing. I often see people wonder why games are so unoptimized today, your answer is right here. \^",AMD,2023-09-22 23:47:17,-6
Intel,k1u6o55,All graphics you see on your computer screen is fake,AMD,2023-09-23 10:52:26,1
Intel,k1t62rv,Path tracing is more demanding than Ray tracing,AMD,2023-09-23 03:43:46,5
Intel,k1u17zi,"That's why I downscale from 1440p to 1080p, running DLSS and using Contrast Limited Sharpening. It looks better than native resolution and still performs well.",AMD,2023-09-23 09:44:22,4
Intel,k1s9p4d,I guess between the 3090 and the 4070,AMD,2023-09-22 23:33:11,0
Intel,k1sjydp,Yep hahaha,AMD,2023-09-23 00:47:40,2
Intel,k1v42p3,RemindMe! 7 years,AMD,2023-09-23 15:26:41,2
Intel,k1s3vi6,"No, this tech is around so game developers can sell more copies of their games by making them look better. The only reason why Nvidia is winning is because their GPUs are much better at intensive ray-tracing.",AMD,2023-09-22 22:51:16,19
Intel,k1s45n5,"This is an absurd, fanboyish thought lmao",AMD,2023-09-22 22:53:15,14
Intel,k1u3kdb,dont let novidia marketing see this youll get down voted into oblivion,AMD,2023-09-23 10:14:19,2
Intel,k1s2le1,I would still argue it's still in proof of concept stage. The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native. That will the allow the lower end GPU'S to get 4k 60 or 1440p 60 upscaled.,AMD,2023-09-22 22:42:21,-7
Intel,k1sylo6,I mean I have no problem playing at 30fps dlss balance at 1080p DF optimized settings on a 2080ti with pathtracing idc what anyone says RT/PT is the future it just looks so much better than Rasterization,AMD,2023-09-23 02:39:51,3
Intel,k1sltal,"Possibly a little more. Assuming that the leaked 1.7x improvement is right and path tracing performance scales the same, that would be about 34 fps at 4k native. Might be possible to hit 60fps or close to it with DLSS Quality.",AMD,2023-09-23 01:01:15,2
Intel,k1u6rsu,It could as well do 60fps if Nvidia adds a lot of path tracing HW into the GPU.,AMD,2023-09-23 10:53:38,2
Intel,k1t37ky,Fanboyism of both kinds is bad,AMD,2023-09-23 03:18:18,0
Intel,k1sbir3,"It won't end, but hopefully we see games that have art designed with RT lighting and reflections in mind.  Cyberpunk isn't it. We'd need a full rebuild of the game, every location, every asset and surface remade, so it looks like originally intended by the artists.",AMD,2023-09-22 23:46:20,-1
Intel,k1u85m7,Since it needs more than 10gb vram,AMD,2023-09-23 11:09:41,3
Intel,k1tq0gr,Cyberpunk is more optimized than majority of games that came out in last two years. Just because you can't run it at 4k 120 fps with path tracing doesn't mean it's not optimized.,AMD,2023-09-23 07:19:12,5
Intel,k1rw9dl,It’s not unoptimized at all. Game runs great for the quality of the graphics. You guys just don’t understand the insane amount of power path tracing takes. The fact you can do it at all rn is insane but once again people just throw “unoptimized” at it because they don’t understand the technical marvel happening in front of their eyes.,AMD,2023-09-22 22:00:02,27
Intel,k1sl1fh,"No, this game is actually very well optimized. It's just extremely demanding at 4k native, which is why DLSS exists.  It would be unoptimized if it ran like this while looking like nothing special, but this is probably the game with the best graphics, period. At the very least, it's certainly the game with the best lighting.",AMD,2023-09-23 00:55:34,7
Intel,k1rx0ka,"and people will be like ""well RT is the future""  raster was fine for over 15 years and raster games look great but we get it we must let incompetent game devs and their bosses make shit quality games because average gamer is ready to spend $100 to pre-order electronic version of horse shit and then justify it for years because they can't admit they wasted money",AMD,2023-09-22 22:04:58,-14
Intel,k1u2f5k,It's settled that you have no idea what you talking about,AMD,2023-09-23 09:59:44,4
Intel,k1sczqa,Its full path tracing u cannot really optimize this much.,AMD,2023-09-22 23:57:00,10
Intel,k1tpj68,"It always did. Most games don't scale well, Cyberpunk does.  Look at the last two years of pc ports and you will understand how cyberpunk looks and performs like it's black magic.",AMD,2023-09-23 07:13:34,3
Intel,k1s9h3d,"Baked illumination requires you to choose where to add light sources and how the lighting is applied to everything, it's a creative choice of the author, the AI needs instructions to know what to do, so it would need constant handholding, on the other hand RT uses simple physics to calculate how light bounced and NN to apply the light on surfaces.",AMD,2023-09-22 23:31:33,1
Intel,k1sprwd,But the destructable environment and the water graphics and other things in Direct X 9 made high end pcs kneel as well.  I remember playing on my 9800gtx/+ with my Intel Q9300 quad core (lapped and oc'ed to 3.0ghz - EDIT: checked some old evga posts got it to 3.33ghz) with 2x4gigs DDR2 @1000mhz cas 5 trying to maintain a sustained 30fps at 900p resolution on my 1680x1050 monitor. And I oc'ed the crap out of that 9800gtx 835 mhz (cant recall if it was unlinked shadder or not now) core on blower air (won the silicon lottery with that evga card).  Tweaking settings was mostly user done and guided with old school limited forum help. Ahh the good old days of having lots of time during school breaks.,AMD,2023-09-23 01:31:15,142
Intel,k1tds1v,"The equivalent of a 4090 at the time, 8800 ultra (768MB) got 7.5 fps at 1920x1200 very high quality. The other cards were not able to run it. Even the lowest tier last generation card runs this, even though it's at 5% speed.",AMD,2023-09-23 04:58:42,28
Intel,k1u7ebf,No and no again lol  The 8800gtx could do 1152x864 high and that was the top dog. DX9 cards could do that at medium but high shaders was like a 25fps average tops and that dropped into the teens on the snow level.   It took things like the gtx480 and HD 6970 to do 1920x1080 30fps max settings. That's before getting into Sandy Bridge being the first cpu that could sustain 40+ fps if the gpu power was there.   Crysis came during the core 2 duo/k8 athlon and first wave dx10 cards era and it took *2 more generations* of *both* gpus and cpus to do it some justice.,AMD,2023-09-23 11:01:03,18
Intel,k1vo73s,"My 8800ultra was getting single digit fps at 1080p ultra, so CP isn't quite as demanding.",AMD,2023-09-23 17:33:46,6
Intel,k1u7ms4,Amd had a tessellation unit. It went unused but it was present,AMD,2023-09-23 11:03:47,2
Intel,k1srvji,"My fun story for PhysX was Mirrors Edge. I don't remember what GPU I had at the time but the game was pretty new when I played it. Ran fine for quite some time until one scene where you get ambushed in a building by the cops and they shoot at the glass. The shattering glass with PhysX turned on absolutely TANKED my framerates, like single digit. I didn't realize that the PhysX toggle was turned on in the settings. This was at a time when PhysX required a dedicated PCIe card in your system.   Once I turned it off it ran fine. Now I can run that game at ridiculous framerates without my system getting warm.",AMD,2023-09-23 01:47:34,161
Intel,k1sqn2i,"I remember around about 2008 when companies like Asus and Dell were selling ""Physics Processing Units"" and some claimed that these would be commonplace in gaming machines just like Graphics cards had become 10 years previously.",AMD,2023-09-23 01:38:00,30
Intel,k1tzlza,"Hardware accelerated physics (as in on the gpu), is different than what's going on the CPU.",AMD,2023-09-23 09:23:04,5
Intel,k1uatm9,Yeah. People with Windows 7 were using AMD to play the game with old Nvidia card for Physx. Nvidia didn't like that and blocked it via driver,AMD,2023-09-23 11:38:16,5
Intel,k1u8wkg,"People used to have ATI/AMD for main and a lower-end NVIDIA for PhysX.  When NVIDIA found this out they pushed out drivers that disabled PhysX on their cards if an ATI/AMD card was detected, limiting you to the intentionally piss-poor CPU implementation of PhysX.  Think about that crap.  One day everything's going fine for consumers, the next day NVIDIA decides they don't like how consumers are legitimately using their cards and gimps everyone, weaponizing a physics engine company that they bought in 2008.",AMD,2023-09-23 11:17:47,17
Intel,k1tdixd,"Oh damn, I forgot about those cards.  I wanted one so badly.",AMD,2023-09-23 04:56:03,3
Intel,k1u3mme,Remember when companies tried to sell physics cards lol,AMD,2023-09-23 10:15:05,3
Intel,k1thwg4,>PhysX  It never was a dedicated Nvidia card - it was a [dedicated psysx](https://www.techpowerup.com/review/bfg-ageia-physx-card/) on which the tech later was bought by Nvidia and implemented in it's own GPU's.  But the things never became really populair.,AMD,2023-09-23 05:43:55,16
Intel,k1tjl5e,"This makes me wonder if we'll ever see something similar with Raytracing, where we get a 2nd GPU purely for RT, and then the main GPU just does all the other stuff. Would that even be possible?",AMD,2023-09-23 06:03:08,2
Intel,k1umriw,I remember when people had a dedicated PhysX card.,AMD,2023-09-23 13:24:29,2
Intel,k1un2sg,The ray/path tracing in this case done by specialized hardware which has more room to grow faster.,AMD,2023-09-23 13:26:56,15
Intel,k1tob83,"I would, transistor shrinking isn't the only method of increasing performance, and honestly, these companies have to keep putting out better cards to make money.  There have been many breakthroughs over the last few years. I give it another 5 as both amd and nvidia are pushing ai accelerated ray tracing on their cards, nvidia is in the lead for now but amd will eventually catch up.",AMD,2023-09-23 06:58:48,5
Intel,k1txgpk,There is still a big leap possible since all lithography processes at the moment are hybrid euv and duv.   But the moment everything is done euv things will drastically slow down.,AMD,2023-09-23 08:55:01,6
Intel,k1ugyqs,"No those are just pennies in the pocket of Nvidia, but as the consequence you as the customer need to take a mortgage on a brand new GPU.",AMD,2023-09-23 12:36:34,2
Intel,k1ubuqx,"Yeah the next 10 years are gonna be spent getting budget cards up to 4090 level performance, rather than bringing the bleeding edge to 5-10x the 4090 performance. As long as 4K is king, there’s not much incentive to do the latter.",AMD,2023-09-23 11:48:42,2
Intel,k1uc6ox,"In 10 years we will be begging one of several thousand test-tube created Musk Family members for $200 so we can buy a cheeseburger.  But the jokes on them, we’re just gonna spend it on space crack.",AMD,2023-09-23 11:51:55,7
Intel,k1uln5g,"Never. Even if performance can be pushed that far, by the time it happens there won't be such a thing as a $200 graphics card anymore.",AMD,2023-09-23 13:15:37,3
Intel,k1ull2c,It's not happening unless the market crashes and they start focusing on offering good price/performance cards instead of bumping up prices every generation.,AMD,2023-09-23 13:15:10,2
Intel,k1tocdn,25 years,AMD,2023-09-23 06:59:12,7
Intel,k1u1mrk,"The software needs to run on hardware, right now it eats through GPU compute and memory.",AMD,2023-09-23 09:49:38,4
Intel,k1tqm89,And sadly ended up with 450 Watt TDP to achieve that performance.,AMD,2023-09-23 07:26:36,13
Intel,k1tvo5h,This. We'd be lucky to see more than 3 generations in the upcoming decade.,AMD,2023-09-23 08:31:28,4
Intel,k1u1jnl,"Having seen a few newer games on relatively low resolution CRT display, I can't help but think it might come down to improved display tech and embedded scaling. Like DLSS3 features in the display instead of the GPU.",AMD,2023-09-23 09:48:32,3
Intel,k1ucdje,Not with that attitude,AMD,2023-09-23 11:53:45,2
Intel,k1ugj3a,Love how it's still gimped on memory size 😂,AMD,2023-09-23 12:32:43,7
Intel,k1u3901,"They just need to render the minimum information needed , and let the ai do the rest",AMD,2023-09-23 10:10:17,3
Intel,k1u9l5i,"Don't get upscaling, to me it looks shite. Why spend hundreds or thousands on a GPU to not even run at native resolution.   It's something Nvidia are pushing because they've got a better version of it at the moment.  Maybe it'll improve, it's pretty much snake oil currently imo.",AMD,2023-09-23 11:25:11,-4
Intel,k1u27rs,"Feels pretty good with 120 frames. Playing with a controller, i don‘t mind or even feel the input lag on a projector screen.",AMD,2023-09-23 09:57:10,5
Intel,k1ufc12,"It's mad that even with a 4090 running games at 4k, they still look less realistic than the low-res FMV sequences in The 7th Guest from 30 years ago.",AMD,2023-09-23 12:21:51,2
Intel,k1t3hpt,"Do keep in mind that despite both being named the same, the devil's in the details. Movies use way more rays per scene and way more bounces too.   Path tracing in CP2077 shows temporal artifacts due to denoising, something that doesn't happen in movies. It is being improved with the likes of DLSS 3.5, but it is still quite off when compared to said movies.",AMD,2023-09-23 03:20:42,158
Intel,k1tek93,"Keep in mind the systems that rendered toy story 1, costing upwards of like 300k IIRC, have less power than your cell phone today and were super delicate/finicky machines. There's a dude on youtube that got a hold of like the second best one that was made at the time and the machine honestly was really impressive for when it came out, but it pales in comparison to even a steam deck really.",AMD,2023-09-23 05:07:01,11
Intel,k1uoawi,"> Path tracing used to take up to a day per frame for films like the original Toy Story  Toy Story didn't use path tracing though, A Bug's Life was Pixar's first movie to use ray tracing (not path tracing) and only for a few frames in the entire movie for specific reflections, they started using ray tracing more generally for Cars and I can't find exactly when they started using path tracing but it should be around the early 2010s which is also when the other Disney animation studios started using path tracing",AMD,2023-09-23 13:36:30,3
Intel,k1su9mw,No it's not a miracle. Because it's downscaled a lot and also uses lower quality math and piggybacking on AI to make up for it. It's basically running through 3 layers of cheats to produce results that aren't that much better than just traditional raster.,AMD,2023-09-23 02:05:53,-21
Intel,k1sbp7z,"That’d be some next level consumerism, paying 1500$ minimum to turn on a single setting in a single game just to play it wayyyyy slower than you would otherwise",AMD,2023-09-22 23:47:36,83
Intel,k1sxtj8,"Hell playing path tracing with my 2080ti at 25 FPS is still looks absolutely fantastic, I would absolutely play with pathtracing on a 4090 constantly. Idc dlss looks great with RR even at 1080p. I won’t upgrade for another year or 2 (just bought a phone so I’m broke right now)",AMD,2023-09-23 02:33:36,6
Intel,k1svx2t,4090 getting 60fps at 4k balanced dlss with no frame gen. 100 with frame gen. I can make it drop into the 20s if I stand right next to a fire with all the smoke and lightening effects or if I go to a heavily vegetated area it’ll drop to mid 40s. But it’s stay consistently at 55-65 and even goes Into the 90s if I head out of the city.   Haven’t tried it at quality or with dlss off though. May go do that now that it says only 19fps lol. Have to try it to see for myself,AMD,2023-09-23 02:18:41,4
Intel,k1sbvoe,"https://www.tomshardware.com/news/nvidia-dlss-35-tested-ai-powered-graphics-leaves-competitors-behind  Well it's so close (54fps) it's more like a 3080Ti or higher from 3000 series or a 4070TI + from 4000 series it seems? The old 3000 series is punching way above it weight vs the 7900xtx, which was meant to deliver similar RT performance to the 3090.. which it doesn't.",AMD,2023-09-22 23:48:54,19
Intel,k1slgjk,At some point the RT pixels are so expensive that native resolution w/o DLSS and frame gen is just not gonna work for the time being.,AMD,2023-09-23 00:58:39,9
Intel,k1tf7yk,"On the other hand, if you set the new DLSS 3.5 to performance (which you should in 4k), and just enable frame generation, you get 90+ in 4k with basically zero issues unless you pause to check frames.",AMD,2023-09-23 05:14:13,3
Intel,k1sad2c,So rendering at 960p? Oof...,AMD,2023-09-22 23:38:00,7
Intel,k1sd3lh,"you take that back, I literally returned my 4080FE for a 4090 liquid x for more frames in cyberpunk.",AMD,2023-09-22 23:57:47,-5
Intel,k1rxn27,I almost see no difference.   And to see the little diffidence I need to take two screenshots and compare them.,AMD,2023-09-22 22:09:06,-18
Intel,k1s6fmd,The game look is ruined on PT it makes everything look completely different.  Regular RT keeps the style of the game and performs good.,AMD,2023-09-22 23:09:38,-24
Intel,k1tzl0f,Dont use useless raytracing and you wont have any problems lol,AMD,2023-09-23 09:22:43,-1
Intel,k1vpidg,"Because PC gaming blew up during a time where you could buy a mid range GPU and not need to upgrade for 5-6 years. Now those sample people buy a GPU, and 2 years later it can't run new games. At least that's my theory.",AMD,2023-09-23 17:42:04,7
Intel,k23ed9f,AW2 looks insane. Can't wait to play soon,AMD,2023-09-25 04:14:36,2
Intel,k1si0t0,Well the 3050 managed to hold with just 8GB while the 3070Yi crashed?,AMD,2023-09-23 00:33:40,23
Intel,k1s5qaj,"Yeah, and people insisted on defending the configurations at launch lmao. The cards just won't be able to handle heavy loads at high resolution such as this game, regardless of how fucking insane the actual processing unit is. You can't beat caching and prepping the data near the oven. Can't cook a thousand buns in an industrial oven at the same time if there's trucks with 100 coming only once an hour.",AMD,2023-09-22 23:04:33,37
Intel,k1sor0g,My 3080 in shambles,AMD,2023-09-23 01:23:27,3
Intel,k1u5gbh,"It looks better than native even with fsr quality, the Taa in this game is shit",AMD,2023-09-23 10:37:48,-1
Intel,k1t69ca,You dont need to increase raw performance. You need to increase RT performance.,AMD,2023-09-23 03:45:25,31
Intel,k1vko89,Imagine!,AMD,2023-09-23 17:11:45,8
Intel,k1vz5q2,DLSS unironically looks better than native TAA even at 1080p in this game because of the shit ghosting.,AMD,2023-09-23 18:42:28,6
Intel,k22igme,Only people who don't give a shit about high quality graphics don't care about ray tracing.,AMD,2023-09-25 00:09:28,3
Intel,k1tamw3,I got a stupidly good deal on the 4090(buddy won an extra one from a company raffle) and it just so happened my XTX was one of the cards suffering from the mounting issue on release.  Honestly very happy that all of that happened. Sure I would have been happy with the 7900xtx if I got a replacement but the 4090 just kinda blew me away especially with all the DLSS tech behind it.,AMD,2023-09-23 04:26:51,4
Intel,k1s8m60,"Nvidia used cp77 as a testing grounds for their dlss3 and rt tech, it'd no surprise it looks and performs relatively good, they literally partnered with cdpr for that.  Do not expect these levels of RT on most games anytime soon, probably in a couple years (with rtx 5000 series) the tech will be more mature and not as taxing for most cards, because needing to upscale and use framegen to get 60fps on an RT PT setting is kinda absurd.",AMD,2023-09-22 23:25:22,3
Intel,k1v3dh2,">only then they become great features  Watch this happen with ""fake frames"" FSR3 in real time",AMD,2023-09-23 15:22:07,7
Intel,k1u1pck,>Ray tracing as of right now is a gimmick   This line was already false years ago when control launched. Now it’s just absurd,AMD,2023-09-23 09:50:33,8
Intel,k1sl137,"There are a lot of new games that features Ray Tracing on them as of now, too bad most of them doesn't look as visually as good as they could be though as most of the time their RT effects are dumbed down or reduced to the point it's pointless.  But the thing is that wasn't the point even from the beginning, RT Overdrive on Cyberpunk is literally considered as a Technological showcase  What matters here IMO for all of us is PC platform in general is showcasing here what a real high-end computers can achieve beyond what current gen console can do, and they are showcasing it really well here on Cyberpunk with much better visuals and using plenty of tricks to make them more than playable, where they shouldn't be considering how much demanding they are over standard, that is impressive enough to me and shows a good sign of technological engineering innovation.   As what Bryan Catanzaro said on his interview on DF Direct, work smarter not harder, brute force is not everything, of course it still matters a lot but doing it alone is starting to become pointless just like how it is on real life.",AMD,2023-09-23 00:55:30,3
Intel,k1u5xqe,Much more fps :) Newer generation cards have at least 50-60% better raster performance then prev gen.,AMD,2023-09-23 10:43:42,-5
Intel,k22yxwq,The Evangelical Church of Native,AMD,2023-09-25 02:08:44,1
Intel,k1vk8iw,"So just to be clear, you don't like running games at native resolution? Because the purpose of DLSS is to improve performance by specifically not running at native resolution",AMD,2023-09-23 17:09:00,2
Intel,k22yk42,Nice but ive been playing video games since the 70s and its Shit end off..,AMD,2023-09-25 02:05:53,2
Intel,k1s3cm8,The future of frame generation is going to be huge. I can definitely see 360p frame rendering upscaled to 4k with 80% maybe even 90% of the frames being generated. All with equal to or even better visual quality.   A 20 FPS game like this will become a 100-200fps game.  We may be looking at the end of beefy graphics cards being required to play games. Just a good enough mid or low tier chip (granted 5-10 years from now) with frame generation might be enough.,AMD,2023-09-22 22:47:37,-14
Intel,k1ssze4,"After 2.0, it's around 100""fps"". Just 60""fps"" (meaning with FG enabled) is far from being playable due to latency. You need at least 90""fps"" for playable experience.   Cyberpunk PT vs RT is a huge difference in terms of graphics, while DLSS Quality vs Balanced is really slim in comparison. What you say would be a really bad trade-off.   At 1440p DLSS Quality it's easily 120+""fps"" which is perfectly comfortable.",AMD,2023-09-23 01:56:05,-4
Intel,k1tpcbe,Cyberpunk is greatly optimized. It looks fantastic for the hardware it asks. Starfield looks like game from 5 years that asks hardware from the future.,AMD,2023-09-23 07:11:12,11
Intel,k1srkr7,"Yeah, sure, now go back to play starfield.",AMD,2023-09-23 01:45:16,7
Intel,k1siya9,"There literally is a /s, what else do you need to detect sarcasm?",AMD,2023-09-23 00:40:26,3
Intel,k1ycket,"Someone made a really cool comment yesterdays on Reddit that even a generated frame using path tracing is less fake than a rasterized frame. It is effectively closer to reference truth. I had never thought about it that way, but people really are clutching their pearls with this shit",AMD,2023-09-24 05:03:07,1
Intel,k1wclz4,"I know, which makes path tracing even worse off imo.",AMD,2023-09-23 20:06:58,1
Intel,k1v47c6,I will be messaging you in 7 years on [**2030-09-23 15:26:41 UTC**](http://www.wolframalpha.com/input/?i=2030-09-23%2015:26:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/16pm9l9/no_gpu_can_get_20fps_in_path_traced_cyberpunk_4k/k1v42p3/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F16pm9l9%2Fno_gpu_can_get_20fps_in_path_traced_cyberpunk_4k%2Fk1v42p3%2F%5D%0A%0ARemindMe%21%202030-09-23%2015%3A26%3A41%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2016pm9l9)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,AMD,2023-09-23 15:27:32,2
Intel,k1vh7xa,haha this is gold 🥇,AMD,2023-09-23 16:50:10,2
Intel,k1sakbd,"At the moment, it’s definitely what it seems like.  Especially seeing how there’s only one card on the planet that can run this. Do you have a 4090?",AMD,2023-09-22 23:39:29,-7
Intel,k1s5318,">I would still argue it's still in proof of concept stage.  Not at all, in fact another AAA game with ""path tracing"" is coming out next month.  >The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native  Forget ""native"", that ship has already sailed. Nvidia, AMD and Intel are all working on machine learning techniques, and several game developers are starting to optimize their games around DLSS/FSR.",AMD,2023-09-22 22:59:52,6
Intel,k1t4yk0,"Nope, not even close.  Shaders 3.0 - that's improvement. Ray tracing is noticeable, but nowhere near to justify the hit.  I've looked techpowerup comparison RT vs LOW (lol). Low looked far better and cleaner, then blurry mess of RT (Overdrive).   I don't need ""realistic"" lighting, I need pleasent bisual part with good gameplay. For ghraphics I have a window in my appartment.",AMD,2023-09-23 03:33:41,-3
Intel,k1sakwq,"In a screenshot, sure. When actually playing the game, unless the fps is low enough to remind them, 99,99% of people will forget if they left it on or not.",AMD,2023-09-22 23:39:36,-9
Intel,k1ryjta,Pixar movies will sometimes take the better part of a day to render a single 4k frame on a render farm. So the fact that Cyberpunk takes more than 0.05 seconds to render a 4k frame in its path-tracing mode on a single 4090 clearly means that it's unoptimized garbage! /s  EDIT: Apparently people in this thread don't understand sarcasm (or don't understand how a path-traced game can take longer than 0.05 seconds to render a 4k frame with high-poly path tracing and still be optimized).,AMD,2023-09-22 22:15:11,-1
Intel,k1stpfv,Man you had to lap your q9300 to hit 3.0ghz? Those chips really weren't OC friendly.  My q6600 hummed along 3.2 no problem on 92mm arctic freezer and a slight voltage bump.  I was so pumped when I upgraded to a gtx 260. Crysis was a dream.,AMD,2023-09-23 02:01:36,40
Intel,k1w9mwj,Funnily Crysis still have better destructible environments than Cyberpunk has tho,AMD,2023-09-23 19:48:19,1
Intel,k1vonge,"This. People losing their shit either weren't around when Crysis launched or have forgotten just how demanding it was . PT CP kicks the shit out of a 4090, Crysis murdered my 8800ultra.",AMD,2023-09-23 17:36:37,8
Intel,k1txp0j,crysis did not do 1080p 60 on maximum setting and dx10 on a 8800GT.   I had a 1280*1024 monitor back then and barely had 30 fps with my 8800GT.   Even the 8800 Ultra did not do 60 fps on full hd.  https://m.youtube.com/watch?v=46j6fDkMq9I,AMD,2023-09-23 08:58:05,16
Intel,k1tg0xs,"Man, I remember how hype the 8800gt was. Thing was a hell of a big jump compared to previous cards that came out, prolly our first REALLY powerful card.  Still remember the old xplay video where they build a PC to play crysis and Bioshock. Put 2 8800GTs in sli in there with a core 2 quad which costed one thousand dollars back then!",AMD,2023-09-23 05:23:00,5
Intel,k1syqzz,"My poor HD 4850 got pushed to the limit to give me 1280x1024, lol",AMD,2023-09-23 02:41:04,4
Intel,k1u23r0,I remember thinking $300-400 (IIRC) was so much for a GPU back around those days.  7800XT is the closest we've seen in a while and not even close to the top end for today.,AMD,2023-09-23 09:55:42,3
Intel,k1u67ug,This is still the case to this day because the game includes a really old DLL file for PhysX. The other day I followed the instructions on the PCGamingWiki to delete some DLL files in the game directory and only then it ran perfectly smooth on my RTX 3070.,AMD,2023-09-23 10:47:01,12
Intel,k1t2x0x,LOL I remember this exact scene also.,AMD,2023-09-23 03:15:45,32
Intel,k1ua0tu,"CPU PhysX back then was single-threaded and relied on ancient x87 instructions if I recall correctly, basically gimped on purpose. Even with a 5800X3D the shattering glass reduces the frame rate to the low teens. Sadly an Nvidia GPU is still required if you want to turn PhysX effects on for games from that era, though I hear that it's possible again to use it with an AMD GPU as primary.",AMD,2023-09-23 11:29:52,8
Intel,k1t6cm4,"OMG I had the exact same experience, hahaha I remember it vividly, I was so confused why that room would just destroy my FPS until I figured out PhysX was enabled LOL",AMD,2023-09-23 03:46:15,18
Intel,k1tlbna,"I'd like to see ray tracing addon cards, seems logical to me.",AMD,2023-09-23 06:23:01,9
Intel,k1tyf7o,"TBH, I could probably run some of the old games I have on CPU without the GPU.",AMD,2023-09-23 09:07:31,3
Intel,k1ub8fu,Had the exact same experience. One scene in the whole game that actually used PhysX,AMD,2023-09-23 11:42:32,3
Intel,k1uhy6a,You could probably run Mirror's edge with physx on today's hardware without gpu fans turning on,AMD,2023-09-23 12:45:10,3
Intel,k1tcex2,Last time I tried on an R7 1700X and RX580 I still couldn't turn on PhysiX without it being a stutter party 2 fps game.,AMD,2023-09-23 04:44:40,3
Intel,k1tsmml,What gpu you have,AMD,2023-09-23 07:51:58,2
Intel,k1tyt5q,Now it even runs fine on a Ryzen 2400G.,AMD,2023-09-23 09:12:37,2
Intel,k1t9hds,"And they were right, PhysX and systems very much like it are still used but things have advanced so much nobody even thinks about it and it no longer requires dedicated silicon.",AMD,2023-09-23 04:15:43,40
Intel,k1vaqdn,"Yeah, it’s been common knowledge for many years now that Nvidia are the most ruthlessly anti-consumer company in PC hardware, and it’s not particularly close.",AMD,2023-09-23 16:09:02,11
Intel,k1tf4z4,Ah good old Ageia before nvidia bought them out https://en.wikipedia.org/wiki/Ageia,AMD,2023-09-23 05:13:17,3
Intel,k1tt5y9,"No, you could actually install two Nvidia cards and dedicate one of them to only PhysX.",AMD,2023-09-23 07:59:01,20
Intel,k1w7xof,"It would certainly be possible, but it wouldn't really make sense. Splitting it up on multiple GPUs would have a lot of the same problems that sli/crossfire had. You would have to duplicate memory, effort, and increase latency when you composite the final image.  It may or may not make sense to maybe have a raster chiplet and a ray tracing chiplet on the same processor package on a single gpu. But, probably makes more sense to have it all on one chiplet, and just use many of the same chiplet for product segmentation purposes instead.   A separate PPU did make sense tho, I'm still annoyed that the nvidia ageia deal essentially kill the PPU in the cradle. Our gaming rigs would cost more if PPUs became a thing, but we could have had a lot better physics then we do today. There is still a revolution in physics to be had some day...",AMD,2023-09-23 19:37:48,3
Intel,k1tyzar,would be cool if 2000/3000 series users could get a small tensor core only PCIe card to upgrade to framegen,AMD,2023-09-23 09:14:51,2
Intel,k1v3dv1,"That's true, but if he's talking about *the equivalent* of a current $200-class card, I'd say about it's 10 years, what do you think?",AMD,2023-09-23 15:22:12,2
Intel,k1uaqi2,At least with the 4090 you can run 70% power target and still hit insane FPS while only pulling around 300w which is the same as my old 3080. The gains with that extra 150w are a couple percent at best. Not worth it to me.,AMD,2023-09-23 11:37:22,7
Intel,k1uke2m,Intel design will be a little bit different as far as now.  If I understood it right AMD chiplets communicate via lines on pcb but Intel wants to make something like chip-on-a-chip.,AMD,2023-09-23 13:05:30,2
Intel,k1udkaf,bedroom ring library cows summer thought aspiring worm north joke   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:05:15,4
Intel,k1tl1au,"It's also worth noting that those early movies don't use path tracing either, Pixar switched to PT with Monster University around 2013 IIRC.",AMD,2023-09-23 06:19:42,25
Intel,k1t9pys,"There is a lot of room for improvement, both in the software and future generations of hardware.  It's coming along though!  Overdrive mode looks nice, but there's just a lot more ghosting than the regular RT mode.",AMD,2023-09-23 04:17:58,16
Intel,k1u3wuq,"They are still the same thing and should be named the same, your disclaimer is just semantics about the implementation.",AMD,2023-09-23 10:18:38,1
Intel,k1sx1p7,"Weird argument.  Traditional raster is cheats upon cheats upon cheats already.  In fact, all of the lighting entirely in raster is essentially cheats.",AMD,2023-09-23 02:27:32,41
Intel,k1t81d2,It will not work well/look good using a single frame worth of data either due to the low ray counts. Digital Foundry has shown Metro Exodus builds the lighting over ~20 frames or so and it seems like Cyberpunk is even more. Even Cyberpunk ray pathtracing doesn't look as good in motion due to that same build-up effect.  You need to cast 20-50x the rays to have enough data for a single frame but then you will be measuring the game performance by seconds per frame.,AMD,2023-09-23 04:01:49,4
Intel,k1szqei,"RT looks significantly better than raster when it’s not just using simple reflections like a Far Cry 6. Raster is all cheats, the so called reflections are essentially rendering the scene or part of the scene being reflected in reverse then applying fake material effects to mimic metal, water, etc. Raster also takes HUNDREDS to thousands of person hours to get the fake reflections and lighting to look decent to good. That’s why devs love RT and want it to grow, as RT/hardware does the work for them.",AMD,2023-09-23 02:49:05,13
Intel,k1syew7,It's significantly better than raster. It kills fps but the quality is great,AMD,2023-09-23 02:38:21,8
Intel,k1ug9ty,Better graphics needing more expensive hardware is hardly a hot take.,AMD,2023-09-23 12:30:24,17
Intel,k1u52xg,"Yes, better graphics costs performance. SHOCKING",AMD,2023-09-23 10:33:12,13
Intel,k1t1ej8,People do it!,AMD,2023-09-23 03:02:50,7
Intel,k1tqio6,It’s not way slower. I get 110 FPS at 4k with all DLSS settings turned on and honestly it’s insane.,AMD,2023-09-23 07:25:19,11
Intel,k1t7tj4,"I shamefully admit I bought a (inflation adjusted) $400 console in the past to play one game. That's marginally better than buying a GPU for one setting in one game, but still.",AMD,2023-09-23 03:59:48,3
Intel,k1sx4o4,Sounds like most nvidia fanboys,AMD,2023-09-23 02:28:11,13
Intel,k1u2qx4,"If you're into the eye candy, I can see a lot of people that have the money doing it.  Not everyone though. That's around a mortgage payment for me.",AMD,2023-09-23 10:03:52,1
Intel,k1u3no0,And yet people will tell you how nvidia is mandatory. Like you want to overspend to play 1 game with shitty fps? I don't understand these people.,AMD,2023-09-23 10:15:26,-1
Intel,k1y7ifv,Nvidia fanboys being happy with fake frames and fake resolution will never not be funny to me.,AMD,2023-09-24 04:12:33,-4
Intel,k1ss6lq,What? I thought amd was always more tuned to raster as opposed to reflections,AMD,2023-09-23 01:49:57,2
Intel,k1staby,Which is why nvidia is rabidly chasing AI hacks,AMD,2023-09-23 01:58:25,19
Intel,k1sv3ph,This and rumors are saying that the 5090 is gonna be 50-70% faster than 4090 which wont be enough for native 4k either.,AMD,2023-09-23 02:12:20,3
Intel,k1sm72w,Say that to the people playing upscaled games at 4k (540p) on PS5.,AMD,2023-09-23 01:04:07,-4
Intel,k1s6hg3,"I understand some RT effects might not catch everyones eyes, but seriously, path tracing cyberpunk vs 0 RT cyberpunk IS a big difference",AMD,2023-09-22 23:10:00,30
Intel,k1s7ntd,Huh? [Watch this is in 4K on a big screen.](https://www.youtube.com/watch?v=O7_eHxfBsHQ&lc=UgyO2vRiSI6CRU5kYmV4AaABAg.9uyI5P8oamz9uylTN_rdmL) The differences are really apparent in each version.,AMD,2023-09-22 23:18:28,12
Intel,k1slxqu,"PT looks way better in the game to me - it makes everything in the environment have a more natural look to it.  It adds, not ruins it...",AMD,2023-09-23 01:02:10,12
Intel,k1s73qo,"That's how I feel as well. It looks awesome don't get me wrong, but it doesn't look like cyberpunk to me.",AMD,2023-09-22 23:14:27,-15
Intel,k1ttfgf,It's not a perfect way of measure but you can clearly see how (at least on Nvidia GPUs) the 8/10GB cards are way behind the >11Gb cards. Meaning you need 11 or 12GB of VRAM for this scenario which cripples the 3080 but not the 3060.  We said from the start these configurations are shit but no-one listened. There you go.,AMD,2023-09-23 08:02:21,11
Intel,k1tby96,The crypto miners did me a comical solid by preventing me from acquiring many countless times and hours I wasted (came from a gtx1070 before finally being able to upgrade). Was able to get my 6900xt eventually for around $600 with 2 games. Becoming increasing thankful for the extra Vram these days.   Once I start getting through my game backlog and into the gard hitting ray tracing ones will hopefully upgrade to something with at least 24gigs of GDDR# lol.,AMD,2023-09-23 04:39:58,2
Intel,k1sb10i,"I mean, Alan Wake 2 is also releasing with path tracing so my guess is we're definitely gonna see more games featuring heavier RT, although not necessarily path tracing due to the performance hit.",AMD,2023-09-22 23:42:52,13
Intel,k1sf5sr,"I expect path tracing support to be the exception for a while because most developers will consider the non-trivial dev time to implement it not worth it. However, I'm sure Nvidia would be willing to throw dev help at most developers who would be willing to implement path tracing with their help.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode. Presumably, a less complex world helps, but they may also have implemented opacity micro maps, which I'm not sure if Cyberpunk has done in the 2.0 update. Perhaps they're cherry-picked examples for the trailer.",AMD,2023-09-23 00:12:51,2
Intel,k1u66ws,Raytracing now is a gimmick *  ( * - in AMD sponsored games where RT effects are so tiny you have to zoom on to notice them),AMD,2023-09-23 10:46:42,5
Intel,k1t46jg,"Except this is a tech demo. CP77 has lots of cut corners to achieve that, NN was trained to keep the interface stable in that game and even then, ray reconstruction creates horrible artifacts at some angles and distances. And that's with Ngreedia's engineeers literally sitting at CDPR's office.",AMD,2023-09-23 03:26:44,0
Intel,k1u6zio,You can already have 120fps on $1000 PC,AMD,2023-09-23 10:56:12,9
Intel,k1vnjmf,It improves both looks and fps so it is a win win,AMD,2023-09-23 17:29:44,0
Intel,k1scklw,"The issue with this is the inputs will only be registered at 20Hz. This would feel awful. The benefit of high FPS is having very low latency. This will just make it look smoother, not feel any better than 20 FPS",AMD,2023-09-22 23:53:55,7
Intel,k1s9sl2,Considering there is no input on the generated frames that is going to feel horrible to play. Base FPS (incl DLSS) really still needs to be 50-60fps to feel playable imo.,AMD,2023-09-22 23:33:53,4
Intel,k1s6x1t,I'm thinking will there ever come a day where 100% of frames are generated by AI. Like the data inputs directly from the card into the neural network and it no longer requires physical hardware limitation to generate frames.,AMD,2023-09-22 23:13:07,4
Intel,k1s97ev,"Don't forget that all NPCs will be controled by AI to simulate a real life.  They'll wake up in the morning, shower, commute to work, work, then go home.  All this to do it over and over and over until they die...  mhhh... this is kinda sad to type out.![gif](emote|free_emotes_pack|cry)",AMD,2023-09-22 23:29:38,1
Intel,k1s30i9,It absolutely is. I’ve been playing at 1440p with DLSS Balanced and Ray Reconstruction and it is absolutely the worst version of DLSS I’ve ever seen. It’s basically unusable.,AMD,2023-09-22 22:45:15,-9
Intel,k1s8eym,There are horrible artifacts with RR. And ghosting. It replaces artifacts with different ones. Needs a lot more work tbh. Or training.,AMD,2023-09-22 23:23:56,-1
Intel,k1wd23h,"The thing about ray and path tracing is that they are demanding on their own, not because they are badly optimised",AMD,2023-09-23 20:09:52,1
Intel,k1save0,"I have a 4070. It runs the game just fine with the tools that have been set in place.  This is absolutely not a pissing match. Cyberpunk is demanding, and takes advantage of lots of tech available for upscaling and ray tracing and frame generation. AMD card’s currently do not match Nvidia’s in those categories, not even close. This us not an attempt to showcase Nvidia tech, but it does take advantage of it.",AMD,2023-09-22 23:41:42,4
Intel,k1sama1,Because it's not about the flagship card. Only if the flagship can attain good performance at native then you will have headroom for lower end cards. Think about it. Of course upscaling is very important. But you also need some headroom to upscale.,AMD,2023-09-22 23:39:52,-2
Intel,k1tastr,"Yeah. I wanted better cooling performance after upgrading to the artic freezer 7 as well with those weird crapy plastic tension clips you pushed in manually, fearing bending them (worse than the stock intel cooler tension twist lock). Kept having random stability crashes until after i sanded it down for better thermals...Good old sandpaper and nerves of steel fearing an uneven removal of the nickel coating.  Was trying to maximize performance as back then core 2 duo and the extreme versions were king and they had way higher single core clocks and were easy to oc. Wanted the multhreaded for Crysis, LoL (back when you had to wait 5min+ to get into a game), BF2, and was eventually playing TF, Day of Defeat and Natural Selection.  My upgrade back then was to the Evga 560ti DS, which I ended up installing a backplate and  a gelid cooler. They had wayy better thermal tape/individual heatsinks for the memory/vram chips for the heat transfer. Evga back then told me if I ever needed to RMA it that I would just need to re-assemble it as it was shipped. Remember using artic solver ceramique instead of as5 due to it potentially not degrading as fast as well.   Good times =)",AMD,2023-09-23 04:28:29,19
Intel,k1tf175,"Yea same here, didn't play crysis until I got a hold of a 260 after having 2 8600gt's in sli. Played mostly wow at the time and those 2 8600gt's in sli got blown away by the gtx 260, but man that card was hot!  Remember in 2011 upgrading to a gtx 570, the first gen with tessellation and playing dragon age 2 which was one of the first games to have it. Ended up turning off tessellation cause it hit the card too hard, least until I got a second 570 in sli, which sadly died like a year later due to heat while playing Shadow Warrior.",AMD,2023-09-23 05:12:08,11
Intel,k1v57gq,Q6600 was the bomb. I ran mine at 3.0GHz all its life and it's still alive today. Had it paired with an 8800GT which was IMO one of the best GPU value/performance of all time.,AMD,2023-09-23 15:33:56,4
Intel,k1uehqr,Had 3.4ghz on my q6600 didnt want to hit 3.6ghz on modest voltage but 3.4 all day and was quite the lift in fps in gta4 compared to stock 2.4ghz. Run 60fps 1440x900 no problem those where the days when OC gave actual performance boost,AMD,2023-09-23 12:14:02,3
Intel,k1uvdfc,> I was so pumped when I upgraded to a gtx 260. Crysis was a dream.  In those days I was a broke teenager and I remember upgrading to a GTS250(rebadged 9800GTX+ with more vram) and crysis still hammered my Athlon x4 965(?) system.  While my richer neighborhood buddy upgraded to a 260 just to play Left 4 dead.,AMD,2023-09-23 14:28:22,3
Intel,k1th9tj,"> q9300  Yes, it was not a good bin. I was at 4.0GHz with my Q9550, with just a copious amount of voltage.",AMD,2023-09-23 05:36:54,2
Intel,k1uvhzl,"After checking sone old Evga posts, (mod rigs server is done, along with old signatures), I was able to get to 3.33ghz.",AMD,2023-09-23 14:29:15,2
Intel,k1vxcz9,"Yeah my lapped Q6600 was my daily driver at 3.2 GHz with a Tuniq Tower 120 Extreme. It ran it pretty cool. I think when Crysis came out it and my 8800 GTX could do 20-25 frames at 1680x1050 with drops into the teens. EVGA replaced it with a GTX 460 when it died, big upgrade.",AMD,2023-09-23 18:31:17,2
Intel,k1u94ng,moving data between the two is the issue,AMD,2023-09-23 11:20:15,10
Intel,k1ud2dh,grey cagey sharp detail handle north grandiose connect sparkle hungry   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:00:31,8
Intel,k1tzket,"I wonder the same thing. But I guess if Nvidia would put it all in an extra card, people would just buy more amd to get the best of both worlds.",AMD,2023-09-23 09:22:31,2
Intel,k1tcnbk,Game physics doesn't seem to be a focus anymore though,AMD,2023-09-23 04:47:04,15
Intel,k1vx974,I'm ashamed I even own one NVIDIA card.    Unsurprisingly it's in a laptop that I bought during the [illegal GeForce Partner Program](https://hothardware.com/news/nvidia-geforce-partner-program) before the public knew about the program.  Not making that mistake again.  Screw NVIDIA.  Every other card I've ever gotten has been ATI/AMD (and obviously the low-end integrated Intel GPUs on some office laptops)  EDIT: Keep them downvotes coming.  You've all been bullshitted by NVIDIA's marketing.  Your boos mean nothing; I've seen what makes you cheer.,AMD,2023-09-23 18:30:38,3
Intel,k1y74dq,Anyone who owns an Nvidia GPU should be ashamed of themselves tbh.,AMD,2023-09-24 04:08:48,0
Intel,k1zf2ze,"There was more nuance to that ""bugfix"" than just ""it was a bug that got fixed""   Modders were able to re-enable the AMD/NVIDIA PhysX combo basically immediately via editing DLLs.  Not driver replacements, but actual DLL tweaks.  The speed at which modders were able to fix what NVIDIA ""broke"" combined with the public outrage put them in a funny legal spot.  If they continued with PhysX disabled and claiming it was a bug then the bug can easily be fixed, as shown by modders doing it first.  So either fix it and get everyone back to normal, or leave PhysX disabled even though it can be easily enabled and open the company up to potential anti-competitive lawsuits.  Obviously not many modern games use the GPU implementation of PhysX anymore and the performance difference between current GPU and CPU implementations are negligible anyway, but their intent at the time was clear once it was shown how quickly modders were able to get everyone back to normal.",AMD,2023-09-24 12:27:53,0
Intel,k1u0kty,"Yes, but not after Nvidia bought Ageia.",AMD,2023-09-23 09:35:56,3
Intel,k1y5xsz,"If the $200 cards of today aren't 3x better than the flagship cards of 10 years ago (they're not), then you shouldn't expect a $200 card in 2033 to be 3x faster than an RTX 4090. Average annual performance gains are more likely to decrease between now and then, rather than increase.",AMD,2023-09-24 03:57:42,1
Intel,k1usgsa,I like how you said: static image  because in motion upscaling is crappier,AMD,2023-09-23 14:07:47,-1
Intel,k1t97dw,"It isn't a weird argument. Rasterization is premised on approximating reality, while RT is simulating it.  The extreme few amount of rays we trace right now, including bounces, means effects are often muted and the performance hit is enormous. To compensate, we're using temporal upscaling from super low resolutions *and* frame interpolation.  Temporal upscaling isn't without it's issues, and you know how much traditional denoisers leave to be desired. Even Nvidia's Ray Reconstruction leaves a lot to be desired; the thread on r/Nvidia shows as much, with ghosting, artifacts, etc. all over again. It's like the launch of DLSS 2.0.  All of that, for effects that are oftentimes difficult to perceive, and for utterly devastating traditional rasterization performance.",AMD,2023-09-23 04:13:02,-3
Intel,k1t7en2,"It's a shame raster is so labor intensive, because it looks so interesting when done right.     I look at screenshots from cyberpunk, Raster / Ray Trace / Path Trace, and in most of them the raster just looks more interesting to me. Not constrained by what would be real physical lighting. The 100% RT render versions of old games like Quake gives a similar feeling.     But I imagine there will be more stylistic RT lighting over time, it saving a ton of labor is all things considered good, freeing it up for actual game design.",AMD,2023-09-23 03:55:55,4
Intel,k1t22gi,">That’s why devs love RT and want it to grow, as RT/hardware does the work for them.  It's the marketing departments that love RT. Devs hate it because it's just yet more work on top of the existing raster work.",AMD,2023-09-23 03:08:23,0
Intel,k1u3zfu,"It *is* way slower, you're just compensating it by reducing render resolution a ton",AMD,2023-09-23 10:19:33,-10
Intel,k1ubnzq,If it was bloodborne i am guilty of that myself,AMD,2023-09-23 11:46:49,6
Intel,k1uerlc,price zealous rinse detail yam rainstorm groovy automatic snails fact   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:16:37,6
Intel,k1y7wcn,4k max without dlss or fsr or frame gen I still get 30fps. Sorry you can’t even hold 10. Guess I’ll shill some more to at least not be spoon fed shit from amd,AMD,2023-09-24 04:16:20,1
Intel,k1tarlv,"Ray tracing and path tracing aren't just ""reflections"" , but this thread is specifically about the new path tracing modes in CP2077.",AMD,2023-09-23 04:28:09,4
Intel,k1tfawo,Rasterisation is a “hack” too,AMD,2023-09-23 05:15:07,40
Intel,k1sxetf,If it works it works....computer graphics has always been about approximation,AMD,2023-09-23 02:30:22,36
Intel,k1t8lwj,"4090 is \~66% more powerful than 3090 overall, but it has 112% more fps in this test.     The RT capabilities of NVIDIA cards has grown faster than general capabilities.",AMD,2023-09-23 04:07:17,19
Intel,k1tqzxo,"I guess on the internet, you can just lie. There’s no game on ps5 that upscale from 540p to 4k.",AMD,2023-09-23 07:31:22,4
Intel,k1vlzcm,"I guess I was wrong on that one, although SWJS upscales from a resolution as low as 648p, which is not much.",AMD,2023-09-23 17:20:00,0
Intel,k1s7up5,It's big in certain lighting situations. In others it's not as noticeable as you'd hope,AMD,2023-09-22 23:19:50,-12
Intel,k1sony7,"PT vs no RT comp is silly.  U should compare PT vs RT, and there, the difference is minor and not even subjectively better. Just different.",AMD,2023-09-23 01:22:48,-6
Intel,k1sot43,Natural? It over saturates the light bleeding into the entire scene. Everything becomes so colorful. Even when floors or ceilings are not supposed to be reflective. Literally tiles with grainy texture suddenly becomes a mirror..  It's ridiculously overdone.,AMD,2023-09-23 01:23:54,-5
Intel,k1s8jx2,Guys I'm trying to suffer while I play the game what settings will let me suffer the most,AMD,2023-09-22 23:24:55,5
Intel,k1ubk56,"I mean this is native RT/TAA. Was never meant to be played this way. You need to use DLSS and Ray Reconstruction. With that setting, i get about 40-50fps at max settings with my 3080. Not too bad.  The thing about AMD is that they don't have any of this AI technology (yet). You have to rely on raw power, which won't get you far.",AMD,2023-09-23 11:45:47,2
Intel,k1snovf,"Until the next generation of consoles, I think path tracing will be added to something like a couple major games per year with Nvidia's help. If I'm right, that's wouldn't be a lot, but it's definitely be a bonus for those who have GPUs that are better at ray tracing.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode (which could be for a variety of reasons, including CP2077's larger world).",AMD,2023-09-23 01:15:25,3
Intel,k1sd914,Alan Wake 2 is also an outlier since the same dev made Control which was a test bed for DLSS 2.0 and one of the earliest games to fully embrace ray tracing. Alan Wake 2 is also NVIDIA sponsored and looking to be another test bed for them.,AMD,2023-09-22 23:58:55,7
Intel,k1ubcdm,"Redditors trying to have reading comprehension (impossible)     >It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it).",AMD,2023-09-23 11:43:38,-1
Intel,k1tno35,"Every single game has cut corners, starfield has multiple cut corners. Tech demo would imply there's no great game under great visuals.",AMD,2023-09-23 06:50:58,10
Intel,k1ucpbc,Get that fps with a 5120*1440 240Hz monitor,AMD,2023-09-23 11:56:59,1
Intel,k1u928y,"It's almost like crushing your fps with all these fancy new graphic features isn't worth it, even on the best cards. It's user preference in the end.  That said, many gamers, once they play at high hz screen and fps, find it very difficult to go back to low fps for any reason. Games feel slow by comparison, no matter how great the lighting looks.",AMD,2023-09-23 11:19:32,-3
Intel,k1vo11q,"Both DLSS and FSR have weird image artifacts (FSR is worse though) so i guess if you don't see them then DLSS only works in your favor then.  I personally want as little noticeable image artifacts from DLSS/FSR when I play so opt for native resolution, generally.",AMD,2023-09-23 17:32:45,1
Intel,k1s3igv,Have you tried dlss quality. Dlss balanced sacrifices image quality and dlss quality doesn't/can enhance it sometimes,AMD,2023-09-22 22:48:46,5
Intel,k1ugb6v,">a lot of the comparisons just make it look like it gives too much bloom  This is due to colors being compressed to SDR. In HDR that ""bloom"" looks awesome as it's super bright like it would be in real life but still not losing any details. You lose those details only on those SDR videos.  PT just looks and feels way more natural than just RT, when only raster literally looks like it was two different games from two different hardware generations.  Once I saw Cyperpunk PT there is no way I'd imagine playing this game in any other way. All other games look absurdly ugly next to it now. It's something like when you get a high refresh screen and can't look back at 60Hz anymore.",AMD,2023-09-23 12:30:45,1
Intel,k1wecws,"Trust me, I know how the technology works. That doesn't change the fact that it's not worth it imo. I'm not  paying 4090 money to have the newest fad that just makes games look over saturated and glossy.",AMD,2023-09-23 20:18:14,1
Intel,k1sbp25,We are talking about running the game just fine now are we? Because rasterization is just fine. I will consider ray tracing reasonable when you can do it maxed out for $1200(GPU) native. Until then it’s a gimmick. IMO,AMD,2023-09-22 23:47:34,-7
Intel,k1sj47l,"So even you, someone who is an enthusiast of this sort of thing, i.e. a small minority of the userbase, had to spend \*minutes\* benchmarking to realize you didn't have RT turned on.   Yeah, you would lose that bet.",AMD,2023-09-23 00:41:39,-5
Intel,k1tbdg8,Those wolfdale C2D were beasts. I had a buddy with one and we took it 4.0. And it kept pace/beat my C2Q as most games didn't utilize MC as well back then.  Man XFX and EVGA all the way. Shame they both left the Nvidia scene.,AMD,2023-09-23 04:34:12,9
Intel,k1tgkh1,"Ah, WoW. That started it all for me on an C2D Conroe with ATI x1400 on a Dell inspiron laptop freshman year of college.",AMD,2023-09-23 05:29:06,3
Intel,k1ueofm,8600gt in SLI man you are Savage!🔥,AMD,2023-09-23 12:15:48,2
Intel,k1v8q15,"I had the same setup, and i managed to get my Q6600 stable at 3.2.  Was a blazing fast machine and it still shit the bed in some parts of crysys",AMD,2023-09-23 15:56:19,4
Intel,k20zqc9,there there’s no way there will ever be another 8800 GT. you got so much for your money.,AMD,2023-09-24 18:32:27,3
Intel,k1v3f8y,"Those 9800gtx+ were solid cards, it's what I upgraded from.  You make due with what you had.  My first system was a Craigslist find that I had to tear down and rebuild with spare parts from other systems.",AMD,2023-09-23 15:22:27,2
Intel,k1vqgag,Seemed like a perfect use case for the sli bridge they got rid of.,AMD,2023-09-23 17:48:01,3
Intel,k1y719d,Why would it need to send the data to the other card? They both feed into the same game.,AMD,2023-09-24 04:07:58,2
Intel,k1uqeyb,Big up the Vega gang,AMD,2023-09-23 13:52:34,3
Intel,k1tgpce,"That is because destructible buildings and realistic lighting REALLY does not go hand in hand. Realistic looking games use a ton of ""pre-baked"" light/shadow, that might change when ray tracing is the norm but it has a delay so things still look weird.",AMD,2023-09-23 05:30:35,36
Intel,k1u8rwh,"I remember playing Crysis, flipped a wheeled garbage bin into a hut, which came down on top of it. I blew the wreckage up with grenades and the bin flew out, landing on it's side. Took pot shots at the castors and the impact made them spin around in the sockets.   Here we are 15 years later and game physics have barely moved an inch from that",AMD,2023-09-23 11:16:24,9
Intel,k1tffr8,Cause regular stuff is so easy to simulate or fake simulate that super realistic complex items are not really worth the trouble.  &#x200B;  water still looks like crap in most games and requires a lot of work to make it right.  and even more processing power to make it truly realistic.  Cyberpunk is perfect example.  the water physics is atrocious.,AMD,2023-09-23 05:16:37,16
Intel,k1toby8,It's because it's not the big selling point now and as the comment you're responding to... Things have gone so far no one really thinks about it anymore. Ray tracing is what physx used to be or even normal map and dx9/10 features you don't consider now.,AMD,2023-09-23 06:59:04,3
Intel,k1w7uac,"Honestly, same here - I hate Nvidia so much for what they do so shamelessly to the average consumer that I’d struggle to recommend their products even if they *were* competitively priced.",AMD,2023-09-23 19:37:13,1
Intel,k1thrmi,"People are just stupid, if game companies just slap ray tracing on and don't tweak the actual look of the games lighting, gaming graphics is doomed. Rt is not the magic bullet people seem to think it is, everything looking 'realistic'=/= good art design.",AMD,2023-09-23 05:42:24,2
Intel,k1t66vz,"I thought RT was loved by devs, as it allowed them to avoid the shadow baking process, speeding up dev time considerably?",AMD,2023-09-23 03:44:48,3
Intel,k1ueiu5,We are guilty of the exact same sin.,AMD,2023-09-23 12:14:20,2
Intel,k1tzi21,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-09-23 09:21:41,0
Intel,k1u5aaz,"All rasterization does is approximate colors for all the pixels in the scene. Same thing does raytracing, path tracing, AI reconstruction and whatever other rendering techniques there are.   The final result is what's most important, doesn't really matter how it's achieved",AMD,2023-09-23 10:35:43,3
Intel,k1u0whe,would you care to explain ? Kinda interested to here this,AMD,2023-09-23 09:40:08,1
Intel,k1scoi2,jellyfish follow simplistic plucky quarrelsome unwritten dazzling subsequent mourn sable   *This post was mass deleted and anonymized with [Redact](https://redact.dev)*,AMD,2023-09-22 23:54:43,8
Intel,k1tcqbd,"It isn’t minor. The PT with DLSS 3,5 is quite a bit more accurate and reactive, as in dynamic lights are reacting closer to instantaneous than without. The best way is to watch on a 4K monitor or to actually play it on someone’s 4090/4080 or wait for AMDs FSR 3 to see if that helps it do path tracing.",AMD,2023-09-23 04:47:55,6
Intel,k256x6l,> Literally tiles with grainy texture suddenly becomes a mirror  PT doesn't change what the materials are bro. You are just seeing what the materials are meant to look like.,AMD,2023-09-25 14:57:47,2
Intel,k1s8ne8,Turn on path tracing. Embrace the PowerPoint.,AMD,2023-09-22 23:25:36,5
Intel,k1ssjdy,Control also has some of the best implementations of RT I've ever seen. It's one of the very few games I've turned on RT and felt like it was worth the performance hit. Everything else I've played has looked very good with RT on but not good enough to be worth the performance hit.,AMD,2023-09-23 01:52:37,10
Intel,k1vpcr3,"Of course it is not perfect but it also allows max RT, PT, RR which improves the graphics greatly...and i do not enjoy below 100 fps either",AMD,2023-09-23 17:41:03,0
Intel,k1s8f3n,"I tried 1080p DLSS Quality and it was just as bad, but I’m not really happy with the performance of 1440p DLSS Quality (like, 30-40fps), so I didn’t want to use it.",AMD,2023-09-22 23:23:57,-2
Intel,k1wloml,"You're is coming off as salty that you *can't* afford a 4090. If other people wanna spend the extra cash and turn on all the bells and whistles, why does that bother you?",AMD,2023-09-23 21:03:39,1
Intel,k1tpscr,"Mate, you can't fucking run starfield at good fps, a game that's raster only that looks like from 5 years.  Look at how many games scale and run badly in last two years and compare that to cyberpunk",AMD,2023-09-23 07:16:33,6
Intel,k1sen3q,"This just absurd and frankly entitled. Not everyone can afford a PC strong enough to run the most demanding games absolutely maxed out.   I never could, couldn’t even afford a PC until this past summer.   This obsession with performance metrics is ridiculous. Like, less than 5% of PC gamers even have 4k monitors. Almost 70% still have 1080p.   CDPR made a VERY demanding game that looks GORGEOUS completely turned up. Modern hardware cannot handle it natively, it can’t. It is not a gimmick that a company has developed a superior technology that allows the game to be played at almost the same quality as native, significantly smoother and faster. You’re in denial if you think otherwise.  I’m not tied to either brand. I have an AMD CPU bc it was the best option for what I wanted, and a Nvidia GPU for the same reason. I care about RT, frame gen, all of that. The Witcher 3 still is my favorite game, and the entire reasoning for building a gaming PC over a console is graphical quality and speed while doing so. That game is absolutely gorgeous with RT on. Same with Cyberpunk, which I can enjoy thanks to DLSS.   You don’t care about those things? That’s fine, but it is solely a personal opinion.",AMD,2023-09-23 00:09:03,4
Intel,k1sdc8m,"Yea well I was defining an arbitrary performance. It's not about having native performance. But, but if a flagship is able to get 4k 60 with path tracing at native then that allows headroom for 60 tier or 70 tier cards to perform well.  In this example the 4090 gets 19.5 fps at 4k. The 4060ti gets 6.6 fps. Let's assume a next gen flagship is able to reach 60 fps at 4k. That would theoratically allow the 60ti card to reach around 20 fps. Once you do your various upscaling and frame generation techniques you can then reach 60 fps ideally with a mid range card.",AMD,2023-09-22 23:59:34,-1
Intel,k1tcqfl,"The quad core showed up the core 2 duos like 2 yrs after because of emerging support and I was on cloud 9. After evga left, i jumped ship with my XFX 6900xt and I think I am here to stay with team Red until value/longevity says otherwise.",AMD,2023-09-23 04:47:57,2
Intel,k1ttjgv,Shame you don't. They did it for a reason.,AMD,2023-09-23 08:03:50,2
Intel,k1v42qp,The 9800GTX was a bad ass. I'd consider it the biggest upgrade in generation until the 1080 and then the 4090.  It was far more powerful than anything near it.,AMD,2023-09-23 15:26:42,3
Intel,k1wmal2,"I do miss it, but it was a pain in the ass to get working with my custom cooler due to the HBM.",AMD,2023-09-23 21:07:26,2
Intel,k1w2bdg,They've actually gone a bit backwards. Devs dont seem to care about implementing physics anymore. It's just an afterthought. And you can forget about destruction,AMD,2023-09-23 19:02:32,5
Intel,k1wmeoj,"I gobble up the downvotes every time I say it and I don't give a crap.  Every other corporation sees through their bullshit.  Otherwise every console would have NVIDIA cards.  No console has an NVIDIA card anymore.  But the general population is more than willing to get bullshitted into buying their cards against their own interest.  EDIT: Forgot about Nintendo Switch, as is tradition.",AMD,2023-09-23 21:08:09,0
Intel,k1zjknf,"Anyone can say anything.  Anyone can change their intent after the fact.  Happens all the time.   Judge them by their actions alone and ignore what they say.  Going solely by **events** :   1) People use (mostly used) NVIDIA cards for driving PhysX   2) NVIDIA disables this ability, locking everyone to the god-awful CPU implementation at the time   3) Modders edit DLLs to get functionality back   4) NVIDIA THEN says it was a bug that will be fixed    It's not foil hat because I'm coming to this opinion based solely on what's measurable and real; events and actions.   It's not conclusive, but I'm not going to rely on any company's PR statements to sway me one way or another because they will (obviously) say whatever they're able to prevent lawsuits and public image damage.  Based strictly on **events** that looks like damage control on a bad decision.  They rectified it, even more recently made PhysX open source, I give them credit for making it right, but the intent at time time was clear.",AMD,2023-09-24 13:05:06,0
Intel,k1ue2iz,This is why I'm excited for Alan Wake 2. It is coming out with path tracing always having been part of it. This will definitely mean they built the art style around that.,AMD,2023-09-23 12:10:02,3
Intel,k1tkit9,"Yeah, I can't really blame people for not recognizing great design if they are not interested in it. I don't think most people are interested in design, even though we all benefit from it without knowing it.     People might get some sort of subtle feeling when they see some great craft made by a experienced professional, but they can't really put it into words or evaluate the quality of the design without experience. They likely can't tell what cause the feeling to begin with even if they can notice it.     But everyone can instantly judge how real something looks, which is the go-to standard for ""good"". And bigger numbers are better, even if they are fake. Niche features that nobody uses also sells, as long as it's a new feature with a captivating name.     This reminded me. Live action lion king, what a travesty, stripping away all the artistry and expression for realism.",AMD,2023-09-23 06:13:48,4
Intel,k1t75sa,"If you're using RT exclusively maybe, but no one in the games industry is - nor will they be any time soon.",AMD,2023-09-23 03:53:39,4
Intel,k1v732r,"A lot of raster techniques reduce the complexity of lighting into very naive heuristics. Like shadow mapping renders the scene from the light's pov to create a depth map, then the objects that are at a greater depth have darker colors drawn (I'm simplifying that but roughly how it works). It's like teaching a kid how to illustrate shadows on an ball vs the how light actually works. Raster techniques have evolved a lot since then, and use raytracing in limited contexts, screen space reflections and ambient occlusion for example, but they're still operating with extremely limited data in narrow, isolated use cases, which is why they often look awful. There's just not enough information for them to look correct, just enough to trick you when you're not really paying attention.",AMD,2023-09-23 15:45:58,7
Intel,k1ub5y8,All lights in a rasterised scene are “fake”.   Someone explains it much better here:  https://reddit.com/r/nvidia/s/eB7ScULpih,AMD,2023-09-23 11:41:49,10
Intel,k1t4gtg,Or a bag of fake tricks.,AMD,2023-09-23 03:29:16,12
Intel,k1u716i,">Every scene is lit completely differently with PT enabled.  It's really not, and it's very disingenuous of you to try and pretend it is. There are plenty of screenshot comparisons showing the difference in certain areas if you want me to link them, and I have the game myself and use a 3090.  There are differences, but saying ""every lit scene looks completely different"" with PT Vs regular RT is false.",AMD,2023-09-23 10:56:45,0
Intel,k1tnk4a,"I'm not talking about DLSS 3.5. I'm talking about PT vs regular RT in CP2077. The visual difference is subjective, it looks different, is it better? Not in my opinion, it over saturates the scene.  DLSS 3.5 is a separate issue, it fixes some of the major flaws of PT, the slow reflection updates.",AMD,2023-09-23 06:49:39,0
Intel,k1szwwn,Have you tried Metro Exodus Enhanced?,AMD,2023-09-23 02:50:34,6
Intel,k1ttno1,At less than 4K resolutions Quality is the only sensible option. The resolution it upscale from becomes too low otherwise and DLSS then struggles to have enough data to make a good image.,AMD,2023-09-23 08:05:18,3
Intel,k1wlywq,"It doesn't, I have no problem with people buying what they want. My problem is people assuming I can't afford something...",AMD,2023-09-23 21:05:25,1
Intel,k1u602a,What is this? A reasonable comment in this dumpster fire of a sub?,AMD,2023-09-23 10:44:29,3
Intel,k1sg0o3,Thinking that a $1200 graphics card should be just strong enough to run everything Max is entitled. I wear that crown.,AMD,2023-09-23 00:19:01,2
Intel,k1tgd2j,"I've got an evga 3080ti now. I suspect my next card is either going to be an XFX 8800xt, or catch an XFX 7900xtx on clearance.",AMD,2023-09-23 05:26:46,4
Intel,k1v2w6l,"I don't blindly support a single company, such as simply switching to AMD because my chosen vendors left nvidia. That's how competition stalls.  That being said the card I have now is a product of what I could get my. Hands on during the crypto boom, and is EVGA.  My next card is likely to be an XFX 8800xt/7900xtx.  So not sure why you're in here attacking me.",AMD,2023-09-23 15:18:58,3
Intel,k1wpc9k,Except it was just a rebadged 8800GTX/Ultra,AMD,2023-09-23 21:27:07,2
Intel,k1wso05,"I mean, the Nintendo Switch is using Nvidia graphics, albeit it’s a tiny 28nm iGPU from 2014… but I get your point. They’ve opened up a large technological lead over AMD thanks to the RDNA 3 debacle, and I’m *still* recommending only Radeon GPUs because Nvidia is just that hubristic.",AMD,2023-09-23 21:49:06,1
Intel,k1tlekn,Pixel art go brr,AMD,2023-09-23 06:23:58,2
Intel,k1uefox,Yeah they will. Lumen and lumen style gi systems will have to exist for fallback so the industry can move on.,AMD,2023-09-23 12:13:29,3
Intel,k1tws4k,"Yeah, I agree. It’s just a shame because it’s not really playable otherwise. DLSS with ray reconstruction is a little mixed.",AMD,2023-09-23 08:46:03,0
Intel,k1shejh,"It just doesn’t make sense. Game developers do not set GPU prices.   The game runs well on a $1200 GPU. Perfectly well. Not completely maxed though, and it’s weird to think the game shouldn’t be able to make full use of available technology if it wants to. My GPU cost less than half that and runs the game very well with RT on thanks to DLSS.",AMD,2023-09-23 00:29:11,4
Intel,k1tykau,"RR is not perfect, but I think overall Cyberpunk 2.0 looks so much better than it did previously - and it looked pretty damn good earlier too!  [Digital Foundry's video](https://www.youtube.com/watch?v=hhAtN_rRuQo) is again the yardstick for how many small things they point out where RR improves the situation, even if it has some things where it fails atm. But maybe DLSS 3.6 will solve those.  I do feel DLSS itself has also improved in the past few years in terms of ghosting and performing at lower resolutions.  On a 1440p monitor these are the starting point resolutions:  - Quality: 1707x960p - Balance: 1485x835p - Performance: 1280x720p - Ultra Perf: 853x480p  So even the highest quality option is sub-1080p. I wish Nvidia introduced a ""Ultra Quality"" preset that would be say 80% per axis resolution, something between DLAA and DLSS Quality. At 1440p this would be 2048x1152.",AMD,2023-09-23 09:09:21,2
Intel,jws0ze9,"Speaking of the AMD overlay, am I the only one that gets near zero CPU utilization every time they pull it up? GPU utilization fluctuates up and down depending on resolution and graphical settings, so it seems to be fine, but my CPU utilization readings never go above more than a few percent tops. I have a 5800X3D.",AMD,2023-08-18 21:33:20,72
Intel,jwrrxaq,GN has already made a [video about it.](https://www.youtube.com/watch?v=5hAy5V91Hr4),AMD,2023-08-18 20:34:56,54
Intel,jwtujwf,Honestly this is everything that I wished Adrenalin overlay had. It's pleasantly lightweight too.   Hats off to Intel's development team. They didn't need to publish this at all.,AMD,2023-08-19 06:46:11,11
Intel,jws0ogv,I wonder if this will get added to Mangohud and Gamescope.,AMD,2023-08-18 21:31:17,10
Intel,jws656f,This is quality. Great work.,AMD,2023-08-18 22:08:17,8
Intel,jwtl7yn,Doesn’t capframeX uses presentmon as its monitoring tool?,AMD,2023-08-19 04:58:08,4
Intel,jwse5d4,I can finally see if it really is the ENB taking down my Skyrim gamesaves,AMD,2023-08-18 23:05:28,3
Intel,jwt3rjk,"FYI, the open-source version has been available for years, was last updated 9 months ago, and does not contain an overlay or GPU busy stats. (It also has version number 1.8 while this new thing is v0.5)  Maybe they're planning to release source code later, but right now the source is closed.",AMD,2023-08-19 02:18:52,9
Intel,jwv7vh1,This is pretty damn awesome and will be incredibly useful when they develop the future features Tom mentioned. (insightful metrics),AMD,2023-08-19 15:06:20,2
Intel,jwx068e,"This is not bad at all. But it's not replacing Radeon overlay for me... yet.  1. Needs an option to minimize to system tray.  2. Many important Ryzen CPU metrics (temperature, power consumption etc.) aren't available without an ability to access Ryzen Master SDK. 3. Radeon Overlay can continue to monitor on the desktop, which I use constantly. I tried manually hooking DWM.exe and it won't. I might just be dumb, but I couldn't get it the overlay working without 3D running. 4. Will give credit where credit is due, unlike Radeon overlay, it works right with OpenGL. 5. And the credit is immediately removed, because unlike Radeon Overlay, it doesn't work right with DXVK.  6 out of 10.",AMD,2023-08-19 21:46:34,2
Intel,jx6nzqo,This is so good. The only thing I dislike is that there isn't a way to scale the whole thing up an down.,AMD,2023-08-21 21:00:38,2
Intel,jwt26ko,"At this rate Intel will have a more polished driver and software stack than AMD in 2 or 3 years, if not before.  Seriously, I suck for being able to buy an AMD GPU like the 6990 I owned, but I can't get near any hardware they made while using my system for work.  I need a CUDA alternative that works on more than 6 GPUs on specific linux distributions when the red moon shines above my building.",AMD,2023-08-19 02:06:15,6
Intel,jwvd88w,it's sort of misleading to call it opensource    when whatever intel beta version released is just MSI installer and binaries inside ...    i see no source of what is available for download ...   what's on gihub is something old w/o overlay,AMD,2023-08-19 15:41:34,3
Intel,jwsaw5e,Thanks Intel! I will try this out at least since I hate MSI afterburner.,AMD,2023-08-18 22:42:00,3
Intel,jwss1oh,The download link gives a warning in Windows:  >This site has been reported as unsafe   >   >Hosted by intelpresentmon.s3.amazonaws.com   >   >Microsoft recommends you don't continue to this site. It has been reported to Microsoft for containing harmful programs that may try to steal personal or financial information.  ???,AMD,2023-08-19 00:48:31,3
Intel,jwsaaac,"I really wanted to test with it why I was getting stuttering in Apex. Unfortunately, due to some work, I would be possible on Sunday.",AMD,2023-08-18 22:37:37,-2
Intel,jwteu8t,Doesn’t work for me. It crashed at the start with an error message and made Dolphin run way worse.,AMD,2023-08-19 03:54:41,0
Intel,jwscc3e,And AmD gives far more than Nvidia.,AMD,2023-08-18 22:52:16,-10
Intel,jwwf6wi,I can't get this to work. Is anyone having the same issue?  CPU Temperature (Avg) NA C  CPU: 5800X3D,AMD,2023-08-19 19:37:09,1
Intel,k25hh4a,I have a 5800x3D and PresentMon does not recognize it. Says UNKNOWN\_CPU.  Anyone?,AMD,2023-09-25 16:00:53,1
Intel,jwsaffm,People have reported that cpu usage in Radeon software is not very accurate.,AMD,2023-08-18 22:38:38,31
Intel,jwsejpm,I saved this because it is posted so frequently lol  https://www.reddit.com/r/AMDHelp/comments/14n55gd/cpu_usage_percentages_differ_between_task_manager/jq66gop?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button,AMD,2023-08-18 23:08:23,7
Intel,jwsu2it,"Probably overlay wasn't updated for W11 22H2.  Microsoft updated something and CPU reading was broken for MSI Afterburner also, after working the same for a decade or more before. They fixed it in the meantime.",AMD,2023-08-19 01:03:33,3
Intel,jwubtsm,"I had the same problem, googled around and found the problem to be c state in the bios. Just turn that off and it should be accurate.",AMD,2023-08-19 10:34:05,2
Intel,jws5mkd,Just use afterburner as OSD.,AMD,2023-08-18 22:04:42,2
Intel,jwtvx7w,I had that issue with my 5600X both with Nvidia and AMD overlay. More recently it randomly fixed itself.,AMD,2023-08-19 07:03:38,1
Intel,k3rohbn,"Yeah adrenalin was useless for it. It was displaying 1-2% usage for me, but msi afterburner, hwinfo, and rtss were showing much more, like 30-50, which made more sense.",AMD,2023-10-06 20:51:17,1
Intel,jwrzqfw,You beat me to it :),AMD,2023-08-18 21:25:03,3
Intel,jx08btc,"Yup, ""new Intel"" definitely seems to be getting nicer and with a ""younger"" company culture. Also, it's a smart move. They're building a nice little ecosystem around Arc cards, which is eventually what is going to drive sales when performance and stability matches NVidia and AMD.",AMD,2023-08-20 15:17:16,9
Intel,jwu03xe,And it's open source!  I'm liking Intel more and more since they're getting their ass kicked.  Apparently it was one guy's pet project.,AMD,2023-08-19 07:58:51,8
Intel,jx8l1n5,Technically it should be possible to add in MSI afterburner because it's open source,AMD,2023-08-22 06:01:19,1
Intel,jwtnlcl,"The new PresentMon shows more information.  Since it's open source, capframeX can implement this as well.",AMD,2023-08-19 05:23:42,5
Intel,jwyec42,It was a pet project of one of the Intel engineers.   6/10 is not bad!,AMD,2023-08-20 03:57:36,3
Intel,jx6uru1,It's still beta. I'm sure they'll fix it.  Intel has amazing engineers.,AMD,2023-08-21 21:43:19,1
Intel,jwtopeu,Intel is working on their 2nd generation GPU and it's said to compete with Nvidia highest tier.  And now AMD isn't planning to release a big RDNA4 chip.,AMD,2023-08-19 05:36:25,5
Intel,jwus2bx,"didn't nvn get merged with mesa recently edit: sorry, I mean NVK not NVN",AMD,2023-08-19 13:12:58,2
Intel,jwwr3eh,I hate afterburner and RTSS. This is way better,AMD,2023-08-19 20:49:19,1
Intel,jwst65i,"people probably spammed microsoft because this PresentMon also gives you the option of enabling amd, nvidia, or intel telemetry.",AMD,2023-08-19 00:56:51,8
Intel,jwtohd3,"Thank you for contributing exactly NOTHING to this discussion.  Truly, the world is a better place because thanks to you.",AMD,2023-08-19 05:33:51,-9
Intel,jwt37au,"Tell that to anyone that needs CUDA. Or GPU accelerated workloads in general.  Id suck for being able to get something like the 6990 I owned back then, but between lack of support for almost all workloads and the driver being shit since like forever... Nope.  Yes, yes, nvidia's control panel is old and looks like a windows 98 app. But it at least works and actually saves youre settings, unlike AMD's one that half the time simply forget about everything.  I currently own 5 systems. 2 nvidia ones, 3 amd ones. And all of the amd ones had driver related issues.  You can't simply work with that. Is not viable. And as a gamer you should not need to see if the drivers are working or not.  Fuck, amernime drivers exists just because how hard amd sucks at it.",AMD,2023-08-19 02:14:23,2
Intel,jwsd0ai,"Well, everyone uses RTSS anyway and it gives you basically everything.",AMD,2023-08-18 22:57:07,1
Intel,jx7hc39,Similar issue here with an i5 12600kf. Can't get temperature readings from it. I'll need to look at it more.,AMD,2023-08-22 00:20:25,1
Intel,jwscka1,"Really? Good to know it's not just me, then. Nonetheless, this seems like a pretty egregious oversight on AMD's part. If I'm not mistaken, CPU metrics are on by default in the overlay and every time someone switches it on, it's an extremely visible reminder of work that AMD still needs to put into the drivers.",AMD,2023-08-18 22:53:52,7
Intel,jwxcu6w,Speaking of said software why does it keep setting gpu max frequency to 1200MHZ?,AMD,2023-08-19 23:14:33,1
Intel,jwsf4i1,"Thanks for sharing your findings. It's pretty damning if even the Windows Task Manager does a markedly better job than Adrenalin. That's probably an understatement since, as I found out via other helpful Redditors here, the Adrenalin overlay may as well be broken. I'm just glad it's not something wrong with my system.",AMD,2023-08-18 23:12:39,1
Intel,jwsnfu5,Afterburner fucks with my settings in adrenaline,AMD,2023-08-19 00:13:59,5
Intel,jwu3toe,"NVIDIA's highest tier? Nah, but around RTX 4080 is the target, which might be bad news for AMD.",AMD,2023-08-19 08:48:58,3
Intel,jwv9n3n,"By NVN do you refer to Nintendo's API for nvidia? Or you wanted to refer to NVK that got into mesa?  By CUDA alternative I mean stuff like ROCm. CUDA have so many years of development on top of it and a stupidly widely support from software vendors.  While yes, NVK could help to alleviate the issue, is not even close to what native CUDA can do regarding GPU usage for general computing.  The issue with ROCm is mainly a lack of support on a lot of software and lack of support for a lot of hardware.  On the nvidia end you can get the cheapest trashiest GPU and it will have the same level of CUDA support as the most expensive enterprise product.  Yes, performance is not the same, but I can use a 3090 Ti for loads that needs more than 12 GB vram without needing to buy a dedicated AI GPU, and if a given task needs serious shit, then again, I can buy a dedicated solution that will run the exact same software with the exact same code I ran on the ""weak"" 3090 Ti.  That is not possible with AMD, if you buy an enterprise grade GPU from them, you need to build all your software stack for it instead of just moving it and keep going.  And while yes, developers from tools like PyTorch COULD improve their ROCm support, I totally get why they are not doing that. ROCm works on like what? 7 GPUs? On specific linux distros?  Is a matter of scalability and costs. ROCm needs to be supported on ALL AMD GPUs in order for it to be something worth developing for, otherwise there is not really a use case where the end user wont be writing their own stack.  And again, if you can start small, then scale up, nvidia is the only option, so AMD on this space is either ""I got a stupidly great offer where AMD provides the same hardware power and performance per watt at like half the prize so I justify building all my software from scratches"" or ""Ok, I'll go for nvidia and just use all the software already built for it and call it a day""  I really, really want to being able to use AMD GPUs aside of trivial tasks, but this is a very, VERY gigante issue that appears a lot, especially on the profesional space.",AMD,2023-08-19 15:18:05,3
Intel,jxi33zm,Except PresentMon seems to not be compatible with Ryzen CPUs at all... :/,AMD,2023-08-24 02:22:09,1
Intel,jwsus0t,I get downvoted for asking a valid question?,AMD,2023-08-19 01:08:51,3
Intel,jwtt73f,Thank you for continuing to contribute Nothing to this conversation.,AMD,2023-08-19 06:29:18,8
Intel,jwslkce,Clearly not more than this beta of presentmon,AMD,2023-08-18 23:59:56,1
Intel,jwsd4sf,I use amds for overclocking and that's about it really. But it's still far more tools than nivida gives,AMD,2023-08-18 22:58:01,-2
Intel,jwsfkyz,"To defend AMD here as loathe as I am lately. CPU utilization isn't a very useful metric to begin with, so it misreporting would be super low priority.  The stat tells you nothing useful about the hardware, just the thread scheduling. CPU could be stuck in wait and reporting a high number while doing no work or it could be bottlenecking somewhere else while reporting a low %.",AMD,2023-08-18 23:16:00,10
Intel,jwxd4ky,Where are you seeing this?,AMD,2023-08-19 23:16:32,1
Intel,jwsfr15,"The Adrenalin overlay is good for really everything except usage. But yeah, this has seemed to have been a problem for a good while now. Ive been using AMD for a little over a year now at least Adrenalin and the CPU usage was always wrong on two different CPUs (AM4 and AM5) as well. Me and that other person tested it on my system and he had 3 systems running different AMD CPUs and all were the same. Ill find the thread one day but its been a while so it is way down the list of my comments.",AMD,2023-08-18 23:17:13,1
Intel,jwsno08,"False setup.  Disable the button in the ui ( not settings)  For ""load on startup""  You literarily told afterburner to load it's settings on start up.  Huge mistake many people do because most people think it's ""boot on startup"" but in reality it's loading it's oc settings and stuff effectively overwriting adrenaline if you leave that enabled.",AMD,2023-08-19 00:15:40,12
Intel,jwuero7,3070 Also was their target for their first generation. I don't expect Intel to suddenly be great just because that's what they aim for.,AMD,2023-08-19 11:08:44,3
Intel,jwu8j4q,And XeSS 1.1 looks better than FSR.  XeSS 1.2 was just released and it has even more improvements.,AMD,2023-08-19 09:51:11,3
Intel,jwv8abu,Great news to gamers though,AMD,2023-08-19 15:09:05,2
Intel,jwvbwid,ah sorry I meant NVK,AMD,2023-08-19 15:32:34,1
Intel,jwsuuv1,"I don't know, I didn't downvote you.",AMD,2023-08-19 01:09:27,5
Intel,jwsec7c,My point is if you've got nvidia card you simply use 3rd party soft and got everything and more. So comparing that AMD's software to Nvidia's is kind of meaningless.,AMD,2023-08-18 23:06:52,5
Intel,jwt2g8b,"I actually agree with you, but I don't need the metric to be some scientifically accurate reading of performance or whatnot, I just think it'd be a useful tool for adjusting game settings. For example, if I'm trying to optimize performance in a game, does this setting make CPU utilization go up or down? How much almost doesn't matter. Is there a bottleneck, where is it, and am I relieving pressure on it with these settings or not?",AMD,2023-08-19 02:08:23,3
Intel,jwxvkn7,It’s where you oc in adrenaline,AMD,2023-08-20 01:27:06,1
Intel,jwuly2q,"There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Not because upscaling, but also because how important is to have drivers that work in tandem with specialized hardware units in the GPU.  Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  AMD fails hard on 2 of those things.  Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.",AMD,2023-08-19 12:21:17,4
Intel,jwv200x,"Yep, XeSS is just great, I happily use it in MWII on my Arc A770. But I still use DLSS on NVIDIA. But both are better than FSR.",AMD,2023-08-19 14:26:34,3
Intel,jwtzdkr,"Then you should be looking at ***GPU*** usage, because CPU usage will not help you answer any of those questions",AMD,2023-08-19 07:49:04,5
Intel,jwuvpd6,"Problem is, by itself it still tells you little to nothing. If GPU utilization drops like a brick you know something is bottlenecking. If CPU utilization changes in the absence of other data you know absolutely nothing of value.   The real kicker is the CPU utilization may not even be ""wrong"", it may just be polling at a very low rate because that's the other thing with these OSDs if you poll too frequently for up-to-date data that can kill performance from the overhead.",AMD,2023-08-19 13:41:23,2
Intel,jwv1po6,"> There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Agreed, not to mention that when they made Alchemist, yes their target was 3070 and they underperformed that, but they will eventually hit their targets as they learn to improve their architectures. For a first shot, getting to around 85-90% of their target is actually quite good. It's not like they undershot by 50% of the target. They got close.   But like you said, software is super important and these days having good software is half of a driver. I have an A770, the driver actually pretty good, like the software menu. It used to suck when it first came out because it was some crap overlay, but over time it's become a separate program and it works really good considering they're not even a year into Arc's release. They also have a good set of monitoring tools built in and the options are all simplified. They don't have super unnecessary things like a browser integrated into the driver suite, or a bunch of other stuff. I'm sure by the time Celestial is around if Intel does stick with Arc, that they will be ahead of AMD on the software game, assuming AMD continues down the same path they have now.  > Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  Yep, tools like [this one from NVIDIA](https://youtu.be/-IK3gVeFP4s?t=51) have made it easier than ever as a dev to optimise for NVIDIA hardware.  > AMD fails hard on 2 of those things. Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.  Yep because Intel recognises that to get close to NVIDIA they not only have to make great hardware, but also great software to go along with it. You can only do so much with hardware to match NVIDIA. You have to make great software along with in-game solutions like XeSS to match NVIDIA. FSR is behind not only DLSS but now also XeSS. AMD is just not competitive with NVIDIA on all fronts. Arc is just getting started, give Intel 5-10 years of time in GPU and they will outpace AMD for sure.",AMD,2023-08-19 14:24:33,6
Intel,jwuapdo,"There are many settings that directly impact CPU usage, so of course CPU utilization reading can be useful.  Not everything needs to be precisely accurate, this is a lousy defense. Getting 0% reads all the time is entirely worthless but some accurate reads is still useful information.",AMD,2023-08-19 10:19:39,2
Intel,jx3nm3i,"You are going out of your way to defend a broken feature.    7800X3D shows up as 1% CPU utilization during games such as Watch Dogs, Stellaris, Age of Empires. Clearly broken. Undefendable.",AMD,2023-08-21 06:56:43,1
Intel,jwv7qel,"And with a bit of luck, they will also hit AMD in the console market.  Right now AMD stays afloat GPU speaking because consoles, they were during a lot of years the only vendor offering both CPU and GPU in a single package, reducing costs A LOT.  Nvidia was not able to compete against that with the lack of CPUs and Intel with the lack of GPUs.  Now that Intel is working on dedicated GPUs they are going to eventually create APU like products, its just the natural outcome of producing both CPUs and GPUs.  And then AMD will NEED to compete against them instead of being the only option.  And if that happens, unless FSR gets turned into a hardware specific tech like XeSS that falls into different techs depending on the GPU its running on, AMD will be murdered.  Better software stack, better general hardware to accelerate and improve upscalling keeping costs down AND being a second player pushing prices down towards Sony and Microsoft?  AMD will be screwed up if they don't up their game. And hard.",AMD,2023-08-19 15:05:22,2
Intel,jwucdfz,"Right, so what kind of useful information can you tell me if I raise graphics settings and CPU utilization goes from 12-16% to 16-17% while framerate goes down from 120 fps to 110 fps?",AMD,2023-08-19 10:40:52,3
Intel,jwuwjmm,"All CPU utilization tells anyone is thread scheduling, and nothing about what the threads are or aren't doing. Like the only situation that it marginally tells someone anything is how many threads a program uses. But that usually doesn't fluctuate with games, games usually scale up to <x> number of threads and that is that. GPU utilization is the one that is far more useful for spotting bottlenecks, general performance observations are far more useful for tweaking.  Even Intel and co. will tell you that with modern processors and all the elements they contain as well as hyperthreading/SMT CPU utilization isn't a very useful stat anymore. Meant a whole lot more when CPUs were one core unit and didn't also include the FPU and memory controller and everything else.",AMD,2023-08-19 13:47:45,1
Intel,jx4di9f,"Cause unfortunately even if it wasn't broken, it's not useful in any meaningful capacity. Should it be broken? No. Would anything notable come of it if it were fixed? Also no. Given the bugs they got and room for improvement elsewhere it's one of those things that should be super low on the priority list or just removed wholesale.",AMD,2023-08-21 12:07:55,1
Intel,jwuithf,"That whatever setting you are changing does not have much of an impact on CPU utilization, and if your GPU allows, you can use it.  What is the purpose of this question, are you really not aware of any settings that impact CPU usage? Some RT settings have big impacts on CPU usage too.",AMD,2023-08-19 11:51:44,4
Intel,jwuq603,"The problem you're missing here is that settings that are CPU demanding generally don't have an easily observable impact on CPU usage, if they have any impact at all.  The reliable way to see if a setting is CPU demanding is to turn it up, see the frame rate go down, and then look at how **GPU** usage changed.",AMD,2023-08-19 12:57:40,3
Intel,jwuozk8,"I'm aware that some settingd impact CPU usage, but I think looking at the CPU usage is about as useful as your daily horoscope, or AIDA64 memory benchmark",AMD,2023-08-19 12:47:47,2
Intel,jwvi4ei,Congratulations on the new gaming laptop. Don’t let people get you down and not everyone has to have the very best to be happy so if it makes you happy then I’m happy for you.,AMD,2023-08-19 16:13:46,25
Intel,jwwunye,"Congrats the 30 series isn't half bad. Laptops aren't bad but gaming desktops add more umph to their performance. My first gaming rig was a gaming desktop with 64gbs ddr4 3200mhz, rtx 3060 12gb, 850w platinum psu, b660m gigabyte motherboard and a intel i5 13600kf. Couldn't have had a better first rig.my current build now is a hellhound raedeon rx 7900xt,gigabyte  z790x ax gaming motherboard,  850w platinum psu, 64gbs ddr5 6000mhz,Samsung 980 1gb ssd, intel i5 13600kf(watercooled).best upgrade I made for 1440p gaming.",AMD,2023-08-19 21:10:15,2
Intel,jx1t2ia,"Congratz!  Also, what's that wallpaper? Looks awesome",AMD,2023-08-20 21:29:18,2
Intel,jwvupca,I don't understand the stickers. Ryzen CPU Check. Both Radeon and GTX graphics? Is one discrete and the other dedicated? Just needs an Intel sticker to make it more confusing.,AMD,2023-08-19 17:32:54,1
Intel,jwv4u8q,"That is not a ""gaming rig"" not even close my friend. ;)",AMD,2023-08-19 14:46:02,-8
Intel,jwvgjd6,3050ti?  Gaming rip?  No,AMD,2023-08-19 16:03:09,-6
Intel,jwwhi65,"5 or so years ago i built my first real gaming rig. Few years later i bought new laptop, because i already had a gaming pc i wanted something capable to game but didnt want to spend too much money for high end. So i bought Lenovo legion with same gpu - 3050ti.   What i soon realized was that i love the comfort of being able to browse the net and game from the couch, being able to put my legs up, put my back against a big pillow etc... Despite having much more capable pc, i was lazy to play on it and was using gaming laptop instead 90% of the time.   Problem is, 3050ti in the last year or so really starting to show its limitations, most of new games are straight up unplayable anymore. Even on lowest settings it doesnt run newest games at playable standards, probably because of only 4gb of vram. Hogwartz, Rachet and Clank, Remnant 2 all pretty much unplayable unless you drop resolution to 720... I really regret not putting few hundreds more for something like 3070. (unrelated, but why stepping up laptop gpu tiers are so expensive, for example 3060 to 3070 is like 300e here for essentially same system otherwise, 3070 to 3080 is like 4-500e extra more, its even more expensive overall than desktop gpu tiers for way lesser performance improvements...).   So my advice would be, if you can get at least 3060, its worth it to pay up a bit more, ideally 6600m or 4060, those few hundreds increase in price will be worth in a few years.  ALSO: not sure if OP knows, but these laptops come with shittiest ram, upgrading it to a dual channel dual rank is pretty much a must as it nets 10-20% more performance.",AMD,2023-08-19 19:51:52,1
Intel,jwxgjhs,There is nothing wrong with laptops I'd build my own if there was a market readily available for it like a desktop.,AMD,2023-08-19 23:39:57,1
Intel,jwz7u2u,No!,AMD,2023-08-20 09:40:51,1
Intel,jx2ecvu,"This isn't exactly what I'd imagine hearing the words ""gaming rig"" but I'm glad you like it. Happy gaming!",AMD,2023-08-21 00:03:02,1
Intel,jz3n7rj,Ignore the haters dude. Congratulations on your new gaming laptop. Gaming is something that is supposed to make us happy not cater to snobs. Not everyone needs 4k 60fps. If it satisfies your requirements it's the best rig for you. I have a Ryzen 5 2500u and I game on that so I am super happy for you.,AMD,2023-09-04 14:46:53,1
Intel,jwvkdlj,"People take things too seriously, and tend to forget that every enthusiast was once a novice.  This is OP's first gaming setup; they deserve to feel that excitement.",AMD,2023-08-19 16:28:13,9
Intel,jwvnufp,who even buys the 'very best' laptop anyway?,AMD,2023-08-19 16:50:14,2
Intel,jwvc7sf,Lenovo Ideapad Gaming 3,AMD,2023-08-19 15:34:40,3
Intel,jx1ui6e,"It's just the default one that came with the laptop, theres a red and a blue version probs from AMD and Intel lol.  You should be able find them online.",AMD,2023-08-20 21:39:12,2
Intel,jwvwcww,Its a ryzen cpu with integrated radeon graphics and an nvidia dedicated gpu. Its completely normal and not really confusing.,AMD,2023-08-19 17:43:04,10
Intel,jwwk8it,APU + dGPU,AMD,2023-08-19 20:08:00,1
Intel,jwvby00,"You know what I mean.   It's my very first PC/Laptop set up so I'm definitely calling it a rig, it's way more powerful than anything I've owed before it.",AMD,2023-08-19 15:32:51,14
Intel,jwv71nz,"It games, it's a gaming rig. :)",AMD,2023-08-19 15:00:53,10
Intel,jwvgp6w,you do realise not every wants or needs a 4070 to play games they like,AMD,2023-08-19 16:04:16,9
Intel,jwyz4ai,I upgraded the ram. Has 16gb ddr 5 dual channel now,AMD,2023-08-20 07:45:41,2
Intel,jwvm2xc,"Thanks dude! I think people forget that not everone needs the same kinda thing from a computer which is why they sell multiple models.  This is great for what I need it for, and I feel like a kid at Christmas, happy to be able to dip my toe into PC gaming, finally!",AMD,2023-08-19 16:39:11,9
Intel,jwvpsjs,You get my point at least you should,AMD,2023-08-19 17:02:04,5
Intel,jwvh0km,"Just to let you know , these laptops have different versions , one I have is something like Ar15mhqp or some shit like that   It is less powerful than yours but decent enough",AMD,2023-08-19 16:06:23,4
Intel,jwvc5tx,That is okay if you see that way and congrats.,AMD,2023-08-19 15:34:19,-7
Intel,jwv9ikl,"There's nothing ""rig"" about a laptop. Laptops aren't built to be rigged.",AMD,2023-08-19 15:17:14,-6
Intel,jwvboff,"No, it's a notebook.  Also have a low end gpu dude.  Not a game rig. It's nice but is not that.",AMD,2023-08-19 15:31:06,-9
Intel,jwvib92,"I know, but the same laptop or any other with 6800m or other AMD gpu would have been a much better choice lol",AMD,2023-08-19 16:15:00,-9
Intel,jwxg0r9,All that matters is that your happy with it,AMD,2023-08-19 23:36:26,3
Intel,jwvhehi,Glad you’re enjoying it. People something think that unless you get something with everything maxed out then you couldn’t be happy lol,AMD,2023-08-19 16:08:55,2
Intel,jwvcmin,"Yeah I'm just happy to be able to start playing around on some games on PC now. I've wanted to play flight simulator on pc instead of my xbox for ages now becuase of all the addons,and now i'll have access to those :D",AMD,2023-08-19 15:37:25,5
Intel,jwvc36g,"3050ti isn't really low end. And it will do for everything I need it for. I have an Xbox too for the most modern stuff.  People think that everyone has to have 4k 60fps in every game these days and not everyone needs or wants that, especially in a first machine.",AMD,2023-08-19 15:33:49,7
Intel,jwvlvjc,"for you maybe, this fits my needs perfectly.",AMD,2023-08-19 16:37:52,6
Intel,jwzgnvu,"Yup I am more than happy!  I've only ever had and played with integrated graphics on any other laptop I've had so this is a huge step for me.  I will play most brand new stuff on my Xbox for the best ecperience but stuff like KF2 I can now play a 1080p ultra and get 100s frames, could only play that before at 900p low.",AMD,2023-08-20 11:29:53,3
Intel,jwvho43,Well you say that right now but one day a game you really wanna play will come and your pc will be shitting itself trying to run it. If I had the money I would buy the best rig.   But if you don’t have a lot of money like me it’s all about price/performance,AMD,2023-08-19 16:10:41,0
Intel,jwvcxpz,That is the important that you are happy but still the reality is one.  Just enjoy the notebook.,AMD,2023-08-19 15:39:35,-7
Intel,jwwilm3,"not too similar but I can recommend grabbing dirt rally 2.0 on a steam sale with all DLC for 8€ / $10 or whatever it is, generally games that simulate some things are pretty much at home on PC",AMD,2023-08-19 19:58:19,1
Intel,jwvcs7g,"Yes, the 3050 ti is a low end card.",AMD,2023-08-19 15:38:30,6
Intel,jwvljzf,I have an Xbox too so I'll be able to play it on there anything that is out of spec.,AMD,2023-08-19 16:35:47,1
Intel,jww7sx0,I have a friend playing Baldurs Gate 3 on a 980/ You don't know what low end is apparently lmfao,AMD,2023-08-19 18:51:09,4
Intel,jwvcxzc,It's better than the 1650 and the 2050 though. And they were the other choices.,AMD,2023-08-19 15:39:38,4
Intel,jwwudqt,And that is normal the game don't requiere a big gpu to run the game is a mid / low graphics.  Besides the 980 was the top of that generation. The 3050 is the low end of that particular generation.,AMD,2023-08-19 21:08:34,1
Intel,jwvw154,"Dont let the haters bring you down dude, if you are happy, then enjoy your new laptop!",AMD,2023-08-19 17:41:04,5
Intel,jwwuzrc,"Both cards are the lowest version of his generation.  Again the notebook is nice but is a low end ""gaming"" notebook and again the point is that you ENJOY IT and play many games.",AMD,2023-08-19 21:12:17,2
Intel,jwwv6vy,He needs to know what he have but also I congrats OP for your purchase. The point is that you enjoy it.,AMD,2023-08-19 21:13:29,1
Intel,nh1zs68,Except it doesn’t (only for one game). Did he watch his own video before narrating it lol,Intel,2025-09-30 18:53:07,12
Intel,nh1pojo,"Is this related to historically Intel compiler not compiling code efficiently for AMD cpus? Does it also make Intel CPUs overhead better? If yes, then they really fixed something. If no, then they wrote code normally for AMD cpus as well as Intel cpus.",Intel,2025-09-30 18:04:03,2
Intel,nh35vfm,"They've tested it with just three AMD cpus. Weird pairings. I would pair B580 with any i5 from last couple years. Seriously their testing is getting less and less ""real life"" and more for the purpose of a good clickbait.",Intel,2025-09-30 22:21:40,1
Intel,nh2q7m7,"Should have tested 2700X, 6 core v/s 8 cores was stupid in the first place.",Intel,2025-09-30 21:00:06,1
Intel,nh4qct7,"I couldn’t believe the video I was watching, the thumbnail was pure clickbait. They fixed one game, and improved some others a little.  And NEVER an Intel CPU in sight. I like the B580, but it deserves better ""testing"" than this.",Intel,2025-10-01 04:07:28,0
Intel,nh2ket3,"The problem is that this title makes it sound like they fixed the cpu overhead issue in every game, but that's not the case. Some games are still affected. Obviously any fix is better than nothing, but depending on what games you want to play, you will still see an overhead issues. It is nice though that in some of the games the overhead problem has been addressed though.",Intel,2025-09-30 20:32:44,8
Intel,nh2q72d,"Haven't watched the video, but damn.   If rue, HUB has officially become clickbait garbage. Not outright lying, but intentionally conveying misleading half-truths. Starting at least with the AMD fine wine video.",Intel,2025-09-30 21:00:02,4
Intel,nh293fj,"possibly. Honestly weird as hell to not through in intel cpu results with the same gpus so they can see if it's a all cpu overhead issue, or an issue with amd cpus with intel gpu. Like even one game if intel/amd gave about the same performance you could likely rule it out somewhat.  Also weird to say they fixed it if by the sounds of it to say it's fixed if a very limited number of games were updated to work better in also seemingly limited scenarios. That's more like finding a specific problem on a specific game they were able to improve rather than an underlying fix which if implemented should help everywhere.",Intel,2025-09-30 19:38:19,4
Intel,nh2wf2h,"No, B580 also performed poorly on i5-8400 and 10400 (both are 5600X-esq)",Intel,2025-09-30 21:30:55,7
Intel,nh4qb2n,"5700x would have made a lot of sense compared to 5600  12400f,13400f should be included as well",Intel,2025-10-01 04:07:07,1
Intel,nh2x17x,Would not have mattered. the cpu bottleneck in the intel driver is single-thread limited and largely due to draw-calls being stored in system memory as a buffer for the frametime render. (at least as far as my debugging could lead me)  (that's also why its less impacted on ddr5 systems no matter what CPU),Intel,2025-09-30 21:34:07,7
Intel,nh2vrkw,"They pushed the 6 core parts too hard as ""good budget parts"" for too long to not keep using them as their reference...  Even when we're seeing games that now have strokes when they don't have 8 cores to play with.",Intel,2025-09-30 21:27:35,2
Intel,nh2w6ck,It's been official for awhile. Funny how it takes a positive Intel title for people to realize.,Intel,2025-09-30 21:29:41,9
Intel,nh2wpqg,Then they optimized drivers.,Intel,2025-09-30 21:32:27,-1
Intel,nh3xdw3,"True, they would have to admit they were wrong.",Intel,2025-10-01 01:01:47,1
Intel,nh328pe,Which also fixed performance on 5600X,Intel,2025-09-30 22:01:23,4
Intel,nfolcd9,"Eventually you get the drivers where they need to be. People love to whine about Arc drivers but on the iGPU side people never complained. The community is never, ever happy with anything. Focusing their eggs on the Higher performance parts and then a slower LTS cadence keeping the old stuff alive is the right move.",Intel,2025-09-22 23:20:53,20
Intel,nfvr586,"They will update, just not day 1 for certain games. Who's playing with integrated graphics?",Intel,2025-09-24 02:32:50,3
Intel,nfw1d1a,"cant say i blame them for scaling down driver updates for pre-arc graphics hardware, ever since they announced intel was getting into gpus proper ive been wondering how long it would take before this happened.  lets be honest no one was needing monthly igpu updates for 2021 hardware that was so underpowered it couldnt even tie with a GT1030 ^(\[why does everything come back to 14nm\].)",Intel,2025-09-24 03:41:26,3
Intel,nge544o,Running an N97 Mini PC with integrated and noticed the recent driver branching. I'm still impressed that Star Trek Online is playable on it.,Intel,2025-09-26 23:18:44,1
Intel,nfxw4kc,Developers have been complaining about iGPU drivers for years. Matrix multiplications in OpenGL driver are still broken on a lot of integrated chips. It's driving me nuts every time I get a user report. 😭,Intel,2025-09-24 13:04:37,3
Intel,ng1xrn2,Get some ARL CPU’s which have matrix math in hardware.,Intel,2025-09-25 01:38:55,1
Intel,nh3mfpi,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-09-30 23:57:22,1
Intel,ne7p90z,Keep em coming.  I saw Intel was hiring for a GPU stack software engineer in Toronto. I expect more good things for the Boys in Blue.,Intel,2025-09-14 18:44:48,59
Intel,neklg4j,Does newegg have a pro or business section for the B50?  I heard CDW or similar outlets may start selling B50 / B60?  I want to look into four B60s for an AI workstation but the B60 may not release through consumer chains.  Has anyone seen the B60 for sale?,Intel,2025-09-16 18:45:51,3
Intel,nesm2sx,Quad mini display port ? WTF,Intel,2025-09-17 23:21:46,2
Intel,nehns55,It is so much weaker than the B580 though with only 70w... Very niche use cases.,Intel,2025-09-16 08:18:22,0
Intel,nebtujp,On the other hand noone buys workstation GPUs on NewEgg. For B50 being bestseller means 95 orders.,Intel,2025-09-15 11:29:34,-3
Intel,neldtpg,Looks like some stores have it available to order currently. Central Computers has it available as a Special Order.   [Central Computers -ASRock B60 24gb Creator $599](https://www.centralcomputer.com/asrock-intel-arc-pro-b60-creator-24gb-graphics-card.html),Intel,2025-09-16 21:01:36,1
Intel,nf3izch,Better than micro-hdmi.,Intel,2025-09-19 16:38:08,1
Intel,nei0pgk,The entire point is that you can power it from the motherboard.,Intel,2025-09-16 10:26:22,23
Intel,nehp2bj,It doesn't matter. The card sells and that's what matters.,Intel,2025-09-16 08:31:40,9
Intel,neoyhmc,Its a small workstation card like the Quadro 1000 etc..,Intel,2025-09-17 12:21:43,3
Intel,ned8agu,I guess I got lucky. I was able to get one off Newegg before they sold out. Should be arriving in the mail today.  Hopefully they will get more in soon. I'm surprised Newegg had so few of them.,Intel,2025-09-15 16:08:09,5
Intel,neekoul,or the usual supply chain is sold out so users buy them where they are available,Intel,2025-09-15 20:01:54,2
Intel,nehd4cy,Those are professional cards. Your company should be getting them from different sources.,Intel,2025-09-16 06:31:07,-6
Intel,ncw7a7r,"It's small but a decent performer, if you ask me. Intel needs to keep them coming.",Intel,2025-09-07 12:29:09,46
Intel,ncxsyv0,A single slot version would've been nice.  70w doesn't need a dual slot cooler.,Intel,2025-09-07 17:32:48,15
Intel,ncxlgty,12W idle power... here does the dream of low idle server... :I,Intel,2025-09-07 16:56:29,7
Intel,nfg5mr1,Where can I buy one?!,Intel,2025-09-21 17:05:57,2
Intel,nczlano,"I should have waited, i could have gotten two!",Intel,2025-09-07 22:42:04,1
Intel,ndvkspz,Where can we buy this? Seems like limited supply and retailers are going to try and raise prices. All the tech companies need to keep the retailers under their boot.,Intel,2025-09-12 20:06:45,1
Intel,ne2iom5,"I know it's not for this, but how does it do at gaming?",Intel,2025-09-13 22:23:17,1
Intel,ngj6yy4,I bet this thing is great for Plex transcoding,Intel,2025-09-27 19:48:54,1
Intel,ncx7evz,they need if they want to survive,Intel,2025-09-07 15:47:09,-10
Intel,ncyeimc,I was thinking opposite ... can we have full passive model?,Intel,2025-09-07 19:12:52,4
Intel,nd09cc6,nvidias alternatives here are also dual slot,Intel,2025-09-08 01:00:08,1
Intel,ndwdl3h,yeah single slot and x16 for pci4 boxes,Intel,2025-09-12 22:37:54,1
Intel,nf23rzh,"I haven't internalized the docs or tried it myself (on an A310), but apparently some folks have managed to bring idle power down quite a lot on Arc cards.    e.g. https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/",Intel,2025-09-19 12:16:12,1
Intel,nfmq2kt,Check Newegg for a resupply. I was able to get one from there.  My [build](https://www.reddit.com/r/sffpc/comments/1nhwb85/sktc_a10_intel_build/) for ref.,Intel,2025-09-22 17:12:04,1
Intel,ne3y0h2,It's currently available for pre-order on Newegg and B&H.,Intel,2025-09-14 03:40:10,1
Intel,ncxaghr,"Yeah, a multi-billion corp that's been dominating the CPU market for decades , with a 75% market share needs an entry level Pro card to survive. Take your meds.",Intel,2025-09-07 16:02:05,-4
Intel,nd38dp6,Something akin to a KalmX 3050,Intel,2025-09-08 14:24:45,2
Intel,ncxc3sw,"company that is falling apart, that had very bad run recently, firing a lot of crew, canceling huge investments that already costed billions.        Having foundries ... that no one want to use and not having load because products don't sell.             Big fall quickly as they have huge costs that cannot be easily scaled down.               Yes Intel is in very bad shape, it needs a good product that will be competitive and what is most important rebuild the brand that is in shambles now.",Intel,2025-09-07 16:10:13,4
Intel,nd4p4rd,"There is good news, the employees got their free coffee back",Intel,2025-09-08 18:42:00,2
Intel,nfx7rp7,"> I mean the price had skyrocketed as well  This is actually not true. As someone who wanted to buy a b580 for a rly long time but only for msrp, the card took like almost 9 months to hit msrp. I was watching it for a while and I did not feel like paying above msrp for it. Like until a few weeks ago all the ones available were like 50+ dollars over msrp. Now you can actually buy one for msrp at multiple places which is a good thing.   > How much of the problem would you say has been solved since release  The overhead problem hasn't been solved. I don't even think intel can fix it or has any interest in fixing it as they knew about it since december of last year and nothing has come out to try fix it. How big a problem is really dependant on what games you wanna play. It can only be like a 5% overhead issue which still puts it faster than the 4060 and 7600 but theres some games where it's like 20% or more where you have to have a very strong cpu to not see the overhead.  Do some research on the games you play and that will tell u whether to buy a b580 or a 9060 xt. I ended up going with a 9060 xt even though I would have preferred a b580 just cause amd was able to supply msrp copies around launch and i didn't feel like waiting any longer.",Intel,2025-09-24 10:12:04,12
Intel,ng40ivy,"There isn't a better brand new GPU to buy at $250 MSRP out there https://www.nowinstock.net/computers/videocards/intel/arcb580/ lots of stock available, great performance for the price, the driver has matured and keeps getting better. If you're gaming at 1080p/1440p I recommend it.",Intel,2025-09-25 11:52:03,4
Intel,ngwf3rn,Here's how it performs in a bunch of recent games  https://www.techpowerup.com/review/silent-hill-f-performance-benchmark/5.html  https://www.techpowerup.com/review/dying-light-the-beast-performance-benchmark/5.html  https://www.techpowerup.com/review/borderlands-4-performance-benchmark/5.html  https://www.techpowerup.com/review/metal-gear-solid-delta-snake-eater/5.html  https://www.techpowerup.com/review/mafia-the-old-country-performance-benchmark/5.html  https://www.techpowerup.com/review/battlefield-6-open-beta-performance-benchmark/2.html,Intel,2025-09-29 21:27:52,2
Intel,nfyr9rl,"The B580 at MSRP is a solid card that I don't regret, but there's probably something to be said for the really cheap 8GB models of the 9060 XT (in the $250-270 range) if your usage leans more 1080P/HRR than 1440+@60hz.   Basically thanks to how the drivers and the GPU's performance shakes out, the B580 is surprisingly good at dealing with higher resolutions but isn't very good at spitting out a ton of ""cheap"" frames.",Intel,2025-09-24 15:43:07,3
Intel,nc7pk6e,"Way to go Intel, keep them coming.",Intel,2025-09-03 16:04:07,74
Intel,nc742br,[https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw](https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw)  From Wendell Level one tech   **The Intel Arc B50 is the Better Alternative to Nvida's A1000**,Intel,2025-09-03 14:17:44,70
Intel,nc82nmq,"[The Intel Arc Pro B50 is designed for professional users, architects, engineers, AI developers, and video professionals, who need a stable, memory-rich, and energy-efficient tool for demanding tasks.](https://timesofindia.indiatimes.com/technology/tech-news/intel-expands-professional-gpu-lineup-with-new-arc-pro-b-series/articleshow/121288086.cms)",Intel,2025-09-03 17:07:27,26
Intel,nc76dba,three fiddy,Intel,2025-09-03 14:29:10,47
Intel,nc9107w,>16GB VRAM  Fuck yes.,Intel,2025-09-03 19:53:01,9
Intel,ncc94ic,"$350, SR-IOV and 16 GB of VRAM   Freedom from green for workstation users",Intel,2025-09-04 08:37:58,9
Intel,ncaod53,"I got a Dell Micro workstation with a 285, 64gb of ECC memory and their fastest SSD.  But I didn't get an Nvidia card as they seemed overpriced and the A1000 doesn't support 6k monitors!  The built in Intel graphics support higher resolution than the cheap Nvidia cards...  I'm going to buy this.",Intel,2025-09-04 01:12:14,7
Intel,nc7bmb1,Anyone have a link to where I can buy one at ?,Intel,2025-09-03 14:55:04,6
Intel,nc839m4,Where is link for it I been looking for this card 😭will micro have it,Intel,2025-09-03 17:10:18,5
Intel,nc849fc,"I would curious how this Card can perform vs RTX 3050 6GB in terms of gaming.  Since Slot Powered GPU are very rare these days on release which the demands are high for those.     And yes, i know, Workastion arent designed for gaming",Intel,2025-09-03 17:14:51,4
Intel,nc9krng,"Waiting for a single slot low profile card. Screw me I guess for choosing a case with such requirements. I already got the a310, but it is limiting to just transcoding with its small video buffer.",Intel,2025-09-03 21:28:43,2
Intel,ncaasjo,god do i wish this was an option for me,Intel,2025-09-03 23:52:11,2
Intel,ncaisxl,I want 4 of these.,Intel,2025-09-04 00:39:15,2
Intel,ncbx3dn,I wonder to ask who is main audience for this GPU?,Intel,2025-09-04 06:38:19,2
Intel,nc7oowi,WHERE GPU,Intel,2025-09-03 15:59:50,3
Intel,ncbe4l6,"That's pretty attractive, need more benchmarks on it.",Intel,2025-09-04 03:57:17,1
Intel,ncj76sp,How would it compare to Blackwell RTX PRO 1000 and Radeon Pro W7400?,Intel,2025-09-05 10:36:42,1
Intel,nc75h41,"Sad to not see any gaming benchmarks, yes not intended but still would be nice to see",Intel,2025-09-03 14:24:44,-9
Intel,ncdsmwy,"8K resolution, but with compression? No, thank you!",Intel,2025-09-04 14:49:33,0
Intel,nc7sjud,Just work on the memory bandwidth guys. Quadruple it (or at least double) and these will sell out like hot cakes.,Intel,2025-09-03 16:18:33,-6
Intel,nc8kqzi,For gaming Nvidia RTX 4000 SFF will run circles around this. The only con is price. It's a titan of small form factor.,Intel,2025-09-03 18:34:09,-10
Intel,nc8ydk1,"Looking forward to his Linux testing of this card, it could be interesting (And on that note I'm also looking forward to the B60 and the dual B60 cards partners are allowed to make).",Intel,2025-09-03 19:40:16,13
Intel,nch1u5o,was just thinking 70W and 16GB of ram no way is this a gaming card lol. awesome value though for an AI dev.,Intel,2025-09-05 00:40:31,1
Intel,nc87z9y,"Check out the level 1 techs video, on Newegg right now",Intel,2025-09-03 17:32:11,4
Intel,nciqoj0,"Pre-ordered on Newegg yesterday (9/4), expected delivery 9/18 +-. Just checked back a few minutes ago (12:30am, 9/5) and shows Out of Stock. Don't really need it (and I ordered a ARC A750 on release day that I didn't need ...lol), so I guess it's a me thing.",Intel,2025-09-05 07:58:31,1
Intel,nc8ugto,"Still a valid question though. Good enough is, well.. good enough when you're on a primarily business oriented machine. Would be nice if it could run some titles at min or close to min quality settings.",Intel,2025-09-03 19:21:19,2
Intel,ncbhi8y,Timespy scores:  3974 - rtx 3050  4269 - hx370 890M  4485 - intel arc a380 with a310 cooler mod  4833 - intel arc 140t  6041 - rtx a2000 12gb  6551 - rtx a2000e 16gb  8514 - Intel ARC b50  9329 - 4070m 8gb @ 70W  11003 - rtx 4060  11261 - ryzen 395+  13564 - rtx 5060,Intel,2025-09-04 04:22:51,1
Intel,ncaj8vd,"this the card where that might be achievable,   id chuck the next gen equivalent into that sexy minisforum ryzen ai max pc thats coming soon..",Intel,2025-09-04 00:41:55,2
Intel,nccrdef,"People running professional applications that require a lot of VRAM and precise compute.  CAD software, photo editing, video editing, digital painting, transcoding, broadcasting, servers.",Intel,2025-09-04 11:20:42,5
Intel,nccrm1o,People who use shared remote login who do need gpu processing power while logged in remotely.,Intel,2025-09-04 11:22:23,2
Intel,nc913z1,https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007,Intel,2025-09-03 19:53:31,7
Intel,nc7dz67,"This card is not designed for gaming. Or if it was, it's a joke to compare it to an A1000, considering that the A1000 is for a low end CAD or pro video workstation. Intel is not even trying to compete in the gaming market.",Intel,2025-09-03 15:06:35,27
Intel,nc7nor3,"There isn't much of a point to it, it's a slow card. It will be slower than a B570. It will be slower than an RTX 5050. For *gaming*, find a review for something that gets called a waste of sand for being too slow and too expensive, and this will be more expensive and slower than that.",Intel,2025-09-03 15:54:53,13
Intel,nc7pepn,"Based on specs and power draw, it should be around a GTX 1070  Edit: Based on Level1Techs' review it's probably more like a 1070 Ti, or somewhere around these two",Intel,2025-09-03 16:03:23,3
Intel,nc8cldm,That's like asking someone to run a marathon in basketball shoes.,Intel,2025-09-03 17:53:51,3
Intel,nc78ebk,level 1 tech did them,Intel,2025-09-03 14:39:14,2
Intel,nc7bsr2,This is not gaming gpu...,Intel,2025-09-03 14:55:55,2
Intel,nc7u65c,The gaming benchmarks have already been posted here.   [https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw](https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw),Intel,2025-09-03 16:26:21,1
Intel,nc9v90m,"The B60 is supposed to have double the memory bandwidth (456 GB/s vs. 224 GB/s for the B50), and the B60 comes with 24 GB memory (And 192 bit vs. 128 bit for the B50).  Will be interesting to see the dual B60 cards that were shown previously that partners can release.",Intel,2025-09-03 22:24:27,5
Intel,nca1lbt,"Shockingly, the modern [$1250](https://marketplace.nvidia.com/en-us/enterprise/laptops-workstations/nvidia-rtx-4000-sff-ada-generation/) workstation card beats the $350 workstation card... and at gaming...",Intel,2025-09-03 23:00:24,9
Intel,nc8zkdx,"RTX 2000 is pretty close though, but that is still double the price of this card.",Intel,2025-09-03 19:46:03,2
Intel,ncbdupw,I would hope it would at that price point.,Intel,2025-09-04 03:55:18,1
Intel,nccf0b4,Release Date: 9/18/2025  https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007,Intel,2025-09-04 09:37:30,2
Intel,nc8edb6,Fly by Night is one of the best,Intel,2025-09-03 18:02:20,2
Intel,nc7ii2u,"I don't give a shit what its intended purpose is, still doesn't hurt to have game benchmarks.",Intel,2025-09-03 15:28:51,-1
Intel,nc7ugah,\>but muh gaming,Intel,2025-09-03 16:27:42,4
Intel,nc8igk2,"Agreed... But the B570 is 150w, and the RTX 5050 is 130w. What about SFF PCs without a PCI-E power connector? What about low power desktops running on battery (RV, etc.)? What about a cheap, compact eGPU that can use a PicoPSU as the power supply?  Currently, for slot powered, the options are slim:  * Geforce RTX 3050 has solid performance at 70w... but only 6 or 8GB of RAM. * Radeon RX 6400 at 53w is a little beefier than a Steam Deck, but not much. * Radeon RX 7400 just came out, is 53w, and is better still - but still limited to 8GB of RAM and appears to be OEM-only. * Alchemist A310 at 75w and 4GB is a joke. * Alchemist A380 at 75w and 6GB isn't amazing, and trades blows with the RX 6400.  An Arc B50 Pro ticks a lot of boxes. 70w means slot powered and not even pushing to the very limit. 16GB of RAM means you won't have massive framebuffer limits. [Geekbench 6 Benchmarks from Toms Hardware](https://www.tomshardware.com/pc-components/gpus/intel-arc-pro-b50-is-up-to-20-percent-slower-than-the-arc-b570-gaming-gpu-in-early-geekbench-tests-almost-doubles-the-a50-in-synthetic-tests) show 69890 OpenCL and 78661 Vulkan, which is slightly better than the 3050's 63488 and 62415 respectively.  The prebuilt eGPUs with a 7600 XT are decent. Only 8GB RAM, but the 7600XT easily beats the B50 Pro and 3050 at GB6 with scores of 83093 and 99525. But... it's $550-$600 for these. They aren't upgradable. And that 8GB of RAM is rough.  And sure, the Ryzen AI 395 with the Radeon 8060S is really solid... but Mini PCs with that are at least $1000, and that's getting a whole new machine. Thunderbolt 3+/USB4 ports are now quite common. Taking an existing laptop or mini PC from '720p low only' to '1080/1440 with moderate settings' is huge,",Intel,2025-09-03 18:22:55,2
Intel,nc7wxde,It's slot powered no PCIe power cable needed,Intel,2025-09-03 16:39:41,2
Intel,nc87hit,What 2080ti is low profile and slot powered?,Intel,2025-09-03 17:29:53,5
Intel,ncdh752,"Intel had to trim pretty far to get the power down. below 70W.  Really nifty, but it does hamstring the memory more than I'd hoped.  Definitely looking forward to seeing what it's capable of, especially for the price.",Intel,2025-09-04 13:53:57,2
Intel,ncb6po9,Does rtx 2000 even exist. Didnt rtx become a thing around 4000s,Intel,2025-09-04 03:05:03,-2
Intel,ncf81ex,Already gone.,Intel,2025-09-04 18:52:20,2
Intel,nc7rnof,Also an excellent album,Intel,2025-09-03 16:14:16,2
Intel,ncb7nvv,"A380 is definitely more powerful than RX 6400, unless the game has driver issues or poorer optimization on Intel arc. But yeah, nothing amazing really, except for the codecs. RTX 3050 LP is pretty much the only viable choice today.",Intel,2025-09-04 03:11:29,1
Intel,ncbc1me,"Rtx 2000 ada sff was released at the same time as rtx 4000 ada sff. So yes, it does exist.",Intel,2025-09-04 03:42:16,1
Intel,nc8eb8x,I agree,Intel,2025-09-03 18:02:03,1
Intel,nc8ekfi,"I'm not aware of anything else that's this small, doesn't need external power, performs this well and has 16GB VRAM, and this isn't for gaming but it can game, as shown in the video. Would make for an interesting ultra SFFPC.",Intel,2025-09-03 18:03:20,4
Intel,nc87l4p,"I have a 5090 thanks, again its slot powered.",Intel,2025-09-03 17:30:21,1
Intel,ncc44ot,Oh yea okay.,Intel,2025-09-04 07:47:17,1
Intel,nc8ce0w,No you don’t.,Intel,2025-09-03 17:52:55,0
Intel,nc8f0qp,"Actually I do..... https://m.youtube.com/@TnTTyler   And other gpus and other systems, so try again",Intel,2025-09-03 18:05:36,1
Intel,nc8h5vp,"You love to call people kid, kid.",Intel,2025-09-03 18:16:31,0
Intel,nc8pdvz,Brave attack there kid. Btw. You do know kid has more than one definition. It’s your actions and attacks that are childish. If you weren’t ignorant you’d know this but instead you’re accusing people of things and crying about AMD everywhere with one of your many accounts. Lame. Try again.,Intel,2025-09-03 18:56:32,2
Intel,ncc9i6e,Whats this product codename? Still Battlemage?,Intel,2025-09-04 08:41:56,11
Intel,ncj2gbl,I miss the days when entry level and mid end GPU doesn't need external power.,Intel,2025-09-05 09:55:05,5
Intel,nci6nqh,forgive my ignorance but this isn’t made for gaming right?,Intel,2025-09-05 04:55:53,1
Intel,ncckx18,A is for Alchemist.  B is for Battlemage.   C is for Celestial.  D is for Druid.,Intel,2025-09-04 10:30:52,23
Intel,ncj2n21,"It's not made for gaming, however Intel drivers allowed it to run game. Kinda like what Nvidia did with their Quadro card.",Intel,2025-09-05 09:56:51,4
Intel,ncit3tb,"Why does it always have to be ""gaming...  No, it's an entry level Pro card, for AI etc. While I'm at it, it is also cheaper and faster, has twice as much VRAM as the nVidia A1000 card.  From what I've seen, it can do 70/90fps at 1440p medium details in CP2077.",Intel,2025-09-05 08:22:21,5
Intel,ncj7ipv,">Why does it always have to be ""gaming...  because 'gaming' is a single task that is generally representative of realworld hardware performance in all aspects due to its unparalleled diversity, no single number will tell you as much about a gpu as the framerate will.",Intel,2025-09-05 10:39:31,-2
Intel,ncjcnll,"This day and age with crappy game optimizations, buggy engines and early access slop being sold as AAA full price ""masterpiece"". Highly debatable.",Intel,2025-09-05 11:18:47,0
Intel,nenev3w,Am I just misremembering that the a750 limited had 16gb of RAM?,Intel,2025-09-17 04:17:02,2
Intel,neki49q,"Such empty in the comments.  They need to release an A750 with 16gb in large numbers.  Intel is not producing many B50 / B60s?  Four A750s with 16gb each could be a decent AI workstation. llama4 requires at least 48gb, 64gb or higher recommended.",Intel,2025-09-16 18:29:41,1
Intel,net932k,you're thinking of the 770,Intel,2025-09-18 01:34:02,3
Intel,neqfxc1,They need B7xx cards. Releasing another Alchemist card right now would be pointless when there already is a 16GB A770.,Intel,2025-09-17 16:55:02,11
Intel,netg18f,"why on earth would anyone buy A750 16gb in 2025-2026?  the A770 couldnt even beat the best of 2018 hardware, and their profit would have to be basically negative if they wanted to compete with AMD's 300-400$ cards nevermind nvidia with the cuda advantage.",Intel,2025-09-18 02:14:35,5
Intel,nbhfmxf,Not surprising  It has 16Xe Cores (equivalent to 32CU/SM)   14Gbps GDDR6 memory   ~2.3Ghz core clocks   B580 has:  20Xe cores   19Gbps memory  2850mhz core clocks,Intel,2025-08-30 12:53:00,35
Intel,nbibnhv,It’s a workstation so,Intel,2025-08-30 15:48:42,10
Intel,nbifzsy,This is a workstation card.  Its drivers won't be optimized for games -- they will be tuned for things like Autodesk Maya and AI.  This rumor is totally immaterial.,Intel,2025-08-30 16:10:25,21
Intel,nbhvq3x,That colorway is actually... Cool.,Intel,2025-08-30 14:27:05,3
Intel,nbivcsx,$90-$100 MSRP? 🤔,Intel,2025-08-30 17:26:25,1
Intel,nbmp556,hope this helps drive down the prices of LP cards on the used market,Intel,2025-08-31 08:18:54,1
Intel,nby1363,"I've one pulling from my uncle's workstation for testing before returning it to his case.    Cons: Even though it has 16GB Vram but the card can only match with 2060 Super/Vega 64 at raw perfomance (I've know it's using for training AI or workstation task) and laking driver, even MSI Afterburner was not reconized with this card...    Pro: The Benmark score on Superposition 8K optimized is 'bout 2100 to 2250 depend on how cooler the card is. God of War 2018 2K High Setting has 30-55FPS and play very smoth without lagging, Cyberpunk has bout 30-40FPS at 2K High Setting so it's playable.    So will you get one for yourself?   _Yes: It's card with 70W TDP (But runing at full benmark consume about 55W) so if you are using under 5 Litters sff case like NV10, A09M with small PSU like Flexguru 250W or Pico PSU 200W above, 250W Gan HD Plex,... or you wanna train small AI or developing or any workstation task that consume much Vram... Buy this card.   _No: The perfomace is not match with it price so better buy 4060 LP, 5050 LP, 5060 LP or much money buy A2000 Ada, A4000 Ada SFF...",Intel,2025-09-02 02:11:25,1
Intel,nbhfu7f,Arc Pro? Who's the target audience? Doesn't look like a gaming oriented card to me.,Intel,2025-08-30 12:54:14,-8
Intel,nbhv3bn,"They're comparing it against the B570, not B580.",Intel,2025-08-30 14:23:37,16
Intel,nbj9s7i,And just 70W  https://www.intel.com/content/www/us/en/products/sku/242615/intel-arc-pro-b50-graphics/specifications.html,Intel,2025-08-30 18:38:55,17
Intel,nbnjk4g,Why do people who visit this sub assume everyone is a gamer?,Intel,2025-08-31 12:43:45,1
Intel,nbtglxd,"This used to be true decades ago (back then even the hardware was significantly different in some cases), but nowadays pro cards are quite similar. The ""special drivers"" usually just exist for certification purposes and are made from the same code as the gaming drivers. It's mostly product segmentation these days. So the pro cards tend to have more memory, sometimes have ECC memory, different form factors and different power/performance optimization. That's it.",Intel,2025-09-01 11:08:47,1
Intel,nbtjnsj,Doesnt really matter. This is just another cut down version of Battlemage. It will scale however they want to chop it up to. Like the Radeon Vega Frontier workstation  card came out and had mediocre performance so everyone outcried saying just wait for the gaming “optimized” Radeon Vega 64. It was the same card and performed identical.,Intel,2025-09-01 11:32:51,1
Intel,nbn02nc,Which part of geekbench said it is a gaming benchmark to you?,Intel,2025-08-31 10:06:55,0
Intel,nbjak4a,MSRP for the Arc Pro B50 is $299.,Intel,2025-08-30 18:42:53,6
Intel,nbhgoj1,Professionals / AI freaks,Intel,2025-08-30 12:59:27,27
Intel,nbhgtgd,Low-end workstations and lab equipment. I had a similar low-end professional GPU installed on an [X-ray diffractometer.](https://www.malvernpanalytical.com/en/products/product-range/empyrean-range/empyrean),Intel,2025-08-30 13:00:18,11
Intel,nbiqh1e,The term “pro” is short for “professional”,Intel,2025-08-30 17:02:31,10
Intel,nbrl67u,"Also depends on the price, if its under $300 then it's a pretty good deal as far as workstation cards go.",Intel,2025-09-01 01:52:35,3
Intel,nboqwvj,Because the vast majority of redditors aren't very smart people to put it mildly.,Intel,2025-08-31 16:33:07,5
Intel,nbp46we,"That's its typical usage.  But you're right, Geekbench is a shoddy cross platform bench, despite its typical use for gaming by bad reviewers.  That said, it is very unlikely that this card's drivers are tuned to Geekbench, unlike the mature drivers in the B570.",Intel,2025-08-31 17:37:36,1
Intel,n9is5y3,I bought my B580 2 weeks ago :(,Intel,2025-08-19 12:55:40,22
Intel,n9ign1k,"Good enough a reason to get my buddy a b570 for his birthday, might just pocket the game!",Intel,2025-08-19 11:43:46,42
Intel,n9iuewn,You gotta be joking. Literally got a 14600K for $150 a few days ago. :(,Intel,2025-08-19 13:08:28,10
Intel,n9mwntx,Hopefully this means BF6 will get an APO profile.  u/Aaron_McG_Official can you confirm?,Intel,2025-08-20 01:25:39,10
Intel,n9isjsk,I got Borderlands 4 with my 5090. Would rather trade it for this.,Intel,2025-08-19 12:57:54,13
Intel,n9ksl0i,Maybe its a good time for them to drop big battlemage on the market??,Intel,2025-08-19 18:47:47,3
Intel,n9nk7b7,I'm surprised hardware companies are still doing this. People can easily abuse this by getting the game for free and returning the product they bought. Well it works in places that have strong consumer protection laws.,Intel,2025-08-20 03:53:03,2
Intel,n9pkqq4,"I bought my intel ultra 7 less than two weeks ago, damn",Intel,2025-08-20 13:36:32,1
Intel,n9zejrm,Deep thought stimulation.,Intel,2025-08-21 23:27:21,1
Intel,na0vyfw,"I wonder what retailers, will amazon be one of them?",Intel,2025-08-22 05:16:50,1
Intel,na35vt4,"That's a pretty sweet deal, especially for pc builders on a budget.",Intel,2025-08-22 15:15:53,1
Intel,na56wcq,Are latest Achemist/Battlemage GPUs faster than my 3080 at 4k?,Intel,2025-08-22 21:20:10,1
Intel,nam49mt,Is this live now?,Intel,2025-08-25 17:08:03,1
Intel,napztur,I bought 13700K on Jan 2023 can I get a serial as a compensate ?,Intel,2025-08-26 06:49:04,1
Intel,nbvyhnv,"Quick question: I got a laptop from Amazon with an Ultra 9 CPU, and Amazon sent me a code. However, no useful instructions on how to redeem it. Usually they have you download nvidia app or something like that, but this says to add the item to your cart and try to use the promo code? Tried that and nothing. Anybody else having trouble with this promotion? Is there an Intel app I need to download to redeem this code?",Intel,2025-09-01 19:10:40,1
Intel,nceleuh,"Bought i7 14700 from amazon uk, yesterday, did not reciece master key from them to redeem offer",Intel,2025-09-04 17:04:49,1
Intel,nd1b35w,Dang I just found out about this today but got my i9 cpu on the 24th of august. Luckily I just got a laptop so I get the code from there but it would've been nice to give one away.,Intel,2025-09-08 05:09:53,1
Intel,nd9spid,"Bought a 14600K, only to be hit by a ""This offer is not available in your country"". There is NO region restrictions outlined in the promotion Terms and Conditions. Very sketchy.  Something that can be looked into, or do I need to return the processor?",Intel,2025-09-09 14:38:16,1
Intel,ndvetzd,Is this a Steam or EA key?,Intel,2025-09-12 19:37:17,1
Intel,ndx53kp,"If anyone wants to flip their BF6 key let me know ill buy it off them at a discount, ive got a 13700 so think I can redeem it",Intel,2025-09-13 01:23:04,1
Intel,ne38u41,Is there a way to redeem the game without actually installing or having a 13/14 gen cpu? I got a 14600k in a bundle and want to resell it as new but I also want to redeem battlefield 6. The guy at microcenter says the system does a hardware check to make sure you have a qualifying cpu installed.,Intel,2025-09-14 00:57:18,1
Intel,n9kg3eh,I bought it at the height of the voltage controversy when the fix wasn't even known. Nothing for me then? 😐,Intel,2025-08-19 17:47:56,1
Intel,n9k1uw4,Even if it's free it's not worth it.,Intel,2025-08-19 16:42:03,-4
Intel,n9hnrzl,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-08-19 07:21:54,0
Intel,n9hxgxo,Price scheme or they are that desperate.,Intel,2025-08-19 09:00:00,-31
Intel,n9ifdfr,I’ll do this deal when my stock sells ha,Intel,2025-08-19 11:34:44,-5
Intel,n9itlw5,Mine just got delivered on Friday too….,Intel,2025-08-19 13:03:56,10
Intel,nap9vbe,Return it and than rebuy it,Intel,2025-08-26 03:17:45,1
Intel,n9ljaij,"That's still a great deal, even without BF6!",Intel,2025-08-19 20:55:16,5
Intel,n9u7unw,Same here lol. I got one to upgrade my sisters build.,Intel,2025-08-21 04:28:01,3
Intel,na6f1ek,Can’t return it?,Intel,2025-08-23 01:42:36,2
Intel,nabo7gy,Check you probably got the previous deal,Intel,2025-08-23 22:53:44,2
Intel,n9sex8a,"Per Intel policy, I can't comment on unreleased products. Historically, APO has tested the top superlative and popular titles; if the team gets them to show improvement then they are added.",Intel,2025-08-20 21:55:58,8
Intel,n9iyx6o,"That's fair. I can trade you a B580 with Battlefield 6 for a 5090, you can even keep the copy of Borderlands 4.",Intel,2025-08-19 13:33:12,38
Intel,n9m1uw7,"That would be amazing, but I doubt it.",Intel,2025-08-19 22:30:13,3
Intel,n9o9p5i,It will likely come in Q4 2025 or Q1 2026,Intel,2025-08-20 07:34:59,1
Intel,nbdcq2g,I have a buddy at bestbuy and asked if this would work and he told me that when you go and return the product the price of the game will be withheld from your refund if the code has been redeemed. He did also say that would happen when nvidia included a code for star wars outlaws and I had no problems getting a full refund with the game redeemed,Intel,2025-08-29 19:30:00,1
Intel,n9q85wa,Same man 😅,Intel,2025-08-20 15:32:52,1
Intel,na9scx4,Nope. 3080 is faster by a long shot.,Intel,2025-08-23 16:41:37,1
Intel,namiblo,Yes,Intel,2025-08-25 18:15:05,1
Intel,ngbq5b8,Doubling back because I still have the code and have no idea what to do with it lol,Intel,2025-09-26 15:52:50,2
Intel,nd889rk,Anyone solved how to get/redeem the BF6 code?,Intel,2025-09-09 07:53:20,1
Intel,ne18nks,"I got the same error, hoping that it can be resolved.  Edit:  Fill out a support ticket indicating the error you got here as soon as possible, selecting the ""I have an issue with a title promo code"" issue topic: [https://softwareoffer.intel.com/Support](https://softwareoffer.intel.com/Support)  The ""not available in your country"" error is one of their FAQs: [https://tgahelp.zendesk.com/hc/en-us/articles/13531766299021-Unfortunately-this-bundle-is-not-available-in-your-country-If-you-received-this-message-in-error-please-contact-support](https://tgahelp.zendesk.com/hc/en-us/articles/13531766299021-Unfortunately-this-bundle-is-not-available-in-your-country-If-you-received-this-message-in-error-please-contact-support)  Their FAQ says to contact support and provide the following: receipt of purchase, master key and country of residence.  I did this and I am now waiting on their feedback. When you submit the ticket, you will receive an email confirmation which says you will receive a response in 3-4 business days. Fingers crossed they apply the offer to my account.",Intel,2025-09-13 18:16:58,1
Intel,ne3dm09,They perform a hardware check with a utility before allowing you to access the key in your account. There's an area for support so that you can send them information with your purchase instead.,Intel,2025-09-14 01:26:54,1
Intel,nco8pf1,Is it fixed now?,Intel,2025-09-06 03:11:25,1
Intel,n9i7nod,A megacorp that has almost 70% of CPU market share is so desperate that they're giving away a game with a purchase of one their CPU's. You're a genius.   I got Dying Light 2 with my old 12th gen.,Intel,2025-08-19 10:35:03,25
Intel,n9iqaui,"I remember getting FarCry 6 with my Ryzen purchase a few years ago. Right now looking at a local pc shop website, you can get Borderlands 4 with a 50 series gpu purchase.  I guess by your logic, both AMD are desperate for CPU sales and Nvidia is desperate for GPU sales ?",Intel,2025-08-19 12:44:55,4
Intel,n9lx4s5,If only there was something called a return period....,Intel,2025-08-19 22:04:36,7
Intel,n9iz1ed,How generous. But no thanks. 😅,Intel,2025-08-19 13:33:51,7
Intel,n9juy35,"I'll up it to a 1070, or my current 3070.",Intel,2025-08-19 16:09:12,1
Intel,n9qirpi,So unlucky lol,Intel,2025-08-20 16:23:13,2
Intel,ngbtfcm,"I figured it out. You need to find the page on intel where this deal is going on and make an account. It will ask you to input the code after a series of options, and after that it will tell you that the codes to download, a completely separate code, will be released on October 3.",Intel,2025-09-26 16:09:21,1
Intel,ngbq7e6,"I have code but no idea how to redeem, amazon sent me it.",Intel,2025-09-26 15:53:09,1
Intel,nek1uv7,"Thank you! I did so a couple of days ago, and was already requested to provide that documentation. I was then replied back stating that the error has been fixed, and I should be able to claim the offer now. Sadly, I'm not in my country right now, so cannot test until next week.",Intel,2025-09-16 17:12:25,1
Intel,ngbqcoi,I have the code where do I enter it to receive my game?,Intel,2025-09-26 15:53:54,1
Intel,nco8ztc,Yup,Intel,2025-09-06 03:13:17,1
Intel,n9ibsnz,dinner languid pet enjoy yoke bike intelligent amusing sip books   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Intel,2025-08-19 11:08:11,22
Intel,n9i9kig,"I got ""Dying Light: The Beast"" and ""Civilization VII"" as a gift from Intel with my Core Ultra 7 CPU.",Intel,2025-08-19 10:50:59,5
Intel,n9m4i3c,Let's not pretend Intel isn't in desperation mode right now. The company is in the worst shape they've been in 30 years.,Intel,2025-08-19 22:44:58,4
Intel,n9kgfth,"Intel: here kid, have a balloon   Kid: what's in it for *me?*",Intel,2025-08-19 17:49:33,3
Intel,n9jpkiv,"> A megacorp that has almost 70% of CPU market share  Since you mentioned your 12th gen, on steam it's under 60% intel now, an 11% reduction in intel cpus in just 1 year. On amazon, there's [0 intel cpus in the top 10 best sellers.](https://i.imgur.com/LGwqOZs.png) Anecdotally the only high end cpus I see recommended in gaming discords is 7800x3d/9800x3d.  Obviously intel still has the lion's share in oem/laptop/server but that's been falling rapidly too, despite intel illegally bribing oems to not use amd products. Apple ending their partnership+their own laptop chips have been chipping away at intel's share too.  One company's been rolling losses year after year with falling market share, one is the complete opposite, the writing was on the wall a long time ago.  It took amd the better part of a decade to claw back from bankruptcy, intel needs to stop their attitude of looking for short term gains if they want to even start a comeback.",Intel,2025-08-19 15:43:53,2
Intel,n9irl7k,"TBF, you shouldn't be looking at marketshare but percentage of new CPUs sold/ profit.  Marketshare just tells us intel was number 1 for a long time. Change in marketshare tells us they're losing ground fast.",Intel,2025-08-19 12:52:20,0
Intel,nalvlmx,Yep that’s exactly what I did since they wouldn’t honor the promotion even though it’s during the return period,Intel,2025-08-25 16:25:38,2
Intel,n9j6dj3,Well you said you would trade so…,Intel,2025-08-19 14:12:27,10
Intel,n9ijalk,AMD literally set up a store at one point to give away games and GPUs with the purchase of a bulldozer.,Intel,2025-08-19 12:01:34,4
Intel,n9jqz06,"What are you talking about. AMD has been on a brink of going extinct since the 90's with the exception of AMD64 (Clawhammer etc) days. 2005-2008 or thereabouts.  Second, every heard of anti-monopoly laws? It would be cheaper for AMD to pay from their own pocket to keep Intel afloat rather than let them go bankrupt. Why do you think AMD is still around after all these years. Intel will never fail.",Intel,2025-08-19 15:50:22,-3
Intel,n9jwxfq,"AMD's zen architecture was the company's final hail mary which is their last decade, ryzen being a literal play on words for risen (from the dead). Their own engineers admitted if it failed the company was going to go under.  >Second, every heard of anti-monopoly laws?  Same laws that intel has been consistently losing for decades?  >cheaper for AMD to pay from their own pocket to keep Intel afloat rather than let them go bankrupt.  That's not how anti trust/monopoly laws works. Company A going bankrupt because they kept shooting themselves in the foot doesn't mean that Company B automatically become subject to anti trust lawsuits. Nvidia isn't exactly paying AMD even when they hold 90-95% of the market. Legislative scrutiny tends to come from manipulative practices, not from just selling a better product.  Since you might bring it up, Microsoft propped up Apple because they wore forcing their own software through Windows, not because they were just simply selling Windows. In a twisted turn of fate, now Apple is going through anti trust lawsuits/legislation around the world despite holding a minority share of operating systems worldwide.",Intel,2025-08-19 16:18:36,5
Intel,n85cm1l,That is surprisingly affordable for how much vram it has. I expected close to 3.5K.,Intel,2025-08-11 17:51:17,15
Intel,n8567ex,"Yep, I totally need this for plex",Intel,2025-08-11 17:20:56,10
Intel,n86ka6v,Perfect for AI inference market Lip Bu Tan is targeting,Intel,2025-08-11 21:29:21,5
Intel,n890k9x,Would this be a competitor to the Framework Desktop?,Intel,2025-08-12 07:19:08,2
Intel,n8at2ae,You can now buy CORE 1 series cpus on desktop motherboards from ali.,Intel,2025-08-12 15:05:20,1
Intel,n8ikpoc,How did I not see this? Has anyone tested the B60? There seems to be extremely limited supply.  Maxsun does not have good support in the US?,Intel,2025-08-13 18:49:15,1
Intel,n886cmq,48GB Vram,Intel,2025-08-12 03:10:43,3
Intel,n8q7z45,It's just two B60 Pros. They're supposedly priced at $500 each if they'll ever actually be available for purchase.,Intel,2025-08-14 21:52:20,1
AMD,nh35f7z,I could not figure out if all ASRock Motherboards are affected or only certain ones?,hardware,2025-09-30 22:19:06,18
AMD,nh2esb4,"I was always surprised the media didn't spend more time discussing some of the crazy launch era voltage defaults all the vendors were using. Granted with Gigabyte frying chips and ASUS frying chips plus their own motherboards honorably committing seppuku for the chipacide afterwards, there was a lot going on.  I built a system a month after AM5 launched with a 7700X and ASRock B650E Riptide Wifi motherboard. Out of the gate VSOC defaulted to 1.25v but was reported at 1.288v by ZenTimings 1.29. VDD Misc was 1.3v, and CLDO was 1.10. Currently VDD Misc defaults to 1.1v, CLDO defaults to 0.95v even when setting 6000 1:1, and now VSOC shows as red if set above even 1.23v. VSOC also now actually delivers the voltage it's set to without running above it, so either ZenTimings changed how it measures or ASRock changed its LLC setting for the VSOC rail.  Will have to finish the video later but I do wonder if Steve is factoring in the ever-mercurial voltage defaults all the vendors were using. I know ASRock personally changed and tweaked every single voltage knob & paired LLC knob that existed and was constantly changing them for the first year, and still tweaking them by year two to try and lock down the memory headaches users were having. But some of those default voltages were still nuts even when the X3D chips first launched. So if users did update UEFI versions religiously I could easily see any initial damage caused before the UEFI was updated weakening the chip, thereby triggering a belated failure later despite now running on safer voltages.",hardware,2025-09-30 20:05:58,35
AMD,nh2m5c2,Every board should follow manufacturers recommendations without exception.  I'm wondering how this could even happen unless users purposely overclock a board?,hardware,2025-09-30 20:40:55,21
AMD,nh3dvv6,[This has happened before!](https://www.youtube.com/watch?v=ssL1DA_K0sI).  This is an extreme problem.  Too extreme,hardware,2025-09-30 23:07:41,9
AMD,nh2hmmg,"Could this be caused by manufacturing tolerances in the physical layout of the pins and pads? Even a slight misalignment or height difference? For example, even if the board they tested already killed a CPU, maybe their new CPU has slightly tighter tolerances where it makes good contact with the pins on the board. As in, there is a minimum range of tolerances on both the CPU and the ASROCK board...and both need to hit that threshold before an issue arises. And maybe ASROCK board has looser tolerances than the other manufacturers leading to more failures on their boards.",hardware,2025-09-30 20:19:35,7
AMD,nh4ptgq,There you go Happy now?   - to those who were crying for gn's say in asrock motherboard situation and spouting negativity everytime a GN related something get post here,hardware,2025-10-01 04:03:29,2
AMD,nh3w1ey,This was happening on boards other than ASRock though.,hardware,2025-10-01 00:54:02,2
AMD,nh3q3ss,so what are people going to complain now about GN coverage of the problem?,hardware,2025-10-01 00:19:07,-5
AMD,nh3izh9,"it roughly seems like any AM5 asrock board + 9000-series cpu, though we don't have too clear of a picture of the specific mobo batches involved. asrock overall seems to reuse bioses quite extensively, so i assume the boards are quite similar.  some collected statistics available in the megathread first post: https://old.reddit.com/r/ASRock/comments/1mvgndh/9000series_cpu_failuresdeaths_megathread_2/",hardware,2025-09-30 23:37:22,19
AMD,nh3frnm,"Difficult to say. They have a known motherboard that's killed a CPU before and they can't get it to reproduce the issue. So you could have a motherboard with the problem but the issue may not occur.  I hear people saying that it might be happening around the memory controller, so it could be only an issue when paired with certain memory  or something. I'm just speculating, we don't actually know what the issue is exactly.",hardware,2025-09-30 23:18:29,8
AMD,nh3i33f,"these specific issues started after the 9000-series launch, and only clearly affect 9000-series cpus. so the old 7000-series bios voltage adventures shouldn't matter here (9000 requires a newer bios).",hardware,2025-09-30 23:32:05,7
AMD,nh4idph,Tech tubers do spec sheet rundowns instead of deep motherboard analysis until someone has an issue and then they milk the controversy for views.,hardware,2025-10-01 03:10:57,3
AMD,nh39m6c,"board vendors need to crank all the settings up so their board looks better in all the reviews and benchmarks. if they lock things down, then the board vendors complain about manufacturers taking control away (ie nvidia forcing partners to design their boards a certain way, locking down power draw through firmware, etc)",hardware,2025-09-30 22:43:09,11
AMD,nh2zvcr,"> unless users purposely overclock a board  1. Enable XMP or EXPO because Intel and AMD are encouraging it, and many CPU reviews use XMP/EXPO.  2. Motherboard jacks voltages to insane levels to guarantee stability.  When I enabled XMP for my CPU, the motherboard picked some strange voltage levels while on auto settings. I had to manually tune the RAM and voltages to bring the idle power usage from +15W to about 6W.",hardware,2025-09-30 21:48:46,16
AMD,nh3e3ja,There are like two or three manufacturers for the socket mechanism and motherboards from other manufacturers also use the ones that AsRock uses,hardware,2025-09-30 23:08:54,6
AMD,nh4axfo,"yeah but the thing is problems can happen to degrees, and Asrock has this problem the worse going by the nonstop conga line of blown cpu post on their subreddit and forums.",hardware,2025-10-01 02:22:30,9
AMD,nh4ik41,Reddit would complain if you handed them a million dollars.,hardware,2025-10-01 03:12:11,3
AMD,nh3yntl,Overt sensationalism? ‘Murderboards’ is quite something for the title of an investigation,hardware,2025-10-01 01:09:21,-2
AMD,nh45cz5,Tech Judas isn't above criticism when it comes about,hardware,2025-10-01 01:48:58,-4
AMD,nh3lluo,thank you for the link to the megathread. good insights there. my motherboard seems to be one of the high risk ones unfortunately. i think i will wait for the issue to be resolved before upgrading my cpu to a x3d one. thanks for commenting!,hardware,2025-09-30 23:52:28,2
AMD,nh3rnre,"People were saying it also affected the 7800X3D when this mess all began... but maybe they were wrong I honestly haven't looked deeply into this to know. That being said I do know some of the voltages were remained high even after the 9000-series launched. Was literally only few a months ago when ASRock changed the UEFI to redline a VSOC setting above 1.23v, before then it was happy to set 1.25v with EXPO. Was only after trouble began this year did ASRock finally start getting conservative with its voltage settings, but the 9800X3D debuted last Nov.  Did finish the video. When Steve mentioned most of the data points were people using 1.4v kits of memory that itself seems like a flag given the warnings I've read about exceeding 1.35v, but it's also the first time I've heard it mentioned as a possible factor. Back from the 7800X3D era I remember a Buildzoid video that mentions one of the three RAM voltages that gets changed by EXPO presets will affect power plane levels within part of the IO die, and there's one voltage plane that shouldn't get too high relative to another lest it cause problems. So having that set at 1.4 seems kind of bad for multiple reasons.",hardware,2025-10-01 00:28:18,1
AMD,nh34miv,True! Just had constant crashes with EXPO enabled at 6000 and 1.4v went to manually tighter timings and 1.35 and it flies xD,hardware,2025-09-30 22:14:39,1
AMD,nh4cd4s,"Google ""sarcasm"". It's kinda GN's comedic style.",hardware,2025-10-01 02:31:33,0
AMD,nh40fe6,If you look at the ASRock mega thread there are still people reporting 7800x3d deaths even on the latest bios.,hardware,2025-10-01 01:19:31,6
AMD,nh40ovq,"the way the voltage rules work is that you want VDDIO to be relatively high compared to both vsoc and vddp, though i don't know if the latter is about other than vddp being derived from vddio.  vsoc < vddio + 100mV  vddp < vddio - 100mV",hardware,2025-10-01 01:21:05,2
AMD,nfn0zw2,"It's kind of annoying that the accepted method is to do benchmarking with smaller models (3B, 8B, etc) and lesser contexts.    It allows things like slow prompt processing (ex the Achilles heel of Macs) to go unremarked since they're not noticeable at smaller sizes.   Especially on something like a Strix Halo where you're probably grabbing the 128 GB chip because you want to run a 70B Q8 with plenty of context.",hardware,2025-09-22 18:03:14,36
AMD,nfmrzmn,"Nice to see it works straight of the box but rather underwhelming. Saw this post '[ROCm 7.0 RC1 More than doubles performance of LLama.cpp](https://www.reddit.com/r/LocalLLaMA/comments/1ngtcbo/rocm_70_rc1_more_than_doubles_performance_of/)' over at r/LocalLLaMA and thought perhaps PP had an edge while Vulkan had TG, though that was on RDNA4, 9070 XT (on a small model). Doesn't seem the case here.   What I find with benchmarking LLMs especially across hardware is the amount of different env and flags needed to be set to find that 'perfect' setup. I usually look over at   [https://github.com/lhl/strix-halo-testing/tree/main/llm-bench](https://github.com/lhl/strix-halo-testing/tree/main/llm-bench)  To find such cases but it's hasn't been updated for ROCm 7. Not only that comparing across HW is usually tough and you really go by it through other users. TG isn't that difficult to guestimate as it's bandwidth bound but finding benchmarks like with gaming outlets is tough. It's cool to see Phoronix continuing with LLM benchmarks and I'd like to see more HW being tested",hardware,2025-09-22 17:21:06,18
AMD,nfnp6pi,From the article it seems like Vulkan is still much better than ROCM.,hardware,2025-09-22 20:10:17,7
AMD,nfnr4ec,"ROCm never fails to disappoint, but it's sadly the only option if you want to do anything more than just running inference on AMD GPUs...  Part of it is just the abysmally bad support for consumer SKUs, but this one in specific is literally marketed as a ML chip...",hardware,2025-09-22 20:20:36,6
AMD,nfqv3r3,"A bit surprising that for interference, there is such a huge difference between GPU and CPU, I would have expected then both to be memory bandwidth bound, even on the higher bandwidth compared to a normal dual channel system.",hardware,2025-09-23 10:59:52,1
AMD,nfmmclp,"Do the Turbo Nerds care about ROCm 7.0 ? Shamelessly asking so I may take confident, aggressive posts integrated into my belief system.",hardware,2025-09-22 16:54:40,-18
AMD,nfn520n,"Yeah, I find PP to be generally lower and the fact that most people that share their benchmarks are doing so at lower context. That said, like I wrote on my own comment, it's difficult to generally find someone or an outlet benchmark and compare across HW. Then you'll get people who champion M4 Max or Ultra for their bandwidth while TG or compute is bottlenecked with longer context or the large model that their fitting in *unified memory*. While I've generally seen good PP on Halo, the lack of cross testing doesn't leave me confident on such conclusion.",hardware,2025-09-22 18:22:45,10
AMD,ng5jbun,"Reviewers have literally no clue about anything AI at the moment, I seen one install NAS software on one of these lol.",hardware,2025-09-25 16:34:43,2
AMD,nfpae7w,amd rocm release note doesnt include ryzen 395 as supported hardware,hardware,2025-09-23 02:08:21,3
AMD,nfe9f0j,">In real-world gaming, the card only managed to gain roughly 8-12% uplift over the stock SKU  well thats a bit less than 25%  >The entire cause of this FPS increase is the enhanced power limit, which increases the non-XT 220 W board power to as much as 300 W, representing a roughly 36% power increase on its own.  very inefficient gains",hardware,2025-09-21 10:26:06,353
AMD,nfepjhd,[Obligatory link to original reddit post](https://www.reddit.com/r/radeon/comments/1nlpqx3/flashed_the_reaper_9070_into_a_fake_xt_boosted_my/?share_id=Cf60552_29-W9lDf8MHMb),hardware,2025-09-21 12:34:44,38
AMD,nfesqie,Source coming from a reddit post (that has a link to the BIOS):  [https://www.reddit.com/r/radeon/comments/1nlpqx3/flashed\_the\_reaper\_9070\_into\_a\_fake\_xt\_boosted\_my/](https://www.reddit.com/r/radeon/comments/1nlpqx3/flashed_the_reaper_9070_into_a_fake_xt_boosted_my/)  Keep in mind in terms of power scaling the 9070 is more or less perfect at \~220W while the 9070 XT at \~300W. I think this BIOS is useful for getting the base 9070 near \~300W at max as Adrenalin at most gives you +10% PL so if you have a 9070 with a stock BIOS TBP at \~220W you can only go at most \~240W.,hardware,2025-09-21 12:54:57,15
AMD,nffx9o2,"Looked up prices for the powercolor reaper 9070 and 9070xt. On newegg its like a $30 difference. Just get the 9070xt. Bios flashing gpus is a nightmare and not worth it unless you know exactly what you're doing. I tried flashing a r9 290 back in the day and bricked it, even with a dual bios because im not smart.",hardware,2025-09-21 16:25:41,22
AMD,nfe9yv2,"It's been 20 years but this is the most ATI story I can imagine, I seem to remember there just being an X700 card you could flash up to X800",hardware,2025-09-21 10:31:22,44
AMD,nfg4s9k,Some things never change,hardware,2025-09-21 17:01:55,2
AMD,nfgrtr2,didn't people do this before and it ended up in some cases nuking there cards prematurely.,hardware,2025-09-21 18:44:06,2
AMD,nfeof1l,"Has anyone tried this with a 3-plug model, like the Sapphire Nitro+ 9070?  I recall comments speculating that a 3-plug model flashed to its equivalent XT would have a power limit of 350-375w, based on the stock XT power limit. I'm not sure if that is/is not including the +15% power limit available via Adrenalin.",hardware,2025-09-21 12:27:02,2
AMD,nfft338,Not like the old days where we could unlock cores and pipelines. :(  Just power limit increases.,hardware,2025-09-21 16:05:34,1
AMD,nfeqbjg,Dumb question but has anyone done the reverse?  Edit: nvm i guess its easier to undervolt LMAO,hardware,2025-09-21 12:39:52,1
AMD,nfew1xn,Im interested in getting the 9070 for a fair price only because it is among the most efficient cards. Its cool increased power limit enables slight overclocking but there is nothing new or interesting in this.,hardware,2025-09-21 13:14:55,1
AMD,nfeqobg,and 8-12% is exactly the average difference you see between the non-XT version and the XT,hardware,2025-09-21 12:42:06,108
AMD,nfgrbrp,Yeah lol why d you turn a 220 watt gpu into a 300 watt one for 8 percent performance. Thats the kind of stupidity intel do with their 13000 a 14000 series cpus.  Just no,hardware,2025-09-21 18:41:56,18
AMD,nff8gil,"You can run the 9070 up to 270w if you buy an OC variant.  So, that's a lot of work to gain 30w more.  You can easily get 15% performance gain without any bios modding.  The mod is cool for enthusiasts, but no one should care to do this.  You gain very little.  Edit:  You risk melting the 8 pin connectors if you push past 300w, since none of these cards come with 3 connections.",hardware,2025-09-21 14:23:12,34
AMD,nfjjpqu,"""up to"" includes 0",hardware,2025-09-22 03:51:15,3
AMD,nflcw7p,It's like 10 usd a year. For gaming I rather have 5 extra fos,hardware,2025-09-22 13:07:43,1
AMD,nfln3pj,They're always very inefficient gains. In today's news...,hardware,2025-09-22 14:03:41,1
AMD,nfegyi8,Assuming a 12% uniform uplift. That puts it like 8% behind a 5080 which goes up to 380watts. So they are the same.     You can power limit a 5080 to like 200watts.... Meh i give up explaining since i dont care. Essentially in about a third of modern titles it would match a stock 9070.    I have to wonder what if they had overclocked gddr7. Might match a stock 5080,hardware,2025-09-21 11:32:04,-16
AMD,nfg19cy,"Yeah it isn't really worth doing unless the price differential is massive. I bought mine the first day that they came out, but my local Microcenter ran out of the MSRP XTs, so that's why I opted for the MSRP 9070. Pricing hasn't come down on those since the initial launch, so I just ended up keeping mine.",hardware,2025-09-21 16:44:58,2
AMD,nfgf9hk,"Don't know status of amd cards nowadays but it's basically impossible to brick an nvidia card by flashing it. You can just put the original back on, though of course you need to use integrated graphics at that point.",hardware,2025-09-21 17:48:38,3
AMD,nfk6xka,For Europe the difference with tax is 150 USD. Honestly with the bios flash 9070 non XT has crazy value,hardware,2025-09-22 07:15:09,1
AMD,nfm5rcp,"I've flashed multiple AMD GPUs since the 2000s and never had an issue, including BIOS files I modified. It's really not hard if you can follow directions and use a command-line interface.  That being said for $30 yeah, buy the XT. That's a no brainer.",hardware,2025-09-22 15:35:22,1
AMD,nfegg9j,"Nah all it does is unlocking power limit. You could had do this with the 470/480 to a 580, vega 56 to 64, 5700 to xt.   This does not unlock more CUs, all it does is raise the artificially imposed power limit that AMD likes to put on their lower end models using the same die.   And gains are usually pretty decent, because AMD actually heavily power limits lower end models as a form of upselling their higher end cards using the same die, otherwise they are literally the same cards with a -5% CU and in some cases; cards with same CU but lower power limit.  If they added other form of segmentation like how nvidia did with the 5070ti and 5080 in the form of large cache cuts AMD wont need to power limit their non xt models heavily every generation.",hardware,2025-09-21 11:27:52,39
AMD,nferlod,"I could do that on the rx580 and my actual 6700xt: increase the power limit and get some more fps.  Also the RAM can get some +10% almost for free (power related, but not on all boards).",hardware,2025-09-21 12:47:56,4
AMD,nfemrf6,It’s par the course for AMD too. The old Phenom X2 cpus could often just unlock 3rd or 4th cores.,hardware,2025-09-21 12:15:35,8
AMD,nffeqav,"Same thing with NVidia Geforce TI 4200 & 4400. If you were lucky, you could get it flashed to a TI 4600.",hardware,2025-09-21 14:55:15,2
AMD,nfepjjn,I wonder what AMD and the GPU market would look like today if it had kept its R&D eggs in Radeon's basket instead of Ryzen's.,hardware,2025-09-21 12:34:45,3
AMD,nfeaua1,It's always AMD too 🤣 ATI/AMD same same,hardware,2025-09-21 10:39:41,3
AMD,nfezo05,"Right, I remember bios flashing my 9800 pro 256MB to and 9800 Pro XT and saving like 40% off the cost of an actual XT  Those days were great",hardware,2025-09-21 13:35:59,1
AMD,nff042o,I did this on my 9800pro like 20 years ago. It's nice to see somethings stay consistent.,hardware,2025-09-21 13:38:31,1
AMD,nfgb8oa,"9000 series BIOS shennanigans :P   They reacted by cutting the trace that made this possible, which was a 5 second fix with a graphite pen.",hardware,2025-09-21 17:31:15,1
AMD,nfgmn99,You could flash a radeon 6950 to a 6970. It would unlock the shaders.,hardware,2025-09-21 18:21:14,1
AMD,nfgycel,"I... technically do? I care about power efficiency a fair bit, so I undervolted my 9070xt and also dropped the power by 25%. Performance reduction of about 3% relative to stock in Steel Nomad (barely worth remarking on) and it runs way cooler, way quieter and sips like 230W instead of 340.",hardware,2025-09-21 19:13:05,4
AMD,nfersas,"I would guess that everybody should do that, undervolt and light overclock, full overclock on RAM.",hardware,2025-09-21 12:49:06,2
AMD,nfg6smm,Unlike the 12v High Power connector the PCIe 6+2 has a more adequate safety factor. The similar EPS 12v can safely handle 300w with its four 12v pins. The 6+2 with three identical 12v pins could handle 225w without issue.  The bigger concern would the cards VRM. Manufacturers will often remove power stages for lower power cards to reduce costs. Since the VBIOS contains the information about the cards power delivery altering it can make the card push itself beyond what it can handle.,hardware,2025-09-21 17:11:19,15
AMD,nfgqi12,"> Edit: You risk melting the 8 pin connectors if you push past 300w, since none of these cards come with 3 connections.  You are still running the 8 pin with more safety margin on a flashed 9070 than the power connector on a 5090 at stock. At least as long as you don't have some bargain basement PSU.",hardware,2025-09-21 18:38:17,6
AMD,nffkle1,The PCIe slot can provide up to 75W (in practice it can be even more than that).,hardware,2025-09-21 15:24:19,14
AMD,nffkhbk,"Wrong, the 3080 TUF OC came with 2 8 pin connectors and was pulling 340W. 150w per connector + 75w available from the PCIe slot. So 375W max. available with dual pin.",hardware,2025-09-21 15:23:45,6
AMD,nff9urg,"A standard 8-pin pcie connector is rated for 150w, so 300w with 2 connections is okie dokie.       I had my 2080 TI even run off single cable on the psu side for years - 280w.  There's plenty safety margin for 8-pin unlike 12vhfr.",hardware,2025-09-21 14:30:27,9
AMD,nfhiaio,The Nitro model has 3 connectors.,hardware,2025-09-21 20:42:54,1
AMD,nfjybr6,"The 295X2 draws way more power than that over 2 8 pin connectors. It's fine.  The official limit is 375W including the 75W from the PCIe slot, unofficially there's a tonne of headroom in the connectors to go even higher.",hardware,2025-09-22 05:53:18,1
AMD,nfemz4b,"a 12% uplift brings it basically to a stock 9070XT, which itself is 20% behind the 5080 (TPU GPU Database)",hardware,2025-09-21 12:17:04,28
AMD,nfezf87,Lmao Reddit math,hardware,2025-09-21 13:34:37,11
AMD,nfeow4l,It could never match a 5080. Where do you get this information from?,hardware,2025-09-21 12:30:19,11
AMD,nfeyy0k,Source? The RTX 5080 and even the RTX 5070 Ti absolutely destroy that card. They're not even in the same arena.,hardware,2025-09-21 13:31:55,2
AMD,nfes6pm,"What? 12% uplift won't even allow it to reach 5070Ti performance, let alone be within 8% from 5080. It will be like healthy 20+% slower still.",hardware,2025-09-21 12:51:36,0
AMD,nfip00e,> you need to use integrated graphics at that point.  I genuinely think a lot of people forget that basically every CPU made in the past decade or more on Intel and any Ryzen on AM5 has an iGPU.  So flashing a card is honestly not a big deal at all.  It's just that this specific bios flash gives a laughable amount of extra performance for the power increase.  This is as far from a Phenom X2 to X4 situation as it gets. Nothing is unlocked. The only thing changing is the power limit.,hardware,2025-09-22 00:38:34,0
AMD,nfejao8,"This time round its probably worth it more than ever, i had the XT bios on my RX 5700 Pulse but the cooling just was not sufficient it would throttle hard with the unlocked power limit. The coolers on the majority of RX 9700 cards are overbuilt and can handle the increased power draw with ease.",hardware,2025-09-21 11:50:30,12
AMD,nffurnk,Vega56 can unlock the gpu mem volt to 1.35v when using the 64 bios.,hardware,2025-09-21 16:13:42,4
AMD,nff46uk,Same with some X4 cpu's you could unlock 2 extra cores to get 6,hardware,2025-09-21 14:00:53,4
AMD,nffunja,Also the Tri-cores were all quad,hardware,2025-09-21 16:13:08,5
AMD,nff00dx,"RT and PT probably would’ve been standardized years ago if AMD was trying  But we’d have weak CPUs. Also AMD can do both, make good cpus & gfx if they wanted. They just don’t want to spend money on gfx so here we are",hardware,2025-09-21 13:37:57,-1
AMD,nfh6k0a,"Oh wow pretty aggressive power limit, how are the temps like in comparison?",hardware,2025-09-21 19:49:28,1
AMD,nfip8iv,How did you drop the power limit?  I got a tad annoyed by the built in Radeon tuner only allowing 10% lower PL. But everything else was fine so I never bothered with MSI Afterburner as I usually do when using Nvidia cards.,hardware,2025-09-22 00:40:01,1
AMD,nfi6xh4,"Unless you get unlucky at the silicon lottery. My 9600 XT isn’t stable if I undervolt it any real amount, and I’ve been undervolting AMD and NVIDIA cards for like 7 years.   Don’t assume you can always undervolt. Sometimes you’ll get silicon that just barely meets the specs.",hardware,2025-09-21 22:51:23,3
AMD,nfggucc,"It depends on how the power circuitry on the card is setup, many draw pretty much nothing through the slot.  A clever self-balancing ""draw exactly up to 75w from the slot and the rest from the cables"" power supply would be pretty complex and expensive for the majority of SKUs.",hardware,2025-09-21 17:55:36,13
AMD,nffnivs,"If I remember correctly, it gets 75W from the PCI-e slot so in theory it's safe. Anyway for me it's absolutely not worth it in any case",hardware,2025-09-21 15:38:51,3
AMD,nffb3wd,"I agree, my point is that you can't add much more than 30w with this mod.  So it's kind of pointless.  You can run past the 150w per cable mark, but you risk transient spikes melting the connector.",hardware,2025-09-21 14:36:57,-5
AMD,ng09bfq,The nitro has a single 12v 2x6 connector actually,hardware,2025-09-24 20:03:08,1
AMD,nff3ljy,https://youtu.be/xSbPHUrOg44 @testing games.   In about a third of those the 9070xt is 10% behind. So if you power limit the 5080 to 200 watts or 50%. It should drop to it or 9070 levels.,hardware,2025-09-21 13:57:42,-3
AMD,nff4r4o,The techpowerup benchmarks put its exactly 10% behind a 5070ti. A 12% uplift would put in 2% faster than it.   I am talking about a power limited nvidia card. As the op focused on the 9070 being limited to 220 watts. The downvoters just can't read.,hardware,2025-09-21 14:03:54,-1
AMD,nfl0i30,">       >  >  >  > I genuinely think a lot of people forget that basically every CPU made in the past decade or more on Intel and any Ryzen on AM5 has an iGPU.  Except both companies still make cpus that do not come with iGPUs. Basically both use F as an indicator on the CPU's model number. AMD just launched the 9700F, Intel has an Ultra 265KF, which I've seen in a few bundles. And yeah I helped a friend build with a 265KF recently, who didn't realize he got the KF model, and needed igpu  Also technically, Xeon and threadripper stuff.",hardware,2025-09-22 11:49:12,3
AMD,nfktaid,Quite recently most Zen CPUs did not come with an iGPU.,hardware,2025-09-22 10:54:53,3
AMD,nfjpwdw,"It's free performance with not much effort at all. If you're not concerned about power draw, what's not to like.",hardware,2025-09-22 04:39:40,1
AMD,nfg56js,"Yeah it basically just switch all voltage and power logic to use the higher end's version.   Some of these cards are literally voltage and power limited thats all. Most insane in recent memory in terms of limitation is 7900gre, as it's memory voltage is for some reason tied to core voltage. Unfortunately theres no bios to flash it to.",hardware,2025-09-21 17:03:49,2
AMD,nfhuzb7,"Theyre like... 50s and 60s, maybe? I don't really keep a close eye on them. They're low, point is. I barely hear the fans unless I turn on path tracing or something.",hardware,2025-09-21 21:45:36,2
AMD,nfkml1y,"I just did it in Adrenaline. It allows up to a -30% power limit. Not sure why its only allowing you 10.   Although its possible youre cranking the power by accident, because 10% is the amount it lets you *raise* the power limit by, I'm pretty sure.",hardware,2025-09-22 09:55:57,1
AMD,nfhhxou,My 5070 Ti reports drawing single digit from the slow as an example.,hardware,2025-09-21 20:41:15,1
AMD,nfgeix2,There's always safety margin and the rating is for constant draw.,hardware,2025-09-21 17:45:25,2
AMD,nffsdrd,The 9070 XT can pull above 300W stock and significantly above 300W with overclocking despite using the same amount of 8 pin power connectors. The PCIe slot provides up to 75W of power in addition so up to 375W continuous power draw is within spec.  I'd be much more worried about cards using the 12VHPWR connector as these actually have melted even in stock conditions without exceeding the 600W rating.  Nothing's gonna melt on a 9070 except the silicon itself. Or maybe the fans achieve liftoff. One of these will come before the connector has a chance to melt.,hardware,2025-09-21 16:02:10,7
AMD,nfz3wrd,"I went from 270w on the stock bios to pulling 348w with the XT bios, so nearly 80w more power and now the clocks are consistently over 3300 while gaming. The only downside has been the slightly increased hotspot temps, but they're still in the low 80's. I have a Sapphire Pure 9070 which uses the same cooler as the XT variant. There are several XT's that only have 2x 8-pin connectors and there is no problems with power delivery.",hardware,2025-09-24 16:43:28,1
AMD,nffc5k3,"Free perf is not pointless though, \~$150 saved for near same performance. even more of a non-issue if done on dual bios cards. lastly, spikes won't melt it, sustained unbalanced load does.",hardware,2025-09-21 14:42:17,0
AMD,ng0im28,"You're right, but it includes a 3x8 adapter",hardware,2025-09-24 20:48:03,1
AMD,nffaaa2,[https://www.techpowerup.com/gpu-specs/radeon-rx-9070.c4250](https://www.techpowerup.com/gpu-specs/radeon-rx-9070.c4250)  Techpowerup puts 5070Ti 17% above 9070 and 5080 34% above 9070.  Even if you try to reverse start point for your favor - 9070 is will be like 14.5% slower than 5070Ti.  No idea what you are on about.,hardware,2025-09-21 14:32:41,0
AMD,nfnlpod,"Absolutely, that's why I said ""basically"" as those are the majority of CPUs in people's rigs.  Point is that a lot more users can do something which used to be a rather risky undertaking 10 years ago and do it very safely nowadays.  And if you can't, no big deal. Not like the 10-15% performance boost with a much, much higher power usage is something the majority of gamers would want anyway.  Win-win for everyone.",hardware,2025-09-22 19:51:54,1
AMD,nflkfa8,> basically ... any Ryzen on **AM5** has an iGPU.  I think that was pretty clear eh.,hardware,2025-09-22 13:49:30,-1
AMD,nflkpos,"Interesting, maybe it's a model thing for the 7900 XTX Pulse.  [There's definitely a big ass minus](https://i.imgur.com/tloZlHA.png), but whatever not a big deal, I mostly just cut clocks at a certain point where performance is fine and that's about it.  Edit: Yep seems to be a driver thing. Even MoreClockTools reports the maximum lower bound of the power limit at 90%. How odd.",hardware,2025-09-22 13:51:02,1
AMD,nfzemxd,"Yeah, 75w+150w+150w seems to be more than enough.  So it's a little better than I was thinking.  Pretty cool.  This might make the 9070 the best card of this generation.",hardware,2025-09-24 17:34:23,1
AMD,nffek7e,Wtf. Either way a 12% overclock as per the post we are all responding to. That would make it 4% worse. Not 14.5%,hardware,2025-09-21 14:54:24,-1
AMD,nfpne1n,"> as those are the majority of CPUs in people's rigs.  I think with Intel and gaming rigs, getting F chips aren't uncommon. Like looking at microcenter, it's a 20$ different between the 265K and 265KF bundles. It's pretty easy to say that's 20$ i can spend on something else.  Even more so with gaming prebuilts, with OEMs always cheaping out. Looking real quick at [best buy's Intel gaming prebuilt PCs](https://www.bestbuy.com/site/searchpage.jsp?browsedCategory=pcmcat287600050002&id=pcat17071&qp=processorbrand_facet%3DProcessor+Brand%7EIntel&st=categoryid%24pcmcat287600050002), basically 80% of the top sorted by best selling all skip the iGPUs.  Even my go-to OEM, PowerSpec by Microcenter, will do a [265KF in their 2000$ gaming PC](https://www.microcenter.com/product/689962/powerspec-g455-gaming-pc).  Definitely less common with AMD chips, but that's more because AMD don't have as many F SKUs.",hardware,2025-09-23 03:40:05,1
AMD,nfn6ish,"One of my computers has a 7500f. AM5, no iGPU",hardware,2025-09-22 18:29:54,1
AMD,nfltijn,">basically every CPU made in the past decade or more  No, that wasnt very clear.",hardware,2025-09-22 14:35:56,1
AMD,nffn791,"It is, as for now, 9070 is 14,5% worse than 5070Ti. 5070Ti, in itself, is \~14% worse than 5080. Overclocking 9070 by 12% will not in any form or function put it within 8% of 5080.  It's not that complicated.",hardware,2025-09-21 15:37:17,1
AMD,nfnlg2v,"Nicely done quoting out of context.  It literally said ""past decade or more **on Intel**"".  Do you just want to be a contrarian and ""akshually some CPUs don't have iGPUs!111""-kinda guy?  Whatever floats your boat, I'm not judging. It just doesn't really do anything for the discussion.  Point still stands: Flashing a GPU bios is a lot easier and less risky nowadays than it was 10 years ago.",hardware,2025-09-22 19:50:28,1
AMD,nfptfvk,Yeah and the card isn't exactly bricked if it can be reflashed - even if the user doesn't have an igpu. Just take it to someone else lol,hardware,2025-09-23 04:31:27,1
AMD,ngksolh,"It says ""future GPU architectures will support DGF"" but I'm fairly certain RDNA4 does as well, correct?",hardware,2025-09-28 01:25:09,15
AMD,ngmkc46,"**TL;DR:**   AMD has announced their DGF format is compatible with animated geometry, where it incurs an insignificant cost of less than 1% of overall frame time.  Dense Geometry Format is used to lower BVH side RT ms overhead.  AMD confirmed HW based DGF decompression for RDNA 5 while current AMD GPUs rely on shaders.",hardware,2025-09-28 10:14:18,17
AMD,ngmjll4,"Yes all cards technically support it but they mean support for HW based decompression. From earlier post from February: *""Dense Geometry Format (DGF) is a block-based geometry compression technology developed by AMD, which will be directly supported by future GPU architectures""*  If I were to guess RDNA 5 based cards and the next gen consoles will have a decompression engine inside each ray accelerator similar to how NVIDIA added a DMM accelerator with 40 series.   This isn't just some baseless speculation there's actually a patent for this in case someone is interested: [https://patents.google.com/patent/US20250131640A1/en](https://patents.google.com/patent/US20250131640A1/en)  This quote is interesting as well:   *""Quantization took less than 1% of the overall frame time, which means this process will not majorly affect rendering times in an animation pipeline. Likewise, animating the position data (Animation) has an almost insignificant contribution to the frametime. BVH Build and Ray Trace dominate the image computation.""*  **TL;DR:** Animating geometry has an insignificant impact on the ray tracing ms cost. ~~IIRC rn animated geometry is usually not implemented in RT games due to BVH overhead concerns.~~ *It's about rebuilds and inefficient BVH management rn not animated geometry overhead. PTLAS to the rescue!*",hardware,2025-09-28 10:06:54,5
AMD,ngoqy41,"As I understand it DGF is a technique for compressing geometry to reduce memory usage and at least in the first paper, reduces performance when tracing. The memory reduction is like a factor of 6x but tracing can be slowed by like 2x. This site is showing that you can slot animation into DGF cheaply (i.e. change the vertex positions and rebuild the blocks). In reality the cost of animating geometry with RT had little to do with the cost of transforming the vertices, GPUs are very good at that.  Touching any part of geometry means you need to rebuild the BVH or you'll be missing movement in the ray traced representation. DGF doesn't address this (its implementation isn't strictly connected to BVHs, although the meshlet blocks can be used as leaves in the structure). So it is expected that BVHs and ray tracing would remain the expensive part since the same stuff happens with or without DGF. Like you stated, the cost of this process is why it's not usually implemented in RT games - the less geometry you change, the more you can delay rebuilding or do partial updates instead. This article is just showing that DGF holds for dense animating geometry too",hardware,2025-09-28 17:53:13,6
AMD,ngp970p,"Thanks for providing additional context from earlier blogpost and papers. Ms overhead is an issue for sure which is why AMD is opting for HW accel in RDNA 5.  One thing for certain is that AMD NEEDS their own RTX Mega Geometry competitor. Especially PTLAS otherwise like you said if they animate just one asset then nonstop BVH rebuilds.   Intel already unveiled Micro-mesh CBLAS in a paper over 2 years ago, and during Summer they unveiled PTLAS support. Meanwhile RTX Mega Geometry implemented in UE5, proprietary engines etc.... and as usual where's AMD. Maybe when DXR 1.3 arrives AMD will bother to do a proper implementation.",hardware,2025-09-28 19:19:39,1
AMD,ngphnue,"Absolutely. DGF with HW acceleration could be great if it could make decompression free, then they could reap memory benefits (if it was adopted, it requires baking to use). RTX Mega Geometry existing kills off any excitement for DGF for me, DGF seems like AMDs answer to DMMs which were lower quality but 3x better at compressing and faster to decompress. Meanwhile DMM acceleration has been killed off from 50 series in favor of Mega Geometry which handles every case DGF wants to: granular BVH, clusters, partial rebuilds, memory reduction. Which also works on earlier series...  Nanite seems to have proven to everyone clusters are the next step in LOD management. Intel Micro mesh, NVIDIA CLAS. I was unaware of PTLAS (thank you for inspiring a deep dive!) but you are right, Intel and NVIDIA again. Shocking AMD do not have any response to either feature (yet??). I guess Project Redstone is probably their focus right now? They absolutely need a response to Mega Geometry!  Edit: I suppose if they can get HW accel building to be fast enough, DGF leaf node BVH could achieve some of the same benefits since its effectively a cluster BVH (which AMD tested by using primitives, maybe their next target to implement in hardware?). I'm not entirely convinced where DGF is going without more insight into the hardware/software limitations",hardware,2025-09-28 19:59:55,3
AMD,ngq17lp,"As usual NVIDIA keeps moving the goalpost and AMD responding to prev gen one (DMM) gen too late (RTX MG).   Like you said Mesh shading and continuous LOD isn't going anywhere. So it seems. Catching up to CUDA, DLSS and porting FSR4 to PS5 Pro prob takes all their SW side ressources beyond graphics R&D :( You're welcome.    Well look at their pathetic responses to DXR 1.2 and the recent Advanced Shader delivery on the DirectX blog. AMD really needs to up their SW and HW game and I doubt we'll hear a single word on CBLAS + PTLAS SDK from AMD until RDNA 5 gets launched, but hope I'm wrong.   The Vulkan Github documentation for MG is a treasure trove for anyone interested. Look to the left section for documents ending with .md, truly great stuff! [https://github.com/nvpro-samples/vk\_lod\_clusters/blob/main/docs/blas\_sharing.md](https://github.com/nvpro-samples/vk_lod_clusters/blob/main/docs/blas_sharing.md)  And it's not like they don't have the talent to push things hard, Holger Gruens and Carsten Benthin former Intel, Matthäus Chajdas and many others. There's just seemingly a lack of will at AMD to really push things except for their GPU workgraphs push which does deserve huge applause.  We'll see, but that would be the next logical step similar to what NVIDIA does in 50 series (new ray/tri engine). Yeah more info needed to be disclosed by AMD but reading the Github documentation for MG this isn't close to being enough. AMD really needs to plan based on DGF not existing, because there's no guarantees devs will even bother to use it.    Still Dense geo format does have interesting use cases beyond BVH management, but that's speculative patent based derived analysis (Look for the KeplerL2 patents shared in the NeoGAF forums a while back: [https://www.neogaf.com/threads/mlid-ps6-early-specs-leak-amd-rdna-5-lower-price-than-ps5-pro.1686842/page-12#post-270687172](https://www.neogaf.com/threads/mlid-ps6-early-specs-leak-amd-rdna-5-lower-price-than-ps5-pro.1686842/page-12#post-270687172)   Not confirmed in any way by AMD. But it looks ideal for a parallel wide INT-based prefiltering testing setup to cull triangles before expensive floating point tests but what do I know. Either way interesting stuff.",hardware,2025-09-28 21:35:48,3
AMD,ngqaubu,"Very interesting, AMD are taking advantage of DGF for rapid and wide culling to speed up intersection testing. This could indeed be their way of hardware accelerating cluster intersections, although I'm intrigued what the practical uplift this gives nor how they address building new clusters. I have no idea what NVIDIA did to achieve the same on prior gens.  I also had no idea NV MG BLAS info was posted. It's conceptually simple but it's a very smart intuition that since RT with a good accelerator is less tri constrained, you can just reuse high poly BLAS and forego swapping LODs. I'm guessing Ray Reconstruction is very useful here to cut back on any extreme aliasing. Very curious now to see how they managed to optimize animated geometry, maybe heavy partitioning with lazy BLAS refit or just brute force rebuilds. Regardless NVIDIA is obviously far ahead with a more united stack of solutions.  Despite AMDs talent I find it more impressive that Intel manage to keep up with graphics developments much quickly. XeSS, ExtraSS, cluster and partition acceleration structures, etc. Their media encoders have also remained competitive. AMDs strategy is a bit confusing to me especially with how they're dragging out RDNA3.5 in new products. I hope UDNA impresses.  Thank you for the reading material, you are very well informed 😁",hardware,2025-09-28 22:26:59,4
AMD,ngsk7bl,"Number one   Yeah so it seems at based on the AMD patents, but it's not just DGF patents, they also have a fallback method called prefiltering nodes, which is probably very similar to how the RTX Mega geometry clusters work on 50 series, but I could be wrong and like you said NVIDIA doesn't exactly spill the beans on architectural intricacies. While DGF is superior (compression and memory accesses characteristics) this fallback is also made for rapid and wide culling like you said.  Apparently the idea is to precompute a set of quantized BVH data matching the full precision data. It can even be leveraged for ray/box intersections but it seems like triangles will benefit the most.   From what I can read INT operations are multiple times more ressource efficient than FP. That is all PPA characteristics, power, performance at area. From what I can read online it's anywhere from 3-5X, might be wrong, but the patents directly mention ""multiple times more"" so it's at least 3x. In effect AMD can probably shrink the current FP pipeline down, given it'll only be used for inconclusive final tests, and at little cost to die area implement a very wide parallel intersection tester unit that eats ray/tri intersection tests for breakfast.      Another benefit of DGF is that you can include pretty much all the relevant data within one node, so you do just one memory access for the entire block and you can begin doing RT. For example opacity micro maps data has a header within the DGF block. Still no info on subdivisions + tesselations but that's no doubt coming as well given MG supports it, or it'll be included in an accompanying template similar to MG. They also talk about rays coalesced against the same node in the patents, where you mass test rays at once before removing the DGF data, but IDK if that's how things are done today already.  Github FTW! Yeah me to as usual NVIDIA holding their cards close :/ I'm pretty sure the animations rely heavily on subdivisions and tesselation based on this: [https://github.com/nvpro-samples/vk\_animated\_clusters](https://github.com/nvpro-samples/vk_animated_clusters) This simplifies the underlying geometry and should massively speed up rebuilds and avoid them entirely.  For sure, NVIDIA as always ahead of the competition and look at the joke of MS's DXR 1.2. Embracing NVIDIA's functionality over 2 years later and it's still not shipping till Q1 2026, while SER and OMM has been supported since Q4 2022 xD on NVIDIA side.  Intel has long played a leading role in graphics and ray tracing for a long time, before NVIDIA even introduced RTX + has invested a lot in research and is behind a lot of open source SW used in rendering applications. In addition, like NVIDIA, Intel went all AI and HW accell, for example they planned to have SER one year before NVIDIA, but Alchemist got delayed.   Meanwhile AMD used the bean counter approach of wait and see and relying on shaders, they still rely on that for BVH processing. Meanwhile NVIDIA and Intel took the full RT core approach right from the start. Look at where that got them. 5 years of ignoring ML super res only to go all in last minute with FSR 4 + no DLL swap until very recently (FSR 3.1) despite NVIDIA having that for over 5 years. I mean who TF runs that SW department, this is incredibly stupid. I agree that AMD's approach makes no sense.",hardware,2025-09-29 07:56:22,3
AMD,ngsk7ne,"Number 2  RDNA 4 is really just a stopgap nothing more, similar to RDNA 1. They also had Vega iGPU for many gens until RDNA 2 came along, RDNA 3.5 looks to be another repeat of that. RDNA 5/UDNA is poised to be another RDNA 2 full stack moment except this time probably a lot better and less complacent on the SW side.  Me too but based on all the changes suggested in patents (we'll see how many actually ends up in products) + rumours of a clean slate overhaul not seen since GCN in 2011 the picture is slowly taking form and best case assuming NVIDIA keeps rebranding Ampere cores (they really haven't done foundational changes since then) gen the nextgen from AMD could be the most competitive since the Terascale based HD series.  Not gonna spill the beans on the patents today, it's too early but right before launch, then perhaps I might eventually do another post similar to the one I did in the Spring that was picked up by tech media. All you need to know is that AMD id seemingly doing a fundamental overhaul of pretty much every aspect of a GPU, with a particularly strong focus on cachemem system efficiency and data locality.  But I can tell about the major scheduling changes in some of the patents, but it's really just the tip of the iceberg alongside the DGF + prefilter stuff.   Scheduling will go from top down orchestrated to localized hierarchical scheduling and dispatch down to the CU unit. Scheduling will be offloaded to Shader Engines with the command processor job being only to prepare work items and do load balancing between Shader Engines through ""work stealing"" indicated on idle or overloaded signals from the individual Shaders Engines. As a new thing scheduling and dispatch can be decoupled from the SPI completely at the CU level allowing each WorkGroup processor to dispatch its own work queue with unprecedented granularity and latency. The patent mentions an order of magnitude improvement in thread launch performance.  I have a post in here on that from \~8 weeks ago in case you're interested that goes more into depth. All this is to deliver better core scaling and prob drive increased performance for branchy code and GPU Work Graphs API workloads. An API that looks like AMD's new Mantle except it's a much bigger deal, Programmable shaders 2.0 really.",hardware,2025-09-29 07:56:27,3
AMD,nguwiqy,"Number 3:    Nope. Still fumbling in the dark.   Just know where to look for info.       Speaking of that the NVPRO and NVRTX pages on Github are a treasure trove of info and I highly recommend giving it at least a browse.    [https://github.com/NVIDIA-RTX](https://github.com/NVIDIA-RTX)   [https://github.com/nvpro-samples](https://github.com/nvpro-samples)  The Neural appearance models research paper used for Neural Materials is interesting too. Uses a neural BRDF + importance sampling:   [https://research.nvidia.com/labs/rtr/neural\_appearance\_models/](https://research.nvidia.com/labs/rtr/neural_appearance_models/)      I can't wait to see this leveraged in future games across many material types to deliver unprecedented realism. Especially for character rendering with cloth, skin, eyes and hair unprecedented offline render quality visuals in real time.",hardware,2025-09-29 17:05:59,2
AMD,nghc19x,Interesting. AMD is using current gen to test drive next gen stuff.  Like how zen3 with 3d cache and  zen4 with zen c core,hardware,2025-09-27 14:02:26,163
AMD,nghv0gr,"Been waiting for High Yield's Strix Halo deep dive (just waiting for Chips and Cheese).  Reconfirming what we know, Granite Ridge CCDs are different to the ones found in Strix Halo requiring a new tape-out. 4% in die space savings when moving from SERDES GMI-narrow links to RDL fan-out interconnect (using TSMC InFO-oS). This may be one of the reasons why we see lower than expected volume for STX-H and it's high price tag (not to mention it's large IOD is \~307mm2).  The benefits of the new 'sea-of-wires' is lower power for data transfer, so lower voltages for freq, which is what you'd find sipping power on current desktop Ryzens on the IOD (also memory) and lower latency and higher bandwidth. [u/high\_yield\_yt](https://www.reddit.com/user/high_yield_yt/) used Aida's $/mem benchmark, which I've barely come across so it's finally nice to see some numbers and as you can see bandwidth is orders of magnitude higher. Though, while we wait for C&C, wished he used Clamchowder's [Microbenhcmark suite](https://github.com/clamchowder/Microbenchmarks) as it is superior to Aida's. Also, would've loved to see C2C latency test, IIRC Aida has one(?) but there are tools like [https://github.com/nviennot/core-to-core-latency](https://github.com/nviennot/core-to-core-latency) that you can easily run.",hardware,2025-09-27 15:42:04,61
AMD,nghqquh,"What would the shorelines look like on 6th gen EPYC? With a taller I/O die, it looks like the higher aspect ratio Zen 5c dies could work. But there are two columns of dies on each side in Zen 5. Could they lay out the CCXes to flank the I/O die on all four sides?",hardware,2025-09-27 15:20:03,18
AMD,ngjoy1f,"I wish I understood this topic better  with videos like this, I am making progress",hardware,2025-09-27 21:23:45,11
AMD,ngh70t0,"On the question of where the 3DVCache would sit, as it is probably can no longer sit below the CCDs: Wouldn't it be viable to move the 3DVCache either as L4 or with an entire L3 implementation onto the IOD which probably has some colder locations and with the new interconnect might still offer very good latency?   The CCDs would maintain full speed and coherence becomes much easier to manage, only question is if the distance is not to large to the IOD to offset some of the 3DVCaches performance. Also would raise questions for servers as the IOD might have different constraints or be to small to carry so much cache...",hardware,2025-09-27 13:33:12,51
AMD,ngiuw51,The sea of wires design doesn't look like it would work for Epyc. There isn't enough room on the IO die to connect all the cpu chiplets if they have to be in physical contact. I wonder what they will do there.,hardware,2025-09-27 18:44:54,7
AMD,ngkkngl,Hopefully this means the high idle power draw of chiplet Zens is finally over.,hardware,2025-09-28 00:34:21,14
AMD,nghy616,"Please, AMD.  Just put 3D/Infinity cache into your APUs for the benefit of the Steam Deck 2, and I’ll be happy.",hardware,2025-09-27 15:57:27,24
AMD,ngh6tbc,"WTF is that audio...? There's constant clicking, almost sounds like someone is clicking their mouse in the background. Terrible production quality...  I really wanted to watch to the end, because the topic is interesting, but it's driving me absolutely crazy!",hardware,2025-09-27 13:32:00,22
AMD,ngjrbry,Whats exciting is. They hold on to die to die interconnects through pcb traces for so long since zen2  On consumer chips especially to keep the costs down with relative performance good enough to topple i9's.  Now that thats finally put to rest. We will see how much performance potential it will give us. Especially for EPYC and threadripper lineups. A one huge die is coming in? Or for GPU? Even though its a stretch cause even here GPU die is a seperate wafer cut out. I at least expect a Navi 31 levels of MCM to power the UDNA flagship. Which Nvidia still refuses to do on their consumer chip architectures.,hardware,2025-09-27 21:37:02,5
AMD,ngj0ll3,Weren't there other leaks saying that Zen 6 and Navi 5 was/are gonna use bridge dies instead?,hardware,2025-09-27 19:15:10,3
AMD,ngnx5z5,I think this is an important enabling technology for hardware for xbox and valve as well as other PC applications. I want to know more about related technologies nvidia and intel or anyone else are using?,hardware,2025-09-28 15:33:17,1
AMD,nglclxg,"For every 1/r reduction in wire thickness that you get by using InFO, you'll need a corresponding 1/r\^2 reduction in wire length to keep the resistance of the individual wire unchanged. Needless to say that the length reduction factor needs to be better than quadratic to actually decrease the resistance and hence, power dissipation per wire.  Too bad he didn't do any power measurements or if he even has the means to do so, because that would be the most important parameter to measure in this type of packaging.",hardware,2025-09-28 03:37:02,-3
AMD,ngic4dg,They also tested a lot of RDNA 4 changes using RDNA 3.5.,hardware,2025-09-27 17:08:02,78
AMD,ngs7ntl,remember bulldozer?,hardware,2025-09-29 05:52:54,4
AMD,ngl8gm0,If these AMD APUs would only be available on desktop/server mainboards. They only seem to be available in laptops and boutique minipcs.,hardware,2025-09-28 03:08:01,14
AMD,ngnry89,"If the dies are smaller, I wonder why they don't refresh desktop and server products using them. Would require a new IOD, but they need one anyway for Zen 6...",hardware,2025-09-28 15:07:23,2
AMD,nghw9tf,"I don't think CCDs on all four sides would be desirable as the memory/pcie phys also prefer the shorelines.   But on the other hand I also don't have a better geometry in mind that doesn't become insane long and fits 16 CCDs.   I think there are 2 possibilities. The first one is as you say, put maybe 16 CCDs in a square around the IOD but then phys become more complex to implement. Or make a scalable IOD where one IOD can hold 8 or 10 CCDs split over two side, has all the phys on the third side and can connect to another IOD on the fourth side totalling 16-20 CCDs, with two sides dedicated for phys for high end epyc, and single IOD for mid-range epyc with up to 8 or 10 CCDs.",hardware,2025-09-27 15:48:19,10
AMD,ngnnp27,"There has been a relatively recent rumor/leak about the [layout of Zen 6 Epyc](https://www.hwcooling.net/en/zen-6-document-leak-more-cores-pcie-6-0-and-2-5d-packaging/) (ignore the top image).  As always, make of the source what you will, but it does look like plausible solution if they want to keep scaling with enough bandwidth. [Zen 5 Epyc](https://www.amd.com/content/dam/amd/en/images/products/data-centers/2909511-amd-zen5-chip.png) is already straining the current IFOP concept, with 16 CCDs needing to connect through the substrate.  Supposedly, it can be up to 4 of those IODs in a row flanked by the larger CCDs, making for a total of 256 cores.",hardware,2025-09-28 14:45:59,2
AMD,ngju9sb,Its very shallow for people to understand. If these die shot videos had to be %100 correct the videos wouldnt exist. Because you cant just call what it is of a nanoscale factory of logic boards and i/o gates circling on a megahighway. Thats the beauty of scaling production and chip design. For example his zen5 indpeth look had a a lot of guessing because AMD is crazy and changes its design a lot every generation.,hardware,2025-09-27 21:53:42,12
AMD,nghyvzx,"AFAIK the contact density from CCD to cache die on Zen 5 X3D is significantly higher than CCD to substrate on Strix Halo. Making a bunch of TSV on the cache die as passthrough for the interconnect on CCD and aligning them properly shouldn't require higher precision than production of current Zen 5 X3D. This way AMD can align the contact at the bottom of CCD with the TSV on cache die and fan out from the bottom of cache die.  I think cache die below CCD on Strix Halo like package is doable, but it may come at a power cost due to the signal going through an extra layer of TSV.  Regarding L4 on IOD, AMD has already said they can do it on Strix Halo, but GPU benefit from the cache more so all the MALL is reserved for the GPU, however, they can configure it to be used by CPU with ""flip of a bit"". IIRC it's in this interview: [https://www.youtube.com/watch?v=yR4BwzgwQns&t=384s&pp=ygUac3RyaXggaGFsbyBjaGlwIGFuZCBjaGVlc2U%3D](https://www.youtube.com/watch?v=yR4BwzgwQns&t=384s&pp=ygUac3RyaXggaGFsbyBjaGlwIGFuZCBjaGVlc2U%3D)",hardware,2025-09-27 16:01:01,44
AMD,nghyny0,"> Wouldn't it be viable to move the 3DVCache either as L4 or with an entire L3 implementation onto the IOD which probably has some colder locations and with the new interconnect might still offer very good latency?  Why not have the cache stacked on top of the CCDs to expand the existing L3 cache, and another stacked cache on the IOD to serve as a L4 cache?  There's also the IBM method from a decade ago where they give every core a large L2 cache, and a portion of the L2 cache can be tagged by another core to be used as a virtual L3 cache. Or a CPU die can use another CPU die's L2 caches as a virtual L4 cache (as long as the CPU dies are on the same package to avoid communicating over the motherboard). In theory, a single core in an entire CPU package could address about 8GB cache with the virtual L3 and L4 setup. This allows their mainframe CPUs to switch between workloads where it favors large L2 caches and workloads where a large L3/L4 cache is preferred.",hardware,2025-09-27 15:59:54,16
AMD,ngj3cin,"You absolutely CAN do that. But latency, bandwidth, and power will be worse, as the signal will need to travel significantly further. The further away a signal needs to go, the worse it becomes in nearly all aspects.  The easy solution is just move the Vcache over a little bit to not sit right where the info goes.  And if you need more? why not stack it back on top as well? I realize at some point interconnects to the vcache get crazy. But there are a LOT of ways to handle this. it's an engineering problem that has many practical solutions.",hardware,2025-09-27 19:29:45,12
AMD,ngkltgl,"L4 on IOD is better anyway because it benefit both chiplets. It acts as a buffer between System memory and L3.   The issue is on the server side, Epyc has 12 chiplets, that means to get equivalent of 12 chiplet 3D Cache, that L4 on the IOD is going to need to be very huge.",hardware,2025-09-28 00:41:36,6
AMD,nghuiss,Interesting thoughts. With this interconnect the latency probably wouldn't be an issue (it's way better than 2 serde jumps for cross-ccd latency on current ryzens). And there's a lot of space on top of the IOD to potentially get even more cache.,hardware,2025-09-27 15:39:35,2
AMD,nglvi24,"It depends on the difference in latency moving data from the cache between the dies. I still think the lowest odds are that they just stack the v-cache on top of the CCDs going forward. They're going to have separate SKUs for v-cache parts for cost reasons anyway, so customers will still be able to get the higher clocking parts if that's what their workload benefits from.",hardware,2025-09-28 06:13:00,1
AMD,nghliga,That is not how cache works. And I don’t believe it could be moved to iOD.,hardware,2025-09-27 14:53:10,-4
AMD,ngk1rah,"Not the existing I/O die, but that doesn't need to stay the same.  The important metric is edge area, so they can make it elongated to increase edge area without blowing up the total die area.  The chiplets would then be arranged around a large rectangular I/O die to do the fanout.  Not saying that's what they'll be doing, but it's an obvious and easy solution to that particular problem.",hardware,2025-09-27 22:38:34,8
AMD,ngm5yd8,"Indeed. I was rather upset with my 5800X3D sucking down 50 watts idling at desktop, so I undervolted my VSOC from 1.078ish volts to 1.000 volts.  No noticeable improvement in idle :(  The main reason I stick with Intel for servers and things that I keep on 24/7. AMD for my gaming and schoolwork PC.",hardware,2025-09-28 07:51:14,7
AMD,ngprryk,"What I wrote in another post  >If Strix Halo (Ryzen AI Max+ 395) is any indication of their new chiplet packaging found in Zen 6 desktop, then there is good news.  >[https://youtu.be/kxbhnZR8hag?si=DcCjKpPWZVF9fC4O&t=270](https://youtu.be/kxbhnZR8hag?si=DcCjKpPWZVF9fC4O&t=270)   [https://youtu.be/OK2Bq1GBi0g?si=Lo6mU0Cs-QQ8Fo93&t=220](https://youtu.be/OK2Bq1GBi0g?si=Lo6mU0Cs-QQ8Fo93&t=220)   [https://youtu.be/uv7\_1r1qgNw?si=adqEnRTICL0D\_HMd&t=393](https://youtu.be/uv7_1r1qgNw?si=adqEnRTICL0D_HMd&t=393)  >\~10W TDP idle (some stuff opened in the background) across two CCDs (pretty much 9950X) and a large IOD housing a big iGPU.",hardware,2025-09-28 20:48:45,1
AMD,ngi7ir6,Are steam decks GPU bottlenecked?,hardware,2025-09-27 16:44:43,13
AMD,nghv5uv,"I can hear it pronounced on my headphones at the 6:00 mark, but at that point his speaking level is rather uncomfortable to listen to. If I switched to my IEMs I'd probably hear them",hardware,2025-09-27 15:42:50,12
AMD,ngio5yj,I didn't notice it until I paid attention to it. I didn't find it that bad but I know my hearing is  failing. Funny because I can normally hear a lot of background noise in most other videos. Like fans whiring away or speaking behind the camera.,hardware,2025-09-27 18:09:15,3
AMD,nghl3nb,I usually also don't like when that happens but I either couldn't hear it or my phone speakers were just loud enough for me to hear his voice without hearing the background.,hardware,2025-09-27 14:51:05,8
AMD,ngi032l,It's just High Yield,hardware,2025-09-27 16:06:56,4
AMD,ngmig5s,Do not watch the video about that cool ultrasonic knife. I couldnt listen for more than a minute.,hardware,2025-09-28 09:55:31,1
AMD,ngthm58,"What's your audio setup? I have jds labs dac+ and amp+ with few various high end headphones and I can ""barely"" make out the mouse clicks when hes talking. Either I'm deaf, you have godlike hearing or your audio setup is really good.   It is easier to hear at 6:00 but anything before that unless I focus on it, cannot really hear it",hardware,2025-09-29 12:44:39,1
AMD,ngheda2,something's fucked on your end,hardware,2025-09-27 14:15:22,-8
AMD,ngiprph,"> There's constant clicking, almost sounds like someone is clicking their mouse in the background.   No, there isn't.  It's just plosives.",hardware,2025-09-27 18:17:42,-5
AMD,ngju9jr,"This isnt really a leak. I mean with strix halo it seems to not be the case and unless AMD thinks it doesn't work good enough they will highly likely use that interconnect.    If leakers claimed a different design that could have many reasons ranging from ""amd planned/tried it but didn't work out / didn't use it"" to ""How do I know? I made it up""",hardware,2025-09-27 21:53:40,6
AMD,ngnoq3s,"You are completely missing the fact that the SerDes is aggregating many channels of digital signals into a single line.  Thus it has to have signal transitions many times faster, which requires stronger drive signals and more current.  A single inFO line is running much slower than a single serdes line from the previous design and thus is driving much less current and thus does not require unchanged resistance.  You are also ignoring line capacitance which is more important than resistance for high speed signals.  The power needed to drive switching signals is proportional to the line capacitance \* frequency\^2.  You are basically having to fill or empty a little capacitor on each transition.  So individually the lower switching frequency InFo lines save on both frequency and capacitance.  Even if you assume that in aggregate all the little info lines add up to the same capacitance as the serdes lines, the switching frequency savings dominates.",hardware,2025-09-28 14:51:12,5
AMD,nh42yfc,Now calculate the effect of reducing clocks by a factor of n.,hardware,2025-10-01 01:34:30,1
AMD,ngm1102,There are only 16 PCIe lanes.  There's no benefit to putting it on a larger board.,hardware,2025-09-28 07:04:09,8
AMD,ngll5lp,Because they require a different platform lmao,hardware,2025-09-28 04:42:33,5
AMD,ngmjx23,"Framework desktop has one  Afaik they are neither compatible with CPU sockets, nor socketed memory...",hardware,2025-09-28 10:10:08,3
AMD,ngnfkvq,"Framework sells just the mini-ITX motherboard:  [https://frame.work/marketplace/mainboards](https://frame.work/marketplace/mainboards)  The CPU only has 16 pci lanes total so there are not going to be a plethora of various motherboard configurations, there just isn't enough I/O.  The framework has an x4 slot, given the I/O configuration I'm not sure it is possible to do an x8:  [https://static.tweaktown.com/news/9/8/98420\_53\_amd-strix-halo-zen-5-mobile-apu-pictured-chiplet-based-integrated-gpu-is-powerful\_full.jpg](https://static.tweaktown.com/news/9/8/98420_53_amd-strix-halo-zen-5-mobile-apu-pictured-chiplet-based-integrated-gpu-is-powerful_full.jpg)",hardware,2025-09-28 14:03:52,2
AMD,ngjvhtt,"If memory latency is lower and bandwidth is higher due to the new design, wouldn't that make X3D cache less benficial?",hardware,2025-09-27 22:00:49,6
AMD,ngknkpt,"Having stacked L3 *and* L4 will have a massive impact on latency, which will impact performance in apps that don't benefit from large cache.  IBMs hardware is quite specific to the needs of their particular niche, so the things they do might not be all that useful for general purpose compute.  Edit: thinking about it, it makes sense if they end up unifying last level cache across multiple chiplets at some point. That could have huge benefits for multithreading",hardware,2025-09-28 00:52:35,7
AMD,nglk3s3,"Zen6 is on n2 so real estate will be extremely expensive, no chance they opt for a giant L2.",hardware,2025-09-28 04:34:08,5
AMD,ngl0vx4,"> There's also the IBM method from a decade ago where they give every core a large L2 cache, and a portion of the L2 cache can be tagged by another core to be used as a virtual L3 cache.  It may not be technical limitations, but rather patents/licensing that prevent anyone else from doing so.",hardware,2025-09-28 02:17:52,2
AMD,ngl1edk,"> The easy solution is just move the Vcache over a little bit to not sit right where the info goes.  I think this nails it. The vcache die has always been smaller than the CCD. So far, they've have to add structural silicon to even everything out.",hardware,2025-09-28 02:21:09,4
AMD,nghqsnw,"In what way is it not ""how caches work"" and why couldn't it be moved onto the IOD at least as an L4? If it was moved to the IOD as L4 the question is if they could keep latency tight enough so that it still has a similar benefit but in principle the ""worst case"" would be a 3DVCache enhanced infinity cache. This might prove to be simple and coherency for multi-ccd variants might be more efficient to maintain.   Best case would be sufficiently low latency and high enough throughput such that it could act as a good dedicated L4 or integrate even tighter with the L3 of the CCD but on the IOD.    Of course it is none-trivial to ""just"" move the cache onto the IOD and the purpose of the cache might need to be redesigned. Latency will very likely take a non-negligable hit so the role of the cache will change. But if this tradeoff comes with full power CCDs and the performance gain in simulation and gaming workload can still be maintained, it might be worth it.   I don't think its obvious to judge whether this is possible or not without having much better data like roundtrip latency of the new interconnect and cache dependeny and structure of Zen 6.",hardware,2025-09-27 15:20:19,24
AMD,ngi5m6n,The Pentium 2 says hello.,hardware,2025-09-27 16:34:46,9
AMD,ngia9xd,"They've already done something similar on their Navi 31/32 GPUs for the L3 caches and memory controllers. In that form it's probably not quite ready for CPUs yet since the cache tiles are only 16MB each and relatively large for what they offer. With CPUs there's also the concern of them being much more latency sensitive than GPUs.  But still, as interconnects get denser and their performance improves, there has to be a point where the question turns from ""Is it even possible?"" to ""Why not just do it?"".   It's not exactly a novel concept, recent Xeons and Apple's Ultra variants already use multiple CPU tiles with caches shared across them.",hardware,2025-09-27 16:58:37,8
AMD,ngimuhi,"Cache is basically ""the faster part of a memory hierarchy"" that stores the most used data  The current stack is loosely  L0(e.g. microop cache)/L1/L1.5 -> L2 -> L3 -> L4 (e.g. eDRAM on broadwell) -> RAM -> SCM (e.g. optane and certain high performance SLC NAND) -> NAND -> HDD     On my NAS, the cache for the HDDs is optane (l2 arc) and then DRAM for the ARC.   We're caching all the way down.",hardware,2025-09-27 18:02:19,7
AMD,ngip5d6,It's exactly how cache works.  Fetching data from the I/O die would still be a huge improvement over going to main memory.,hardware,2025-09-27 18:14:25,5
AMD,ngmhhu6,> 5800X3D sucking down 50 watts idling at desktop  Is that whole system? CPU be itself should be <20W.,hardware,2025-09-28 09:46:05,9
AMD,ngn7hh7,There is NO WAY a 5080x3D uses 50W on idle unless something is seriously broken with your whole power management.,hardware,2025-09-28 13:16:06,9
AMD,ngpcllj,"Huh. According to HWiNFO64, my 5600x's SOC idles at about 6.5W (VSOC undervolted to 0.913V while running manually tuned DDR4-3600) and the cores idle at about 3-5W.  Did you use XMP or manually overclock your RAM? What's your Infinity Fabric speed? That can dramatically ramp up the SOC idle power usage.",hardware,2025-09-28 19:35:44,1
AMD,ngin2kz,"We can make a faster steam deck, but not one that is significantly faster without also increasing the TDP.  If AMD can keep the thermals down while preserving the battery life and deliver a big enough bump in performance to allow the steam deck to work with many of the newer titles then that would be amazing and warrant a Steam Deck 2.  Until then, Valve has said they're not really interested in doing minor upgrades.",hardware,2025-09-27 18:03:30,15
AMD,ngiodov,"Generally yes, and since it uses DDR5L shared with the CPU, it’s also memory bandwidth starved.  And power starved…and sometimes there’s a CPU bottleneck as well. Honestly it’s a remarkably well balanced device, especially given its 800p screen (handhelds with higher performing APUs with 1080p screens don’t move the needle much in framerate due to the higher resolution, and typically also require significantly more power).",hardware,2025-09-27 18:10:23,10
AMD,ngp0sw5,"Even if more 3d cache wouldn't increase performance (it will for most code due to cache being faster than RAM), it would still decrease data movement power consumption and allow more battery life.",hardware,2025-09-28 18:39:36,2
AMD,nghnllu,"Yeah, might try later on speakers. I was on earphones so I felt the clicking needling my brain :P",hardware,2025-09-27 15:03:49,2
AMD,ngto2fr,"My ""setup"" (I wouldn't even call it that :P) is just plain budget BT soundcore (anker) wireless earbuds... for like $30. And my volume almost never exceeds 50% at the OS level.  It's not like I'm an audiophile, the video was just THAT bad :P I've never had similar issues before.  I wouldn't say my hearing is godlike either, but I'd admit that sometimes I get annoyed by such things more easily than others... ¯\\\_(ツ)\_/¯",hardware,2025-09-29 13:22:29,1
AMD,nghfm2b,Nah it is there. Turn the volume up slightly than what you normally have and you can clearly hear it if your ambient noise is low.,hardware,2025-09-27 14:22:08,13
AMD,nghett2,"No, it's not. It's the video. Pay attention and you'll hear it.  Edit: It's pretty clear towards the start of the third part (2:20) and forward.",hardware,2025-09-27 14:17:53,13
AMD,ngit8ju,"It doesn't sound like plosives... As someone else pointed out earlier, it becomes super clear around 6:00  My best guess would be some sort of bad hardware (microphone) trying to do bad quality heuristic noise canceling or something.",hardware,2025-09-27 18:36:11,11
AMD,ngq4hbb,"Yeah, but it’s been corroborated across multiple different leakers and products. Like the “Magnus” apu and Medusa point",hardware,2025-09-28 21:52:30,1
AMD,ngm33j5,The AMD's 8000 series has 20 with 4 reserved for the io chipset. Probably the same.  As you don't need anything for the GPU that can surely support a nice HBA + 10GBe + 3-4 m.2. But I surely acknowledge that more would be better.,hardware,2025-09-28 07:23:47,2
AMD,ngln949,obviously I meant as motherboards with mounted cpu as it's BGA. They would be perfect NAS to small desktop platforms.,hardware,2025-09-28 04:59:55,7
AMD,ngomjvx,"Thanks, looks like they really deleted the 4 lines the 8000G had for the IO chipset. Sigh...  With boutique I meant the framework:-)",hardware,2025-09-28 17:33:31,3
AMD,ngl8oli,"They could always tweak the architecture to better utilize the improved cache system.  Zen 5 benefits more from X3D than Zen 4 since the architecture is more bandwidth-hungry and is actually bandwidth-starved on GMI narrow config without X3D, which is partially the reason desktop Ryzen 9000 non-X3D has underwhelming performance uplift vs Ryzen 7000.   Zen 6 can be engineered to be even more bandwidth-hungry than Zen 5 to take advantage of both the next-gen interconnect and X3D.",hardware,2025-09-28 03:09:30,15
AMD,nglvchb,"> wouldn't that make X3D cache less benficial?  Maybe a little? A lot of the benefits are from use-cases where larger things can fit in cache, so those bigger lumps of data don't need to be retrieved from RAM which will still be vastly slower than retrieving the data from a slightly slower cache.",hardware,2025-09-28 06:11:34,3
AMD,ngtn893,"im some scenarios yes. but if you have high hit rate of something thats larger than regualar cache but smaller than x3D cache, youlll have massive improvements regardless. The difference wont be as big, but it will still be there as invalidating cache and going to RAM is expensive time-wise.",hardware,2025-09-29 13:17:41,2
AMD,ngtnf6e,as long as latency is bellow latency of talking to RAM it will offer benefits. You can ignore l4 cache if you can fit in L3 cache.,hardware,2025-09-29 13:18:48,1
AMD,ngin6gu,"I suspect it's better/cheaper, at least on most systems, to do what IBM is doing... just make the cache on a CPU bigger and allow sharing. My guess is that it trades some IO complexity for A LOT of flexibility.",hardware,2025-09-27 18:04:04,6
AMD,ngohda6,"In modern cpu, L3 latency (when miss) is about 40ns, and DRAM is as fast as 50ns. If we move L3 or L4 over IOD, i.e. without direct connection to the CPU, which suggest the requirement of SerDes, it totally defeat its purpose.  also, given the larger size of 3DVCache, even a cache hit could take several lookup/match in the associative array table.",hardware,2025-09-28 17:09:53,-2
AMD,ngifhna,"still not over io die, I believe. Architecturally that cache is still inside the CPU.  Edit 1: On Pentium II, where the CPU contains 2 dies, one is CPU itself, the other is the cache. The whole CPU is on a board that plug onto the motherboard that contains north bridge and south bridge. So, no the cache is not on IO die (north/south bridge chips)  Edit 2: people down vote me have no idea how cache works. The cache memory is not indexed by regular memory address, they are mapped/indexed by content. To have a hit/miss in cache, CPU logic circuit needs to go through a serial (row-by-row) of data storage that contains a mapping pair of memory addresses and cache address. When the memory address match (cache hit) , it takes the cache address and go to cache memory and load the data. Because the CPU needs to match this table serially, the cache is always pretty small otherwise the cache miss penalty will be huge. Therefore, it is hard for this to be outside of CPU, architecturally or physically.",hardware,2025-09-27 17:25:08,-3
AMD,ngjvyee,"Cache is fundamentally different than ram, because they are content match or content indexed not address indexed.",hardware,2025-09-27 22:03:32,7
AMD,ngjwflx,"Like I said, that’s not how cpu cache works. You need to know that the data inside cache memory is not indexed by memory addresses. There is no “address” in cache, so an io die can not fetch the data inside cache memory.",hardware,2025-09-27 22:06:23,0
AMD,ngnkczp,"Just going off of CPU Package Power in HWInfo. Core draw was 0.4 watts, rest of chip was reading about 30-50ish depending on when you glanced at it.  At some point I will hook up a watt meter and check the draw from the wall.",hardware,2025-09-28 14:28:56,5
AMD,ngnksjd,"Power plan is set to balanced, and I have undervolted the cores by -25 or -30mV, I can't remember which. Power plan is set to balanced. VSOC also undervolted.  It bobs between 30-50 watts in HWInfo's total package power, depending on when you glance at it. Cores take 0.4 watts, rest-of-chip is where the money comes from.",hardware,2025-09-28 14:31:11,2
AMD,ngn8bv2,performance power plan will do that for sure. no need for things to be broken,hardware,2025-09-28 13:21:22,1
AMD,ngpdnuw,"I used XMP, and my Infinity Fabric speed is 1,800 MHz. DDR4-3600 memory.  [Here's a screencap of HWInfo.](https://imgur.com/a/bVtiLWi)  I have Discord open, 4 tabs of Waterfox, and a Minecraft modpack chilling at the main menu screen. So not *totally* idle, but it should be light work.",hardware,2025-09-28 19:40:47,3
AMD,ngjdw3o,">not one that is significantly faster without also increasing the TDP  It can absolutely be done. Remember that Van Gogh is on the old N6 node. There is just not enough volume to justify a dedicated chip by AMD, which is why it hasn't happened so far, and instead we got bulky and not really that efficient laptop scraps, with useless NPUs instead of a mall cache, and either too many cpu cores or too few GPU WGPs.",hardware,2025-09-27 20:25:07,12
AMD,nguigli,you should be a sonar technician,hardware,2025-09-29 15:58:16,1
AMD,nghjg2v,"I can hear it, but it's not a problem",hardware,2025-09-27 14:42:26,-12
AMD,nghj70f,"yes, I can totally hear it, but the hysterical overreaction is on your end",hardware,2025-09-27 14:41:07,-11
AMD,ngm6on7,You just described 28 to 32 lanes worth of peripherals.,hardware,2025-09-28 07:58:06,7
AMD,ngse5n1,Whats the point of actually using one of these specifically for a storage server though? I dont see any actual benefit from the main purpose of strix halo (the massive igpu) for your usecase over a 9950x?,hardware,2025-09-29 06:54:53,1
AMD,ngmv8nw,They are in active development. There were some articles that came out a couple months ago on that subject.,hardware,2025-09-28 11:51:39,2
AMD,ngqy8n3,"Well it's on an industry standard ITX motherboard, you should have no issue finding a desktop or server housing for it.",hardware,2025-09-29 00:44:56,1
AMD,ngm3wgo,"> Zen 5 benefits more from X3D than Zen 4 since the architecture is more bandwidth-hungry  And on top of that, 9800X3D also runs at higher clocks than 7800X3D thanks to moving the V-Cache die under the CCD.",hardware,2025-09-28 07:31:22,10
AMD,ngnlqia,"Zen 5X3D seems to gain more in gaming vs Zen 5 in comparison to Zen 4 because of the clock speed gains that one gets from Zen 5's 3D stacking improvements, more than the core being more bandwidth hungry or anything.   Chips and Cheese profiling gaming on Zen 5 claimed that the core is held back by front end latency more than the back end, and when it is a backend memory bound it was more due to latency, not bandwidth.",hardware,2025-09-28 14:36:02,4
AMD,ngk2gxo,"The problem is  that SRAM does not scale, so takes up a lot of die space, driving up cost. For someone like IBM, that doesn't matter as much due to the nature of their business.  The idea with 3D cache or moving to putting more cache on the IOD is that they can use older and/or cheaper nodes to increase cache sizes without a significant increase in cost.",hardware,2025-09-27 22:42:51,6
AMD,ngq46e7,"Look at the video. It is discussed how Zen 6 will no longer need serdes to communicate with the IOD as the chips networks will be directly connected.   Also if you take a look at this article by chips and cheese: https://chipsandcheese.com/p/amds-9800x3d-2nd-generation-v-cache   We can see that the latency for L3 cache is around 10ns, while DRAM latency with the old interconnect is ~80ns.",hardware,2025-09-28 21:50:54,2
AMD,ngj0gkr,So is the io die.,hardware,2025-09-27 19:14:25,8
AMD,nglo0h5,"> To have a hit/miss in cache, CPU logic circuit needs to go through a serial (row-by-row) of data storage that contains a mapping pair of memory addresses and cache address.   > Because the CPU needs to match this table serially, the cache is always pretty small otherwise the cache miss penalty will be huge.  No... That's not how cache works at all.  Most modern caches are typically 4-way or 8-way [set associative](https://en.wikipedia.org/wiki/Cache_placement_policies#Set-associative_cache) (other numbers are possible, but 4-8 tends to work best).   With a 4-way set associative cache, the lower bits of the memory address are used to select a single set within the cache, which stores four cache lines. The data is guaranteed to be in one of those four memory locations (or not at all).   Which makes lookups very fast, you look at the memory address to find which set it should be in, and fetch that set's tags from the tag array. All four tags can be checked in parallel with special hardware, and then it knows exactly where in cache the data will be found.   -----  The cache needs to be small because of physics. A combination of speed of light delays and (more importantly) capacitance means the larger the array, the higher the access times.",hardware,2025-09-28 05:06:20,8
AMD,ngofgcc,It's still part of a caching hierarchy.   RAM caches HDD/SSD contents.,hardware,2025-09-28 17:00:48,3
AMD,ngqxh5w,You know cache used to be physically on an entirely different chip right? Using an IO die instead would present zero issue.,hardware,2025-09-29 00:40:20,1
AMD,ngnkf49,Power plan is set to balanced :/,hardware,2025-09-28 14:29:14,2
AMD,ngpzy5x,"Something tells me there might a motherboard setting that's screwing with things.  The troubleshooting idea I can think of to help narrow down what could be the root cause:  1. Set everything to default (e.g. RAM running at JEDEC speed and voltage), then check idle power usage.  2. Enable only the CPU Curve Optimizer, check idle power usage.  3. Enable PBO (not sure if your motherboard allows that for the 5800X3D).  4. Then enable the XMP.  If the idle power shoots up with XMP, it might be the motherboard is overvolting things to guarantee Infinity Fabric and RAM stability. In that situation, you may have to manually configure the IF and RAM settings, and all of the associated voltages, to take the voltage control away from the motherboard.  My motherboard's voltages were all over the place when I left them at auto while overclocking my RAM. I had to lock those down.",hardware,2025-09-28 21:29:28,5
AMD,ngsszlb,"In this picture minimum power your cores used are 11.4W, CPU Core Power (SVI2 TFN) value. Your CPU SoC Power (SVI2 TFN) value is also a bit high, should be 8 to 9 watts.  So with true idle you would get <25W even with a bit high SOC power.",hardware,2025-09-29 09:28:38,1
AMD,ngkhfuk,"The reality is that the old chip is more than fast enough for many of the titles people play on the go, and most people who have a steam deck have a dedicated PC also.  Valve had to pay AMD up front to develop the Van Gogh APU for the original steam deck, and will have to cover that cost again for the next whether it's up front or baked into the per-unit price. They don't sell enough units to do that annually or anything, but they've probably already got AMD at work on their next APU.",hardware,2025-09-28 00:14:08,1
AMD,ngi5qri,Goal post status: moved,hardware,2025-09-27 16:35:25,14
AMD,ngma89h,"pcie 3 x8 covers a 16 port sata HBA, 1 lane of pcie4 is 4GB/s which easily covers 10gbe, 2 lanes with 4-8GB bandwidth is good enough for m.2.",hardware,2025-09-28 08:32:41,7
AMD,ngsehtd,Look at the lower cpus. They have really attractive TDPs.,hardware,2025-09-29 06:58:06,1
AMD,ngof9hr,"You can put that same cache on the CPU cores themselves instead of the IOD though.   What's the point of having cache that MUST take the performance hit over having cache on the CCD that can potentially be used at full performance.   It's a pointless downgrade. The only potential benefit is somewhat reduced interconnect.   On a consumer system with 2 CCDs, that's not a problem.   On a server with 4 CCDs... also not a problem.   If you're going up to 8 CCDs then you're probably looking at a ring bus topology for distributing cache... and at that point it MIGHT make sense to centralize the last level cache on the IOD.  Even then... dual ring bus could probably fix that and you just treat the next level out as another cache level.",hardware,2025-09-28 16:59:52,1
AMD,ngqbutp,"10ns is when cpu is >5ghz and cache hit. You need to consider cache miss. cache miss latency should still be smaller than DRAM latency.   This actually proves my point, because: how do you put V3Cache over IOD when the substrate is already filled with ""sea of wires"" ? You will not be able to support the complicated associative array lookup circuit and controller for data load up from DRAM and to L2 cache.",hardware,2025-09-28 22:32:33,1
AMD,ngjwuue,"No, back then there are north bridge and south bride chips. North bridge chip deals with memory and south bridge deals with io to isa bus and other stuff.",hardware,2025-09-27 22:08:52,1
AMD,ngmup4d,"What you described, 4 way or not, has nothing to do with the fact that cache hit/miss is done row by row in the tag array(mapping/index table).",hardware,2025-09-28 11:47:26,0
AMD,ngok77p,"sure, but because they are indexed differently, to determine a cache hit/miss, it requires tremendous more complicated circuits at very high speed.  When cache missed happens, you also need to move data from lower level cache or RAM, it adds so much overhead. This overhead has a limit because it should not make the higher level cache perform slower than the lower level memory.",hardware,2025-09-28 17:22:48,2
AMD,ngrkjd6,"that is when the actual IO is done on the other side of CPU Bus. On another chip != on an IO die. When the CPU is 100mhz, sure you can have that on another chip that is 1cm away. Try that now in 2025.",hardware,2025-09-29 02:57:08,1
AMD,ngtos5d,"Enabling Eco mode on motherboard saw significant improvements in idle consumption for both 3800X and 7800x3D for me. It also reduced (but not eliminated) random pointless boosting on idle and small tasks. No, i dont need 4 GHZ and temperature spiking to 75C to open a file folder. I can wait 3ms longer for the folder to open. In fact i wont even notice the difference.",hardware,2025-09-29 13:26:32,1
AMD,ngi5w0e,"nothing was moved, maybe you misunderstood my initial comment",hardware,2025-09-27 16:36:10,-11
AMD,ngoxb86,"There seems to be only one 10G ethernet controller chip that supports PCIe 4.0, which only recently came out, and it's not particularly cheap.  You can't rely on higher PCIe revisions to cover the throughput requirements, as peripherals always lag far behind for compatibility reasons.  In any case, I stand by my assessment.  There's no benefit to putting Strix Halo on a standard-sized PC motherboard.",hardware,2025-09-28 18:22:56,4
AMD,ngq806v,"FYI one lane of PCIe 4.0 is \~20Gbit/s or \~2GB/s.   A PCIe 4.0x4 M.2 uses upto \~80Gbit/s or \~8GB/s.   Sata is 6Gbit/s.  The Ryzen 8000 series has 20 Lanes in PCIe 4.0, no 5.0.  But a X870 Chipset **tunnels** 4x4.0 Lanes into another 8x4.0 lanes, 4x3.0 lanes, 4x Sata ports and several USB ports.   X670 has even more ports/lanes, but sucks twice as much power.",hardware,2025-09-28 22:11:21,1
AMD,ngsgncu,I think youre confusing strix halo for strix point which is not a chiplet design. strix halo is configurable tdp down to 45 watts which you can also configure 9xxx series,hardware,2025-09-29 07:19:34,1
AMD,ngkw7u5,You're incorrectly assuming cache on the io die would have to be accessed as though it were off chip.,hardware,2025-09-28 01:48:15,2
AMD,ngn9oba,The good old days,hardware,2025-09-28 13:29:40,1
AMD,ngrlk5u,"It's not done row-by-row. That would be too slow.  The memory address gives you the index within the tag array, and the cache controller can directly access that row. Each row contains one set of four ways.  There is simply no reason to check any of the other rows.   -----------  Fully associative caches do exist (where the data could be in any row), but they aren't used for caching data/instructions (you usually see them things like caching TLB entries, or in network switches). And even these fully associative caches can be searched fully in parallel, thanks to the magic of [Content-addressable memory](https://en.wikipedia.org/wiki/Content-addressable_memory) or CAM.   But CAM arrays are very expensive to implement in hardware, so you don't see it anywhere it's not needed.",hardware,2025-09-29 03:03:59,2
AMD,nh428eb,As long as it's closer than the memory controller I'm not seeing the issue.,hardware,2025-10-01 01:30:10,1
AMD,ngj0a0n,No,hardware,2025-09-27 19:13:26,6
AMD,ngp0es2,If it had the 4 pcie lanes for an io chipset that wouldn't be a problem. Unfortunately that doesn't seem to be the case so you are right.,hardware,2025-09-28 18:37:41,2
AMD,ngkyhjn,"Are you talking about Pentium II or new chip design ?  If it is Pentium II, then the cache was not on io die.  If you are talking about new design, then of course it is, because the data would be serialized and destabilized when it is off the cpu die. Take a look at Strix Halo where it uses new Fan-out to avoid serDes in this video: [https://www.youtube.com/watch?v=maH6KZ0YkXU](https://www.youtube.com/watch?v=maH6KZ0YkXU)  If you use off-die design, you can not avoid SerDes.  If you want to argue that it could still be done at all cost, then sure, but no one would actually design a cpu that way, because cache miss penalty would be too great to be useful.",hardware,2025-09-28 02:02:39,1
AMD,ngsewt4,You are right. I was mistaken about matching it up one by one. I forgot that the rows are indexed by part of incoming address. It’s been too long since last time when I still had the book.,hardware,2025-09-29 07:02:10,2
AMD,nh4bh16,">An electron accelerated from rest in a vacuum by a 12,000 V/m electric field would cover 1 cm in approximately 30.8 nanoseconds.   do you know how many nano seconds per cycle for a cpu running at 6Ghz ? it is 0.6 nano seconds. And L3 cache usually does not go over 50ns when cache miss, or 30ns when cache hit. 1cm will be a big problem.",hardware,2025-10-01 02:25:55,1
AMD,ngq18o8,The entire point is how to avoid serdes.,hardware,2025-09-28 21:35:57,1
AMD,newkjp5,I think the more valuable thing for AMD than this product in itself is testing the new chiplet interconnect on a hardware level which will probably guide Zen 6 development.,hardware,2025-09-18 15:36:07,27
AMD,newanqd,How loud are the systems pulling 180W?,hardware,2025-09-18 14:49:28,18
AMD,newec75,"The RDNA3.5 8060S found in Strix Halo, at a TDP of 120W (dynamic between CPU+GPU) performs in raster similarly to an RTX 4060 desktop (that's around 100W with a 9800X3D). The die housing the GPU, IO, media engine, NPU etc. is \~305m2 while AD107 is 159mm2. Though with power the sweet spot is more 50-85W, Perf/watt in gaming is impressive, but you can argue die area not so much.  Also, since just hearing the Nvidia and Intel news, honestly really excited for this space in RTX chiplets used for future Intel SOCs. RDNA 3.5 can hold itself well, but hopefully this pressures AMD to be more aggressive with their future SOC designs. They already have a new packaging direction, even going as far as redesigning Zen 5 CCDs on Strix Halo with their new IFOP replacing GMI, so going for X3D + not lagging behind their dGPUs not wasting time on RDNA3.5+ and going for RDNA5 and hell why not add 512-bit bus should be interesting.",hardware,2025-09-18 15:06:47,45
AMD,newm4b9,"The CPU is obviously much faster than the one found on the PS5 even with less cache compared to desktop version, it is obvious that the limited power consumption as well as lack of bandwidth is clearly holding back the Desktop APU here.  I wonder when AMD can release version of this APU with built in GDDR7 Vram built in and unlocked power that can reach up to 300 Watts TDP... Given with enough cooling I wonder how this will perform against the PS5 Pro even.",hardware,2025-09-18 15:43:27,8
AMD,newd7li,"I see this more competing against Apple chips than anything else. They seem pretty impressive considering you can game on them properly and do everything else as well including ""productivity"". Depends on price though and what devices they end up going into.",hardware,2025-09-18 15:01:27,14
AMD,neypbbb,"There are about two dozen mini PCs with this chip and only one shitty  overpriced 14 inch HP laptop with it. Fuck AMD for giving HP exclusive rights to that chip for laptops. As shitty as Intel and Nvidia can be, you can trust that they'll make their chips available to everyone. They don't make you wait a year until an exclusivity deal is over to finally get access to the chip. I hope they make AMD pay for this kind of anti consumer practice.",hardware,2025-09-18 21:42:57,9
AMD,nf09s7y,"AMD has a lot of low hanging fruit with whatever follows this that will easily have it beating a desktop 5060 or PS5 pro  1) Still not using an RDNA4/4.5 gpu. Just doing that should give 30-50% better frame rates especially with RT on  2) Still haven't used V-cache, that will add 10% or more and should decrease power draw also  3) Still using old IO die with only 8000 MT/s. They will can go to 11000MT/s and gain another 30% memory bandwidth   4) Can easily give in the same cache as regular desktop ryzen which should add 2-3%   5) They could go with a single 8 core CCD which would cut power draw and give the GPU more power. Zen 6 is rumored to have a 12 core CCD which will give a single CCD config much better performance while keeping latency down  6) LPDDR6 should give much more bandwidth also and improve performance  Just the first 2 which can easily be done today would put this past a 5060 if not 5060ti, so substantially better than a laptop 4070 and would crush PS5. If the benchmarks in OP are correct that means it's likely you could cut it to 50W and still beat desktops using 250W",hardware,2025-09-19 03:07:53,1
AMD,newhdxn,"Kind of a pointless product unless they manage to reduce manufacturing cost (which is hard since it has a big die, compare to even dGPU die).   Looking for a gaming laptop? A RTX dGPU will be better any day while costing you 1/3 the price  The only good potential use now is AI, with big unified memory. But again AMD software stacks are behind Nvidia.   I see many comments still stuck with the old iGPU mindset (being cheap).",hardware,2025-09-18 15:21:05,-4
AMD,newwt8v,It’s powerful because it has “AI Max+” in its name.,hardware,2025-09-18 16:34:30,0
AMD,nfdlolz,Yeah server and especially desktop Zen desperate need better interconnect and packaging to lower the Uncore power draw.,hardware,2025-09-21 06:35:25,1
AMD,neweuh7,Good question  [https://youtu.be/uYLwDkGZOJk?si=4cQxYgWc12uQ0SLU&t=703](https://youtu.be/uYLwDkGZOJk?si=4cQxYgWc12uQ0SLU&t=703)  Found this for a Cinebench run full throttle.,hardware,2025-09-18 15:09:10,8
AMD,nex0zk7,Robtech's review measured 50 decibels at 30 cm away for the performance mode. The balanced mode measured at 48 decibels.,hardware,2025-09-18 16:54:21,3
AMD,newhozz,If you are pairing AD107 with an AMD 16 core 9955HX (which would be the closest equivalent) then you need to account for the 122mm2 I/O die.  Strix Halo uses only a little more silicon than your hypothetical.,hardware,2025-09-18 15:22:31,28
AMD,newgnni,"I still think best case is they utilize a form of hybrid cache, whether it be L4 or shared x3d, to aide in the igpu ram bandwidth issue.  A 16cu RDNA4 igpu with on soc cache would be a force in the low power/portable sector. And the design could easily allow larger cu counts with less bandwidth constraints",hardware,2025-09-18 15:17:38,13
AMD,nexzanh,Sucks it will take a few years minimum for anything from intel + Nvidia to bear fruit. Waiting sucks lol,hardware,2025-09-18 19:37:24,3
AMD,nfd37q8,"i really wonder if we can't have a product segment that is no (or only token) iGPU in the SoC, but with oodles of memory channels (soldered or CAMM or what have you I couldn't care less, ideally HBM lol) and pcie gen 5 or gen 6 x16 (or two x16!! ahhh yesss)  Unified system memory designed for use with dedicated GPUs, trimmed down compared to server platforms but offering similar main memory bandwidth.",hardware,2025-09-21 03:58:16,1
AMD,nexdb5c,"GDDR7 uses 44 data lines for a 32-bit connection, and runs at a much higher bitrate along with using PAM3 encoding.",hardware,2025-09-18 17:51:52,6
AMD,newi2tt,The tech is impressive but still far from perfect. The cost is a real barrier and is mostly being attracted by the local LLM crowd. The miss for Halo in this space is from the bus width. An M4 Max has double the bandwidth and while compute is competitive not filling out the checkbox for bandwidth doesn't leave much confidence. Also their decision to stagnate RDNA 3.5 just leaves out a lot of perf when RDNA4 is arch is shown to be great.  Not only that AMD has many other priorities needed to be checked like their media engine and HW acceleration like RT which is equally a huge chunk why Apple silicon sells not just gaming which is very small in comparison,hardware,2025-09-18 15:24:21,22
AMD,newe9ex,You can game & do productivity on dgpu laptops which are cheaper for many people's perf/$,hardware,2025-09-18 15:06:25,9
AMD,nez5le1,Is the 2025 Asus ROG Flow Z13 unusable as a laptop?,hardware,2025-09-18 23:13:19,6
AMD,nf2it34,"Where are you getting this exclusive nonsense from? There are like five products in the world with this chip, two of them laptops and those are from different manufacturers.",hardware,2025-09-19 13:41:55,5
AMD,nf0t2gm,"V-cache only makes sense with TOP GPUs. Paying $200-300 extra for a feature that only gains 0-5% performance increase with such a weak GPU make zero sense. See HUB's recent CPU scaling video.   Jarod showed that the extra cache (9955HX vs 9955HX3D), when paired with much faster **5090** laptop GPU, barely showed a 6% improvement at 1440p. So yeah, cache makes no sense with an iGPU so AMD will not release this in the following years. Maybe with a 512-bit iGPU when that's ready.",hardware,2025-09-19 05:28:08,6
AMD,newr31e,These things are high end AI workstations and are being sold as such. The gaming part of it is basically irrelevant when people are using these for work and not as a toy.  Nvidia has zero products in this segment by the way. You need to spend $3000+ to get the kind of VRAM needed to compete here if you want Nvidia.,hardware,2025-09-18 16:07:01,8
AMD,nex3hr6,"Well to be fair this is one of the few ""AI"" branded things that people actually use for running AI stuff locally.",hardware,2025-09-18 17:06:06,15
AMD,newqsyy,Aren't GDDR6 PHY are much larger than a ddr5 ones,hardware,2025-09-18 16:05:40,10
AMD,nfdm076,Fire range IO die uses the ancient N6.,hardware,2025-09-21 06:38:22,1
AMD,newo36y,"MALL already fixes that, it already has 32mb. The real problem is a size constraint and AMD is better off improving their memory subsystem which is what they're going for",hardware,2025-09-18 15:52:45,9
AMD,newifoq,"Shared cache is a lot harder to implement, Intel tried it once and dump the tech and pretended it never existed(desktop broadwell).   AMD had it separated for over 10 years for a good reason, and I assure you they have more masters and doctorate under their helm than any random redditor. There is a reason why they have not done it despite being in the game for so long.",hardware,2025-09-18 15:26:03,6
AMD,nfdmivd,"Don’t worry, in the meantime AMD isn’t gonna get -Halo into any mainstream laptop in any meaningful volume.   I will eat my sock if Medusa Halo exists in an 16” Alienware/Dell Pro Max or 16” Zbook/Omen Max or 16” Thinkpad/Legion Pro 7. I wonder if the total shipment of that 14” Strix Halo Zbook exceeded 100 units yet?",hardware,2025-09-21 06:43:07,1
AMD,newpot3,"Yea for the price it doesn't really make any sense at least not to me. If it was around the price of a console it would make a lot more sense. Not sure why anyone would buy into Apple for gaming. Even on native games performance leaves a lot to be desired, at least from the benchmarks I have seen.",hardware,2025-09-18 16:00:18,8
AMD,nex1i36,>  The cost is a real barrier and is mostly being attracted by the local LLM crowd.   AMD could lower costs by cutting the CPU in half (8 cores is enough) and removing the NPU.,hardware,2025-09-18 16:56:45,5
AMD,newlzw4,"APU should always be cheaper, but fact that it is not shows poorly on AMD and Intel, who want to double dip into dGPU market by not pushing forward APU performance and bandwidth.  Apple has nothing to lose in dGPU market, so they are the only ones racing ahead on SoC market.   MacBooks are best for perf/$, at least excluding the heavily milked highend/RAM upgrade stuff. Same with PS5 and Xbox X with APU, best value for money in gaming.",hardware,2025-09-18 15:42:52,6
AMD,nez71e9,That's a tablet form factor with terrible thermals.,hardware,2025-09-18 23:21:45,10
AMD,nf7s7vr,It’s an exotic way to shit on AMD,hardware,2025-09-20 08:44:04,2
AMD,nfk6pzf,"is HUB still not testing any CPU intense scenario? Last time i checked their CPU tests were made on GPU bottleneck software.  If you ever played MMORPGs, the x3D cache can double your framerate quite often even if you have a low en GPU because its CPU thats being the limiting factor. Also in strategy/sim games if the simulation fits inside cache performance is great and you can see it drop off a cliff when the simulaltion exeed cache. the difference in cache sizes are day and night. Im talking more than 2x performance here.",hardware,2025-09-22 07:13:03,1
AMD,nexxocg,"It has the same performance in Moe models as a mainsteam cpu with fast ddr5 and a 3090. The output performance... The prompt processing is just bad.  For dense models the bandwidth is too low.  For any other type of AI, anyone sane will get something with CUDA.  It's almost useless.",hardware,2025-09-18 19:29:34,9
AMD,nexdrmg,High end AI work station lmao  Last time I checked it didn't even have ROCm support,hardware,2025-09-18 17:54:00,6
AMD,nexg335,">Nvidia has zero products in this segment by the way.  That's not true. NVIDIA started it. They have a long history of SOCs that combine a decently powerful GPU with a CPU. There is the Jetson Line and the DGX Spark will follow soon.    Jetson is basically the same concept as Strix Halo, the Orin was Ampere based and released 3 years ago and is still pretty competitive in performance. Recently, the Thor released and is already superior in pretty much every way.    DGX Spark adds scalability and enables clustering of multiple mini PCs  The reason why they never got much attention was the fact that they run Linux and are not suitable for Gaming (nvidia showcased Gaming with Raytracing on ARM on Linux though). But that doesn't mean they don't exist.    That's also the reason why they're working with intel now, to release x86 versions.   NVIDIA is the defacto market leader in that segment as of today",hardware,2025-09-18 18:04:57,0
AMD,nex89eq,"I thought it was the opposite, at the same bus width",hardware,2025-09-18 17:28:28,5
AMD,nex1b7q,AMD talked about the MALL cache on this chip can be used by either CPU or GPU. AMD decided to give it to the iGPU. But it's something they can change. Would like the ability for the user to configure this personally.,hardware,2025-09-18 16:55:51,5
AMD,newjhdz,"I still am not sure why broadwell L4 was dropped so quickly. those chips were gaming monsters in their day. and back then igpus were pitiful.  I spoke with another poster many months ago with solid knowledge (i believe he had an engineering background) who stated how and why hybrid cache was doable with a few tricky bits.  We already know infinity cache works great, the question becomes implementing it in a form on the SOC where its useable by both cpu and gpu.  As for the 10yr separate thing. there are a lot of things they are doing now they never did before. A lot of it comes down to refined ways, as well as nodes getting to a point where the designs become feasible.",hardware,2025-09-18 15:31:03,7
AMD,ney4ptj,"AMD does make a single chiplet version (the Ryzen AI 385) with 8 cores; the issue is though is that the CPU chiplets are pretty small compared to the i/o, NPU, iGPU die - 71 mm2 for 8 cores vs 307 mm2 for the other die, so dropping one chiplet doesn't actually save much in manufacturing costs (the cost to AMD for one such die is probably like \~$25).  Granted, that second die could have been a fair bit smaller if they didn't include that stupid NPU unit in it; we can blame Microsoft and the laptop OEM marketing teams for that, they have been heavily pushing the Copilot+ branding and that needs a large NPU to sit around and waste silicon.",hardware,2025-09-18 20:02:46,8
AMD,neww0rg,MacBooks are best for perf/$?  Consoles have always been best value since they're usually sold at loss at first and pretty low margins (if you dont play online)  But PCs are better value since you get 2-1,hardware,2025-09-18 16:30:41,0
AMD,nezkchm,It's unfortunate that even a long-term launch partner like Asus had to resort to such hacks like a weird top-heavy form factor just to get supply allocations.,hardware,2025-09-19 00:40:50,4
AMD,nfdn1ib,Indeed. I prefer to just say it how it really is: they don’t sell at what they want it for and no mainstream oem actually wants it.,hardware,2025-09-21 06:47:50,1
AMD,nfk8xef,"[https://www.youtube.com/watch?v=gpN4nyftQ3M](https://www.youtube.com/watch?v=gpN4nyftQ3M)  [https://www.youtube.com/watch?v=D1OrxkyA\_WY](https://www.youtube.com/watch?v=D1OrxkyA_WY)  I gave you concrete proof that 3D-cache makes zero sense with a weak integrated GPU. Decisions to develop and commercialize a product are made when there's a large range of benefits, not just a few edge cases.",hardware,2025-09-22 07:35:40,1
AMD,ney2c88,"No, I was speaking about this with respect to Token Generation, not prompt processing. Token Generation scales with memory throughput, which doesn't give CUDA any reasonable advantage in any LLM benchmarks I've seen. People choose Nvidia because consumer cards and older Enterprise cards are just easier to obtain. This also why cards like the P40 and Mi50 still have relevancy today.  I am talking exclusively about consumer based solutions for agentic coding workflows. In those scenarios everything is a mess and these are really the only competition that exists to ageing cards like the 3090 which demand multiple in parallel to use modern coding models, and massively overpriced and overpowered cards like the 5090 which are only purchased because they can barely fit Qwen3 Coder into VRAM.  I really would love to see what you recommend for a coding llm workflow at $2000 and how many tok/s you'd get from it and tell me with a serious face that these aren't the best bang for the buck when it comes to performance/wat, and when it comes to buying new hardware. Oh by the way power costs exist, and I'm really tired of people saying that 2-4 300W cards is better than something like this especially when I'd be spending near what? $0.50/hr to run a setup like that?",hardware,2025-09-18 19:51:47,6
AMD,nf2joto,Got a link for that 3090 with > 100 GB of ram?,hardware,2025-09-19 13:46:36,2
AMD,nezz4dl,ROCm works great on it. Im running Comfy UI and all sorts of image generation on mine right now.  It's literally just 3 commands to download and install ROCm + PyTorch: [https://github.com/ROCm/therock?tab=readme-ov-file#installing-from-releases](https://github.com/ROCm/therock?tab=readme-ov-file#installing-from-releases),hardware,2025-09-19 02:05:05,6
AMD,nf20opj,"Sorry for getting to this late.  My 7950X3D's iGPU has ROCm support, these absolutely do have ROCm support (you just have to enable the override). ROCm gets a bad rep for reasons beyond my comprehension - probably because the older drivers were so bad and because it's equally as hard to get set up as CUDA.  In either case, Vulkan as a backend is _very_ good.",hardware,2025-09-19 11:56:18,2
AMD,neyekij,Is this true?  How do people keep claiming these are AI workstation products then?,hardware,2025-09-18 20:49:05,-1
AMD,nexnsgj,"You have no idea what I am talking about. Products like the Orin (I own an Orin), Spark, and Thor are all arm based solutions. They do not really matter in the space I'm talking about (consumer LLM usage). Right now some of the best devices you can get in the consumer LLM space are these Strix Halo APUs (that and Apple's M4 chips). Nvidia doesn't have a direct competitor and doesn't attempt to get one because they are very ARM and Enterprise centric. ARM because of their long history with ARM and the technology they leeched off of Mellanox (I have used Bluefield 1/2/3s) and Enterprise because that's where the absurd profits are. Gamers see no value in the massive price inflation that happens past 24 of VRAM, especially when Coding LLMs really need 32/48GB of VRAM these days for best inferencing.  There is exactly 1 consumer GPU from Nvidia that can do 32GB of VRAM and it's the 5090. AMD has Strix Halo. Apple has numerous Apple Silicon products. Yes these iGPUs perform _very_ well with LLM workloads because most LLM workloads are heavily bottlenecked by IO throughput and unified memory delivers that.  Everyone knows that 32GB+ of unified memory (or dedicated VRAM) is in demand but gamers because gamers are using toys and not doing actual work.",hardware,2025-09-18 18:42:06,7
AMD,nfkokzp,"So from those links i judge that no, they havent tested CPU cache intense scenarios.  I wouldnt call things like games with >10 million active players edge cases.",hardware,2025-09-22 10:14:18,1
AMD,nf7y393,"I wouldn't say that it has the same speed if i was talking about models that fit in a 3090. The 3090 would run circles around it.  Moe models at around \~80-100GB with llama.cpp will run at the same speed in a ddr5 100GB/s bandwidth mainsteam system with some parts of the model + the KV cache in a 3090 or 5060ti. You will also have 16-24gb more free ram for other uses and far better prompt processing  It may look as a good option, but it doesn't offer anything for AI, except for portable devices.",hardware,2025-09-20 09:43:30,3
AMD,nf20z3f,"It's people who don't use the tools and are trying to act like they have knowledge /experience in an area they don't.  If you're talking about inferencing llama.cpp supports overriding just fine. And if you use a vulkan backend, I've even run inferencing on polaris and iGPUs.  https://github.com/ggml-org/llama.cpp/discussions/10879  (Note: You need to consider the small size of the model being used here - models people use for coding workflows are usually much larger (e.g. 20-40B is not out of the ordinary these days).  Is a good point of reference of the performance of different platforms under inferencing workloads. Traditional review sites (e.g. Phoronix) are way behind the curve on providing adequate benchmarking results as the drivers are constantly changing (see the Arc results in the thread).",hardware,2025-09-19 11:58:11,2
AMD,neymw78,"Check for yourself. Sure you can make it work, but buying an expensive hardware, with AI in the name, and not having it listed on the official support list is a fcking joke  https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html",hardware,2025-09-18 21:30:28,3
AMD,neyysw9,">You have no idea what I am talking about. Products like the Orin (I own an Orin), Spark, and Thor are all arm based solutions.   I literally said that myself. That's why the intel announcement is so relevant.   >They do not really matter in the space I'm talking about  Then, define what space exactly you're talking about.   >that and Apple's M4 chips  You're trolling, right? Do you know that Apples CPUs are ARM too? You say ARM SOCs from nvidia do not count because they're ARM but ARM CPUs from Apple are ""*some of the best devices you can get*""?  >Yes these iGPUs perform very well with LLM workloads because most LLM workloads are heavily bottlenecked by IO throughput and unified memory delivers that  No, not really. The only benefit from strix halo is offering lots of relatively fast ram but performance really lacks behind nvidia and Apples offers.  Why exactly don't Digits Spark or Jetson count?    You're throwing both development work on LLMs (professional workload) and consumers (which you cannot define?   NVIDIAs jetsons can be used as a desktop PC with Linux it and they can even run games. But Software support can still be an issue in some, especially windows. But that's why nvidia announced x86 versions literally today.   Most LLM devs run hardware like this as servers anyway. There is no point in giving each developer its own training system. And most want to LLMs on Linux but still use Windows for their desktop.   But yes, for people who only want one system, do heavy llm development but don't care that much about performance and don't mind having their desktop system doing the heavy work all the time, can only run Windows but don't mind paying a lot of money, Strix Halo is the only option.   The key difference is that AMD advertises them as gaming/mobile systems, nvidia doesn't. But have a look at r/LocalLLM, r/LocalLLaMA etc..",hardware,2025-09-18 22:34:42,3
AMD,nf1wn2z,It's weird how much attention these products are getting in that case...,hardware,2025-09-19 11:28:35,0
AMD,nf22hyc,"You're just trying to mince words here. I am talking about consumer products, not tinker or prosumer parts. No consumer is going to spend $3500 for an AGX Thor, just like nobody bought the Orin for $2000. These products are used in industrial automotive systems (I know, because I maintain a custom kernel to put on them).   > Apple is an ARM product.  You totally missed the point of my post. Apple is a consumer ARM product, not an SBC.  I'm sorry, I'm only skimming your posts as I don't really have the attention span to read half dozen paragraphs about this.",hardware,2025-09-19 12:08:06,2
AMD,ng8dxqj,Haven’t seen any hard dates on the refresh,hardware,2025-09-26 01:37:35,43
AMD,ng9dg8h,It looks like price comparison not performance. The 265k is better than 9900x,hardware,2025-09-26 05:53:04,21
AMD,ng94saj,"I can predict shit as well. I predict that I will be sleeping, eating, shitting every day, with perhaps better digestion given I've incorporated lot's more of fiber and fruit in my diet.   Until independent 3rd party testing comes out, NO ONE should be taking anything at face value, regardless of company.",hardware,2025-09-26 04:38:31,25
AMD,ng8ckce,">The flagship Core Ultra 9 285K is pitted against AMD's flagship part, the Ryzen 9 9950X3D. The gaming performance is shown to be mostly trailing by single-digit percentages, while content creation performance sees performance gains in favor of the 285K in 4 out of 5 tests.",hardware,2025-09-26 01:29:31,18
AMD,ng9jzsg,Wait how the these cpu were arrow lake refresh? The cpu names are exactly same as arrow lake  CPUs   I am confused,hardware,2025-09-26 06:53:08,5
AMD,ngb1mr7,"We're at the stage now where even if Intel has the faster CPU, I still won't buy them just because of their BS with changing motherboards as frequently as they do.",hardware,2025-09-26 13:51:18,8
AMD,ng9kqey,"Arrow Lake might not be excellent in gaming, but it's a big step in the right direction for Intel.  Power draw and heat are WAY down compared to previous gens.",hardware,2025-09-26 06:59:56,9
AMD,ng8j8p1,"The charts does not show how big the Intel CPUs power consumption is relative to the AMD CPUs, though. I think that is an important metric as well.",hardware,2025-09-26 02:09:50,11
AMD,ngy7o42,Problem is still that they are getting horrible results in gaming compared to the x3d chips. Intel really needs to nail that down. Because if they had a vcache competitor right now Intel WOULD have the better cpus all around.,hardware,2025-09-30 03:48:49,1
AMD,nga1zu6,intel has recently pushed through products on deadlines that underperform simulation/projection by quite a bit. just a general indication of being overstretched and falling behind on the empirical engineering iterations.,hardware,2025-09-26 09:52:12,1
AMD,ng8pzy5,"For others curious, all we know is “2026”, per an investor conference.   https://www.techpowerup.com/340836/intel-confirms-arrow-lake-refresh-next-year-nova-lake-scheduled-for-late-2026",hardware,2025-09-26 02:51:43,26
AMD,ng9yib4,>shitting every day  Not if you run into the Wu-Tang Clan.  They’ll keep you well fed though.,hardware,2025-09-26 09:18:22,10
AMD,ng8uitg,And that’s the best gaming results intel could cherry-pick. We’ve all seen way worse in reviews.,hardware,2025-09-26 03:21:25,32
AMD,ngdwbhs,Arrow Lake-S isn't Arrow Lake refresh. They just updated data after performance fixes over the last year.,hardware,2025-09-26 22:26:02,6
AMD,ngbjwvk,">Power draw and heat are WAY down compared to previous gens.  That's not that great of an achievement when you consider that you can get comparable performance, comparable thermals, and a comparable power draw from just dialing the 12th, 13th, and 14th gen back from their stupidly high clockspeeds.  It's a thoroughly mediocre product all the way around.  Most of the tiles aren't even fabbed by Intel itself.  Moving the memory controller onto its own tile has resulted in such a steep latency hit that the only way to  kinda mitigate it is by throwing stupidly fast and expensive DRAM at it.  PCIe 5.0 devices (specifically SSDs) may not operate at their full speed because of the IO tile not being quite up to snuff.   The P Cores are thoroughly meh and Intel's insistence on using a heterogeneous architecture on a platform where power draw isn't that much of a consideration introduces other compromises (be it OS scheduling issues or having to limit certain SIMD instructions because the E-cores lack them).  The NPU isn't fast enough to be all that useful even in the applications that could use it.",hardware,2025-09-26 15:22:32,11
AMD,ng9tfhf,The big problem for them is they got there by using the best and most expensive external node available. They simply can't do that forever.,hardware,2025-09-26 08:26:46,5
AMD,ng8mbjh,"[Arrow Lake, on average, is just about comparable compared to Zen 5 in power consumption.](https://www.techpowerup.com/review/intel-core-ultra-9-285k/24.html)  Intel has a big win in idle power usage though.    Zen 5 is slightly faster on average (1-5%), and notably faster in certain workloads, including most games.",hardware,2025-09-26 02:28:39,45
AMD,ngc3dge,"Afaik, ARL performed as projected. Which made their initial claims of bugs/underperforming that much more of a lie.",hardware,2025-09-26 16:56:51,3
AMD,ngap3wt,I'm still half convinced that he had to have misspoke about it being in 2026 and not this year lmao,hardware,2025-09-26 12:42:09,11
AMD,ngayapk,"This would be so dumb if they refreshed Arrow Lake then release Nova Lake less than a year later.  Then again, this meme release cadence has happened before",hardware,2025-09-26 13:33:26,5
AMD,ngeppfy,>Moving the memory controller onto its own tile has resulted in such a steep latency hit  ARL's latency issues are more than just having the IMC on a separate tile. Even L3 has issues,hardware,2025-09-27 01:27:08,5
AMD,ngdii12,This is all true,hardware,2025-09-26 21:09:27,2
AMD,ngbo9ri,Alright.,hardware,2025-09-26 15:43:17,-2
AMD,ng9u9tv,N3B is trash. Only 2 companies chose it (Apple and Intel) and both got lackluster architectures from it. Apple's N3E product was surprisingly much better as a comparison).     Even according to marketing from TSMC. N3B is at best 10% better than N4P,hardware,2025-09-26 08:35:29,4
AMD,ngarbar,">Intel has a big win in idle power usage though.  Yeah, it would be nice if AMD could fix that with their upcoming Zen 6. Most of the time my Ryzen 9700X spends its time idling with low work (like browsing, office work) so lower power consumption would be appreciated.",hardware,2025-09-26 12:54:52,8
AMD,ng99924,Not when independently tested. Zen 5 is 30% faster.   https://www.phoronix.com/review/amd-threadripper-9970x-9980x-linux/9,hardware,2025-09-26 05:15:42,4
AMD,ng9l0dj,Idle means nothing tbf,hardware,2025-09-26 07:02:30,-18
AMD,ngc2y4b,"Yeah, it's going to be a repeat of Rocket Lake. Anyone who gets ARL in 2026 is setting themselves up for disappointment.",hardware,2025-09-26 16:54:50,4
AMD,ngg8cug,"My suspicion of why their L3 underperforms so hard is that it’s designed so bLLC can be added. It performs like a huge L3 cache, but only has a normal capacity. I guess we’ll find out with Nova Lake if the cache performance between standard and bLLC cache is that different.",hardware,2025-09-27 08:52:21,2
AMD,ng9vx4w,That's still head and shoulders above the N7-class node Intel was using before.,hardware,2025-09-26 08:52:13,14
AMD,ng9yydz,"And how much better N4P is compared to Intel nodes (let's say I4)?  Also, N3E isn't really an improvement over N3B, especially regarding efficiency, mostly it has a better cost structure. DTCO improvements are what allowed Apple to improve on what's basically the same node I guess.",hardware,2025-09-26 09:22:48,9
AMD,ng9y92q,"Improvement is trash?  If you go up by 10% year over year and your competition is down 10% year over year, who improved?  Your statements are all over the place and quite frankly puts on display how poor your critical thinking skills are.",hardware,2025-09-26 09:15:47,13
AMD,ngc50tk,"If Strix Halo (Ryzen AI Max+ 395) is any indication of their new chiplet packaging found in Zen 6 desktop, then there is good news.  [https://youtu.be/kxbhnZR8hag?si=DcCjKpPWZVF9fC4O&t=270](https://youtu.be/kxbhnZR8hag?si=DcCjKpPWZVF9fC4O&t=270)   [https://youtu.be/OK2Bq1GBi0g?si=Lo6mU0Cs-QQ8Fo93&t=220](https://youtu.be/OK2Bq1GBi0g?si=Lo6mU0Cs-QQ8Fo93&t=220)   [https://youtu.be/uv7\_1r1qgNw?si=adqEnRTICL0D\_HMd&t=393](https://youtu.be/uv7_1r1qgNw?si=adqEnRTICL0D_HMd&t=393)  \~10W TDP idle (some stuff opened in the background) across two CCDs (pretty much 9950X) and a large IOD housing a big iGPU.",hardware,2025-09-26 17:04:41,7
AMD,ngas273,It’s not going to unless they change the way they do chiplets.,hardware,2025-09-26 12:59:04,3
AMD,nga0awh,"> Not when independently tested  TPU is independent, not sure what you meant there.   > Zen 5 is 30% faster.  So I decided to dig around to see why Phoronix's results were so different to others given that [Puget's benchmarks](https://www.pugetsystems.com/labs/articles/intel-core-ultra-200s-content-creation-review/) show that Intel the 285k trading with the 9950X.   Reading through it, it seems that almost all of the scoring difference came from CPU based inference benchmarks and AVX512 support for machine vision. I'm not entirely sure how that maps onto the typical workload which doesn't use AVX512 and is almost certainly not going to be performing CPU based inferencing. On top of that, [HotHardware](https://hothardware.com/reviews/intel-core-ultra-200s-arrow-lake-cpu-review?page=4) suggests a significant performance uplift when using the NPU, and while that is unlikely to close to gap caused by AVX512 support it is something that wasn't mentioned in the review.  A benchmark suite with more non-AI focused tools like [Phoronix's original review](https://www.phoronix.com/review/intel-core-ultra-9-285k-linux/18) shows only 17% performance difference between the 9950X and the 285k, and since then [they have found a 6% increased in performance](https://www.phoronix.com/review/intel-arrow-lake-ubuntu-2504/10), which brings it more in line with the other reviewers like [GN](https://gamersnexus.net/cpus/get-it-together-intel-core-ultra-9-285k-cpu-review-benchmarks-vs-7800x3d-9950x-more#285k-production-benchmarks), [HWBusters](https://hwbusters.com/cpu/intel-core-ultra-9-285k-cpu-review-performance-thermals-power-analysis/11/) and others",hardware,2025-09-26 09:36:05,15
AMD,ng9wtc6,That's threadripper...,hardware,2025-09-26 09:01:15,5
AMD,ngbxk4h,Is your CPU running at 100% power 24/7/365?,hardware,2025-09-26 16:28:33,4
AMD,ngc6l6v,"Rocket Lake at least got a year though before Alder Lake released, this isn’t even getting a year",hardware,2025-09-26 17:12:06,2
AMD,ngal3ag,RPL is still selling in record volume that is supply constrained and has better margin and cost than their ARL CPUs just show how much screwed ARL was as a design.,hardware,2025-09-26 12:17:20,5
AMD,ngcazk5,"cost efficiency matters.   If a node is half the price, you could conceivably slap in 1.5-2x the cores at iso-cost.",hardware,2025-09-26 17:32:55,1
AMD,ngfcq33,What's APU STAPM? It says it's pulling 40W in this video.,hardware,2025-09-27 04:03:48,1
AMD,ngasc0e,I understand that but still hoping that idle power usage will be reduced.,hardware,2025-09-26 13:00:34,1
AMD,nga8b21,"> TPU is independent, not sure what you meant there.   Ah, I thought he was referencing the Intel numbers from this thread. Even though the issue is like you pointed out through updates the newer gen CPUs got a lot of optimizations.  This one for example  https://www.tomshardware.com/pc-components/cpus/intels-arrow-lake-fix-doesnt-fix-overall-gaming-performance-or-correct-the-companys-bad-marketing-claims-core-ultra-200s-still-trails-amd-and-previous-gen-chips  > Perhaps more importantly, compared to the fastest patched 285K results on the MSI motherboard, the Ryzen 9 9950X is now 6.5% faster (it was ~3% faster in our original review)  It made Zen5 3.5% faster on top of the 3% it already was.  > Reading through it, it seems that almost all of the scoring difference came from CPU based inference benchmarks and AVX512 support for machine vision. I'm not entirely sure how that maps onto the typical workload which doesn't use AVX512 and is almost certainly not going to be performing CPU based inferencing. On top of that, HotHardware suggests a significant performance uplift when using the NPU, and while that is unlikely to close to gap caused by AVX512 support it is something that wasn't mentioned in the review.  Yeah you a right. AVX512 makes the Zen5 chips great CPUs for applications. That is why I like to reference this benchmark its a lot more complete than others giving a clearer view of the performance.",hardware,2025-09-26 10:46:58,3
AMD,ng9xjzp,"Yes, but they also tested the 285k and the 9950x. Look at the last graph, ""Geometric Mean Of All Test Results"".  There were tons of updates to take advantage of the newer CPUs. You can't just go by the release reviews. Zen 5 pulled by a lot.",hardware,2025-09-26 09:08:48,2
AMD,ngdk35l,"No, but you buy PC to use them. Once you actually game or use your PC for actual work, you lose all that idle advantage from Intel.",hardware,2025-09-26 21:17:53,0
AMD,ngcqezu,No but unless it’s somthing like a server or NAS it isn’t going to be turned on and sitting idle much,hardware,2025-09-26 18:48:25,-2
AMD,ngc7hbg,"That is not the case. Rocket Lake was released in March and Alder Lake in November, both 2021.",hardware,2025-09-26 17:16:20,5
AMD,ngcakv6,Rocket Lake had like half the MT performance of ALD and markedly worse ST and like half the perf/watt.   No scheduler BS with it or AVX512 BS but... it didn't age nearly as well as something like a 12700k.,hardware,2025-09-26 17:30:57,3
AMD,ngbixpc,">RPL is still selling in record volume that is supply constrained    To be fair, this is likely due to macroeconomic factors. OEMs are not predicting good times for 2026, so cheaper hardware is what they're buying.",hardware,2025-09-26 15:17:32,5
AMD,ng9ynh5,"If you look at individual tests they're much closer, apart from some extreme outliers In the AI related tests. Possibly due to the lack of AVX512, but the difference is so large that I don't even know.",hardware,2025-09-26 09:19:47,7
AMD,ngd2erk,The average user leaves their computer on idle or sleep mode for multiple hours a day,hardware,2025-09-26 19:48:26,3
AMD,ngdjqzw,So glad I held out another 6 months for my 12900k and didn’t wait any longer past that.,hardware,2025-09-26 21:16:04,3
AMD,ngbmzc5,They can put a made in US Stamp lol 🤣🤣,hardware,2025-09-26 15:37:06,2
AMD,ngdk8fv,There is a thing called turn off your PC when you're not gonna use it for a long time.,hardware,2025-09-26 21:18:40,0
AMD,ngdlc62,The average user doesn't do that,hardware,2025-09-26 21:24:35,2
AMD,ngdll1z,sure,hardware,2025-09-26 21:25:55,1
AMD,nggd3xx,Maybe in the US where they just waste waste waste,hardware,2025-09-27 09:41:16,0
AMD,nfx3tpx,"So phantom spirit is still king, nice.",hardware,2025-09-24 09:34:16,25
AMD,nfx0ux4,"i have an RP130 and a 9800x3d, it gets to 70C+ when compiling shaders, generally 40-50C when gaming. CPU runs at 90C+ if i don't use negative PBO though, my current temps are from using -30 PBO.  I think these coolers dont run well at low RPMs, at least not when compared to the PS120 or PA120 that are optimized to have low RPM and good temps.  They're probably able to handle higher heat loads due to the larger heat sink and RPMs, but at the cost of having lower efficency at low RPM. That said, I don't think I'll be switching it anytime soon since it works just fine, and if I were to switch to something it would be the PS120 Evo.",hardware,2025-09-24 09:03:36,5
AMD,nfzubxs,Phantom Spirit keeps my 9800x3d around 40C idle. Stock fans were replaced with one A12x25.,hardware,2025-09-24 18:49:50,3
AMD,nfx9ai8,Another victory for the Phantom Spirit,hardware,2025-09-24 10:26:01,3
AMD,nfxsdg2,All these tests on X3D CPUs seem to miss something vital *(that Hardware Canucks showed multiple times)*:  cooler temperatures on Zen 5 don't translate into meaningful **performance** differences   Only on Intel do we see clock speed/benchmark numbers that matter.,hardware,2025-09-24 12:42:48,4
AMD,nfx63pf,"I agree, I think it would have been better to install two TL-K12.   I own the standard RP130, but I'm disappointed that the fan clip shape is unique and makes it difficult to use with other fans. Were you able to install the T30 without any problems?   The Phantom Spirit 120 Vision EVO with TL-K12-X28-R9 fan looks pretty good (though I don't need the display...)",hardware,2025-09-24 09:56:22,2
AMD,nfwygkc,"I have the Thermalright Frost Commander 140, their previous flagship, and I'm really happy with it.  It cools my 5800X3D without any issues.",hardware,2025-09-24 08:38:50,3
AMD,nfxarea,PHANTOMO SPIRITO! God i fucking love Japanse.,hardware,2025-09-24 10:38:57,4
AMD,nfycxj2,"Yeah I don't even get it at this point but I'll take the cheap cooler. I had the original royal pretor thinking it would be better, then reviews came out and showed it was at best nearly the same, or sometimes worse than the royal pretor I could get for half the price, so I returned and got that.  Been happy ever since! I even had an issue with the pretor where at certain RPM's the fans would make extra noise like a buzzing, that the phantom spirit does not have. So it's a win-win.",hardware,2025-09-24 14:33:25,3
AMD,ngc5yfd,is there anything better that's not shown in this video?,hardware,2025-09-26 17:09:08,1
AMD,nfz09ab,-30 can't be stable btw,hardware,2025-09-24 16:25:50,6
AMD,ng98ak4,> Stock fans were replaced with one A12x25.  why did you,hardware,2025-09-26 05:07:28,2
AMD,ngt71jb,cooler temperatures havent translated into performance differences (unless you are getting throttled) for many many years.,hardware,2025-09-29 11:33:33,2
AMD,nfxwxnx,X3d is even more power efficient under load than regular version.  Intel CPUs consume a lot of power and produces a lot of  heat especially Raptor Lake.,hardware,2025-09-24 13:09:13,3
AMD,nfz0k5l,cooler temperatures allow you to do +200 PBO more easily  also having cooler temperatures in your system is kinda fucking great you know,hardware,2025-09-24 16:27:17,2
AMD,nfx9j2d,"It’s a bit complicated. I actually own both versions of the TR Royal Pretor (Ultra and the regular one). I’m using the clips from the non-Ultra version on the Ultra together with Phanteks T30s (check the color — they’re silver). You have to stretch them a bit more, but they fit just fine.  Recently I built a mini-ITX system out of spare parts and used the regular Royal Pretor with one T30 in the middle, plus the standard 120 mm fan at the front (the clip there is black). The black clip on the Royal Pretor Ultra doesn’t work with fans thicker than 120 mm in the front position. But the silver clip from the regular Royal Pretor *does* fit even with the T30 at the front.  Also, here’s my mini-ITX build post if you’re curious: [link](https://www.reddit.com/r/sffpc/comments/1nj1k8f/not_your_typical_parts_ryzen_5700g_apu_with/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).",hardware,2025-09-24 10:28:08,2
AMD,nfxriyo,"yeah but... X3D chips, esp 8c X3D chips are kind of cool running  esp ones where you can't push past the wattage limits (IE not the 9000 series) and try and OC the thing.   if a 14900K with a screaming OC was cooled nice then that is another thing, but those just need a 360 if not 420 AIO to cool properly.",hardware,2025-09-24 12:37:39,3
AMD,nfx09jm,"Define ""cools"". What clocks do you hit when compiling shaders?",hardware,2025-09-24 08:57:23,-3
AMD,ng37vfs,"Binning exist. My 9800X3D does not run -30, but it runs at -20 plus -10/-15 on the higher clock settings. It just don't like a high offset on lower clocks.    So you can tune it to be stable even with a total of -30.  A quick run on Cinebench R23 shows 105W and <70°C (22°C roomtemp) with a Phantom Spirit 120SE, but it can rise to <80°C on other specific loads like shader compile.",hardware,2025-09-25 07:33:13,1
AMD,nfzfx6h,"it didnt crash on anything that i tested, and effective clocks were good in cinebench (5200MHz), so I guess im fine. OCCT and P95 was stable as well. Temps went from 93C in cinebench to 70+C. Been gaming on it since January so i think its stable.",hardware,2025-09-24 17:40:24,1
AMD,ng2ljks,Wrong. It's unlikely to be stable. It could be just a really good chip,hardware,2025-09-25 04:15:07,1
AMD,ng99m4q,"To compare performance. 2C difference under load, idle same temp. Quieter.",hardware,2025-09-26 05:18:51,3
AMD,ng15d8s,"> also having cooler temperatures in your system is kinda fucking great you know  Er, not exactly. It's the same heat load either way; your CPU doesn't really care if it's 50c or 70c except for very, very long term reliability and available thermal headroom. You want to be looking at watts, not celsius or kelvin.  Your overclocked/high performance RAM however cares about temperature, and Thermalright's stock fans don't really move enough air to cool the RAM appropriately unless you have some kind of really amazing front intake/top exhaust and no flow-through GPU cooler.",hardware,2025-09-24 22:50:56,1
AMD,nfxhq24,"I see, so that's how it was. I wish they included a compatible clip lol   My RP130 had a humming noise coming from the front fan, so I replaced it with a TL-H12-X28-R7. There was no noticeable change in temperature, but it's quieter and looks better:⁠ [link](https://www.reddit.com/r/lianli/comments/1n74j3j/my_all_white_lancool_207_build/)",hardware,2025-09-24 11:33:06,2
AMD,nfx9amz,"I have a Phantom Spirit 120SE and it kept my previous 5800X3D at the 4,45Ghz limit on all cores at all times.   My current 9800X3D stays at 5,23Ghz (with UV) on all cores with the same cooler at around 70-80°C.  I also use a lower RPM and Noctua 120x25 fans.",hardware,2025-09-24 10:26:03,6
AMD,ngb2rjt,i see. why not get a noctura cpu cooler?,hardware,2025-09-26 13:57:15,1
AMD,ngfdz8b,One A12x25 performs better than two stock fans? Is it installed in the middle?,hardware,2025-09-27 04:13:24,1
AMD,ng39fx4,"If you have headroom, you can overclock it. Otherwise it stays at the powerlimit it was set to out of the factory.   But you can also set RPM of your fans lower, if you have headroom and don't overclock.  As for RAM, right now mine is idling and it sits at 31°C with 1W of powerusage in a Lancool 217 case with stock fans.   When the system is running it sucks only around 2W per stick and doesn't go above 50-60°C.  A CPU air cooler is just much better then any AIO for MB cooling. Some AIO makers realized that and build fans into the CPU block...",hardware,2025-09-25 07:49:07,2
AMD,ng1dqo1,"> your CPU doesn't really care if it's 50c or 70c  yeah, but lower temperature gives you: less heat coming out of PC, less heat leaking onto other components (RAM included), possibility to use lower RPM on fans etc",hardware,2025-09-24 23:39:44,-2
AMD,nfz3fst,I didn't know that something like TL-H12-X28-R7 existed. You have a sick build there.,hardware,2025-09-24 16:41:12,2
AMD,nfzu8bz,First I've heard of an air cooler keeping a 5800X3D cool like that.,hardware,2025-09-24 18:49:21,0
AMD,ngc2xjo,Already had the noctua fan lying around. At that point just need a cheap $35 high quality chunk of metal like the PS,hardware,2025-09-26 16:54:46,2
AMD,ng7nwfi,"> As for RAM, right now mine is idling and it sits at 31°C with 1W of powerusage in a Lancool 217 case with stock fans. >  > When the system is running it sucks only around 2W per stick and doesn't go above 50-60°C.  This *really* varies from kit to kit, and with my kit at least on absurdly tight timings it will start to error about 65c. Turning up the front intake a little so they stay closer to 45c means no errors after 24 hours.",hardware,2025-09-25 23:01:41,1
AMD,ng1lmzt,"> yeah, but lower temperature gives you: less heat coming out of PC, less heat leaking onto other components (RAM included), possibility to use lower RPM on fans etc  That's not right at all, no. A heatsink is not removing heat from your PC, it's just moving it off of the silicon. It makes the same heat either way.",hardware,2025-09-25 00:26:43,3
AMD,ng1xr1d,"If the heat load (in watts) is the same, adding a better CPU cooler will result in more heat coming out of your PC, not less. A better CPU cooler is more effectively moving the heat away from the CPU and (with an air cooler) dumping it into your case, for your case fans to exhaust. This also means a better (air) cooler is going to heat up your RAM more, as it will generally increase air temps inside the case.",hardware,2025-09-25 01:38:48,2
AMD,ngt7b4z,"This is incorrect. In fact the lower you keep CPU temperature the more heat you are removing from CPU, thus more heat into the room.",hardware,2025-09-29 11:35:32,1
AMD,nfzwsjw,The 5800X3D used 105W just as the 9800X3D now.  Also binning DOES exist. The 5800X3D I've got was a good sample. The 9800X3D I have is a bit below average.,hardware,2025-09-24 19:01:44,2
AMD,ng9of5v,"I have just a 2x32GB 6000CL30 (36-36-76) Vengeance kit from Corsair. Voltage about 1.4v.  But honestly, I don't know much about RAM. :D Never did manual timings and such.   I concentrated more on CPUs and GPUs.  But I can say that in general I build and tuned my PC (5080/9800X3D) in such a way that ALL parts don't go above 60°C, except the GPU or CPU, which might go upto 70°C in **rare** cases, in a non-climated room.    Fans are at or below 50% RPM, which is 1000RPM or below.",hardware,2025-09-26 07:35:58,2
AMD,ngt7es7,"if CPU is cooler, then that heat  has been transfered somewhere else, in example his room. Lower CPU temperature means hotter room (altrough the difference is really negligible here).",hardware,2025-09-29 11:36:17,2
AMD,ng224se,"This isn’t true either - if the heat load (in watts) is the same, at steady state the same amount of watts gets dumped into your room. A worse cooler just has the CPU get hotter; the temperature getting hotter enables the worse cooler to move the same amount of watts out of the CPU.",hardware,2025-09-25 02:04:44,2
AMD,ngaigwq,"If you're not mem OCing or running really high performance memory, then temp doesn't matter. It's just jarring that DDR5 really cares about what exact temperature it is rather than DDR4's just ""below 75c in general"".  That kit's timings aren't that tight and the voltage is average, pardon my French, but I wouldn't give two shits about what temperature they are if I were you. It's not running tight enough for the temperature to have an effect on what is and is not stable. And with an X3D chip you don't have the power limit/boost headroom to really strain your VRM's, so as long as they have a heat pipe and some fins I also wouldn't care about what temperature they are, MOSFET's can burn and be perfectly content.  Ofc, the CPU will care about low temps if you're doing PBO offsets per-core.  [Some games are insanely sensitive to RAM performance.](https://www.reddit.com/r/overclocking/comments/18z4rm9/some_fresh_zen4_ramif_overclock_scaling_data/)  Even with an X3D chip here, it's jarring how much performance is dictated by memory performance.  Do note the only game/test in these charts that seems to have any measurable performance impact from bandwidth is Riftbreaker, and even Riftbreaker still performs better with lower latency than higher performance. For a game like Path of Exile or Star Citizen, bandwidth is a bit more relevant.",hardware,2025-09-26 12:00:27,1
AMD,ngtxugl,"That's what case fans do, yes. Temperature delta between inside/outside of case can be shockingly high without a lot of airflow. Unless you have a chimney setup, which is actually kind of hard to setup and get working properly...",hardware,2025-09-29 14:15:53,1
AMD,ng26rah,"You're right. I was thinking in terms of an insufficient cooler, one that makes the CPU thermally throttle. If both coolers are ""sufficient"" (as in, both coolers are capable of moving enough heat to where the CPU is not forced to thermally throttle) then they will both emit the same heat, to my understanding. The difference will be in the CPU temperature. You're definitely right on that front",hardware,2025-09-25 02:33:28,1
AMD,ngt7jsh,"any heat energy that stays on the CPU (CPU gets hotter) is heat energy not transfered outside the case. I agree the difference here would be immaterial, but technically better CPU cooler does mean hotter room.",hardware,2025-09-29 11:37:19,1
AMD,ngu9jus,case fans often have less impact than people think. the CPU fan already creates enough airflow that in a well designed case without external blocking the heat gets expelled just fine through the back-plate.,hardware,2025-09-29 15:14:46,1
AMD,ngv5bk0,"My experience is intake matters more than exhaust. You are correct in that your components will make their own air path, but intake fans are important for getting a good air circuit in the space around the case, as well as getting some airflow over board components like the chipset, RAM.  In that regard, I've found high CFM intakes to do the best job.",hardware,2025-09-29 17:47:23,1
AMD,nekcv7r,"Guess AMD will keep doing new am4 releases till am5 dies lmao. I wonder what happened with the whole bios problem to support new am4 releases? I have a gigabyte A520M DS3H - REV 1.3 and[ its support page doesn't even list the 5500x3D](https://www.gigabyte.com/Motherboard/A520M-DS3H-rev-1x/support#support-cpu), which is the CPU I want to upgrade to from my 4500.",hardware,2025-09-16 18:04:16,76
AMD,nel65b5,"5600F is a terrible name. It should mean Ryzen 5600 without iGPU, but of course the 5600 already lacks one. They could've used almost any letter except X, G and F...",hardware,2025-09-16 20:25:16,64
AMD,nelbk5r,the boost clock is quite a bit slower than the regular 5600 (4ghz vs 4.4ghz)  i wonder why they didn't call it a 5550 or something instead,hardware,2025-09-16 20:50:43,25
AMD,neklj0t,"At this point this is almost farcical. AMD hasn't designed any new Zen 3 dies in years (for obvious reasons), so they just keep releasing ridiculously minor variations of the same Vermeer and Cezanne silicon. This is what, the 8th or 9th desktop hex-core Zen 3 SKU they've released now? 😂",hardware,2025-09-16 18:46:14,49
AMD,nem55t1,How low can they go?  * Ryzen 5 5500 CPU ? * Ryzen 3 5400 CPU ? * Ryzen 3 5300 CPU ? * Ryzen 3 5200 CPU ?,hardware,2025-09-16 23:31:47,11
AMD,nel9lno,I just wish they'd just make a 5950/5900x3d. Would be a nice sendoff to AM4.,hardware,2025-09-16 20:41:32,9
AMD,nekwdwh,Somehow Palpatine Returned,hardware,2025-09-16 19:38:36,4
AMD,nellyxr,So disingenuous to say am4 is still “supported” with these niche releases no one ask for,hardware,2025-09-16 21:43:16,6
AMD,nelns3j,"who even cares about this? This fake ""extending life"" story is getting old.",hardware,2025-09-16 21:52:52,7
AMD,nekuqbf,"This is fine, the 5600T, 5600XT, 5800XT and 5900XT were actually very bad for AM4, cause AMD just withheld perfectly functional CPUs and applied a meaningless overclock to them to sell them as a new SKU for a higher price.       This one seems to be actually made from defective 5600 dies so this is totally fine, the only other option is to throw them away cause they can't match the spec of any existing SKU.",hardware,2025-09-16 19:30:29,4
AMD,nekl143,At this point amd is just selling defective chips for am4. Way to milk the people.,hardware,2025-09-16 18:43:52,5
AMD,nep6r93,0 days since last AM4 CPU reveal,hardware,2025-09-17 13:10:27,2
AMD,nekbfh3,If older motherboards gets updated BIOS I see the new AM4 CPU as a win.,hardware,2025-09-16 17:57:25,3
AMD,neka2ts,"Hello swordfi2! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-16 17:51:06,1
AMD,nexie2q,"I don't know why anyone would be up in arms about this. I AMD can make a new sku off of a lower binned chip and sell it at a lower price, where is the downside here?",hardware,2025-09-18 18:16:01,1
AMD,nembtfx,"All of these zombie silicon releases stopped making sense after the 5600X3D, not counting the 5700X3D because that can't clock high enough for the 3D V-Cache to really be useful. If they had just ended it with the 5600X3D or the March 2022 batch (which was the one that introduced the great CPU ever) then AM4 would have gone out with a bang in my opinion.",hardware,2025-09-17 00:10:48,0
AMD,nennynu,"Always great to have options. My guess is this will be in some budget prebuilds as well. AMD was smart to not kill AM4 in early/mid life of AM5. When you hop on AM4 and use AMD as a platform it's more likely you will continue to do so with next gen platform - so folks who go online for advice will either get the ""Go AM5 + 7500F/7600 for budget, great uplift"" or ""Skip AM5, jump to AM6"" it's a win for AMD in both cases.",hardware,2025-09-17 05:30:58,1
AMD,nekhc0w,"Does it need a new BIOS? Email support and ask.  For the 5700X3D, a new BIOS may not have released as it was supported under the 5800X3D BIOS.",hardware,2025-09-16 18:25:53,30
AMD,nel917w,"""basically the same"" CPUs don't necessarily need new BIOSes.",hardware,2025-09-16 20:38:51,15
AMD,nelj6jt,It might be way Intel decided to get in on the fun and bring back the 10th gen 10400f as the Core i5 110    14+++,hardware,2025-09-16 21:28:44,14
AMD,nekmq7o,"its likely just their strat now  no actual value parts, but keep all mistakes / seconds in a warehouse somewhere and when the next gen comes out, they become the value parts and thats it  when AM6 comes out, AM5 becomes what AM4 is, and really we have seen this already in AM5 with discounted 7000 series coming out in force from ali",hardware,2025-09-16 18:51:55,11
AMD,neo3gqa,"""new"" releases as in downbinned SKUs leftover from other production?",hardware,2025-09-17 07:58:01,4
AMD,nekwvpp,">  I wonder what happened with the whole bios problem to support new am4 releases?    Same as it always was, it was a storage capacity issue on the early boards. if your mobo doesn't say it supports it, it never will; unless you modify the bios files yourself, which is do-able.",hardware,2025-09-16 19:41:01,0
AMD,nemzfrs,was never a problem.,hardware,2025-09-17 02:30:31,1
AMD,nelrikc,AMD doing to AM4 what Intel did (and doing) to 14nm.,hardware,2025-09-16 22:13:19,-9
AMD,nely7zw,Should've use either S or T as it's once use to differentiate the standard clock SKU or reduced clock SKU.,hardware,2025-09-16 22:51:47,13
AMD,nema79c,"I would say Ryzen 5 5500X or 5550, but then the problem is that it clocks lower than the 5500.   Higher clocks but APU core with less cache and only PCIe Gen3, or Full fat Vermeer with all the (standard) cache and PCIe Gen4 but in its lowest clocked form. Pick your poison.",hardware,2025-09-17 00:01:12,15
AMD,neo3eag,5600O it is.,hardware,2025-09-17 07:57:21,9
AMD,neo34aj,"The F, is for Fuck intel.",hardware,2025-09-17 07:54:33,5
AMD,nf16ctc,Tbf isn't it intel who uses the F naming scheme to denote there being no IGPU. I'm not saying the naming isn't stupid i am just pointing that out.,hardware,2025-09-19 07:26:57,1
AMD,nelzg91,"Hmmm, I assume that it retains the 5600's higher L3 cache amount? Looks like the specs in the link provided don't confirm this tho..",hardware,2025-09-16 22:58:53,12
AMD,nemamaf,5500X or 5550 would have been a more understandable name... but then a problem is that it clocks lower than even the 5500. So then it's a battle between full fat Vermeer but clocked in its lowest form vs. Cezanne with half the cache and only PCIe Gen3.,hardware,2025-09-17 00:03:39,9
AMD,nemqd8x,"These are just chips that failed binning. When you're using a manufacturing process that have good yields, your binning only gets a few that don't pass the requirements in each bactch. It takes time to amass stock good enough to sell as a lower part.  This is pretty much why we don't see Ryzen 3 these days. AMD doesn't have enough to sell because TSMC has a really good yields for them.",hardware,2025-09-17 01:35:52,40
AMD,nelppno,What could have been if the zen4 and zen5 ccd are compatible with am4 io die,hardware,2025-09-16 22:03:17,6
AMD,neufl1l,"They get praise for it, so it works for them. It would be one thing if they were pushing new heights of performance (like 5800X3D), but no, this is a PR push that works for them somehow.",hardware,2025-09-18 06:43:40,2
AMD,nem9dmv,"5500 already exists, it's a 5600G without the IGPU. it was rather unpopular in its first six months of existence, but later on it got cheap and underwent a bit of a redemption arc from what I have seen.     as for the Ryzen 3 parts, if those ever come into fruition then it's far too late to to helpful, given that I've seen the 5500 for as low as $60 brand new.",hardware,2025-09-16 23:56:19,19
AMD,nepupam,"Iirc, AMD did have an engineering sample that's a Zen 3 part with x3D on both CCD's.",hardware,2025-09-17 15:12:59,5
AMD,neoqgai,"I honestly don't think we're going to get that but I would be totally on board for Ryzen 3 5400X3D because we never got a ryzen 3 normal chip out of the 5,000 series except on mobile this would be perfect.",hardware,2025-09-17 11:28:05,2
AMD,netjghk,"That would be the perfect upgrade for me right now, really want to upgrade my 5900x but sadly the x870 boards are still a little to expensive here on brasil.",hardware,2025-09-18 02:35:27,1
AMD,neo2m38,"Im sure there's data in there somewhere behind the decision.    I just upgraded to a new AM4 CPU, and I have no intention of running AM5, but I've been saving up for the big build once AM6 comes out (or a year after)   Im sure there's many people who feel the same way about their existing AM4. Worth upgrading the CPU but not worth upgrading mobo + ram",hardware,2025-09-17 07:49:27,3
AMD,nemavyd,What's fake about it? It's still incredibly popular due to the continued support.,hardware,2025-09-17 00:05:17,6
AMD,neufque,Are you kidding? People love this,hardware,2025-09-18 06:45:08,2
AMD,nepvbrh,"Imo, the main benefit is that the CPU's are still in production, which puts enormous pressure on the used market and increases the supply overall in the long run. And as new generations come out, the last generation prices get lower and lower.  Often, the fastest generation for a socket ends up with a steep premium on the used market after production ends, but Zen 3 has been sold for long that there's a massive supply of it. I wouldn't be surprised if there's more units of Zen 3 in existence than all prior generations combined.",hardware,2025-09-17 15:16:18,0
AMD,nel99q1,Isn't the 5900XT just a weaker 5950X?,hardware,2025-09-16 20:39:59,13
AMD,nelere6,"* 5900XT = 16C, cheaper 5950X * 5800XT = heavily discounted after the 5800X went OOS  Right now the 5800XT is still available for about the same price as the 5500X3D, and the 5500X3D is crippled enough (6C @ 3.9GHz) that it wouldn't beat the 5800XT in many games.",hardware,2025-09-16 21:06:14,9
AMD,nembixc,"the 5600T was especially pointless, I have not seen ONE out in the wild (i.e. any physical stock at retailers) or let alone someone buying them. I think I saw an amazon listing for a 5600T once, but it was rather sparse.",hardware,2025-09-17 00:09:05,2
AMD,nel6o9i,None of what you said makes 5600T etc bad. These are AMD's products not ours they are free to do with them whatever they want.,hardware,2025-09-16 20:27:44,-2
AMD,nekwgap,"All less-than-highest-end CPUs are ""defective"", with fuses burned off to disable parts of the chip that didn't pass QC to sell it as a lower model tier. Both Intel and AMD do this. Hell, nVidia, too. Their lower tier cards are often higher tier models with less shaders and a lower clock.  It's not a conspiracy.",hardware,2025-09-16 19:38:56,55
AMD,nekxcmk,"you want them to throw away the chips instead? so you want higher prices for the non-""defective"" chips.",hardware,2025-09-16 19:43:17,21
AMD,nelds3y,Literally how all chips from the second down are. They aren't throwing away something that costs them money because a core or two is defective or doesn't meet clock targets.,hardware,2025-09-16 21:01:23,12
AMD,nelktya,Every single Ryzen CCD chiplet is quite literally either a QC-failed or downbinned Epyc CCD.,hardware,2025-09-16 21:37:19,10
AMD,nemupo9,"You are clueless . You have less than zero idea what you are talking about. ""cant clock high enough for 3d v cache to be useful"" it was 5% slower at maximum than the 5800x3d, and at the price difference already become the better option between the two long time ago.",hardware,2025-09-17 02:01:23,12
AMD,nekyjr7,The 5600x3d never got a release either. It works fine since it's just a 6 core version of the 5800x3d. The 5500 is the 6 core version of the 5700x3d,hardware,2025-09-16 19:49:05,19
AMD,nekzsgy,"Some SKUs do. eg The 5500GT needed an update to add support despite being just a -300mhz base clock 5600G.  Afaik none of Gigabyte's A320 chipset boards support it, except for the ""A320"" boards which use relabelled B350 chips and so used bioses from the more frequently updated B350 lineup.",hardware,2025-09-16 19:55:00,12
AMD,nem8vz4,"14nm+++++++  (one for each 14nm re-release, that being Skylake, Kaby Lake, Coffee Lake, Coffee Lake-R, Comet Lake, Rocket Lake, and now Comet Lake 5 Years Later)",hardware,2025-09-16 23:53:27,8
AMD,nelnn2o,"They forgot the ""motherboard compatibility"" part.",hardware,2025-09-16 21:52:07,4
AMD,neq4msr,There's only so much cutting edge TSMC production to go around so if you can serve parts of the market with the old nodes it's better than those customers going to competitors.,hardware,2025-09-17 16:01:07,1
AMD,nema79p,Pretty awful comparison. Supporting a platform for a decade certainly isn't like anything Intel have done or will be doing anytime soon.,hardware,2025-09-17 00:01:12,8
AMD,nf1tqrc,"There already exists a 5600T, so I suppose we're left with 5600S.  At least, according to your argument.",hardware,2025-09-19 11:06:52,3
AMD,nf0vpva,"5500X3D isn't a Cezanne but they called it 5500X3D anyway, so there's that",hardware,2025-09-19 05:50:11,1
AMD,nf17v6q,"Nope, AMD do the same since they started adding iGPUs to Ryzen with the 7000 series. For example the Ryzen 7500F is a slightly weaker 7600 without iGPU; there's also Ryzen 7400F, 8400F, 8700F and more.",hardware,2025-09-19 07:42:05,2
AMD,nerhp03,Same cache as 5600. Official spec pages confirm it  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-5000-series/amd-ryzen-5-5600f.html  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-5000-series/amd-ryzen-5-5600.html,hardware,2025-09-17 19:54:04,2
AMD,nerqmtl,"I went from a 5500 to a 5700X about a year ago. Upgrading to AM5 would have been much more expensive, I think I'll also upgrade soonest in a couple of years. The performance is fine, but I'd love to have more and faster PCIe lanes.",hardware,2025-09-17 20:36:09,0
AMD,neno4p8,"because this is CPU from 2020, which is also gutted and downclocked to oblivion?",hardware,2025-09-17 05:32:30,6
AMD,neo3wzu,theres no continued support. this is just AMD cleaning out the trash bin.,hardware,2025-09-17 08:02:36,7
AMD,nenh8rq,"5600F, 400mhz downclocked, already aging 5600. Just buy a 12100f and call it a day. Both are dead platforms anyway",hardware,2025-09-17 04:35:43,7
AMD,neltnef,"You're right, I mis-remembered",hardware,2025-09-16 22:25:14,1
AMD,nen2vtc,I just built a system with one for myself.  I've found the 5600T to be more readily available than the 5600 and its 1$ more expensive. I guess it depends on the countries we live in,hardware,2025-09-17 02:52:33,2
AMD,neltvnh,"True, it's a correct business decision, but it's still bad for the customers.",hardware,2025-09-16 22:26:31,-3
AMD,nel3t9p,5090 is literally a defect. It is only 90% working. The full working die is in those 10k dollar gpu called rtx 6000 pro.,hardware,2025-09-16 20:14:19,28
AMD,neo3t84,i want them to stop pretending they are doing this to support the platform.,hardware,2025-09-17 08:01:31,0
AMD,neoq3ja,"No the bigger problem with a320 boards most of them wasn't the slowness of the update it was that they put the smallest possible bios chip on there so they can't add new CPUs to a lot of the boards without starting to remove the old ones they started off by removing the old bulldozer based APUs, but if they wanted to add the newest Verizon that have to start dropping Ryzen 1000 and possibly 2000 support.",hardware,2025-09-17 11:25:32,4
AMD,neq6nax,"yeah, its an optimization of the stack that AMD did that I think Intel needed to do for the turnaround, esp with chiplets that can scale up and down.  I think its a good idea if the pricing was good, that means bottom of the market naturally gets better over time if your top end is pushed.  Which is strange why they haven't been doing the same thing for GPUs esp as they exit top end GPU market, but hey, what do I know.",hardware,2025-09-17 16:10:54,1
AMD,neo2oaw,"This isn't ""supporting"", it's refreshing. Besides Intel also recently released i5 10400 as  Core i5-110.",hardware,2025-09-17 07:50:05,3
AMD,nf1h09q,Oh i was unaware of that sorry for being stuoid and trying to correct you i usually know everything about the about any PC hardware that is somewhat recent i just have somehow managed to not see anything about the existing of any F skew AMD CPUs.  Did they release them a good bit after the initial launch or something?  I agree with you that having a 5600F is just stupid naming they should have just called it like a 5500L or something for low power or low clock speed or something.,hardware,2025-09-19 09:13:42,1
AMD,nerkhsg,"Sweet, thanks for confirming that!",hardware,2025-09-17 20:07:22,1
AMD,nenvf3t,People still want them though. Developing countries especially aren't buying AM5 any time soon,hardware,2025-09-17 06:38:51,-1
AMD,neq7g9w,I don't know what you call continued support but my x470 motherboard just got a security fix bios update yesterday and an AGESA update bios last week.,hardware,2025-09-17 16:14:45,1
AMD,nescgpj,"> CPU releases yesterday  > ""dead platform""  Pick one.",hardware,2025-09-17 22:27:19,1
AMD,nel4u7w,That's the dirty non-secret of computing. Everything is a lower-tier defect if you're buying consumer grade products. Those chips in your new NVMe drive? C-Grade compared to what gets sold for 1000x more in server grade parts.  Expensive consumer SD Cards and CPUs and Ram are still shit tier compared to the highest cuts of the wafer.,hardware,2025-09-16 20:19:09,18
AMD,neo3zr0,"developing countries are buying am5, mostly because AM4 is barely produced anymore. So if you arent salvaging some old used board you are getting am5.",hardware,2025-09-17 08:03:22,8
AMD,nenwy96,7400F with some A620/B650 board isn’t that much more expensive,hardware,2025-09-17 06:53:19,2
AMD,neudbjm,I dont think you know what continue support is if you think security fix for bios is continued CPU developement.,hardware,2025-09-18 06:22:40,1
AMD,nesuq8m,"This 2020 board gets security BIOS updates and no would call Z490 a ""live"" platform.  https://www.gigabyte.com/Motherboard/Z490-GAMING-X-rev-10/support#support-dl",hardware,2025-09-18 00:11:18,0
AMD,neskm98,"The last actual new cpu release that wasn't just a refresh for am4 was the 5800x3d in 2022. Nothing that has released after that has actually been anything new to keep the platform alive, AMD does this to just sell off old inventory, not to keep a dead platform alive.",hardware,2025-09-17 23:13:32,4
AMD,neufw00,"Its a dead platform. Arrowlake refresh is coming, but people still call it a dead platform. core i5 110 based on 10th gen just launched. No one calls it reviving the dead platform.",hardware,2025-09-18 06:46:26,0
AMD,nel6vsa,"Its not dirty, you don't have to buy them.",hardware,2025-09-16 20:28:43,-4
AMD,neo06lb,DDR4 and AM4 boards are dirt cheap from places like Ali Express,hardware,2025-09-17 07:24:51,3
AMD,ney7ssc,5700X3D and 5600X3D both offered something for those interested in 3D vcache but didn't have the budget for a full 5800X3D.,hardware,2025-09-18 20:17:17,1
AMD,neymc36,"Sure, they are sticking to old designs, but stop and listen to yourself. If it's being kept alive, how is it dead?",hardware,2025-09-18 21:27:35,1
AMD,nf81dgc,"Looking back, the 20 series aged incredibly well",hardware,2025-09-20 10:15:28,79
AMD,nf8ni5h,If I didn't need a laptop I'd still be running the 2070. Was such a beast.,hardware,2025-09-20 13:02:39,12
AMD,nfaszdh,"\>only 69% upvoted  Yeah, reddit absolutely hates it if you even suggest that the 9060 XT 8GB is actually a very good value card for the price. I've seen people unironically recommend a 3060 12GB over it just because of the extra VRAM which as you can see from the video, is just silly. They should have included the B580 in the video as well. It doesn't make sense for it not to be there, it's the same price range as the 9060 XT right now.",hardware,2025-09-20 19:43:06,38
AMD,nfrr9tt,"Once again, we have a video clearly putting AMD over Nvidia, and the comments are just pro-Nvidia posts getting upvoted endlessly while basically every pro-AMD post gets downvoted into oblivion.  Do people think they're clever or something? Like it isn't blatantly obvious what's going on here?",hardware,2025-09-23 14:14:58,3
AMD,nf8a4ka,I don’t get why AMD would rather push an 8GB 9060XT for like $230–260 instead of a 9070XT at $650. Pretty sure the 9060XT still costs them significantly more than half of what the 9070xt does to make.,hardware,2025-09-20 11:30:53,16
AMD,nfeze6o,"and now, the conclusion, but please have some patience, because before we tell you that amd is clearly the winner in not to long words, we will  have to remind you in a 3 minute take how bad amd did in the last years and never where competetive at msrp even if this msrp never reflected real prices.  thanks steve.",hardware,2025-09-21 13:34:27,3
AMD,nfag892,"a well done comparison. only missing intel and current pricing value charts, though mixing used and new prices would be a headache.. and eh, not exactly a current buyer's guide unfortunately.",hardware,2025-09-20 18:36:20,3
AMD,nfccws0,The 20 series cards are far from obsolete but the 9060xt kills it for the value  on new cards,hardware,2025-09-21 01:02:20,2
AMD,nf8z2kl,do we even need a comparison?   AMD pretty much gave up Radeon discrete graphics. Its not like AMD are flooding it with good price/performance and pushing market share aggressively now.,hardware,2025-09-20 14:07:42,-14
AMD,nf8dvwx,Because Turing has the same level of hardware features as current gen Consoles plus AI upscaling. Especially with games built with RT haven't fully arrived en masse.  We probably haven't even maxed out Turing optimisation probably. I feel Pascal and older need to be fully ditched. RTX 20/RDNA q/2 along with nVME SSDs need to be the baseline TBH. Optimisation is being held back.,hardware,2025-09-20 11:58:56,60
AMD,nf86q49,"Still using one! the RTX Titan, might be a special case but a 2080Ti would still be just as usable. It's fine, I'll probably sell it soon though given the silly value it still has and replace it with something more modern.  Kid has a 2060 Super PC as well and it's mostly fine",hardware,2025-09-20 11:03:09,14
AMD,nf8vtpr,It will be one of the influential GPU series of all time.  Completely reshaped the graphics landscape. Also the first mass market GPU with AI acceleration.,hardware,2025-09-20 13:50:26,28
AMD,nf8a11j,"Yeah the 2080 Super is an absolute workhorse. I sold mine and got a 4080 about three years ago, and then repurchased one a few months back for my wife’s rig. She still is hitting 90+ fps on 1440 decent settings",hardware,2025-09-20 11:30:05,20
AMD,nf97ug5,"After the DLSS2 update came out, it was pretty obvious that Nvidia had a technology that AMD needed to match as soon as possible. It was really only after FSR4 came out that I could seriously consider buying an AMD GPU, with the last time being in 2013.",hardware,2025-09-20 14:53:08,21
AMD,nfhnkhv,"1000 series released and all were happy. 2000 released and no one cared about the new features. I would say it didn’t age well 2 years ago, but does today where 1000 and bellow are screwed by missing said features.",hardware,2025-09-21 21:07:53,3
AMD,nf95kl5,Who could have seen this coming? (Everyone but tech youtube).,hardware,2025-09-20 14:41:30,11
AMD,nfam37a,"Swapped my 2070 Super for a 9070 XT, and honestly if it wasn't for 4K gaming or MH Wilds it would have still kicked ass. It'll be perfect for a racing sim or light editing rig later, but right now it's wasting away inside a music production PC. Killer card",hardware,2025-09-20 19:06:32,3
AMD,nf8255d,Pre and Post crypto boom cards all aged well because    1. Performance uplifts since are terrible and   2. Every company tried their asses off to flood the market with infinite demand. So 2nd hand market of rdna2 and ampere cards are dirt cheap and dragged down rtx 20 and rdna1 prices with them.,hardware,2025-09-20 10:22:56,1
AMD,nfa46sa,only aged well because there was no innovation until blackwell  if anything this should tell you that nvidia could have acted sooner on these technologies but strung everyone along until they could justify selling a 70 class GPU for $600,hardware,2025-09-20 17:35:23,-13
AMD,nfd4w38,I've seen people who are budgeting for 5070 get told they should get the 9060XT 16gb because it's 'more future proof'.  Literally talking people into buying cards that are 35-40% slower than what they budgeted for because VRAM lol. It's mind boggling.,hardware,2025-09-21 04:10:56,32
AMD,nffhnwr,In some regions B580 is quite a bit cheaper. Here for example 9060Xt 8GB starts from 340€ while B580 can be had for 290€. 5060 slots in the middle at 315€.,hardware,2025-09-21 15:09:43,3
AMD,nfbvi1r,Yeah it's weird to not include Intel when they're *the* budget option.,hardware,2025-09-20 23:16:30,5
AMD,nfezlm8,"lmao you are smoking crack if you think 8GB vram is enough. i can't play at some games at native with a 3080 10GB and a 3080TI with 12GB VRAM still probably would be unplayable  [this is tw3, a 2015 game with ray tracing added on, eating up 9.5GB  at 1440p](https://i.imgur.com/hinVGHw.jpeg). it should need at least 12-14GB for 2160p ultrawide, so i'm stuck with monitor upscaling. an 5k2k DLSS ultra performance, low settings, with my 10 GB gives same FPS with single digit 1/0.1% lows",hardware,2025-09-21 13:35:36,-1
AMD,nfnl915,Because most consumers will balk at dropping $650+ on a GPU.,hardware,2025-09-22 19:49:24,4
AMD,nf9upkr,"And who is going to be buying 9070 XT's? The only thing it offers over the 5070 Ti is a price advantage and street pricing has been more favorable towards Nvidia with NV cards showing up far more often at MSRP than the 9070 XT has. The smaller the NV tax, the smaller the reason to go AMD.  NV also has the Borderlands 4 promotion going on now, so even in my case where I can walk into Microcenter and get either one of these cards at MSRP, if I value the BL4 promotion at all, the pricing gap has now shrunk in favor of NV.",hardware,2025-09-20 16:49:29,9
AMD,nfd4ctx,"9070XT is not selling well even at $650. Every single Microcenter has had the $650 Reaper card for while and inventory isn't going down. What is selling and where inventory is going down is the MSRP 5070ti.  9070XT had it's opportunity to sell well, really well at the MSRP starting a few months ago.......now it's a too late. You'd need to bring it at or even below MSRP to get more people steered towards it. That's just what the situation is for AMD right now.",hardware,2025-09-21 04:06:52,3
AMD,nfalsno,">do we even need a comparison?   Evidently so. The 9060 XT 8 GB is the best option in this price range—it's 22% faster than the 5060 at 1080p and 1440p ultra, and it's regularly $20-30 cheaper.",hardware,2025-09-20 19:04:59,21
AMD,nf9qb9d,"The 9070 XT and non-XT are some of the most competitive AMD cards in years. They're better value than the 5070 Ti and non-Ti, and even rival the 5080 in some games.",hardware,2025-09-20 16:27:34,8
AMD,nf8kewq,Pascal is ending driver support next month so there’s that,hardware,2025-09-20 12:43:28,32
AMD,nf8xccd,2080ti user here. BL4 running at low settings on an ultra wide at 90fps. Had to download frame gen for it to improve from 40s. I'm still debating upgrading like you said to pull some of the value out. Waiting on 5000 supers,hardware,2025-09-20 13:58:40,7
AMD,nfbe0gr,People whined hard at the time with how useless this was. But they aged so well.,hardware,2025-09-20 21:34:48,17
AMD,nf92gr0,It is BY FAR the most revolutionary arch since unified shaders.,hardware,2025-09-20 14:25:22,21
AMD,nf8k0m0,how’s the 4080 going for you,hardware,2025-09-20 12:40:53,4
AMD,nf9hcqf,> it was pretty obvious  You'd think so but the amount gaslighting has been absolutely insane. Pretty much 100% tech reviewers straight up refused to _properly_ test the technology for over a year and people who'd never seen it run on live hardware just kept making stuff up arguing with each other.,hardware,2025-09-20 15:41:56,25
AMD,nfbtcle,"I remember seeing that Death Stranding DLSS 2 trailer and thinking ""man I got to get a 3xxx series card this shit is the future"".",hardware,2025-09-20 23:04:02,2
AMD,nfbjp5n,And that makes it even sadder because so many people parrot YouTubers it gave AMD a free pass for dragging their feet.,hardware,2025-09-20 22:06:56,7
AMD,nf85898,"Not true.  If you buy a RX 5700 \[RDNA 1\] today it will come with many compromises that will make them not worth it even on used market anymore such as not supporting DX12 Ultimate and proper Upscaling such as DLSS.  Whereas the RTX 20 series they aged like fine wine despite being so hated back when they got released. No reviewers other than Digital Foundry saw the future of Upscaling and DX12 Ultimate features, everyone is stuck on Rasterization only matters mindset.",hardware,2025-09-20 10:50:31,40
AMD,nfa1tvk,None of the AMD cards in the last 3 generations have aged well. All of them were significantly lacking in features which at the time many downplayed the significance of but it has come back to bite them.   Whereas on the Nvidia side only the VRAM crippled cards have not aged well. But that's something one could have seen coming from a mile away.,hardware,2025-09-20 17:23:49,21
AMD,nf83tra,Not because of that. Because of RTX and upscaling.  The 2060 Super and 5700 were nearly the same price. But the 2060 Super would have given you a significantly better experience due to supporting DLSS upscaling. Even if you ignore all the other features that came with RTX,hardware,2025-09-20 10:38:28,18
AMD,nfhp32y,"I remember seeing people showing 3060 12gb vs 3070 8gb and 70 series absolutely make sense.   In my experience (1660ti 6gb) i had to lower settings anyway to hit framerate that was relevant. Sure 16gb had been fun on that card, but loading settings that require that much would push me to 20fps or less anyway, because it just ain’t faster.      I’ll gladly enable DLSS / fsr and many are the same. I would always pick the higher tier.     With that said, 8gb is really not a lot today cause most new gpu DO have enough punch to atleast use up to 12gb….    Also, most buying these kind if cards, don’t play on 4K screens so the 4K vram need is rarely relevant",hardware,2025-09-21 21:15:22,4
AMD,nff09zj,"[i can't play a 2015 game with ray tracing not because i don't have enough horsepower, but because manufacturers are cheaping out on VRAM](https://i.imgur.com/hinVGHw.jpeg)  this is the witcher 3 1440p RT high/ultra with DLSS balanced btw. 2160p RT low with DLSS ultra performance needs at least 12-14 GB VRAM to stop hitching",hardware,2025-09-21 13:39:27,-6
AMD,nfc9qqt,"What is weird is suggesting people buy a product almost noone has, with who knows what support, history of bad drivers from a company that is collapsing",hardware,2025-09-21 00:42:46,4
AMD,nfdhu1g,Because that makes AMD look bad.  The point of this video is for AMD to make Nvidia look bad.  They are called AMDUnboxed for a reason.,hardware,2025-09-21 06:00:11,0
AMD,nfhu7sb,"> lmao you are smoking crack if you think 8GB vram is enough  I literally use an 8gb card. Did you even watch the video?   > this is tw3, a 2015 game  I don't know why you're emphasizing the year of release as if it didn't get a heavy next gen update just 2-3 years ago. It's almost like if I took half life 2 rtx and made a heavy emphasis on it being from 2004 or oblivion remaster being from 2006 as if the original release year has fuck all to do with how heavy the shit they piled on top of the already existing game is.   Regardless, I can run that game on my 8gb 4070 laptop, a substantially less powerful card than yours with less VRAM and capped at like 105 watts, at 1440p medium with ray tracing on dlss balanced and [it's still getting 45-50fps with no frame generation](https://i.imgur.com/BbkvmVE.jpeg). It's playable without stutters. You know what I'd do if it got to the point of being unplayable? And I know this is extremist: I'd turn a setting or two down. Fucking crazy right? That not everything has to run at 4k240hz max ray tracing all the time, especially in the budget category? Turning off the ray tracing alone doubles my fps, which is what I personally would do in this game if I ever got around to actually playing it. I don't think you know what ""unplayable"" means.  > it should need at least 12-14GB for 2160p ultrawide, so i'm stuck with monitor upscaling. an 5k2k   The fact that you're even bringing up **4k ultrawide** in a conversation about a **budget** card that has been selling for **$225-270** is telling me that I'm not the one ""smoking crack"" here.",hardware,2025-09-21 21:41:37,15
AMD,nfkrjw2,"No, this is in fact a 2022 game that your screenshot is from.",hardware,2025-09-22 10:40:29,3
AMD,nfah08z,>And who is going to be buying 9070 XT's?  Linux users who want a hassle-free experience.   edit: WTH. I keep getting downvoted for answering a query. I have a 9070 XT on my gaming PC & a 4060TI on my AI/SFFPC.,hardware,2025-09-20 18:40:13,20
AMD,nfe2kz3,> NV also has the Borderlands 4 promotion going on now  unfortunate considering 9070xt is much better in that game than the 5070ti in fact 9070xt is better than the 5080 in that game  So you save more money by getting a 9070xt over 5080 for that game despite the promotion,hardware,2025-09-21 09:18:55,4
AMD,nfc2492,A $650 9070 XT is not as bad of a value proposition as you think vs a $750 5070 Ti. Borderlands 4 notwithstanding,hardware,2025-09-20 23:55:32,3
AMD,nfc9yuu,The 9060xt is 60-70€ more expensive,hardware,2025-09-21 00:44:07,-6
AMD,nfa3mk9,"No, the fake MSRP killed that narrative in less than a week.",hardware,2025-09-20 17:32:35,14
AMD,nfkruyu,they are not better value than the 5070ti. the 5070ti is much more performant.,hardware,2025-09-22 10:43:07,1
AMD,nfa2rnl,"I have an old laptop with Pascal GPU, still kicking in pre-UE5 games and especially in very extensive indie and smaller games library. ""Fully ditched in new games"" and end of drivers support is not the same thing and kinda sucks... That means not even a slight support and security updates?",hardware,2025-09-20 17:28:22,8
AMD,nflv6dc,"My 1070 just died. It went with dignity, while still supported. Rip",hardware,2025-09-22 14:44:06,2
AMD,nf8yqgk,"Yeah, 5070 Super seems like the obvious upgrade if the specs turn out to be true. I think I can still get £500 from the Titan thanks to people craving the VRAM which seems ridiculous given its age but who am I to judge, I don't need it but would like a comfortable 18GB card.",hardware,2025-09-20 14:05:58,5
AMD,nfbsu2o,"because how small the jump in raw raster was from 10 series, and how much prices shot up  the 1k+ 2080 ti was a shocker for the top end buyers, and then if you went for 2080 as normal, you got a huge % difference in performance because its just like the 5090 vs 5080 were on entirely different chips that was massively cut down vs salvage die of 30 series.  the 20 series was a massive cash grab, make no mistake. the DLSS being good is a much later development.",hardware,2025-09-20 23:01:03,16
AMD,nfrnz5x,That's probably because of how dogshit tier dlss 1.0 was. Couldn't even swap the dll out 😭,hardware,2025-09-23 13:58:22,2
AMD,nfbif8z,"That's because for some odd reason the new generation of media (YouTubers) don't know the history of PC features. Unlike consoles where major shifts in visual fidelity and performance happens per generation. On PC they come out almost nilly willy and the userbase is fragmented on what their hardware supports.   Now forwarding looking concepts like ray tracing (a holy Grail type of feature to PC gamers of the 90s), YouTubers didn't like it and kept slamming Nvidia for working on it while praising AMD for charging consumers more for raster performance.   These hacks, HUB, still stood by their RX 5700 XT recommendation when they did a retrospect video during the mesh shader nonsense.   People finally going to wake up? AMD and YouTubers have been selling/promoting products with less features but almost equal prices.   RDNA4 finally makes things equal and it's AWOL thanks to AMD resource allocation.",hardware,2025-09-20 21:59:35,10
AMD,nfa85pn,Nvidia weren't kidding by calling it *Graphics Reinvented*.,hardware,2025-09-20 17:54:56,11
AMD,nfrp5aa,Yep. 8800gtx was absolutely mindblowing. Unlike previous gens you never thought performance was being kept off the table because either the pixel or vertex shaders weren't getting fully utilized.,hardware,2025-09-23 14:04:20,1
AMD,nf8peff,"Can’t complain, I basically play high/max settings and average 100 ish fps in demanding games like Rust and Escape from Tarkov, anything with lower reqs is maxed with normally 144fps as I have a 144 hz monitor",hardware,2025-09-20 13:13:57,10
AMD,nfbtho4,"if you had an early 20 series card like I did, DLSS 1 was bullshit on anything not 4k, and using it to hit 4k60 was the most it could really do and you better have it on quality scaling and not anything farther.  DLSS2 came out later, and more in line with the 30 series launch (not exact, 2 came out earlier), that allowed it to be used for 1440p scaling and 4k scaling at more than just quality setting (IE performance or even ultra performance for some games where its really good on).",hardware,2025-09-20 23:04:51,10
AMD,nf86mbh,No bad gpus only bad prices. Rdna1 isnt being hailed as a dx12ultimate game machine right now. Same with rtx 20 series. You wont be able to enjoy the games that require the tech that dx12u requires. Maybe except for 2080ti. Which you paid 1000 bucks at 2018 (1200 of todays dollars) and yes it aged well as hardware. But it still wasnt worth it waiting 6+ years for tech to mature with a 1000 dollar sticker price. And with rt features it offered at launch. Push back is completely justified for an uncooked featureset.,hardware,2025-09-20 11:02:16,-12
AMD,nf88k01,">No reviewers other than Digital Foundry saw the future of Upscaling and DX12 Ultimate features, everyone is stuck on Rasterization only matters mindset.   It was just Nvidia's marketing, quite frankly.  DF's track record isn't nearly as squeaky clean as one might think.  They were the only ones at the time who were ""impressed"" by DLSS version ""1.9"" in Control (basically DLSS 1.0 with minor tweaks), which wasn't entirely dissimilar to FSR 2.0 and ran on standard FP32.  Yet, they kept comparing it with PS4 Pro's vastly inferior checkerboard rendering technique to present it in a positive light.   In hindsight, it was 'probably' IGN that was twisting their arms behind the scenes, but that's hardly an excuse for bad (or at least biased) journalism.",hardware,2025-09-20 11:18:16,-19
AMD,nfa5cu4,"Rasterization is good though. Nvidia is pushing for software gimmicks to act as makeup for game developers. Just like how developers decided to make games bloated because of storage, developers will make games increasingly shittier to run outside of DLSS and frame gen.   I have to wonder if there's an ulterior motive by nvidia to have game developers make their games less optimized on middling hardware so that they can upsell GeForce Now.",hardware,2025-09-20 17:41:02,-13
AMD,nfa4l7l,AMD is currently working to make FSR4 reverse compatible. They will age like fine wine.,hardware,2025-09-20 17:37:20,-14
AMD,nf8k0gl,"Can't help but wonder what the 40 and 50 series will look like in 5 years, and if it'll be the same scenario. I bought a 4070 Super a year ago, with a 7900gre being another option I don't regret avoiding. If AI horsepower is more important than VRAM , which in expecting will happen again, and only get more severe, I made the right decision.",hardware,2025-09-20 12:40:52,10
AMD,nf841w1,DLSS gets you so far when baseline performance isnt good. Thankfully they are good cars from 20 generation.,hardware,2025-09-20 10:40:26,-15
AMD,nfgpi3n,"..........and I rest my case. This ladies and gentle is the perfect example of what's wrong with today's mentality and increasingly impossible expectations.  Ray Tracing is Ray Tracing buddy, doesn't matter if you added it to a game from 2015, the computational power needed isn't going away because the game is old. ESPECIALLY if you're doing 4K low RT or 2K Ultra RT....even with DLSS> No 8gb card has enough computational power to give meaningful FPS, even today, and this holds even more true for prior gen cards...and anything that does have that power now to run these settings has the higher VRAM.  But let me guess, you think XX60 class cards are middle high end cards with ""enough horsepower"".",hardware,2025-09-21 18:33:54,11
AMD,nfkrdst,Witcher 3 does not have RT or DLSS. You are talking about the re-release from 2022.,hardware,2025-09-22 10:39:04,3
AMD,nfh3pj6,I'm not suggesting people buy it. I'm suggesting it be included in a comparison video.,hardware,2025-09-21 19:36:47,3
AMD,nfcnh80,"Intel ARC won't get any better if no one buys it doofus, no one would recommend Battlemage cards if they're truly dogshit either btw. Might as well buy only NVIDIA with that logic.",hardware,2025-09-21 02:09:15,-3
AMD,nfdiylh,Extremely curious to hear how a card that is weaker at the same price point while also having that CPU scaling problem somehow makes AMD look bad,hardware,2025-09-21 06:10:27,-5
AMD,ngmwj4m,"when i read that the 5080 supers with 24 GB would delayed and likely scalped, i ran outta patience. i upgraded from my ""budget"" card i bought for $500 used in 2023, when TW3 ray tracing dropped and miners and AI farmers took all the GPU stock. i'm sorry my card is only worth $250. 3080 ti 12GB MSRP $1200 owners can also eat shit according to u  [my recently purchased 5090 shows 16 GB at 4k is still not enough](https://imgur.com/a/N5pdHu3). ah, i wish money could buy happiness...and reading comprehension, since i clearly stated with piss-low settings i'm still VRAM limited. turn down settings my ass, read how i fucking did that going to 1440p.",hardware,2025-09-28 12:01:29,1
AMD,ngmsh0l,"i upgraded to a 5090 recently and found out my [2022 4k game needed 14GB for piss-low+RT 5k2k settings, 18 GB for high-ultra + RT, and 20 GB for high-ultra + RT + FG](https://imgur.com/a/N5pdHu3). so the 5080 is for gamers okay with mediocre from the start, no?  and they say money cannot buy happiness...if it allows to me to avoid a corpo bottleneck, by all means. i grew up in the era of vendors putting [excessive VRAM on GPUs to farm money from idiot consumers](https://www.amazon.com/Glorto-GeForce-Profile-Graphics-Express/dp/B0CMX8XWHD?th=1) so i'm not gullible",hardware,2025-09-28 11:29:00,1
AMD,nfai9qe,So a number of users so inconsequential it's not even worth considering in terms of manufacturing output for these GPU's.,hardware,2025-09-20 18:46:39,17
AMD,nfbevty,"Yeah no, i run 4090 and 5070 it in my two Linux builds. Both are hassle free. And have been so for years.",hardware,2025-09-20 21:39:43,6
AMD,nfkrpib,So a whole 5 of them. Hardly a market to design a product for.,hardware,2025-09-22 10:41:49,4
AMD,nfctj1z,That's the 16 GB model. This video compares the 9060 XT 8 GB against the 5060 and other cards that also launched with an MSRP near $300.,hardware,2025-09-21 02:48:55,7
AMD,nfcc4hb,"depends on the country, imo if you can get a 9070XT for ~15% cheaper than the 5070 Ti it's completely worth it",hardware,2025-09-21 00:57:30,6
AMD,nfdkhem,There are currently three 9070 XT's for sale *by* newegg for $669 right now. Compared to the $789 for the 5070 Ti.   I could understand why someone would buy the 5070 Ti over the other if they were always the same price. But for less? I'm taking the 9070 XT. And I did.,hardware,2025-09-21 06:24:21,3
AMD,nfm950l,"It's absolutely not ""much"" more performant. The 9070 XT is a few percent behind in a broad range of games and it pulls ahead in quite a few.",hardware,2025-09-22 15:51:26,2
AMD,nfa3ao3,only security updates   > GPUs will only receive driver updates for critical security items beginning October 2025 and continuing through October 2028,hardware,2025-09-20 17:30:57,22
AMD,nfa7zb0,Yeah true but I think 2027 should be the absolute latest Pascal should be supported.,hardware,2025-09-20 17:54:02,1
AMD,nfkrhh0,"Yeah, and really DLSS and RT didn't get good until the time of 30 series and those GPUs had really nice performance uplifts for the higher half of the stack (tho 3060 and lower definitely was short changed, other than 3060 having lots of vram for what it was) so buying 20 series to future proof really wasn't exactly the move unless you needed the upgrade THAT generation and couldn't wait. Then again 30 series was ruined by the boom so I guess either way not great.",hardware,2025-09-22 10:39:56,4
AMD,nfbt7wq,> That's because for some odd reason the new generation of media (YouTubers) don't know the history of PC features.  It's interesting that LTT - generally mocked here - was one of the few channels to get this point early on while the more 'serious hardcore' channels had to be dragged to it/still are not really understanding.,hardware,2025-09-20 23:03:17,16
AMD,nfbrzwn,"That makes no sense... Reviewers review for the current buyer, not the age like fine wine dealie. Most people should be buying for now, not the promise of possibility, the ones who keep saying fine wine in a serious way are a joke for sure.   The pricing of the 20 series at launch was ASS, even with hindsight here, RX 5700 XT was 400 bucks, which it trades blows on average with the 2070 that was 500 dollars (although, at the time it was HEAVILY game dependent, where some it would outpace very well others just die). The exact trade off that gets made later, but this time at a much more acceptable price point, 100 dollars less on a 400/500 dollars product is actually sizable, not so much on a 1000 dollar product.   The lack of features for DLSS didn't matter until really 30s gen when the updated DLSS came out, and really until the latter point of it or 40 series when it got mass adoption.  If your upgrade cycle is 2 cycles, it was a good idea and then pick up a 40 series because by then DLSS really did come into its own, and AMD's lack of competition then was an issue (that they worked to address with PS upscaling that turned into FSR4)",hardware,2025-09-20 22:56:10,-9
AMD,nf8t4v3,1.0 soon. It will be perfect!,hardware,2025-09-20 13:35:29,2
AMD,nfrphtf,Even then @4k it was still shit. I remember trying it with ffxv and tasting sick in my mouth. Looked so bad.,hardware,2025-09-23 14:06:06,1
AMD,nf8bofv,"An RTX 2060 Super will play games like Indiana Jones The Great Circle at optimized settings paired with DLSS 4 Upscaler, with RX 5700 you need to do swap out to an entirely different Operating system as well as many mods just to make it run on a non DX12 Ultimate compatible GPU.  Nope. They aren't the same, the RTX 20 series Turing aged far better than RDNA 1 RX 5000 series.",hardware,2025-09-20 11:42:54,37
AMD,nf95yu4,"> No bad gpus only bad prices.  I hate this truism because somebody could say ""This card isn't even worth buying for 50 dollars because of outdated features/support"" and you would just smugly say ""well then the price is bad isn't it"". It's a functionally worthless phrase that is just a worse version of 'supply/demand'. It's a thought terminating cliche almost.",hardware,2025-09-20 14:43:30,18
AMD,nf8k0tx,"> DLSS version ""1.9"" in Control (basically DLSS 1.0 with minor tweaks)  DLSS 1 was a spatial AI upscale, DLSS ""1.9"" was a temporal upscaler, they were nothing alike, not ""DLSS 1 with minor tweaks""",hardware,2025-09-20 12:40:56,21
AMD,nf8azdb,"First off even DLSS 1.9 looks far better than the FSR 2, FSR 2 only looked close when comparing between standstill image which gave some false analysis which most of the tech journalist were guilty off including Hardware Unboxed. And DLSS 1.9 is barely relevant to use anyways as that version of DLSS AFAIK only were used on a single game.  Everything else eventually used DLSS 2, which on comparisons has proven itself to be much better as well compared to FSR.  Second IGN at the time doesn't own Digital Foundry they were under Eurogamer at the time and by then they were still independent, they were only purchased by IGN back on 2024 and then Digital Foundry eventually bought their independence back off them just recently this year.  Some people like you keep saying that they are biased or bad journalist, when in reality they seem to be the most mature of them all especially compared to the likes of Hardware Unboxed, Gamers Nexus and every other tech tuber which seems to rely on gamer rage click bait to get views nowadays.  No offense to them I still appreciate their content and still watches them to this day. But any day I prefer the likes of Digital Foundry which focuses more on technical state of the games they are reviewing also, plenty of game developer interviews that gives us more insight on how the future of visual game rendering technology is going to be in the future, which they correctly predicted with Upscaling as well as Ray Tracing, DX 12 Ultimate features in general rather than the Hardware rasterization focused only.  Which gave us some false insight on what the future is going to be like. Back in 2019 everyone is focuses on rasterization only matters, upscaling, ray tracing bad train.  6 years later it's very obvious which became the standard, and this is something like subreddit like r/pcmasterrace will never admit, I guess.",hardware,2025-09-20 11:37:34,18
AMD,nfa4v7p,"That's not going to happen. They may choose to call it ""FSR 4"" but it will look nothing like it.",hardware,2025-09-20 17:38:40,13
AMD,nf91730,PS6 might full AI with fsr4 or fsr5. Fsr4 was created in collaboration with Sony. Sony vision is most likely going for ML upscaling with their upcoming consoles,hardware,2025-09-20 14:18:44,-1
AMD,nf87ae0,"That's not how that works.   Again, the point is it aged a lot better. You basically got a ""free"" \~30% performance boost over the AMD counterpart.   Fundamentally the 20 series cards are technologically not too different from today's cards. That's why something like a 2080 Ti is still pretty damn good, and even a 2060 Super is perfectly usable (around RX 6600 XT performance, but with upscaling, so real world way better experience).   Meanwhile let's be honest, the 5600 XT or 5700 are like stone age tech. And even the RX 6600 cards are worse than the 20 series cards. It's just crazy.",hardware,2025-09-20 11:07:52,13
AMD,nfe0jyu,Well.. yes? Thats what everyone does. Where I am maybe 5% have amd. Its not.our responsibillity to fund corporations so they can make better products. It goes the other way,hardware,2025-09-21 08:58:33,7
AMD,nfh1fnk,> Intel ARC won't get any better if no one buys it  Will nobody think of poor Intel?,hardware,2025-09-21 19:26:42,3
AMD,ngst9qk,"A 5080 -  a 4k card, is not going to run its best on 5k, yes. 5k is such a niche application of course noone is going to consider it their performance target.",hardware,2025-09-29 09:31:35,1
AMD,nfaq26q,AMD has already settled for a tiny market share. Linux might not be an irrelevant proportion of it.,hardware,2025-09-20 19:27:28,12
AMD,nfe0fld,You are right my bad,hardware,2025-09-21 08:57:20,1
AMD,nfqmri1,I wouldnt call more than a generational different a few percent.,hardware,2025-09-23 09:44:29,0
AMD,nfkuqa0,"yep, and 30 series was a time of grab what you can. MSRP was not a thing if you brought off the shelf, everything was inflated to fuck.  I was able to sell a 999 2080 TI black edition (IE the shitty ones from evga) for 900 in person all cash, and that was doable because I got a 3080 ti aio one from evga from the queue and that was a small enough outlay that I just went for it.  it was a wild time and the only time I ever flipped a GPU because how stupid things were.  The 20 series was a shitshow, it started the DLSS tech sure, but the gen itself was a shitshow of uplifts, and very much not worth buying at the time because the tech was just so not good at the time. sure, DLSS means that if you buy a used 2080 it would be able to do a lot more than something without DLSS like a 1080 ti (which had close enough perf but drastic cheaper at the time) or if you want 2070 vs 1080 ti that would have had a way bigger win on 1080 ti on raw raster but the dlss means you can try and stretch the thing that much longer.",hardware,2025-09-22 11:06:13,3
AMD,nfcvbuo,"It's crazy seeing people upset they can't run new features. Man, that's what made reviews for new generations and refreshes exciting.  New feature introduced or fixed/improved, or both, and you were just constantly in awe. Now good portion get snake oiled into ""raster is king"" and they try to burn down the company that is moving the needle while PC gaming gets stuck in console version of generational shifts, ie too damn long between hops!  Had AMD got at least pushed to put hardware in RDNA2 at the earliest, we'd have way more RT optimized games, but nah let's not even do it for RDNA3!   RDNA4 and with it FSR4 is out and suddenly ""FSR3 looks like ass!"" And the gimmick is now a demanded feature to be backported.",hardware,2025-09-21 03:01:17,14
AMD,nfkqjv7,LTT is doing this long enough to remmember how it works from personal experience.,hardware,2025-09-22 10:31:49,3
AMD,nfq30kp,Probably because the “hardcore” channels only care about fps benchmarks and competitive online shooters. They don’t really care about tech innovation.,hardware,2025-09-23 06:03:58,1
AMD,nfctsrg,"> Reviewers review for the current buyer, not the age like fine wine dealie.  Yes, so a product that is within 1-5% of another product in raster, with more features for the same price is dismissed because of ""gimmicks"".  https://tpucdn.com/review/amd-radeon-rx-5700-xt/images/relative-performance_3840-2160.png  Option A: $400, 95% raster, Tensor Cores, DLSS 1.0 (Garbage), Ray Tracing (limited support)  VS  Option B: $400, 100% Raster....  > RX 5700 XT was 400 bucks  The 5700 XT was announced at $450, targeting the 2070, but AMD got blindsided by the Supers, leaving it trailing a now $500 2070 Super, tying a $450 2070, and duking it out with a now shifted up in price 2060 Super because AMD got greedy and was forced to price cut before launch.  https://www.techpowerup.com/257106/confirmed-amd-to-cut-rx-5700-series-prices-at-launch  > The lack of features for DLSS didn't matter until really 30s gen when the updated DLSS came out  And what did AMD respond with? RDNA2 and STILL no hardware solution, they went with software and continued to get clubbed like a baby seal. And what did reviewers do? Wouldn't mention DLSS as a competitive feature because it wasn't apples to apples, leading to directly misinforming their audience through omission, and would actively downplay ray tracing whenever possible.  EDIT: fixes",hardware,2025-09-21 02:50:45,14
AMD,nfrrvij,"the thing is, it is usually say 40 fps vs solid 60, and usually that was enough of a trade off even if it as you said, looks meh.  depending on the game, that is a worth it trade off at 4k at least to me, but yeah, DLSS1 wasn't great and if you weren't on 4k it had no use for you really.",hardware,2025-09-23 14:17:56,1
AMD,nfkqzup,Its nonsense. There are bad GPUs. GPU that burns your house down is a bad GPU. Even if its free.,hardware,2025-09-22 10:35:43,0
AMD,nf8lkmy,"I think there needs to be a distinction between technological foresight vs buying advice. They don't necessarily overlap.  DF definitely had a better read of future trends in the industry. But at the same time, as a consumer in 2019, making purchase decisions based on games and features that wouldn't have materialized until years later wasn't a particularly wise move.",hardware,2025-09-20 12:50:45,3
AMD,ngtb7v2,4k= niche  ultrawide = niche  u = world's expert,hardware,2025-09-29 12:03:11,1
AMD,nfay0j6,"I suspect it's still an irrelevant amount since AMD's benefits on Linux are often gaming oriented, which based on Steam Hardware Survey is only about 2.64% and heavily driven by Steam Deck and similar hardware using AMD APU's rather than discrete GPU's.  If you're using Linux in a work enviornment, the value of CUDA is so enormous that even under Linux you're still seeing massive Nvidia use. ROCm is not a remotely competitve option.",hardware,2025-09-20 20:09:40,1
AMD,nfhj3y2,RDNA2 did introduce dedicated raytracing hardware.,hardware,2025-09-21 20:46:40,0
AMD,nffe43z,">Option A: $400, 95% raster, Tensor Cores, DLSS 1.0 (Garbage), Ray Tracing (limited support)  All of this was true. You guys are rewriting history: DLSS1 was not good, and like 2 games had RT.  >And what did reviewers do? Wouldn't mention DLSS as a competitive feature because it wasn't apples to apples  Every review I've seen since 2021 mentions DLSS vs FSR, including the channel in the OP.",hardware,2025-09-21 14:52:10,-1
AMD,nfcwcgh,"Again, on 20 series launch, it had support for BF V and Metro, it was a gimmick and wasnt until DLSS2 it was good, and only really mass adopted by new games by later on in 30 gen.  Again, it was a gimmick.  And right, the 400 dollars was fighting a 450 2070 or a 400 2060 S or 500 2070 S  again, for the two games it supported, 3 if you counted control added later on, DLSS was not a huge thing and was a gimmick.   I brought a 2080 ti to run a 4k screen because I brought one super early to pair with a 980 ti, and really it wasnt until way later I can use DLSS to actually help running with games, the first major one where I felt it was actually helpful and necessary was launch cyberpunk 2077 partly because that was when it was starting to struggle to hit 4k60 with games, and partly because that was when DLSS was getting to the point where it looked good enough to properly upscale even at 4k compared with DLSS 1 that was a joke.  I am telling you as someone who has brought them and lived thru the time, DLSS 1 was a joke and a half, just like FSR pre 4.",hardware,2025-09-21 03:08:24,-5
AMD,ngtuvzm,"4k is not niche, though its still only a significant minority. Ultrawide is a niche and i hope it stays that way. Me = aware of statistics and intended use.",hardware,2025-09-29 14:00:23,1
AMD,nfhllqo,"Just like FSR, it was a light version that, like tessellation, aimed at a ""more efficient"" approach, efficient meaning it doesn't cripple our hardware.  Which was fine, but still insufficient to close the gap by any significant margin. If anything it muddied the water back to ""tessellated unseen oceans"".",hardware,2025-09-21 20:58:22,5
AMD,nfcy7ud,"> Again, it was a gimmick.  Fine, it was a gimmick. That doesn't change, for the same price a buyer could pick the product with the gimmick, and end up no worse if the gimmick fizzled out but far better if the gimmick succeeded.  AMD had barely any advantage at the same price point, so going down the features list to devalue having MORE FEATURES isn't a serviceable recommendation.",hardware,2025-09-21 03:21:41,10
AMD,nfkqpty,AMD did the same mistake with tesselations. If you remmeber Witcher 3 launch showed this to a very wide audience. One game having a bug where underground water was tesselated (later removed with patch) is hardly proof that tesselation was designed to cripple hardware.,hardware,2025-09-22 10:33:18,1
AMD,nfd0iw0,"I dont know what to tell you if you think a card with 2070 performance for the price of 2060 S is not good enough.  at that point the difference between cards are small sure, but that was still a half tier down.  not to mention, if you looked at actual card prices then, nvidia being nvidia had far more AIB cards not at MSRP than AMD did (reminder this is the time the FE was more expensive than ""reference"" models, by 100), for better or worse that is. As AMD's MSRP-ish cards could have issues like weak cooling / ram cooling while nvidia is better with some nice designed stuff but again you are paying for it while the ""reference"" stuff are largely okay but not spectacular but at least you wont see random 100C memory temps on them.",hardware,2025-09-21 03:38:23,-1
AMD,nfd20kn,"> I dont know what to tell you if you think a card with 2070 performance for the price of 2060 S is not good enough.  I believe you are missing my point. I'm not saying the 5700 XT wasn't a good product, I'm saying the 2060 Super was a better product SOLELY because of the features that reviewers called ""gimmicks.""  What sane person would pick an option with less features for the same price? Worst case scenario features never get used, you end up same as if you picked other product, best case scenario features get used and end ahead likely ahead of if you picked other product.  And this was JUST for RDNA1. For RDNA2 it got even worst because now you had DLSS 2.0 and actual RT games with passable usage/effects, and AMD still sat on their hands. And by RDNA3 you can no longer even defend this, yet reviewers still didn't want to admit they were wrong, they weren't gimmicks anymore but we had to sit through the gaggle of ""fake frames"" stupidity.",hardware,2025-09-21 03:49:24,11
AMD,ng9mr06,"Their SKUs are likely all the same size, but they bin them to account for the relatively low yields of these modern foundry nodes  Also reduces engineering complexity and increases reuse in the design",hardware,2025-09-26 07:19:17,58
AMD,ngahfir,The die is relatively large as well at 287mm2 for X2 Elite it can't be cheap to produce on N3P and with On Package Memory which caused Lunar Lake issue margin issues will OEMs take risk ?,hardware,2025-09-26 11:53:35,15
AMD,nga2pbv,"Yeah this always confuses me with Qualcomm. Seems a bit wasteful, but I guess it lets them scale different models without having to redesign the chip. You don’t really see this with Apple or Intel.",hardware,2025-09-26 09:58:44,11
AMD,ngcispq,"Increasing the bus width definitely increases die space, but not by much if you start from an already big die. To make a cut-down version with a narrower bus, you first need to weigh in the cost of the redesign (although probably not huge), the cost of a full maskset for validation + most likely a new set for production, the cost of a new package design and manufacturing, the cost of the reduced economy of scale now that your top of the line and the lower tier do not share the same package, so the volumes are smaller for each, versus the cost of additional wafers due to the wasted die area if you stick with one single die for all SKUs. If you don't foresee to sell a lot of lower SKUs versus the top ones, it may not be worth going with two different dies.  EDIT: Also, the cost for faulty dies: if your bottom tier SKUs use a different die, you now can't bin down a partly defective high-end die into a lower SKU and instead you need to toss it. That's cost as well.",hardware,2025-09-26 18:10:35,4
AMD,ngf3vhh,"Binning, wafer allocation, and mask costs all argue in favor of using a single design for several SKU's.    It's one thing to say you can make a smaller die and get more per wafer, but it's another to put up the hundred million dollars plus it takes just to make the masks for that die.  Then you have to figure out how many wafers to make of one die versus another.  There's also the design costs, though I expect those would contribute less than the other factors to making a decision like this.  And, of course, you're really overestimating how much space on the die those memory PHY's are taking up.  The memory bus difference probably accounts for somewhere around 5% of the die at most.",hardware,2025-09-27 02:59:20,5
AMD,ng9vjm0,"They probably don't expect sales to be flat across the SKUs. If they're weighted more towards the upper end one, could make sense. ~~Alternatively, has it been confirmed they're only making a single die? If there are actually two, could explain it better.~~ [Edit: specs would probably rule this out]  Could also be a cost play. Higher end memory configs significant increase platform cost. Might be that they're expecting a number of OEMs to make that tradeoff, but they still want to advertise higher peak perf. Also possible it makes the lower 2 SKUs drop-in compatible, while the higher end one is a separate platform.",hardware,2025-09-26 08:48:26,8
AMD,ngb8dnz,"According to die photo such as this one https://www.techpowerup.com/327130/qualcomm-snapdragon-x-elite-die-exposed-and-annotated?amp the memory are pretty small periphery circuits.   I'm not sure if they segment the packaging to save running the extra 64 traces and bumps, that is probably more expensive than the die area",hardware,2025-09-26 14:25:48,2
AMD,ngls6a2,"When it comes to cost, you need to consider:  Case A (different design)  * Design & validation cost * Entire chips that would be wasted since its unbinnable  Case B (same design binned)  * Wasted die area on every non-flagship chip  Case A has a lot more upfront cost (2 designs, 2 validations, 2 contracts) and uncertainties (what if yield is low? what if memory is needed later down the line?), while case B is safer overall since in the grand scheme of things, memory controller is a small part of a die",hardware,2025-09-28 05:42:53,1
AMD,ng9mur1,It's signal integrity thing. Lane voltage transition from 0 to 1 or 1 to 0 level isn't instant.,hardware,2025-09-26 07:20:18,-12
AMD,ng9vfg7,"What? We're talking N3, which is mature now. Even on relatively new nodes, you wouldn't expect to routinely cut 1/3rd of your memory bus.",hardware,2025-09-26 08:47:16,31
AMD,ngav3na,This is more of a packaging issue than binning.   FWIW yield have gone up consistently with modern nodes if anything.,hardware,2025-09-26 13:16:00,6
AMD,nganfok,"Yea, 287mm2 3nm is a HUGE jump up from 173mm2 4nm  Its only 2.3x faster GPU perf, so seems like the GPU is still only a tiny 3 Slices? Just upgraded from the 8g2 GPU arch to the 8Eg5 GPU arch & clocked higher?  Qualcomm's new P cores should be smaller from the node shrink, and the additional E cores+sL2 cluster should only be roughly ~10-15mm2  I don't understand where all the silicon went  Edit: I just did a die size estimate myself, I think Andreas used incorrect LPDDR5X packages dimensions  I got about 215mm2 using [LPDDR5X packages dimensions of 12.5 x 7.1mm2, same as in LNL](https://www.techpowerup.com/img/Ocdpo16i9bxAEUlk.jpg)  215mm2 is more in-line with what I'd expect based on the specs we currently know",hardware,2025-09-26 12:32:00,19
AMD,ngaukg4,Do you have a source for that die size number?,hardware,2025-09-26 13:13:01,3
AMD,ngaym51,"Sacrificing a memory controller in the big scheme of things is far more efficient than having to spin a different die design for each SKU.  If anything Qualcomm is the less wasteful.   Apple and Intel do that at even larger scales BTW. E.g. M-series Max dies for example have all 16 scalar cores and 40 GPU cores, even though most common SKUs only have 14 scalar and 32 GPU cores enabled.  Similar thing with Intel, for many designs the i3,i5,i7 SKUs were basically the same die.",hardware,2025-09-26 13:35:09,7
AMD,nga5pka,"Apple literally does have several configurations for each tier.   For instance: the M4 alone has a 9-core CPU configuration for the base iPad Pro, an 8-core CPU and 8-core GPU configuration for the base iMac and a 8-core GPU configuration for the base MacBook Air. All of them can be brought with the full-fat CPU and GPU config (which is 10 cores for each) but of course at a additional cost.",hardware,2025-09-26 10:25:24,6
AMD,ngk9nv8,Someone else posted a die shot. It looks like the entirety of the PHYs make up less than 5% area.,hardware,2025-09-27 23:26:32,1
AMD,nga8flo,"How unlikely is it that the memory controller actually supports lppddr6 as well, thus requiring a 192bit bus?",hardware,2025-09-26 10:47:58,2
AMD,ngcpdm8,What are the chances their anemic GPU can't saturate the full width and there's simply no benefit except at the high end for maximum memory capacity?,hardware,2025-09-26 18:43:19,0
AMD,ng9o82j,Can you explain this further?,hardware,2025-09-26 07:33:58,1
AMD,ng9xljo,"The binning is not about memory bus, but cores, caches and the clock speeds they can achieve.  Having the same memory bus for all of  your SKUs allows you to bin along your entire product range",hardware,2025-09-26 09:09:14,25
AMD,ngavwsk,"Core count has gone up 50%, and caches/register files are also larger while SRAM hasn't scaled down that well from 4n to 3n for TSMC (and everybody else really).  It also has more PCIe lanes and a larger NPU. I don't know if they have integrated baseband on these SKUs (I read they were planning to a while back).  I am surprised they didn't prioritize GPU on this gen, since it was their big Achilles heel for 1st gen (on Oryon family).",hardware,2025-09-26 13:20:27,15
AMD,ngb0y4x,"They didn't say it was 2.3x faster, they said 2.3x performance per watt.  So I seriously doubt they are just clocking the GPU faster (the least power efficient method of increasing performance).  The NPU got a lot more performance as well.  So increased area for GPU/NPU/Memorybus.",hardware,2025-09-26 13:47:40,4
AMD,ngaop2c,They went from 12 cores to 18 cores   And the P cores are using less dense transistors to reach 5Ghz   Hopefully they announce Snapdragon X Plus by CES too with their 12 cores which should have a die comparable to last gen,hardware,2025-09-26 12:39:41,6
AMD,nganowd,Memory bus it hogs die area as well also 80TOPS NPU can't be Cheap.,hardware,2025-09-26 12:33:34,4
AMD,nga9qjv,everyone does binning but the point here is designing one extra 64-bit memory interface just for 1 SKU seems excessive,hardware,2025-09-26 10:58:11,11
AMD,ngbzvos,"To my understanding, the way LPDDR6 structures its channels would make it impossible to translate the bus width 1:1 with LPDDR5. You'd have the option of 128b LPDDR5 vs 192b LPDDR6.",hardware,2025-09-26 16:39:55,4
AMD,ngar4e1,"Very unlikely. When lppddr6 commercially releases it will be slower and more expensive than lpddr5x, it will likely take two more X Elite generations until lpddr6 is more viable.   Additionally the overhead to validate the platform for lpddr6 and to integrate compatibility is to high just for it to be a gimmick in maybe 18 months or so.",hardware,2025-09-26 12:53:47,0
AMD,ng9q1pr,"The PCB, the copper wires may have resistance, capacitance or electromagnetic crosstalk/interference between each other. That's the reason why Strix Halo can't use LPDDR5X on a CAMM stick and the chips have to be around the SoC, very close to it.  Then there is binning of the chip. It may be that they went for a tradeoff of a cheaper designs in exchange for only perfect chips being stable at such bandwidth and speed and/or they could have designed a wider bus with the ability to fuse it off to 128-bit when defects show up. Like console chips have slightly more GPU cores that what you get in the product. They checked statistics and added extra cores to handle cores disabled by defects.",hardware,2025-09-26 07:52:22,5
AMD,ng9xwq2,"> The binning is not about memory bits, but cores, caches and the clock speeds they can achieve.  The X2 Elite (higher end, X2E-88-100) and X2 Elite Extreme (X2E-96-100) have the same core counts, both for CPU and GPU, as well as cache. It would not make sense to cut down the memory bus by 1/3rd just because of a couple hundred MHz.  Notice that essentially every client chip you can name ships with its full, native memory bus, regardless of other binning.  Edit: Added SKU numbers for clarity",hardware,2025-09-26 09:12:23,25
AMD,ngblhrl,Don't forget power and thermal envelope.,hardware,2025-09-26 15:30:10,2
AMD,ngd7x18,"Oh my bad, 2.3x performance per watt is far better  That would explain the jump in die size if the GPU is saying 2-3x larger",hardware,2025-09-26 20:15:48,1
AMD,ngau52h,"Oh true, it'll be interesting to see the 8Eg5 vs X2E core size difference for the reaching 5GHz  I hope Qualcomm doesn't nerf the X2 Plus' clock speeds so much this time",hardware,2025-09-26 13:10:40,2
AMD,ngam7vt,"M4 Max also has a group of memory controllers being disabled for it’s base config, cuts down the memory bandwidth from 546GB/s to 410GB/s",hardware,2025-09-26 12:24:25,2
AMD,ngcpndl,> 128b LPDDR5 vs 192b LPDDR5.   Is one of those supposed to be a 6?,hardware,2025-09-26 18:44:39,3
AMD,ngao1dh,"> It would not make sense to cut down the memory bus by 1/3rd just because of a couple hundred MHz.  But they want 3 SKUs with notably different performance. A few hundred more MHz will barely move the needle in benchmarks, but 50% more memory bandwidth will.  > Notice that essentially every client chip you can name ships with its full, native memory bus, regardless of other binning.  CPUs, yes.    But GPUs have been doing memory channel based market segmentation for decades.",hardware,2025-09-26 12:35:43,9
AMD,ng9z2p4,"On the website, it shows 3 versions, the last had 12 instead of 18 cores and 34 instead of 53MB cache.  The other two differ in clock speed and memory bandwidth. The bus width could be their way to put the SKUs in the desired performance brackets. Or they have something in their memory subsystem design that results in relatively low yields when going for full bandwidth.",hardware,2025-09-26 09:23:58,3
AMD,ngav0im,"Most likely they will still reach 4.6Ghz, last gen gen had tons of issues   Even the lowest X Elite SKU reaches 4.7Ghz in ST",hardware,2025-09-26 13:15:30,3
AMD,ngcgzni,"The M4 max is a larger chip with a much larger 512bit memory bus that is only cut down on one config, and cut down proportionally less vs X2 cutting it on all but one. That's not normal.",hardware,2025-09-26 18:01:38,3
AMD,ngcqqmr,"Err, yes. Second one. Fixed now. Apparently I got my autocorrect to learn LPDDR5, but not LPDDR6 -_-",hardware,2025-09-26 18:49:59,1
AMD,newtdst,"TL;DW - An OEM only CPU called the FX-4200 has four modules with four integer units, four floating point units, and access to all the cache. In synthetic FPU benchmarks, it dominates over the FX-4350. In gaming there isn't much a difference.",hardware,2025-09-18 16:18:04,104
AMD,newyyw4,Phenom 2 was pretty damn good and AMD might have held up better by doing a die shrink refresh of it rather than gambling on the weirdness of Bulldozer.  It might have saved them some desperately needed money too that could have been better invested in a good successor architecture. I'm fairly sure the only reason the shared fpu thing happened was to save die space and therefore money as AMD were up shit creek at the time financially.,hardware,2025-09-18 16:44:53,43
AMD,nexgjx9,"It's an interesting oddity, I had never heard of the FX-4200  2011-2013 was still so single-thread dominated I don't think this difference would have even registered to most, especially when Intel was still pulling off insane die shrinks that gave them 20% more performance for 20% less power  But these videos are also a reminder that FX series didn't age as badly as it benchmarked in those years, at least.",hardware,2025-09-18 18:07:11,17
AMD,newu2xv,That 40 seconds at the start of the conclusion felt like it 10 minutes long.,hardware,2025-09-18 16:21:23,14
AMD,nexll67,"It seems like the high inter-module latency is hurting the 4200 in some cases which causes the 4350 to have better performance. I remember reading something from chips and cheese I think that showed the inter-module latency was somewhere over 300ns. I think this is also why Microsoft changed the scheduler behavior in Windows 8+. In windows 7, it loads the first core in each module first essentially treating it like a CPU with SMT. In windows 8 and newer the scheduler keeps threads on the same module if it thinks they have a lot of inter-core traffic.  I honestly don’t think the construction core design wouldn’t have been that bad if the L1 and L3 cache weren’t so poorly performing alongside the poor 32nm Global Foundries node. 28nm wasn’t much better either from Global Foundries. I would’ve liked to have seen a full blown desktop Steamroller and Excavator chip with 4 modules and L3 cache.  AMD learned a lot with these designs and carried over quite a bit into Ryzen.",hardware,2025-09-18 18:31:24,7
AMD,ney7svz,Almost went for a fx 8350 back in the day before I even knew anything about their weirdness. Somehow ended up with a G3258 instead. Was so impressed with their clocks vs intel.,hardware,2025-09-18 20:17:18,7
AMD,nexx63k,I had a 3 core i believe! Tried to enable the 4th but it wouldnt boot lol.,hardware,2025-09-18 19:27:07,4
AMD,nf2yn4b,"When I was hesitating to upgrade, ryzen was already put but I gave my PC one last hurrah by buying a used fx-9590 lol. Then I had issues with two games that had issues with FX CPUs causing massive FPS drops so I upgraded to the ryzen 3900x and my FPS in certain games literally doubled.",hardware,2025-09-19 15:00:41,1
AMD,nfpm7rm,"I had an FX-57 in 2005. Played CoD, Eve Online and WoW on it for years. It was a beast!",hardware,2025-09-23 03:30:45,1
AMD,nf0297g,"Wrong question, what if Intel didn't sabotage multiprocessor development.",hardware,2025-09-19 02:23:20,-1
AMD,nex12iy,Really curious where the 8 core variants compare to the 4c4m versions in the synthetic benchmarks as it would be 8 core for integer tasks but quad core in fpu tasks essentially  I know when I ran ESXi on my desktop FX-8120 it ran pretty good so long as I set the core preferences for each VM,hardware,2025-09-18 16:54:44,21
AMD,nex7rod,"Llano was technically a 32nm shrink of Phenom II, but with a rather big IGP slapped in. I recall than it had very poor frequency scaling due to having used a process to favour the GPU density over CPU clock speed, so it didn't left a good impression.",hardware,2025-09-18 17:26:11,29
AMD,ney3f86,"From a marketing perspective too I think Bulldozer benefited too. A 4.2 GHz ""quad core"" FX-4350 sounds great for $122. Sort of like how 3 GHz Pentium 4 sounds better than a 2.2 GHz Athlon 64.",hardware,2025-09-18 19:56:45,16
AMD,ney0ndb,"It was also designed to hit 6ghz stock, where it would have easily beaten Phenom and all the design compromises would have made a ton of sense. The actual silicon ended up not coming close.   Edit: I'm not sure where I read/heard 6ghz specifically. However if we take the (very safe) assumption that they intended to beat Phenom's performance, and it was 33% narrower with the corresponding IPC loss, then the intended clock speed would need to be 40-50% higher.",hardware,2025-09-18 19:43:53,12
AMD,nf01te6,"Honestly there's a fairly straight line AMD could have taken to evolve the Stars cores into something akin to Zen (or just hold down the fort until Zen). Bulldozer was a tremendous waste of resources (during some lean years as well). Llano showed that even with a VERY light touch, it was trivial to squeeze another 5 to 10 percent more IPC out of Stars, even though the focus of that 32nm shrink was the iGPU. They could have bought a little time by making a Phenom III by slapping some L3 cache back on there and running 6 cores by default like Thuban. Then the next step would have been to widen things a bit and implement all the instructions they had fallen behind on. The next step would have been a uop cache and SMT and well.... you're looking at something mighty close to Zen1, assuming you could fab it on 28nm since 20 wasn't going to happen.",hardware,2025-09-19 02:20:46,11
AMD,nf0shx3,Wasn't bulldozer competing with sandy bridge at that time? It want even close.,hardware,2025-09-19 05:23:23,6
AMD,nf02ehp,Thubans were great chips!,hardware,2025-09-19 02:24:12,4
AMD,nf0y96j,"Phenom II was good for the time (I had a 955BE), but I'm not sure how much farther they could have pushed the architecture. Once Intel released Sandy Bridge I don't think there was much AMD could have done at that point. It was such a huge leap that it served as the foundation for Intel for nearly a decade, with only minor tweaks. Even the original Ryzen barely matched it in IPC and certainly not in clock speed.",hardware,2025-09-19 06:12:08,4
AMD,nexg0ub,Phenoms were a beast. I got my 955 BE almost to 4ghz. Ran that thing from 2009 to 2017 with an upgrade to ryzen.,hardware,2025-09-18 18:04:39,6
AMD,nf8p6yn,"My daughter's gaming rig is still powered by a 1100T. Granted, nothing she's doing is all that Intensive, but for a 15 year old CPU, it does everything asked of it without it being obvious it's technologically ancient.",hardware,2025-09-20 13:12:44,1
AMD,ney8x26,"I barely knew about their weirdness, and that put me off, even though it was  brand new. When my MB died, I had to choose between Phenom II X6, FX-8320 and i5-2500K for basically the same price. Thought the FX was weird, the Phenom was great, but old, so I went with the i5. Best decision I have ever made in regards to hardware.",hardware,2025-09-18 20:22:25,6
AMD,nezlzb0,I was able to do it on a Gigabyte board,hardware,2025-09-19 00:50:07,6
AMD,nexagaa,"If memory serves correct, and I could be off, anandtech did a 4T / 4M test. FP perf per thread went up but was lower overall since 8T going to the what was, loosely speaking, a 4 FPUs with SMT had a performance uplift.   There was also ""typical"" scaling for INT workloads going from 4T / 4M to 8T / 4M in that it was ""close enough"" to 2x.",hardware,2025-09-18 17:38:36,15
AMD,nexbbp8,"I'd forgotten about Llano. I didn't get my hand on an A series APU until the Steamroller A-10 generation. Even with the crappie bulldozer based cpu architecture that A-10 really impressed me with its GPU performance, it was a massive jump up from the Intel IGP despite still being far below discreet GPU performance and Intel CPU performance.",hardware,2025-09-18 17:42:39,10
AMD,nezgxsj,"I know the design goal of Pentium 4 was to get higher clock speeds (to which the laws of physics said no), but I've not heard of Bulldozer being the same.",hardware,2025-09-19 00:20:36,5
AMD,nf0tecw,Bulldozer was crap. Nobody denies that.,hardware,2025-09-19 05:30:47,12
AMD,nexsnzy,"same here, it's sad that they are so often equated to bulldozer chips.",hardware,2025-09-18 19:05:25,9
AMD,neydr1w,Phenom IIs were good but the Phenom64 was pretty mediocre for the level of hype it had. I had a 9950BE which was the fastest one they made IIRC and I couldn’t get it stable past 2.8Ghz with modest air cooling. I think quad cores weren’t exactly ready for prime time back then and a core 2 duo probably would have served me much better.  When I got a 955BE later on it performed much better,hardware,2025-09-18 20:45:12,3
AMD,ng2n4ab,You hate your daughter that much? Lol,hardware,2025-09-25 04:27:10,1
AMD,neyb7si,2500k was the goat. Back when overclocking could actually give you major gains. The G3258 Actually won an overlooking competition with it on a Evo 212 lol. Such a cool little CPU wish they'd make interesting cpus like that again. Think I hit like 4.8 ghz or something like that haha. Although I jumped ship on ryzen 1700x. AM4 is the goat. Still got me a 5800x3d no reason to upgrade now. Missing being a broke kid finding the best deals,hardware,2025-09-18 20:33:06,9
AMD,nexbq9l,"Makes sense given the hardware constraints and the clock speed uplift would probably show a bit of performance improvement on FPU tasks.  I probably read that article, completely blanked on it, and then you reminded me and now I can't look at it because the site is more or less down now, which is a shame.",hardware,2025-09-18 17:44:33,4
AMD,nfk66ra,multithreading getting close to 2x performance uplift? Man whatever test they used must be horrible at feeding the frontend. Under hypothetical ideal I/O feeding SMT has a performance downgrade from overhead.,hardware,2025-09-22 07:07:41,2
AMD,nexz95t,"The whole time I had it on air too, I wonder what I could squeeze out of it with my push pull 360 aio I have now. Got it to boot at like 4.1ghz but would crash if you tried to do anything. I'm surprised I didn't melt it.",hardware,2025-09-18 19:37:12,6
AMD,nf0eho2,"Performance-wise, original Phenom was 65nm and had a crippled L3 cache capacity from die space constraints. 45nm Phenom II fixed this with triple the L3.",hardware,2025-09-19 03:38:58,5
AMD,ng43dlh,"The most CPU intensive thing she does is Minecraft, which it still happily kicks out at 60FPS, I see no reason to retire it.",hardware,2025-09-25 12:10:31,1
AMD,neyheao,"Yeah, if you put that 5800X3D on your mainboard from your 1700X, that's longevity right there. And it will be good for the next years as well!",hardware,2025-09-18 21:02:45,6
AMD,nexey20,I think it's actually this  [https://www.extremetech.com/computing/100583-analyzing-bulldozers-scaling-single-thread-performance](https://www.extremetech.com/computing/100583-analyzing-bulldozers-scaling-single-thread-performance),hardware,2025-09-18 17:59:32,5
AMD,nfmo4f6,I didn't make the test or set up the OS or any of that.   With that said... it does say something about how software was set up back in the day and was janky.   I don't think anyone is out there saying that the FX series hit all design goals. There were definitely cases where a 6T/3M FX CPU was a better choice than a 2C/4T i3 but that was based on pricing not the CPU design... and Zen was a big enough step up that it exceeded my expectations.,hardware,2025-09-22 17:02:53,2
AMD,ney07fh,"I was using an aio with it the entire time, the stock cooler i initially got with it was very noisy, I got them to send a new one, but by that time i was already using the aio; i believe i got it stable at 4.1 or 4.2 but I don't really remember right now; not sure if this was a factor but i had it paired with low-cas memory.",hardware,2025-09-18 19:41:47,3
AMD,nf2y6js,"Oh wow, somehow I never realized just how paltry the L3 cache was on the first Phenoms.",hardware,2025-09-19 14:58:31,3
AMD,neyibf2,9800x3d doesn't seem big enough bump to upgrade so I'm fat chilling. 4090 and 5800x3d is enough for me lol,hardware,2025-09-18 21:07:16,6
AMD,nexfbty,"Thanks for the link, makes sense from the hardware layout",hardware,2025-09-18 18:01:19,3
AMD,nf78dtf,"me too, but with the 5090.",hardware,2025-09-20 05:36:51,1
AMD,ng0il2o,"Test platform benchmarks for 8 Elite G5 (Android phones)    https://hothardware.com/news/qualcomm-snapdragon-8-elite-gen-5-benchmarks     No power, just perf. 3793 ST /  12287 nT in GB6 (unclear which version).   These devices can score a bit higher than shipping devices, after all the gunk OEMs put on, but should be pretty close.",hardware,2025-09-24 20:47:55,29
AMD,ng0qtng,"It looks pretty good, but really underscores how badly they messed up with gen 1, getting blown out by their own phone SoCs and leaving a bad (2nd? 3rd?) impression with the ""this time for real!"" WoA relaunch.",hardware,2025-09-24 21:29:56,41
AMD,ng0yo6j,I guess this really proves that X Elite had some serious technical issues causing high power consumption.  This is finally the perf/watt that will make Intel and AMD very nervous as this chip can take a 50% x86->ARM translation penalty and still be good enough. The only thing holding it back (for some people) looks to be GPU drivers.,hardware,2025-09-24 22:12:04,22
AMD,ng0x1iu,OPPO X9 Pro(Dimensity 9500) reaches ST 3700 and MT 10700...,hardware,2025-09-24 22:02:57,7
AMD,ng17int,"Beating Apple by that much in browser benchmarks is crazy. Speedometer is Apple's bread and butter, a very ST-limited benchmark. Crazy good, this is something where the end user will actually legit see the difference.",hardware,2025-09-24 23:03:28,15
AMD,ng0zlnt,Pretty good scores considering no SME2 support.,hardware,2025-09-24 22:17:17,5
AMD,ng0in8v,How does it compare to the A19 pro?,hardware,2025-09-24 20:48:13,11
AMD,ng0lijh,really cool stuff.   All major new high performance ARM uarchs are leaving x86 in an uncomfortable place.,hardware,2025-09-24 21:02:33,11
AMD,ng1eyxu,If these chips are really performing like how they say they are then AMD and Intel really need to do something. Qualcomm and Apple are getting gains like this every year meanwhile AMD and Intel wait years to barely have any performance upgrade (AMD at least has the x3d chips),hardware,2025-09-24 23:46:56,9
AMD,ng29vke,Basis of benchmark is Geekbench not SPEC,hardware,2025-09-25 02:53:06,5
AMD,ng2msti,What do they mean by ISO performance?,hardware,2025-09-25 04:24:41,1
AMD,ng5ofrp,That’s well and good but what kills arm on windows is windows itself not having a interpretor as good as rosetta 2.,hardware,2025-09-25 16:58:40,1
AMD,ng1pntb,"X86 really need to buck up here, and the companies that are developing such products have to stop sticking to th past legacy that consumers and everyone else will buy them. ARM is now more efficient and faster than X86 while being handicapped by the translation process. idk what to day here, even the cadence of ARM is greater than X86.",hardware,2025-09-25 00:50:37,-4
AMD,ng0jz7z,19% increase over 8 Elite in Geekbench 6  13-17% in 3DMark graphics,hardware,2025-09-24 20:54:51,19
AMD,ng0kf8z,"\>fanless designs also struck us as impressive, especially considering the Snapdragon X2 Elite an 18-core chip, featuring a third-gen Qualcomm Oryon CPU complex running at up to 5 GHz, a first for ARM64 architectures.      On the X Elite gen 2 part, that is neat, 5Ghz is nuts",hardware,2025-09-24 20:57:03,17
AMD,ng0smwh,"Usual Qualcomm use phone (Asus rog to be exact) it has good cooling but still it's a phone.   Mediatek in other hand always use Thick tablet, and score from engineer device it's hard to replicate with phone.",hardware,2025-09-24 21:39:32,1
AMD,ng0r4zj,"Yes that is true, however how bad the 1st gen was, it simply was competitive vs Zen 4 and what Intel had. Now they are simply going past AMD and Intel, it's up to the software houses now to migrate  It seems that QC is also investing a bit into enterprise software for managing devices to compete with Intel here.  Reaching 4000 ST on geekbench on a Windows laptop is impressive",hardware,2025-09-24 21:31:36,20
AMD,ng0y21g,"The 8 Elite gen 5 reaches 3850 ST and 1200 MT, beating both Apple and Mediatek handily.  But let's wait for reviews, this is a QC platform",hardware,2025-09-24 22:08:38,1
AMD,ng0kl7r,"We will have to wait for Geekerwan, but it matches the M4 in Multicore on geekbench",hardware,2025-09-24 20:57:53,22
AMD,ng1qh27,"X3D or not, the results are the same. ARM is catching on at lower prices while we have to wait 2 years just to buy a good X86 processor at probably some amount more than an ARM chip, which has WiFi and Bluetooth now integrated (like finally) and upgrade cycles are so much more frequent and actually impact full compared to X86. Geekerwan really loves the A19 and said that it's the best thing that has ever happened since A15, but what about X86 average IPC increase. If moving to a new node doesn't improve ST or MT as much, then we know our answer already.",hardware,2025-09-25 00:55:22,9
AMD,ng3e2s3,Geekbench is used to draw perf/power curves because it's far faster to benchmark and correlates to SPEC. Geekerwan explained this before   But we can see the uplift in SPEC already as geekerwan published the review,hardware,2025-09-25 08:36:52,4
AMD,ng613e5,"""Isos"" in Greek means ""the same"" or ""equal"", so 75% more power-efficient at the same performance as competitors' chips.",hardware,2025-09-25 17:57:35,1
AMD,nga88ae,Hopefully the Qualcomm GPU drivers are better on windows than last gen.,hardware,2025-09-26 10:46:21,1
AMD,ng0mhrk,"Oh, you meant to highlight 5 GHz, yes, as fanless. That will be really impressive. I assume IPC increases too, but I’ve not seen any numbers yet.   Apple’s M5 may get close to 5 GHz, but I think it’ll be just under.   For 5 GHz, power and which SKUs (IIRC, X Elite only had 1-2 very rare SKUs for the top clocks). I’d hope that is fixed this cycle and if there are truly fanless 5 GHz 1T laptops, what an achievement.",hardware,2025-09-24 21:07:35,13
AMD,ng0lej0,"Fanless is huge (for X2 on PCs). I hope it means multiple OEMs actually ship those versions. IIRC, maybe just a tablet or something for X Elite (Oryon V1).   I know most enthusiasts don’t care or actually demand active cooling in every laptop, but fanless computing ought to become the norm for consumer laptops, just like it has with phones.   Not every laptop is meant to be a performance-focused device.   Absolute silence, zero HW maintenance ever, supremely cool on your lap & in the summers, and usually fantastic battery life. With high perf/W and sipping idle power, a fanless laptop is just a pleasure to use.   I’m always surprised how warm actively-cooled laptops get when I have to switch back.",hardware,2025-09-24 21:01:58,5
AMD,ng0z4cr,"It beats Apple in MT (I'm assuming while using more power as well) while losing in ST. I don't see how thats handily.  Edit: Looking at Geekerwan graphs in Bilbili, it seems that at similar wattage, they basically overlap.",hardware,2025-09-24 22:14:34,15
AMD,ng0p0ca,With 3x the P cores??,hardware,2025-09-24 21:20:32,2
AMD,ng224ry,It doesn’t. M4 is ~14500 in multi core. Gen 5 is ~12500 with extreme cooling.,hardware,2025-09-25 02:04:44,2
AMD,ng3ekni,Geekbench has been gamed with Instructions now nonetheless,hardware,2025-09-25 08:42:07,2
AMD,nga9z9o,I want Windows on Arm on my smartphone,hardware,2025-09-26 11:00:02,2
AMD,ng0m5ns,"We won't see X2 Elite fanless IMO, too much performance left behind if they do fanless, fanless X2 for sure there will be though",hardware,2025-09-24 21:05:50,9
AMD,ng0mo7x,"Why is it ""huge""?   It's a design decision. The X1 could also be fanless, but manufacturers chose to give it fans so they perform better. It'll be the same for the X2. Give it a fan and it'll perform better.   In the end whether it's fanless or not just depends on how many watts you want to feed it. The more watts the faster.  Phones are fanless due to the form factor. That's all. I am sure you know even phones benefit from active cooling, despite their tiny <15W chips.",hardware,2025-09-24 21:08:30,3
AMD,ng0s8w0,"There's no world where a fanless device will be considered perfomant if the same device exists but with even as little as a heatsink, the difference in performance and system stability is way more important to 99.9% of people than reducing the already non-existent ""noise"" from modern systems.   A modern system will only be loud if you build it to be loud or put it in a situation where it needs to get loud, you already can go and config your system to turn off your fans and massively throttle your clocks and see how marvellous fanless is.",hardware,2025-09-24 21:37:28,2
AMD,ng10ut4,"Yeah they are basically similar. Which is big, closer than ever, but",hardware,2025-09-24 22:24:27,5
AMD,ng0q63n,"No, that's the 8 Elite gen 5 that matches the M4 in MT, the X2 Elite most likely is a bit below the 9950X     Edit: Above 9900X in MT, below 9950X in cinebench",hardware,2025-09-24 21:26:33,12
AMD,ng2dhq5,"Also to note that while 20% may not seem a big deal comparing to a ""laptop"" chip. 8 Elite Gen 5 is using M4 class power here (nearly 19W)",hardware,2025-09-25 03:17:28,5
AMD,ng3er6u,"True, but it's the same for x86 and those instructions matter, albeit not always",hardware,2025-09-25 08:44:00,3
AMD,ng0mt0s,"Oh! I misread that quote, you’re right. X2 (with fewer cores I assume) will still be great to have fanless options in the laptop category.",hardware,2025-09-24 21:09:11,5
AMD,ng0qz0s,">It's a design decision. The X1 could also be fanless, but manufacturers chose to give it fans so they perform better.  Huge if ***Qualcomm*** designs an X2 SKU specifically for fanless operation. People ought to know most Windows laptop OEMs do not prioritize on battery life. Why? If they can beat another OEM by 1-2% in a heavy nT benchmark with more power, they will.   Windows OEMs are not in the business of offering ""slower"" devices—if Dell / HP / Lenovo can unlock a higher PL1 & PL2 (base / boost) power level, they will.   Of course if you *want* to consume 2x the power for usually something like 15% more perf, you have plenty of Windows laptops to choose from. A fanless device requires more restraint.  The primary way fanless CPUs launch is through SKUs that ***can't*** be unlocked and that is a *choice* by CPU firms: Intel, AMD, Apple, Qualcomm, MediaTek, etc.  Again, I'm not saying ***you*** need to be forced to buy a fanless laptop. I'd just like more designed-as-fanless choices.  You can turn *most any* PC fanless with a custom fan curve & PL1 / PL2 limits, but it's tricky, space inefficent, and not easy for normal consumers.   Fanless-by-***design*** laptops always work fanlessly without needing to download custom tools, allow more space for batteries, zero vents, less weight, guaranteed zero maintenance, etc.  //  This shouldn't be controversial. Qualcomm themselves advocated for ***some*** fanless PCs in its [earlier Windows on Arm PCs](https://www.techpowerup.com/289530/qualcomm-expands-portfolio-with-snapdragon-8cx-gen-3-and-7c-gen-3-to-accelerate-mobile-computing):  >During the annual Snapdragon Tech Summit, Qualcomm Technologies, Inc. expanded the portfolio of solutions for Always On, Always Connected PCs with the introduction of the Snapdragon 8cx Gen 3 compute platform, designed to deliver the performance and exceptional **experiences users deserve in premium ultra-slim and fanless laptops**.  The only difference is that they had SKUs that *were* designed as primarily fanless. No OEM could unlock 2x or 3x more power for that last bit of performance.",hardware,2025-09-24 21:30:43,1
AMD,ng0zn00,">There's no world where a fanless device will be considered perfomant if the same device exists but with even as little as a heatsink, the difference in performance and system stability is way more important to 99.9% of people than reducing the already non-existent ""noise"" from modern systems.  Yet most consumers use their fanless MacBook Airs M1 / M2 / M3 / M4, their performance is quite fast and more than enough daily computing.   Claiming ""system stability"" is compromised on fanless SoCs is a little silly.   Apple's 4+4 fanless choice is great.   >A modern system will only be loud if you build it to be loud or put it in a situation where it needs to get loud, you already can go and config your system to turn off your fans and massively throttle your clocks and see how marvellous fanless is.  Again, plenty of consumers are not stressing their workloads to throttling on already-existing fanless laptops.  Sure, you can turn most any PC fanless with a custom fan curve & PL1 / PL2 limits, but it's tricky, space inefficent, and not easy for normal consumers.  Fanless-by-***design*** laptops always work fanlessly without needing to download custom tools, allow more space for batteries, zero vents, less weight, guaranteed zero maintenance, etc.",hardware,2025-09-24 22:17:30,6
AMD,ng11aaz,Apple's E core designs are hard carrying them here despite the core count deficit.,hardware,2025-09-24 22:26:56,9
AMD,ng12ndh,Looking at Geekerwan's review on Bilbili rn. Its pulling 19W on the MT side to acheive that.,hardware,2025-09-24 22:34:53,3
AMD,ng3ff8b,Yeah but the problem I see is the SVE2 is not part of the core and we are counting it as a part of core.,hardware,2025-09-25 08:50:50,2
AMD,ng2infs,I think the real thing missing will be a premium fan less option cause the X2 is the lower chip and they reserve premium models for premium chips. But still if somebody can consider a design with the ever common 2800x1800 120hz OLEDs around and a haptic trackpad with a nice metal body it would be a solid choice.,hardware,2025-09-25 03:53:51,3
AMD,ng0vuds,"All you're saying is ""make it worse/limit it on purpose"" so it can be fanless.  It doesn't matter if it was optimized to work better at low power (like AMD Zen c cores, or smartphone SOCs). Even smartphones, which are basically the peak of energy efficiency, still get faster if you feed them more power than they can cool continuously (phones do peak at like 15W power draw, but they can't sustain it passively).  If you give a chip more power, it will be faster. It's this simple.  It is a design decision, like I said.",hardware,2025-09-24 21:56:22,1
AMD,ng1brtx,X2 or 8 Elite Gen 5 is pulling 19W?,hardware,2025-09-24 23:28:15,3
AMD,ng0x16x,"*Worse* for you. All CPUs are ""limited on purpose"" to fit inside a certain TDP. Do you think every Intel & AMD SKU allows unlimited TDP? No. They specifically create TDP-specific tiers to fit into every device form factor.  Fanless just calls for one more rung on the **lower** end of TDP, instead of the higher end of TDP.  The # of people clammoring for actively-cooled smartphones is nil. Most consumers love fanless devices ***when*** they are performant and efficient.   >It is a design decision, like I said.  This is simply explaining this choice is predominantly controlled by the CPU designer in the world of Windows laptops, which most folks don't understand.",hardware,2025-09-24 22:02:54,4
AMD,ng2jdwc,The phone SOC. Yes.,hardware,2025-09-25 03:59:12,2
AMD,ndndmm7,"Im impressed that Qualcomm hasn't made a good support for linux of their Elite lineup.    There isn't a lot of Linux people out there, but a lot of them would happily use an ARM laptop.    Im even thinking of buying that ARGON laptop that you use with a Pi5 just because it looked fun",hardware,2025-09-11 15:21:37,37
AMD,ndnud5x,The X Elite 2 could be very interesting if Qualcomm gets full Linux support working at hardware release time by getting the major distributions installing on it out of the box. I reckon it would do wonders for their mind share having devs & enthusiast tech early adopters switch to it.  I'm not hopeful of them managing it though. Whoever is in charge of the X Elite program really screwed up the entire dev hardware release and outreach.,hardware,2025-09-11 16:43:24,23
AMD,ndoshby,"> While there are some quirks in setting it up like keeping WoA installation for extracting the necessary firmware  Yeah you can call that a ""quirk"" for sure.",hardware,2025-09-11 19:23:51,10
AMD,ndo8pb7,"Yea, linux is the only relatively popular OS family where transition to ARM would be painless. Yet here we are.",hardware,2025-09-11 17:49:57,21
AMD,neb0lwc,"Why would Qualcomm, a company thats all about locking their clients into proprietary walled gardens, want to support linux?",hardware,2025-09-15 06:48:06,1
AMD,ndyhw40,>The X Elite 2 could be very interesting if Qualcomm gets full Linux support working at hardware release time by getting the major distributions installing on it out of the box.  This is very unlikely to happen. Qualcomm is not that kind of company.,hardware,2025-09-13 07:39:11,4
AMD,ndrmhgd,"Android is so far off from Linux nowadays, that I’m not surprised. The driver fragmentation in mobile is unbelievable, and they even have problems with updating a system…",hardware,2025-09-12 05:25:14,11
AMD,ndsob0q,I am really buffles with the bad linux support. Mediatek has ChromeBook support for the recent high end chip.,hardware,2025-09-12 11:12:40,5
AMD,ndykifi,"ARM Chromebooks have great kernel support, but their boot firmware isn't really supported by anyone that isn't specifically ""arm chromebook distro""",hardware,2025-09-13 08:04:39,2
AMD,ndodlvp,As opposed to the painful experience of simply buying an ARM laptop?,hardware,2025-09-11 18:12:19,-5
AMD,ndykjig,There's patches on LKML for it,hardware,2025-09-13 08:04:56,1
AMD,ndofhcr,yes,hardware,2025-09-11 18:21:13,12
AMD,ne04nrz,Can you point me to a Snapdragon X Elite device that has out of the box support (working drivers for all hardware components) for a major linux distribution?,hardware,2025-09-13 15:00:35,4
AMD,ndrw21t,"it's the kernel modules that are the concern heh, the drivers.",hardware,2025-09-12 06:50:22,5
AMD,nh2rolv,"Here's my take:  [PCPartPicker Part List](https://de.pcpartpicker.com/list/kMx7VF)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 5600 3.5 GHz 6-Core Processor](https://de.pcpartpicker.com/product/PgcG3C/amd-ryzen-5-5600-36-ghz-6-core-processor-100-100000927box) | €91.90 @ Alza  **Motherboard** | [Gigabyte A520M DS3H V2 Micro ATX AM4 Motherboard](https://de.pcpartpicker.com/product/rxfxFT/gigabyte-a520m-ds3h-v2-micro-atx-am4-motherboard-a520m-ds3h-v2) | €65.01 @ Senetic  **Memory** | [G.Skill Ripjaws V 16 GB (2 x 8 GB) DDR4-3600 CL18 Memory](https://de.pcpartpicker.com/product/n6RgXL/gskill-ripjaws-v-16-gb-2-x-8-gb-ddr4-3600-memory-f4-3600c18d-16gvk) | €49.89 @ Computeruniverse  **Storage** | [Crucial P3 Plus 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://de.pcpartpicker.com/product/chzhP6/crucial-p3-plus-1-tb-m2-2280-nvme-solid-state-drive-ct1000p3pssd8) | €67.98 @ notebooksbilliger.de  **Video Card** | [PowerColor Reaper Radeon RX 9060 XT 16 GB Video Card](https://de.pcpartpicker.com/product/fzh2FT/powercolor-reaper-radeon-rx-9060-xt-16-gb-video-card-rx9060xt-16g-a) | €358.90 @ Mindfactory  **Case** | [Montech AIR 100 ARGB MicroATX Mid Tower Case](https://de.pcpartpicker.com/product/M7Z9TW/montech-air-100-argb-microatx-mid-tower-case-air-100-argb-black) | €45.90 @ Alza  **Power Supply** | [ADATA XPG Core Reactor II VE 650 W 80+ Gold Certified Fully Modular ATX Power Supply](https://de.pcpartpicker.com/product/yhpQzy/adata-xpg-core-reactor-ii-ve-650-w-80-gold-certified-fully-modular-atx-power-supply-corereactoriive650g-bkcus) | €79.90 @ Alza   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **€759.48**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-09-30 23:06 CEST+0200 |  You can upgrade RAM to 32GB later down the line by adding two extra sticks.",buildapc,2025-09-30 21:07:08,1
AMD,ngx7bt1,i mean best bang for buck is like the 9600x  7800x3d is if you want a top of the line chip without being wasteful pretty much,buildapc,2025-09-30 00:04:40,114
AMD,ngxmngt,Wanna laugh? I am rocking a 1080Ti with a 7800x3D with 32gb DDR 5 ram.,buildapc,2025-09-30 01:34:13,13
AMD,ngyts60,"It never was. It was the best, but never the best «bang for the buck» because it was always expensive.  The 7500f, 7600x, 7700, 7700x, 9600x and 9700x are bang for the buck processors.",buildapc,2025-09-30 06:58:43,11
AMD,ngxak83,"It depends on your budget. If we're talking about a 1500 to 2000 bucks PC, then yes. But just from a price/performance optimized build, no.",buildapc,2025-09-30 00:23:40,8
AMD,ngxdkpu,"Yeah it's pretty good value, 2nd fastest chip for ~350. 9800X3D is the only thing better, but assuming a 1440p or higher resolution only pairing with a 5090 would make any difference, and then only really at 1440p.",buildapc,2025-09-30 00:41:30,6
AMD,ngxl3c6,"The X3D versions are great processors if you pair them with a high-end GPU or are doing tasks that particularly take advantage of the high amount of on-CPU cache. But they aren't a magic bullet. If you're framerate limited by your GPU, for example using a 5070 at 4k, the CPU won't make a big difference. But, that being said, if you can find a good price on one, it's not going to be a bad purchase. It's going to be at least as good as the regular version, if not better.  The real choice comes down to comparing the price of the X3D CPU versus a non-X3D, and your budget. For example, a 9900X is often around the same price as a 7800X3D (it was down to $355 recently, don't know what it is right now), and performs much better in a lot of workloads. Or, spending less on the CPU could allow you to get a better GPU for the same budget, resulting in better FPS per dollar.  Since it seems like you don't upgrade very often, which is good, don't worry about future proofing or the platform so much. If Intel wasn't in so much disarray, I'd still be recommending (and buying) the 14600k as the bang-for-the-buck king. In general, get what's going to meet your needs now and provide the best value within your budget.",buildapc,2025-09-30 01:25:00,4
AMD,ngxev19,"The 7800x3D is still an expensive CPU, good bang for the buck if your looking for topmof the line performance. But the 14600k is the true bang for the buck CPU right now for gaming and productivity. No upgrade path, but itll last years anyway.",buildapc,2025-09-30 00:49:02,13
AMD,ngyqkhc,"None of AM5 x3d chips are good value for money performance wise (7500F, tray 7700, 9600x, 14600k are all much better value for performance they offer). They’re fastest CPUs for gaming though.",buildapc,2025-09-30 06:28:27,2
AMD,ngxcu7k,Yes get the 7800X3D,buildapc,2025-09-30 00:37:09,3
AMD,ngx71bw,If you can get a deal from microcenter it's absolutely one of the best price to performance CPUs you can get.  if not it's definitely a bit up there.,buildapc,2025-09-30 00:02:58,2
AMD,ngxlsvz,"It would be a great choice, though not all that cheap.  What is your budget looking like? If you'd have to skimp on the GPU to get the 7800X3D, I'd get a non X3D chip instead, even down to the 7600X, so you can shift some money to the GPU budget. You'll still be on AM5 allowing for upgrades for at least this and the next generation, and have better performance in the short term.",buildapc,2025-09-30 01:29:13,1
AMD,ngxmy1c,I recently got it for 385 before tax,buildapc,2025-09-30 01:35:53,1
AMD,ngxwpw0,How well does my 5800x3D hold up?,buildapc,2025-09-30 02:34:35,1
AMD,ngy3e72,I'd say it's the OEM version of Ryzen 7700 because it's very cheap and holds its own against high end CPUs in many things. A great allrounder.,buildapc,2025-09-30 03:18:09,1
AMD,ngzmdu6,"Yes..but there is an argument for 9800x3d too, as you will delay the inevitable upgrade for longer.",buildapc,2025-09-30 11:29:06,1
AMD,nh0ammz,"If it's for gaming, I think the 7600x3d is also great value if you have access to a microcenter for the bundles",buildapc,2025-09-30 13:56:13,1
AMD,nh0sq99,This is like a cheap fiat vs a Lambo. The difference is substancial and this comes from someone who upgraded from a I9 9900k to a 9800x3d.,buildapc,2025-09-30 15:25:56,1
AMD,nh284vo,"I'm looking to upgrade i7-7700K and 1080Ti, so would be interested to hear what you get.  So far I have purchase a 9600x and waiting on a good deal for a GPU.",buildapc,2025-09-30 19:33:34,1
AMD,ngyzcmh,"Just leaving this here, https://youtu.be/QVgZtewOgHQ?si=mOUNF9hRx61LYT1q 7700 goes for about $160 at aliexpres.  Fps difference is pretty much equal or 5% improvement at most.   https://youtu.be/EP3TD6owpHs?si=xoO-Mbmrd-UtjQOS (aliexpress 7700 test)",buildapc,2025-09-30 07:54:13,-1
AMD,ngxke2b,The 7800x3d and 9800x3d are literally only necessary for 4090/5090s.,buildapc,2025-09-30 01:20:52,-5
AMD,ngxitbk,"Going from a 9 year old i7 to a 9600x would be world changing. Cheap performance now, and he can still pick up an x3D later once Zen6 releases.",buildapc,2025-09-30 01:11:44,12
AMD,ngxan74,"I agree, and to be more specific, the Intel 14600k is the best bang for the buck if you're okay buying into a dead end platform and just want performance right now.  9600x is the best bang for buck on AM5 and gives you a lot of room to upgrade.",buildapc,2025-09-30 00:24:10,23
AMD,ngz3mi1,specially if u get it for 156 dlls omg it crushes all,buildapc,2025-09-30 08:38:31,1
AMD,ngyccel,"How’s it handling? Looking to upgrade my old system too, but I have just the regular 1080 GTX lol",buildapc,2025-09-30 04:24:27,3
AMD,ngxzqut,Ur not alone sir.,buildapc,2025-09-30 02:53:51,0
AMD,ngxca7x,"Yeah I should have been more specific in the post. 1500-2000 is around the budget im looking at for hardware (reusing case and coolers, at least temporarily).  Specific to the bang for buck, I should have clarified that im wanting a highish performance build capable of running games like cyberpunk at ultra or high settings.",buildapc,2025-09-30 00:33:53,2
AMD,ngz4xbh,How long do you reckon the 7800x3D will keep being really good for heavy PC games? I’m hoping to get 4 to 5 years out of mine,buildapc,2025-09-30 08:51:53,5
AMD,nh0hbqo,"Aiming for 1500 to 2000 for the build, reusing a few components off my old rig (case, power supply)  Looking at pairing with a 9070XT or 5070TI",buildapc,2025-09-30 14:30:11,1
AMD,nh3713p,Im kind of in the same hold pattern. Hoping to make some moves around black Friday if there's any solid sales.,buildapc,2025-09-30 22:28:16,1
AMD,ngxjrb0,It's rarely worth wasting $200 to upgrade later. Even if he sells it and loses like 50,buildapc,2025-09-30 01:17:11,4
AMD,ngxk4cq,How would a 9600x compare to a i7- 4790k?,buildapc,2025-09-30 01:19:18,11
AMD,ngza715,60-90 fps on medium mostly high settings on all games on 1440p.. Except very few games like Alan Wake 2. Make sure you change thermal paste and adds thermal pads. Changed it recently from 84c on load to 45-60c.,buildapc,2025-09-30 09:45:35,3
AMD,nh0zvrx,"for the 7800x3d its helping pushing some open-world games or big maps games like Hell Let Loose (fps increase and makes it smooth) however, it's bottleneck for sure because of the 1080Ti.",buildapc,2025-09-30 16:00:38,2
AMD,ngy85rj,"Based on those requirements, the 7800x3d is a great option at current prices, then spend as much as you can on the gpu.   You can prob squeeze a 5080 in there if you're in the US.",buildapc,2025-09-30 03:52:23,4
AMD,ngyhw3q,"only online competitive games benefit from 7800x3d chips, but since you're on a very good budget. Go for it, should cover you for the next 5-7 years",buildapc,2025-09-30 05:10:17,1
AMD,ngzehs6,"I just picked up an 7800x3d paired with a 5080, and it's phenomenal at Cyberpunk.    If you can afford a 7800x3d, it'll be a good investment for the future, as you seem to keep your gear for a lifetime generations.",buildapc,2025-09-30 10:25:39,1
AMD,ngz9ekm,easily 4-5 imo unless gpus have a breakthrough in the next couple generations,buildapc,2025-09-30 09:37:42,7
AMD,ngxkudb,"I don't see it as a waste, but your take is valid too. I'm trying to do a similar upgrade going from a 6700k, just waiting for Zen6 to drop. And in the meantime save some money for their top tier x3D chip. If they redesign the chips similarly to what they did to Strix Halo, we may get some real nice performance uplift.",buildapc,2025-09-30 01:23:33,6
AMD,ngxrlrd,"insane, probably like 3x as fast",buildapc,2025-09-30 02:03:21,23
AMD,ngy0hqx,"You'll be asking yourself ""Why didn't I do this upgrade sooner?"".   A 10 year newer CPU... is fast.",buildapc,2025-09-30 02:58:46,20
AMD,ngzaqpc,I went from a 4790k to a 5800x to a 9700x. It's a staggering improvement. Remember that one doesn't keep everything else the same tho. I went from 1080 60hz to 1440 240hz.  The smoothness of 60-80 1% lows and avg of 100-140 is pretty nice.,buildapc,2025-09-30 09:50:50,3
AMD,nh0thug,I went from a heavily overlocked i7 5930k to a 5900x in 2021 and it was a massive improvement all around. Already replaced that with a 9800x3D which was a massive improvement in games though not as noticeable in other general use.,buildapc,2025-09-30 15:29:41,1
AMD,ngykw08,"For those games, unless they're gaming at 1080p, they're probably better off with a 9700x or 265k.",buildapc,2025-09-30 05:36:35,2
AMD,ngz9xjz,Music to my ears.,buildapc,2025-09-30 09:43:01,2
AMD,ngxp875,"Rumor mill is saying AMD is targeting 7GHz for Zen 6.   Strix Halo update will be a bit later, those aren't drop in chips, they are for motherboards manufacturers and laptop manufacturers. So you won't be upgrading the CPU to them, afaik.",buildapc,2025-09-30 01:49:12,2
AMD,ngy06zf,"You do you, but what gpu are you even using? Just buy a 9800x3d now which pretty much maxes out a 5090, then if the new chip is really that much better (doubtful) just sell that instead of selling the 9600x. If you have the money for a 5090, why are you so worried about $200 (and if you don't a 9800x3d is plenty)?",buildapc,2025-09-30 02:56:49,1
AMD,ngzdo5j,If he can afford a 7800x3d/5080 there is no benefit to go lower.,buildapc,2025-09-30 10:18:16,5
AMD,ngzxio8,"It's even quite likely, that it will be good enough for most games for even longer than that. The games that benefit the most from the X3D CPUs are CPU intensive games (strategy games, MMOs, competetive games at 1080p). If you play AAA games or Story games mostly, you will not be bottlenecked for a loooooong time.",buildapc,2025-09-30 12:42:52,2
AMD,ngxq17i,"I'm thinking more in the sense that Zen6 may drop infinity fabric and use the redesigned interconnector they did with Strix Halo. Something about better core to core latency. However I am no expert on that, and don't claim to be.  Theres a video on the AMD subreddit from someone really knowledgeable on computer science going over the schematics of the chips and how its a preview of what can potentially come to the new mainstream chips. I highly recommend it, its quite interesting. Of course until theres benchmarks, its all speculation.",buildapc,2025-09-30 01:53:58,3
AMD,ngyeftr,"Right now a 6950XT thats severely underutilized but I got it for like ~$400 a couple years ago if I remember correctly. I would be able to buy a 5090 if I really wanted to as well, but I had to spend quite a bit of my fun money savings this year for some important things. So I'll probably wait until next year to before I do the new build. Were it not for that I probably would have gone 9800x3D already.  I think going back to your initial point of it rarely being worth it, I considered going top of the line CPU and GPU until I had to divert that money elsewhere. Thats why I briefly was thinking of getting the AM5 set up with something more suited for my 6950XT. Seemed tempting, and I think I would have done it if I was more impulsive. Heres to hoping next gen CPUs bring a substantial increase in performance.",buildapc,2025-09-30 04:41:18,1
AMD,nh1b2lk,Sure there is — more money for other things.,buildapc,2025-09-30 16:55:29,0
AMD,ngzyzg3,"I do play some single player games, bit of COD here and there (although the last few ones suck), but I mostly play flight sim. I will upgrade in a few years though, so I guess I'm good",buildapc,2025-09-30 12:51:34,2
AMD,ngxxqmp,"So, that may be the case. The difference between the Serdes and the point to point is better, I just don't know right now is that is implemented for the AM5 platform. The reality it, I am not sure you will get much benefit from that is it is just used to connect the memory controller where Strix Halo uses it to connect the GPU, which is a bit different.  It would be nice, though.",buildapc,2025-09-30 02:41:06,3
AMD,ngy0t2m,Lower memory latency won't help x3d chips that much and 8 cores is enough for now. It's probably for llms,buildapc,2025-09-30 03:00:48,1
AMD,ngyp00w,"The 9000s were basically no gain, so doubtful, at least for gaming. Maybe if they add more 3d vcache.   The cashflow thing makes more sense vs waiting for a cpu that either won't be a big gain or else you won't have the gpu to utilize it. But a 7800x3d isn't that much more than a 7600.",buildapc,2025-09-30 06:13:50,2
AMD,nh4ephd,That suggests they can't afford it lol,buildapc,2025-10-01 02:46:35,0
AMD,ngyevfm,"Certainly would be nice, since I'm hoping to keep this AM5 build for a long long time.",buildapc,2025-09-30 04:44:52,2
AMD,nh4s5di,"Just because someone can afford it, doesn't mean they should spend that money. I can go and buy a $75,000 car tomorrow -- doesn't mean I need it or I should spend that money.  By getting the cheaper CPU, they could spend the money on a few games and get way more enjoyment out of that money. They won't notice a difference between the CPUs while playing",buildapc,2025-10-01 04:21:03,1
AMD,ng3l6tj,Yes 850W is enough unless you plan do some insane overclocking.,buildapc,2025-09-25 09:48:47,6
AMD,ng3lzwi,850W unit of good quality does it. I suggest an ATX 3 unit with native 12v2x6 cable.,buildapc,2025-09-25 09:56:21,4
AMD,ng3m1xb,"850w good one or 1000W if you want more free ""space""",buildapc,2025-09-25 09:56:51,2
AMD,ng3nk15,850 watts is plenty. RTX 5080 uses around 350 at peak.,buildapc,2025-09-25 10:10:43,2
AMD,ng3o4t2,"I have the same exact setup with corsair 850w psu. Works great.    I usually game in 4k cs2, farm sim, satisfactory etc. And gpu load is around 250-300W with OC. And CPU around 60-80W. Peaks can be higher.",buildapc,2025-09-25 10:16:01,2
AMD,ng3omwe,"If you're in the US, Montech Century II 1050W is only $20 more than the 850W variant. Good value A-tier PSU. It's not needed, but it's nice to have for $20.",buildapc,2025-09-25 10:20:34,2
AMD,ng3pdmu,I got 1000W for futureproof. I’ll just change out the gpu for something newer in the far future. Hopefully by then gpus won’t need more than 1000W system lol,buildapc,2025-09-25 10:27:09,2
AMD,ng40j00,"850W is plenty, just get a quality power supply.",buildapc,2025-09-25 11:52:04,1
AMD,ngijf5w,"I use same cpu with 5090 with 850w MSI 3.1 PSU, works great. So yeah, with a 5080 you're okay with 850w",buildapc,2025-09-27 17:44:57,1
AMD,ng3vo78,Yeah won't be doing insane overclocking on it. I'll be sticking with the 850 then,buildapc,2025-09-25 11:17:39,1
AMD,ngijixe,You'd really need to do crazy insane OC to run into problems with 9800x3d/5080 on 850w lol,buildapc,2025-09-27 17:45:28,1
AMD,ng3m81v,I'm picking the **Corsair RM850x (2024) 850 W Fully Modular ATX Power Supply.** Will this suffice?,buildapc,2025-09-25 09:58:25,4
AMD,ng3vmbt,Hmm yeah the 850 seems to be more than enough for what i'll use then,buildapc,2025-09-25 11:17:15,1
AMD,ng3n00l,Yes. Great unit.,buildapc,2025-09-25 10:05:36,5
AMD,ngwyubd,"B850 motherboards are effectively rebadged B650 motherboards where the primary M.2 slot is *required* to support PCIe 5.0 instead of just PCIe 4.0, but many B650 motherboards did anyway. Most B850 motherboards also generally support PCIe 5.0 for the main PCIe slot, though it is not required and some do not. A small number of B650 motherboards did support PCIe 5.0 for the main PCIe x16 slot, but this was much rarer.  For a gaming on a 5070, this will have no performance impact either way. The RTX 5090 sees a 2\~3% performance difference in two games, and the rest have no meaningful difference between PCIe 4.0 and 5.0. Even PCIe 3.0 only sees a maybe 1-2% performance loss. 8 GB GPUs when running games using settings that require more than 8GB of VRAM generally see massive performance losses regardless of if you're using PCIe 4.0 or PCIe 5.0, though it's *less* when using PCIe 5.0, it's still usually significant enough that you're better off just not running those quality settings to begin with, or buying a better GPU. There's effectively no measurable difference between PCIe 4.0 and PCIe 5.0 with any other PCIe 5.0 capable GPU. It took nearly a decade after PCIe 4.0 became standard that we started to be even able to measure a difference. I suspect it'll be about that long before PCIe 5.0 becomes relevant as well.  Similarly, PCIe 5.0 for the SSD has no meaningful benefit for gaming. A small handful of games see tiny fractions of a second faster load times vs a PCIe 4.0, or even a decent PCIe 3.0 SSD. It's not worth even looking at.  The last thing some B850 motherboards have that few B650 motherboards have is WiFi 7 instead of WiFi 6E. WiFi 7 is legitimately faster ... for short range line of sight between WiFi 7 devices. Otherwise WiFi 6E and WiFi 7 are basically the same.  The MSI Tomahawk motherboards are well liked by a lot of people due to generally having a lot of USB ports on the rear I/O, as well as black styling.  The B850 Gaming Plus Wifi and Pro B860-P Wifi are almost literally the same motherboard with different screen printing on the PCB and slightly different shaped heat sinks. They also swap between having an HDMI or DP on the rear I/O, and the Gaming Plus has an ""EZ-Conn"" header for use with some proprietary MSI fans. The Pro motherboard also has the pads for this connector, there's just no header soldered to it, because *it's the same motherboard*.  You can also get an Asus TUF GAMING B650-PLUS WIFI or Gigabyte B650 Gaming X AX V2 which apart from the lack of PCIe 5.0 on the x16 slot, and are using WiFi 6E aren't much different.  Asrock motherboards have been excellent options for their price, but they've been having some weird issues lately, especially with X3D CPUs, so it might be best to avoid them for now.",buildapc,2025-09-29 23:16:10,13
AMD,ngwurtd,"7800X3D doesn't consume much power, so basically any AM5 board will be able to run it, as long as you don't want to overclock memory too much (something that you don't really need to do with an X3D chip anyway).  Features - that depends on what you need. I needed optical audio out, so I was limited with my choice. If you don't need WiFi you can get a cheaper MB that doesn't have WiFi, etc.  -  The MSI MAG B650 is the most high-end board here, if you theoretically want to upgrade to a CPU with higher power consumption ~3-4 years down the line.",buildapc,2025-09-29 22:53:09,4
AMD,ngwx1nx,Maybe just not Asrock if you plan on upgrading down the line with a 9800x3d. I love Asrock but just not this gen.,buildapc,2025-09-29 23:05:59,2
AMD,ngyomg2,Asrock may have fixed their BIOS's finally as I haven't seen many new posts of their boards killing AM5 CPU's but I think I'd skip that brand anyway just to be safe.  [https://www.newegg.com/msi-b850-gaming-plus-wifi-atx-motherboard-amd-b850-am5/p/N82E16813144688](https://www.newegg.com/msi-b850-gaming-plus-wifi-atx-motherboard-amd-b850-am5/p/N82E16813144688) plus the $30 coupon code is probably the best you're going to get out of that list.,buildapc,2025-09-30 06:10:23,1
AMD,ngz3ikm,B850 Tomahawk :),buildapc,2025-09-30 08:37:22,1
AMD,ngz74n0,"i have 7800x3d with asrock b650m pro rs Wifi for more than a year. No problems so far. Even though i've seen dying 9800x3d cpus on asrock mobos, it doesn't seem to affect 7800x3d as much or at all.",buildapc,2025-09-30 09:14:27,1
AMD,ngzh3hj,"I have had great luck with 2 MSI AM4 systems and a recent Gigabyte Am5 build,  and have been shopping around online for a B650E or B850 board to pair with a 7800x3d.  I think the Gigabyte B850 Gaming X is where I'm going to go. It checks all my boxes, especially at 179.99.  Might be one to check out, tests and reviews look good.",buildapc,2025-09-30 10:48:04,1
AMD,ngzhq23,"Most of my experience over the past decade is with MSI boards. I have both the B650 and B850 Tomahawks.  Either is fine, especially if Wi-Fi is not a big determining factor. The B650 comes with the AMD branded WiFi card, which is not great, but can get the job done. I swapped mine out for an Intel card immediately. The 850 comes with a Qualcomm card, which is fine. The 850 does have a bug with the LAN port. Some things such as unplugging the cable while is use will cause the port to vanish entirely. Restarting the computer or trying to reinstall drivers won't fix it. You have to do a full power cut. Shut it down., physically remove power from it and hold the power button for about 30 seconds. Then restart it and the port is back like nothing happened. So far I've only run into the issue once so it's not a major issue unless you don't know how to resolve it.",buildapc,2025-09-30 10:53:12,1
AMD,ngxzrf2,The awesomeness of your response deserves more attention.,buildapc,2025-09-30 02:53:57,2
AMD,ngz1w4d,"Wow, thanks for such a detailed breakdown! That clears up a lot of the confusion I had between B650 and B850 boards, especially about PCIe 5.0 basically not mattering for gaming right now. Good to know about the WiFi 7 difference too, and the Tomahawk USB ports are definitely appealing. I’ll keep the ASRock issues in mind as well, really appreciate you taking the time to lay all that out!",buildapc,2025-09-30 08:20:22,1
AMD,ngz285p,"Got it, thanks for the heads-up! I’ve mostly been looking at MSI anyway, so that helps me narrow things down. Appreciate the advice!",buildapc,2025-09-30 08:23:51,1
AMD,ngz2a5o,"Thanks for the info! Yeah, I think I’ll play it safe and stick with MSI. Appreciate the link and the heads-up about the coupon too, that’s really helpful!",buildapc,2025-09-30 08:24:26,1
AMD,ngzjsu7,"I wish I could afford that one too, but it’s about $110 more expensive than the B650 variant where I’m from.",buildapc,2025-09-30 11:09:51,1
AMD,ngzjw23,"Good to hear your experience has been solid with the 7800X3D on that board! Yeah, I’ve also seen those posts about 9800X3D issues with ASRock, so it’s reassuring to know the 7800X3D doesn’t seem to be affected in the same way. Thanks for sharing!",buildapc,2025-09-30 11:10:33,2
AMD,ngzk1mn,"That does look like a solid board, and the price is really tempting. Someone else mentioned they had a bad experience with Gigabyte in the past, but from what I’ve seen the B850 Gaming X looks pretty solid on paper. Definitely one I’ll keep in mind, thanks!",buildapc,2025-09-30 11:11:43,1
AMD,ngzket3,"Thanks for sharing your experience! That’s really useful to know about the WiFi differences between the B650 and B850, and also the LAN bug on the B850. I’ll probably be using Ethernet most of the time, so it shouldn’t be a big issue for me, but it’s good to know there’s a way to fix it if it does happen. Really appreciate you taking the time to explain all that in detail!",buildapc,2025-09-30 11:14:32,1
AMD,nh4j524,Sell build A and build a new one. https://pcpartpicker.com/list/GPT69C  Build B is upgradable  https://pcpartpicker.com/list/3Z7m9C  If you want to build 2 new ones just do two of the first. I would also recommend keeping an eye out for deals. 5060 ti’s were just on sale recently.,buildapc,2025-10-01 03:16:12,8
AMD,nh4fern,A is less capable. but you can get 2 extremely good new PCs for ~$1000-1500 ea.,buildapc,2025-10-01 02:51:05,15
AMD,nh4hdfo,"This would do you both good, you could do much cheaper and still have a great, upgradeable build. You can customize the aesthetic as you'd like, and you could reuse some parts as well https://pcpartpicker.com/list/kVPRcx",buildapc,2025-10-01 03:04:05,6
AMD,nh4jsv7,Both of these are remarkably similar,buildapc,2025-10-01 03:20:45,4
AMD,nh4s65q,Computer A isn't upgradeable. Computer B can be upgraded but buying a completely new PC is a pretty reasonable choice at this point.,buildapc,2025-10-01 04:21:13,1
AMD,nh4p9se,"Thanks for the approval! (from A) Does this build sound ok? Corsair frame 4000d, msi mpg x870E edge AM5, AMD ryzen 5 7600X, corsair vengeance 32 gb, xfx swift radeon RX 9060 XT : ~$1300",buildapc,2025-10-01 03:59:28,2
AMD,nh4tl1m,"Mine was significantly older/slower than yours.  * CPU : i5 6600 @ 3.30GHz * Mobo : ASUS H170 Pro Gaming * GPU : **GTX 1050TI** * RAM : Corsair DDR4 **16GB** (2x8GB) * SSD : Samsung 970 EVO Plus 1TB  2 years ago, games were slowing down significantly. I was thinking about whole system replacement but that's the time Intel CPU are having lots of problems and I am not familiar with AMD builds. Therefore instead of replacement, I upgraded my GPU to **RTX 4070**, a lot of people might disagree with my choice (CPU bottleneck) but it turn out completely fine for me and I am happy about it. After GPU upgrades, all my games run smoothly with high graphic settings.  This year I want to run something like multiple VMs but restricted by 16GB RAM. I tried to add Corsair 16GB more but caused mismatch between new and old RAM. So I returned Corsair 16GB and ordered Kingston DDR4 (4x16GB) **64GB** and replaced the old RAMs. Also replaced all chassis fans and CPU cooler.  Current performance fulfilled all my requirement, I don't need to upgrade in a few years. All upgrades cost me about 800$ in total. My games are WoW, Everspace2, Frostpunk 1/2, Ixion, Desperados III, Diablo 3/4, Unruly Heroes.",buildapc,2025-10-01 04:32:27,1
AMD,nh4iv55,"What monitors and cases do you have? Also, what games do you play? That'd help set a much better baseline for a new build for both of you  Although build B is better, I disagree that it can be meaningfully upgraded. AM4 is dead. I'd just spend $1,500 on a new build for both",buildapc,2025-10-01 03:14:18,1
AMD,nh4gvos,build b is better and you can upgrade the cpu in a decent way  build a has to be rebuilt from scratch  https://pcpartpicker.com/list/hktPC8  you could buy something like this 2x...,buildapc,2025-10-01 03:00:47,-2
AMD,nh4g5kr,https://pcpartpicker.com/list/wD2Kjn,buildapc,2025-10-01 02:55:58,4
AMD,nh4rj14,"It’s funny, I recently had both a 1650 and a 580 before upgrading. Got a wee bit better performance from the 580, had a bit more VRAM which helped a lot in some titles. For reference, I got like 30fps on RDR2 with the 1650 on low settings, and about 40fps on medium settings (1080p)",buildapc,2025-10-01 04:16:15,2
AMD,nh4q30o,"Way too expensive of a motherboard for no reason at all. Replace it with a better GPU  Again, could you say what monitors you two use, what cases you have presently, and what games you play?",buildapc,2025-10-01 04:05:27,5
AMD,nh4gqpf,"You can totally build a solid system for $1000. 3k should be enough for new systems, monitors, and m+k each.",buildapc,2025-10-01 02:59:53,2
AMD,nh4h7yl,disagree... build b is fine if you upgrade to an x3d cpu and upgrade the gpu... costs around 700 bucks including gpu...  build a has to be built from scratch...,buildapc,2025-10-01 03:03:05,1
AMD,nh4ikzh,"Look at how expensive AM4 X3D CPUs are now. It's not feasible. Both are looking at an upgrade to AM5 and a new GPU, which is basically just a new build",buildapc,2025-10-01 03:12:22,3
AMD,nh4iisy,"Look at how expensive AM4 X3D CPUs are now. It's not feasible. Both are looking at an upgrade to AM5 and a new GPU, which is basically just a new build",buildapc,2025-10-01 03:11:56,4
AMD,nh4haw1,"Could you not use this as a baseline for two new systems? In what world could you not just build two of these? Their systems are at the point of needing to be rebuilt, not upgraded. They can upgrade some parts as they go, like GPU and power supply, but if I'm going to replace my power supply anyways, I'm just going to rebuild the entire system.  Stop being an asshole dude.  Edit: Also, I'm just throwing down a build within that budget. Not every single comment has to tackle the whole problem.",buildapc,2025-10-01 03:03:37,4
AMD,nh4n7rw,Huh?,buildapc,2025-10-01 03:44:44,2
AMD,nh4iz8h,if it breaks their budget no if they can build 2x am5 setups with decent gpu sure...   which gpu do you suggest? rx 9060 xt 16 gb or below that? that would be roughly 800 bucks for the gpus alone...  ryzen 5 9600x costs about 200 bucks vs ryzen 7 5700x3d 350...  https://pcpartpicker.com/list/hktPC8,buildapc,2025-10-01 03:15:05,2
AMD,nh4smcf,"https://pcpartpicker.com/list/2tZ69C  This works out to $1,200 each, which is right in their budget. Then they have $300 each for a monitor if they have the extra money",buildapc,2025-10-01 04:24:46,1
AMD,nh4tyy6,looks solid  they can also sell their old parts if they want a better gpu i guess...,buildapc,2025-10-01 04:35:33,1
AMD,nh2kbd0,That CPU draws like 100W.  A dual tower air cooler is plenty for it.  The <$40 Thermalright Phantom Spirit is the most popular cooler to pair with it here.,buildapc,2025-09-30 20:32:16,34
AMD,nh38j3h,"If you want the best overall, which I don't recommend is, Noctua's NH-D15 G2  Phantom Spirit 120 SE or Evo",buildapc,2025-09-30 22:36:51,6
AMD,nh2m6lu,Phantom spirit,buildapc,2025-09-30 20:41:05,4
AMD,nh2pmrr,Phantom Spirit. Just upgraded my system with 9800X3D and am using that. Temps are great with it.,buildapc,2025-09-30 20:57:20,3
AMD,nh2yo1e,"hot cpu's were issues of the 13th and 14th gen, ryzen actually run efficiently and won't kill themselves. A decent air cooler like a peerless assassin or phantom spirit would keep it very cool",buildapc,2025-09-30 21:42:30,2
AMD,nh3k47g,"Just built with a 9800 x3d and used a Peerless Assassin 120. Temps are very good (low 40s at idle, mid 60s gaming). Highly recommend!",buildapc,2025-09-30 23:43:50,1
AMD,nh3xc22,Be Quiet! Black Rock Pro 5 keeps my 9800x3d at 45°C consistently.,buildapc,2025-10-01 01:01:30,1
AMD,nh45oce,I’ve got NH-C14S. Could have gone the smaller NH-U9S.,buildapc,2025-10-01 01:50:51,1
AMD,nh4wh71,You can easly undervolt the 9800x3d. I did and I runs less cooler(17C less). Yiu can handle it with air cooler without issues.,buildapc,2025-10-01 04:56:11,1
AMD,nh4ypej,"As people have pointed out, most coolers can cool the thing at stock settings. However, I use an ITX cooler for esthetic reasons, and all I had to do was adjust the TDP by a few tens of watts in BIOS, and now my PC is whisper quiet even with underpowered cooling.",buildapc,2025-10-01 05:15:17,1
AMD,nh2l686,"Note that Thermalright releases new tiny bit better coolers all the time so a Royal Knight, Peerless Assasin (V3?), Royal Pretor etc, all should be a bit better at similar pricepoints. The last one might be more expensive.",buildapc,2025-09-30 20:36:19,10
AMD,nh2pagj,Even single tower coolers are rated for like 150w,buildapc,2025-09-30 20:55:42,1
AMD,nh3mih8,"NH-D15 G2 LBC in particular for an AMD system, but only if you plan to stay with AMD for the life of the cooler (or *really* don’t care about cost). LBC is designed specially for AMD systems, HBC for Intel, and then there’s the jack-of-all-trades base model.",buildapc,2025-09-30 23:57:50,2
AMD,nh2lw5c,The Phantom Spirit is slightly better than the Peerless Assassin due to having an extra heatpipe.  The Royal Knight is offset and has a slim side fan to not cover the RAM but still has the same number of heatpipes as the Peerless Assassin.  That slim side fan probably slightly reduces its performance though.  Not sure about the Royal Pretor.,buildapc,2025-09-30 20:39:43,5
AMD,nh3uteo,I wouldn't trust that at sustained loads though. I have a 260w dual with a 180w cpu. During gaming it's fine and barely hits 60⁰ while 40-50 utilization. Productivity and hitting all cores that gets near saturation,buildapc,2025-10-01 00:46:54,5
AMD,nh443ae,"""Rated for"" and 'actually capable of cooling efficiently' are not the same thing. Cooler TDP ratings are entirely made up bullshit and should never be used.  Single-tower coolers are good for <80W CPUs like the 5600 / 9600X etc. Beyond that, you should be looking at thicker single-towers or dual-towers. As always, reviews and benchmarks are your friend.",buildapc,2025-10-01 01:41:21,4
AMD,nh3syjr,I have this specific cooler (180€) for 7800x3d and it’s hella cool but my girlfriends 40€ cooler has the exact same noise and temperature… silicone lottery is worth more is my takeaway,buildapc,2025-10-01 00:36:03,1
AMD,nh4eqhg,What about frozen a720?,buildapc,2025-10-01 02:46:45,1
AMD,nh2nhot,"It was better than V1 of peerless assassin, I think is still better than v2, I don't know about V3.",buildapc,2025-09-30 20:47:16,1
AMD,nh3tscl,It’s a shame that nobody (to my knowledge) sells binned CPUs for overclocking anymore – not even Thermal Grizzly and their delidded CPUs.,buildapc,2025-10-01 00:40:52,4
AMD,ng5dim5,"Yeah, that means it has a BIOS that's new enough to support the CPU.",buildapc,2025-09-25 16:07:01,24
AMD,ng5e6fo,"It will work, but you can still do the update to have the latest AGESA etc. Depending on the BIOS date, you could have one of the first 9000-series compatible BIOS versions, and those weren't too stable.",buildapc,2025-09-25 16:10:12,11
AMD,ng5hth5,"Should work, but I'd still recommend seeing if you can get an update for it",buildapc,2025-09-25 16:27:31,3
AMD,ng5jnw4,"It should work just fine, probably should upgrade bios anyway after.",buildapc,2025-09-25 16:36:19,1
AMD,ng5kiao,"Yes it should have already been updated, but if the board is released before the ryzen 9000 series then the bios version would be the second thing to check if it doesnt boot (after checking everything is seated and connected correctly).",buildapc,2025-09-25 16:40:23,1
AMD,ng66ti4,By definition it means the motherboard will work with the first generation of 9000 series CPUs as is.  The 9600x is a first generation 9000 series CPU.,buildapc,2025-09-25 18:25:24,1
AMD,ng6dew3,I had box with this label but it didn't work with 9950x until I updated bios,buildapc,2025-09-25 18:57:52,1
AMD,ng6fo3m,Always update the BIOS to the latest version. If your BIOS gets compromised your computer is fucked.,buildapc,2025-09-25 19:08:58,1
AMD,ng6jx3h,It will work and you should still update to the latest known stable bios,buildapc,2025-09-25 19:30:01,1
AMD,ng70jxh,"Yes, definitely update. It's a healthy habit to make sure everything is updated. And use UPS system in case of power loss during update.",buildapc,2025-09-25 20:51:02,1
AMD,ng5oq4j,Your 9600X will work. A BIOS update is recommended (mandatory) for stability.,buildapc,2025-09-25 17:00:00,1
AMD,ng6m7uh,I updated the bios anyway. Just for precaution through a usb as I haven’t assembled the pc yet,buildapc,2025-09-25 19:41:29,3
AMD,ng722jk,"Well not always,when you update stuff SOMETHING usually breaks or stops working correctly",buildapc,2025-09-25 20:58:31,0
AMD,ng8lfh3,"It's the same as me. Every PC I assemble always gets the latest BIOS possible, since there is a lot of fixes and compatibility improvement now, both on Intel and AMD side. Both companies had problems with CPUs burning themselves due to too high default voltages in first BIOS releases (7800X3D, 9800X3D, 13-14th gen Intels etc.).  So always latest BIOS plus I keep with the news and if there is a critical flaw fixed, I let my clients know that they will need an update, just for the peace of mind :)",buildapc,2025-09-26 02:23:07,1
AMD,ngvfkc1,the b650. save $110,buildapc,2025-09-29 18:35:58,2
AMD,ngveq1k,"For that big of a price gap, just get the B650.  The PCIe 5.0 on the B850 is not going to make much of a difference for your 5070.",buildapc,2025-09-29 18:31:57,1
AMD,ngvfg98,That’s too much for the B850. Just get the 650.,buildapc,2025-09-29 18:35:25,1
AMD,ngvgwxa,B650. You’ll probably not see a difference at all between the two.,buildapc,2025-09-29 18:42:23,1
AMD,ngvmo7s,Both are really good motherboards but if its a $110 gap this is one of those times is ok to cheap out on the motherboard. For the msot part they are nearly identical and the B650 version definitely punches above its wieght class.   Id go B650,buildapc,2025-09-29 19:09:58,1
AMD,ngvjud7,"Yep, and skip on the 7800X3D and put the money saved towards a 5070 Ti instead. People are too obsessed with the X3D chips that offer poor value for money when most of the time they are going to be GPU limited.",buildapc,2025-09-29 18:56:16,1
AMD,ngw7d2l,"Thanks! What kind of CPU would you recommend, something like the 9700x?",buildapc,2025-09-29 20:49:26,1
AMD,ngw86jw,That would be fine. Any modern CPU from the last few years will be capable of pumping out high framerates.,buildapc,2025-09-29 20:53:22,1
AMD,ngwb0mu,"Alright, thank you!",buildapc,2025-09-29 21:07:13,1
AMD,ngzbsug,"U can, that case is cheap for a reason  im not sure what ur budget is for new fans but u could also set a fan curve in bios to slow them down a little and hopefully reduce noise",buildapc,2025-09-30 10:00:54,1
AMD,ngnzv24,"The case is a hotbox  You don't need an x870 mobo, B850 would do the same thing",buildapc,2025-09-28 15:46:19,9
AMD,ngo0k93,"Besides the case (which seems to have lower air flow), The PC is really good. You could save money on some components which are overkill (like PSU, motherboard, SSD, air cooler would be enough) and put these money into a better GPU for example.",buildapc,2025-09-28 15:49:31,4
AMD,ngocgrf,"The motherboard and power supply are overpowered for that system. If you can return them i would and get a b850 motherboard and a 850W good power supply.   From the money you save i would try to get a better graphics card. The one you have is gonna be alright.. ofc it depends on the resolution you play at,  but if you can get a better one for the same budget why not try.  And no, this year AMD made pretty good cards so you don't have to worry about amd being bad  If you can't return it's not the end of the world you just overspent a bit on some stuff but you could use them later when you upgrade the system",buildapc,2025-09-28 16:46:33,1
AMD,ngpjlke,"1000W power supply seems to big, 850 would have been fine.  You also went overboard on the mobo, a B850 is even maybe overkill, a B650 would have been fine.  Not a big deal, just could have saved a bit there.",buildapc,2025-09-28 20:09:14,1
AMD,ngo1umf,You made an unforgiveable mistake by switching to the forbidden fruit (which is not to be named)!,buildapc,2025-09-28 15:55:34,1
AMD,ngo57oq,Why pay for all them pci5.0 lanes and opt for a 4.0 ssd. I would go up on that,buildapc,2025-09-28 16:11:35,0
AMD,ngo8n9x,"Id have gone with a different case, and an nvidia gpu. AMD drivers arent even close to as good as nvidias. Im sure it will work fine though. just keep an eye on your temps.",buildapc,2025-09-28 16:28:10,0
AMD,ngo9033,What are the differences? Is it worth me getting a B850 over a B650?,buildapc,2025-09-28 16:29:53,1
AMD,ngsarh6,Thanks,buildapc,2025-09-29 06:22:06,1
AMD,ngo1vf3,I agree. The great thing about building PCs though is if you have some overpowered things like that you just future proof it for upgrades later 😆,buildapc,2025-09-28 15:55:41,1
AMD,ngog28h,> air cooler would be enough  Then again that AIO is like $70. Any decent air cooling will get near that as well.,buildapc,2025-09-28 17:03:43,1
AMD,ngsajef,thanks,buildapc,2025-09-29 06:19:58,1
AMD,ngsalv8,Thanks,buildapc,2025-09-29 06:20:36,2
AMD,ngpf4cx,Fruit > Transparent Pane for work. Transparent Pane is such a clusterfuck of an OS.,buildapc,2025-09-28 19:47:46,1
AMD,ngo80wg,What did you get?,buildapc,2025-09-28 16:25:13,1
AMD,ngoechr,"I think b850 has pcie5, but only b650e mobos have that.",buildapc,2025-09-28 16:55:30,1
AMD,ngoo3sf,Pcie5,buildapc,2025-09-28 17:40:25,1
AMD,ngokryj,[https://pcpartpicker.com/product/hYxRsY/thermalright-peerless-assassin-120-se-6617-cfm-cpu-cooler-pa120-se-d3](https://pcpartpicker.com/product/hYxRsY/thermalright-peerless-assassin-120-se-6617-cfm-cpu-cooler-pa120-se-d3)  [https://pcpartpicker.com/product/GpbRsY/thermalright-phantom-spirit-120-se-6617-cfm-cpu-cooler-ps120se](https://pcpartpicker.com/product/GpbRsY/thermalright-phantom-spirit-120-se-6617-cfm-cpu-cooler-ps120se)  These have really great price / performance value if you can find it around 35usd price tag. But yeah Arctic has well priced and well performing AIOs.,buildapc,2025-09-28 17:25:28,1
AMD,ngob3hx,"Oh I'm still on my am4 build for a while. I have an sn850. But since you're starting fresh, might as well get the most out of your hardware. Otherwise get a cheaper am5 that has only 4.0 m.2, maybe one 5.0 slot for the future, to save some cash",buildapc,2025-09-28 16:40:03,1
AMD,ngg4khb,"I went with the 9700X over the 7800X3D and 9800X3D after lots of debating.   Has exceeded my expectations, with no buyers remorse whatsoever.",buildapc,2025-09-27 08:13:32,36
AMD,ngfql2n,"Hardware Unboxed has a fantastic video with CPU scaling testing. tl;dr even with a 5080, in 1440p the difference between 9700X and 9800X3D is rather small.   Certainly not worth that price hike.",buildapc,2025-09-27 06:00:39,55
AMD,ngfqgr0,Any 7000/9000 series is acceptable for this setup and use case IMO. 9800X3D could be useful if you play on low settings and/or CPU intensive games. To get a full picture what are the prices for:  7500F  7600  7600X  9600X  7700  7700X  9700X  7600X3D  7800X3D  9800X3D,buildapc,2025-09-27 05:59:34,7
AMD,ngghe4x,"9800x3d also helps with the low 1% and I think that is necessary even at 1440p since I care more about lows rather than avg fps. If you can afford it, I would say go for the 9800x3d.",buildapc,2025-09-27 10:23:46,5
AMD,ngglwm1,"See people building 9800X3D systems when they should be spending more on the GPU and less on the CPU.  It's fine for high end video cards at lower resolutions, but not as necessary with slower cards or at higher resolutions, but people get caught up in herd mentality and everyone now wants AMD.  Fact is a 265K bundle from microcenter is a good value and perfectly fine for gaming at high resolutions.",buildapc,2025-09-27 11:05:28,5
AMD,ngg92x8,"The 9700x is more than enough, no need to drop $500 for a 9800x3d.  The difference at stake is minimal.",buildapc,2025-09-27 08:59:43,7
AMD,nghb2r9,"Нема сенсу витрачати стільки грошей на 9800X3D, особливо якщо граєте у сінглплеєрні ігри. У мене самого Ryzen 7 7700 + RTX 4080 Super, що по характеристикам майже як 9700Х + 5070TI. І моя відеокарта завжди завантажена на 100% у всіх сучасних іграх (у 1440р).",buildapc,2025-09-27 13:56:53,6
AMD,ngfyzhy,For single player games at 1440p definitely the 9700x. Maybe try to get the 7700x bc is the same and cheaper.,buildapc,2025-09-27 07:18:20,4
AMD,nggh9sv,">I practically don't play online games at all.  You buy the 9800X3D because it performs better at unoptimized ""PvP"" oriented games (of all shorts), which obviously having 150 FPS with drops to 100, is better than 120 FPS with drops to 60.  For singe player games and not having the delusion that you lost because of the FPS difference, 9700X, 9600X, 13700, 14700 are all perfectly capable for the next 4-5 years.",buildapc,2025-09-27 10:22:36,2
AMD,nggz8jt,"Not worth it at all, the price difference is too big, and 9700X is more than enough since you are mostly playing single player game, X3D have more use in cpu intensive online game like Path of Exile/CS2, there isnt much opportunity for you to utilize it in single player game, except maybe Total War or such that need lots of computing for units",buildapc,2025-09-27 12:45:33,2
AMD,nghypnw,"For that much of price difference, 9700X is much better value. Where I live, which is South Korea, 9700X is around $250 while 9800X3D is around $380, so it makes sense to go for 9800X3D if you're a heavy gamer.",buildapc,2025-09-27 16:00:09,2
AMD,nghz1bz,"I literally have tons of iterations of gaming benches for both of these chips.  But for sake of easy comparison:  9700x CPUZ: https://imgur.com/a/x5s4wPE   9800x3d: https://www.reddit.com/r/ASRock/comments/1ldq81h/9800x3d_asrock_b850m_pro_rs_wifi/mycnwvg/  In gaming when tuned ram, 1440p max, CPU matters much less than the reviews all based on 1080p.  You won't feel the difference unless you have telemetry up to show numbers are a bit different.  1% lows on 9700x is lower than even OC'd 7800x3d, but you won't be able to tell.  You're not aiming for top end components, there's no need to spend extra 250.  It is not worth that much more",buildapc,2025-09-27 16:01:47,2
AMD,ngk34v6,"Get a 9800X3D only if you have a 5090. This is how good this CPU is.  I've got a 9700X with a 9070XT, gaming on 1440p, and it's already too much, I could easily have gone with a 9600X",buildapc,2025-09-27 22:46:53,2
AMD,ngkdg1t,"IIRC Spider-Man is quite heavy on CPU so 9800X3D may have some edge in reaching 120fps for this game. GOW is a cross gen game so it has to be super optimized for CPU. Doom is also super optimized for CPU  If reaching 120fps is your top goal, you can always try to strike it with with a lower actual rendering resolution and DLSS so my way of thought is always only considering the raw performance of the CPU",buildapc,2025-09-27 23:49:29,2
AMD,nghid0j,"I think most of your games are just GPU heavy, my pick would be 7600/7600x/9600x. These will be the best price-for-performance and 6 core cpus are pretty capable cpu.  I personally don't even consider 9700x a decent upgrade,you are paying almost 2x or two times of what you pay with 6 core cpus + performance gains are very slim and not worth the extra premium. I have 7600, if i was to choose higher cpus then it would have been 7800x3D(this is pretty good value currently, in most regions) or 9800x3D. But again, i don't need it.  9700x = 290,  7500f/7600/7600x/9600x = 150-180(save 100 bucks or buy extra storage)  A video of [hardware unboxed](https://youtu.be/gpN4nyftQ3M?si=myBvrpSdJe8udNZ8).",buildapc,2025-09-27 14:36:45,2
AMD,nggtwxx,If you're playing single player go for 9700x. X3ds are only worth the premium IF you care about *competitive* performance,buildapc,2025-09-27 12:08:50,1
AMD,nghdwyy,I would even go for the 9600x instead,buildapc,2025-09-27 14:12:54,1
AMD,ngi7dja,"For a gaming computer that X3D is really nice. However, the $250 gap between the 9700X and 9800X3D is a lot. I would strongly recommend looking at what a 7800X3D costs in your area. In the US it usually works out to roughly $50 more. The generational gain of the 9800X3D vs the 7800X3D isn't all that much. The X3D chips are also well known to have significantly better 1% lows and more consistent framerates. Sudden frame rate drops can be quite noticeable.  [Check out the benchmarks from Gamer's Nexus.](https://gamersnexus.net/cpus/rip-intel-amd-ryzen-7-9800x3d-cpu-review-benchmarks-vs-7800x3d-285k-14900k-more#9800x3d-gaming-benchmarks) All the X3D chips are clustered at the top. The 7800X3D is the 2nd best CPU on the market for gaming. It is the best bang for your buck if you can get one. Hence why you saw a lot of 7800X3D builds even after the 9800X3D came out.  Don't forget that a lot of modern games utilize upscaling so you'll probably be running your games at 1080p internally.  I'm kind of shocked so many people are saying to get the 9700X when the 7800X3D is still available. The 7600X3D would be a great deal too, but there's no way you can get that in Ukraine. It is microcenter exclusive.",buildapc,2025-09-27 16:43:59,1
AMD,ngif6m2,Since you said no online gaming the 9700x. If you plan on online gaming the 9800x3d.,buildapc,2025-09-27 17:23:34,1
AMD,ngip52b,https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/19.html   Look at the charts. 9800x3d is typically around 8% faster than 9700x in 1440p,buildapc,2025-09-27 18:14:22,1
AMD,ngjbxd1,A buddy is helping me build a PC specifically for Battlefield 6. I originally started with the 9700X but ended up switching to the 9800X3D specifically because of how CPU intense BF 6 will be.,buildapc,2025-09-27 20:14:54,1
AMD,nh22fo5,if you plan on playing escape tarkov go with a x3d cpu,buildapc,2025-09-30 19:05:57,1
AMD,nggdeu2,Honestly if u are looking to save some money id get a 7700x if its cheaper. The difference is almost nothing when compared to a 9700x.   Now if u are looking for top tier high refresh rate gaming then maybe consider the 9800x3d as it is great for those with high refresh rate monitors,buildapc,2025-09-27 09:44:17,1
AMD,nggl44y,"If you're only gaming on your PC then a Ryzen 5 5600 would be fine, and they only cost ~£55.  Lots of people buy the best CPUs because they're the fastest, but find that they don't really need all that power for their use case.  I've been gaming at 4K with an i5 12400 (and a 7900XTX) for the last few years. The more powerful (and expensive) CPUs are only needed if you're streaming, multitasking or editing.",buildapc,2025-09-27 10:58:31,1
AMD,nggrqsp,If you have the budget for a Ryzen 7 9800X3D I would definitely go for that as it is an absolute beast of a CPU.,buildapc,2025-09-27 11:53:09,1
AMD,nghvfww,Honestly the price premium on X3D is not worth it.  Other then benchmarks... I don't think you'd notice.,buildapc,2025-09-27 15:44:15,1
AMD,ngfx0v7,"If you can't afford aford 9800x3d, get 7800x3d on that card there won't be any performance difference, 7800x3d is 30% faster than 9700x at gaming, about 7% behind 9800x3d.",buildapc,2025-09-27 06:59:53,0
AMD,ngg4q0b,Go big or go,buildapc,2025-09-27 08:15:04,0
AMD,ngftkil,"Go ahead with the 9800x3d, its worth.",buildapc,2025-09-27 06:27:50,-4
AMD,ngh7biu,"The Problem most people tend to forget is upscaling. If you use DLLS quality your gpu renders the image at below 1080p. Due to that you can run very easily into a gpu bottleneck . PC Games Hardware hast a good chart for that which is also way more balanced than HUB video. In CPU intensive games like Battlefield, Baldurs Gate 3 and so on a 9700x can and will bottleneck a 5070 and upwards. You should look at the 7800x3d imo in most region only a 50$ difference . But wouldnt pay more than 70$ plus for the 3d chip more than that its not worth it",buildapc,2025-09-27 13:34:57,0
AMD,ngh1kw0,"9700X has been fantastic for me, too.",buildapc,2025-09-27 13:00:40,8
AMD,nghcgaw,"Glad you're happy about your choice! I am currently considering both CPUs, what made you choose the 9700X?",buildapc,2025-09-27 14:04:47,3
AMD,nghty7z,"Thank you, bro. God bless you",buildapc,2025-09-27 15:36:40,1
AMD,ngfrdvf,"Thank you, my friend, God bless you.",buildapc,2025-09-27 06:08:01,19
AMD,ngg9uyk,And adding to this the price hike in some countries gets you the 12 core ryzen 9 which is almost definitely the better one because more cores cores are more beneficial than more cache,buildapc,2025-09-27 09:07:44,-16
AMD,ngfr8pl,Hi friend!    * 7500F - 164$  * 7600 - 186$  * 7600X - 217$ * 9600X - 237$ * 7700 - out of stock * 7700X - out of stock * 9700X - 290$ * 7600X3D - out of stock * 7800X3D - 400$ * 9800X3D - 550$,buildapc,2025-09-27 06:06:42,10
AMD,ngmws3d,Ty man. God bless you,buildapc,2025-09-28 12:03:23,1
AMD,ngjkcwj,I got a full pc with 9800X3D and 5090 FE for only 4K $,buildapc,2025-09-27 20:58:55,2
AMD,nggb5a7,"Thank you, bro. God bless you",buildapc,2025-09-27 09:20:59,3
AMD,nghpx2g,"Дякую вам за ваш час і допомогу, земляк :)",buildapc,2025-09-27 15:15:42,4
AMD,nghplgk,"Yeah, bro, I'm not a fan of online games like CS or Dota. I like traditional story-driven games where you upgrade and follow your character's story. It's like a book to me, only much more interesting (and more valuable). Lmao      Thanks for your attention and reply.",buildapc,2025-09-27 15:14:02,2
AMD,ngmwo62,"Ty, man. God bless you",buildapc,2025-09-28 12:02:32,1
AMD,nghtiva,Thanks for your attention and reply <3,buildapc,2025-09-27 15:34:30,1
AMD,nghzkvf,It's crazy how some people would argue against this by using 1080p reviews on games people won't be playing.  They don't even have skin in the game,buildapc,2025-09-27 16:04:30,-1
AMD,ngg8kg3,Home,buildapc,2025-09-27 08:54:29,2
AMD,nghjt8p,"Not a psychic, but it's going to be bang for the buck",buildapc,2025-09-27 14:44:21,5
AMD,ngha59s,yeah 7800x3d if you want an x3d,buildapc,2025-09-27 13:51:30,5
AMD,ngga91u,Not for gaming tho. That x3d cache is what makes it the best processor for games.,buildapc,2025-09-27 09:11:43,18
AMD,ngh6fly,Most games utilize up to 8 cores,buildapc,2025-09-27 13:29:47,1
AMD,ngfryl9,"My picks would be: 7500F, 9600X, 7800X3D, 9800X3D at those prices (unless you need 8 cores for something). Here's how they compare when completely CPU bound (720p with a 4090):  https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/17.html  At 1440p the difference will be lower, here's 1440p Ultra with a 4090:  https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/19.html  But if you lower settings and enable DLSS at 1440p the difference will grow again",buildapc,2025-09-27 06:13:17,10
AMD,ngh64zv,"I just got a 9600x for $193, not on sale or anything",buildapc,2025-09-27 13:28:03,1
AMD,ngmwx7g,"Wow it’s amazing, but 5090 in my region = 2500$… :(",buildapc,2025-09-28 12:04:27,1
AMD,ngneszh,I watched some video and I found Spiderman 2 is so heavy that 9800X3D may also drop below 60 when maxed out and 9700x will probably drop harder. Switching to a more reasonable setting is probably a good idea but 120fps might be just very difficult to reach for any CPU,buildapc,2025-09-28 13:59:32,2
AMD,nghp6kp,"It's only a 40 dollhair difference here, that's why I asked. Is the difference bigger in Murica?",buildapc,2025-09-27 15:11:56,1
AMD,nggajtj,If your playing at 1080p at low settings then it makes a noticeable difference. If youre playing 1080 with everything maxed out or at higher resolutions youre more GPU bound and it makes like 5-10% difference at most. The 12 core ryzen 9 would make the whole PC feel more responsive in comparison,buildapc,2025-09-27 09:14:49,-11
AMD,ngmxnpl,"Here in the EU it was around 3000€>  Since September, the FE has been in stock on Nvidia 's official store for 2100€ (basically MSRP)  Bought it immediately.",buildapc,2025-09-28 12:09:54,1
AMD,nghsw52,"In US the price difference is like 150 USD, which is more inline with official pricing.  I also went with a 9700X. At 1440p and a mid range GPU, there's little benefit to a 98000X3D. I was actually going to do 9900X, which is still much less than 9800X3D, and way better at most production tasks, but I found a 9700X deal.",buildapc,2025-09-27 15:31:15,3
AMD,nghxe1z,Makes sense. Thanks for sharing!,buildapc,2025-09-27 15:53:45,1
AMD,nf3fq6h,"Lol 5500/4090 is a wild combo. If you don’t want to switch sockets, try to find a used 5700X3D or 5800X3D.",buildapc,2025-09-19 16:22:20,31
AMD,nf3gat9,"That might be the craziest GPU/CPU combo I have seen.  As far as AM4 CPUs go, that is the fastest one you can get for gaming and should keep up with the 4090 in just about any game out in 1440p and above.  Depending on how much the 5800x3d costs, it may be a better option to look into AM5 CPUs that would have more longevity. You would need a new board and RAM but generally it is better value to move to AM5 instead of spending $250+ on a good AM4 x3d CPU right now.",buildapc,2025-09-19 16:25:06,16
AMD,nf3h2gs,Get AM5 honestly,buildapc,2025-09-19 16:28:47,10
AMD,nf3g4j9,"The 5800X3D was discontinued like a year ago.  There is also the 5700X3D, but it was discontinued about a month ago.  See if you can find either of them used for a decent price.",buildapc,2025-09-19 16:24:15,6
AMD,nf3mpty,"A 5**6**00 or 5600X is already about a 15% performance lift over the 5500.  A 5800X3D is about 60% faster than the 5500, but is *insanely* priced at the moment, going for nearly $700. The easier to find 5700X3D is 50% faster than the 5500, but still goes for $350 retail at the moment. Though you can find it for around $260 used on eBay.  For around $430 you can get a 7600X, decent B650 motherboard, 32GB of DDR5 6000 CL30, and a cooler which is roughly on par with the 5700X3D, and with another $20 more the 9600X roughly matches the 5800X3D.  [https://pcpartpicker.com/list/CrhWpK](https://pcpartpicker.com/list/CrhWpK)  You can also get a used 7500F off of eBay for around $130, bringing the total cost down below $400. If you can fit an ATX motherboard in your case, the Gigabyte B650 Gaming X AX is also $129 right now, cutting another $20 off that price. Meaning you'd be looking at around $370 for a full AM5 swap with performance roughly on par with the 5700X3D.  I should note the 7500F is only *slightly* slower than the 7600X. Not the same extreme difference as the 5500 and 5600X. The Ryzen 5 5500 is AMD's mobile version of the Zen 3 CPU with the iGPU fused off, where as the 5600 and higher are the desktop Zen 3 with significantly more cache and higher clock speeds. The Ryzen 5 7500F is a 7600 with the iGPU fused off and 100Mhz lower clock speed.",buildapc,2025-09-19 16:55:57,5
AMD,nf3gzt1,"it would be a nice boost .. if your motherboard supports it, id be sure, and id also check if the board supports pcie4. if it doesnt id probably just get an budget am5 setup cpu/board/ram, and even a budget am5 will outperform a x3d on a 450 board ..",buildapc,2025-09-19 16:28:27,3
AMD,nf3l9fc,"I bought my step-daughter a prebuilt 5500 with a 3060 two years ago. Her father stole money she had been saving for a gaming pc. At that time, that was all I could swing. I’ve since replaced the 5500 with a 5800X3D and swapped out the 3060 with a 7800XT. It is like a completely different machine. But honestly, with prices where they are on the 5800x3d I would just put in a AM5 board and a 7800x3d.",buildapc,2025-09-19 16:49:06,2
AMD,nf3rer4,"5700x3d/5800x3d roughly matches 7500f/7600 performance levels. Unfortunately, I dont see those AM4 chips under $260-$300 used very much while the 7500f can be had for as little as $160 on Aliexpress. At this point, I would go straight to AM5 instead of upgrading AM4. Throw in about $100 for an mATX motherboard and $80 for 32gb DDR5, finally sell your old cpu/mobo/RAM for about $120 and honestly you may come out ahead on a current platform with faster memory.",buildapc,2025-09-19 17:18:05,2
AMD,nf40ghp,"it makes no sense to buy a 5X00x3d anymore since you're gonna pay $300 minimum and it is probably preowned. at that point another $100-200 gives you an am5 mobo/ddr5/cpu.  also, with a 4090 you're not playing at 1080p like the cpu-bound benchmark videos.  just get a $160 ryzen 5800xt instead: [amazon link](https://www.amazon.com/AMD-RyzenTM-5800XT-16-Thread-Processor/dp/B0D6NNDQ92)",buildapc,2025-09-19 18:01:09,2
AMD,nf4eaxt,Being discontinued? The 5800X3D production has been discontinued since 2024. You can't get a good retail price 5800X3D now.,buildapc,2025-09-19 19:09:32,2
AMD,nf4k8wc,Honestly you really should upgrade to AM5 and just get a modern CPU and RAM for that GPU,buildapc,2025-09-19 19:39:28,2
AMD,nf5k86n,"I had this dilemma too, upgrading from a 5500, I ultimately decided to just get a 5070ti then wait a few months and fully upgrade to AM5 cause GPU was the first priority for me",buildapc,2025-09-19 22:50:36,2
AMD,nf629sw,"What is your budget? A 7600x bundle will cost about the same as a 5800x3d so I'd just go AM5. If you have the budget, you could go even faster, like a 9700x or 7800x3d. The 4090 is a beastly card. What resolution is your monitor and what games do you play?",buildapc,2025-09-20 00:37:15,2
AMD,nf7ddea,"If you buy a new 5800x3d it will be the same cost as a new 7600x3d mobo+cpu+ram combo from microcenter. Gets your foot in the door to the next socket, a good gaming cpu, and 32gb of ram.",buildapc,2025-09-20 06:21:13,2
AMD,nf3jzo9,Honestly I’d sell the 4090 and get a new setup with a 4080 lol. How much do you actually need the 4090?,buildapc,2025-09-19 16:43:04,2
AMD,nfog2l1,"the ryzen 9 5950x is super cheap atm, but the 5800x3d is much better for gaming",buildapc,2025-09-22 22:46:49,1
AMD,nf42btz,i think people who haven't looked at prices in a while assume they cost $200 or $250. but it's like $300 now for any am4 x3d.,buildapc,2025-09-19 18:10:21,16
AMD,nf3g3rh,i know i got my pc prebuilt as a gift 2 years ago and im just know doing research on it 💔,buildapc,2025-09-19 16:24:09,4
AMD,nf7bzhf,So crazy,buildapc,2025-09-20 06:08:25,1
AMD,nf3hna8,"costs is way more, cpu+mobo+memory vs just a cpu.",buildapc,2025-09-19 16:31:35,-3
AMD,nf8n1pn,I bought one at micro center last week..139$,buildapc,2025-09-20 12:59:51,1
AMD,nf3rgcb,This is the one !!,buildapc,2025-09-19 17:18:17,2
AMD,nf3tlhl,would getting a combo mobo cpu ram on newegg also be a good option?,buildapc,2025-09-19 17:28:26,1
AMD,nfahje4,i don’t really have a budget if i need to i’ll get it eventually. I have a 1440p monitor right now and i mainly play fps games like marvel rivals and overwatch.,buildapc,2025-09-20 18:42:56,1
AMD,nf3kmde,"I’m not really sure. This pc was a prebuilt gift, and i’m just now realizing how weird my gpu/CPU combo is. I’m trying to “fix it” so my fps in games are better and i’m not bottlenecking my cpu",buildapc,2025-09-19 16:46:05,3
AMD,nf7bxha,You now I understand that!,buildapc,2025-09-20 06:07:54,2
AMD,nf3i4a1,"With how insane the 5700X3D and especially 5800X3D pricing has gotten, it's not.",buildapc,2025-09-19 16:33:53,12
AMD,nf3k0sb,5700X3D is way too overpriced,buildapc,2025-09-19 16:43:13,2
AMD,nf3rnqw,Unfortunately the AM4 x3d chips are way too expensive rn. He'll have the ability to sell his AM4 stuff and make some money back too. AM5 just makes too much sense in this market,buildapc,2025-09-19 17:19:15,2
AMD,nf3vukt,"It could be. Its not always a huge savings vs buying everything on its own, and Newegg limits the combo options. I prefer to use PC part picker, sort price low to high, and buy the highest rated option with the features I want. Heres an example of what I found: [https://pcpartpicker.com/list/VzKkKX](https://pcpartpicker.com/list/VzKkKX)  I made a newegg combo thats very similar but with the 7600 instead of 7600x. The Newegg combo saved about $15 but for a slightly slower clocked CPU.",buildapc,2025-09-19 17:39:10,2
AMD,nfcj8cd,"A 7600x will be fine in most games, especially at 1440p 144hz but as a fellow fps player I'd get a higher tier for better 1% lows (especially in rivals). A 9700x or 7/9800x3d chip will give you a smoother experience. If you have a 240hz panel then you should definitely get the faster chips.",buildapc,2025-09-21 01:42:21,2
AMD,nf3raxr,"Nah don’t sell the 4090 they are getting up in value as the time goes by, if you want to save money you can buy some used parts, you could even begin with a 7600x and a decent b650, and later on upgrade to a 7800x3d, also AliExpress has really good deals on cpus, I bought a R7 7700 for 165",buildapc,2025-09-19 17:17:35,4
AMD,nf4ydgt,"yes as other commentator said, do not sell 4090, it is a very powerful card and it will last very long",buildapc,2025-09-19 20:49:47,3
AMD,nf3jbh3,yeah it’s avg 350-400 right now.,buildapc,2025-09-19 16:39:48,2
AMD,nf40z1x,but you don't need to buy those overpriced cpus.,buildapc,2025-09-19 18:03:42,1
AMD,nf52gh5,£210 here.,buildapc,2025-09-19 21:10:51,1
AMD,nf3rw7p,"For $400 just go to AM5, AM4 is a dead socket and is 100% not worth spending that kind of money to get more performance out of it.   For $500 you can get a used 7800x3d with a mobo and ram that would beat the 5800x3d in everything while being on the newer platform.",buildapc,2025-09-19 17:20:21,8
AMD,nf3za0m,"$400 was the price I paid for a 9700x/b650/32gb ram bundle I bought. Not worth getting the AM4 CPUs at those prices, unfortunately.",buildapc,2025-09-19 17:55:25,5
AMD,nf43hhf,"The comparison is because the performance between the entry AM5 CPUs is roughly on par with the 5\*00X3D CPUs.  I mentioned below that the Ryzen 5600 is already about 15% faster than the Ryzen 5500. The Ryzen 5800XT is maybe the fastest non-X3D AM4 CPU for gaming, but it's 4% faster at best than the Ryzen 5600. Looking at AM4 CPU pricing right now, I'd actually suggest the Ryzen 5700 since it's got 8 cores and slightly higher base clock speeds, and is only $10 more than the 5600. A few weeks ago the 5800XT was on sale for the same price as the 5600, which would have been a great buy. But at it's current price isn't worth the premium.",buildapc,2025-09-19 18:16:02,2
AMD,nf6l422,Then what is he upgrading to?,buildapc,2025-09-20 02:38:24,1
AMD,nf6lt69,"on amazon, the 5600 is $140 and the 5800xt is $160.  so if you're sensitive about a $20 diff (hey, get the 5600 then), then surely a full am5 upgrade is not on the table since it's going to be over $500 to swap everything.  i think your point is to just not upgrade at all since it's so marginal (unless you pay big for the x3d) and instead plan on am5 being your next upgrade when you need it.",buildapc,2025-09-20 02:43:07,1
AMD,nf6lyc5,"nothing. unless they have a specific perf target and the 5600 is too slow somehow, what are they upgrading for.",buildapc,2025-09-20 02:44:07,0
AMD,nf8itsh,But he wants to upgrade so your comment is not helpfull,buildapc,2025-09-20 12:33:04,1
AMD,nf9v9wp,"everyone wants to upgrade because it's mostly consumerism. their options are pay big for am4 x3d or pay even bigger for am5.  their question has been answered like 20 times, so now we can move on to the commentary about the answers people provided. got it?",buildapc,2025-09-20 16:52:09,0
AMD,nfa31wp,"Nope, not helpful.",buildapc,2025-09-20 17:29:45,1
AMD,nfa41e0,is your feedback helpful?,buildapc,2025-09-20 17:34:39,1
AMD,nflcen0,"I play at 1440p with a 5600X and 6750XT. Normally I will hit 50-60% utilization. With the release of Borderlands 4,  a very demanding UE5 game that I use FSR and FrameGen on, I am consistently seeing 80-90+% utilization.",buildapc,2025-09-22 13:04:54,11
AMD,nflgygs,I recommend you run some other CPU related benchmarks to see where you performance is at.,buildapc,2025-09-22 13:30:27,7
AMD,nfmztrq,"Why are you concerned about this and why are you locking games to 60-90 fps when you have a 240Hz screen?   Is performance hampered if you play it at normal settings?   The usage should be high if your frames are uncapped and your GPU isn't stressed, which yours won't be. Operating temps of 80-85C are also normal.",buildapc,2025-09-22 17:57:44,3
AMD,nflqtws,Your cpu temps seem a little high. If this was a newer chip it wouldn’t be a problem.    You might be able to gain a little performance with a repaste or a different cooler.  Can you tell us what you have now so we can see if there is a problem?,buildapc,2025-09-22 14:22:33,2
AMD,nfm16zo,Do you have DLSS/FSR enabled?,buildapc,2025-09-22 15:13:16,1
AMD,nfm4a4m,"What games are struggling to run at 60 fps at 1080? I’d think your system would be more than capable of pushing double that, even at 1440p   In task manager, performance, click on CPU and see if all the cores are being maxed out, or just a few",buildapc,2025-09-22 15:28:12,1
AMD,nfm91yw,"I would say increase your graphics settings. By using low settings, at 1080p, you’re not letting the GPU work and is “showing” the CPU “bottleneck”.   Those temps for the 5800x on an air cooler aren’t too far out of normal especially if you’re near 100% usage. It’s a hot chip. IF you want it to run at a lower temp, you could just set 65w eco mode in the bios. It has a small performance hit that most people claim to not really notice/care about.  Something still seems off though. But do report back how your FPS changes when utilizing more GPU.",buildapc,2025-09-22 15:51:03,1
AMD,nfo8mw9,My 5800x was running so hot I had to lower the voltage and clock rate to stop my computer from shutting down whenever i played an intensive game. But I believe it was overclocked without me realizing it. No more sputters and crashes after I lowered the settings a bit in Ryzen Master,buildapc,2025-09-22 21:59:22,1
AMD,nfo8rld,5800x gets hot very fast and stays this way. Apparently that's how it should work. It was said it's because of the higher power draw. Don't know if that's correct though. Anyway because of temperature I took it out 20 minutes after I've put it in and returned it. Got 5700x instead and it's ok.,buildapc,2025-09-22 22:00:11,1
AMD,nfqmlqm,"Yeah, this sounds about right newer games will use as much CPU as u can give them. Maybe turn down some of the more CPU intense settings.   For B06 definitely turn off the water caustics and extra shadows. Same with reflections and particles",buildapc,2025-09-23 09:42:45,1
AMD,nfl98it,"The 5800x3d can't have a 90% utilization in bo6 with a 60fps lock. Something is wrong. Maybe some cores are deactivated or an software issue? If you open the task manager and check utilization while playing, are really 80-90% of the CPU utilization going towards the game?  Normally it's not a bad thing to have a high utilization on the cpu as long as you get the needed fps.",buildapc,2025-09-22 12:46:04,2
AMD,nfl83s6,1080p or 1440p?  Is your monitor plugged into the GPU or the mobo?,buildapc,2025-09-22 12:39:07,0
AMD,nfl6wj1,"i dunno, so far i havent run into any games that pushes my 5600 past 70 or so percent ..i know theres some crap coded games out there though, especially new releases, that can even choke a 5090 lol .. i wouldnt cater my build to a junk game ..",buildapc,2025-09-22 12:31:32,0
AMD,nflgepd,I think from another persons suggestion it maybe me not playing at 1440p possibly causing the problem as the GPU isn’t being used to its fullest.,buildapc,2025-09-22 13:27:23,5
AMD,nflj1qy,What programs would you recommend?,buildapc,2025-09-22 13:42:02,3
AMD,nfna1jw,"I'm concerned because the performance is being hampered with stutters on some games. I'd like to run at a smooth fps and the reason I lock it at 60-90fps on some story games is because I don't need all the frames as opposed to a comp game. Again I only play zombies on Black ops 6 not much multiplayer so no need to be on 240hz, 120fps on zombies is just fine.",buildapc,2025-09-22 18:47:47,1
AMD,nfltl6q,The cooler I have now is a noctua dh-15,buildapc,2025-09-22 14:36:17,1
AMD,nfm3mh7,"I tend to force DLSS 4 for newer games, and sometimes use FSR",buildapc,2025-09-22 15:25:01,1
AMD,nfm3n28,"I tend to force DLSS 4 for newer games, and sometimes use FSR",buildapc,2025-09-22 15:25:06,1
AMD,nfnbajc,"Black Ops 6, Stellar Blade, the new skate they all use like 100% of my CPU. However just looking at task manager and all my cores are being used",buildapc,2025-09-22 18:54:24,1
AMD,nfma0ij,"The trouble I find with increasing the graphics sometimes is figuring out what settings impact the GPU and what ones impact the CPU, thankfully some games will tell you",buildapc,2025-09-22 15:55:32,1
AMD,nflz411,"> The 5800x3d  They have a 5800x, not a 5800x3d.",buildapc,2025-09-22 15:03:06,4
AMD,nflayjk,Well to be fair in Black Ops 6s case it’s locked to 120fps,buildapc,2025-09-22 12:56:24,3
AMD,nfl8hff,1080p and yeah it’s plugged into the GPU through display port as I’m on a 240hz monitor,buildapc,2025-09-22 12:41:29,2
AMD,nflgswe,> Is your monitor plugged into the GPU or the mobo?  How is this relevant to the high CPU utilization?,buildapc,2025-09-22 13:29:36,2
AMD,nflx530,As I understand it it's actually lower resolutions that require more CPU as that tends to push FPS higher.,buildapc,2025-09-22 14:53:39,1
AMD,nflpz8i,Use passmark and cinebench r23,buildapc,2025-09-22 14:18:19,4
AMD,nfmgsnp,Cinebench r23 is very good choice,buildapc,2025-09-22 16:28:17,2
AMD,nflk6b7,Passmark and userbenchmark should be enough.   __________________   Note: userbenchmark's opinions and rankings are questionnable to say the least. But the benchmark pass itself should also be appropriate to highlight any performance difference between your CPU and the average 5800x,buildapc,2025-09-22 13:48:10,0
AMD,nfnieb0,"What's CPU usage on idle? You can easily check what's eating up the usage just using task manager. Stuttering could be caused by a number of things ranging from minor to replacement part. Unfortunately it's not very descriptive.   My 2 cents is that there's absolutely no reason to be limiting your fps here. You paid for 240hz, you should use it regardless of if it's a competitive game or not. Change all settings back to normal, uncap the FPS and enable/disable Vsync and see if it makes a difference.  Your rig is more than capable of 1440p high refresh gaming, let alone 1080p.",buildapc,2025-09-22 19:33:34,2
AMD,nfluhqa,"Those are decent. I would check fan curve and airflow are ok, clean from dust and consider a repaste with better stuff or ptm.",buildapc,2025-09-22 14:40:46,1
AMD,nfm5ohx,"Thats why. DLSS and FSR leverage CPU performance.   This is why for those who run 1440p and especially 4K, its why CPU performance still matters - because of scaling tech.    If you disable it entirely to run native 1080p you'll see CPU utilization drop to 40% or less and GPU utilization will be higher.   IMO DLSS/FSR at 1080p is kinda useless. An argument can be made regarding it looking ""cleaner"" and ""less aliased"" than native, but that can also be achieved by running the game at a higher base resolution with DLSS, or use DLAA - either as an in-game option if available, or at the driver level.   In which case, its all normal and totally dependent on what your settings are.   If you're happy with the performance you're getting, then no need to change anything as everything is working as intended.",buildapc,2025-09-22 15:34:59,1
AMD,nfni9w9,Time for a 7800x3d or 9800 if your feeling fancy,buildapc,2025-09-22 19:32:53,1
AMD,nfm2uew,"Ah, my mistake. Nonetheless should it perform better.",buildapc,2025-09-22 15:21:15,0
AMD,nflca0i,Still. It should look like this https://youtu.be/WjNIJOIscpY?si=3d1WmX_lY_pSDFu6 (Minute 9:40). Even at 1080p Low settings your 3070 should be the limiting factor.,buildapc,2025-09-22 13:04:10,1
AMD,nflhxoo,"Some people, having bad graphic results, switch all their settings to more cpu intensive graphics. Step one of diagnosis: maybe he had bad graphics because he wasn't using the GPU.",buildapc,2025-09-22 13:35:54,-1
AMD,nfmniou,"Effectively, yeah; it basically takes the same amount of work for the CPU to generate the information for a frame at 1080p or 1440p, but at lower resolutions it's easier for the GPU to generate a frame, which means it's asking the CPU for new information to draw more often.",buildapc,2025-09-22 17:00:03,2
AMD,nfnbguo,Would you by any chance have any recommendations for good fan curves?,buildapc,2025-09-22 18:55:21,1
AMD,nfm6hg0,That was my initial problem with turning off DLSS or FSR as most games have TAA forced and it makes game look very blurry.  So to add onto your point would you recommend me using in the nvidia control panel DSR to upscale to 1440p? Even though I’m on a 1080p monitor?,buildapc,2025-09-22 15:38:51,1
AMD,nflfkq1,"Sadly it’s not doing that for me, I mainly play zombies and sometimes it’ll dip below 90fps when I’ve locked it at 120fps.",buildapc,2025-09-22 13:22:48,1
AMD,nflomia,5800x and most AM4 stuff don't have any integrated graphics.,buildapc,2025-09-22 14:11:31,4
AMD,nflimmu,"I see what you are getting at, but there is no world in which running BO6 results on the integrated graphics of a 5800x results in higher CPU usage than GPU usage. That's literally impossible, that iGPU can barely drive minesweeper.",buildapc,2025-09-22 13:39:43,-1
AMD,nfnlpi6,"That's going to vary by system.  There are a lot of variables like the case, fans in use, cooler, CPU, other components, etc.    The first thing to try is just putting your fans at 100% and see if it can keep the chip cool. Obviously that isn't ideal long term.  Then I would put some kind of load on it like cinebench or something and then play with the fan curve in software while it's going.  You want to balance temps with noise and find what you can tolerate while still maintaining good temps.  if your CPU is hot on 100% fans, then either the fan setup is bad (not enough, pointed the wrong way, not enough intake, etc) or its not enough for the chip.  A 5800x should be fine with your cooler.  I've run a 5800x on a smaller noctua than that and done ok.  However, my system had several additional case fans too.",buildapc,2025-09-22 19:51:53,1
AMD,nfm7gz4,"No, using DSR is a global change to your desktop.   You can simply go into each games settings and there should be an option for internal resolution/render resolution - 178% of 1080p would mean internal render resolution would have the game run at around 1440p. Then you can use DLSS/FSR to Balanced or Quality to rescale it to a lower output resolution at around 1080p.   Or, again, force DLAA.",buildapc,2025-09-22 15:43:34,1
AMD,nfllv4n,And its not an overheating issue? In the video you see the frequency the CPU hits. Does your cou hit the Same frequencies?,buildapc,2025-09-22 13:57:08,2
AMD,nflpp6y,There is no igpu on that model,buildapc,2025-09-22 14:16:56,3
AMD,nfm7tdz,"Ahhhh I understand ya, I’ll give it a go",buildapc,2025-09-22 15:45:10,1
AMD,ngxs1xm,What's your current hardware spec and what type of plugins do you plan on using? What buffer size and sample rate?,buildapc,2025-09-30 02:06:00,1
AMD,ngso767,"Since your GPU is dying, I’d say get the best one you can afford… with the intent of replacing the CPU. At 1080, most GPUs will be great but the CPU will struggle feeding it. If there’s no intention of a CPU upgrade down the road, I’d even consider a 3060 for 1080p.",buildapc,2025-09-29 08:38:36,10
AMD,ngsmyi5,"Best bet is get the Rtx 4060, or the better value Rx 6700 XT because they can play almost any game at 60 fps. If it's available, get the Rtx 2080 ti because it's the Best performer of them both, and will utilize the CPU to it's absolute limit. My recommendation is the Rx 6700 XT because it will have infinite drivers and has 12GB of vram, so if you wanna play at 1440p, you can.",buildapc,2025-09-29 08:25:22,5
AMD,ngsovje,I had the same setup.  I upgraded to a 6800xt and then swapped the CPU out for a 5700x.  I have no problems playing games at 2k ultra wide.,buildapc,2025-09-29 08:45:40,3
AMD,ngssokx,"I’m currently upgrading from the same cpu, the gpu that was in the pre build was a 1660. Ngl, it’s served its purpose well for many years.   Don’t get me wrong it’s not an amazing card, but I don’t play games on high really. Never had issues with games like apex and alike.   Not telling you to get a 1660 just throwing my 2 cents in",buildapc,2025-09-29 09:25:25,3
AMD,nguf127,"Why worry about bottlenecks? Why try to be efficient?   You have a 5 year old CPU and a what, six year old GPU?   Upgrade to the best GPU you can afford. Then think about CPU next year.   I've had a 3600 for 5 years and been running 3060ti for three years. It still goes fine - been wanting an upgrade for a while but everything I play is still totally fine at 1440p so it's hard to justify.  Buy the best GPU you can reasonably afford without being silly. Eventually you are going to upgrade that CPU and probably Mboard and RAM too. And a 40 series card or, even better 90series AMD will still be relevant for years to come.  9060XT 16GB is such a sweet price point if you're on 1080. Unless you want the Nividia software tech then just don't worry about it.",buildapc,2025-09-29 15:41:44,3
AMD,ngsmli2,where do you live and what's your budget?,buildapc,2025-09-29 08:21:31,2
AMD,ngsn3zx,"maybe just 4060, used to have the same cpu and was fine with 4060. when I upgraded the graphics card further I upgraded the cpu because it didn't keep up",buildapc,2025-09-29 08:27:00,2
AMD,ngtdqjh,"Take [this table](https://www.tomshardware.com/reviews/gpu-hierarchy,4388-2.html) and find your GPU. I see 23% at 1440p, where a 4090 would be 100. Keep this number in mind.  Now you can use this table to calculate price/performance on any GPU you see for sale in your local used market. You can also use it to ballpark the performance uplift you want.",buildapc,2025-09-29 12:20:05,2
AMD,ngsq0m3,I’d go for a 16gb 9060xt.,buildapc,2025-09-29 08:57:30,1
AMD,ngtoads,The 3060 12gb version did so good eith the 3600.  I had it for a few years until I had to upgrade for borderlands 4 lol,buildapc,2025-09-29 13:23:45,1
AMD,ngtz29p,"5700xt  is real cheap, but it doesn't support Raytracing",buildapc,2025-09-29 14:22:08,1
AMD,ngstm23,5090,buildapc,2025-09-29 09:35:12,1
AMD,ngsmuix,"i’m South African and budget doesn’t matter rn because it’s speculative, so whatever is best for me i’ll save towards.",buildapc,2025-09-29 08:24:14,2
AMD,ngsrhl0,woah 😭 won’t that be bottleneck galore?,buildapc,2025-09-29 09:12:52,1
AMD,ngtrn7y,looking for something a little more powerful,buildapc,2025-09-29 13:42:47,1
AMD,ngssesl,"id go around the 9060xt 16gb, anything higher will be insane bottlenecks. even with it u will be cpu bottlenecked quite hard but its a relatively cheap gpu with alot of vram and insane clock speeds. and amd is amazing",buildapc,2025-09-29 09:22:33,6
AMD,ngsths0,"Ignore anything you read about ""bottlenecks"" - it's much more a marketing term than anything else for most situations.  You buy the best GPU you can afford to buy. You upgrade your system over time to the best you can afford to, whether that be upgrading the CPU to a 5800X (or 5700X3D if you can find one cheaply) or to upgrade to an AM5 system later on.",buildapc,2025-09-29 09:33:58,4
AMD,ngsulis,"No, what would be about the max your cpu should be able to handle without any real bottleneck. Maybe some newer game won’t like it as much, but it would still be the best you can get. The card is still only as fast as a 3070 and people paired that with a 3600 too.",buildapc,2025-09-29 09:45:15,1
AMD,ngzu7py,i'd go with the RX 9070 more VRAM better drivers better performance its more money but should last you longer,pcmasterrace,2025-09-30 12:22:18,13
AMD,ngzv59y,Bro that thing looks less like a PC and more like it’s about to open a portal to hell,pcmasterrace,2025-09-30 12:28:12,5
AMD,ngzu1xj,9070!!,pcmasterrace,2025-09-30 12:21:16,2
AMD,ngzvcow,"If you can afford it, the 9070 is way faster than the B580. It's not even close. And I think pairing it with an 11700K is totally fine. That's still a decently fast CPU.",pcmasterrace,2025-09-30 12:29:29,2
AMD,ngzvtsp,"That is legitimately one of the coolest PCs I've seen, such a shame it's nearing the end of it's life...  You BETTER build a worthy successor of it!!!",pcmasterrace,2025-09-30 12:32:28,2
AMD,nh0hyaz,man i playing on 1440p and 4k on a 9070. get that and thank yourself.,pcmasterrace,2025-09-30 14:33:21,2
AMD,nh00tsp,The 9070 XT is 1000 times better... Intel is dead 5 years ago...,pcmasterrace,2025-09-30 13:02:05,1
AMD,ngzvdkc,"So, put it in my current build first then build the other other one and swap over?",pcmasterrace,2025-09-30 12:29:38,3
AMD,ngzv973,Sick right? 😁 lmao,pcmasterrace,2025-09-30 12:28:52,2
AMD,ngzvfss,"Thanks, that's what I was concerned about",pcmasterrace,2025-09-30 12:30:02,1
AMD,ngzvy82,"Yes I know, I'm very easy to please just with Star wars stuff",pcmasterrace,2025-09-30 12:33:15,3
AMD,nh0ixmz,Think my current system can handle it?,pcmasterrace,2025-09-30 14:38:14,1
AMD,nh01b5l,Rip 🙏,pcmasterrace,2025-09-30 13:04:52,1
AMD,ngzvmir,yeah i know some people still rocking 11th gen intel you should be fine with the new GPU now and then a full upgrade later,pcmasterrace,2025-09-30 12:31:12,5
AMD,ngzxgty,You and me both,pcmasterrace,2025-09-30 12:42:34,3
AMD,nh1ezbz,with the 1050? fuck no,pcmasterrace,2025-09-30 17:13:54,1
AMD,ngzxdww,"Awesome good to know, thanks 👍",pcmasterrace,2025-09-30 12:42:04,2
AMD,nh1fzme,No I'm replacing it. The 1050. With the 9070. Can my 11700K and ddr4 ram work with the 9070?,pcmasterrace,2025-09-30 17:18:35,1
AMD,nh2oe2u,"that should be good depending on the game. cpu intensive games can be tough, but if youre planning on 1440p and/or 4k the gpu gets taxed alot more anyway. the 9070 is a great card, ive owned one for about 4 months now and im very impressed with it. theres some OC headroom too.",pcmasterrace,2025-09-30 20:51:28,2
AMD,ngg3qih,Look and see if UEFI CSM mode is disabled.,pcmasterrace,2025-09-27 08:05:09,1
AMD,ngg5qft,"Thanks. Yes, i did it.",pcmasterrace,2025-09-27 08:25:21,2
AMD,ngpms41,Both are horror themed experiences.,pcmasterrace,2025-09-28 20:24:30,69
AMD,ngpxpwv,"Well, let's compare the xenomorph from Alien: Isolation vs. Microsoft Office.  - Present because of the whims of an evil corporation? Check. - Has access to your entire system (or space station)? Check. - Soulless and uncanny? Check. - Wants to use you to nihilistically and apathetically advance its own interests? Check. - Plants its offspring in your chest, such that they later burst forth violently, killing you and unleashing a new terror into the world? Yes to the alien, but I'm assured those reports about Word are *probably* just fantastical speculation from crazy people.",pcmasterrace,2025-09-28 21:18:20,40
AMD,ngpkxjb,"You'd think they checked the VERSIONINFO data structure that is helpfully provided in nearly every Windows .exe and not just matched the file name, but apparently they're not smart enough for that.",pcmasterrace,2025-09-28 20:15:37,6
AMD,ngr59kx,"Alien Isolation's executable is probably ai.exe, and MS Office also has that. Weird how AMD's driver goes off just file name though, would have thought it would use more information than that.",pcmasterrace,2025-09-29 01:25:56,3
AMD,ngri864,it thinks Batman Arkham knight is elder scrolls online too,pcmasterrace,2025-09-29 02:42:26,3
AMD,ngsdreh,"I mean, ms office is way scarier than alien isolation playthrough",pcmasterrace,2025-09-29 06:51:06,2
AMD,ngsdzp2,r/technicallythetruth,pcmasterrace,2025-09-29 06:53:18,1
AMD,ngt9csz,https://preview.redd.it/32ri4p82e3sf1.jpeg?width=736&format=pjpg&auto=webp&s=7883aea66e850a824f302a5088c5953168f984ac  Nuke MS Office from orbit. It's the only way to be sure!,pcmasterrace,2025-09-29 11:50:17,1
AMD,ngqyul6,lmao the worst part is that ive never played alien isolation on this pc,pcmasterrace,2025-09-29 00:48:32,9
AMD,ngqyraj,spreadsheets in 240 FPS,pcmasterrace,2025-09-29 00:48:00,8
AMD,ngsvdhq,"On my PC, for some reason MSI Afterburner thinks Visio (and only Visio) is a game that needs FPS and CPU/GPU stats displayed on the upper left everywhere.",pcmasterrace,2025-09-29 09:52:53,1
AMD,ngt6v4i,Nvidia does the same right?,pcmasterrace,2025-09-29 11:32:15,1
AMD,nguef8r,![gif](giphy|LP0VSNTizueUU1YFlr|downsized),pcmasterrace,2025-09-29 15:38:47,9
AMD,ngucm38,"AMD, ""We designed it like that"".  /S    DDU and reinstall drivers my friend.",pcmasterrace,2025-09-29 15:29:54,9
AMD,ngv1ust,"And 0 W! incredible, you are generating power from nothing!",pcmasterrace,2025-09-29 17:31:13,3
AMD,ngupb5j,You can run Crysis now :),pcmasterrace,2025-09-29 16:31:36,2
AMD,ngvniaz,But can it run Minecraft.,pcmasterrace,2025-09-29 19:14:02,1
AMD,ngud9eo,"Apparently all it needed was a restart, the readings are now fine.",pcmasterrace,2025-09-29 15:33:05,3
AMD,nguszti,On 1440p even!,pcmasterrace,2025-09-29 16:49:19,1
AMD,nguug7o,You posted a bug without even trying to restart first?,pcmasterrace,2025-09-29 16:56:10,-6
AMD,ngv1osw,8k,pcmasterrace,2025-09-29 17:30:26,1
AMD,nguxdxj,"I posted a screenshot of something I found to be funny, I was not worried about it.",pcmasterrace,2025-09-29 17:10:10,9
AMD,ngv1do7,"This is reddit, OF COURSE he did lol. People on here do fuck all of their own troubleshooting.  Most would really just like you to fly to their location and fix it while you nibble on their toes and give them a handjob at the same time.",pcmasterrace,2025-09-29 17:28:58,-9
AMD,ngvruz3,The fact that you replied under ops reply that literally days he posted jt cause it was funny not cause he needed help is wild work,pcmasterrace,2025-09-29 19:35:15,1
AMD,nh0xqqz,There are several other pre-builts for the same price that feature a better CPU and a 16GB version of the 9060XT.         Absolutely not worth it.,pcmasterrace,2025-09-30 15:50:18,9
AMD,nh1cods,[https://www.walmart.com/ip/seort/17028970936](https://www.walmart.com/ip/seort/17028970936) $1100. add a 2nd stick of ram when u get it  they also have a 7800x3d and 5070 system $1200 when retocks. 3rd party scammer is reselling $1650 right now.,pcmasterrace,2025-09-30 17:03:01,2
AMD,nh0x6bf,"The 8400f is junk, the 9060xt 8gb is junk, so no.   I mean I sold a $500 PC with a Ryzen 3600 and rtx 2070 super this week. It cost me under $400 to build. Worse, absolutely. Meaningfully worse? Not really.   The 8400f is L3 cache limited, which is an issue. It's also pci-e lane limited, running the GPU at pci-e gen 4x4 which also limits a 9060xt a tick. The 8gb of vram on the GPU causes you to not really be able to crank the settings up cause you'll run out of vram.   The $1000 or less new PC budget, you're generally going to be looking for a used PC in the $500-600 range instead. The New options are so crippled at that price point you're better off with something a bit weaker for less money, saving you $400 for a future upgrade to the next PC.   Now, $1000 new-old stock is where the fire sale PCs can fall, but those will be available in limited quantities and sell fast. Like last year lots of 12th/13th gen i7 and rtx 4070/7800xt PCs hit that $1000 price point and were significantly better than what you're looking at right now.",pcmasterrace,2025-09-30 15:47:36,1
AMD,nh108fv,"No. Absolutely not, the CPU is bad and the GPU only has 8GB of VRAM.",pcmasterrace,2025-09-30 16:02:23,1
AMD,nh158yz,"Price too high for this build, 9060XT are 8GB version, and 8400f aren't that great, you can find better for that price.",pcmasterrace,2025-09-30 16:27:08,1
AMD,nh2now4,Try to get at least a GPU with 12-16GB of Vram   9060xt 16GB PC with a 7500F you can build yourself for around 1K (adequate for 1440p gaming). A Pre built PC with the same specs shouldn't cost more than 1.1k imo. But maybe you want to build one for your girlfriend. Give it a bit of personality :),pcmasterrace,2025-09-30 20:48:12,1
AMD,nh0y2hm,"""The $1000 or less new PC budget""  ***proceeds to say ""***you're generally going to be looking for a used PC in the $500-600 range instead***""***  You're not wrong, however if he has the money and wants to go full spending, he can't aim for a 500$ with a 4 generations-old GPU and a decade old CPU.",pcmasterrace,2025-09-30 15:51:52,3
AMD,nh0zmab,"It's a weird deal where I just can't recommend going there because the stuff is so crippled you really box yourself in.   The thing is, the new-old stock when they do fire sales at $1000 blows the shit outta something like a 8400f/Rx 9060xt 8gb.   Used, it's worse. But not really, because long term the limitations with a used PC and the new 8400f/9060xt 8gb are the shit CPU and 8gb of vram. Same limitations, once the Ryzen 3600 is a problem, so is a 8400f like a few months later. Once the 2070 super's 8gb of vram is the issue, so is the 9060xt with its 8gb of vram.  It's a weird deal where these PCs with a 8400f/9060xt just are so crippled in certain ways you get screwed down the road hard with them.",pcmasterrace,2025-09-30 15:59:20,1
AMD,nh28p1r,Make sure your BIOS is updated because you are supposed to be able to adjust that in the BIOS. It might be named something different. Check your manual,pcmasterrace,2025-09-30 19:36:21,1
AMD,nh2r4tv,"Should be option in the bios.  Or just get a gpu card (anything pretty much) and not use the integrated gpu ro free up the ram.  If that's outside your budget,  adding another 8gb ram will be a relatively cheap and better upgrade.",pcmasterrace,2025-09-30 21:04:28,1
AMD,nfanop8,This is like when people put the wrong badges on their car lol,pcmasterrace,2025-09-20 19:14:54,89
AMD,nfas57q,Holy shit its the ryzen 4070,pcmasterrace,2025-09-20 19:38:36,23
AMD,nfanc6z,2000w version?,pcmasterrace,2025-09-20 19:13:05,12
AMD,nfaovr0,There's literally an article about this  Edit: here's the link!  https://www.tomshardware.com/pc-components/gpus/rare-franken-gpu-has-both-amd-and-nvidia-branding-amd-radeon-geforce-rtx-9070-xt-is-the-new-ultimate-midrange-gaming-champ,pcmasterrace,2025-09-20 19:21:10,3
AMD,nfas4om,Lisa Su and Jensen Huang are cousins...,pcmasterrace,2025-09-20 19:38:31,3
AMD,nfawi84,The holy Ryzen RTX AI 5090 X Ti,pcmasterrace,2025-09-20 20:01:42,3
AMD,nfb9lqs,You and the Radeon guy should swap,pcmasterrace,2025-09-20 21:10:22,3
AMD,nfbuv04,Someone who works in the factory that makes them is laughing their asses off right now.,pcmasterrace,2025-09-20 23:12:46,3
AMD,nfam3mx,Is Radeon or GeForce?,pcmasterrace,2025-09-20 19:06:36,5
AMD,nfilt5i,AMDidia RXTX 9070 XTi,pcmasterrace,2025-09-22 00:18:52,2
AMD,nfanvgz,The GPU was manufactured with the wrong silicone and it helps it's dysphoria.,pcmasterrace,2025-09-20 19:15:53,4
AMD,nfao2wh,Is this a Ryzen 3070?,pcmasterrace,2025-09-20 19:16:58,1
AMD,nfqalbs,lol Rtx 9070 Xt Tie yup include Ray Tracing Frame gen dlsfsr,pcmasterrace,2025-09-23 07:25:37,1
AMD,nfap1wj,By this point we are going to get these posts once in two months,pcmasterrace,2025-09-20 19:22:05,1
AMD,nfc8kio,Coé 👋,pcmasterrace,2025-09-21 00:35:31,1
AMD,nfc5lg5,another one??,pcmasterrace,2025-09-21 00:16:52,0
AMD,nff08fu,But what is this card actually?,pcmasterrace,2025-09-21 13:39:13,0
AMD,nfapopp,stop reposting,pcmasterrace,2025-09-20 19:25:27,-8
AMD,nfavwda,I am getting flashbacks...  https://preview.redd.it/8d98c7yykdqf1.jpeg?width=1200&format=pjpg&auto=webp&s=b0a70a46c3a3308965b152a97595e717ddc53b0e,pcmasterrace,2025-09-20 19:58:33,48
AMD,nfaonqe,"Reminds me of Audi or Seat cars using the same parts as Volkswagen. So if you go to an Audi dealer the market up price for parts is insane, compared to buying the same parts for VW or Seat cars.  Same for Lamborghini, some of their cars have same parts as Audi too.",pcmasterrace,2025-09-20 19:20:00,12
AMD,nffhrff,You mean the Radeon 4070,pcmasterrace,2025-09-21 15:10:12,1
AMD,nfaw4b8,close to 3000w,pcmasterrace,2025-09-20 19:59:42,6
AMD,nfohgv5,Link the article  ![gif](giphy|M5zhoj9rhwkhy),pcmasterrace,2025-09-22 22:55:43,1
AMD,nfbv18o,"A ""unharmful mistake"" lmao",pcmasterrace,2025-09-20 23:13:47,1
AMD,nfamnkg,"No its definitly radeon, theres no 16pinout",pcmasterrace,2025-09-20 19:09:31,9
AMD,nfamqma,both 😎 /s,pcmasterrace,2025-09-20 19:09:58,2
AMD,nfc8s2r,It's my 9070 xt that came from the box with a GeForce side panel.,pcmasterrace,2025-09-21 00:36:49,1
AMD,nfawe3y,"pardon me, I am not that active in this community 🤷",pcmasterrace,2025-09-20 20:01:07,1
AMD,nfcogrs,"eae meu fi, roubei kkkk claro q ia dar créditos né po, tmj",pcmasterrace,2025-09-21 02:15:35,0
AMD,nfia9ob,"To be honest, I don't really know, I think it's an error of the factory lol",pcmasterrace,2025-09-21 23:10:17,1
AMD,nfaw2kx,wdym? never saw anyone posting this atrocious thing here lol,pcmasterrace,2025-09-20 19:59:27,3
AMD,nfaqiy7,Lots of luxury brands are like this. Buick and Cadillac use Chevy parts. Lincolns are mechanically Fords. A Lexus ES350 is a Toyota Avalon. An Acura Integra is a Honda Civic. Infiniti is just Nissan. Land Rover and Jaguar use loads of Ford parts.,pcmasterrace,2025-09-20 19:29:55,12
AMD,nfb69bz,Yeah they don't just share parts but whole platforms.  Under the exterior and interiors they are all the same.  I have a golf with a few 'upgrades' from audi.  It's all the same.,pcmasterrace,2025-09-20 20:52:34,2
AMD,nfbrkzs,probably,pcmasterrace,2025-09-20 22:53:46,2
AMD,nfohqc4,Done! Lol...I dont normally post links cuz some subreddits dont allow it 😅,pcmasterrace,2025-09-22 22:57:25,1
AMD,nfaooc6,That is no longer accurate. Certain amd models now also use that 12vhpwr connector,pcmasterrace,2025-09-20 19:20:05,6
AMD,nfcqgav,I have this model Asus 9070 xt mine says Radeon on the side,pcmasterrace,2025-09-21 02:28:20,1
AMD,nfb5fer,u legit said credits.....,pcmasterrace,2025-09-20 20:48:12,-7
AMD,nfd3mjy,Lamborghinis also sometimes use parts from ford,pcmasterrace,2025-09-21 04:01:24,1
AMD,nfapyk8,Only the sapphire nitro iirc,pcmasterrace,2025-09-20 19:26:56,2
AMD,nfb6tzt,"i said *here*, if you go into his profile he never posted that here lmao  and he claims to be the owner too. not my fault bruh",pcmasterrace,2025-09-20 20:55:33,5
AMD,nfas3dk,ASRock Taichi as well,pcmasterrace,2025-09-20 19:38:20,2
AMD,nfb9b1s,"Thanks for the ""do not buy"" list.",pcmasterrace,2025-09-20 21:08:45,6
AMD,nfqzcxa,"A lot of MMOs being cpu bound, and the 5070ti being only 5-8% less performance than the 5080…   Go for the 9800X3D and 5070ti combo.",pcmasterrace,2025-09-23 11:31:48,29
AMD,nfqw8ch,9800X3D obviously,pcmasterrace,2025-09-23 11:08:33,39
AMD,nfqx3vt,"The 5080 is the stupidest GPU choice you can get right now. There is about 5-10% performance increase from the 5070Ti and then there's an absolute grand canyon of a difference from 5080 to 5090.   The 9800X3D is basically the best gaming cpu you can get right now and the 5070Ti is the best gaming option if you're not a gajillionaire, so the combo is wonderful.",pcmasterrace,2025-09-23 11:15:19,31
AMD,nfr1b6l,X3D with 5070 Ti any day of the week for CPU intensive games.,pcmasterrace,2025-09-23 11:45:31,6
AMD,nfr0kvc,7800x3d and 5070 ti. Use 200 euros difference for alcohol.,pcmasterrace,2025-09-23 11:40:28,8
AMD,nfr0t06,I dont think any MMO has the fidelity to profit from a 5080. However CPU loads can get pretty high so you benefit from the X3D more. That said even with the lesser of both parts i dont think you would have issues in literally any MMO and would still be able to max out 4k 120fps.,pcmasterrace,2025-09-23 11:42:02,3
AMD,nfr4e5o,"5080 is often >30% more expensive for being just a little faster than 5070ti/9070xt.  Edit. Also online games tend to be light for GPU and sometimes heavy for CPU, so an X3D CPU should be a way better chocie.",pcmasterrace,2025-09-23 12:05:45,4
AMD,nfqy0ak,9800X3D and 5070Ti.  5080 isnt worth it. Its 10% faster but costs 300-500 more.,pcmasterrace,2025-09-23 11:22:02,3
AMD,nfqx402,What MMORPGs?,pcmasterrace,2025-09-23 11:15:20,1
AMD,nfr1qbo,Yes.,pcmasterrace,2025-09-23 11:48:24,1
AMD,nfr8191,"For MMOs? 9800X3D, especially WoW. You gain a huge fps boost from the X3D.",pcmasterrace,2025-09-23 12:28:41,1
AMD,nfr9jkn,X3d us a game changer for mmo. I highly recommend it.  Mmo are cpu heavy games.,pcmasterrace,2025-09-23 12:37:55,1
AMD,nfr9klq,"my previus rig was cpu 5600x + gpu rx6800 I upgraded the whole rig except for gpu, 9800x3d + ddr5 rams + x670e mobo. Uplift was unbelievable, 1% skyrocketed. No more stutter. Than bought a 5080, I was impatient and bought it day 1. If I had patients I would definitely got a 5070ti. when it comes to oc 5080 is very good I managed to score 10.000 pts on steel nomad as a daily use stable oc.",pcmasterrace,2025-09-23 12:38:04,1
AMD,nfralyn,"Have you considered the Ryzen 7 7800X3D? It isn’t that far off behind the 9800X3D, yet it’s so much cheaper and only a bit more expensive than the 9700X. I’d get that with the 5080",pcmasterrace,2025-09-23 12:44:13,1
AMD,nfrb8fz,9800X3D + 5070Ti should do,pcmasterrace,2025-09-23 12:47:49,1
AMD,nfrbss3,MMOs will benefit more from the 9800x3d over the 9700x than a 5080 over a 5080 Ti.,pcmasterrace,2025-09-23 12:51:03,1
AMD,nfrgbgk,9800 with 5070 @ 2k,pcmasterrace,2025-09-23 13:16:44,1
AMD,nfrqfj9,Definitely go for a 7800x3d instead. Not a single MMO out there would be able to crank that CPU to its maximum. A 9800x3D is just plain overkill and a waste of money for your usecase imo. Rather spend it on a RTX 5080 (although that is also overkill for MMO's).,pcmasterrace,2025-09-23 14:10:45,1
AMD,nfs1jn1,The 9800x3D any day of the week  That thing'll last you generations,pcmasterrace,2025-09-23 15:04:57,1
AMD,nfvgp7j,I actually have the 5070ti and 9800X3D. Currently playing throne and liberty Maxed out graphics on DLSS quality and I’m capping my 160hz monitor. Everything feels so smooth I highly recommend this combo. Btw I’m doing 4K graphics,pcmasterrace,2025-09-24 01:29:54,1
AMD,nfr1u55,"X3D.  You're not really upgrading from a 5070 TI and a 5080.  It's like sub 10% across all applications.  Even a 7800X3D would be more of an upgrade from a 9700X, let alone a 9800X3D, in gaming applications of course.",pcmasterrace,2025-09-23 11:49:07,1
AMD,nfro5op,9800x3d 9070xt,pcmasterrace,2025-09-23 13:59:18,1
AMD,nfqz1p9,"I don't think there are any mmos like that, the only thing that comes to mind is T&L, it's UE5, and, there is no pc around that will help you, cos it's not a pc issue, it's an engine issue",pcmasterrace,2025-09-23 11:29:36,0
AMD,nfqxqdu,"This. Also I'd check the games you play, might be able to save a couple bux and get a 9070XT, same performance on average, but check the games you play.",pcmasterrace,2025-09-23 11:20:00,6
AMD,nfr7gg8,"Personally, I had to settle for the 7800x3d as the 9800 wasn't available where I was.  Also decided to go on the ""cooler"" side with a windforce GPU rather than one that runs hotter. My MSI one degraded within a couple years due to lack of cooling. Which is odd given the case had ten fans.",pcmasterrace,2025-09-23 12:25:05,0
AMD,nfr49d8,...or don't buy a PC and just go party,pcmasterrace,2025-09-23 12:04:55,-3
AMD,nfrbgek,"Just realize you say 2k ultra, 5080 makes no sense at all.",pcmasterrace,2025-09-23 12:49:05,1
AMD,nfr23s4,Isn't 9070xt like way above MSRP? It is very close to 5070ti and sometimes it is more expensive   While 5070ti is at msrp currently,pcmasterrace,2025-09-23 11:50:57,3
AMD,nfqyadt,"Yup, this, or you can wait until christmas cause the 50-Supers should come out and they are supposed to be priced pretty well and have 24gigs of memory.",pcmasterrace,2025-09-23 11:24:05,0
AMD,nfr8gap,"That sounds weird. Did you have the fans on the gpu even running? Every gpu I've had the stock air coolers are plenty, even on my 5090 I rarely go over 65°",pcmasterrace,2025-09-23 12:31:16,1
AMD,nfszkbd,"7800X3D is only 8% slower than the 9800X3D on average. There are a few outlier games that see ~20%. You didn't settle by much, you got a better deal.",pcmasterrace,2025-09-23 17:45:06,1
AMD,nfsr8lb,I've seen 9070XTs for under 675. MSRP on a 5070ti is 750.,pcmasterrace,2025-09-23 17:06:36,1
AMD,nfr8tg9,It was a 3070 the MSI.  Highest the gigabyte windforce 5070 Ti has his was 66C in Helldivers with around 100% utilization.   Not bad given how that game isn't optimized at all.  Usually around 62C in UE5 games when maxed out.,pcmasterrace,2025-09-23 12:33:31,1
AMD,nft4t8d,"I mean it did come with a GPU with a better clock speed, but again stock issues.  Sadly it's sometimes cheaper to buy pre-made in Canada than to make our own. In this case $600 cheaper.",pcmasterrace,2025-09-23 18:09:50,1
AMD,nfsstne,You sure it is not the 9070 and tbh 75$ cheaper ain't that good of a deal and anyone would pay the extra 75 for 5070ti cause of dlss 4 which is still better than fsr 4 and ofc you get cuda and a better path tracing performance if that matters,pcmasterrace,2025-09-23 17:14:00,-1
AMD,nfr92lp,Thats wild since its a lower powered card. Any 3 fan card should do fine with airflow. Did you ever repaste it? Should repaste every 3~ years ime,pcmasterrace,2025-09-23 12:35:04,1
AMD,nfub0gp,"I checked before I posted.  Definitely was an XT.  75 is borderline not worth the cost IMO. FSR4 is close enough to DLSS at the resolutions your likely targeting with that level.  Can you pause and pixel peep show  Everyone cites cuda but no body can tell me what they actually use it for.  I do all sorts of work - content and video editing, AI, coding, etc. I've never had issues running professional software on AMD systems (or Nvidia).   My thought process is this: If you're shopping under top end, you're likely working with a budget. The 9070XT is very competitive with the 5070Ti. So if you're on a budget, check the games you play - see how they perform on each. Path tracing is kinda silly since it's only in a couple games but it may be important to some.  It's likely not worth it at this performance tier anyway (my 4090 even wasn't great at it and these are slower). 75 dollars could be the difference between an x3d chip and a regular chip.",pcmasterrace,2025-09-23 21:32:21,2
AMD,nfr9hwi,"No never reached the three year marker on it.  I also don't overclock so I never saw a need to repaste.   More than likely the PSU probably caused some damage. Was forced to use a DeepCool as Corsair and ThermalTake were hard to find, and very expensive during the pandemic in Canada.",pcmasterrace,2025-09-23 12:37:38,1
AMD,nfuhqyt,Cuda does everything and software does but better the biggest gap is in 3d rendering where amd simply sucks   Anyway dlss 4 has way more support than fsr 4 no modding needed and it looks better like dlss 4 on performance looks as good as fsr 4 on balanced they are close it can be overseen but still and you get a superior framegen I have tested fsr 4 on my brothers 9060xt 16gb and r5 5600 it looks as good as dlss 3 but still it is not quite dlss 4 accuracy it just isn't if we include dlss fg and mfg the difference is worth 30$ if you ask me and the other 45$? That is for cuda alone and ofc the path tracing difference is still big   And 675 isn't competitive with 5070ti and availability at this price is absurdly low 95% of the time you will see a 700-750$ 9070xt still not worth it 75$ is something when you are comparing 5060ti and 9060xt but 5070ti and 9070xt that is 10% Saving and for what? for the sake of saving  Fanboying has never been so stupid 9070xt is better at 600$ at 675-750 that is 5070ti territory it is like when amd tried to sell 7900xt at 700$ and expected people to pay just cause it is cheaper   Rdna 4 is amazing I tried the 9060xt this thing is 1 step from becoming as good as my 3080 but still the path tracing performance needs more work the lack of cuda is an L for many yeah it is a lot better than the rx5500xt my brother had before but still need quite more work you save money when the products are identical or the cost difference is higher than 10%   Have a great day,pcmasterrace,2025-09-23 22:07:58,0
AMD,nfr9qza,Yea thats either a manufacturer defect or your psu fried it. Never heard of a gpu dieing from heat unless they somehow didnt have any fans running at all haha,pcmasterrace,2025-09-23 12:39:08,1
AMD,nfra1y4,"Usually it only ran, at the highest, 80C.  Not as cool as the one I got now, but I also had issues with MSI that have kind of pushed me away.  They weren't playing nice on warranty, the bios issue was hell, and the exhaust fan broke twice in three months.  Suspect they've been cheaping out since the pandemic.",pcmasterrace,2025-09-23 12:40:56,1
AMD,nfrafge,Definitely a manufacturer defect. I only use aorus for my gpu's and I've had amazing luck thankfully. I do use msi motherboards though as I like their bios and vrm specs. 30 series was definitely warmer but 80° on a 3070 seems high to me. Good thing you have a good gpu now though,pcmasterrace,2025-09-23 12:43:10,1
AMD,nfraw7v,"Thankfully.   Only thing I need to watch on this card is the thermal gel thing.  I doubt it'll happen though as the card, when I checked the serial number, was made in May after they acknowledged the issue. It's also not vertical mounted. Either way I'll see if it starts heating up, and I got a two year warranty from the retailer in addition to Gigabyte's.",pcmasterrace,2025-09-23 12:45:52,1
AMD,ngi3l1p,Ssd? What game? Disable vsync use freesync and cap fps to where it's enou with radeon chill min max,pcmasterrace,2025-09-27 16:24:22,1
AMD,ngi9ld3,"Ssd i actually dont really know, its 1tb and i have 150gb free atleast but i unfortunately dont know the rest, i could check when i get access to my pc if that would help? Games that i see major stuttering is watch dogs 2 and gta 5(both dx11) and a minor stuttering in fortnite, skate., cod, pretty much every game.",pcmasterrace,2025-09-27 16:55:09,1
AMD,ngfc1yj,"it can do rdr2 at about 30fps if you set the game to 900p at the lowest settings. Some other games it'll do better at. Overall you'd have a pretty similar experience to someone using a steam deck in most titles, though always a little on the better side.",pcmasterrace,2025-09-27 03:58:40,1
AMD,ngffu1v,This is the only game i have RN and I am running 1080p fsr2 performance max sharpness ultra textures everything else set to medium except shadows settings I put them to low and I have minimum of 26 fps and max 34fps avg 31 fps,pcmasterrace,2025-09-27 04:28:10,1
AMD,ngfjqmx,"That's pretty good, pretty much the worst case for PS4 era games tbh",pcmasterrace,2025-09-27 04:59:45,1
AMD,ngpkior,"The 7900 xt would probably not be my first choice unless I found a really good deal on one. For $600 brand new you can get an Rx 9070 which is just as fast as native, has faster raytracing, and FSR 4 support as more games implement it. You shouldn't really be crashing though. Have you updated your motherboard's bios?",pcmasterrace,2025-09-28 20:13:36,2
AMD,ngpnv5r,"game crashes often arent graphic card related unless you are using older drivers or card is overheating. mostly its windows releated, ram memory isnt stable or some junk is working in the background.   maybe get new RX 9070 instead.",pcmasterrace,2025-09-28 20:29:42,1
AMD,ngpmb5u,noo! I have never updated my bios :( going to give it a try! thanks for the info,pcmasterrace,2025-09-28 20:22:12,1
AMD,ngpwwix,just updated my bios it was really old i hope it helps a bit :(,pcmasterrace,2025-09-28 21:14:17,1
AMD,ngjcydw,"Seems too good to be true, so yeah. That's a good pic.  Make sure it has warranty or a return policy to make sure it's real",pcmasterrace,2025-09-27 20:20:15,1
AMD,ngjetlr,I’d be careful about getting an asrock board with a 9000 series ryzen. They say they’ve fixed the issues that were killing the CPU’s but I’ve heard that some people are still having problems.,pcmasterrace,2025-09-27 20:29:58,1
AMD,ngjizx5,"It's a good PC, but there's also this Walmart prebuilt deal rn. Better CPU & GPU at lower cost  [https://www.walmart.com/ip/Acer-Nitro-60-Gaming-Desktop-NVIDIA-GeForce-RTX-5070-AMD-Ryzen-7-7700-32GB-RAM-2TB-SSD-N60-181-UR24-Black/16993820531?classType=REGULAR&athbdg=L1103&from=/search](https://www.walmart.com/ip/Acer-Nitro-60-Gaming-Desktop-NVIDIA-GeForce-RTX-5070-AMD-Ryzen-7-7700-32GB-RAM-2TB-SSD-N60-181-UR24-Black/16993820531?classType=REGULAR&athbdg=L1103&from=/search)",pcmasterrace,2025-09-27 20:51:48,1
AMD,ngjje3d,"Nvm don't recommend that one, there's only 1 stick of RAM. Although Walmart has similar build, but with 7800x3d 2x16GB RAM sticks for $1200",pcmasterrace,2025-09-27 20:53:51,1
AMD,ngjsqum,It prob has 2x sticks. That's prob just the product picture.,pcmasterrace,2025-09-27 21:45:01,1
AMD,ngqp0bo,pitcairn is the unsung greatest of all time tbh,AMD,2025-09-28 23:48:09,167
AMD,ngpsw3c,this is why linux rocks the llamas ass,AMD,2025-09-28 20:54:10,367
AMD,ngqi9t9,"8th gen console GPUs, 8 GB shared RAM for 2013 PS4/Xbox one. While average midrange PC gpu in 2013-2015 were running in 2 GB VRAM  Best midrange value for that console generation: 2017 RX 500 series. With 4 GB minimum VRAM. 8 GB VRAM for 10 year usage.",AMD,2025-09-28 23:08:45,107
AMD,ngqntbh,"They were very good gpus, unfortunately lack of funds caused limited development for quite a bit of time on the Radeon side, while amd tried to survive. My own 7970 was only retired from frontline use the year the new 7000 series came out, but was replaced with a 6800. I had bought a few other used gpus the last few years before for trouble shooting … issues ended up being the cpu",AMD,2025-09-28 23:41:08,27
AMD,ngsqvxb,"Very fond of those cards, HD7770GHz was my first GPU, incredible the gaming experience £100 got me back then.",AMD,2025-09-29 09:06:34,10
AMD,ngr71j1,based i wish i kept my 390x that thing was a beast,AMD,2025-09-29 01:36:36,11
AMD,ngppotz,"Saying ""HD 7000"" makes it sound old, but it's AMD Radeon R9 200 series",AMD,2025-09-28 20:38:39,38
AMD,ngsw8zu,o7,AMD,2025-09-29 10:01:13,2
AMD,ngsxapt,"I was running FSR 3 frame generation in Cyberpunk on a FirePro W700 (equivalent to an HD 7850) and it was working really well actually. I also got XeSS running, but that tanked performance understandably, because these GPUs don't natively support SM 6.4 but it seems that they've received driver updates to support SM 6.5 or something. GCN 1.0 only natively supports SM 5.7 iirc. I've been doing some tests on that HD 7850 FirePro GPU because it has 4GB of GDDR5 which is a minimum nowadays for most games. But I was even playing Doom Eternal at native 1080p at over 60fps at low settings. I've got a video about it in my channel if anyone's interested in [that.](https://youtu.be/5rcLg7Xo2rs)",AMD,2025-09-29 10:11:15,2
AMD,ngujy45,This is THE FineWine(tm). :),AMD,2025-09-29 16:05:30,2
AMD,nguu8c8,"Ah yes , like back when the 7990 was the tHe mOsT PoWeRfUl gPu iN ThE WoRlD!!!!1 and the shitty driver forced me to use Compiz over KDE!  I member  Guess is good thing for whoever still in series HD 7000 but that’s an era I would like to forget.  (I wasted my hard earned money on that shit of a card)",AMD,2025-09-29 16:55:09,1
AMD,ngvymes,"I see it's gonna be added to the kernel, but do you need to do anything to take advantage of it? I tried installing Nobara on a 7970, and yeah it was...odd.",AMD,2025-09-29 20:08:09,1
AMD,ngyrhtx,Best miner ever,AMD,2025-09-30 06:37:16,1
AMD,nh1evj4,my r7870 overclocked to 1475MHz gpu 1510MHz mem.,AMD,2025-09-30 17:13:24,1
AMD,nh1ey45,The important thing is that we somehow get a fourth 'Radeon 7000' series in 7 years.,AMD,2025-09-30 17:13:44,1
AMD,ngqqrbv,All 5 people are pretty excited,AMD,2025-09-28 23:58:49,-8
AMD,ngrscak,See this 1 week after my 8990 decided to die. One of the worse cards I've owned. Wonder if these updates would of made it decent,AMD,2025-09-29 03:49:48,0
AMD,ngrrx66,precious developers time wasted on obsolete hardware...,AMD,2025-09-29 03:46:52,-10
AMD,ngshhog,They rebranded that shit soooo many times. That's how good it was.,AMD,2025-09-29 07:28:14,56
AMD,ngwncgs,The 7970 GHz edition was a beast for GPU mining back in the day.,AMD,2025-09-29 22:11:03,5
AMD,ngpv4gu,"Sir, I think you're mixing that saying with Winamp.",AMD,2025-09-28 21:05:15,115
AMD,ngruigl,"Yup, as soon as anti-cheat widely adopts linux to the point where all my fav multiplayer games run on it, I'm fully switching to linux and never looking back cause an OS that just does what I tell it to (and also has low-latency audio, unlike windows audio, turns out our audio reaction time is 3 times faster than even the top esports players visual reaction time, making audio latency actually kinda important) without any weird extra steps sounds like heaven.  Like seriously, only installing new things myself that I actually want and use? That's something I look forward to, might actually get to try out some AI softwares when a new one isn't popping up every other day like some kind of FNAF jump-scare.  Fingers crossed the steam deck and steamOS gets widespread linux compatibility over the final hurdle, game compatibility is literally the only reason I'm not running it (though R6 is kinda ass now so I might end up switching anyways).   Of course banning kernel-level anti-cheat and any other kernel-level software that has any ulterior function other than anti-virus, is the ultimate goal since anything with access to your kernels is fully capable of using your computer to commit crime. However the general public doesn't understand that kernel-access = prime crime time, so I'm not holding my breath for that.",AMD,2025-09-29 04:05:00,-2
AMD,ngqo6cc,Such a disappointment as a generational hardware… compared to how powerful the 360 was to pc gpus at launch. AMD cut a sweetheart deal to stay alive and we had an underwhelming generation that impacted game development till about 2 years ago,AMD,2025-09-28 23:43:16,26
AMD,ngwpwaa,"I also had the 7770, about US$70 from TigerDirect(rip)... Was a different time where I didn't care about chasing graphics. It just worked solidly. Nowadays I'm complaining to myself about my 5070ti not running E33 smoothly at Epic settings in 4k lol",AMD,2025-09-29 22:25:11,1
AMD,nguts9p,I still have my 7970ghz! Maybe I'll actually get around to building a Linux box with the littany of parts I've collected from upgrading various pcs over the last 13 years haha.,AMD,2025-09-29 16:53:03,1
AMD,ngprpf5,"No. These are older than that. It literally says the 7000 series and that’s what they were called - 7970, etc.   The R9 290X was released in 2013.",AMD,2025-09-28 20:48:24,155
AMD,ngpszvn,"Wrong. GCN 1.0 was 7000 series (with exception of 7790 which was GCN 1.1). R9 200 series was a mix of 1.0, 1.1, and 1.2 (Hawai’i and Bonaire were 1.1, Tahiti, Pitcairn, and Oland 1.0 and Tonga 1.2).",AMD,2025-09-28 20:54:40,45
AMD,ngpriot,"The bug-fix addresses Tahiti and Pitcairn GPUs, which as far as I remember, are GCN 1.0.",AMD,2025-09-28 20:47:32,21
AMD,ngvmhh8,o7  indeed,AMD,2025-09-29 19:09:04,1
AMD,ngygtmd,"Actually now that you mention it, yes. You need to enable the ""new experimental"" version of the installed driver with kernel boot options (included in the main amdgpu driver as part of the kernel).[AMDGPU - ArchWiki](https://wiki.archlinux.org/title/AMDGPU)  radeon.si_support=0   amdgpu.si_support=1 radeon.cik_support=0 amdgpu.cik_support=1",AMD,2025-09-30 05:01:02,3
AMD,ngrz17m,"Yeah, I wonder how many people are still using a 2013-vintage GPU these days?",AMD,2025-09-29 04:37:58,1
AMD,ngs4yag,They did all this work for 5 people. Moron,AMD,2025-09-29 05:28:11,-1
AMD,ngu03er,well that and AMD was struggling thats why the CPU team had to go to Lisa Sue to be like listen we gotta stop focusing on the next reiteration so we can do a whole new design. then ryzen was born. that was actually a make or break moment for the company Lisa whent for it. cool videos with the engineers at amd offices on gamers nexus.,AMD,2025-09-29 14:27:26,19
AMD,ngqm1k8,...which doesn't exist for Linux btw.,AMD,2025-09-28 23:30:31,45
AMD,ngrqe6l,WinAmp best.  Windows is in the name.,AMD,2025-09-29 03:36:07,3
AMD,ngt3qy3,"Kernel-Level-equivalent Anticheat will never come to Linux. It might come to some ""Android""-esque Linux kernel based desktop operating system over which you have no control over though.  I wouldn't call that ""Linux"" though.  Sorry, if you want to give games companies full control over your computer in order for them to let you play their games then you'll just have to stick to closed platforms.",AMD,2025-09-29 11:08:01,4
AMD,ngrxxtd,Been using Bazzite for a few months. Not going back...,AMD,2025-09-29 04:29:14,4
AMD,ngs3yl8,Easy to leave now then .. Not ALL your games will work.. All mine do however :),AMD,2025-09-29 05:19:14,5
AMD,ngtzoof,"anti cheat companies are paid under the table to not support linux, like how intel paid dedl land hp to not use AMD chips for their flag ship pcs for all those years.",AMD,2025-09-29 14:25:20,1
AMD,ngsx177,the moment linux starts to become more popular it will attract hackers too..,AMD,2025-09-29 10:08:43,-2
AMD,ngqrf2p,Xbox 360 has 512 MB total RAM.  Midrange/entry PCs were bleeding in 7th gen consoles era (2006 PS3) with Sub-512 MB graphic card and Windows Vista demanding memories (above 1 GB RAM requirement).  until 2009 Windows 7 hardwares came affordable with 1 GB VRAM in midrange graphic card pricing and 4 GB DDR3 RAM. Able to get 60 fps HD resolution.,AMD,2025-09-29 00:02:58,26
AMD,ngvvseq,"PC hardware has only been marginally better than consoles from the PS4 onward, come on now. We're seeing diminishing returns on this tech altogether, not ""stagnation"". There isn't much room for a new Crysis sort of project anymore.",AMD,2025-09-29 19:54:19,1
AMD,ngqbio2,The 270X is a rebrand of the HD 7870 and the 280X is a rebrand of the HD 7970,AMD,2025-09-28 22:30:42,40
AMD,ngqh3pe,Yep and those were rebands of the 7000 series.,AMD,2025-09-28 23:02:08,14
AMD,ngq0ttc,"What you're mentioning makes it sound like it was a whole sleuth of cards in the 200 series that weren't rebrands, when realistically it was two or three cards that were actually new.",AMD,2025-09-28 21:33:51,11
AMD,ngq9u8x,"We can call them GCN 1-2-3 now, it's fine. Everyone calls Polaris and Vega GCN4 and 5 after all.",AMD,2025-09-28 22:21:26,3
AMD,ngq0ucl,"R9 280x says hi.  Edit: I didn’t read the post properly, and that it was mentioning GCN 1.0.",AMD,2025-09-28 21:33:55,-3
AMD,ngq4ehk,"https://www.techpowerup.com/gpu-specs/amd-tahiti.g120  Both, as 280 is a rebranded Tahiti.  7970 was a hell of a card though. HD 7850 is doubtlessly the best card I've ever owned. Absolutely wonderful experience coming from a GTX 460 and many Nvidia cards before.  Edit: To add I think it's the last truely great generation from ""ATI""/AMD. 4000 series was pretty banger too, HD 4770 was major value.",AMD,2025-09-28 21:52:05,9
AMD,ngydz1n,Dozens. GPUs are expensive.,AMD,2025-09-30 04:37:34,3
AMD,nh2eexq,I got it on my second machine at home. I actually finished a playthrough of Cyberpunk2077 with it and my daughter uses it when it comes to my home mostly to play GtaV.  The 7970 has served me well and still does.,AMD,2025-09-30 20:04:08,2
AMD,ngqrqok,We have our own tho. Look up XMMS.  100% compatible with Winamp skins iirc.,AMD,2025-09-29 00:04:58,18
AMD,ngr4h68,"True, but Winamp works just fine through Wine or Bottles.",AMD,2025-09-29 01:21:22,8
AMD,ngvumus,"It very nearly got a Linux port way back in the original v3/v5 days. Sadly it was canceled. But then XMMS, and later Audacious, were created, possibly as a response.  The developers of the current Winamp successor, WACUP, also stress their Wine support. WACUP would *like* to have a native port to Linux, but they don't really have anyone willing to do it right now. Most of WACUP development is still just one guy far as I know.",AMD,2025-09-29 19:48:45,3
AMD,ngvvbg2,"Not a great reason. Putting ""Win"" in the name was to differentiate it from DOS-based players. That's how old Winamp is. Winamp even had a DOS version at first.  There was supposed to be a Linux port, though it was canceled. There were successful Mac ports since the beginning.",AMD,2025-09-29 19:52:02,2
AMD,ngs83j3,"Yeah, I guess, though there are still a ton of games I play that are windows exclusive unfortunately.  What's funny is I'm waiting for windows to ban kernel level access, because once that happens linux compatibility will probably become universal (or at least, the games that don't work on linux, also won't work on windows).",AMD,2025-09-29 05:57:00,5
AMD,ngqwu5i,"Memory was a weak point yes, but in terms of processing power the 360 gpu was arguably top of the market when it hit, being pushed down to second place a month or so later. PC hardware was moving very quickly then, but the concepts placed in its gpu carried over to next gen gpus in the pc quite well.   I would also say shared memory on the Xbox one was deceptive as more was reserved (10% originally) for the system lowering useable pool by the gpu and cpu, home towers by the point often had 8gb of memory with gpus between 2 and 4gb, though quickly moving to 6 to 8. The 2 and 3 gig gpus left at moderate settings quite quickly.   Further I would say the evidence of hindering next gen games can be seen in what we know were axed from titles supporting it, like infinite local co-op, and comments by developers like the Xbox one/ Xbox series gen. Vs how the games that made the jump from the 360 generation to the one generation faired.   The series S while a sales darling is another boat anchor around game development (see baldurs gate 3) for the Xbox brand, it’s truly unfortunate how often they knee cap themselves… I say this as an original Xbox owner and until about 2 years ago gold/ultimate player.",AMD,2025-09-29 00:36:32,10
AMD,ngu0cgu,"we all ran XP till 7 beta came out.  we also had more than 1.5gb of ram, at least the peeps i hung out with on IRC.",AMD,2025-09-29 14:28:42,3
AMD,ngw0fnm,"I would say on the hardware front we have seen great gains, it’s more the development front that has stagnated relying too much on middleware and not leveraging hardware properly… or where they do for features who’s returns are dubious…. It’s not the 00s where each generation was a wizbang upgrade, nor the early 10s where resources for resolution popped…. Nvidia is focused else where, and is still recovering from almost dieing as a company, and gpus paid a heavy price… and intel is dealing with a decade of poor choices…. Despite all of that hardware has advanced pretty well, while your hardware can probably last a decade at this point with settings degrading until playability is lost, it’s not a hardware progression issue. Check out what they were doing with the 360 at the end of its run… compared to what came out the end of the Xbox one run… not to much of a change outside of some major temples like rdr2, and that quality of a game is far between release as to what we should have…. Developers have been leaning on performance improvements to put less work into games",AMD,2025-09-29 20:17:03,2
AMD,ngqm95e,Doesn't change those were release in 2011,AMD,2025-09-28 23:31:45,29
AMD,ngqxmd6,The 290X is not a rebrand of anything.,AMD,2025-09-29 00:41:12,6
AMD,ngq3n2r,"Not really sure how that's relevant since my point was that it goes back even further than the RX series, but it would apply to all of them if they are using the same GPU.",AMD,2025-09-28 21:48:10,15
AMD,ngqyiiy,"The 290X was about 50% faster than the 7970 predecessor.  It provided the same performance as the new $1000 GTX Titan at a bit more than half the price.  Yeah, ""slightly improved"" sounds reasonable.",AMD,2025-09-29 00:46:33,8
AMD,ngq5suh,There also was the RX 285 and R7 250. Other than that they were all rebrands.,AMD,2025-09-28 21:59:36,-3
AMD,ngq3dr4,He literally said a mix of GCN 1.0-1.2 on the 200-series though. So his comment isn't wrong. But it kinda contradicts his statement.  The 2XX series had a lot of rebranded cards.,AMD,2025-09-28 21:46:52,9
AMD,ngqbnfj,Isn't that just an overclocked and rebranded HD 7970 GHz?,AMD,2025-09-28 22:31:26,3
AMD,ngrrbsb,My 270X was faster and had more memory than my HD7x50 class card.,AMD,2025-09-29 03:42:48,1
AMD,ngqp8mt,ran dual 7870s for 4-5 years. Crossfire was overhated when it worked it was amazing.,AMD,2025-09-28 23:49:32,2
AMD,ngtrq84,"I still love my 5700xt, it was kinda dud at launch (it still had driver issues after like 1yr lol) but it's treating me super well even today",AMD,2025-09-29 13:43:14,1
AMD,nh2hucr,"Aha, well... I even got a gifted MSi nVidia 4070 super something from friend who used it, but my Silver-certified BeSilent 400W PSU wont ever power that thing.    Maybe I should sell that thing for bux. Hm....",AMD,2025-09-30 20:20:37,2
AMD,ngu0mhg,Missed opportunity to call it Linamp,AMD,2025-09-29 14:30:07,8
AMD,ngrqp22,Thank you.,AMD,2025-09-29 03:38:15,2
AMD,ngtzgaf,tweaks and fonts required? new rebooted winamp or the original?,AMD,2025-09-29 14:24:08,0
AMD,ngvwda6,"Well, it is Windows AMP.  But a DOS version existed too. So... upvote?",AMD,2025-09-29 19:57:06,1
AMD,ngsfzgx,a ton ?,AMD,2025-09-29 07:12:54,3
AMD,ngrh1dw,I’ve always been curious how 360/PS3 games would look if the consoles had double or triple the ram. 1gb shared for 360 and 512mb/512mb for PS3,AMD,2025-09-29 02:35:09,9
AMD,ngrzzlh,"That's quite generous, it was a paper launch on december 22nd, 2011, with availability starting january 9th",AMD,2025-09-29 04:45:40,8
AMD,ngs6kfc,Yeap.,AMD,2025-09-29 05:42:51,1
AMD,ngtoty9,"Rebranded, yes. But 7970 GHz is higher clocked than 280X. The 7970 GHz edition is therefore faster, marginally though. [280X](https://www.techpowerup.com/gpu-specs/radeon-r9-280x.c2398) [7970 GHz](https://www.techpowerup.com/gpu-specs/radeon-hd-7970-ghz-edition.c365)",AMD,2025-09-29 13:26:50,1
AMD,ngsvgg4,"Aye, and the dual GPU cards was pretty cool. Nvidia also had a few! Not to mention 3DFX which actually created SLI.",AMD,2025-09-29 09:53:41,1
AMD,ngtwohi,"I never used that generation, but it is infamous for the driver issues.    I never had any more issues with ATI/AMD than with Nvidia. Far less black screens with AMD/ATI, especially early on with ATI as their ""VPU"" recover was far superior to Nvidas ditto. To be fair it took a few years for Nvidia to make a similar function that worked, if I remember correctly. More issues on new releases with AMD though. Which is expected as developers would be stupid to not focus optimizing for Nvidia due to market share. But in general hardware has been really stable last 10-15 years IMO, at least compared to late 90's early 2000's :D",AMD,2025-09-29 14:09:53,1
AMD,ngugvu8,"Calling it Linamp would imply it only runs on Linux.  The creators of XMMS were thinking big. XMMS not only runs on Linux, but also BSDs, Solaris and more. It can even run on Windows using Cygwin or MingW. As long as it supports Unix C and has an X server, it runs XMMS.",AMD,2025-09-29 15:50:44,4
AMD,nguclg3,Original.,AMD,2025-09-29 15:29:49,3
AMD,ngvut7e,"No tweaks needed, no fonts required unless you want them. You just pick whatever font you want for Winamp.  Both the original and WACUP work under Wine.",AMD,2025-09-29 19:49:36,1
AMD,ngshfff,Most multiplayer games that use kernel level anti-cheat cause Linux says no to anything touching its kernels.  Unfortunately I predominantly play multiplayer games so I'm SOL when it comes to linux gaming.,AMD,2025-09-29 07:27:34,7
AMD,ngt79qe,Like 5... Mayyyyyyybe 6.,AMD,2025-09-29 11:35:15,2
AMD,ngrto53,"I would say better draw distance, and slightly better texturing at the end of the generation…. It’s really amazing what they pulled out of that hardware to the end of the generation. The extra memory would have probably been very nice for gta/skyrim … I think a bigger question is what would PlayStation be like if the second cell chip as gpu had worked out, and programming for cell was a bit easier… the kissing cousins of powerpc architecture from the g5 to cell to xenon is an interesting read",AMD,2025-09-29 03:58:58,12
AMD,ngswfm2,"Oh yeah? Well, your mom is a rebrand of your grandma!",AMD,2025-09-29 10:02:57,4
AMD,ngutzin,"I had pretty regular crashes (maybe twice a week) for a year and a half or a so after i got it lol, even after ""the drivers have been fixed"" for the third time hahaha  It's a rock for me now, and I was never frustrated with it really because I got it knowing about driver issues and getting it cheap, and I was so happy with it's performance uplift vs my 970ti. I wasn't rich enough to get anything better and it was a steal, liek £150 off, becuase of driver issues I think. so I was really happy.   I get nauseous at low fps/hz, so I think that's probably teh biggest reason why I didn't care about the crashes - getting stable high frame rates in more games was way more significant than any crashes. I think if i was a normie I might've regretted it lol  it did feel like vintage hardware though lmao, it was so shitty the first couple months I had it especially",AMD,2025-09-29 16:54:01,1
AMD,nh2zcfo,Posixamp doesn’t have the same ring to it,AMD,2025-09-30 21:46:00,2
AMD,ngskezs,I play multiplayer too,AMD,2025-09-29 07:58:32,-1
AMD,ngrtzhz,Was there plans for a second cell chip?  I know initially they didn’t have the Nvidia GPU but I thought it was a “this single cell chip and SPEs can do everything!”,AMD,2025-09-29 04:01:17,3
AMD,ngviuzo,"Dang. I would've been fairly annoyed with that amount of crashes to be honest :P But I also sacrifice all I can to keep a high refresh-rate.  I've refunded games locked at 60FPS many times. But since AFMF2 released some have been spared if it worked well, like SOMA.  Edit: Wow, double posted apparently ""error 500""..",AMD,2025-09-29 18:51:39,1
AMD,nh41xmo,Also there’s already a separate project called Linamp. Apparently it’s some touchscreen in car entertainment system that runs Linux.,AMD,2025-10-01 01:28:24,1
AMD,ngrvyms,"I could have sworn it was, but it’s been a bit since I read through it I know the ps3 gpu was a very late addition to the hardware",AMD,2025-09-29 04:14:42,4
AMD,ngu1tsd,If I remember well the CELL itself was suppossed to be used for graphics processing on it's own too.  After all it is a CPU with a single core with SMT and seven SPEs as co-processors   But it wouldn't panned out well so they added an NVidia GPU late in development.  Still the CELL had some strong points compared to the GPU so some devs used it to improve graphics too.,AMD,2025-09-29 14:36:19,2
AMD,ngucu2u,"Cell was a very interesting chip, especially if you had been following PowerPC before it. So much possibility and power, but required a lot of the dev team, and I don’t think Sony ever got the dev tools really worked out as well as they wanted. Huh, I wonder where I got the second cell chip from going to have to research that now",AMD,2025-09-29 15:30:59,2
AMD,ngenrt2,"I'm glad they've finally gotten PyTorch to use ROCm, but it's a shame more computational routines/libraries don't use Vulkan... you can already get LLMs working on GPUs as old as GCN 2.0 (RX 300-ish) on that. The one thing Vulkan can't access (yet) are the tensor cores... hopefully we'll see that soon.",AMD,2025-09-27 01:14:37,15
AMD,ngj22gm,They promised this like since RDNA2. And RDNA2 is already in a maintenance state now. At lease it finally got there.  I used to think pytorch support will never arrive at amd card on windows though.,AMD,2025-09-27 19:22:57,1
AMD,ngnjl4v,>Vulkan can't access  It can! There's an [extension](https://registry.khronos.org/vulkan/specs/latest/man/html/VK_KHR_cooperative_matrix.html) that lets you use the matrix multiplication accelerators on all vendors.,AMD,2025-09-28 14:24:58,3
AMD,nge0ohc,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-09-26 22:52:14,1
AMD,ngbe95s,"this is huge for those that run any kind of AI workloads locally, I use a lot of those on Nvidia GPUs, making Radeon cards also viable and with good VRAM was long overdue",AMD,2025-09-26 14:54:45,121
AMD,ngchflo,rx 7800xt not included is some bullshit,AMD,2025-09-26 18:03:49,31
AMD,ngbwf27,"Great, but where to download it? Is it part of Adrenaline or as a standalone?",AMD,2025-09-26 16:23:09,19
AMD,ngbl0cd,This is huge. Why isn't this making national headlines?,AMD,2025-09-26 15:27:55,40
AMD,ngbyem3,It's about time,AMD,2025-09-26 16:32:39,7
AMD,ngcfhy8,Any way to get this working on 6000 series? i haven't had a need to upgrade my 6900xt.,AMD,2025-09-26 17:54:28,4
AMD,nge04vd,Why not 6000? Since it had amazing rocm support,AMD,2025-09-26 22:49:00,6
AMD,nge047x,Why not 6000? Since it had amazing rocm support,AMD,2025-09-26 22:48:53,3
AMD,ngen5f4,"what a coincidence, I was just trying to get pytorch stuff to work on my 7900xtx a few days ago. learned it wasnt supported yet.",AMD,2025-09-27 01:10:39,4
AMD,ngcyikl,whyyyyyyyyyyyyyyyyyyyyyyyyyy not 6000,AMD,2025-09-26 19:28:37,2
AMD,ngg7ugd,"Lack of support of some feature like this is why I'm still stuck with Nvidia. Let's hope amd catches up in this area as well over time. At least they have more vram on their cards at reasonable prices.  Yes, I mostly game on my PC but sometimes I need to do something else as well.",AMD,2025-09-27 08:47:09,2
AMD,ngdc8zf,"See, this is the problem with AMD. We just started reading about the performance improvements of Vulkan compared to ROCm on AMD cards and even some users found considerable improvements of LLM/PP improvements with ROCm-7 ([https://www.reddit.com/r/LocalLLaMA/comments/1nr5h1i/60\_ts\_improvement\_for\_30b\_a3b\_from\_upgrading\_rocm](https://www.reddit.com/r/LocalLLaMA/comments/1nr5h1i/60_ts_improvement_for_30b_a3b_from_upgrading_rocm))  So, if I want performance for LLMs, I have to upgrade to ROCm7 or later but if want to use SD, I have to switch to this (6.4.4) or we have to keep using 6.4.4 for Stable diffusion and vulkan for LLM's. Again, there is no mentioning about support for M150 or 6xxx series cards anywhere.  AMD for AI is Shit, there are not 2 ways about it.!",AMD,2025-09-26 20:37:33,5
AMD,ngdwmcb,This is exactly why I am so pissed.   Where is 7.0 for Windows like promised? Delayed. Of fucking course.,AMD,2025-09-26 22:27:50,1
AMD,ngi28c0,"Is the driver the same as the normal one with extensions or something totally different? I would try it and compare it to my wsl setup, but only if I can game with the driver.",AMD,2025-09-27 16:17:32,1
AMD,ngqj01y,So is this a driver I could / should use with my RX9070XT or is this something for developers...not gamers like me?,AMD,2025-09-28 23:12:51,1
AMD,ngswpgz,yippee!,AMD,2025-09-29 10:05:36,1
AMD,ngf4oy6,Maybe one day ROCm will Just Work™ like CUDA. One day.,AMD,2025-09-27 03:05:01,1
AMD,nggehtm,It's worked for basically since launch if you just bothered to use Linux you will always get less tokens per second on a Windows platform It is not for AI it is for gaming,AMD,2025-09-27 09:55:10,1
AMD,ngaw5kk,"What is this and is it useful for me as a gamer with a 9000 card? When? How to utilise it?  Only for ""ai-workloads""?",AMD,2025-09-26 13:21:46,-34
AMD,ngc084i,"They still need to start to more broadly support rocm on their cards. This is noz even the newest version of rocm. They still got plenty of work todo. While this is a huge change, it's more of a too little too late.",AMD,2025-09-26 16:41:37,47
AMD,ngcqikm,Same here. Wtf!,AMD,2025-09-26 18:48:53,12
AMD,ngh332q,Ah the classic AMD's ML experience. Polaris and Vega users be like: first time?,AMD,2025-09-27 13:09:46,6
AMD,ngdxpfi,No fucking way,AMD,2025-09-26 22:34:19,6
AMD,ngcdw2l,I dug through some pages and found it:  [https://www.amd.com/en/resources/support-articles/release-notes/RN-AMDGPU-WINDOWS-PYTORCH-PREVIEW.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-AMDGPU-WINDOWS-PYTORCH-PREVIEW.html),AMD,2025-09-26 17:46:46,9
AMD,ngbr58h,Call your local Senator tell them we need more AMD!,AMD,2025-09-26 15:57:54,30
AMD,ngbu7nb,"it's kinda niche lol. and most of the people whom this news attracts are probably already on team Nvidia, so yeah",AMD,2025-09-26 16:13:02,17
AMD,ngbqow6,lol,AMD,2025-09-26 15:55:36,6
AMD,ngc65tw,only 6% of all graphics cards are AMD and those using them for productivity/ai probably make up 10%> of that   if this was national headline material we might as well publicize magnitude 1.1 earthquakes,AMD,2025-09-26 17:10:06,1
AMD,nghkuqv,The driver for this is currently very limited in scope of cards covered.  I think you will have to wait and see if the high end RDNA2 cards get support later on.,AMD,2025-09-27 14:49:48,1
AMD,ngfo2ma,"As far as I am aware, you can have multiple rocm versions installed and in use by multiple applications, though I don't know about actively running multiple at the same time",AMD,2025-09-27 05:37:52,5
AMD,nggnu4b,amd is shit for ai and ray tracing and videos acceleration,AMD,2025-09-27 11:22:06,-2
AMD,ngaxp9b,"Irrelevant for gaming  Edit: I really don’t get the downvotes for the comment above, the guy maybe just didn’t know.",AMD,2025-09-26 13:30:10,33
AMD,ngb6xpw,Pytorch is a python library for data science. Using rocm you can run parts of your program on your gpu for fadter computation of embarrisngly parallel problems.   As a gamer you don't care.,AMD,2025-09-26 14:18:34,19
AMD,ngbavab,surprising computers existing for workloads that are not gaming,AMD,2025-09-26 14:38:13,12
AMD,ngb1zat,"Yes, ML/AI.",AMD,2025-09-26 13:53:08,3
AMD,ngbznn0,why the fuck we down voting people for asking questions?,AMD,2025-09-26 16:38:49,2
AMD,ngax5et,"It might be useful for fsr redstone support but I am not sure. Mostly, it's just for AI.",AMD,2025-09-26 13:27:09,-13
AMD,ngc101q,"well... almost up to now, Nvidia had even support for the old GTX cards... so yeah... huge support on every front there. Everything was ready for the compute / AI boom...",AMD,2025-09-26 16:45:25,15
AMD,nge38ik,"Llama.cpp is built primarily around Vulkan and receives near-daily optimizations; CUDA comes second, while ROCm almost never gets the improvements it should.   https://i.redd.it/mj1j3wrwblrf1.gif  ROCm will increasingly focus on supporting large-scale inference engines like vLLM and SGlang.",AMD,2025-09-26 23:07:33,16
AMD,ngfq9jy,"I'd be included in that. I held out for many years in the hope that AMD would figure something out for Windows support (e.g. Windows ROCm, HIP), but I couldn't hold out any longer and went for Nvidia. Now I can run whatever I want without worrying if my GPU supports it or not. I wish it wasn't like this, but I don't write the rules.  Universal APIs like Vulkan are what I hope become the standard, so that consumers don't become boxed into a single hardware vendor; but alas, I'm preaching to the choir.",AMD,2025-09-27 05:57:44,3
AMD,nge3lxu,Over a quarter of all dGPUs are AMD. You’re confusing quarterly shipment share with the installed dGPU base.,AMD,2025-09-26 23:09:48,7
AMD,ngdrp2q,"This makes them more usable I guess... well at least the ones that are supported by rocm, never mind the correct version.   Question is, this is so basic why didn't this happen 9 years ago?  But yeah, this could only make headlines in reddit or any parody newspaper that wants to make fun about the state of rocm.",AMD,2025-09-26 21:59:19,2
AMD,ngjeyub,"Yes, but it's (slowly) changing.  With HIP 6.4.2 and ZLUDA my 6800 XT @ 2700MHz is usually only 3-5% slower than a 3080 12G in ComfyUI, and thanks to the extra VRAM I can run model parameters that causes an OOM on the 3080.  Ray tracing still sucks on my card, but the RX 9000 series is basically just 1 generation behind as opposed to 2 or 3 generations on 6000/7000 series.  Same goes for video acceleration, the 9000 series codec is *actually* good now. Yes, that does nothing for 6000/7000 series, but they did finally listen to demands and address the problem.  As others have said, it's mostly a bit ""too little too late"", but if you compare the resources and R&D budget between Radeon and nVidia it is actually incredible that they're even remotely comparable when it comes to tech and features.",AMD,2025-09-27 20:30:45,6
AMD,ngc9rbj,I think it is because of ai-workloads with quotes. It makes the whole comment sound rude.,AMD,2025-09-26 17:27:05,5
AMD,ngc3iy7,Cheers mate. <3,AMD,2025-09-26 16:57:34,2
AMD,ngc9ns9,I think it is because of ai-workloads with quotes. It makes the whole comment sound rude.,AMD,2025-09-26 17:26:36,1
AMD,ngb0or2,Fsr doesn't use pytorch lol,AMD,2025-09-26 13:46:17,9
AMD,nggk96l,"hear ye hear ye, I feel ya man",AMD,2025-09-27 10:50:47,3
AMD,nge3ziq,"This is far from basic... it takes a large team of engineers and constant effort to optimize and resolve issues across countless combinations of hardware, software, OS, and drivers. Just nine years ago, AMD was on the verge of bankruptcy.",AMD,2025-09-26 23:12:03,5
AMD,ngklpyy,9 years ago AMD was on the brink of bankruptcy due to catastrophically low sales of FX series.   It didn’t happen because AMD was scrambling to fix their real money maker. They simply weren’t focused on anything other than trying to get good CPUs to market to fix their bleeding.,AMD,2025-09-28 00:41:00,2
AMD,ngc9z2a,"Oh, I guess I didn't get that in a rude way",AMD,2025-09-26 17:28:06,3
AMD,ngd2z9o,And ignorant.,AMD,2025-09-26 19:51:19,-3
AMD,ngb6znh,"Yeah, bring on Redstone instead.",AMD,2025-09-26 14:18:51,2
AMD,ngbeecn,I skipped the pytorch part but I thought rocm wasn't available on windows and that fsr redstone would use rocm to run on previous gens.,AMD,2025-09-26 14:55:26,1
AMD,ngm6d28,"You cannot say ignorant bro. We are not in Localllama or stablediffusion subs, we are in AMD sub and you should not expect everyone to know or care about GenAI.",AMD,2025-09-28 07:55:07,1
AMD,ng8f8u2,These things always hit so late. The hype for Monster Hunter Wilds has died down considerably and I don't think that either thing is going to boost the other's hype personally.,AMD,2025-09-26 01:45:11,52
AMD,ng7yybk,"so... its just a regular rx 9070 xt but with a stencil on it publicizing the game, it doesnt come with the free game or someting actually useful?",AMD,2025-09-26 00:09:07,45
AMD,ng8ou25,Nope,AMD,2025-09-26 02:44:25,9
AMD,ng7yjet,"Whats kinda funny, kinda sad, is that this card would not be able to play the game very well. No card can really play the game well, its still a shit show.  Edit: I think what I am getting from all of the replies is that Linux makes a big difference, and 9070 and also 9070XT actually plays this game a step above the rest of the cards in their tier",AMD,2025-09-26 00:06:36,33
AMD,ng90gn9,Collaborating with Capcom when Dragon's Dogma 2 and Monster Hunter Wilds are notoriously unoptimized 💀,AMD,2025-09-26 04:04:33,3
AMD,ng95lz3,ASrock 9070XT Performance Issues Edition,AMD,2025-09-26 04:45:08,4
AMD,ng8pqvz,damn that is actually hideous,AMD,2025-09-26 02:50:07,2
AMD,ng9bkbc,Should do a Chun-Li and a Cammy edition to print money…  Character art should be based on mod skins for SF6.,AMD,2025-09-26 05:36:14,1
AMD,ng9iwyd,"Wonder how much this will cost, it seems like it's just their Steel Legend card with a different colour.",AMD,2025-09-26 06:43:13,1
AMD,ng9os8z,"So they will fix the driver bug affecting 9070 series, right? Having crazy perf issues since the May or June drivers, tanking fps to the 20s from simply rotating the camera too much, eg in scarlet forest.   On earlier drivers it rarely dips below the 80s under the same conditions. Now I need frame gen for the game to run tolerably.",AMD,2025-09-26 07:39:41,1
AMD,nga2clr,I want velkhana,AMD,2025-09-26 09:55:28,1
AMD,ngawucf,They need to do this for devil may cry 6 if it releases,AMD,2025-09-26 13:25:30,1
AMD,ng8fugb,I must be tired.  I thought AMD was helping improve the game on RX 9070XTs  > Capcom to launch the ASRock AMD Radeon RX 9070 XT MONSTER HUNTER WILDS EDITION 16 GB limited edition **co-branded graphics card**,AMD,2025-09-26 01:48:41,1
AMD,ng8obyf,"I don't think anything's going to improve the hype for MHW, given how bad its was in performance and content pipeline. When I looked 2-3 months ago, it has lost over 95% of its peak player base.",AMD,2025-09-26 02:41:16,24
AMD,ng871ie,"From my limited testing with a 9070 XT against a 5070 Ti, the 9070XT actually performs about 15-20% faster than the 5070 Ti in Wilds. Making the 9070XT one of the best cards in existence to play the game.  I suspect the reason is the GPU code having a lot of serial-limited performance, which is completely silly to run on a GPU in the first place, resulting in most Nvidia GPUs not having the boost clock headroom to compete in performance. The flip side is that Wilds will run at ~70% of normal power draw on Nvidia GPUs.",AMD,2025-09-26 00:57:26,10
AMD,ng82d64,"9070 non xt here, game runs fine for 70+ series cards from both gpu manufacturers. You can easily get a locked 1440p 70+fps with a 9070 (FSR Quality). I keep mine unlocked and can get as high as 120+fps in some areas.  Youre not gunna get super high fps unless you have a 5090 or something but to say 60+fps = cant play the game well is so ridiculous. Yeah the game has performance issues but people saying its unplayable for mid/high tier cards is stupid.",AMD,2025-09-26 00:29:49,6
AMD,ng8ek4u,"I have a 9070XT. I play Wilds on Linux, it's pretty solid at 60FPS, roughly 1440p (4k but with FSR), HDR and ray tracing. It's a perfectly good card for this, especially if you happen to use Linux.",AMD,2025-09-26 01:41:12,6
AMD,ngfpl5h,this is unbelievably false. i don't know what you call not-very-well or a shit-show but i have a 7800xt and it is far from unplayable or terrible. i will admit Capcom needs to really get their shit together with their last two releases having terrible optimization but this game runs well with my pc with some compromises to graphical fidelity. i'm playing on 1440p / fsr3 quality with most settings set to high and some to medium. it's not a polygonal mess or blurry textures or stuttery mess. i'm on windows as well not linux.       again it's not optimized but i can still achieve 60fps at 1440p with my 7800xt. a 9070xt i would imagine would run this much better than my setup.,AMD,2025-09-27 05:51:33,1
AMD,ng8xfzh,"Wilds will be fine, title update 4 will bring the performance patch and the expansion will bring players back  The problem is all the new players got into worlds expansion iceborne with all the title updates and expected wilds to launch out of the gate like that, thats pretty unfair  Performance is pretty poor for low/mid end hardware for sure, but the same thing was said for world which was also really terrible performance wise.    The thing Monster Hunter has going for them is that its not a GAAS, so its pretty much timeless to play. Plenty of people are still playing rise, world, and even Generations ultimate which is getting really old at this point because theyre fun games.  Capcom already knows they messed up with wilds and theyre not gunna let their golden child burn to the ground like Dragons Dogma",AMD,2025-09-26 03:42:16,-7
AMD,ngbjd6p,"Thats actually really interesting, had not heard this before. I remember MH World ran slightly better on my RX480 8GB than my friends 1060 6GB at the time. AMD might have sponsored that game though, I don't remember.",AMD,2025-09-26 15:19:42,2
AMD,ng83eia,"I have a 6950XT but inarguably a shit tier CPU, so I was getting bad frame dips. I talked to a friend running a 5070ti and 9800x3D with 64GB of ram and he gets very similar frame dips, even after the patch from a couple months ago. Some areas just chug hard too, despite having pretty good frame rates elsewhere.  And like I love the aesthetics, I think the character details and monsters look amazing. But sometimes in battle it will just break out into bad stutters, and that makes the game feel really off. That's what makes it unplayable for my friends and I.",AMD,2025-09-26 00:35:57,4
AMD,ngrkzry,That's the point that it needs fsr.... to run alas it's not optimized at all,AMD,2025-09-29 03:00:10,1
AMD,ngoc2ny,"Same here, and for the fans for Monster Hunter game, if you are looking for a new GPU, this would be a great buy.",AMD,2025-09-28 16:44:40,1
AMD,ng9tlwp,"> Wilds will be fine, title update 4 will bring the performance patch and the expansion will bring players back  A **potential** performance fix a year later, somebody please give Capcom a noble prize. They are only even entertaining it because they will want to peddle their expansion in late 2026.   > The problem is all the new players got into worlds expansion iceborne with all the title updates and expected wilds to launch out of the gate like that, thats pretty unfair  It is generally expected that new entries improve and add to experience from previous game - you do understand that new game in the series doesn't mean a complete reset? World was closer to core MH experience, that's why it's still so insanely popular - it's not just iceborne. Hell, Wilds still doesn't have some of the features and friction that base World did on release..  > Performance is pretty poor for low/mid end hardware for sure, but the same thing was said for world which was also really terrible performance wise.  Nonsense, World ran at a stable 60 fps with everything maxed on my entry level 3060 with no upscaling - its performance wasn't even close to being as bad as the one in Wilds which struggles to keep stable framerate on my 5090 and 9800x3d if I don't enable framegen.   > Capcom already knows they messed up with wilds and theyre not gunna let their golden child burn to the ground like Dragons Dogma  The only reason they will be attempting to fix the performance is because they want that expansion money but Wilds has far more issues than just performance, it has deviated way too far from core MH experience and Capcom won't see the sales numbers as high ever again for MH. The good will from World has been burned to the ground, Capcom themselves have stated that post launch Wilds has barely sold any copies which is a testament to how bad your product is when it doesn't have the hype of the previous game to carry it.",AMD,2025-09-26 08:28:37,6
AMD,ng91m61,"> Wilds will be fine, title update 4 will bring the performance patch and the expansion will bring players back  The game will be close to a year old by then. People aren't coming back for optimizations after so many big games will have launched by then. That update isn't saving it.  >Performance is pretty poor for low/mid end hardware for sure, but the same thing was said for world which was also really terrible performance wise.   No, the performance was awful almost universally. Daniel Owen ran the benchmark with a 5070 Ti and 9800X3D, and it couldn't reach 80 FPS at 1440p. They turn on frame gen on minimum performance targets because it runs so badly.  >The thing Monster Hunter has going for them is that its not a GAAS, so its pretty much timeless to play.  Except their launch was a 100% GaaS experience. Broken, promising content that you're bragging should be out a year after launch. If that's not GaaS, nothing is.  >Plenty of people are still playing rise, world, and even Generations ultimate which is getting really old at this point because theyre fun games.  Because Wilds was so disappointing for them. There are almost as many people on World ad Wilds, according to SteamDB. If that's because World is fun, then Wilds (which has triple the peak player base) must be the least fun game ever.  Edit: To add, since I looked. The 24-hour peak for this game is 20K, 1.5% of the peak player base. 98.5% of the players are gone, and it's ""timeless"" and ""fun?"" They'll come back in a future update, but it's not a GaaS offering?",AMD,2025-09-26 04:13:24,9
AMD,ng9v7g7,Assuming there even is a performance patch coming.,AMD,2025-09-26 08:45:01,5
AMD,ng90d4t,I do not experience this. 5800x3d Linux 9070nonxt winetkg+patchset,AMD,2025-09-26 04:03:48,3
AMD,ngwd0f2,"It doesnt really ""need"" FSR, infact when im playing off stream i just use the Native AA option which still keeps me over 60fps.  Quality looks just as good as off in 1440p because of the anti-aliasing solution it provides on its own. Jaggies and shimmering are so annoying.  Besides in like 3-4 years when everyone upgrades to the next budget cpu and gpu people are gunna be able to brute force wilds and have a crapton of content due to all the title updates/DLC/final updates like world.",AMD,2025-09-29 21:17:12,1
AMD,ngbazcw,">A **potential** performance fix a year later, somebody please give Capcom a noble prize. They are only even entertaining it because they will want to peddle their expansion in late 2026.  People seem to forget world on PC/PS4 ran like dog water and that was ""fixed"" plus they already announced on their official social media platforms that a huge performance patch is coming in TU 4 in December.  >  >**It is generally expected that new entries improve and add to experience from previous game - you do understand that new game in the series doesn't mean a complete reset?** World was closer to core MH experience, that's why it's still so insanely popular - it's not just iceborne. Hell, Wilds still doesn't have some of the features and friction that base World did on release..  This is what I mean when I say World players don't understand Monster Hunter content. Every single monster hunter game Is a hard reset with new mechanics and new monsters (with returning monsters) With an expansion later on to add master rank. Its been like this since the original monster hunter and has done this **Every. Single. Game.**  World threw away a lot of the core Monster Hunter experience and made it more casual. Scout flies that auto track monsters, being able to move while using items, whetstones being infinite instead of being a consumable item, etc etc.  >  >Nonsense, World ran at a stable 60 fps with everything maxed on my entry level 3060 with no upscaling - its performance wasn't even close to being as bad as the one in Wilds which struggles to keep stable framerate on my 5090 and 9800x3d if I don't enable framegen.  When world released your GPU didn't exist. So you literally don't know how bad it was when the only way to get a locked consistent 60 fps was having a GTX 980TI at minimum **At 1080p**. Wanted to play at 1440p 60fps locked? 1080ti was your only option, and don't get me started at 4K LMAO.     The game was even more unstable than wilds     [https://www.reddit.com/r/MonsterHunter/comments/1ixv4fq/what\_was\_the\_pc\_performance\_of\_world\_on\_release/](https://www.reddit.com/r/MonsterHunter/comments/1ixv4fq/what_was_the_pc_performance_of_world_on_release/)",AMD,2025-09-26 14:38:46,-2
AMD,ng9u6a9,"> Except their launch was a 100% GaaS experience. Broken, promising content that you're bragging should be out a year after launch. If that's not GaaS, nothing is.  Not just that, we know for a fact many things from Capcoms so called 'Free Title Updates' were things that were supposed to be in the main game on release that then were repackaged as Updates.   I truly hope that Capcom understands that after TU4, they need to start releasing actual Title Updates because most of the things added so far are just what we were supposed to have in the base game.",AMD,2025-09-26 08:34:29,3
AMD,ngb7dfs,">The game will be close to a year old by then. People aren't coming back for optimizations after so many big games will have launched by then. That update isn't saving it.  Nonsense, Look at worlds player count over the years.  >No, the performance was awful almost universally. Daniel Owen ran the benchmark with a 5070 Ti and 9800X3D, and it couldn't reach 80 FPS at 1440p. They turn on frame gen on minimum performance targets because it runs so badly.  That was at launch and I'm pretty sure he hasn't tested it afterwards, Ive gotten at least a 10+fps jump over the updates.  >   Except their launch was a 100% GaaS experience. Broken, promising content that you're bragging should be out a year after launch. If that's not GaaS, nothing is.  The game has a beginning and ending, Monster hunter gets updates for a year before the expansion releases then gets another year or so of content updates before development is finished. The ONLY leg people have to stand on in this is that title update 1 was probably meant to be in the base game at launch, otherwise every other title update is brand new content.  >Because Wilds was so disappointing for them. There are almost as many people on World ad Wilds, according to SteamDB. If that's because World is fun, then Wilds (which has triple the peak player base) must be the least fun game ever.  >Edit: To add, since I looked. The 24-hour peak for this game is 20K, 1.5% of the peak player base. 98.5% of the players are gone, and it's ""timeless"" and ""fun?"" They'll come back in a future update, but it's not a GaaS offering?  Because every monster hunter is different. When Rise released people continued to play world over it, but both games sold extremely well even though the concurrent player count says differently. No two monster hunter game carries its core mechanic for that game over to the next one (Generations had skill attacks, world had clutch claw, rise had wirebug/skills) that change the way the game is played.     Wilds is also crossplay, and im sure its console concurrent playerbase is currently much larger than pc atm.     Yes the game is timeless and fun as referenced that all modern monster hunter games still have tons of players playing it every day.     If you love looking at concurrent player data then check when Monster Hunter World released and how its player base fluctuated after launch, after its expansion release, and after its final update back in 2020 Wilds has a lower but similar situation to World. The only thing that would destroy Wilds currently is if they mess up the expansion and updates afterwards. Then itll be unsalvagable",AMD,2025-09-26 14:20:45,-1
AMD,ngaj3sf,They already confirmed this   https://x.com/monsterhunter/status/1957473035885121806,AMD,2025-09-26 12:04:38,0
AMD,ngbii3a,"I think what I am getting from all of the replies is that Linux makes a big difference, and 9070 and also 9070XT actually plays this game a step above the rest of the cards in this tier. Gonna have to redownload it after I switch over to Linux next month.",AMD,2025-09-26 15:15:21,1
AMD,ngbh94q,"> People seem to forget world on PC/PS4 ran like dog water and that was ""fixed"" plus they already announced on their official social media platforms that a huge performance patch is coming in TU 4 in December.  All Capcom did was release some slight VRAM tweaks, it never addressed the underlying issues which are mostly CPU bound just like in DD2. You don't need to do reddit damage control for a multi billion dollar corporation.   > This is what I mean when I say World players don't understand Monster Hunter content. Every single monster hunter game Is a hard reset with new mechanics and new monsters (with returning monsters) With an expansion later on to add master rank. Its been like this since the original monster hunter and has done this Every. Single. Game. >  > World threw away a lot of the core Monster Hunter experience and made it more casual. Scout flies that auto track monsters, being able to move while using items, whetstones being infinite instead of being a consumable item, etc etc.  'World players' lmao  I guarantee that I've played MH series since before you were born. Nothing about **MAINLINE** MH core design has changed since MH: FU, the biggest mainline diviation for the worse occured in Wilds.   Your argument is nonsensical as you seem to be under the belief that every MH game follows the same formula when it is convenient for Capcom while attempting to excuse bad design choices 'because every MH game is different durr'.   World added conviniences to existing core design, Wilds just erased core designs - a small but important distinction.    > When world released your GPU didn't exist. So you literally don't know how bad it was when the only way to get a locked consistent 60 fps was having a GTX 980TI at minimum At 1080p. Wanted to play at 1440p 60fps locked? 1080ti was your only option, and don't get me started at 4K LMAO.  Oh really, is this '*you don't know bad it was*' in the room with us right now? Equivalent of the SAME hardware today can't even hit a stable 45fps without framegen in Wilds, stop deluding yourself:  https://www.youtube.com/watch?v=O1ScWR6HuIQ",AMD,2025-09-26 15:09:17,2
AMD,ngd5x78,https://www.techpowerup.com/review/performance-analysis-monster-hunter-world/4.html  Rx 580 averaging above 60 fps at 1080p no problem. 1070 Ti for 60 fps @ 1440p. Both at highest settings minus volumetric rendering. Conclusion by the author of the article: 'Monster Hunter: World is a solid port of the console smash hit to the PC platform.',AMD,2025-09-26 20:05:55,1
AMD,ngb889o,">  >Not just that, we know for a fact many things from Capcoms so called 'Free Title Updates' were things that were supposed to be in the main game on release that then were repackaged as Updates.  >I truly hope that Capcom understands that after TU4, they need to start releasing actual Title Updates because most of the things added so far are just what we were supposed to have in the base game.     We only know the first title update was ""maybe"" supposed to launch with the base game because a lot of the files were already there. No one has any clue about TU2 and onwards and were just speculating because there were no files datamined for Lagiacrus or having an area made specifically for him including underwater combat.     Also I didn't know the Final Fantasy Collab in Title Update 3 was supposed to be in the base game hmm.",AMD,2025-09-26 14:25:03,-1
AMD,ngbubvv,"6 months after launch, they said they'll fix SOME stuff in another 4-6 months? GREAT!",AMD,2025-09-26 16:13:34,3
AMD,ngbb9nc,"> We only know the first title update was ""maybe"" supposed to launch with the base game because a lot of the files were already there. No one has any clue about TU2 and onwards and were just speculating because there were no files datamined for Lagiacrus or having an area made specifically for him including underwater combat.  Wrong on both counts.   Both the TU1 and parts of TU2 were already in the data on release, Lagicarius specifically had the largest dataset available and given where it sits in terms of quest Hierarchy of Wilds and story relevance, it was absolutely intended to be in the base game. Not to mention the fact that Zoh Shia and gathering hub were not even included tells one all there is to know.   > Also I didn't know the Final Fantasy Collab in Title Update 3 was supposed to be in the base game hmm.  FF collab shouldn't be in game period, a stupid weeb slop that no one cares for while pushing Gog to Decmeber update.",AMD,2025-09-26 14:40:13,1
AMD,ngd8w2o,They have done some small optimization here and there. World was even worse and thats considered one of the best games now.  Its just the world we live in now but under all those performance issues is a really good game.,AMD,2025-09-26 20:20:40,1
AMD,ngbd8to,Weeb slop LMAO no one can take you serious if you say that about a Japanese game that's straight up anime at this point.,AMD,2025-09-26 14:49:53,-1
AMD,ngbdosc,"I mean, if you believe MH and FF ever had anything in common you must have never played the two on their own.",AMD,2025-09-26 14:52:01,1
AMD,ngbge71,"They both have monsters.  Monster hunter has collabed with attack on titan, hatsune miku, and fucking pizza hut. Theyve always done wild collabs its nothing new.  YoshiP ACTIVELY plays monster hunter when he has the time and has stated he loves the series.Tsujimoto or Tokuda has also implied at one point that they wanted to hire YoshiP before he took the took on the role as producer for FF14",AMD,2025-09-26 15:05:05,-1
AMD,ng7wvu1,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-09-25 23:56:33,1
AMD,ng94t35,"Wow, at this point I'm convinced amd has a whole warehouse full of zen 3 silicon leftover.",AMD,2025-09-26 04:38:41,80
AMD,ng8drxw,5100x3d 🙏,AMD,2025-09-26 01:36:38,75
AMD,ng8aqsd,So it's just a 5300g with the Vega igpu disable,AMD,2025-09-26 01:18:52,42
AMD,ng86sl5,AMD just won't let AM4 die...,AMD,2025-09-26 00:55:59,21
AMD,ngde6ip,"Hmm, what's next? Can I hope for ryzen 3 7100~~x3d~~? *just for giggles*",AMD,2025-09-26 20:47:16,2
AMD,ngpggo7,"if it was a vermeer chip I would be happier, it better cost 40 bucks for being worse than 3600 that costs 75 already.",AMD,2025-09-28 19:54:11,1
AMD,ngvz3iy,"My first thought was Black Friday ""Gaming PC"" specials, but what's the 2025 equivalent of a GTX 745 2GB?     OMG, were gonna see AMD dump the last of the RX 6500 XT stock...",AMD,2025-09-29 20:10:30,1
AMD,ng94359,So a 2025 CPU worse than Ryzen 3 3100,AMD,2025-09-26 04:32:55,-13
AMD,ng9j1ek,Zen 3 Milan is still being produced... so yes.,AMD,2025-09-26 06:44:22,38
AMD,ngdjidt,"5 years of official support, starting from late 2020 less than 5 years ago.",AMD,2025-09-26 21:14:47,11
AMD,ngb07nx,A 4c/8t x3d CPU would be the king of budget gaming!,AMD,2025-09-26 13:43:46,20
AMD,ng9ln1y,Why would a Ryzen 3 5100 be worse than a Ryzen 3 3100?,AMD,2025-09-26 07:08:31,15
AMD,nge6xdc,*Userbenchmark aneurysm noise*,AMD,2025-09-26 23:29:39,8
AMD,ngatl4y,"Going off [Tom's Hardware](https://www.tomshardware.com/news/ryzen-3-5100-budget-cpu-could-excel-on-the-retail-market), it seems the Ryzen 3 5100 has 4MB of L3 while the Ryzen 3 3100 has 8MB of L3. However, this was misreported, as according to the source Tom's Hardware used ([a Gigabyte CPU support list](https://www.gigabyte.com/Ajax/SupportFunction/Getcpulist?Type=Product&Value=6894)) the Ryzen 3 5100 does indeed have 8MB of L3 cache. It wouldn't be unusual coming from AMD to do this though (see Ryzen 8400F v.s. 7400F, Radeon RX 5300 v.s. RX 6400, etc)",AMD,2025-09-26 13:07:36,8
AMD,nga0rn3,Isn't ZEN 2 equal or better than Cezanne in most of the workloads including gaming? It also got PCIE 4.0 support and more L3 cache on average.,AMD,2025-09-26 09:40:36,1
AMD,ngau6bb,"The Ryzen 3 5100 [*does*](https://www.gigabyte.com/Ajax/SupportFunction/Getcpulist?Type=Product&Value=6894) have the same amount of L3 cache as the Ryzen 3 3100. While PCIe 3.0 is a bummer, it's really only gonna affect GPU performance if you're using something like a 4090 with your Ryzen 3 5100, or if you're using bandwidth-starved GPUs that you most likely shouldn't be buying in the first place, like the RX 6500 XT. Can't speak for the architecture & cores though, and given AMD's made worse products than the previous generation (ex. RX 6400 v.s. RX 5300) it wouldn't be a surprise.",AMD,2025-09-26 13:10:52,4
AMD,ngmxuox,The price is gonna be nuts.,AMD,2025-09-28 12:11:19,4
AMD,ngyio6f,"Going to come with a small generator, because that battery has no shot!",AMD,2025-09-30 05:17:05,1
AMD,ngnwmuw,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-09-28 15:30:42,0
AMD,ngdwl0j,"Funny who people don’t realize how versatile this APU(CPU+iGPU) is, especially with the power profiles that it provides.",AMD,2025-09-26 22:27:37,12
AMD,ngy8t2j,But where is official support of rocm hip for zen3 and older igpu?,AMD,2025-09-30 03:57:10,1
AMD,nghda04,There simply isnt a more capable package for the money.   One could argue m4 ultra but arm has limitations. And that cost ...,AMD,2025-09-27 14:09:27,6
AMD,ngwhodj,It's been praised everywhere it's been reviewed.. what do you mean?,AMD,2025-09-29 21:41:12,1
AMD,nh0o6km,"There is, take the ARM based SOCs like Qualcomm’s Snapdragon. With their unmatched power efficiency, absolutely ideal for Handhelds. I could imagine a Handheld with a Snapdragon CPU and an AMD RDNA5 GPU (chiplet) that runs on just 15W or even less.",AMD,2025-09-30 15:03:53,1
AMD,ngx3sdu,There are some people here on Reddit and elsewhere that could not believe it’s also ideal for Handhelds. Maybe you have not seen them?,AMD,2025-09-29 23:44:18,2
AMD,nh2p0gg,I meant currently available. That would be quite the handheld though.... Would need to make sure the arm side was powerful enough to allow for translation of x64 based code without hindering performance.,AMD,2025-09-30 20:54:23,1
AMD,nh32a7o,Completely agree. Eventually it will happen?  Edit: you mean x86_64/amd64  [Check](https://devicetests.com/difference-between-x86-64-amd64-64-bit)  although most of the older games are x86_32,AMD,2025-09-30 22:01:37,1
AMD,ngb4lc1,"Price is bonkers, sadly.",AMD,2025-09-26 14:06:42,42
AMD,ngbe1q6,ridiculous pricing,AMD,2025-09-26 14:53:45,9
AMD,ngbe8av,"They're obv trying to make a more premium product, so not gonna hold it against them.",AMD,2025-09-26 14:54:38,10
AMD,ngdo52f,"I don't know why your wouldn't go for the Framework desktop instead of this. Costs less and Minisforum is notoriously bad with updates, firmwares, etc. They basically don't support their products, which is fine, but in this case they are more expensive. Maybe it makes more sense for different markets?",AMD,2025-09-26 21:39:45,5
AMD,ngazqm7,"$2300. Wonder where all the ""I can't wait until strix halo goes into mini pcs"" bros are. . .",AMD,2025-09-26 13:41:14,25
AMD,ngcm3ee,"Do want, but that price is crazy",AMD,2025-09-26 18:26:58,2
AMD,ngbhe6b,"I’ll prefer to pay 2,300 in tokens rather than having an entry level “AI” machine with the power of a laptop RTX 4060M.",AMD,2025-09-26 15:09:58,-7
AMD,ngfcn5i,I try to stay away from these malware computers.,AMD,2025-09-27 04:03:10,-1
AMD,ngbtcp3,"A Mac Studio with M4 Max, 128 GB of RAM and 2TB storage is over $4000.",AMD,2025-09-26 16:09:00,16
AMD,ngfh2c8,"yeah we are not professionals or ai, sadly",AMD,2025-09-27 04:37:54,2
AMD,nglr0wn,"It is, but until something that can stack RAM to these numbers comes out, it's gonna basically be the local LLM darling of hardware purchases and I'm pretty sure they're gonna sell quickly.",AMD,2025-09-28 05:32:38,1
AMD,ngbixo9,"Small and powerful is usually not cheap, and here the manufacturer did compromise on the cooling to have a quiet system to keep the cost down. Go have a look into the build of a Mac Studio and the cooling it has for quiet running.",AMD,2025-09-26 15:17:31,4
AMD,ngswcpl,"> Costs less  Not really.  Framework desktop starts at 2359 EUR (the 395+ version; no coupons available) naked, without any drives, fan, cords or (replacable) ports.  Minisforum is 2399 EUR before coupons, can currently go down to 2169 EUR with coupons, and includes 2 TB drive, (non-replacable) ports, fan, power cord and a Windows 11 license.  Other than that, Framework has plastic case, single 5 GbE networking. The second M.2 provides full-performance with 4 lanes of PCIe 4. The after-sale support will probably will better.. The Minisforum has metal case, dual 10 GbE networking, but the second M.2 is only single lane PCIe4 and the after sale support is expected to be worse.  So both choices have pros and cons.",AMD,2025-09-29 10:02:12,1
AMD,ngbqs3x,"I would love to have one of those mini-PCs  But Jesus christ, never expected to be priced like this. And I have to ship from overseas since they dont sell in my country, and it would get even more expensive",AMD,2025-09-26 15:56:03,8
AMD,ngcj1pa,"With 16 cores and 128GB memory this isn't aimed at mainstream gamers. Even then, this is launch pricing that will go down. GMK has the 128GB for $1800. The more sensible 64GB version is going for $1500.   You can go even cheaper on the Strix Halo line: Framework sells the Max 385 board with 32GB memory for $800. No Mini PC announcements yet. You save money by dropping unneeded CPU cores (16 to 8), but unfortunately GPU cores drops from 40 to 32.  Still, this would give a substantial jump in performance over the 890m found in the HX 370.   It's still frustrating that there isn't an affordable successor to the 780m GPU like found in the 7000 series, which goes for $300-500. And looks like we'll have to wait until 2027 for new APUs from AMD. So yeah... not great right now and wish there were more competitors in this space.",AMD,2025-09-26 18:11:49,14
AMD,ngb6yiv,"No sane person really expected this to ever be cheap, that's too much copium",AMD,2025-09-26 14:18:41,16
AMD,ngh0w2b,Here i am. I think i need to wait a bit longer.,AMD,2025-09-27 12:56:16,1
AMD,ngrrsiz,"I'm here, I'll just wait 😭",AMD,2025-09-29 03:45:59,1
AMD,ngbytg8,Your rtx 4060m has access to 128gb of unified ram?,AMD,2025-09-26 16:34:41,12
AMD,ngbz194,Well yes but you can get the Strix Halo and 128 GB of RAM in a much cheaper mini-pc from other brands,AMD,2025-09-26 16:35:45,15
AMD,ngfgl07,Strix halo performs like the M4 Pro not Max.,AMD,2025-09-27 04:34:04,5
AMD,ngfh5ka,"apple is not even an option here, it's obvious",AMD,2025-09-27 04:38:37,2
AMD,ngbxixu,"I tried to tell everyone if this came to mini pc's, they'd be $1500+. This isn't going in the $4-600 mini pcs people are used to.",AMD,2025-09-26 16:28:23,8
AMD,ngdom0w,"There will be. I also suspect we'll see the fruits of the Steamdeck success as well as Xbox and Sony handheld development soon. And by soon I mean ~yr away, best case. I think that's when we'll see the massive leap gen to gen with handheld gaming performance.  You also have Nvidia and Intel likely plotting something as well.",AMD,2025-09-26 21:42:20,3
AMD,ngglta3,"Panther Lake is coming to handhelds, maybe Nvidia with their N1 chip but perhaps they'll wait for N2 before entering the handheld market.",AMD,2025-09-27 11:04:40,3
AMD,ngc0ybd,It's not hard to find posts expecting this to be $1500-1000.,AMD,2025-09-26 16:45:11,10
AMD,ngdh740,"You can get it for cheaper, especially if you buy direct from China but it's not ""much"" cheaper. It's still around the $2000 ballpark for this kind of spec.",AMD,2025-09-26 21:02:36,2
AMD,ngzoi91,"But not with 10 GbE, that is limited to very few models (Framework Desktop, Beelink GTR9 Pro) and they are not much cheaper",AMD,2025-09-30 11:44:13,1
AMD,ngg46f0,"People can dream, but how much copium do you have to inhale to expect this thing to cost less than GMKTec while having 2x 10G LAN and 2x USB4v2?",AMD,2025-09-27 08:09:39,2
AMD,ngdl9f0,"Import taxes say ""No"" to your idea",AMD,2025-09-26 21:24:10,6
AMD,nghcjzy,I don't think those connections add a ton of cost. They can't do anything to the chipset so even having 2 connectors doesn't mean they'll automatically both be full bandwidth.,AMD,2025-09-27 14:05:22,2
AMD,nfv8xmx,*yawn* let me know when it's in more than 1 laptop you can buy.,AMD,2025-09-24 00:44:27,6
AMD,ng35nds,I don’t know why you’re getting downvoted. It’s strange that there hasn’t been more laptops out with Strix Halo.,AMD,2025-09-25 07:10:56,7
AMD,ng41az5,There's a lot of copium from people who want strix Halo in sub $1000 products.   My biggest upvoted comment is a picture of tacos. I don't really care about reddit popularity 🤷‍♂️,AMD,2025-09-25 11:57:11,6
AMD,nfv2xy5,I bought a 9800x3d at launch for msrp and was fully expecting this thing to go down in price. Amd is normally pretty aggressive with price cuts and the rest of zen 5 got discounted in 2 months after launch. We are only getting a 50 dollar discount now cause intel shit the bed hard and gave no competition. The biggest competition amd has is to themselves with the 7800x3d lol.,AMD,2025-09-24 00:09:03,148
AMD,nfv0hja,7800X3D consistently on ebay for $300. That’s the AM5 X3D CPU to get at that price,AMD,2025-09-23 23:54:34,110
AMD,nfw8dk6,Absolutely savage CPU. It makes shitty games run well-ish and good games run amazing.,AMD,2025-09-24 04:33:41,24
AMD,nfvdakn,"Been almost a year, and price only went down 10%, while demand was likely incredible.  Congratulations AMD, you found a winner  Release date    * November 7, 2024	 * US $479  Now * 23 Sept 2025 * US $429",AMD,2025-09-24 01:09:46,34
AMD,nfwndvq,"Motherboards becoming as expensive as they have, has made upgrading a lot less appealing with AM5.",AMD,2025-09-24 06:46:34,20
AMD,nfx4qyy,"cheapest its 412 euro in italy, and 605 euro for a 9070 xt reaper,   both at the price of a 5080",AMD,2025-09-24 09:43:25,3
AMD,ng2tgss,Just bought mine last weekend…I FUCKING LOVE IT. Well worth the switch from my I5 12600K,AMD,2025-09-25 05:18:47,4
AMD,nfzm6gm,aaaand its gone,AMD,2025-09-24 18:10:09,3
AMD,ng03xe1,"I mean, at least AMD is giving us a price break regardless.  They easily could've sat on the top of the pile with the $479 price but a $50 break is much appreciated, that can give a little bit of breathing room for some mid-range builds (outside the bloody graphics card)",AMD,2025-09-24 19:36:53,17
AMD,nfw94az,"Out of all the X3D chips that have released so far, the 7800x3d is still my favorite. Insane value back then, and insane value right now.",AMD,2025-09-24 04:39:34,40
AMD,nfve34r,"Ehh if you only build mobo+RAM+CPU once every 6+ years, it's better to go for 150€ more and get the good stuff.",AMD,2025-09-24 01:14:25,15
AMD,nfxv2gb,"Agreed, especially being this late into the cycle.  7800X3D also serves as a stopgap where you can swap the chip when the 10800X3D comes out.  Sounds like AM5 will be sticking around for another generation at LEAST.",AMD,2025-09-24 12:58:34,1
AMD,nfveal2,"I got one from aliexpress for $171 with discounts and cashback.  No warranty, but it works great.",AMD,2025-09-24 01:15:37,2
AMD,nfyyq7x,the x3d chips are just stupid good. 5800x3d GOAT for a dead platform that will be good for me until am6 x3ds come along when I might consider going up.,AMD,2025-09-24 16:18:32,16
AMD,nfvolfx,What's sad is I bought a 9950x for $5 more back in July.,AMD,2025-09-24 02:16:46,12
AMD,ng046bi,"Demand is still high, that's the crazy part.",AMD,2025-09-24 19:38:07,6
AMD,nfwsdh4,"I mean with $200 you're already in. I'd just avoid ASROCK at the moment :-/  Compared to price hikes in GPUs this is peanuts, especially if you factor in inflation.",AMD,2025-09-24 07:36:01,11
AMD,ng4pqtf,"As someone who bought Intel from 2600K to 7700K, yes, they could have easily sat on top of the pile. Last-gen Intel CPUs retailed for full price even after their successors were out on the market.",AMD,2025-09-25 14:13:53,6
AMD,ng0ie6x,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-09-24 20:46:59,1
AMD,nfvy4vg,Not worth the price for marginal gains at higher resolutions. 7800x3D is *that* good.,AMD,2025-09-24 03:18:57,37
AMD,nfwgu58,upgrade path is more like 2 years for many   the 9800x3d is snappier for usage vs the 9800x3d   I had the same mboard the last 4 years so upgrading from a 7800x3d to a 9800x3d is a no brainer,AMD,2025-09-24 05:45:35,-9
AMD,ng2hjm5,"I’m tempted to get this a bit now, but I might just wait until the next gen x3d chips come and hold out a little longer on my i5 12600K since it’s still good enough for everything I play.  When I get a jump I want it to be massive. I wonder how big an improvement I’d see now going from a 12600K to 9800X3D",AMD,2025-09-25 03:45:59,2
AMD,ng04c1a,"Yeah, this is a valid argument early in AM5's lifetime but DDR5 RAM prices have plummeted and most midrange MOBOs are in the $200 range.  The biggest bottleneck to enter PC gaming is now the GPUs.",AMD,2025-09-24 19:38:55,5
AMD,nfvzwvv,"Small note is iirc 9800X3D is easier to cool, because they moved the 3D v-cache. So the gains might be marginal, but it potentially has a longer life because of easier cooling.   Doesn't apply of course to people not using the CPU at full load, but it's a valid concern to solve with $120.",AMD,2025-09-24 03:31:08,28
AMD,nfwvj7e,People really upgrade every two years? Damn I'm poor. I usually keep my systems 5 to 10 years.,AMD,2025-09-24 08:08:19,15
AMD,nfw9bs1,"7800X3D uses way less power though, it's nuts how efficient that chip is.",AMD,2025-09-24 04:41:14,17
AMD,nfwzn1j,People are stupid. My millionaire uncle still has an iPhone 11. He is on the phone 7-24. Meanwhile his employers are upgrading to a new one every year with a basic minimum salary.,AMD,2025-09-24 08:50:57,11
AMD,ng3atn9,"fr. The 5800X3D was an amazing upgrade for my existing build, in place of a 3600. I'm still running it, paired with a 3080. Likely not upgrading to 9070XT/AM5 just yet for the, what, 20 to 40% uplift?",AMD,2025-09-25 08:03:06,1
AMD,nfxlpr6,Pretty sure at any given fps cap the 9800X3D will use the same or less power.,AMD,2025-09-24 12:00:43,16
AMD,nfxrc1s,"Same here. My family is full of physicians, nobody is driving new expensive cars. One Uncle is a Neurosurgeon in the US with a private practice for 4 decades. During Covid he bought a 15 year old Mercedes.",AMD,2025-09-24 12:36:29,6
AMD,nfxk1kc,Kudos to your uncle!,AMD,2025-09-24 11:49:26,2
AMD,nfzhjg6,"Man's in here acting like a millionaire on an iPhone 11 who's talking 24/7 makes sense.  The guy's time is worth enough to buy the latest iPhone with the largest battery, don't act like he's doing something amazing.  Your uncle's likely autistic as fuck and still uses the 11 because it's the last iPhone that looks good.",AMD,2025-09-24 17:47:57,-1
AMD,ng3i8pm,"Exactly, because it’s easier to cool it will happily use more power for minima gains. It’s still faster at the same power.",AMD,2025-09-25 09:19:33,7
AMD,nfztw3c,"He says he doesn't need it. Camera works fine, speed is fine, all apps working, probably he doesnt have a problem with battery as he also have work phone for email and calls.   If he needs a new phone, of course he would buy the most expensive one.",AMD,2025-09-24 18:47:42,3
AMD,ng4q8qa,"I wonder how they compare, both in eco mode, with a 60fps frame cap in a game that will easily run higher.",AMD,2025-09-25 14:16:19,3
AMD,nfmnxkb,"Tested it in Cyberpunk. Has some strengths over XeSS but also worse with RT particularly shadows (easy test spot: your apartment building boxing area), very stable image overall. Ofc big performance hit but probably worth it at least for some scenarios (rx6800 here).  Look forward to official support but reverted game version (had 2.2 modded already) & drivers, not such a big difference to be worth the extra hassle.  Given that it can maintain 30 fps w/ PT and look quite good at ~~4K (perf mode)~~ (later edit: kept switching resos & ratios, double checked and it was actually 2560x1080 not 4K for PT, apologies; still fine for consoles, but ofc not 21:9) on my rdna 2 card I wonder why they don't do a similar mode for ps5 pro and sell a lot more copies.",AMD,2025-09-22 17:01:59,125
AMD,nfmwysy,hope this gets officially released,AMD,2025-09-22 17:44:27,28
AMD,nfn7n4t,On Rog Ally FSR 4 performance is miles better than FSR3.1 quality,AMD,2025-09-22 18:35:29,32
AMD,nfndwkx,"In certain games, it does make a pretty big difference, especially where there are moving objects, such as swaying grass, hair and a lot of particles.   The performance hit is a little higher than XESS, for substantially better image quality, and stability. For those on RDNA 2 and 3, this makes a massive difference as there are only lateral upgrades available for those cards. nvidia charges about 750 usd for a card that can actually use 16 gb of VRAM (5070 ti).       So there isnt really a tangible upgrade path for those on 16 gb versions of RDNA2/3 such as 6800xt and above except the 4090 or 5090.",AMD,2025-09-22 19:08:34,19
AMD,nfn8mln,"FSR Redstone can't come soon enough, even if performance and quality gains are minimal it working on everything in an official capacity will be a godsend.    10-20% performance hit is normal IMO, Turing cards get hit hard when running DLSS 4",AMD,2025-09-22 18:40:22,23
AMD,nfmkad7,"FSR 4 with INT8 has significantly worse image quality and more artifacts than regular FSR 4 with FP8, and the performance hit on RX 6000 and 7000 is quite big as the title says.  It is better than FSR 3 though.",AMD,2025-09-22 16:45:05,65
AMD,nfqn6pn,AMD! RELEASE FSR4 SUPPORT FOR RDNA 2 AND MY LIFE IS YOURS!,AMD,2025-09-23 09:49:04,6
AMD,nfn7e57,"Either I did something wrong, or a 6959 XT acts different. I did the thing with replacing two DLL files in the newest installer pack with older files. In BL4 I get 70-100 fps with FSR and it looks way better. Before FSR was visual pain and only TSR worked somewhat fine. Tested no other Games so far, BL4 is hooking lol",AMD,2025-09-22 18:34:13,1
AMD,nfou9h7,I’ve been trying this version on my steam deck and Control looks so much better,AMD,2025-09-23 00:19:08,1
AMD,nfp52qq,Planet Coaster 2 has an FSR option but I have no idea how to set it up or adjust it.,AMD,2025-09-23 01:32:02,1
AMD,nfq9o7s,I assume this can only be tested in games that already have the DLL present so it can be overwritten?,AMD,2025-09-23 07:15:10,1
AMD,nfqqwqp,"i tried it on dying light the beast on 6900XT, and its bad to be honest real bad, lots of ghosting and blur and terrible reflections and little performance gain, which fsr3 that game offers doesnt have obviously its because not being implemented officially. though if you tried it and had diffrent outcome please tell me how.",AMD,2025-09-23 10:24:55,1
AMD,nfqt6it,"I appreciate the fact that we no longer have to deal with bad AA's but im not very excited by this and people are overhyping this ihmo.   The reason i changed my 6800 was mostly performance, so i don't really see how this should be THAT beneficial if i have to play at low medium settings with barely any RT anyways.",AMD,2025-09-23 10:44:20,1
AMD,nfqzkgn,This means there's a chance the RX 7900 XTX gets FSR4 maybe officially?,AMD,2025-09-23 11:33:18,1
AMD,nfrjrda,we need a tutorial on how to implement all this.,AMD,2025-09-23 13:36:00,1
AMD,nfzpd67,"I finally got FSR 4 w/ Optiscaler working. I also did the 23.9.1 driver .dlls copy to 25.9.2. to fix shimmering, etc. I tested The Forever Winter. Amazing results. In my opinion, FSR 3 already looks good in-game, but FSR 4 made it look near-native. GPU power draw only went up 15 W, but temps (40c) and hotspot (60c) stayed the same. But I didn't test fps, I had capped at 60 with AFMF 2 for 120 fps, no microstutters or frame drops. My only issue though is after leaving the game, my deskop gets very slugglish. My guess is the modded 25.9.2 driver. In-game is fine, but going back to the desktop is pretty bad lol. Resetting the PC fixes it, but playing the game again causes the desktop feel very sluggish. Tried DDU, still didn't help. It was nice to try, regardless.",AMD,2025-09-24 18:25:38,1
AMD,ng2brh9,Tried FSR4 on my wife's XTX with witcher 3. It's incredible. Far better than XESS at least on witcher 3,AMD,2025-09-25 03:05:40,1
AMD,ng785ch,"Using it in KD2, Horizon Zero Dawn, Control, Hogwarts on my rx6800 and it looks great. Need to use performance mode at 4K.",AMD,2025-09-25 21:29:41,1
AMD,ngkhqum,I can't seem to find a link to download Compiled Int 8 FSR dll,AMD,2025-09-28 00:16:02,1
AMD,ngq8s1a,"Now I'm playing STALKER 2 on RX 6950 XT in 1440p with FSR 4 Quality.  Please AMD, make FSR 4 official with future driver updates!",AMD,2025-09-28 22:15:38,1
AMD,nfms2ih,"at 10-20% performance hit, is fsr4 really worth it over 3.1? i think at that point, i would rather have somewhat lower fsr quality.",AMD,2025-09-22 17:21:29,0
AMD,nfodm4y,I thought the point of upscalers was to lower the render quality then upscaling it to get better performance,AMD,2025-09-22 22:30:49,1
AMD,nfn8cwl,but no one could get the Arc version of XeSS to work on RDNA3,AMD,2025-09-22 18:39:00,0
AMD,nfq2pjy,"20% performance hit means 60% fps gain only, with added latency. Not very good.",AMD,2025-09-23 06:00:47,-2
AMD,nfqsihy,I need bigger uplift not downlift BRUH,AMD,2025-09-23 10:38:51,-2
AMD,nfn4nz3,Really disappointed with fsr4. I tried it in cyberpunk on my 7900xt. It still has worse shimmering than xess2. Which is crazy.,AMD,2025-09-22 18:20:53,-13
AMD,nfpmg9h,i have the same gpu how did you maintain 30fps with PT,AMD,2025-09-23 03:32:35,17
AMD,nfqgzgv,"I have a 6800 too, getting 60\~ with RT reflections at 1440p and FSR4 performance  nearly 120fps with framegen  [https://www.reddit.com/r/Amd/comments/1nibdtj/comment/nf7pjzc/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/Amd/comments/1nibdtj/comment/nf7pjzc/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)  (dunno why I was downvoted for sharing lol)  Didn't notice issues with shadows...  The quality of the upscaling outweighs the narrower framerate uplift, AMD could ship it in this preliminary state and it would already immensely benefit RDNA2 users  I guess even down to 6700XT we'd still benefit  Not sure it's okay for lesser models though (6700, 6650XT, 6600XT, 6600), because the INT8 TOPS drop sharply, they might be limited to performance mode at best and not always get an uplift  Well, the perf mode already looks better than FSR3 and XeSS anyway :)",AMD,2025-09-23 08:39:08,15
AMD,nfoo2h0,PS5 Pro will be receiving FSR4 enhanced PSSR next year,AMD,2025-09-22 23:38:44,3
AMD,nfq8c3b,such old gpu isnt made for RT,AMD,2025-09-23 07:00:21,-12
AMD,nft6ef2,I feel like the leak was intentional. It allows the community to develop an implementation to bring FSR4 to older hardware without AMD taking on the responsibility for an official release.,AMD,2025-09-23 18:17:31,8
AMD,ng1dar9,It works great on some games in others not. The only game I tested fsr4 mod to work well is on dying light the beast. It has native fsr4 built in. I tested fsr4 on spiderman remastered and graphical glitches occured because of using the old driver 23.9.1,AMD,2025-09-24 23:37:10,1
AMD,ngfngpd,> Turing cards get hit hard when running DLSS 4  Definitely not true  You might be referring to Ray Reconstruction but you should've stated so because that's not the same as the main Upscaler component,AMD,2025-09-27 05:32:22,1
AMD,nfmnali,"the difference is night and day, the weird shimmering around objects just disappear with frs 4 while on frs 3 it's god awful, Honestly I would take performance fsr 4 over quality frs 3.",AMD,2025-09-22 16:59:01,84
AMD,nfmyv05,"I believe the leaked int8 model was also a fair bit older than what is currently used in the released fsr4, so it may not be directly comparable.",AMD,2025-09-22 17:53:13,24
AMD,nfnewq4,from what I've seen this FSR4 mod performance cost is more or less equivalent to XeSS running on non-Intel GPU while having better image quality,AMD,2025-09-22 19:14:07,6
AMD,nfqtmay,"I am quite happy with performance loss of about 7 % with my 7800xt compared to Fsr3. Tried it on Beast, difference is sky and earth.",AMD,2025-09-23 10:47:55,3
AMD,nfn08lk,"I think part of it has to do with the fact that the INT8 dll everyone is using only has Model 0. FSR Models, which are like DLSS Presets, are the settings that the algorithm uses like frame accumulation, etc. and ""Model 0"" is best tuned for Native/Ultra Quality settings.",AMD,2025-09-22 17:59:39,5
AMD,nfncuj1,Don't artifacts only appear in RDNA2?,AMD,2025-09-22 19:02:45,2
AMD,nfn93us,I'm only targeting 1080p resolution and I'm using a 7800XT will it tank my card using fs4 below 60 if playing cp2077,AMD,2025-09-22 18:42:50,4
AMD,nfo8wr0,It's not that much different from FP8 in quality,AMD,2025-09-22 22:01:03,1
AMD,ng40ymv,"I have a 9070XT and a 7800XT using FSR 4 with OptiScaler, and I can't see any differences between FSR 4 INT8 and FP8, but compared to FSR 3, the difference is really big.    Are you sure it has significantly worse image quality?",AMD,2025-09-25 11:54:56,1
AMD,nfroutx,"If you are on a newer driver for RDNA 2, you have to downgrade to 23.9.1 driver.  It sucks having to use a 2 year old driver, but it fixes all of the ghosting issues.",AMD,2025-09-23 14:02:51,2
AMD,nfrtxhx,"Yes, higher possibility than Rx 6000s.",AMD,2025-09-23 14:28:06,1
AMD,nfrp1hc,https://www.reddit.com/r/pcmasterrace/s/ZZmJzL1NuZ,AMD,2025-09-23 14:03:47,1
AMD,nfmvo1r,"I think its compared to the performance of FSR3, not native.  It's still better frame rates than native.",AMD,2025-09-22 17:38:28,28
AMD,nfn0y2y,The comparison is FSR 4 vs FSR 3 at the same quality level. FSR 4 performance mode will look better than FSR 3 native while clawing back some of the lost FPS. This is not as black and white you think it is.,AMD,2025-09-22 18:03:00,24
AMD,nfn2im3,"In my testing, FSR 4 is only slightly faster than the next higher tier of FSR 3 but it looks **significantly** better. So FSR 4 Balanced performs ~= FSR 3 Quality, but it looks almost as good as Native with some minor shimmer on very thin objects.",AMD,2025-09-22 18:10:32,8
AMD,nfocjdf,"its only comparing resolution scale percents. with FSR4, you can afford to use FSR4 Balanced over FSR3 Quality because there are things FSR4 properly renders that FSR3 outright cannot do, which mostly revolves around transparencies and thin object stability.  Thats why if you are going to do this, you should compare FSR4 Balanced to FSR3 Quality/Ultra Quality to see which one you like more. Some games might favor one over another. iirc for example with Ancient Gameplays tests, Black Myth Wu Kong will heavily prefer FSR4 regardless of setting because FSR3 can't handle the transparencies of the attacks, waterfalls, and leaf backdrops (similar to Ratchet and Clank intro scene that Hardware Unboxed tests). In a game like Marvel Rivals, FSR4 is marginal, so you might prefer getting the FPS that FSR3 provides over it.",AMD,2025-09-22 22:23:53,3
AMD,nfotqzg,Yes night and day visual difference,AMD,2025-09-23 00:15:34,1
AMD,nfrjdwd,"at that point, just disable FSR. Not sure how people are missing this.",AMD,2025-09-23 13:33:54,0
AMD,nfog2i2,It does increase performance over native quality.,AMD,2025-09-22 22:46:48,9
AMD,nfqkc08,"Upscalers feature settings and tweaks offering various levels of picture-quality-performance tradeoffs  They're not exclusively about increasing the FPS, but finding the best middleground for your needs and satisfaction  Good upscalers like DLSS4 and FSR4 depending on the settings go further  They can actually 'clean up' the often terrible native output that features forced TAA  So yeah it's not just purely about boosting FPS, it's a bit bigger deal than that, it's expanding video games specifics into the realm of video and photo processing (which have been longtime precursors, both in upscaling and frame interpolation)  Especially now that upscalers have become nearly unavoidable for AAA games and/or higher settings and resolutions displays anyway, like it or not",AMD,2025-09-23 09:17:31,2
AMD,nfqhv89,Render scaling not quality. It just so happens scaling down lowers quality.,AMD,2025-09-23 08:49:19,1
AMD,nfqmbnt,"this might be an issue due to driver version, similar to what he describes at the beginning of this video: [https://www.youtube.com/watch?v=gTrfnLvZbu4](https://www.youtube.com/watch?v=gTrfnLvZbu4)",AMD,2025-09-23 09:39:40,6
AMD,nfqedk7,"Just cap to 30. This was on vanilla 2.31 version of the game, but you can further tweak it for performance with mods like ultra plus ray tracing. Keep in mind that it's not really worth playing the game like that and with PT it is very crash prone. For us RT+PT from the same mod is much better (basically it adds extra bounce to RT GI so it looks closer to PT, but way more performant).",AMD,2025-09-23 08:08:44,9
AMD,nfytw5c,Would you reckon it's way more possible it comes to rdna3 like the 7000 series. Would love to see it come to my 7800xt so I don't need to upgrade my gpu again 😅,AMD,2025-09-24 15:55:23,3
AMD,nfrf3np,"It's RT shadows (all lighting settings turned on) and in this case it was with ceiling grate shadows. Curiously it also happened with FSR 2 & 3 but not XeSS. Test there if you can maybe it was only happening for me (boxing area your apartment building, check floor for hexagonal shadows see if it fizzles).  Tbh it's actually good for all GPUs, even steam deck, because you'd want to run it for quality and are already at lower fps = more frametime available, so it's easy to stomach the performance cost trade-off. Where it's hard to use is for higher fps >60 because frametime budget is so tight the fixed cost becomes enormous, even on 7900 XTX. You need RDNA 4 for that scenario.",AMD,2025-09-23 13:09:52,2
AMD,nfszo0q,"as some one with an rx 6650 xt, its still worth it with the better image quality. and sill usable with it being in my testing around 20% worse than xess.",AMD,2025-09-23 17:45:34,2
AMD,nfqehq2,Works well enough for me.,AMD,2025-09-23 08:10:04,7
AMD,ngi4mwd,"I mean they could just add it to Radeon Sofware as flip switch (disabled by default) with a warning, that it will have performance hit on older cards compared to FSR3. FSR4 is visually superior and still provides gains over native. Can't see where's the problem to support it officially.",AMD,2025-09-27 16:29:43,2
AMD,ng308s4,"I'm hopeful that things will improve rapidly, and that we get something official from AMD soon. I only used it in Linux (tried it when the driver trick wasn't known) on the Stellar Blade demo and it worked great.",AMD,2025-09-25 06:19:25,1
AMD,nfmno2o,"FSR 3 is unusable, I agree",AMD,2025-09-22 17:00:45,37
AMD,nfn42sz,not in alan wake 2,AMD,2025-09-22 18:18:02,5
AMD,nfrj09v,the best performance and image quality boost you can get is entirely disabling FSR then.,AMD,2025-09-23 13:31:47,1
AMD,nfqvqf8,Now imagine how good DLSS 4 is and then you understand why upscaling is so important and why a 4070 Super with DLSS 4 Quality gives you a similar experience at 4k as the 7900 XTX  Edit: Lol. You say something good about AMD: upvotes. You take the same exact logic and say something good about Nvidia: downvotes,AMD,2025-09-23 11:04:43,2
AMD,nfoca71,I just tested the int8 dll on my 9070XT and it has all 5 models showing in Optiscaler. Even changes model depending on Quality setting.,AMD,2025-09-22 22:22:14,6
AMD,nfobthk,RDNA2 particularly artifacts if you use any driver that released from RDNA3 launch and onward. It implies internally something changed in the driver that borked rdna2 from using it if you use any modern driver.,AMD,2025-09-22 22:19:16,5
AMD,nfnr00a,Isn't it the same FSR4 INT8 model regardless of which GPU it runs on?,AMD,2025-09-22 20:19:57,2
AMD,nfnqtre,I meant you lose a little fps vs running FSR 3. You still get a little more than running native.,AMD,2025-09-22 20:19:01,6
AMD,nfnlg14,No,AMD,2025-09-22 19:50:28,8
AMD,nfoa03u,"someone shared a really nice comparison picture with me:   [https://imgsli.com/NDE2MDg1](https://imgsli.com/NDE2MDg1)  (if they look the same, it's bugged and try a different browser)",AMD,2025-09-22 22:07:52,1
AMD,ng4etvz,I am sorry to hear about your blindness,AMD,2025-09-25 13:17:52,2
AMD,nfrwod0,"""but it fixes all of the ghosting issues""  with fsr4 or you mean in general?",AMD,2025-09-23 14:41:37,1
AMD,nfn7md5,"i only upgraded to a 9070xt a few weeks ago. before that, i was using a 5700xt sind 2019.   please believe me if i say: i know how low quality fsr looks and looked like in games XD  i always found it acceptable and worth the performance increase. of course. its different with a faster card and more options.    well. you guys are in a better spot then me to judge this.",AMD,2025-09-22 18:35:22,1
AMD,nfvummz,Someone dropped a modded driver that eliminates that shimmer. This is why I love the AMD community lol.,AMD,2025-09-24 02:55:19,1
AMD,nfz7q8x,"Yeah it's almost certainly coming to 7000 series, otherwise people will riot  Not certain for 6000 though...  AMD probably want to polish their image (pun intended), but they also want to sell GPUs, I guess...",AMD,2025-09-24 17:01:32,1
AMD,nft069a,"I think it's just that Cyberpunk has a shitty FSR implementation, and since FSR 4 basically hijacks FSR 3 inputs, it's also affected",AMD,2025-09-23 17:47:55,2
AMD,nfqf9wn,glad you enjoy your 30fps blurry image.,AMD,2025-09-23 08:19:11,-16
AMD,nfr3i27,The 7800XT comes with instructions that specifically accelerate this kind of workload as compared to RDNA 2; not really sure what you’re on about.,AMD,2025-09-23 12:00:02,2
AMD,nfqr41g,yes a gpu from 2020 is old. and amd never had a good gpu. 6800xt was a mid tier gpu that couldnt compete with the 3080.   the 6800xt needs the blurry fsr upscaler above 1080p to get 60fps+    you need and cant have reason to pretend amd is in any good spot.,AMD,2025-09-23 10:26:43,-15
AMD,ngoaaq3,"Anything you support officially is always your problem.    Having it off by default and having a warning that it's an experimental feature isn't a bad idea. You're still gonna have people dinging your support because something in your driver that you officially released is causing issues, or isn't working properly. It's just the way things are.   It could also be that they wanted to quietly gauge the community's response to the lower performance of the INT8 model before officially supporting it. If they had released it officially, and the community had an overwhelmingly negative response to the performance, they would face a good deal of backlash. This way they can make the release seem ""accidental"", which avoids the community backlash in the event of a negative reception, while allowing people to test it and determine the interest and acceptance of it.",AMD,2025-09-28 16:36:13,3
AMD,ng30fer,Depends on amd. Because fsr4 is one of their main selling point for the 9000 series. But if nvidia can supportd dlss4 on older carss amd should too to be competitive.,AMD,2025-09-25 06:21:06,2
AMD,nfmzjsp,"To be honest, FSR 3 is only good for low end hardware that cant run the game otherwise. If a game runs at 35fps native but 60 with fsr balanced, then shimmering is 100% worth it. For example I game on integrated 780m GPU, and many games run native 20-25 FPS, but with FSR performance they reach to 60fps, thus making them playable.  But if it already runs at 80 and you push it to 100 with FSR 3 then it's not worth it.  Furthermore FSR 3 quality is absolutely the worst preset ever, problem isnt the lower upscaled resolution but the shimmering, jitteriness and fuzziness. So it is better to run balanced or performance preset  to get FPS boost to justify such downgrade in visuals.  DLSS or FSR4 is on the other hand gives almost free FPS with quality and balanced presets, assuming you are playing on at least 1440p resolution, there is absolutely no reason to not use them to get higher FPS.",AMD,2025-09-22 17:56:25,25
AMD,nfw5sp0,"The fact you got this comment upvoted on this subreddit in 2025 is absolutely hilarious.  In the past some of the narratives spun on this subreddit have been:  > ""FSR1 isn't that much worse than DLSS anyway and it works on all GPUs so it's okay that the developer didn't add DLSS""  > ""FSR2 isn't that much worse than DLSS anyway and it works on all GPUs so it's okay that the developer didn't add DLSS""  > ""FSR3 isn't that much worse than DLSS anyway and it works on all GPUs so it's okay that the developer didn't add DLSS""  Today?  >""FSR 3 is unusable""",AMD,2025-09-24 04:14:01,3
AMD,nfmnx9x,At what point do you just fall back to dp4a XESS though? That still seems like the best middle ground for the older rdna cards,AMD,2025-09-22 17:01:57,1
AMD,nfocus4,"I'm on an XTX, and it's always 0. Dunno if both INT8 and FP8 are on the .dll, and it switches to FP8 with all of its models if you have RDNA4. it's almost twice as big as ""official"" releases after all",AMD,2025-09-22 22:25:56,2
AMD,nfodfez,you have to set the right model for given resolution.  I don't need that I play game with it as I have both 7900xtx and 9070,AMD,2025-09-22 22:29:35,0
AMD,ng4xjsx,"I'm assuming you have a very trained eye, and you also have an RDNA 3 and RDNA 4 card to do side-by-side comparisons.    Correct?",AMD,2025-09-25 14:51:24,1
AMD,nfrzldk,With FSR4,AMD,2025-09-23 14:55:39,3
AMD,nfz8vwf,I'd just buy a gtx card if it wasn't scalped and msrp'd at 2k usd,AMD,2025-09-24 17:07:04,1
AMD,nfqfdlp,"That is with PT, RT is 60 and higher res.",AMD,2025-09-23 08:20:23,6
AMD,nfqsqlb,"your user name..  What on earth are you on about? The 6800XT was a really good card, and still relevant. Its AMDs 1080ti by the looks of it. There is no upgrade thats really worth it if you're sitting on one.",AMD,2025-09-23 10:40:42,12
AMD,nfrtcx3,"Lmao.  With all due disrespect, sit down and shut up.",AMD,2025-09-23 14:25:17,2
AMD,ngpusl2,"I mean people whine about all sorts of issues all the time - high power draw on dual monitors at idle was problem for so long. I think RDNA1 was plagued by some blackouts, etc.. Having something optional that runs not as good as on RDNA 4 is no biggie. I mean people could also whine they can't OC by X offset or undervolt by Y offset, yet they give a ton of tools within their software hidden behind small warning",AMD,2025-09-28 21:03:35,1
AMD,nfn69n5,If you’re on a steam deck the difference is night and day.,AMD,2025-09-22 18:28:40,6
AMD,nfyu4og,Better than fsr 2.2,AMD,2025-09-24 15:56:30,1
AMD,nfqhxh7,The funniest thing was that FSR 3 was considered pretty good right until FSR 4 dropped. Then people said it was shitty lol. Same with DLSS 3.,AMD,2025-09-23 08:50:01,2
AMD,ng6u8u1,The DLSS that was used when FSR1 came out isn't the same DLSS that exists now. Both technologies advanced.,AMD,2025-09-25 20:20:23,1
AMD,nfxchyr,"The cope is insanely strong with AMD fanboys (of which there are for sure more than Nvidia fanboys on reddit).  FSR 3 and 2 was always trash compared to DLSS and basically unusable due to temporal instability and lack of detail. But they couldn't admit it, because it was all they had and not having competitive upscaling is a huge disadvantage.   But now that there is FSR 4, all of a sudden it's easier to admit that FSR 3 was trash all along.",AMD,2025-09-24 10:53:09,-3
AMD,nfmp5md,"For RDNA2? DP4a can only ever look like a reasonable alternative when stationary, in motion INT8 FSR4 looks leagues better.  For RDNA3 it's an actual no-brainer, XeSS isn't even measurably faster in all cases.",AMD,2025-09-22 17:07:43,7
AMD,nfox56q,"One of the first DLLs that floated around for the int8 FSR4 has a broken model selection, but it should work fine with the ones people are using now. If you select default on optiscaler you should see it switching when changing presets.",AMD,2025-09-23 00:38:45,5
AMD,ng0ndie,Gtx is over since the glorious 1080ti,AMD,2025-09-24 21:12:07,1
AMD,nfn6pfe,"Yes, this is what I mean. FSR 3 or any upscaler actually is a god sent for low end hardware. But if someone has 7800XT or 7900XTX or something similar, I hardly recommend FSR 3.2.  Alongside having igpu PC, I also have 7800XT main gaming PC, and more often than not I regret turning on FSR3 to push 120fps to 144fps max refresh rate of monitor. I use lossless scaling or smooth motion instead.  For example on The last of us part 1, as soon as I turn on FSR 3.2, Joels hair gets all fuzzy. The fences become blurry, the power lines become jittery, the reflections becomes fuzzy, all this sacrefice to get 20 fps more? Not worth it.   But I will accept all of these on low end hardware.",AMD,2025-09-22 18:30:49,5
AMD,nfrf8y4,"Not really. Back when upscaling became a thing, Both FSR and DLSS were considered ""okay"". Then DLSS improved so much that FSR3 started to be considered really bad, way before FSR4 premiered with 9000 series.",AMD,2025-09-23 13:10:42,5
AMD,ng9tqf8,DLSS 2.5 is older than FSR3 and is the basis of all non DLSS 4 image quality,AMD,2025-09-26 08:29:54,2
AMD,nfmpczn,Yeah I imagine 7000 series probably is definitely worth it. But I’ve heard it’s pretty bad on the 6000 series yeah.,AMD,2025-09-22 17:08:42,0
AMD,nfoxd5t,Interesting. I'm using the Limewire-hosted dll from a week ago. I guess I'll look for an updated one.,AMD,2025-09-23 00:40:13,1
AMD,ng0nhf1,"Gtx isn't 20, 30, 40 and gtx 50 series? Edit: ah yes it's now rtx",AMD,2025-09-24 21:12:41,1
AMD,nfyuhor,"I have a 7800xt and tested it just now. Are you sure it's not on your end? My joels hair, fences power lines ect do not shimmer pretty much at all.",AMD,2025-09-24 15:58:11,2
AMD,nfmqa21,On Windows it is currently due to a driver regression. 23.9.1 and prior drivers render INT8 FSR4 properly on RDNA2.,AMD,2025-09-22 17:13:04,1
AMD,nfoxv5h,If you're using the limewire one it should work. Not sure what the issue is then,AMD,2025-09-23 00:43:32,2
AMD,ng0nks5,Its RTX nowdays,AMD,2025-09-24 21:13:10,1
AMD,nfmsh7f,So that’s on Linux yeah? Will windows be able to emulate it in the future you think?,AMD,2025-09-22 17:23:27,1
AMD,nfozqrb,"Yeah, it's inop for me. Limewire 4.0.2 dll on OptiScaler 0.7.9 with Dukem's FG and FakeNVAPI. Models aren't switching either automatically nor manually.",AMD,2025-09-23 00:56:00,1
AMD,nfmw13b,"No... that's the naming scheme for all of AMD's Windows drivers. People have tested FSR4 INT8 on drivers from 2 years ago and earlier and it works fine. 23.9.2 introduces some sort of change somewhere in the DX12 driver that breaks INT8 FSR4 (unintentionally, as INT8 FSR4 wasn't even a thing at that time) and it was never fixed again after that.",AMD,2025-09-22 17:40:08,1
AMD,ngq1y19,Great video. It now explains exactly why AMD chiplet processors up until now have had much higher idle power (20-40 watts) compared to intel processors and even AMD's APUs. The lower idle power (6-12 watts) is confirmed by [this recent review of Strix Halo](https://youtu.be/BTIUL-yaY4s?feature=shared&t=425).,AMD,2025-09-28 21:39:35,223
AMD,ngprtbf,"Another quality High Yield video!   Really amazing tech, but my question is how much will the cost of the organic RDL impact Zen 6 prices.",AMD,2025-09-28 20:48:55,110
AMD,ngpyjnq,"It's such a shame the single CCD chips couldn't use both SERDES, 9800X3D would be even better. I suspect it's because they didn't want to have two different substrates for the consumer platform.",AMD,2025-09-28 21:22:22,71
AMD,ngqzkfv,Hoped someone would post this yesterday. Glad to see people are checking it out. Def a bit more speculation than what we know for sure still rather interesting,AMD,2025-09-29 00:52:50,21
AMD,ngrrxr5,"Sometimes I wonder who came up with crazy designs like this. Is AMD coming up with this and tells TSMC to make it, or TSMC made it and markets it to everyone? If it's the latter, can other parties, say, Intel and NVIDIA use it too?",AMD,2025-09-29 03:46:58,14
AMD,ngqw3ow,"I actually didn’t know that Strix Halo was using this design and just assumed it’s a monolithic chip. Now that AMD is doing chiplets Intel-style and it might get applied to Zen 6 and beyond, there should be an immediate gain from this.",AMD,2025-09-29 00:32:02,24
AMD,ngt11mu,This news together with last weeks AMD's patent on doubling DDR5's bandwidth. What I do wonder is can these features for the next Ryzen be implemented using the current AM5 (lga1718) socket? Or does this perhaps indicate Zen6 will make the jump to AM6 (lga2100)?,AMD,2025-09-29 10:45:43,4
AMD,nguadpf,Ive been disconnected the last few years from hardware news. Did AMD ever implement chiplets on their GPUs like they had planned? and did it work out like it did for their CPUs?,AMD,2025-09-29 15:18:50,2
AMD,ngr5301,"My guess is that for EPYCs AMD will either make one very large IO die or perhaps ""glue"" together a few (maybe even with the old serial links,) just so that they find enough space for the chiplets.  I wouldn't be surprised if in the long run core chiplets were to become quite a bit less square just to retain density as well.",AMD,2025-09-29 01:24:49,1
AMD,nh2skgf,why didn’t they call it Next Zen…,AMD,2025-09-30 21:11:28,1
AMD,ngqzl1q,"Can already see AMD patent every option just to make it harder competition to compete, honestly think patents are stupid however if AMD does not patent it some one else will and they just be harming them self, so its not really AMD fault either.",AMD,2025-09-29 00:52:55,-8
AMD,ngr86sb,I thought MLID reported that zen 6 was going to use bridge dies. Will be interested to see which they go with,AMD,2025-09-29 01:43:24,-5
AMD,ngrsdyy,"Is the same tech being used for the ""lower"" AI 300 series? If so, I wonder how they plan to up their APU game, seeing as Lunar Lake has seemingly caught up.",AMD,2025-09-29 03:50:08,-1
AMD,ngqlm2r,"To add some detail, the very nature of SERDEs (serialize/deserialize) requires very high switching speeds to get full bitstrings from A to B one bit at a time.  Look at the size of the silicon that was eliminated.  Consider that in the remaining silicon, bitstrings move in parallel over more wires at much lower frequencies.  There's the power savings.",AMD,2025-09-28 23:28:02,80
AMD,ngxyy83,Any word on PCI lanes and memory lanes? 40 lanes and support for 8 sticks of DDR5 would blow up the local LLM world,AMD,2025-09-30 02:48:46,5
AMD,ngr78e1,i dont mind the higher idle power of amd stuff keeps my room warm with a 100w idle heater lol,AMD,2025-09-29 01:37:45,-72
AMD,ngq1fk1,"Might be a bit higher but remember, amd always prices high at the beginning, then drop prices months later",AMD,2025-09-28 21:36:56,39
AMD,ngrptev,"Since the IOD only has a 32B read/write connection out of the memory controller and a single link can do 32+16B, there would be very little benefit. The CCD would get 32+32 instead of 32+16.  This is not the case on threadripper/epyc because their IOD has a connection to the memory controller which is 64B+ wide, so using a second link allows them to deliver 64+32 to one CCD.  They need to get rid of the ridiculous infinity fabric bottleneck within the consumer IOD which can only move data half as fast as DDR5 peak bandwidth (2000fclk = 64GB/s read and write max, while dual-channel DDR5 8000 can deliver 128GB/s read).",AMD,2025-09-29 03:32:24,33
AMD,ngr7d88,sounds interesting i guess its something their keeping in the bank for zen 6/7 ?,AMD,2025-09-29 01:38:34,-9
AMD,ngrfagu,"I tried posting it, but it got auto-deleted because I included both titles in the title. ¯\_(ツ)_/¯   (On the Android app it was titled one way; by the time I opened it in Chrome to copy the title, the title had changed to another way.)",AMD,2025-09-29 02:24:37,7
AMD,ngtoncv,"These advanced 2.5D and 3D packaging technologies are developed by companies most people on Reddit have never heard of (e.g. Invensas and Ziptronix, both acquired by Tessera Technologies).  Intel, AMD or TSMC then like to showcase the results of them licensing this stuff like it's some kind of exclusive in-house effort.",AMD,2025-09-29 13:25:48,20
AMD,ngs91gh,They work so closely that there would be a lot of collaboration. Example is stacked cache. I'm sure it's evolving due to adoption despite it being introduced as tech by tsmc if I'm not mistaken.,AMD,2025-09-29 06:05:49,12
AMD,ngsew1x,"I think AMD designs everything themselves. TSMC just makes it for them. I believe how it works is, TSMC gives them a spec sheet of what they are capable of making. Then AMD designs their chips within the tolerances specified in the spec sheet.",AMD,2025-09-29 07:01:57,10
AMD,ngt79et,"I think this is tsmc tech   ""This is done through TSMC's InFO-oS (Integrated Fan-Out on Substrate) along with a redistribution layer (RDL)""",AMD,2025-09-29 11:35:11,7
AMD,nh0r87b,"die stacking is generally the tech of whoever does the packaging, there might be some cross-licensing going on though, and of course things are more complicated than just having the capability to connect dies",AMD,2025-09-30 15:18:38,1
AMD,ngqzzxd,"AMD pioneered chiplets lol, Intel is the one making their chips AMD style lmao.",AMD,2025-09-29 00:55:20,27
AMD,ngx3hau,"These changes have nothing to do with the socket. It's all about reducing latency penalties for internal communications across the CPU itself. AM6 will come out whenever AMD wants to adopt DDR6 and PCIe 6, which for them is typically around 12 months after they first become available in consumer hardware. Zen 6 next year is going to be on AM5 given AMD's recent [comments](https://www.pcgamer.com/hardware/processors/amd-promises-am5-socket-life-support-through-to-2027-and-beyond/), the question is whether Zen 7 will.",AMD,2025-09-29 23:42:34,5
AMD,ngv6txu,"Yes, they did with RDNA3 but went back to monolithic with RDNA4. RDNA3 used multiple MCD’s connected to one GPD. UDNA/RDNA5 will be chiplet again but in a different way, one GPD connected to an IOD of some kind.",AMD,2025-09-29 17:54:26,11
AMD,ngvcb1f,I suppose you could even call the Vega 64 a chiplet GPU since it has HBM on package.,AMD,2025-09-29 18:20:28,4
AMD,ngx57im,"They implemented chiplets on RDNA3, but it didn't work out as planned with power efficiency or cost due to the packaging techniques they ended up going with, hence why RDNA4 is monolithic. RDNA3 would have been greatly helped by backside power delivery to overcome some of the power efficiency losses introduced by the packaging, which was not available at the time but will be available for future chiplet architectures.",AMD,2025-09-29 23:52:26,3
AMD,nh0sq2i,"They haven't been able to split up the actual graphics logics yet, though on the server side they have managed to split the compute to multiple dies, IO is also separated (both for MI300 and RDNA3). So far the tech doesn't seem cost effective enough to bring to low end or mid range GPUs, but we could see it for high end at some point.",AMD,2025-09-30 15:25:55,1
AMD,ngv2xkb,"Not yet, supposed to come with thier UDNA arch, maybe UDNA2",AMD,2025-09-29 17:36:20,-1
AMD,ngw9enl,"dont mention moore laws, redditors hate this guy.",AMD,2025-09-29 20:59:14,2
AMD,ngt22mn,MLID is an idiot.,AMD,2025-09-29 10:54:22,2
AMD,ngs0m13,"No need for the entry level stuff, its monolithic.",AMD,2025-09-29 04:50:43,4
AMD,ngyw8lk,"what we would give for more RAM channels. Hell,  do quad channel on 4 sides of the CPU for identical trace length.....",AMD,2025-09-30 07:22:39,5
AMD,ngrxev5,as a heater intel actually can help you more,AMD,2025-09-29 04:25:15,50
AMD,ngwhsbf,I’d much rather have the SerDes’s power budget available for compute performance,AMD,2025-09-29 21:41:45,2
AMD,ngr3wg4,"It’s in line with silicon die economics. First batches always have lower yields than later batches, because engineering has time to work out the kinks. Wafers are in the tens of thousands of dollars, so a few extra chips that make it can significantly reduce the overall price.",AMD,2025-09-29 01:18:11,16
AMD,ngs8o1z,"I think the IF limitations will be eliminated or at least started to be fixed with this move, having a much wider bus at lower power will give the engineers more freedom to upgrade IF to make use of this technology.",AMD,2025-09-29 06:02:20,11
AMD,ngsl3lb,Hmm. Where does the extra memory bandwith comes from with dual CCDs then?   Shouldnt a single ccd with both links enabled equal dual ccd with single links?,AMD,2025-09-29 08:05:37,1
AMD,ngsz493,"With a partnership at close as TSMC and AMD, there would undoubtedly be a few TSMC engineers embedded in AMDs projects, especially those like Zen 6 which are the first to show off a new node. TSMC is just as invested in the first impression of their product as AMD is in theirs.",AMD,2025-09-29 10:28:28,9
AMD,ngr1d08,"Nah. The ‘tile design’ is something that Intel had in the lab for the longest time, it’s just that AMD brought a much simpler (cruder?) design way earlier to the market to cut production cost.  What Intel has been doing has advantages vs what AMD has been doing, with reduced latency and monolithic level idle power consumption. But it’s more expensive and harder to both design and implement. See Intel chip breakdown and it’s basically a freakin Tetris, every single segment divided into different chips and sometimes even produced by two different fabs (IFS/TSMC)  Even with Strix Halo, AMD approach to chiplet is more conservative but at the same time should be easier to implement. AMD tends to dump everything that is not core CPU logic into an IO chip made on a cheaper node, and over the last few generations they’ve just gotten extremely good at this. No need for Intel Tetris whatsoever, just a simple connection between CPU core and IO core and that’s it. And now without the interposer adding a gap between everything, with all the advantages Intel should have had. So it’s pretty neat.",AMD,2025-09-29 01:03:25,29
AMD,nh44pr0,Isn't zen 6 coming to AM5? You think they'll make it compatible with both DDR5 and DDR6? Feels like we barely started in the DDR5 era,AMD,2025-10-01 01:45:08,1
AMD,ngwam90,noted lol.,AMD,2025-09-29 21:05:13,1
AMD,ngv3b62,he called strix halo 2 years away,AMD,2025-09-29 17:38:06,1
AMD,nh4xdqa,Oh god that would look stupid as hell…. I’m in,AMD,2025-10-01 05:03:54,1
AMD,ngtpw2m,"Not it can't. No intel part browses the web st 70w like my 7950x does.   Huh, the down votes for mentioning facts, I can't stop laughing with how butthurt amd fans are about reality.",AMD,2025-09-29 13:32:51,-11
AMD,ngv3vs5,I hope so!,AMD,2025-09-29 17:40:45,2
AMD,ngv2xsu,"There isn't significant extra read bandwidth. It's 32+32 instead of 32+16 on allcore.  Some old and/or bad tests like aida64 are inflated by the extra CCD having its own set of cache - they're inflated even by the cache of one CCD, reading impossibly high bandwidth and low latency -  but 2000 fclk doesn't actually do more than 64,000 MB/s read or write in any circumstance.  It has the same bandwidth per clock as it did with DDR4, when it would have had to double in width to maintain parity.",AMD,2025-09-29 17:36:22,3
AMD,ngr9o8p,"I agree that AMD's design is cruder lmao. Although I'd like to contend that AMD released the 1900 series Threadrippers all the way back in 2017 and subsequently Intel infamously called them ""glued together,"" mocking the multi-die design. So I'm not actually sure Intel had multi die designs in the works before AMD. It's like Intel was just watching because they didn't see AMD as a real threat but AMD using the previous refined node for the IO die plus AMDs ability to have a SKU for every single die no matter how messed up it was (even if it only had 2 viable cores) just slashed costs so much that Intel could not compete and they did nothing about it (besides release 14nm again).",AMD,2025-09-29 01:51:55,17
AMD,nh47oyh,"> Isn't zen 6 coming to AM5?   Yes, that's what I said. Zen 6 will be on AM5, it will not be compatible with DDR6 which is not coming to consumer until 2027 at the earliest.",AMD,2025-10-01 02:02:47,1
AMD,ngyi1bh,Don't bother this sub or any hardware sub full of toxic people,AMD,2025-09-30 05:11:33,1
AMD,ngztcns,They will still hate him for some reason even if zen 7 would be releasing on am5. Then call that some other excuse.,AMD,2025-09-30 12:16:44,1
AMD,ngw1679,"*Even a broken clock is right twice a day.*  If you throw out enough random ideas with nothing to back them up, chances are you're going to be right about something sooner or later.  MLID is still an idiot.",AMD,2025-09-29 20:20:35,-1
AMD,ngu98lk,"how is your temp under load with intel vs amd ? i bought a high end cpu for a task not idle, so i can't complain because energy efficient cpus mostly in laptops where performance not the first goal. Maybe i'm wrong but it's like browsing the internet on the work station grade cpu not the best use case",AMD,2025-09-29 15:13:15,5
AMD,ngwr5pt,"for browsing and gaming from the power bank (in blackout) i'm using 8845hs tiny pc, ~5w browsing and youtube, gaming something like fallout 1080p ~20w. So maybe you need something more efficient for your needs, if you are concerned about idle power",AMD,2025-09-29 22:32:23,0
AMD,nh04b9y,"Ok, so its just an cache artifact basicly. That makes sense. Thank you.  I couldnt wrap my head around it. So i started to wonder if the link to the memory controller was maybe operating at uclock speeds rather than fclk speeds. But that didnt make sense to me either.",AMD,2025-09-30 13:21:45,2
AMD,ngrbrte,"IIRC Intel's infamous ""glued together"" line was just a marketing ploy to try to save their reputation at a time when the business side of the company realized AMD was putting them under serious pressure. I doubt it had much to do at all with what Intel had in the lab. I think Intel probably did have multi die designs in the works, but vastly underestimated the ""cruder"" solution AMD had come up with, or thought they had such a lead that they could take the time to ignore it.",AMD,2025-09-29 02:03:56,17
AMD,ngt40f8,"glued together was always ironic given the Q6600 used 2 dual core dies and connected them over the northbridge which yeah is horribly slow but worked good enough at the time.   so gluing chips together wasnt something new, having ryzen use a much faster internal link was a big leap forward but still using serdes is a performance and efficiency hit. makes it that much worse that intel could never capitalize on the drawbacks that will be going away in the future direct connected chiplets.",AMD,2025-09-29 11:10:07,7
AMD,ngrktli,"Intel has always had EMIB since early 2010s, it’s just that the yield and cost efficiency wasn’t where Intel wanted it to be, while AMD opted for a simpler interposer design primarily aimed at cutting costs right at that time. Meaning Intel always had the “right glue” but they just didn’t use it - And by the time they actually got to use it, AMD was already better at the actual application of the concept because they have been playing this game for what, 10 years by now? on a mass scale.",AMD,2025-09-29 02:59:02,9
AMD,ngrmzug,"I guess from intel skunk works development perspective, the crude ""glued together"" comment is true.  But it worked, at scale, and bet Intel to market by years.  And now AMD can play ""catchup"" on the technicalities while still being in the lead generally.  Its only now as moores law kicks in that packaging has to tighten up and becomes so critical, running out of node shrinks ahead.  2 CPU tiles seems overkill,  id love to see a single tile option with x3d  (next tiles/chiplets will move from 8 to 12 CPU anyways),  and even move to a few more ram channels for extra bandwidth, for the local AI crowd, this thing has the capacity, but bandwidth requirements are THIRSTY",AMD,2025-09-29 03:13:27,4
AMD,ngsmmi3,"gosh I havent seen a Ryzen 3 in the wild for years now,  its great that the 6 core CPU is the worst they got now!  really sets the tone.",AMD,2025-09-29 08:21:49,1
AMD,ngzf9lv,"Browsing the web, excel spreadsheets etc isn't idle, that's my actual work.   Temps are similar, intel consumes more power at full load but it's easier to cool",AMD,2025-09-30 10:32:24,1
AMD,ngzfbne,"The 8845 is a monolithic chip, doesn't have the IO die and the interconnects.",AMD,2025-09-30 10:32:55,1
AMD,ngygy5m,"I don't think anyone was ever debating that the Q6600 was ""glued together"". The Q6600 just so happened to be released a good 7 generations of Intel CPUs before Zen 1, and Zen 1 had all the associated problems ""glued together"" designs have. Even today, we're still seeing the drawbacks with multi-CCD chips. It was a quick and dirty way to get an ""8-core"" CPU out of the gate at a low price, not an optimal or advanced solution by any means.",AMD,2025-09-30 05:02:07,1
AMD,ngvzyb3,"Intel didn't have an interface like Infiinity Fabric connecting CPUs, in production, when the first ""chiplets"" from AMD were coming out.  You can argue about what was sitting around in the labs, but production is king, everything else is still a hope and dream.",AMD,2025-09-29 20:14:42,4
AMD,nh09yma,"He's referring to how AMD denigrated the Q6600 as ""glued together"" because their Phenom was a [true monolithic quad-core](https://web.archive.org/web/20121016232834im_/http://images.anandtech.com/reviews/cpu/amd/phenom/announcement/nativequadcore.jpg) design. Nevermind that the Q6600 kicked its butt.  In practice whoever is sour graping about ""glued together"" is likely to have the worse performing chip so they're grasping at straws with which to attack the other team.",AMD,2025-09-30 13:52:42,1
AMD,nh4ikpu,"You would be right except that Intel actually still had the better performing chip when they were making that criticism, and still held the performance crown in gaming benchmarks up until the X3D series. In fact, besides 3D cache, the most important change AMD made to improve the performance of their chips was simply to stop gluing them together and make 8 core CCDs instead. I do think it's a valid criticism, especially for consumer applications.",AMD,2025-10-01 03:12:19,1
AMD,ngx2wxz,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-09-29 23:39:19,1
AMD,ngx3gsc,"This is really exciting, especially for the 9060xt. Driver level frame gen isn't perfect, but for some games like Helldivers 2 it works wonders.",AMD,2025-09-29 23:42:29,95
AMD,ngxzx7c,Please include support for borderless windowed 🙏🏻,AMD,2025-09-30 02:55:01,26
AMD,ngy1qwx,Man YES please! AFMF2 works wonders in a few games! Ass in most. I really want this for older games locked at 60FPS.,AMD,2025-09-30 03:07:03,18
AMD,ngxqy5b,"I would not be surprised if its RDNA 4 or RDNA 4 + 3 exclusive, despite the fact theirs no technical reason it can't support RDNA 2. Much like RIS 2 and other features.  When it comes to DRIVER features, AMD only supports the latest or second latest GPUs, they're very horrible with driver support, even when compared to NVIDIA.  Like when they released RSR for only 2 GPUs then NVIDIA released NIS for every GPU still getting driver updates. I really hate this about them to such an extent it's the reason I switched back to NVIDIA.  AMD = Better support for game features  NVIDIA = Better support for driver features  Although I'd argue NVIDIA is only worse at game features because theirs a technical reason for it. We were given ray-reconstruction, reflex 2, and DLSS 4 for every GPU that supports it even though they're really old. Whereas AMD only does so their features are more adopted since their marketshare is small.  Kinda like when AMD said ""we can't provide Anti-Lag+ for non-RDNA 2 GPUs"" with no explanation, then when they had to make it a game feature due to bans they ended up supporting most of their GPUs... AMD isn't as pro-consumer as people think",AMD,2025-09-30 01:59:25,16
AMD,ngznqm0,"I had to turn it off, it was giving me like 500-1000fps lol I only got 144hz monitor. But pretty cool!",AMD,2025-09-30 11:38:52,2
AMD,ngyjkhg,Woah,AMD,2025-09-30 05:24:51,1
AMD,nh053dn,"as a 9060XT user, this sounds pretty cool",AMD,2025-09-30 13:26:09,1
AMD,nh1jl3o,"AMD,  as always ahead in the game",AMD,2025-09-30 17:35:30,1
AMD,nh3g78j,Difference between AFMF and Frame Generation?,AMD,2025-09-30 23:21:02,1
AMD,ngxrr1p,Frame generation is silly. Who actually thinks this looks good?,AMD,2025-09-30 02:04:13,-9
AMD,ngxxxvn,"Idk but everytime I used AFMF 2, all I got was a laggy mess",AMD,2025-09-30 02:42:22,-3
AMD,ngyjr0e,"I hope it at least gets on par with Lossless Scaling, because I've found that afmf2 is barely usable in comparison.",AMD,2025-09-30 05:26:27,-1
AMD,ngx4fl8,I wish AMD would drop all these extra features and focus on making a small set of features really robust.,AMD,2025-09-29 23:47:56,-32
AMD,ngx942v,Here comes multi-frame generation.,AMD,2025-09-30 00:15:06,-19
AMD,ngyj4a3,A F****** Mother F***** 3,AMD,2025-09-30 05:20:58,-12
AMD,ngx88sa,"Hope it works for older GPUs too, AFMF 2 is decent but it does have it's limits. Definitely nice in HD2 though when I want a smoother picture but also want to cap my framerate lower to keep the room cooler.",AMD,2025-09-30 00:10:05,27
AMD,ngxtzo7,Definitely helps a lot in HD2 especially after the recent updates that destroyed framerates.,AMD,2025-09-30 02:17:37,11
AMD,ngxybs2,"I just wish AFMF were accessible through Linux though, one of the few things I genuinely miss lmao",AMD,2025-09-30 02:44:49,8
AMD,ngyha5n,Also had a very! good experience with it while playing Hellblade Senua 2,AMD,2025-09-30 05:04:59,3
AMD,nh0v20o,Ow this be amazing for hd2 I could finally play it at 4k with better motion smoothness,AMD,2025-09-30 15:37:21,2
AMD,nh0h5oh,"...huh? I've been using AFMF 2.1 in borderless games for as long as it's been a thing. Cyberpunk 2077, Path of Exile 2, Hell is Us, Everspace 2. Zero issues.",AMD,2025-09-30 14:29:21,6
AMD,ngxsi5f,"Yeah I agree, no clue why they gate basic features like a better sharpening filter but that's AMD. I'd switch back to Nvidia myself but the cost is preventing me, that and I'd need at least a 5070 to see any real performance gains vs my 6800xt and that's at the cost of 4GB VRAM",AMD,2025-09-30 02:08:41,16
AMD,nh0i0kt,lol true.,AMD,2025-09-30 14:33:40,3
AMD,nh3v5x1,"AFMF is driver level so you can turn it on in any game(I guess DirectX/Vulkan not sure) Frame generation has to be implemented in the game by developers, AFMF2 is already quite good for driver level FG.",AMD,2025-10-01 00:48:55,1
AMD,ngy24e2,"Frame generation is awesome when it works well. You'd be stupid not to try it if you can't reach stable FPS matching monitor. Often it won't work well, but when it does it's awesome.  Edit: Just to add a lot of you find 60FPS fine. But 60 base frames resulting in 120~ with frame gen which has more or less the same latency when it works well is bad? That's some real mental gymnastics. Do yourself a favor and actually try it, it won't hurt you.",AMD,2025-09-30 03:09:31,14
AMD,ngym224,It has some obvious artifacts in motion but some people prefer to put up with that for smoother frames,AMD,2025-09-30 05:47:06,2
AMD,ngy2e9s,Saved me from refunding SOMA. I'm really glad about that.,AMD,2025-09-30 03:11:22,1
AMD,ngxgslq,Stfu,AMD,2025-09-30 01:00:03,8
AMD,ngxdg87,To be fair when you use it minimally it works pretty well. None of this 3 or 4 time fake frames. Even lossless scaling on 2x frame gen is great.,AMD,2025-09-30 00:40:46,6
AMD,ngxptc1,"Umm... Even if they improve AFMF significantly, without input from the game MFG will look gross...",AMD,2025-09-30 01:52:40,2
AMD,ngxc262,"Pfft just migrate to Canada instead!   But you're right, AFMF has been good to me on HD2. Aside from the other bugs the game has, its technically smooth",AMD,2025-09-30 00:32:33,13
AMD,ngxgahb,"AFMF2.1 has less latency than simply rendering one frame ahead, which for games with a cfg/ini/console that permits queue 0 makes it more or less strictly better.  You only hold back the first frame half a frametime plus the framegen time, display frame 1, and then a few ms later the second frame is done (and frame 3 starts) and you can render the gen 1.5 frame and display it, then wait to show frame 2, then frame 3 is done and you cook gen 2.5, etc.  At 60fps base we're talking like 9-10ms latency for 120fps with FG vs 8ms for 120fps natty flip queue 1. And your GPU is absolutely screaming for the latter.",AMD,2025-09-30 00:57:10,5
AMD,nh2d9t0,Apparently it is only a feature for 7000-series & up,AMD,2025-09-30 19:58:37,4
AMD,nh2cvzg,"Everytime I have tried, Adrenalin says the game must be fullscreen when it is in borderless windowed. I am on 6000-series, though.",AMD,2025-09-30 19:56:46,1
AMD,nh2db15,i could have sworn it only works for borderless window games it's the only way I play games,AMD,2025-09-30 19:58:47,1
AMD,ngyi801,"Yeah, also couple that current day Nvidia drivers (for the current gen cards) are unstable as hell.  Ever since the 30 series, Nvidia just doesn't really care about making their drivers stable because people will buy their shiny new cards anyway, it's only when they wanna get rid of old stock for the next release that they'll finally give you stable drivers.  You kinda have to pick your poison really, enjoy the GPU now? Or enjoy the GPU in 4 years while you stare at the new gen wanting to buy again anyways.  Though I do half understand RDNA2 not getting as much support, like it's defo capable don't get me wrong, but also comparing it to Nvidia's first gen RTX cards RDNA2 gets the same level of old-gen driver support.   It's probably a bit of laziness too of course, but also it does look like it's at least partially a gen1 RT GPU problem as well.  Though it does look like things are going to get better in the future, 3D rendering programs are switching to modern API's which favour AMD equally to Nvidia, which will hopefully get the ball rolling in terms of AMD's rnd and finally get some healthy competition going again (which would mean lower prices all around, current prices are only so insane because Nvidia has a mini monopoly and so does AMD, whenever the two are actually competing is when we see the largest generational uplifts and the lowest prices).",AMD,2025-09-30 05:13:08,5
AMD,ngzqbeb,"I've tested it and it's bad, but I wouldn't mind its existence if studios didn't see it as a reason not to optimize their games.",AMD,2025-09-30 11:56:33,1
AMD,nh0p77z,I did and it looks goofy. Artifacts and blur.  Downvote me again.,AMD,2025-09-30 15:08:50,1
AMD,nh31dfq,"i played around with frame gen in animation back in the day, weirdly enough i found that 300% (x3) was the golden number",AMD,2025-09-30 21:56:43,1
AMD,ngxlxag,"Yeah, Lossless Scaling ""MFG"" is fine for some games, though has really distracting artifacts involving text and sudden camera transitions. Like having 60 FPS capped games ""unlocked"" to adaptive with my monitor's max Hz is fun.",AMD,2025-09-30 01:29:57,0
AMD,ngxinep,edit: deleted some potentially incorrect stuff  Any flip queue change works just as well without framegen so it's not valid to compare apples to oranges with the fg-off profile intentionally weakened by running the wrong settings.,AMD,2025-09-30 01:10:46,12
AMD,nh2dy6g,"Ah, that would make sense. It feels like an odd thing to be restricted based on hardware, but GPU stuff is hilariously complex so the fuck do I know.",AMD,2025-09-30 20:01:52,2
AMD,nh3a7cj,"Definitely works on the 6000-series too, I've personally used it.  Maybe you're forgetting to turn off vsync?",AMD,2025-09-30 22:46:31,1
AMD,nh2ayv2,"I have a 4070Ti Super, I have several buddies with 50 series cards, those issues were ironed out months ago. Which only became to happen because of a new launch, just like AMD driver issues.",AMD,2025-09-30 19:47:28,1
AMD,nh0wo4z,"Never downvoted anyone here. I said, you'd be stupid not to try. I also specifically said it usually doesn'y work well. But when it does, it's awesome.  Frame gen has never affected the quality of my visuals though, only latency. So I'm kinda leaning towards that you are lying. I mean I am fucking tilted on how sure I am that you're lying.",AMD,2025-09-30 15:45:13,3
AMD,nh1w8es,"Oh no, you don't see the magic here! Say we have 100fps base for easy math.  You finish frame 1 at time t=0ms   You will finish frame 2 at t=10ms  Say it takes 2ms to make a gen frame.  You wait until t=7ms to show frame 1. (7ms latency)  then at t=10ms you have frame 2 done and immediately begin rendering frame 3 which will be done at t=20ms, and also begin making the gen frame 1.5  at t=12ms you have finished frame 1.5 and you display it immediately  at t=17ms you display frame 2 (7ms later than it was finished)  at t=20ms you finish frame 3 and being working on frame 4 and frame 2.5  at t=22ms you finish frame 2.5 and show it immediately  at t=27ms you show frame 3   etc  -----  so you are only adding 7ms of latency compared to zero renderahead and are actually below the 10ms inherent latency of queueing one frame natty  The AFMF2 framegen lag metric actually maps exactly to this because as far as I know, this is how they do it. How else could they claim less than 1 frametime of latency? They'd have to simply be lying",AMD,2025-09-30 18:35:56,3
AMD,nh1wvd2,"You may be right there! It actually lines up better with some of my data, but i didn't consider it in this way.  For example going from 113fps to 452 via quad framegen (DLSS) in a CPU-limited environment, the overlay reports only an additional 4.2ms render latency. That would maybe make sense if the framegen work time was ~1ms.  In GPU-limited environments the latency addition is complicated by the GPU frametime worsening as it dedicates resources to framegen, but in CPU-limited environments you can get flat 2x/3x/4x and the base frametime stays the same. You get a purer measurement of the framegen latency penalty that way.  It is worth a note however that using framegen when GPU-bound can very substantially lower the base framerate and hurt latency that way - both making the frame latency worse, and forcing the framegen process to add more latency to it. It's much less beneficial in GPU-bound environments.",AMD,2025-09-30 18:39:04,3
AMD,nh22bq6,"This is such interesting technology, I appreciate the smart people who develop it so I can enjoy my games with better framerates. FSR4 and DLSS4 are literally magic as well, my Darktide settings were reset without me knowing and I didn't even realize dlss balanced was enabled instead of dlaa until I manually checked. Maybe if I had a side by side monitor comparison, but just wacking heretic skulls I was none the wiser. FSR4 Quality is absolutely fantastic as well.",AMD,2025-09-30 19:05:25,3
AMD,new6d2w,AMD Radeon™ RX 7700 ?,AMD,2025-09-18 14:28:33,86
AMD,new7bd0,>Failure to launch may be observed while using the Oasis Driver with Windows Mixed Reality headsets.  I need to head over to the r/WindowsMR and see if this allows Oasis to work.,AMD,2025-09-18 14:33:14,60
AMD,new9ceu,"Boy, that's a lot of intermittent crashes.",AMD,2025-09-18 14:43:08,34
AMD,newb3mf,"Seems like it doesn't appear for 9000 series, also it is tagged as an optional driver.  I guess, if everything works fine for your, don't update. Nothing special regarding this one.",AMD,2025-09-18 14:51:33,24
AMD,new9nhl,u/AMD_Vik can you please inform the boys that Alan Wake 2 with RT/PT crashes on 7900 XTX on I think all branch 25.10 drivers?,AMD,2025-09-18 14:44:38,16
AMD,newcizg,"Crashing was fixed for me with the last driver update, not sure if I want to update just yet. Please let me know how it goes for yall!  6750xt",AMD,2025-09-18 14:58:16,7
AMD,newkls6,My rx550 chilling in the corner:,AMD,2025-09-18 15:36:24,7
AMD,new6tpn,"Still didnt fix the VR issues, damn.",AMD,2025-09-18 14:30:48,24
AMD,newhp81,Please fix Beyond All Reason crashes on the 7000 series GPUs!,AMD,2025-09-18 15:22:33,5
AMD,nex9qss,Need fix for beyond all reason driver timeouts on 6000 series!,AMD,2025-09-18 17:35:20,4
AMD,nez2wt5,So… no fix for weird pink pixels appearing in browsers? An issue that appeared after last update for many users?,AMD,2025-09-18 22:58:04,4
AMD,newji7i,> ... disable Radeon™ Anti-Lag as a temporary workaround.  Anti-Lag vs Anti Cheat solutions II:  *Electric Boogaloo*.,AMD,2025-09-18 15:31:09,8
AMD,newcz5u,How can I backup all by adrenaline settings to do the DDU reinstall workaround to avoid bricking the performance metrics? (New 9070xt owner),AMD,2025-09-18 15:00:21,4
AMD,nexguol,"Why is the VR bug on the 7000 series still a thing? It's been 6 months by now, that's honestly unacceptable at this point",AMD,2025-09-18 18:08:39,4
AMD,new8ln7,The NBA my career crash going to get fixed before NBA2k27,AMD,2025-09-18 14:39:34,7
AMD,new83tu,The VR issue still not being fixed is a complete blunder!,AMD,2025-09-18 14:37:07,8
AMD,newvgqa,"At this point, I’m not even going to try. I have zero hope for an artifact fix while hardware acceleration is on.  update : i tried and its not fixed ofc 🤦",AMD,2025-09-18 16:28:00,4
AMD,nex0um7,>Intermittent system crash may be observed while playing World of Warcraft while watching YouTube on Radeon™ RX 7900 GRE graphics products.  Nice.,AMD,2025-09-18 16:53:43,2
AMD,nexbedm,Atualização da AMD 👏👏👏 .... Ansioso pelo FSR Redstone!!!,AMD,2025-09-18 17:42:59,2
AMD,new9gfe,"I am not able to turn on noise suppression anymore, I think this driver broke it.",AMD,2025-09-18 14:43:41,3
AMD,newvi0w,Has anyone with these previous issues:  https://www.reddit.com/r/AMDHelp/comments/1khzck4/version_2551_update_rx6800_crashes_often/   https://www.reddit.com/r/Amd/comments/1k60kzh/amd_ryzen_chipset_driver_release_notes_70409545/   https://www.reddit.com/r/AMDHelp/comments/1k2a66p/blue_screen_with_the_new_2531/   Tried this driver yet? Especially RX 6700XT? The last driver I am able to use without issue is 24.12.1,AMD,2025-09-18 16:28:11,4
AMD,new7845,Damn I was hoping for a big update on official implementation of FSR 4 to RDNA 2 and 3.,AMD,2025-09-18 14:32:47,5
AMD,newfra4,"""Crashing may be observed in 2077 with path tracing"" yeah no shit sherlock we all have known that for god knows how long",AMD,2025-09-18 15:13:25,2
AMD,nex1641,25.9.1 has been causing High cpu in games for some reason for me.  I have the 9070xt and 7800x3d.    I had to roll back an update and it fixed it,AMD,2025-09-18 16:55:12,2
AMD,nex2lzv,I guess we aren't gonna get more officially approved FSR4 games. AMD should really be adding to the FSR4 list even though they allow every fsr3.1 game to upgrade automatically.   There are many people around the internet still claiming that there are only 85 FSR4 games.,AMD,2025-09-18 17:01:59,2
AMD,newh3x4,Experiencing the same issue in world of Warcraft but I have a 9070xt.,AMD,2025-09-18 15:19:46,1
AMD,newi4ni,I was getting some microstutters on my 9070 XT yesterday playing HL Alyx but that was on 72 hz. Could it be linked to the same problem mentioned here?,AMD,2025-09-18 15:24:36,1
AMD,newj1af,Is the gtfo fix coming next driver? Or later in the year?,AMD,2025-09-18 15:28:54,1
AMD,newkckl,"AMD noise suppression is not working or activating, when i manually run executable from its folder its greeted by a Windows warning.  This app can't run on your PC with a prompt to close it.   [https://i.imgur.com/YxGSE2B.png](https://i.imgur.com/YxGSE2B.png)",AMD,2025-09-18 15:35:11,1
AMD,nexcjco,"I need a fix for CS2 Hammer. Keep getting VRAD ERROR when trying to compile a map, drivers/gpu crashed. And not just me another friend with AMD card is having the same issue, was ok few months back",AMD,2025-09-18 17:48:18,1
AMD,ney1xxa,"But does this fix the hardware acceleration artifacting in chromium-based applications (i.e. Steam, Discord, and Chromium Browsers)?  Edit: For 7000 series cards",AMD,2025-09-18 19:49:58,1
AMD,neysuhe,"Adrenalin won't open when I use the ""minimal"" install. It was working fine in 25.9.1",AMD,2025-09-18 22:01:30,1
AMD,neyxpok,"The long-await news for Oasis support! While it's not yet fully resolved, but great significance in AMD's official note. Whether the remaining journey lies with mbucchia or AMD remains to be seen, but really hope Radeon will finally meet Oasis.",AMD,2025-09-18 22:28:26,1
AMD,nezgsyp,I really want to play 2k26,AMD,2025-09-19 00:19:47,1
AMD,nezpyol,"No fix listed for right clicking in explorer launches the Radeon software, le sigh~",AMD,2025-09-19 01:12:56,1
AMD,nf0gmbe,I see World of Warcraft mentioned   Does this mean I can now play the new zone in DX12 mode,AMD,2025-09-19 03:53:30,1
AMD,nf60zfs,"This driver dont work with RX 6800 XT, The installation doesn't complete, the screen doesn't return, and it restarts automatically. It doesn't install properly because there's some bug. This happens with the updated WIN 11 24H2.",AMD,2025-09-20 00:29:23,1
AMD,nf7xjm5,"Already had my first driver timeout while playing dying light the beast we going this way again, where i cannot play new games because drivers simply aren't ready yet ?",AMD,2025-09-20 09:38:00,1
AMD,nfcbq9t,This error trying to install 25.9.2 and reverted back to 25.9.1  https://i.redd.it/t2z8q8tb1fqf1.gif  Never had this error before since 2020 (rx 6800 \~> 7900 xtx),AMD,2025-09-21 00:55:04,1
AMD,nfkylsq,"Insane performance boost using these drivers on DLTB with 9070XT: from 93 to 109(!) fps at 3440x1440p with FSR4 Quality and every setting on High, and from 56 to 67 fos at native resolution.",AMD,2025-09-22 11:35:34,1
AMD,nfr2yk7,* Failure to launch may be observed while using the Oasis Driver with Windows Mixed Reality headsets.  HP Reverb G2 + Radeon 7900XT is finally working! (Thes are the BEST NEWS after MS bad news)!   I will also test with 7900XTX (but i guess there will be no surprises and it will just work as well).  THANK YOU!,AMD,2025-09-23 11:56:32,1
AMD,nfyuy67,F1 25 the VR mode is still bugged since 25.4.1 also in this release,AMD,2025-09-24 16:00:20,1
AMD,ng41flc,"Anyone playing wow currently?   In the new zone K'aresh, everytime I load in and look at the sky, my gpu crashes. Lose monitor signal and take like 2-3 minutes to come back.   Sometimes, my whole computer freezes and I have to hard reset my pc.   Don't know if it is a AMD issue or what. I reinstalled the game and is still happening.  On a 9800x3d/9070xt only game it happens on.",AMD,2025-09-25 11:58:00,1
AMD,ngkv1k2,"estou tendo problemas com a imagem, parece ghoost, começou ontem, vi as cores estranha no jogo, borrando quando movimento a tela, quando eu rolo a tela as palavras borram enquanto em movimento. ai atualizei para versao amd software v 25.9.2, pois achei que fosse devido a isso, mas nao melhorou.   uso um monitor aoc ips 24g30e cabo dp 2.1 e uma rtx 6600 8gb asrock.   obs: amd adrenalin parou de detectar a temperatura do processador, apenas detecta o uso, parou de detectar a uns dias.",AMD,2025-09-28 01:40:32,1
AMD,new7d2n,Bro we are waiting for Redstone that's all,AMD,2025-09-18 14:33:28,1
AMD,newbylm,Issues with the Cyberpunk 2077 are since beginning of 2025...just fix them fsf,AMD,2025-09-18 14:55:35,1
AMD,newu7dm,My whole system locks up and have to keep enabling the 9070xt in device manager. Must have reported this multiple times. Switched to bazzite and while Linux has its own issues have yet to have the entire system crash like on windows with amd ugh.,AMD,2025-09-18 16:21:58,1
AMD,neye896,The Warzone stutter issue has been taken out of the notes what does this mean ???,AMD,2025-09-18 20:47:28,1
AMD,nex0mko,"Can u fix ""leaked"" FSR4 compatibility on RDNA2? FSR4 works with RDNA2 on the driver 23.9.1 or bellow and on Linux but 24/25.X.X Windows drivers have terrible ghosting and artifacts. Thanks!",AMD,2025-09-18 16:52:40,0
AMD,newnt6d,"If they're releasing more RDNA3, then that hopefully means they're working hard to backport FSR4 to it. Fingers crossed for them, even though I have a 6950XT lol",AMD,2025-09-18 15:51:26,24
AMD,new7pwc,"hah, i was curious about that too...",AMD,2025-09-18 14:35:13,17
AMD,new7wq6,"It should, this fix is implemented in a way that avoids the need for EDID hacks as well.",AMD,2025-09-18 14:36:09,53
AMD,nex97ly,What is this? I thought mixed reality is dead? Is there a tool that I can use to make my HP Reverb G2 a functional headset again?,AMD,2025-09-18 17:32:51,3
AMD,newjuqb,yup staying on 25.9.1 for now.,AMD,2025-09-18 15:32:49,3
AMD,nfil857,Also does not appear on the 6000 series.,AMD,2025-09-22 00:15:14,1
AMD,newjxsq,Also on CS2 and POE2!!! Driver not found error. I have tried under volting and so forth in the performance settings and cannot get always stable. Only mostly stable.,AMD,2025-09-18 15:33:14,6
AMD,nezczw7,"Yeah, I honestly wonder the same with my 6700xt. Like, I’m guessing majority support is for 90 series by now, so do I risk it with the new drivers..? It’s not like I have bad performance now and I don’t imagine there’s much juice to squeeze out nor do I have much confidence in optimization boosts.",AMD,2025-09-18 23:56:52,2
AMD,nfrk22m,I wish I could hold on to cards like that. I have an RX 7700XT and i already want to get rid of it,AMD,2025-09-23 13:37:39,2
AMD,new8vz9,"If you look back to the 25.9.1 reddit post, an AMD rep said that they plan on releasing the VR patch for 25.10.1.  I hope I interpreted the reps response correctly and that this happens.",AMD,2025-09-18 14:40:56,26
AMD,newz1ii,Are you talking about the one where the screen is tinted green on Oculus Rift S?,AMD,2025-09-18 16:45:13,1
AMD,nf1eos9,And no mention of the guaranteed shutdown after 5-6 hours of SteamVR continuous use since last year's driver..,AMD,2025-09-19 08:50:41,1
AMD,nf2lmlr,"Yes, I also noticed this since several prior driver versions. It's rare in occurrence.",AMD,2025-09-19 13:56:46,3
AMD,newk6cp,Yes I also have to disable all these settings or VAC will boot me. Anti lag and image sharpening,AMD,2025-09-18 15:34:22,1
AMD,nf7nx61,"There is the ""export settings"" option in the system/preferences tab (top right in adrenalin) and you can re-import the zip file directly to restore setrings after installing new driver.",AMD,2025-09-20 08:00:55,2
AMD,newdwfm,"Maybe it's implied by you saying ""reinstall workaround"", but unless you're having issues there's really no point wasting time doing driver updates with DDU.",AMD,2025-09-18 15:04:43,2
AMD,neyg7uh,Thank you for your service of trying.,AMD,2025-09-18 20:57:00,6
AMD,newi1x2,"it was pretty ass to begin with if you mean the audio one, not surprised it doesnt work anymore",AMD,2025-09-18 15:24:14,7
AMD,new8gv8,I wouldn't expect that to happen before Redstone.,AMD,2025-09-18 14:38:54,31
AMD,newedol,It's a new driver version mainly released to support the RX 7700 and provide optimized support for today's launch of *Dying Light: The Beast*.,AMD,2025-09-18 15:06:59,4
AMD,newo5om,I hope it comes to RDNA2 but I am not holding my breath on it.,AMD,2025-09-18 15:53:04,1
AMD,newmnqi,"Nope that would happpoen with Redstone, amd would announce that feature,its not to put in a driver.",AMD,2025-09-18 15:46:00,0
AMD,newjz4d,lol no dude won't be seeing anything on that until the end of the year.,AMD,2025-09-18 15:33:25,-1
AMD,nex6mzi,"I had high CPU usage with the same card. When I would open task manager, it would go back down. Never could figure it out",AMD,2025-09-18 17:20:54,3
AMD,nf7o4p0,Did the .2 driver fix it? Cpu temps has been the issue for me in the one before 25.9.1,AMD,2025-09-20 08:03:02,1
AMD,news00u,I have a 7900xt but haven’t been experiencing these crashes since swapping off Windows. I’ve had plenty of keys fail because others are crashing,AMD,2025-09-18 16:11:27,2
AMD,nfm9ore,"Same dude, how did you fix it?",AMD,2025-09-22 15:54:00,1
AMD,newi4ea,I legit just swapped my 7900xtx for a 5080 because I crash constantly in dungeons.,AMD,2025-09-18 15:24:34,1
AMD,newluhw,"The driver workaround is tentatively aligned to an October release, we're still working with the developer on an actual fix.",AMD,2025-09-18 15:42:10,7
AMD,nf1b1pt,"Are you asking if AMD mentioning a system crash observed while playing WoW while watching Youtube on RX 7900 GRE as a Known Issue (ie. not fixed) would mean that your issues in WoW are solved by this patch?  I mean... probably not, right? But testing it out won't hurt.",AMD,2025-09-19 08:13:43,2
AMD,nf877vn,"Happens with me on 6700 XT, you can finish the install by installing the driver again but I will just use the 25.8.1 for now.",AMD,2025-09-20 11:07:16,1
AMD,newexfx,what is redstone?,AMD,2025-09-18 15:09:33,3
AMD,nf1fcly,Last patch notes AMD said they'd already done everything they can on the driver side and were working with the game devs to fix it.,AMD,2025-09-19 08:57:15,3
AMD,nfrkhzs,Makes you wish you bought an Nvidia card... ffs..,AMD,2025-09-23 13:40:03,1
AMD,ney2w0n,man...,AMD,2025-09-18 19:54:17,9
AMD,newgxe9,[https://videocardz.com/newz/amd-launches-radeon-rx-7700-with-2560-cores-and-16gb-memory](https://videocardz.com/newz/amd-launches-radeon-rx-7700-with-2560-cores-and-16gb-memory)   New launch! :D,AMD,2025-09-18 15:18:55,25
AMD,newtvjg,"Hi, can you give me some details about the change?  Several users (myself included) reported that they don't see any difference. No one has reported success so far AFAIK.",AMD,2025-09-18 16:20:25,15
AMD,new8mh5,Well this is exciting! Should be able to try it myself later.,AMD,2025-09-18 14:39:40,13
AMD,newettf,Can you pass this somehow to the Ryzen Master devs?   There has been a bug in Ryzen Master instalation that it will fail when you use specific windows region options instead of US. In my case when set to Bulgarian/BG. I have reported it numerous times and it seems that it is not fixed. I still get thank you coments on my 3 yeard old post about this issue.  https://www.reddit.com/r/AMDHelp/s/Z7HLxnjmpQ,AMD,2025-09-18 15:09:05,10
AMD,newxsnp,Massive kudos to the dev team but it could have been communicated better with the Oasis dev.,AMD,2025-09-18 16:39:18,9
AMD,nf4mz9g,hi gaben is too busy with his yachts - can you guys do anything to improve the performance of cs2 - thanks =),AMD,2025-09-19 19:53:08,1
AMD,nexqvnq,Yes! Well there is if you have an Nvidia card! An MS dev created a project called Oasis which replaces the Windows MR stuff that MS removed when they decided to kill everyone's expensive hardware! The Dev has had no luck getting help from AMD so while he got Nvidia cards working us AMD users have been stuck. This looks like a sign that AMD has indeed been working on fixing the road block. It still doesn't actually work on AMD but hopefully now the Oasis Dev can get it working.   I posted the new driver over there. It is worth following the sub for news/updates as things progress.  [https://www.reddit.com/r/WindowsMR/comments/1nka1it/new\_amd\_driver\_claims\_to\_fix\_failure\_to\_launch/](https://www.reddit.com/r/WindowsMR/comments/1nka1it/new_amd_driver_claims_to_fix_failure_to_launch/),AMD,2025-09-18 18:56:50,3
AMD,nfir572,Now it should appear. It didn't originally.,AMD,2025-09-22 00:51:29,1
AMD,nf4hwt4,Yeah there are no boost in performance since years (6650xt here). I guess just update if you need optimization for a new game that you want to play or a new driver feature.,AMD,2025-09-19 19:27:46,1
AMD,ng970dx,"Yeah,suppressing the urge to upgrade is hard (here's a tip:if you don't have money you automatically unlock the no upgrade mode)",AMD,2025-09-26 04:56:40,1
AMD,newn5mx,Ah thanks! guess i will wait just a bit more then,AMD,2025-09-18 15:48:20,3
AMD,nex2wvz,"No, i have my VR games with the Quest 2 constantly freeze every couple of seconds for about one second or crash entirely",AMD,2025-09-18 17:03:23,2
AMD,nf7plyf,"Thanks, Sir!",AMD,2025-09-20 08:17:52,1
AMD,newgov0,"Every time I update the drivers via adrenaline, the performance tracking is broken and single sensors (CPU temp i.e.) don't show up anymore.",AMD,2025-09-18 15:17:48,4
AMD,nezqae4,i did and it has nothing to do with it,AMD,2025-09-19 01:14:46,1
AMD,newlfzh,Huh? Noise suppression is awesome and helps tremendously in my recordings.,AMD,2025-09-18 15:40:18,2
AMD,nex5g3c,"And that still a big if. I wouldn't hold my hopes up for that. I am just happy, that we got FSR4 on older cards as we did.",AMD,2025-09-18 17:15:17,0
AMD,nex6qya,What did you end up doing.?,AMD,2025-09-18 17:21:24,2
AMD,nf4g36z,"It looks like virus behavior, of the gen.32 type - I remember it well, it has a very clever mechanism spreading silently between executables.    Only Malwarebytes can remove it.",AMD,2025-09-19 19:18:31,2
AMD,newig1z,"9070xt had been incredible for me since march, only started with this issue maybe a couple weeks ago.   I might try rolling back to whatever driver I installed when I assembled the build that worked great for me before raid tonight. Otherwise I’m sure it’ll cause at least a few wipes lol. Hopefully they fix it soon.",AMD,2025-09-18 15:26:06,2
AMD,newyudi,Amd sorry but your software has a problem with my rx I use windows 11 with a rx 6000 series and every time I turn off and on the pc I have to reset the fans because it loses the configuration can you fix it I am forced to use msi afterburner for the fans and driver only for the amd drivers,AMD,2025-09-18 16:44:17,1
AMD,nf1hpls,Yeah a guy can dream  aMD time out issues,AMD,2025-09-19 09:20:47,1
AMD,newjw0o,Next version of FSR 4. Going to be released in October or November.!,AMD,2025-09-18 15:33:00,2
AMD,nf22ce6,"I havent played with it myself yet, but there are threads of people using FSR4 on RDNA2 GPUs on Windows already if that's what you're asking. I have heard there are lots of issues with ghosting though, which are not present on Linux when doing the same thing, so seems to be a driver issue potentially. I too am holding out hope that 6800XT gets FSR4 support because that visual difference is insane, even looking at performance on FSR4 vs quality on FSR3. And as a software engineer I dont see a reason that isn't feasible, but I doubt AMD would put that out (maybe a setting in optional drivers if they did at all).",AMD,2025-09-19 12:07:06,2
AMD,newnj7y,"What an odd card. VRAM capacity, speed, and bus width of the 7800 XT, but less CU's (40 vs 54) than the 7700 XT.  Guess they had some leftovers where all MCD's worked but not all CU's on the GCD did.",AMD,2025-09-18 15:50:07,30
AMD,newwrft,"Hey there, quoting my colleague from Display:  > The driver will autodetect WMR from EDID and register it for LVR use. Oasis should not do or need to do anything to make the HMD work with SteamVR and LVR",AMD,2025-09-18 16:34:16,19
AMD,nf4ouy7,I've passed this on - thank you,AMD,2025-09-19 20:02:18,2
AMD,newym50,[We touched upon this here](https://old.reddit.com/r/Amd/comments/1n40oz8/amd_being_difficult_with_wmr_3rd_party_driver/nbw7t4t/?context=3),AMD,2025-09-18 16:43:12,16
AMD,nfirkj4,No luck here (6950XT): [https://i.imgur.com/0bOefAN.png](https://i.imgur.com/0bOefAN.png)  Maybe it's a slower roll out.,AMD,2025-09-22 00:53:59,1
AMD,ng9peyr,"Sorry. I was just talking to myself. I want to get rid of the 7700XT only because the model i received from Sapphire has been a big issue with coil whine, fan noises and driver issues with the mainboard. It was not eligible for RMA or refund because the card runs games fine most of the time and no issues with Windows 11. Infact Im just waiting to sell it for 280 dollars and use the money to fund a new card",AMD,2025-09-26 07:46:02,1
AMD,nexw8th,Probably unrelated but for HTC Vive I had to disable Motion Smoothing in SteamVR or it will gobble up more and more system RAM making game stutter then eventually crash (Half Life Alyx for example).,AMD,2025-09-18 19:22:43,2
AMD,newk8se,"Hmm weird. I stopped using Adrenalin for CPU metrics a while back, but I do remember experiencing some weird behaviour with Ryzen Master.  Nowadays I just update over the top. There's also a box you can select when doing clean install to preserve settings.",AMD,2025-09-18 15:34:41,1
AMD,nexubua,"Even after using DDU, I still cant get performance tracking to work when I installed 25.9.1. None of the average FPS in my games track, but hours played does. I use a different overlay than AMD's so is it because I dont have the overlay physically enabled when gaming? Idk who wants to stare at that giant thing on the side of their screen anyway while actually playing",AMD,2025-09-18 19:13:30,1
AMD,newoise,Krisp and Nvidia broadcast are vastly superior and I say this as a 9070 XT day 1 owner  It's a no contest if you have tried them all  Especially in live situations like voice or video calls  Genuinely terrible in comparison,AMD,2025-09-18 15:54:47,2
AMD,ney6jow,Just minimized task manager. Only troubleshooting I really did was to make sure it wasn't malware.  I thought it could have been a process ending itself when task manager opened but I didn't find anything. I know the latest Monster Hunter was a game it happened on.,AMD,2025-09-18 20:11:24,2
AMD,nexx2mp,Hmm. It doesn't sound like a driver problem.,AMD,2025-09-18 19:26:40,3
AMD,newt078,Oh wtf.. support for rx 7900xt?,AMD,2025-09-18 16:16:15,3
AMD,nf4pkhk,The ghosting issue is fixed with driver downgrade to 23.9.1  Edit: [https://youtu.be/c3YWjqpQu-U](https://youtu.be/c3YWjqpQu-U),AMD,2025-09-19 20:05:48,1
AMD,newplza,In line with the latest 5600F launch  Totally unclear why this production process is still alive and who is gonna buy this,AMD,2025-09-18 15:59:56,5
AMD,nf7nka1,Im sure the core clocks overclock like crazy if you're willing to change power limit a bit,AMD,2025-09-20 07:57:23,1
AMD,nexwptw,More VRAM than the 7700 XT… wtf. Just goes to show the XT model should have had 16 GB from the start.,AMD,2025-09-18 19:24:59,0
AMD,nex4pv6,"Thank you. I'll have to look more closely, it probably requires changes on my side as well.",AMD,2025-09-18 17:11:51,12
AMD,nfisfyi,"It's optional, so go to AMD's website  [https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6950-xt.html](https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6950-xt.html)",AMD,2025-09-22 00:59:08,1
AMD,ney1527,"Yeah i saw people mention that but its not enabled for Oculus or Windows MR headsets, which explains why i couldn't even find that setting.",AMD,2025-09-18 19:46:13,1
AMD,newy2hk,"Been on green over over 20 years until I gave the big middle finger last month and retired my evga 1080ti Hybrid FTW and jump on the Red Boat. So i'm very used to the DDU process with Nvidia even though I do the clean up option.  As such, what's the best process for team Red?      uninstall and purge all remnants process?     clean uninstall option with new one?     Do I do update via the old one or download new one and run it?  If I tweaked my fan settings and start doing under volting, I will have to start over with that each driver update?   I already installed 9070xt on new 24h2 windows 12 install on my nvme drive last month. So I'm asking how to ""update"" radeon drivers properly",AMD,2025-09-18 16:40:36,1
AMD,newsv30,"That's a crazy opinion. I've found Krisp to be absolute dogshit in comparison, like almost unusable. Nvidia is definitely superior, but it's such a small difference I'd struggle to tell if I wasn't knowing to pay attention.  ninja-edit: I wonder if it's different with the 9000 series? I've used Noise Suppression on 6000 and 7000 series only.",AMD,2025-09-18 16:15:35,3
AMD,nf4h0ib,Sometimes this surge in resource usage can also be noise suppression from the GPU driver conflicting with Windows or other sound-related software coming from the motherboard.,AMD,2025-09-19 19:23:13,1
AMD,neztb1j,I said that the fans don't work sometimes with adrenaline software not that the drivers don't work to fix their broken software,AMD,2025-09-19 01:32:10,1
AMD,nf2412p,"I hope for you, at least a light version.",AMD,2025-09-19 12:17:48,1
AMD,ney3a03,maybe but dont hope too much,AMD,2025-09-18 19:56:04,0
AMD,nf4rkxl,"I stand corrected, thanks for letting me know!",AMD,2025-09-19 20:15:47,1
AMD,newqm6s,I'm imagining it's less new production and more getting any leftover bins that couldn't be used for previous 7000 series cards for whatever reason,AMD,2025-09-18 16:04:46,11
AMD,nex1t45,"Many people around the globe want to play games and have salaries of about 15 dollars per day. Budget PC may be big IMO, If they produce those chips, someone is buying them",AMD,2025-09-18 16:58:10,7
AMD,nf1390a,It will compete with the 5060 and has double the VRAM. Worth it if it's under 300 dollars.,AMD,2025-09-19 06:57:15,1
AMD,nfcwicg,lol even my 6900 xt and 6800 xt came with 16gb,AMD,2025-09-21 03:09:32,1
AMD,nex9ens,"All good - for whatever it's worth, we successfully initialised our WMR devices using the private driver you'd sent over. Not sure if anything relevant had changed between then and now.",AMD,2025-09-18 17:33:46,16
AMD,nfiu1no,Oh! I see. Thank you very much; I appreciate that!,AMD,2025-09-22 01:08:37,1
AMD,nex8dwq,"I only switched to AMD a couple years ago myself (last card was HD4890 from 2009) so I was hesitant to jump myself after being with Nvidia for the better part of 15 years. However, performance and stability have been entirely faultless with my 7900XT.  Anyway, regarding driver updates, I've never had any trouble just installing straight over the top of the existing driver via the Adrenalin interface. It used to be that you just download and run the new version directly from Adrenalin, but at some point in the last few months I've got some AMD Install Manager on my PC that does it instead, so YMMV if you have that or not.  I usually _don't_ perform a clean install (I think the tick box is labelled ""Factory Reset"" on the installer window), but there should be an option to preserve settings/config if you do want to do this. I have done it before and it's kept all of my bindings and stuff.  So yeah, tl;dr DDU is definitely unnecessary IMO. Just download the new version and install over.",AMD,2025-09-18 17:29:03,1
AMD,nf5cq87,"Just DDU's ur older driver and install the new, if you're having problems there's some troubleshoot that u can try, but for only installation use ddu, also you can save ALL your configuration in one document, on the AMD app, everytime you do a DDU, just use that document to put everything of your fan and unvervolt configuration. Also, don't upgrade your driver if you're not having problem.",AMD,2025-09-19 22:06:54,1
AMD,nexyr6o,"It's most likely this given how late this is coming out and there doesn't seem to be a DIY release. Plus, this card has the same hardware cost as the 7800 XT while performing slower than the 9060 XT and 7700 XT. So there's really no reason besides salvage.",AMD,2025-09-18 19:34:47,4
AMD,nezo4oc,If they have professional SKUs on the same silicon they may be obligated to keep the production line going and keep accumulating strange bins,AMD,2025-09-19 01:02:26,2
AMD,nex5qmk,"While this might be true, building a cheaper product on the newest node and technology would be an answer too.   I don’t think the demand for amd gpus is much higher than supply. Where is the 9060 non xt? Or gre",AMD,2025-09-18 17:16:39,-1
AMD,nf18pwz,Also FSR4 is closer and closer to be fully working on RDNA3. So that would be a big boost too,AMD,2025-09-19 07:50:30,2
AMD,nexfkvo,"Thanks. Yours had a special mode disabling EDID override, which I now need to make part of the workflow.",AMD,2025-09-18 18:02:31,10
AMD,nf3np10,The WoW driver issue on 7xxx series isn't new. The game has been spontaneously crashing in dungeons for the year I've been back to playing it.,AMD,2025-09-19 17:00:33,1
AMD,nf5d43z,"Nah, I'll do someone's earlier comment and just use the clean uninstall on the new driver download",AMD,2025-09-19 22:09:06,1
AMD,nf2ky4j,"True, I didn't think of it that way. Professional GPU generations typically have longer periods between new releases I'm pretty sure, so bins that don't work for that use case could also be the reason for this release",AMD,2025-09-19 13:53:13,1
AMD,nexygoq,"I imagine AMD can get more profit from selling a 7800 XT for 450-500 than a 9060 XT for 370, despite the pricier node. They also may not have the supply to make 9060 XT's to satisfy the entire global market.  This card, however, has the same hardware cost of the 7800 XT while performing slower than the 9060 XT 16GB. So it's likely only being made due to defective dies.",AMD,2025-09-18 19:33:23,2
AMD,neyjs82,"We automatically register with LVR.  You don't need to do anything for the WMR HMD, SteamVR will should see it as any other HMD today.  However, OS is _not_ aware of the display since, like all LVR HMDs, it is hidden from the OS (not that SteamVR cares, they use LVR).",AMD,2025-09-18 21:14:37,11
AMD,nfcy62z,"Only issue I’ve had with WoW was when using RSR and alt tabbing between windows. However I’m still using 6900XT. Waiting for AMD to release something a bit more high end GPU wise to make the leap to AM5. My Nitro 6900 XT SE has still been keeping up with most games on ultra/high graphics, occasionally taking advantage of Optiscalar/DLSS swapper to squeeze a bit more juice out of it. The 9070 XT seems like a good first release with UDNA cores, now it’s time to see how far they can push their tech with their next major GPU release ;) Tempted to start installing driver without software to see if it helps avoid the extra complications that many experience.",AMD,2025-09-21 03:21:20,1
AMD,neynbg0,"Thanks. That explains the new behavior that users are seeing, where the initial step (unlocking) fails to see the device entirely. I need to make that step conditional to GPU. There's another possible issues, where some headsets require a USB command to power the display on. Right now, I block vrcompositor from starting until I can enumerate the device (using NVAPI). unless I can somehow get the proper LVR API to enumerate D2D devices, I will need a timer or something arbitrary.",AMD,2025-09-18 21:32:39,11
AMD,nf17xgy,"I was able to confirm this behavior, thanks. I need to coordinate another change with Valve and some slight tuning in my driver, but with that it should work in the end!",AMD,2025-09-19 07:42:43,7
AMD,nfb2o2j,"Hi u/AMD_Aric (and u/AMD_Vik),  I was able to use driver 25.9.2 with some headsets successfully. However I have had no luck with the HP Reverb G1. SteamVR crashes immediately inside of LVR, and that is even before my Oasis driver is loaded.  (Edited: Corrected, only the HP Reverb G1 seems to have issues so far. HP Reverb G2 appears to work)  Here is a core dump: [https://drive.google.com/file/d/1uXePxj7YZ6mC0XjwkBBGaLcRh17QVHaw/view?usp=sharing](https://drive.google.com/file/d/1uXePxj7YZ6mC0XjwkBBGaLcRh17QVHaw/view?usp=sharing)  The error that is printed on the console:  `2 [VulkanLoaderFunctionTable]   Error: C:\constructicon\builds\gfx\one\25.10\drivers\liquidvr\private\impl\src\mantle_dyn.cpp(1451):alvr::VulkanLoaderFunctionTable::CreateVulkanDevice MANTLE_ERROR(0x3ba20978) vkCreateDevice() failed`  While I don't have access to the D2D API, I have traced it to ""ALVRDisplayFactory::CreateManager()"".  ~~Note that the HP Reverb & Reverb G2 use a ""delayed"" power up sequence, where a USB command needs to be sent in order to fully power up the display. I am not sure whether that is related. I attempted to send the command long before LVR is initialized, but the issue persists.~~ Edited: Not the issue, given that HP Reverb G2 was confirmed to work in the end.  Thank you.",AMD,2025-09-20 20:33:50,4
AMD,nfcek13,"Also, [u/AMD\_Aric](https://www.reddit.com/user/AMD_Aric/) and [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/) (sorry for back-to back messages), I wanted to report that outside of this crash with HP Reverb G1, I have now had several users reporting success with the updated AMD driver and the updated Oasis driver. Success with HP Reverb G2, Lenovo Explorer, Acer AH101 and Samsung Odyssey... So things are looking pretty good. Thank you for the support.",AMD,2025-09-21 01:12:47,5
AMD,nfm7qgx,"I see. Our LVR contact tells us:  > 0x3ba20978 (1000475000) means error: ""private display removed"".  Thanks for the core dump. Unsure as to why this specifically occurs with the G1; we'll need to take a look at this and try to reproduce with this headset internally (I'm not sure we have the G1 so may need to look into procuring one).",AMD,2025-09-22 15:44:48,2
AMD,ngieos7,And largely against the non-x3d lmfao.,Intel,2025-09-27 17:21:03,77
AMD,ngif1q6,Aren't they just showing that AMDs CPUs are better for gaming?,Intel,2025-09-27 17:22:52,26
AMD,ngmmadi,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Intel,2025-09-28 10:33:19,2
AMD,ngiqxv3,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Intel,2025-09-27 18:23:57,-16
AMD,ngp4a4b,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Intel,2025-09-28 18:56:21,0
AMD,nglqard,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Intel,2025-09-28 05:26:07,29
AMD,ngiw9gz,I assume they compared with CPUs in a similar price range,Intel,2025-09-27 18:52:05,30
AMD,ngl774g,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Intel,2025-09-28 02:59:30,4
AMD,ngj2d2a,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Intel,2025-09-27 19:24:33,47
AMD,ngmt8qo,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Intel,2025-09-28 11:35:30,10
AMD,ngn0xy1,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Intel,2025-09-28 12:33:09,1
AMD,ngix2qg,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Intel,2025-09-27 18:56:22,32
AMD,ngiy5wa,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Intel,2025-09-27 19:02:07,8
AMD,ngir4n9,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Intel,2025-09-27 18:24:57,3
AMD,ngj6exq,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Intel,2025-09-27 19:46:01,3
AMD,nglqeum,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Intel,2025-09-28 05:27:08,2
AMD,ngkj8l9,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Intel,2025-09-28 00:25:26,13
AMD,ngkwq9d,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Intel,2025-09-28 01:51:37,17
AMD,ngmy2qz,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Intel,2025-09-28 12:12:58,2
AMD,ngmx0bu,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Intel,2025-09-28 12:05:06,2
AMD,ngn1g9m,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Intel,2025-09-28 12:36:42,2
AMD,ngiyobq,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Intel,2025-09-27 19:04:51,-5
AMD,ngizavn,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Intel,2025-09-27 19:08:11,5
AMD,ngjh5f6,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Intel,2025-09-27 20:42:11,4
AMD,ngm7i8q,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Intel,2025-09-28 08:05:53,5
AMD,ngkykmi,That sounds like an AMD Stan argument circa 2020,Intel,2025-09-28 02:03:12,16
AMD,nh261w2,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Intel,2025-09-30 19:23:27,1
AMD,ngn25ws,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Intel,2025-09-28 12:41:35,0
AMD,ngjdprn,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Intel,2025-09-27 20:24:13,11
AMD,ngj1pgu,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Intel,2025-09-27 19:21:01,7
AMD,ngkqt0z,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Intel,2025-09-28 01:12:59,1
AMD,ngj49y7,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Intel,2025-09-27 19:34:44,3
AMD,ngk1b4v,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Intel,2025-09-27 22:35:49,3
AMD,ngmkwf8,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Intel,2025-09-28 10:19:51,8
AMD,ngnw7zu,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Intel,2025-09-28 15:28:44,3
AMD,ngl05xq,Expand ?,Intel,2025-09-28 02:13:17,-2
AMD,nh27g09,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Intel,2025-09-30 19:30:10,1
AMD,ngtvgdv,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Intel,2025-09-29 14:03:26,4
AMD,ngw45vk,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Intel,2025-09-29 20:34:34,-1
AMD,ngjg69y,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Intel,2025-09-27 20:37:03,7
AMD,ngqfmrw,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Intel,2025-09-28 22:53:49,1
AMD,ngk26op,did you do it,Intel,2025-09-27 22:41:09,1
AMD,ngmi765,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Intel,2025-09-28 09:53:01,1
AMD,ngk22ke,can you reset settings then choose ray tracing ultra preset.,Intel,2025-09-27 22:40:28,2
AMD,ngnx0bd,because they exclusively exist in DIY build your pc enthusiast bubble,Intel,2025-09-28 15:32:30,4
AMD,ngl3zfu,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Intel,2025-09-28 02:38:06,12
AMD,nh2848m,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Intel,2025-09-30 19:33:29,1
AMD,ngtxj1d,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Intel,2025-09-29 14:14:15,2
AMD,ngmif3t,"Okay, I did it",Intel,2025-09-28 09:55:13,2
AMD,ngmglra,"No, I didn’t remember good",Intel,2025-09-28 09:37:10,1
AMD,ngk41a9,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Intel,2025-09-27 22:52:22,2
AMD,nh2a34y,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Intel,2025-09-30 19:43:15,1
AMD,ngk5zrq,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Intel,2025-09-27 23:04:17,2
AMD,ngbpsza,Cam someone confirm or is this gas lighting?,Intel,2025-09-26 15:51:04,20
AMD,ngbym0c,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Intel,2025-09-26 16:33:40,16
AMD,nghesqk,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Intel,2025-09-27 14:17:43,4
AMD,ngc7w1b,Intel comeback real?,Intel,2025-09-26 17:18:16,7
AMD,ngf1ik5,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Intel,2025-09-27 02:43:51,2
AMD,ngbsck8,3D v-cache has entered the chat.,Intel,2025-09-26 16:04:06,8
AMD,ngbr9eb,Take it as a grain of salt. Intel marketing LOL,Intel,2025-09-26 15:58:30,1
AMD,ngfrla1,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Intel,2025-09-27 06:09:56,0
AMD,ngfguk7,Thats cool ...but lets talk about better pricing.,Intel,2025-09-27 04:36:12,-1
AMD,ngc573e,Tech Jesus has entered chat :).,Intel,2025-09-26 17:05:31,-10
AMD,ngcjbbq,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Intel,2025-09-26 18:13:09,-11
AMD,ngbqjhe,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Intel,2025-09-26 15:54:51,37
AMD,ngdvx9l,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Intel,2025-09-26 22:23:43,11
AMD,nge3sfi,What do you mean by gaslighting in this case?,Intel,2025-09-26 23:10:51,4
AMD,ngcf9aj,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Intel,2025-09-26 17:53:19,1
AMD,ngcutw5,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Intel,2025-09-26 19:10:03,1
AMD,ngl3adb,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Intel,2025-09-28 02:33:31,1
AMD,ngdfut5,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Intel,2025-09-26 20:55:42,1
AMD,ngbxbws,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Intel,2025-09-26 16:27:28,-7
AMD,ngfqkoh,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Intel,2025-09-27 06:00:33,2
AMD,ngfebe1,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Intel,2025-09-27 04:16:03,2
AMD,ngfqbry,Nova Lake bLLC about to ruin Amd X3D party.,Intel,2025-09-27 05:58:18,1
AMD,ngc2ju0,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Intel,2025-09-26 16:52:57,19
AMD,ngc2czl,I always wondered if Intel marketing budget is higher than the R&D budget,Intel,2025-09-26 16:52:01,-6
AMD,ngfrkpn,Intel Arrow Lake is much cheaper than Amd Zen 5.,Intel,2025-09-27 06:09:47,5
AMD,ngch561,>Tech Jesus has entered chat :).  He kind of lost that title a couple years ago. His channels stopped being about technology and just became corporate consumer activism. He hasn't had a legit unbiased review in ages.,Intel,2025-09-26 18:02:24,7
AMD,ngemp1j,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Intel,2025-09-27 01:07:45,6
AMD,ngeo8em,only an AMD fan would worry about replacing their shit CPUs under 3 years,Intel,2025-09-27 01:17:36,0
AMD,ngchr9a,">I dunno if it is worth the time investment for some reviewer like TPU or HWUB  Here is the rub.   It's ABSOLUTELY worth their time from a consumer standpoint. If educating viewers on products was a priority for them they would do this in a heartbeat.  It's NOT worth their time from a revenue-generation aspect. Youtube's algorithm heavily punishes reviews of ""old products"", so any re-review or revisit of something will not perform well and HUB knows this.     So what is HUB's priority, educating viewers with valuable information or making money?  The fact that they almost NEVER re-visit products gives you your answer.",Intel,2025-09-26 18:05:25,24
AMD,ngbzwzr,Hardware unboxed isn't a reliable source.,Intel,2025-09-26 16:40:06,8
AMD,ngf1ob8,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Intel,2025-09-27 02:44:54,6
AMD,ngealuz,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Intel,2025-09-26 23:51:44,1
AMD,nge8xbh,Telling people that its performance is better than it actually is?,Intel,2025-09-26 23:41:39,3
AMD,ngca7el,The ones with similar pricing not performance,Intel,2025-09-26 17:29:11,6
AMD,ngigkrj,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Intel,2025-09-27 17:30:41,2
AMD,ngfrgqn,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Intel,2025-09-27 06:08:45,0
AMD,ngerbdr,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Intel,2025-09-27 01:37:32,0
AMD,ngezf04,Quite common for AM4 in my experience.,Intel,2025-09-27 02:29:45,1
AMD,ngihhii,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Intel,2025-09-27 17:35:17,1
AMD,ngerg53,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Intel,2025-09-27 01:38:23,0
AMD,nggftxh,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Intel,2025-09-27 10:08:25,-1
AMD,ngeozwu,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Intel,2025-09-27 01:22:31,-2
AMD,ngg1fuo,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Intel,2025-09-27 07:42:27,-1
AMD,ngerrz8,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Intel,2025-09-27 01:40:28,13
AMD,ngeao9a,Sooo they are in the YouTube space for the money not for the love of tech,Intel,2025-09-26 23:52:08,4
AMD,ngfq1bg,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Intel,2025-09-27 05:55:39,2
AMD,ngdp9bd,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Intel,2025-09-26 21:45:51,-2
AMD,ngc0yus,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Intel,2025-09-26 16:45:15,10
AMD,ngeb3z7,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Intel,2025-09-26 23:54:48,8
AMD,ngmlzg8,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2025-09-28 10:30:22,1
AMD,ngtp3t3,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Intel,2025-09-29 13:28:22,1
AMD,ngf9mk9,">They frequently revisit products and test old hardware  No, they don't. On their Q&A videos Steve has explicitly said they don't do this because it doesn't make enough money and ""they have to whatever makes the most money""",Intel,2025-09-27 03:40:40,5
AMD,ngc36bx,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Intel,2025-09-26 16:55:54,4
AMD,ngcbde9,Sure but charts seem about right to me,Intel,2025-09-26 17:34:45,1
AMD,ngci7en,">  Incorrect, the biggest improvements have been on the software side - APO with the software companies and scheduling with Microsoft.",Intel,2025-09-26 18:07:39,3
AMD,ngcxbac,APO is game specific. I'm referring to what has changed overall.,Intel,2025-09-26 19:22:34,4
AMD,nh3mfpi,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-09-30 23:57:22,1
AMD,nfolbmr,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Intel,2025-09-22 23:20:45,51
AMD,nflslxh,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Intel,2025-09-22 14:31:25,42
AMD,nfm76rf,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Intel,2025-09-22 15:42:11,33
AMD,nfnaznn,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Intel,2025-09-22 18:52:47,10
AMD,nfpb0n3,"i think it's bad for us, consumers",Intel,2025-09-23 02:12:30,3
AMD,nflx3jg,Was the team up really to crush AMD or Nvidia's answer to enter China?,Intel,2025-09-22 14:53:26,8
AMD,ng4ik7u,AMDware unboxed only cares about AMD anyway,Intel,2025-09-25 13:37:58,3
AMD,nfm1wz0,This hurts the arc division way more than this could ever hurt amd.,Intel,2025-09-22 15:16:45,14
AMD,nfoivfo,They will crush user's wallet,Intel,2025-09-22 23:04:45,2
AMD,nftk1b6,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Intel,2025-09-23 19:23:17,2
AMD,nfv8a1x,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Intel,2025-09-24 00:40:38,1
AMD,ng0xe2q,Remember Kaby Lake G? No? This will also be forgotten soon.,Intel,2025-09-24 22:04:55,1
AMD,ng2ck7r,Yes.,Intel,2025-09-25 03:11:04,1
AMD,ngzb138,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Intel,2025-09-30 09:53:37,1
AMD,nfmdikv,AMDUnboxed on suicide watch.,Intel,2025-09-22 16:12:26,0
AMD,nfn7em1,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Intel,2025-09-22 18:34:17,1
AMD,nfmh1rz,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Intel,2025-09-22 16:29:30,1
AMD,nflsfzz,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Intel,2025-09-22 14:30:36,0
AMD,nfmihzp,Ohh noooerrrrrrrrr,Intel,2025-09-22 16:36:30,0
AMD,nfvhp2d,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Intel,2025-09-24 01:35:50,0
AMD,ngcdph8,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Intel,2025-09-26 17:45:54,0
AMD,nfnsqrd,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Intel,2025-09-22 20:29:14,-2
AMD,nfmr987,welcome to the Nvidia and amd duopoly,Intel,2025-09-22 17:17:38,-5
AMD,nfmy4sf,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Intel,2025-09-22 17:49:51,2
AMD,nfn29jk,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Intel,2025-09-22 18:09:19,-9
AMD,nfma1mz,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Intel,2025-09-22 15:55:41,4
AMD,ng59q6y,a partnership doesnt mean they get free reign over license lol,Intel,2025-09-25 15:49:13,2
AMD,nfodhll,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Intel,2025-09-22 22:29:59,-2
AMD,nfm0v8n,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Intel,2025-09-22 15:11:42,20
AMD,nfm237h,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Intel,2025-09-22 15:17:35,6
AMD,nfm66b6,Why would it?,Intel,2025-09-22 15:37:21,9
AMD,nfm5ru9,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Intel,2025-09-22 15:35:26,12
AMD,nfmevnp,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Intel,2025-09-22 16:19:02,2
AMD,nfm4tdl,Past != Future,Intel,2025-09-22 15:30:48,4
AMD,nfnb9ui,"nvidia also used to make motherboard chipset, with mixed success.",Intel,2025-09-22 18:54:18,2
AMD,nfnz3qn,Arc’s offerings aren’t far behind either.,Intel,2025-09-22 21:03:31,8
AMD,nfoib8h,FSR 4 looks like the later versions of Dlss 2 did,Intel,2025-09-22 23:01:07,4
AMD,nfo2bk7,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Intel,2025-09-22 21:21:54,2
AMD,ng9kwgg,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Intel,2025-09-26 07:01:28,0
AMD,nfmqv0q,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Intel,2025-09-22 17:15:46,6
AMD,nfoz7q1,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Intel,2025-09-23 00:52:30,0
AMD,ng6jjhd,Never said that.,Intel,2025-09-25 19:28:08,-1
AMD,nfm1xh4,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Intel,2025-09-22 15:16:49,14
AMD,nfm6de6,Why do so many people think that this will kill ARC?,Intel,2025-09-22 15:38:18,10
AMD,nfm8iqb,The market for Arc is the same as for Nvidia.,Intel,2025-09-22 15:48:31,9
AMD,nflxpfk,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Intel,2025-09-22 14:56:21,3
AMD,nfmpyxd,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Intel,2025-09-22 17:11:35,5
AMD,nfncgcw,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Intel,2025-09-22 19:00:37,2
AMD,nfpd2vm,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Intel,2025-09-23 02:26:29,1
AMD,nfq8t4o,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Intel,2025-09-23 07:05:35,-1
AMD,ng2cqnz,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Intel,2025-09-25 03:12:17,1
AMD,ng2dv4q,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Intel,2025-09-25 03:20:00,0
AMD,nfm6s3o,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Intel,2025-09-22 15:40:15,5
AMD,nfmtpsf,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Intel,2025-09-22 17:29:18,14
AMD,nfm7mbh,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Intel,2025-09-22 15:44:16,0
AMD,nfm91t2,Nvidia does not have an A310 competitor.,Intel,2025-09-22 15:51:01,-2
AMD,nftpodz,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Intel,2025-09-23 19:50:16,2
AMD,nfq8uoi,I have not lied,Intel,2025-09-23 07:06:03,0
AMD,ng2iom6,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Intel,2025-09-25 03:54:05,1
AMD,ng17you,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Intel,2025-09-24 23:06:01,1
AMD,nfonq2p,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Intel,2025-09-22 23:36:31,3
AMD,nfm9qnn,Intel doesn't have a current gen A310 competitor either.,Intel,2025-09-22 15:54:15,8
AMD,nfma2t0,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Intel,2025-09-22 15:55:51,0
AMD,nfm0jnd,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Intel,2025-09-22 15:10:08,2
AMD,nftspxa,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Intel,2025-09-23 20:04:43,2
AMD,nfrlmki,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Intel,2025-09-23 13:46:08,0
AMD,nfu9p3r,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Intel,2025-09-23 21:25:34,2
AMD,nfpz6gm,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Intel,2025-09-23 05:25:09,3
AMD,nfqz1sj,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Intel,2025-09-23 11:29:37,2
AMD,nfrltqc,The later versions of Dlss 2 look like Dlss 3,Intel,2025-09-23 13:47:11,2
AMD,nfq8fx8,Nvidia probably feels the same about their low end SKUs.,Intel,2025-09-23 07:01:31,1
AMD,ngn9bw4,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Intel,2025-09-28 13:27:33,1
AMD,nfrmbyw,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Intel,2025-09-23 13:49:53,0
AMD,nfq8hgn,Yeah lol,Intel,2025-09-23 07:02:00,1
AMD,nfrmiwb,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Intel,2025-09-23 13:50:54,2
AMD,nfrmnqg,"""Everyone I don't like is biased""-ass answer",Intel,2025-09-23 13:51:36,1
AMD,naz5fcr,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Intel,2025-08-27 16:46:20,47
AMD,nazj7k0,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Intel,2025-08-27 17:50:18,15
AMD,ncitd5t,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Intel,2025-09-05 08:24:57,1
AMD,naz95i7,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Intel,2025-08-27 17:04:24,1
AMD,naz6p5r,Just hodl until you get the biscuits,Intel,2025-08-27 16:52:25,38
AMD,naz7hoi,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Intel,2025-08-27 16:56:15,10
AMD,nazkvn7,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Intel,2025-08-27 17:57:46,6
AMD,nazb0q3,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Intel,2025-08-27 17:13:24,2
AMD,nb2ag3p,"why even panther laker when it was only for mobile, cancelled it and just released nova lake next year",Intel,2025-08-28 02:25:25,0
AMD,nazazfk,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Intel,2025-08-27 17:13:14,36
AMD,nb330i7,I thought Arrow Lake refresh was in the cards for 2025.,Intel,2025-08-28 05:57:34,1
AMD,nb04wuk,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Intel,2025-08-27 19:33:00,-2
AMD,nazl60x,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Intel,2025-08-27 17:59:03,6
AMD,nb8e4jm,This entire thing is a mobile roadmap so why are you here?,Intel,2025-08-29 00:35:26,2
AMD,nb1w9ji,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Intel,2025-08-28 01:01:42,3
AMD,nazbb9i,"Yeah if it's Surface roadmap, it's a nothing burger.",Intel,2025-08-27 17:14:47,8
AMD,nb02q5l,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Intel,2025-08-27 19:22:51,1
AMD,nb171ho,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Intel,2025-08-27 22:38:31,3
AMD,nb1xsvd,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Intel,2025-08-28 01:10:35,1
AMD,nb1vmub,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Intel,2025-08-28 00:58:05,3
AMD,n73y5u9,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Intel,2025-08-05 20:10:52,34
AMD,n73iurw,"I like how they just throw random words around to pad their ""article"".",Intel,2025-08-05 18:43:24,66
AMD,n73t59w,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Intel,2025-08-05 19:42:58,28
AMD,n741jok,"""leaks""",Intel,2025-08-05 20:28:54,7
AMD,n74lp88,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Intel,2025-08-05 22:16:47,5
AMD,n77fxeb,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Intel,2025-08-06 10:20:26,1
AMD,n7koh28,Scared of their rumor?  Lets release our rumor!,Intel,2025-08-08 09:59:01,1
AMD,n758aa3,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Intel,2025-08-06 00:22:30,1
AMD,n76sf5r,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Intel,2025-08-06 06:42:22,-2
AMD,n74sw92,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Intel,2025-08-05 22:56:59,-5
AMD,n75af32,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Intel,2025-08-06 00:34:46,4
AMD,n754qqc,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Intel,2025-08-06 00:02:20,9
AMD,n7582e3,bLLC is not stacked cache,Intel,2025-08-06 00:21:15,4
AMD,n767h6n,What do you consider random? The article was perfectly clear.,Intel,2025-08-06 03:54:03,10
AMD,n748zl3,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Intel,2025-08-05 21:08:09,14
AMD,n741l8k,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Intel,2025-08-05 20:29:08,2
AMD,n76jhxw,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Intel,2025-08-06 05:24:22,1
AMD,n7s4doe,Yeah. Definitely just you,Intel,2025-08-09 14:25:48,4
AMD,n757qwt,You could literally make that claim with any CPU performance increase.,Intel,2025-08-06 00:19:24,9
AMD,n7615sm,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Intel,2025-08-06 03:11:36,6
AMD,n77a60y,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Intel,2025-08-06 09:28:27,1
AMD,n76ahn8,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Intel,2025-08-06 04:15:00,5
AMD,n757yiy,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Intel,2025-08-06 00:20:37,1
AMD,n749ios,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Intel,2025-08-05 21:11:01,16
AMD,n76z9ct,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Intel,2025-08-06 07:45:28,2
AMD,n77cfm2,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Intel,2025-08-06 09:49:46,6
AMD,n77cdot,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Intel,2025-08-06 09:49:17,6
AMD,nam3zf1,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Intel,2025-08-25 17:06:43,0
AMD,n75cvtk,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Intel,2025-08-06 00:48:56,2
AMD,n7ddfzp,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Intel,2025-08-07 06:10:13,1
AMD,n7dcz9u,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Intel,2025-08-07 06:06:08,1
AMD,nax8p6j,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Intel,2025-08-27 10:26:21,1
AMD,n7ddyd1,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Intel,2025-08-07 06:14:41,1
AMD,n7deaek,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Intel,2025-08-07 06:17:37,1
AMD,n9hwp92,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Intel,2025-08-19 08:52:24,63
AMD,n9hupxb,It's not sorcery. Its just Intel doing the game developers work.,Intel,2025-08-19 08:32:04,80
AMD,n9i0i3y,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Intel,2025-08-19 09:30:05,7
AMD,n9ic94k,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Intel,2025-08-19 11:11:41,10
AMD,n9k23nq,Never count out Intel. They have some very talented people over there.,Intel,2025-08-19 16:43:12,8
AMD,n9hygne,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Intel,2025-08-19 09:09:52,8
AMD,n9qyqfh,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Intel,2025-08-20 17:38:05,2
AMD,n9tmgam,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Intel,2025-08-21 02:04:20,1
AMD,nab3aup,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Intel,2025-08-23 20:50:51,1
AMD,naohj1m,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Intel,2025-08-26 00:28:42,1
AMD,nattrzq,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Intel,2025-08-26 20:25:28,1
AMD,nd42fep,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Intel,2025-09-08 16:53:22,1
AMD,ndg3xrf,I will follow all!,Intel,2025-09-10 13:49:20,1
AMD,n9j60kj,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Intel,2025-08-19 14:10:38,-7
AMD,n9iw99o,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Intel,2025-08-19 13:18:38,11
AMD,n9ihm3d,How is this doing the game developers work?,Intel,2025-08-19 11:50:25,-5
AMD,n9imvpt,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Intel,2025-08-19 12:24:19,3
AMD,n9ipgu8,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Intel,2025-08-19 12:39:59,5
AMD,n9ktypv,The intel software team is pure black magic when they allowed to work on crack.,Intel,2025-08-19 18:54:19,3
AMD,n9iml8f,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Intel,2025-08-19 12:22:32,4
AMD,n9s6nj5,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Intel,2025-08-20 21:12:41,5
AMD,n9iyl6t,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Intel,2025-08-19 13:31:21,7
AMD,n9u57ww,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Intel,2025-08-21 04:07:48,4
AMD,na9spi1,Balanced in full load will just do the same thing as High Performance.,Intel,2025-08-23 16:43:30,1
AMD,n9lhg6u,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Intel,2025-08-19 20:46:34,20
AMD,nah6zpc,Or... Just process lasso.,Intel,2025-08-24 21:05:07,1
AMD,n9ij60y,Because it’s optimizations on how it can efficiently use the cpu.,Intel,2025-08-19 12:00:43,22
AMD,n9iv66n,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Intel,2025-08-19 13:12:40,1
AMD,n9iwird,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Intel,2025-08-19 13:20:05,4
AMD,n9mv7uv,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Intel,2025-08-20 01:17:18,4
AMD,n9iojt9,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Intel,2025-08-19 12:34:27,1
AMD,n9y05el,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Intel,2025-08-21 19:06:28,1
AMD,n9lzy5i,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Intel,2025-08-19 22:19:43,3
AMD,n9mhmhz,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Intel,2025-08-19 23:59:05,1
AMD,nah75n6,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Intel,2025-08-24 21:06:00,1
AMD,n9kb8x1,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Intel,2025-08-19 17:25:29,-4
AMD,n9ivrcr,You need to download the Intel Application Optimization app from the Windows store,Intel,2025-08-19 13:15:54,1
AMD,n9iy6ma,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Intel,2025-08-19 13:29:09,6
AMD,n9t6inr,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Intel,2025-08-21 00:31:46,1
AMD,n9xzseo,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Intel,2025-08-21 19:04:43,1
AMD,n9ipdm4,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Intel,2025-08-19 12:39:26,3
AMD,n9opxwe,Where to download this APO,Intel,2025-08-20 10:12:53,1
AMD,n9ktn8y,Except its THE game devs job to optimize games for multiple cpus and gpus.,Intel,2025-08-19 18:52:47,8
AMD,n9ks0ao,It’s literally their job to do so? wtf you talking about?,Intel,2025-08-19 18:45:02,3
AMD,n9mt1qu,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Intel,2025-08-20 01:04:38,1
AMD,n9iw056,"Will do, I got a 265K. Performance is already great tbh.",Intel,2025-08-19 13:17:15,1
AMD,n9iz3ih,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Intel,2025-08-19 13:34:10,4
AMD,n9irrhk,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Intel,2025-08-19 12:53:21,1
AMD,n9q1a42,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Intel,2025-08-20 14:59:31,2
AMD,n9kvap0,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Intel,2025-08-19 19:00:36,-4
AMD,n9p3czn,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Intel,2025-08-20 11:55:36,1
AMD,n9kt4hv,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Intel,2025-08-19 18:50:20,0
AMD,n9mx3nm,That's not simply what APO does.,Intel,2025-08-20 01:28:12,1
AMD,n9iwijq,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Intel,2025-08-19 13:20:03,2
AMD,n9j8cqp,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Intel,2025-08-19 14:22:18,3
AMD,n9iublo,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Intel,2025-08-19 13:07:57,6
AMD,n9pa899,i am not talking about older games. i am talking about newer games.... really dude?,Intel,2025-08-20 12:37:54,0
AMD,n9l8ne6,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Intel,2025-08-19 20:05:16,4
AMD,n9iyd7e,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Intel,2025-08-19 13:30:09,5
AMD,n9iuumd,Thanks a lot.,Intel,2025-08-19 13:10:53,5
AMD,n9pc7ju,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Intel,2025-08-20 12:49:28,2
AMD,n9lby71,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Intel,2025-08-19 20:20:50,3
AMD,n9pd885,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Intel,2025-08-20 12:55:15,0
AMD,n9lgtiq,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Intel,2025-08-19 20:43:35,2
AMD,nadrsxq,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Intel,2025-08-24 08:29:02,0
AMD,n9li080,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Intel,2025-08-19 20:49:11,0
AMD,n6bumwm,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Intel,2025-08-01 11:02:22,62
AMD,n6bnc4i,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Intel,2025-08-01 10:01:27,33
AMD,n6chmtn,Did the DP4A version also improve from 1.3 to 2.0?,Intel,2025-08-01 13:26:20,4
AMD,n6i4k9c,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Intel,2025-08-02 10:05:35,1
AMD,n6bj9ci,Okay but why would I want to use that instead of NVIDIA DLSS?,Intel,2025-08-01 09:23:32,-22
AMD,n6bwcya,It’s the least you should get after not getting FSR4.,Intel,2025-08-01 11:15:16,18
AMD,n6ck8qn,You'd use it over FSR if that's available too?,Intel,2025-08-01 13:40:07,3
AMD,n6i0iq2,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Intel,2025-08-02 09:25:17,1
AMD,n6c91ju,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Intel,2025-08-01 12:38:17,11
AMD,n6btrfp,And they expect people who bought previous RDNA to buy more RDNA,Intel,2025-08-01 10:55:34,4
AMD,n6nl7sh,Not by a significant amount.,Intel,2025-08-03 06:43:34,1
AMD,n6i0oud,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Intel,2025-08-02 09:27:02,0
AMD,n6bp1iu,for the games that dont support DLSS,Intel,2025-08-01 10:16:31,29
AMD,n6bul99,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Intel,2025-08-01 11:02:02,22
AMD,n6bqgih,10 series cards will benefit from this,Intel,2025-08-01 10:28:41,12
AMD,n6bnz56,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Intel,2025-08-01 10:07:08,1
AMD,n6cmxm3,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Intel,2025-08-01 13:54:01,17
AMD,n6d0adu,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Intel,2025-08-01 14:58:55,5
AMD,n6i1gij,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Intel,2025-08-02 09:34:56,2
AMD,n6emb0z,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Intel,2025-08-01 19:37:45,1
AMD,n6guokn,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Intel,2025-08-02 03:16:52,1
AMD,n6nl9o1,DP4a is cross vendor.   XMX is Arc only.,Intel,2025-08-03 06:44:04,3
AMD,n6bqj7p,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Intel,2025-08-01 10:29:20,-9
AMD,n6cx6cb,1080ti heard no bell,Intel,2025-08-01 14:44:10,3
AMD,n6i2ufc,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Intel,2025-08-02 09:48:51,1
AMD,n6r59ev,where did amd touch you bud?,Intel,2025-08-03 20:25:24,2
AMD,n6d2yn8,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Intel,2025-08-01 15:11:40,-3
AMD,n6f09jv,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Intel,2025-08-01 20:47:10,3
AMD,n6iu4q0,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Intel,2025-08-02 13:21:11,2
AMD,n6klfjc,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Intel,2025-08-02 19:05:15,23
AMD,n6q6qct,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Intel,2025-08-03 17:28:08,2
AMD,n7ht68l,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Intel,2025-08-07 21:45:26,1
AMD,n72229n,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Intel,2025-08-05 14:15:36,1
AMD,n7ewvah,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Intel,2025-08-07 13:19:41,2
AMD,nanq5zn,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Intel,2025-08-25 21:47:41,3
AMD,n7m0sli,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Intel,2025-08-08 14:57:07,1
AMD,n6m90qi,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Intel,2025-08-03 00:48:59,5
AMD,n6nytko,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Intel,2025-08-03 08:55:06,3
AMD,n6l6ttw,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Intel,2025-08-02 21:03:36,4
AMD,n7m8w5n,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Intel,2025-08-08 15:35:34,1
AMD,n8s3s3l,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Intel,2025-08-15 04:54:15,1
AMD,n6orqc9,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Intel,2025-08-03 12:59:19,6
AMD,n6qeres,Thank you :),Intel,2025-08-03 18:07:58,1
AMD,n4v3wzh,Wouldn't the 300 series actually be Arrow Lake Refresh?,Intel,2025-07-24 08:11:30,39
AMD,n4v46fh,"Videocardz is wrong      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Intel,2025-07-24 08:13:59,41
AMD,n4y92ft,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Intel,2025-07-24 18:56:57,12
AMD,n4w6o9h,This is pat's work,Intel,2025-07-24 13:07:04,15
AMD,n4v6nq7,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Intel,2025-07-24 08:37:59,6
AMD,n4vlaga,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Intel,2025-07-24 10:49:31,8
AMD,n4vi19u,Large L3 cache without reducing latency will be fun to watch.,Intel,2025-07-24 10:22:42,19
AMD,n4w07wg,Is Intel finally making a comeback with their cpus? I hope so,Intel,2025-07-24 12:30:48,11
AMD,n4v3ue7,the specs sure do shift a lot..,Intel,2025-07-24 08:10:48,3
AMD,n4v8k6w,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Intel,2025-07-24 08:55:55,4
AMD,n4z4yed,it's just a bunch of cores glued together - intel circa 2016 probably,Intel,2025-07-24 21:27:20,2
AMD,n5798px,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Intel,2025-07-26 02:39:20,2
AMD,n4w3arv,Bout time,Intel,2025-07-24 12:48:32,1
AMD,n4w52bq,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Intel,2025-07-24 12:58:18,1
AMD,n510yns,Will it be available in fall of this year or 26Q1?,Intel,2025-07-25 04:05:49,1
AMD,n525u2m,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Intel,2025-07-25 10:03:56,1
AMD,n54i8vo,Noob question:  Is this their 16th gen chips?,Intel,2025-07-25 17:44:39,1
AMD,n5c2bcs,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Intel,2025-07-26 21:56:10,1
AMD,n5r7teb,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Intel,2025-07-29 07:47:01,1
AMD,n5rui8a,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Intel,2025-07-29 11:14:18,1
AMD,n4yqz30,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Intel,2025-07-24 20:20:50,1
AMD,n55s05h,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Intel,2025-07-25 21:27:44,1
AMD,n4xrnev,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Intel,2025-07-24 17:35:44,-1
AMD,n4vu505,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Intel,2025-07-24 11:52:38,0
AMD,n4wlmkp,"So alder lake on cache steroids ?  If the heavier core count chips don’t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Intel,2025-07-24 14:23:50,0
AMD,n4vi1cf,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Intel,2025-07-24 10:22:43,-12
AMD,n50egti,"""E-Cores"",  Ewww, Gross.",Intel,2025-07-25 01:40:31,-2
AMD,n4y77tz,"**Hi,**  I’m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPU’s clearly starting to bottleneck a bit, but it’s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Intel,2025-07-24 18:47:57,0
AMD,n4vx5x8,NVL will definitely be the 400 series. PTL is 300.,Intel,2025-07-24 12:12:08,11
AMD,n4vqabe,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Intel,2025-07-24 11:26:19,25
AMD,n4x2p0n,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Intel,2025-07-24 15:43:01,7
AMD,n4vnjk4,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Intel,2025-07-24 11:06:28,43
AMD,n4vbjjr,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Intel,2025-07-24 09:24:22,9
AMD,n4vqnzd,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Intel,2025-07-24 11:29:01,13
AMD,n4xnlen,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Intel,2025-07-24 17:17:52,2
AMD,n4y6q9e,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Intel,2025-07-24 18:45:35,2
AMD,n9a1zq5,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Intel,2025-08-18 01:51:09,1
AMD,n54f312,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Intel,2025-07-25 17:30:13,1
AMD,n50iof9,More like *despite* him.,Intel,2025-07-25 02:05:32,-6
AMD,n4wx0ud,All the SKUs rumored so far are BLLC,Intel,2025-07-24 15:17:14,1
AMD,n4w5o0o,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Intel,2025-07-24 13:01:33,11
AMD,n4vwjon,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Intel,2025-07-24 12:08:14,-1
AMD,n51lyb6,Why not 8 P cores with HT + even more cache and no e cores at all,Intel,2025-07-25 06:56:21,-1
AMD,n4wel6l,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Intel,2025-07-24 13:49:07,15
AMD,n4w5tga,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Intel,2025-07-24 13:02:22,4
AMD,n4wwsgj,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Intel,2025-07-24 15:16:10,2
AMD,n8gsgik,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Intel,2025-08-13 13:37:20,2
AMD,n4xic5y,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Intel,2025-07-24 16:54:39,0
AMD,n51u6hg,Nova lake? More like 26Q4.,Intel,2025-07-25 08:13:29,1
AMD,n5272v5,Only Pantherlake for mobile,Intel,2025-07-25 10:15:01,1
AMD,n52fjpx,"It doesn’t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Intel,2025-07-25 11:22:11,2
AMD,n4yb3g9,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Intel,2025-07-24 19:06:33,1
AMD,n4vwh5f,It is really happening. The only question is when? Or can they release it on the next year without delay?,Intel,2025-07-24 12:07:46,1
AMD,n4wxgvv,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Intel,2025-07-24 15:19:15,3
AMD,n4wm6y8,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Intel,2025-07-24 14:26:35,1
AMD,n514twx,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Intel,2025-07-25 04:33:51,1
AMD,n5153vl,"E cores are the future, P cores days are numbered.",Intel,2025-07-25 04:35:54,6
AMD,n50iukn,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Intel,2025-07-25 02:06:36,2
AMD,n514kx3,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Intel,2025-07-25 04:32:00,1
AMD,n65knix,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.     You can definitely wait a bit.",Intel,2025-07-31 12:35:52,1
AMD,n4vw5ub,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Intel,2025-07-24 12:05:46,10
AMD,n4vrhxo,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Intel,2025-07-24 11:34:53,18
AMD,n588bec,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Intel,2025-07-26 07:21:16,2
AMD,n4xxr8n,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Intel,2025-07-24 18:03:15,3
AMD,n50iwkc,The former ceo,Intel,2025-07-25 02:06:56,3
AMD,n4xy0ww,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Intel,2025-07-24 18:04:31,3
AMD,n4x3wyl,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Intel,2025-07-24 15:48:35,8
AMD,n4wb0qc,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. You’re using dated info.,Intel,2025-07-24 13:30:18,12
AMD,n4xgvaw,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Intel,2025-07-24 16:48:02,6
AMD,n4vzlh2,No different to 12-14th gen then.,Intel,2025-07-24 12:27:02,-2
AMD,n4wathc,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Intel,2025-07-24 13:29:14,-1
AMD,n51m5t6,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Intel,2025-07-25 06:58:12,3
AMD,n5267k5,E core is 30% faster than hyper threading,Intel,2025-07-25 10:07:14,1
AMD,n9ruz42,HT is worse than E cores,Intel,2025-08-20 20:15:01,1
AMD,n4wi0su,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Intel,2025-07-24 14:06:24,5
AMD,n5m42rc,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Intel,2025-07-28 14:26:44,1
AMD,n4xxluj,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Intel,2025-07-24 18:02:33,3
AMD,n50irzt,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Intel,2025-07-25 02:06:09,2
AMD,n526u6b,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Intel,2025-07-25 10:12:51,1
AMD,n514fo0,ARM chips regularly do this sometimes on a yearly basis.,Intel,2025-07-25 04:30:55,0
AMD,n5maqpy,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Intel,2025-07-28 14:59:12,2
AMD,n6ijxrr,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Intel,2025-08-02 12:13:51,1
AMD,n4yg12q,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Intel,2025-07-24 19:29:54,-2
AMD,n4wewa9,"> They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Intel,2025-07-24 13:50:43,2
AMD,n4x5x1h,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Intel,2025-07-24 15:57:43,0
AMD,n5m9vma,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Intel,2025-07-28 14:55:00,1
AMD,n5awbw7,"Lmao, some of my games still runs on e cores",Intel,2025-07-26 18:06:33,1
AMD,n50x83s,People are still disabling e cores for more performance.,Intel,2025-07-25 03:39:29,1
AMD,n525z7c,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Intel,2025-07-25 10:05:10,1
AMD,n5m4vc2,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Intel,2025-07-28 14:30:40,2
AMD,n511bh4,fym no different they're 50% faster,Intel,2025-07-25 04:08:20,5
AMD,n5m5ymk,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Intel,2025-07-28 14:36:05,3
AMD,n528glp,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Intel,2025-07-25 10:27:04,1
AMD,n4wm3t2,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Intel,2025-07-24 14:26:09,5
AMD,n4wln0q,"As a advice, Nova Lake will further increase memory latency.",Intel,2025-07-24 14:23:54,-4
AMD,n50qnfg,In which gen iteration?,Intel,2025-07-25 02:55:31,0
AMD,n526q2r,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Intel,2025-07-25 10:11:49,1
AMD,n5m8bv4,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Intel,2025-07-28 14:47:35,1
AMD,n4xhpq9,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Intel,2025-07-24 16:51:53,1
AMD,n4xw2bt,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Intel,2025-07-24 17:55:25,9
AMD,n4zgeej,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Intel,2025-07-24 22:26:02,5
AMD,n52evsy,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Intel,2025-07-25 11:17:22,6
AMD,n588e12,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Intel,2025-07-26 07:21:58,2
AMD,n5266cc,They don't pay attention,Intel,2025-07-25 10:06:55,2
AMD,n5m6uax,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Intel,2025-07-28 14:40:21,1
AMD,n5dzs6f,That is more so bc the 9950x3d isn’t really a 16 core cpu it is two 8 cores,Intel,2025-07-27 05:47:15,1
AMD,n51sxrb,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Intel,2025-07-25 08:01:41,3
AMD,n50ijg8,How do you think they're doing 2 compute tiles if the memory controller is on one?,Intel,2025-07-25 02:04:42,2
AMD,n4wml1q,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Intel,2025-07-24 14:28:27,3
AMD,n5m2xjb,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Intel,2025-07-28 14:21:04,2
AMD,n528fgg,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Intel,2025-07-25 10:26:47,3
AMD,n5m9sth,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Intel,2025-07-28 14:54:37,1
AMD,n4xx7gl,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Intel,2025-07-24 18:00:40,2
AMD,n51508q,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Intel,2025-07-25 04:35:11,1
AMD,n526cpd,That didn't stop Intel with N3B,Intel,2025-07-25 10:08:30,0
AMD,n55ndnc,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Intel,2025-07-25 21:04:09,-1
AMD,n5qxo88,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Intel,2025-07-29 06:12:03,1
AMD,n80drlt,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Intel,2025-08-10 21:49:03,1
AMD,n4yjbs7,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Intel,2025-07-24 19:45:28,0
AMD,n55qjrv,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix the problem. It doesn't happen very often but when it does it's very annoying.",Intel,2025-07-25 21:20:18,5
AMD,n4z1sm2,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Intel,2025-07-24 21:11:47,2
AMD,n4z2fz2,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Intel,2025-07-24 21:14:56,1
AMD,n526jxa,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Intel,2025-07-25 10:10:18,1
AMD,n4znijc,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Intel,2025-07-24 23:05:11,0
AMD,n53d0lp,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Intel,2025-07-25 14:33:18,0
AMD,n3haicz,Will it also introduce Lunar Lake successor?,Intel,2025-07-16 17:36:05,15
AMD,n3htfns,I wonder if this could become avaliable on LGA 1954 because that'd be a good alternative for budget builds potentially if the iGPU is potent enough.,Intel,2025-07-16 19:02:16,8
AMD,n3h5hhc,Is anyone left?,Intel,2025-07-16 17:13:41,22
AMD,n3hw0vs,I would love to see a Nova Lake-A (strix halo like) APU with an 8+16 tile + 20Xe3 cores which is equal to 2560 FP32 lanes or 40 AMD CU's with 16-32mb of memory side cache to handle igpu bandwidth demands,Intel,2025-07-16 19:14:21,3
AMD,n3h8963,"Wonder what node such a product, if it does exist, would use for the iGPU tile.   AFAIK, rumor is that the iGPU tile for the standard parts would be on 18A or an 18A variant. However, if this is a flagship part, I would assume they would be willing to pay the extra cost to use N2...   However that would also presumably involve porting the architecture over to another node, which I don't think Intel has the resources, or wants to go through the effort, to do so.   Also wonder if such a product will finally utilize the ADM cache that has been rumored since like, forever.   And this is a bit of a tangent, but I swear at this point in the leak cycle for a new Intel product, there's always usually a ton of different skus that are leaked that may or may not launch - right now we have NVL bLLC, NVL standard, and now this, and a bunch of them end up not launching (whether they were cut internally, or the leaks were just made up). So it would be pretty interesting to see if any of the more specialized rumored NVL skus end up actually launching.",Intel,2025-07-16 17:26:01,5
AMD,n3hcp5u,These designs are going to be the go-to for creator and mid-range gaming at some point in the future. Having it all integrated together should provide opportunities for optimization and form factors that are difficult with discrete cards,Intel,2025-07-16 17:45:45,6
AMD,n3jhgvd,How long do I have to wait for something that is more powerful than an AMD 7840HS and still cheaper?,Intel,2025-07-16 23:53:49,1
AMD,n3k7s38,Good to hear! I have a strix halo asus tablet the 390 one and its only got 32GB ram but its fast!  Would love to see an intel version…,Intel,2025-07-17 02:31:14,1
AMD,n3mdvwq,"This is great move by Intel since they aren't making dGPU for laptop so they can get some marketshare by making strong Intel ecosystem. If Nova Lake AX released then people don't need to buy overpriced laptop with Nvidia dGPU anymore, not to mention with Nvidia shitty small vram.",Intel,2025-07-17 12:44:29,1
AMD,n4c03u2,"I'm still using Alder Lake, I suppose all these chips will be released in 2026 ?",Intel,2025-07-21 12:54:03,1
AMD,n3huqus,That's Panther Lake in a few months,Intel,2025-07-16 19:08:25,11
AMD,n3ik298,Lunar lake was always confirmed even by Intel as a 1 off design with soldered memory and many of its design choices.,Intel,2025-07-16 21:05:21,8
AMD,n3lcumc,"No, the memory config wouldn't work.",Intel,2025-07-17 07:51:04,1
AMD,n3h7xps,No not really.  It's pretty f'n bleak atm.,Intel,2025-07-16 17:24:37,16
AMD,n3i46cl,"That isn't that different from the AI MAX 395+ already released, which has 16 Zen 5 cores and 40CUs and some MALL cache (I think 20MB) to help with bandwidth.  For this to be a meaningful upgrade, you'd want more than 20 Xe3 cores, which means you need more bandwidth, which means you need probably LPDDR6 memory to get it high enough as otherwise the cache demands would be prohibitive and LPDDR6 memory isn't coming to client until probably 2027.",Intel,2025-07-16 19:53:15,7
AMD,n3h9kp1,It's N2.  Sadly Intel Inside has almost entirely become TSMC Inside,Intel,2025-07-16 17:31:54,5
AMD,n3l4fgi,"Nova Lake-AX will not be released, and DLLC will not be released, and nova Lake will not be released.",Intel,2025-07-17 06:35:04,-1
AMD,n3irdqf,"Unless they can find some way to bring down the cost.....these aren't gonna get much traction, the same applies to Strix Point of course. While more efficient than your typical 2000$ laptop with a decent GPU.....the traditional laptop is still gonna outperform them based on pure specs.  You get more battery hour and less heat but people who use these kind of laptop are used to having it plugged 24/7. Feels more like experimental models but they have a long way to go before they see proper adoption. Particularly the cost.",Intel,2025-07-16 21:39:27,6
AMD,n3hywlr,"> mid-range gaming  Strix halo is only in $2000 systems. high end price, mid-range performance.  Good for stock margins i guess.",Intel,2025-07-16 19:27:56,5
AMD,n3k42qb,I'm sure that Xe2 in Lunar Lake is faster than the 7840HS iGPU on average if you run it with the max performance mode in the OEM software that comes with your laptop.,Intel,2025-07-17 02:08:16,7
AMD,n3k6ewf,The 7850HS can't compete with the Xe2 igpu because it's bandwidth starved.   The 8mb of memory side cache insulating the 4mb of L2 from main memory allows the Xe2 Arc 140V igpu in LL clocked at 1950mhz to beat the RDNA 3.5 890m clocked at 2950mhz as it only has 4mb of L2 as a last level of cache before hitting memory.   TLDR: RDNA 3.5 is bandwidth starved on strix point due to lacking memory side cache,Intel,2025-07-17 02:22:43,3
AMD,n3jnkuo,PTL's not really a LNL successor.,Intel,2025-07-17 00:29:37,2
AMD,n48vx1p,"Yeah, it was just to prove a point that ARM is overrated.",Intel,2025-07-20 22:58:27,1
AMD,n3h8tf7,"I hear you, questioning my decision to return under Pat’s hire-back spending spree. Dodged this one… but this is hitting differently.",Intel,2025-07-16 17:28:32,12
AMD,n3k5a4s,What about an 8 + 16 big LLC tile and 32Xe3 cores (4096 FP32 lanes or ~60 AMD CU's) + 64mb of memory side cache?   Or even better a mid range part with an 8+16 tile and 16-20Xe3 cores + 16-32mb of memory side cache,Intel,2025-07-17 02:15:39,1
AMD,n4sfvrm,Just read about the JEDEC spec for lpddr6 of 14400mt/s.  That is wild!,Intel,2025-07-23 21:35:03,1
AMD,n3kgv3j,Who said it's It's entirely TSMC ? they have been pretty open about majority of tiles Intel the CPU Tile will be shared with N2 ofc though,Intel,2025-07-17 03:30:53,6
AMD,n3jgnpe,"It should be Intel here, TSMC there.  Or a little bit of Intc, a little bit of TSMC.",Intel,2025-07-16 23:49:16,4
AMD,n3hafqh,IDM 2.0: Where we proudly declare our fabs are world-class—while quietly handing the crown jewels to TSMC.,Intel,2025-07-16 17:35:45,2
AMD,ngib62p,Source?,Intel,2025-09-27 17:03:09,2
AMD,n3jfseq,"The BOM is lower, so the question is where the markup is coming from.",Intel,2025-07-16 23:44:28,2
AMD,n3k5vvd,Consoles have big APUs and they have better value than anything. I know they're subsidized however.,Intel,2025-07-17 02:19:23,2
AMD,n3k8ay7,A mid range Nova Lake-A SKU with a 6+12 tile and 16/20Xe3 cores with 16-32mb of memory side cache would be sick,Intel,2025-07-17 02:34:31,2
AMD,n3khaz8,Thats the problem with AMD sticking an additional 8 cores instead of infinity cache to fix the bandwidth bottleneck.,Intel,2025-07-17 03:33:57,1
AMD,n3k63l5,>still cheaper,Intel,2025-07-17 02:20:44,1
AMD,n3k776n,"It's the closest that we will get to a Lunar Lake successor   Really, panther lake is a combined successor to both Arrow Lake-H and Lunar Lake",Intel,2025-07-17 02:27:36,10
AMD,n3ugqr7,"Low volume and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference.  Combine that with not moving a lot of them compared to the rest of the lineup, and each one of those units has to make up more of the costs from things like bad dies or general spin up costs for a new chip.",Intel,2025-07-18 16:50:56,1
AMD,n3l78dq,It already has 32MB infinity cache.,Intel,2025-07-17 06:59:18,2
AMD,n3k81ho,The 7840HS is cheaper because it is older.,Intel,2025-07-17 02:32:52,6
AMD,n3lcowt,"It competes in the U/H lanes, so what is today ""ARL""-U and ARL-H. But yes, in practice it should bring *most* of the LNL goodness over, but PTL-U is still not a true successor in the original ~10W envelope LNL was designed for.",Intel,2025-07-17 07:49:34,1
AMD,n3ut4ta,"> and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference   It's no bigger than the equivalent dGPU, and you save on memory, package, and platform costs. Half the point of these things is to use the integrated cost advantage to better compete with Nvidia dGPUs.",Intel,2025-07-18 17:47:58,1
AMD,n3kh6ij,The new Ryzen AI 250/260 with 780M is still cheaper because it doesn't have copilot+ .,Intel,2025-07-17 03:33:05,-1
AMD,n3levrg,I will see the performance envelope of PTL U and decide should dump my LNL or not,Intel,2025-07-17 08:10:07,3
AMD,n3uubsu,"The low volume part of that quote is important. If you're making heaps of them, each can take up a little bit of the startup costs for that design. If you only make a few, those costs get concentrated on what you do make.  Low volume also means a single defect is promotionally more of your possible units, and hits the margins harder. You do get more total defects in a larger run, but at sufficiently high quantities of those defects, you start making lower bins to recover some of the defects. If you hardly make the volume to cover one widespread SKU, you aren't likely to have enough to make a cheaper one with any degree of availability. At some point you hit a threshold where it's not worth selling what would be a lower bin because there's not enough of them.",Intel,2025-07-18 17:53:36,1
AMD,n3ki3cu,Because putting a 40 TOPs NPU for the Copilot+ certification is costly.,Intel,2025-07-17 03:39:32,6
AMD,n3l7fsl,"It doesn't have a 40 TOPS NPU, it's the same 16TOPS one in the 8840HS it's literally just another rebrand. It's cheap because it's from 2023.",Intel,2025-07-17 07:01:04,1
AMD,n3laz95,Exactly. Hence your Xe2 lunar lake being faster is irrelevant to the question because it won't be cheaper regardless of age when they have a NPU that takes close to 1/3 of die space.,Intel,2025-07-17 07:33:37,1
AMD,n3l7mu1,I think I didn't say anything that deviates from what you just said.,Intel,2025-07-17 07:02:48,1
AMD,n3lach0,"Maybe I misread or misunderstood, I thought you said the 260/250 had a 40TOPS NPU",Intel,2025-07-17 07:27:39,1
AMD,n1d5sl1,enjoy rhythm aware outgoing practice bike attempt library versed cake   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Intel,2025-07-04 20:26:29,20
AMD,n1d44wc,Haha wow that is hilariously bad.   And here I thought ASUS newer bios update was bad due to some higher temps,Intel,2025-07-04 20:17:30,16
AMD,n1hmxgn,"The new Gigabyte BIOS also has booting issues for me, and memory is even more unstable.  At least the UI is entirely in English now.",Intel,2025-07-05 16:10:42,3
AMD,n1je3rl,"who needs quality control, what can go wrong?",Intel,2025-07-05 21:53:07,3
AMD,n1fn5q9,"This is why you don't rush to update software, let others do the testing for two weeks",Intel,2025-07-05 07:05:46,3
AMD,n4zj8x9,Gigabyte is trash,Intel,2025-07-24 22:41:41,1
AMD,n1j4fq9,The Elon Musk method,Intel,2025-07-05 20:58:12,4
AMD,n2m6czi,"> “Yeah just push it to production nothing’s gonna happen, no need to QA/QC this”  Seems to be the motto of a lot of the tech world",Intel,2025-07-11 20:40:54,1
AMD,n25f2x8,"Yup indeed.   I would even go as far as one full month   Hence, why I learned to avoid installing new Windows Update automatically",Intel,2025-07-09 10:28:21,1
AMD,mzzaf4q,Intel really needs to be able to compete with X3D or they're going to continue getting dominated in the enthusiast consumer market. I like Intel CPUs and was happy with my 12600K for awhile but X3D finally swayed me to switch over.,Intel,2025-06-27 00:15:57,64
AMD,n023uge,"Intel has had plans for big ass L4 cache for almost a decade now, just that it never made it past the design board.  Supposed to be marketed as Adamantium. But it got ZBB’d every time I suppose due to cost.  For Intel to implement Adamantium, regular manufacturing yield has to be good enough I.e cost is low so they can splurge on L4.  Of course now they are forced to go this way irrespective of cost. I’d love 16p + L4 CPU.",Intel,2025-06-27 13:03:00,11
AMD,mzzyzii,"Honestly, good. I've been using AMD for a while now but we need healthy competition in the CPU space for gaming otherwise AMD will see a clear opportunity to bring prices up",Intel,2025-06-27 02:44:24,13
AMD,mzz7wta,"Something interesting is that the extra cache isn't rumored to be on a base tile (like it is with Zen 5X3D), but rather directly in the regular compute tile itself.   On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.   I think Intel atp desperately needs a X3D competitor. Their market share and especially revenue share in the desktop segment as a whole has been ""cratering"" (compared to how they are doing vs AMD in their other segments) for a while now...",Intel,2025-06-27 00:01:01,16
AMD,n029b76,Hasn’t this been on their roadmap for a while now? I’m pretty sure they said 2027 is when they’ll have their version of x3D on the market,Intel,2025-06-27 13:33:26,3
AMD,mzz8z0y,"These core count increases could be a godsend at the low end and in the midrange. If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming, Intel will have a massive advantage on price.  That being said, if leaked Zen 6 clocks (albeit they’re from MLID, so should be taken with a grain of salt) are accurate, Nova Lake could lose to vanilla Zen 6 in gaming by a solid 5-10% anyway.",Intel,2025-06-27 00:07:18,7
AMD,n07rvul,"Funny how non of this news posted on reddit hardware sub or even allowed to be posted. Guest what? R amdhardware will always be amdhardware! It's painfully obvious that unbearable toxic landfills sub is extremely biased to Amd. Meanwhile all Intel ""bad rumors"" got posted there freely which is really BS!  I still remember i got banned from that trash sub for saying ""People need to touch grass and stop pretending like AMD is still underdog because they aren't"" and the Amd mods sure really mad after seeing my comment got 100+ upvotes for saying the truth, but that doesn't matter anymore because i also ban those trash sub!",Intel,2025-06-28 09:38:27,4
AMD,n0tqugl,Intel should be ahead of the curve on things not looking to compete on previously created tech,Intel,2025-07-01 20:43:31,1
AMD,n0x6eyy,"Very fine-tuned ARL-S almost reach 9800X3D performance. Extra cache could help to close the gap   Given people are willing to overpay for price-inflated 9800X3D, I wonder if it could work given buyers need an entirely new platform. 9800X3D users are fine for a pretty long time like 5800X3D users did",Intel,2025-07-02 10:56:54,1
AMD,n5g03d8,"Lol, requires a new socket. Intel is such trash.",Intel,2025-07-27 15:10:56,1
AMD,n05c074,Intel will simply always be better than amd,Intel,2025-06-27 22:40:39,2
AMD,mzzj3f4,"AMD gains tremendously from X3D/v$ because the L3 cache runs at core speeds and thus is fairly low latency, Intel hasn't seen such low latency L3 caches since skylake, which also has much smaller sizes, so the benefits of this could be much less than what AMD sees.   Only one way to find out, but I advise some heavy skepticism on the topic of ""30% more gaming perf from 'intel's v$'""",Intel,2025-06-27 01:08:00,-9
AMD,n00a0en,"Either more cache or resurrecting the HEDT X-series... Doesn't matter, as long as there is an affordable high-end product line.",Intel,2025-06-27 03:59:29,20
AMD,n01ajgx,"The 12600k was a fine chip, but AMD had the ace up Its sleeve. I upgraded from a 12600k to a 7950x3d and it was one of the best PC upgrades I ever made.",Intel,2025-06-27 09:25:00,8
AMD,n03naj2,I mean 9800x3D and 14900K offers basically the same performance in the enthusiast segment. Going forward though it would be nice to have more cache so normal users doesn't have to do any sort of memory overclocking just to match 9000x3D in gaming.,Intel,2025-06-27 17:33:15,4
AMD,n022v6j,4070 ti won’t cut it man - upgrade!,Intel,2025-06-27 12:57:19,-2
AMD,n067n87,Broadwell could have been so interesting had it planned out.,Intel,2025-06-28 01:50:48,5
AMD,n0254vg,"I want a 32 Core/64 Thread 3.40 GHz Core i9-like CPU. Not Xeon like with Quad-Channel and stuff, just 40 PCIe 5.0 lanes and 32 Power-Cores instead of little.big design. 😬",Intel,2025-06-27 13:10:22,6
AMD,n06s4h5,">Otherwise AMD will see a clear opportunity to bring prices up  AMD already did, as you can see zen 5 x3d is overpriced as hell especially the 8 core CPU. Zen 5 is overpriced compared to zen 4 which is already more expensive than zen 3. Not to mention they did shady business like keep doing rebranding old chip as the new series to fools people into thinking it was new architecture when it wasn't and sell it with higher price compared to chip on the same architecture in old gen.  Intel surely needed to kick Amd ass because Amd keep milking people with the same 6 and 8 cores CPU over and over with price increases too! Not to mention radeon is the same by following nvidia greedy strategy.  Edit: Some mad Amd crowd going to my history just to downvote every of my comments because they are salty as hell, i won't be surprised if there are from trash sub r/hardware. But truth to be told, your downvote won't change anything!!",Intel,2025-06-28 04:12:23,5
AMD,n028uwl,"Even though it's not stacked, I believe it's still going to fix the last level cache latency issue MTL and ARL have.   Ryzen CPUs have lower L3 latency than Intel because each CCX gets their own independent L3, unlike Intel's shared L3. Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3, so possibly giving the existing cores/tiles their own independent L3, improving latency and bandwidth over shared L3.  But one thing intrigues me. If this cache level has lower latency than shared L3, wouldn't this more properly be called L2.5 or something below L3 rather than last level cache? Will NVL even still have shared L3 like the previous Intel CPUs? I know the rumor that it will have shared L2 per two cores, but we know nothing of the L3 configuration.",Intel,2025-06-27 13:30:56,5
AMD,n014nai,"> On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.  It is already a non-issue since AMD moved the 3D V-Cache to underneath the compute tile.",Intel,2025-06-27 08:27:08,11
AMD,n00xlat,"Adamantaium was on the interposer, did they change plans?",Intel,2025-06-27 07:17:41,3
AMD,n02fp39,"Don't remember them saying anything like that, but by around that time their 18A packaging is supposed to be ready for 3D stacking.",Intel,2025-06-27 14:06:40,5
AMD,n0r9clc,"Nova lake= skip of it's just as good as zen, you would be looking at 2 gens after that and then swap from AM5 to intel.",Intel,2025-07-01 13:40:00,1
AMD,n027hjt,"> If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming  Doubt that since it'll probably lack hyperthreading and the E-Cores are slower, even 6C12T CPUs are starting to hit their limits in games in the last few years, faster cores won't help if there's much less resources to go around, it kinda feels like intel went backwards when they removed hyperthreading without increasing the P-Core count.",Intel,2025-06-27 13:23:25,-1
AMD,mzzxdtg,Intel managed to run Sandy Bridge's ring bus clock speeds at core clocks which resulted in 30 cycles of L3 latency.   Haswell disaggreated core and ring clocks allowing for additional power savings.   Arrow Lake's L3 latency is 80 cycles with a ring speed of 3.8ghz,Intel,2025-06-27 02:34:12,19
AMD,n05blhb,"I'd like to see the HEDT X-series come back too, but Intel would have to come up with something that would be competitive in that area.  It's not hard to see why Intel dropped the series when you take a look at the Sapphire Rapids Xeon-W lineup they would have likely been based off of.  I think AMD would also do well to offer something that's a step above the Ryzen lineup, rather than a leap above it like the current Threadrippers.",Intel,2025-06-27 22:38:16,6
AMD,n01isnc,Well it was a downgrade on system snappiness as intel have way higher random reads than amd.,Intel,2025-06-27 10:39:30,13
AMD,n0421zg,> I mean 9800x3D and 14900K offers basically the same performance  LMAO,Intel,2025-06-27 18:42:47,10
AMD,n5exp59,"Huh? 9800x3d is universally known to be like 20-25 percent faster, even in 1 percent lows.   https://www.techspot.com/review/2931-amd-ryzen-9800x3d-vs-intel-core-14900k/",Intel,2025-07-27 11:11:38,1
AMD,n835hud,Maybe that is your experience.  Neverthelesss if you compare most gamers who switched to 9800x3D they report a significantly noticeable uplift in fps and 0.1 fps. Maybe a negligible few reported a decrease. And this has very likely nothing to do with the x3D CPU but other causes.,Intel,2025-08-11 10:24:29,1
AMD,mzzqcpb,"Ah you’re missing the final piece. As far as i’m aware this pretty much requires controlling the OS as well (or at least solid OS support). Consoles get their own custom operating system, Apple built a new version of MacOS for M chips. Intel and AMD though don’t control windows.",Intel,2025-06-27 01:51:25,21
AMD,mzzxpgo,UMA is such a hassle That's why I don't see it much except for calculation purposes (HPC/AI)...,Intel,2025-06-27 02:36:14,8
AMD,mzztuv0,"Application developers are supposed to try to avoid copies from GPU memory to CPU memory, instead letting it stay in the GPU memory as much as possible",Intel,2025-06-27 02:12:29,7
AMD,n014s2d,">so there is still the cost of useless copies between system RAM vs allocated GPU ram.    There is none, AMDGPU drivers have supported GTT memory since forever, so static allocation part is just to reduce burden for app developers but if you use GTT memory you can do zero-copy CPU+GPU hybrid processing.",Intel,2025-06-27 08:28:28,3
AMD,n0r948t,"Intel needs something decent because AMD has taken a page out of intel (up to gen7) playbook, same cores no changes. Intel now provides more cores but it's the 100% core increase Vs AMD 50% and bLLC that should shake things up, hopefully they keep the temperature down as I don't want to have to replace case and get a 360mm rad just to not throttle, and not ever again do a 13th and 14th gen degradation show.   If all goes well going back to intel for a few years then AMD, brand loyalty is for suckers, buy what's best for performance and value. Hopefully intel i5 has 12P cores and i7 18-20P cores that would be nice to have",Intel,2025-07-01 13:38:44,1
AMD,n03y3vh,"bLLC is just a big-ass L3$ and since Intel does equal L3 slices per coherent ring stop, it'll be 6\*12 or 12\*12 with each slice doubling or quadrupling. The rumor is 144MB so quadrupled per slice, probably 2x ways and 2x sets to keep L3 latency under control.",Intel,2025-06-27 18:23:40,6
AMD,n0798ym,"Intel and AMD have effectively the same client L3 strategy. It's only allocated local to one compute die. Intel just doesn't have any multi-compute die parts till NVL.   > Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3  8+16 is one tile, in regardless of how much cache they attach to it",Intel,2025-06-28 06:37:39,3
AMD,n01j4io,It is a massive issue for amd. You're voltage limited like crazy as electron migration kills the 3D cache really fucking fast. 1.3V is already dangerous voltage for the cache.,Intel,2025-06-27 10:42:13,6
AMD,n01g781,"I still think there's a slight impact (the 9800x3d only boosts up to 5.2GHz vs the 5.5GHz of the 9700x), but compared to Zen 4, the issue does seem to have been lessened, yes.   And even with Zen 4, the Fmax benefit from not using 3D V-cache using comparable skus was also only single digits anyways.",Intel,2025-06-27 10:17:10,6
AMD,n01fnuq,"Adamantium was always rumored to be an additional L4 cache IIRC, and what Intel appears to be doing with NVL is just adding more L3 (even though ig Intel is calling their old L3 the new L4 cache? lol).   I don't think Intel can also build out Foveros-Direct at scale just yet, considering they are having problems launching it for just CLF too.",Intel,2025-06-27 10:12:22,8
AMD,n02ibwo,"I'm an e-core hater but arrow lake e-cores are really performant and make up for the loss of HT. arl/nvl 4+8 would wildly beat 6c12t adl/rpl.  HT was always a fallacy anyway. If you load up every thread, your best possible performance is ~60% of a core for a games main-thread.  I would much rather pin main-thread to best p-core in a dedicated fashion and let the other cores handle sub threads. Much better 1% lows if we optimize for arrow lake properly (still doesn't hold a candle to 9800X3D with HT disabled though).",Intel,2025-06-27 14:19:43,9
AMD,n045qi7,"Yeah, I somewhat agree with this. I suppose it depends if Intel’s latency problem with their P+E core design is at all a fixable one - 4c/8t is still shockingly serviceable for gaming, but 4c/4t absolutely is not.",Intel,2025-06-27 19:00:29,2
AMD,n3fijqh,It's the same ratio as 285K 8P+16E vs AMD 16P and we know that 285K is competitive despite no hyperthreading,Intel,2025-07-16 11:51:08,1
AMD,n0ap7lk,"Sooo a few months ago, I helped a buddy of mine troubleshoot a black screen issue on his newly built 9800X3D and RTX 5090 rig, a fairly common issue with Nvidia’s latest GPUs.  While working on his PC, I'd notice a series of odd and random hiccups. For example, double clicking a window to maximize it would cause micro freezes. His monitor runs at 240Hz, and the cursor moves very smoothly, but dragging a window around felt like it was refreshing at 60Hz. Launching League of Legends would take upwards of 10+ seconds, and loading the actual game would briefly drop his FPS to the low 20s before going back to normal. Waking the system from sleep had a noticeable 2-3 seconds delay before the (wired) keyboard would respond, which is strange, considering the keyboard input was what wake the system up in the first place.  Apparently, some of these things also happen to him on his old 5800X3D system, and he thoughts that these little quirks were normal.  I did my due diligence on his AMD setup: updated the BIOS and chipset drivers, enabled EXPO profile, made sure Game Bar was enabled, set the power mode to Balanced. Basically, all the little things you need to do to get the X3D chip to play nice and left.  But man... I do not want to ever be on an AMD system.",Intel,2025-06-28 20:15:41,10
AMD,n02j14s,did they measure responsiveness and timed the click to action? and was it significantly different? how much difference are we talking about?,Intel,2025-06-27 14:23:07,10
AMD,n04xv5b,"can you explain exactly what you're talking about here? are you talking about a situation where the system needs to do random reads from an ssd? aka: boot time, initial game load time?",Intel,2025-06-27 21:22:28,6
AMD,n0429de,"How was ""system snappiness"" measured?",Intel,2025-06-27 18:43:46,9
AMD,n01uxxc,No.,Intel,2025-06-27 12:08:29,11
AMD,n08zo5w,"Now both AMD and Intel chips are “disaggregated “ which means between cpu and other agents like memory controllers, pcie, and storage there is higher latency than the 12/13/14th gen parts. AMD has higher latency due to the larger distances involved on the package.  Also Intel is not really improving the CPU core much. There won’t be a compelling reason to upgrade from a 14700 until DDR6 comes out. At least not in desktop. Nova lake high cache parts will cost $600 or more so value/dollar will be low.",Intel,2025-06-28 14:51:50,2
AMD,n02l2ku,So? Major upgrade for everything else,Intel,2025-06-27 14:33:03,3
AMD,n0x0cru,"I had an 12600 not k. I had the opposite experience, I upgraded to a 7800x3d and the snappiness was a night and day upgrade. I can recommend a x3d to anyone. Pair that cpu with Windows 11 IoT LTSC and you have a winner <3",Intel,2025-07-02 10:05:39,0
AMD,n046538,"Meant to say ""Gaming Performance""  >Higher avg on X3D  >similar or same 1% lows on both platforms >Higher .1% lows on Intel.",Intel,2025-06-27 19:02:27,8
AMD,n043af1,"any comment that starts with ""I mean..""  I never go any further, its like some weird reddit think where everyone with ignorant comments seems to start out with this,  at least often anyway.",Intel,2025-06-27 18:48:47,-1
AMD,n5fdqig,"""Enthusiast Segment"" my good sir. All the benches you see are poorly configured or stock 14900K. With tuning it's a different story. Intel craptorlake scales with fast ram.",Intel,2025-07-27 13:07:23,0
AMD,n84bzza,"As a long time AMD user I know that Intel needs to be tuned to perform best. So when you tune the 14900K or even 285K you get like 20% performance uplift vs stock. X3D just performs great out of the box because of the huge L3 Cache. At the very least if you do not like microstutters or frame drops and want consistent gaming performance Intel 14th gen is superior vs current AMD's offering. Anyone with a specific board like Apex, Lightning, Tachyon, or even Gigabyte Refresh boards + i7/i9 13-14th gen with decent memory controller can achieve similar gaming experience. I'm speaking from experience since I also have a fully tuned 9950x3D/5090 on my testbench. For productivity task Intel feels much better to use as well. I feel like Intel is just better optimized for Windows and Productivity too.",Intel,2025-08-11 14:53:13,1
AMD,n1f1onn,"Actually Intel thermal is already better than Amd ever since Arrow Lake and Lunar Lake released. Even Core Ultra 7 258V is arround 10c cooler than Amd Z2E and Strix Point on the same watt.   On MSI Claw 8 AI+, Lunar Lake temp at 20w is just arround 62c while the Amd version is arround 70c. I wouldn't have a doubt Nova Lake and Panther Lake will also have good thermal because it will have 18A node with BPD and RibbonFET GAA which is more advance than traditional silicon when it comes to power delivery and efficiency.",Intel,2025-07-05 03:53:59,1
AMD,n0jysi1,Ah so bLLC on both tiles is a possible configuration? Any chance Intel actually goes for this?,Intel,2025-06-30 10:29:27,1
AMD,n02dy44,You can very simply get 9800x3D to 5.4 with little effort,Intel,2025-06-27 13:57:44,5
AMD,n0qy2r3,I haven't seen any of those issues on AMD where the underlying cause wouldn't also cause those issues on Intel.,Intel,2025-07-01 12:35:15,5
AMD,nc1pr0f,"7800x3d here and never had these issues, came from intel",Intel,2025-09-02 17:25:21,1
AMD,n02jgm4,Difference between 85MBps and 140MBps in q1t1 random reads and writes.,Intel,2025-06-27 14:25:11,-2
AMD,n01v9di,Lets just ignore the whitepaper WD and Intel did about this.,Intel,2025-06-27 12:10:35,4
AMD,n5fmwbs,So what configuration (tuning and ram settings) can a 14900k match a 9800x3d?,Intel,2025-07-27 14:00:49,1
AMD,n006ga4,That (Apple Silicon is good) and UMA are different stories I already know that Apple Silicon is good,Intel,2025-06-27 03:34:22,9
AMD,n03645p,On which benchmark(s) / metrics?,Intel,2025-06-27 16:12:47,4
AMD,n1jzzta,"Haven't kept up with mobile since AMD 5000 and intel 10th gen, all I remember is intel needing XTU undervolting and then intel blocking XTU so using third party undervolt programs, AMD like I said 5000 never needed undervolting.   Desktop side, AMD 7000 is a hot mess, like seriously ridiculous. Seems that was sorted on 9000,    Let's wait and see on nova lake and zen6, like I said, brand loyalty is stupid, bought into AM5, so it's cheaper for me to go for zen6 with 244Mb of L3, but no am6-7 that will be intel.",Intel,2025-07-06 00:05:28,1
AMD,n0ksjbi,"In theory, yes. For packaging reasons and market segmentation, probably not.",Intel,2025-06-30 13:51:44,2
AMD,n02kkbn,"what's that on in terms of percentage, or seconds to person? was it noticeable?  i'm not techy enough, but is random reads and writes for clicking things and accessing data, and less so on copying and pasting a file?",Intel,2025-06-27 14:30:33,9
AMD,n0226kr,where is this whitepaper,Intel,2025-06-27 12:53:19,7
AMD,n02akpt,"You can send white papers all day but if most people buy these for gaming or productivity, AMD is winning in both categories.",Intel,2025-06-27 13:40:12,6
AMD,n5g6rxb,Stock clocks 5.7/4.4/5.0 HT Off With DDR5 7600-8000 on Windows 10 22H2 or Windows 11 23H2 is enough to match 9800x3D at 5.4ghz with 6000 c28/6200 c28,Intel,2025-07-27 15:44:33,1
AMD,n7xvp9r,So MLID leaked in his most recent video about this (and he ranks it with a blue color code - 'very high confidence')  What do you think about this?,Intel,2025-08-10 13:55:29,1
AMD,n023hsg,[https://youtu.be/0dOjvdOOq04?t=283](https://youtu.be/0dOjvdOOq04?t=283) This explains it.  Gonna find the whitepapers link again.,Intel,2025-06-27 13:00:57,1
AMD,n5gbwrf,Based on what evidence?   I looked online for a few moments and found:  (1) Buildzoid from actually hardcore  overlocking did a 12 hour live stream where they couldn't even get a 8000 mhz overclock stable on the 14900k. No wonder why people haven't benched this lol  https://www.youtube.com/live/bCis9x_2IL0?si=ht3obVoBLcRFCyXI  (2) Plenty of benchmarks where an overclocked 9800x3d is about 10 percent faster than an overclocked 14900k with 7600 ddr5,Intel,2025-07-27 16:09:50,1
AMD,n007dg1,"Haven't you been listening? The conversation is strange (Confused) You first brought up the story of UMA, right?",Intel,2025-06-27 03:40:49,8
AMD,n03knav,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-06-27 17:21:00,2
AMD,n046z2m,"That channel has a lot of videos and even on this specific video it would help if you point the specific time you're referring to.  Now regarding AI, I assume you are talking about token generation speed and not prompt processing or training (for which Macs are lagging due to weak GPU compute).  I happen to have expertise in optimizing AI algorithms (see https://www.reddit.com/user/Karyo_Ten/comments/1jin6g5/memorybound_vs_computebound_deep_learning_llms/ )  The short answer is that consumers' PCs have been stuck with dual-channel RAM for very very long and with DDR5 the memory bandwidth is just 80GB/s to 100GB/s with overclocked memory.  Weakest M4 starts at 250GB/s or so and M4 Pro 400GB/s and M4 Max 540GB/s.  Slowest GPUs have 250GB/s, midrange about 800GB/s and 3090~4090 have 1000~1100GB/s with 5090 having 1800GB/s bandwidth. Laptop GPUs probably have 500~800GB/s bandwidth  LLMs token generation scales linearly with memory bandwidth, compute doesn't matter on any CPU/GPU from the past 5 years.  So by virtue of their fast memory Macs are easily 3x to 8x faster than PC on LLMs.  The rest is quite different though which is why what benchmark is important.",Intel,2025-06-27 19:06:33,3
AMD,n7xwq6a,"I'm highly suspicious, for packaging reasons if nothing else. I'm not going to call it impossible, but that would put enormous strain on the X dimension. Think about it, it's something on the order of 3-4x what the socket was originally for.  And obviously goes without saying, but MLID ""very high confidence"" doesn't mean shit.",Intel,2025-08-10 14:01:29,1
AMD,n5ghx4j,"Based on my own testing , the games I play and benchmark results. Bullzoid is not relevant to this conversation. You can keep downvoting me for speaking the truth it's okay. I can't blame you because youtube and mainstream techspot are 99% misinformation. If you believe 9800x3D is 30% faster then go buy it, nobody is stopping you. I have investments with AMD and you're doing me a favor by supporting them.",Intel,2025-07-27 16:38:58,1
AMD,n13yjw6,Just to be clear compute and hardware support for things like tensor cores have a massive impact. HBM is king but on older cards like the mi100 (released five years ago) can be out paced by a mid range card like the 4070.  All I wanted to convey is llm and token generation is a complex topic with limitations and struggles beyond memory bandwidth.,Intel,2025-07-03 11:37:55,2
AMD,n5gkgn2,"I haven't downvoted you at all, what are you even talking about.   You want people to believe in some conspiracy theory where any other information is a lie, and only you provide the ""real truth"".",Intel,2025-07-27 16:51:17,1
AMD,n0081vw,"Besides, UMA wasn't first developed by Apple. Even if Intel introduces it, the software side or the software framework… Moreover, the OS side has to deal with it, so it is necessary to consider it a little. That's what you said earlier",Intel,2025-06-27 03:45:34,11
AMD,n049nak,"I take no side there. I'm a dev, I want my code to be the fastest on all platforms  I have: - M4 Max so I can optimize on ARM and MacOS - Ryzen 9950X so I can optimize with AVX512 - in the process of buying an Intel 265K so I can tune multithreaded code to heterogeneous architecture.  The problem of Intel and AMD is segmentation between consumer and pro.  If Intel and AMD want to be competitive on AI they need 8-channel DDR5 (for 350~400GB/s), except that it's either professional realm (Threadripper's are 8-channel and EPYC's are 12-channel) with $800~1000 motherboards and $1500 CPUs and $1000 of RAM.  Or they make custom designsvwith soldered LPDDR5 like the current Ryzen AI Max 395, but it's still a paltry 256GB/s.  Now consumer _need_ fast memory. Those NPUs are worthless if the data doesn't get fetched fast enough. So I expect the next-gen CPUs (Zen 6 and Nova Lake) to be quad-channel by default (~200GB/s with DDR5) so they are at least in the same ballpark as M4 chip (but still 2x slower than M4 Pro and Max).  I also expect more soldered LPDDR5 builds in the coming year.",Intel,2025-06-27 19:19:40,2
AMD,n5gmhbv,"Have you not seen HardwareUnboxed 9070 XT Finewine video? Tell me why should I trust someone like that? I'm just sayin if you want real information test it yourself. I just don't trust product influencers like GamersNexus, HUB , Jayz2cents and other mainstream channels. Id rather buy the product and test it myself. Im not forcing you to believe what I said, its for people that actually knows what Raptorlake is capable off. If you want to see a properly tuned 14900k check out facegamefps on youtube. This is a very capable platform.",Intel,2025-07-27 17:01:05,0
AMD,n0rzbmt,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-07-01 15:46:52,1
AMD,n1s9740,Is it safe to install an Arrow Lake CPU without a third-party contact frame over the long run?,Intel,2025-07-07 10:28:03,1
AMD,n212egt,"Edit: solved. Windows is capable of disabling e-cores on boot. The setting to turn them back on in MSCONFIG, boot tab, cores dropdown. Whatever feature causes this, I do not know but would love to find out, and furthermore their is no legitimate purpose for this feature, and Intel should prevent Microsoft from doing this, in my opinion, if there is any legal means available. Unless it interferes with Intel’s revenue.  Poor performance, incredibly low benchmarks. Beginning to suspect bad CPU.  I have tried bone stock with cleared CMOS settings. I have loaded multiple overclocking profiles, clearing CMOS in between. I have tried undervolting, and adjusting Load Line. I have changed Lite Load settings. I have tried with and without XMP enabled. Changed windows power plans. Nothing gets my cinebench r23 score above 11k. CPU-Z benchmarks it as 43% as fast as the previous generation CPU. I never overclocked this cpu, and I changed to the updated bios with new microcode the day it was available. CPU Purchased from Intel, retail box.  Windows 11 Build 26100.4484  CPU: Intel I9 14900K, stock clock, 360 AIO liquid cooling  RAM: Patriot Viper Venom DDR5 32GB (2x16) 7200MT/S CL34  GPU: MSI Ventus NVIDIA RTX 4080S  Motherboard: MSI Z790-S Pro Wifi with most recent bios  Storage: WD Black 2TB NVME on CPU; Toshiba 20GB X300 Pro  PSU: NZXT C1000 PSU (2022)  Display: Samsung Odyssey G93SC  I am at my wits end with this thing. I first noticed games crashing to desktop a few months ago. I thought it was just poorly coded (Helldivers 2). I checked my FPS in the game, and while previously I had to frame limit it 10 144FPS, I was now getting close to 60-80FPS.  Is my CPU a toaster or is there something I'm missing?  CPU-Z output, and HWiNFO64 sensor readings:  [https://imgur.com/a/ROuZOKS](https://imgur.com/a/ROuZOKS)",Intel,2025-07-08 18:18:07,1
AMD,n2cfmmi,"I bought a 265kf CPU from Amazon. On the checkout page it clearly stated that my purchase qualified for the Intel Spring Gaming Bundle and that I would receive an email with a Master Key. Well, I didn't receive anything and I spent all day being transferred from one Amazon support staff to another to no avail.  The promotion is literally still active and if I try to buy the CPU again it shows the same offer.  But somehow no one on Amazon or Intel support can tell me why I didn't get the email.",Intel,2025-07-10 11:27:48,1
AMD,n2h44v7,Does anyone know if the upcoming Bartlett Lake-S 12 p-core no e-core CPU will suffer from the same stability issues as Intel 13th gen and 14th gen CPUs?,Intel,2025-07-11 01:52:33,1
AMD,n4at6sf,Any news to share regarding this link? https://www.reddit.com/r/intel/s/Bg4QnVzIdD,Intel,2025-07-21 06:57:04,1
AMD,n4vm1wc,"Bug report: Latest ARC driver 32.0.101.6972 causes crashing using Speed Sync  I get crashing in ALL games when attempting to use the new Speed Sync option on this driver.  Specs are:  Ryzen 9700x, Arc B580, 32GB DDR5 RAM, 2tb SSD.  Monitor is non VRR compatible (older gsync monitor ASUS PG348Q)  Hope this gets resolved as it sounds like a great feature.",Intel,2025-07-24 10:55:22,1
AMD,n5c8s6s,"Hello,  I own a i9-14900K, mobo MSI MAG Z790 Tomahawk. I am wondering if it’s possible a specific program can cause my cores to be power limit exceeded, and is there any fix to that? Currently it drops the cpu speed to 0.8 GHz. This one program LeagueClient.exe for a game, League of Legends, has recently started to cause this problem, the only fix is to restart the computer. I have updated my bios, changed power limits within the bios, disabled c state, disabled EIST, disabled e-cores, tried to go into safe boot but the program won’t launch, reinstalled multiple times.",Intel,2025-07-26 22:34:12,1
AMD,n663btu,Has the degradation of the 13th and 14th gen CPU’s been fixed yet?,Intel,2025-07-31 14:14:55,1
AMD,n6pmegm,"Been getting the following error A LOT while playing Expedition 33, and now after 18 hours I can't start the game without it crashing:  `LowLevelFatalError [File:C:\Hw5\Engine\Source\Runtime\RenderCore\Private\ShaderCodeArchive.cpp] [Line: 413] DecompressShaderWithOodleAndExtraLogging(): Could not decompress shader group with Oodle. Group Index: 760 Group IoStoreHash:52bcbf8ac813e7ee35697309 Group NumShaders: 29 Shader Index: 9301 Shader In-Group Index: 760 Shader Hash: 3BF30C4C9852D0D23B2DF59B4396FCC76BC3A80. The CPU (13th Gen Intel(R) Core(TM) i7-13700KF) may be unstable; for details see` [`http://www.radgametools.com/oodleintel.htm`](http://www.radgametools.com/oodleintel.htm)     Am I just screwed because I bought the wrong generation of Intel CPUs 2 years ago?",Intel,2025-08-03 15:45:17,1
AMD,n7b4kpy,"I'm not sure if this matters, but when running `garuda-inxi` on my laptop, it tells me that my i3-8130U is Coffee Lake Gen 8, not Kaby Lake Gen 9.5. I've seen similar problems reported with users using CPU-Z, is this just a known bug or is there something else going on?  I'm going to be tweaking my CPU performance soon, so I'd like to make sure of the CPU capabilities/options first.",Intel,2025-08-06 21:44:11,1
AMD,n85zrk7,"I've been planing to build a PC from scratch for video editing and Ultra 7 265k and Arc B580 are good in my price bracket. Now, I'm worried with all the talk of Intel potentially going bankrupt or having layoffs, is it a good idea to still buy their products? Will we lose support with drivers and stuff like that?",Intel,2025-08-11 19:48:10,1
AMD,n8auefg,My gaming laptop Intel core i7-12700h runs at a constant temperature 95 . Doesn't matter if I'm playing a high end game like cyberpunk or some indie game like hollow knight. Is this thing suppose to always run at this temp?,Intel,2025-08-12 15:12:02,1
AMD,n8i5ccq,"con el nuevo microcodigo de intel , los juegos de ubisoft (ASSASINS CRREED ODYSSE) tienen tiempos  de carga excecivamente altos, diria que al menos 10 veces mas de lo normal, Al devolverr la bios al microcodigo anterior todo funciona bien, cuando lo van a parchar?",Intel,2025-08-13 17:35:56,1
AMD,n8k6lv9,is it worth the upgrade for ai preformance cus my 5070 ti wont work for some reason so now my cpu is my main accelertor and user so should i upgrade to ultra 9? and i  will remove 5070 ti from my flair soon so yeah .,Intel,2025-08-13 23:38:35,1
AMD,n9pv76c,"ok now this is something. my Intel HD Graphics Control Panel is no longer there? no idea how long its been gone but i clearly remember it being there at a point.  using a Lenovo G510(i7-4700MQ, HD Graphics 4600)   Windows 10 Home 22H2 (Build 19045.6216)  windows apps in settings shows the intel driver but not the control panel   drivers from [lenovo's website](https://pcsupport.lenovo.com/in/en/products/laptops-and-netbooks/lenovo-g-series-laptops/lenovo-g510-notebook/20238/downloads/ds103802-intel-vga-driver-for-windows-10-64-bit-lenovo-g410-g510?category=Display%20and%20Video%20Graphics) are dated 16jul 2015. version seems to be 10.18.15.4240   drivers from [intel's website](https://www.intel.com/content/www/us/en/download/18388/intel-graphics-driver-for-windows-10-15-40-4th-gen.html?wapkw=intel%20hd%20graphics%204600) are dated 9jan 2015. version is 15.40.7.64.4279  now whats funny is that my device manager shows that my driver is dated 8mar 2017 which is version 20.19.15.4624   i also have another driver that i can see in the update drivers menu(drivers already present on my device) along with this one which is dated 29sep 2016 version 20.19.15.4531  i have tried reinstalling the driver from the update driver menu using the 8mar 2017 version, no change at all.  my drivers dont seem to be DCH drivers.   intel also says that they [discontinued the ms store version of the control panel](https://www.intel.com/content/www/us/en/support/articles/000058733/graphics.html) anyway.  this is all that i could think of writing here. any other details required just ask.   any help would be good lol",Intel,2025-08-20 14:29:49,1
AMD,na637rq,"my i5-14600KF is being throttled at low temps and refuses to go past 0.8ghz of clock speed. Nothing I do seems to get it to stop throttling. According to throttlestop i have a red EDP OTHER ongoing throttle under CORE and RING. My average CPU temp is 31 degrees C across all cores and im getting 0.69 Voltage to my CPU  CPU: Intel i5-14600KF stock settings no overclock   GPU: Intel Arc B580 ONIX Odyssey BAR resizing enabled   Motherboard Gigabyte Ultra Durable Z790 S WIFI DDR4   RAM: Corsair Vengeance DDR4 16 GB x 2 (32GB)   Storage: 1TB Corsair MP600 CORE XT SSD + 2 TB WD Black SN770   PSU: Cooler Master MWE Gold 850 V2  EDIT: NEW INFO ACQUIRED   When running in safe mode and when booting into BIOS settings my CPU acts normally and receives typical voltage. Something running on my computer is throttling my CPU as i boot into windows. If i open Task Manager quickly after booting, I see system interrupts consume a mild amount of CPU before quickly going away, rather than sticking around when their is an ongoing hardware issue. I have reason to believe a program, either maliciously or due to error, is fucking with my CPU. Also worth noting is that Intel Graphics Software reports my CPU utilization as far higher than task manager, anywhere between 2-50% higher.",Intel,2025-08-23 00:28:09,1
AMD,nand9o5,"Currently in the planning/purchasing phase of a small NVR/Steam Cache server. Information on VROC on X299 is pretty limited. so far I've seen mixed information on the drives supported. Before I purchase x4 Intel P4510 drives, I was hoping someone on here has a similar configuration that works.  The mobo manual states that only Intel based drives are supported but doesn't clarify which intel drives. Also saw on the intel forum that X299 CPU raid is further limited to only Optane based NVME drives. This drive will not be booted to, and I dont want to do a windows based raid.  My planned specs are:  CPU: 10900X  Mobo: X299 Taichi CLX - One of the few that seems to support bifurcation & VROC  Drive: Intel P4510 1TB  VROC Key: VRoc Standard  Are the Intel P4510 supported for Vroc on the x299 platform?",Intel,2025-08-25 20:44:18,1
AMD,natm1ml,"My Intel I210 ethernet device has device id 1531 meaning unprogrammed. The freebsd ethernet driver does not work with 1531. It needs device id 1533 meaning programmed. (Can I use a different driver? No, it's an embedded system that only supports this driver.)  I was linked this:  https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html  but 1) have no idea how to do it 2) cannot access the .bin file and tool it requires  Does anyone have ELI5 steps for getting the device to show devid 1533? Where can I get the .bin?",Intel,2025-08-26 19:48:39,1
AMD,nb5mwcf,"The RMA process for Intel is absolutely atrocious. I don't know if  anyone can give advice on this but here is what is happening:  Based in Germany, for geolocation info. I was one of the early adopters of the 13th gen processors, but as I don't follow tech news too strongly I didn't find out about the issues with these chips until autumn 2024 when the issues I was having with my PC escalated to a point I couldn't ignore any further. Identified the CPU as the likely culprit and started the RMA process.  Firstly, Intel would not offer any solution where I could continue to use my PC whilst they analysed my CPU. As I use my PC for work, having it out of action for weeks/months was simply not an option, so I was forced to pause the RMA ticket whilst I saved up for a new cpu way ahead of my expected timeline.  With that aside, I reopened my ticket and the requirements they lay out are near impossible to meet - they wanted a clear photo of the matrix on the front of the chip and the matrix on the pcb itself.  The front of the chip was simple enough but on this series Intel printed the matrix in dark grey on a dark green pcb. It's barely visible just with the naked eye and I've tried so many ways to take a picture of the matrix with my smartphone and nothing I do is getting a clear picture.  I don't understand how your average consumer can possibly meet this requirement - solutions online apparently suggest purchasing a special type of expensive scanner or a macro lens for your smart phone? Which is ridiculous to me.  There is no way they cannot verify my chip with the rest of the information I have been able to give them, as well as far as I am aware, the matrix on the side of the chip on the 13th gen is literally the same as the one on the front of the chip.  The whole experience is proving to be awful, time consuming, feels like it should be illegal and has completely put me off ever using Intel again, or recommending it to anyone I help spec builds for.",Intel,2025-08-28 16:16:20,1
AMD,ncohq8s,"Hi, I have a Lenovo Ideapad with an Intel CPU and Intel GPU.  I find Windows font rendering unreadably faint. I have an astigmatism, and a light sensitivity, so even with prescription sunglasses I can't use dark mode, or bright screens, respectively.  I've tried finding Intel Graphics Software app settings which might help. The contrast settings quickly get too bright for my eyes, so they don't fix this. The right gamma settings might help though, one profile to make medium shades darker to make text bolder, and one to make them lighter to make images bolder.  I tried the support site here, but it has buggy scrolling which triggers my migraines, and it doesn't work with Firefox's reader view:  https://www.intel.com/content/www/us/en/support/products/80939/graphics.html#211011  How can I find or create these profiles? The Intel Graphics Software app doesn't seem to have an option to create profiles.  P.S. I can use the Windows Display Calibrator; if I ignore the instructions, and turn it as dark as possible during gamme, I get readable text at the end; if I turn it as light as possible, I get clearer images. But I can't see  a way to save those and switch without going through the whole rigamarole again.",Intel,2025-09-06 04:15:45,1
AMD,ncywbwe,"Can someone explain to me the differences between all the different names of the Intel CPUs, I’m new to laptops and am trying to learn. What’s the difference between 155H, 255H, and Meteor/Lunar/Arrow Lake and which one is “better”? All the different names get so confusing to me",Intel,2025-09-07 20:35:29,1
AMD,ndcu0v4,"I've been chasing down a crashing issue in a Unity program. Memtest86+ ran for 24 hours and cleared 18 passes so I was confident it's not RAM, and I started setting core affinity in an elimination pattern to see if it might be CPU related. I have discovered it crashes within minutes if I set affinity to Core 8, but it's rock solid on Cores 0-7 (haven't individually tested any of the cores past 8 yet but I have it on 9 now and it seems fine so far).  I have a 13900KF (not Overclocked) which is part of the batch that had the microcode issue. My BIOS was updated, but I am now suspicious of this core. Is it likely I should RMA this? Is there something else I should try first?  So far the system itself has been stable but this one Unity program crashes consistently, and I was having tab crashes in both Firefox and Chrome that also resolved when I removed Core 8 from their affinity...  The CPU passed the Intel CPU Diagnostic tool but because the crashes are so specific to core 8 it makes me suspicious.  [edit] RMAd the CPU and the system is rock solid stable now. WHEW!",Intel,2025-09-09 23:36:23,1
AMD,ndjq7jq,"i'm trying to get a specific driver for the storage but i can't findd a site with the driver, only setup exes (my old computer will not let it work and the new one needs the driver to install windows). is there a place where i could find the driver itself?",Intel,2025-09-11 00:09:09,1
AMD,nduwi1y,"Hi There. I been trying to get a replacement cpu for my 14700 and i chose option 2 to pay the 25 dollor fee (nonrefundable) etc. but my case now has a new number, a differnt worker. and he said that a credit card specilist is gonna called me and im supposed to give him my credit card infomation  edit 1: He also said the intel website isnt secured/safe for creditcard transaction",Intel,2025-09-12 18:07:38,1
AMD,nf5gpjt,"Hi, I have Intel AX200 160Mhz Gig+ in 3 PC's (on board Gigabyte B550 x 2 & X570). My Router supports 160Mhz, it's enabled and was working getting me up to 900Mbps on my Gigabit plan. and in the last few months I've noticed the 160Mhz option in the intel wifi drivers has disappeared. I discovered this as I noticed the household PC's were struggling to get above 600Mbps most of the time. I double checked the router setting, nothing changed there 160Mhz still eneabled, only changes over the last couple of moths were 2 bios updates each for the motherboards and Intel wifi driver updates. Has anyone else had the 160Mhz option disapear? Any tips on how to get it and my full network performance back?",Intel,2025-09-19 22:29:52,1
AMD,ng6kvna,"### 2 Cores always parked   I dont know if this is intended behaviour but I got a Dell Pro PC14250 which has a Intel Ultra 5 235U (Dell Pro 14 PC14250)   Whenever I check the Task Manager 2 cores are always parked. Regardless of what I am doing, whether just browsing or doing some heavy stuff in VS2022 (build/compile etc). They are ALWAYS Parked and have small spike once or twice  I have updated all the drivers, I used the intel support assistant and updated everything but still they are always parked. I have set everything to ultra performance mode in the Bios, windows doesn't have that plan.   https://i.vgy.me/Pn8RcH.png  https://i.vgy.me/yES6ja.png",Intel,2025-09-25 19:34:48,1
AMD,nh3l2tu,"I have a friend with quite an old laptop, it has an i3 4010U. She's trying to use an up to date version of ON1 Raw for photo editing, but it's giving her some error about it missing Vulkan 1.1. Someone else was helping her with it and they tried downloading the latest drivers from the manufacturer's site, but knowing they can often be behind with their driver versions I went straight to the Intel site and found the latest driver for that chip, which seems to be [Intel Graphics Driver 15.40](https://www.intel.com/content/www/us/en/download/18369/intel-graphics-driver-for-windows-15-40.html) from 2021. The next available driver is from 2015.  When trying to install the driver, the install package gave some warning about not being able to authenticate the drivers. This was the same error they got from whatever driver Lenovo had on their site. I didn't have time to really look at it - I wish I'd taken a screenshot of it - but it didn't seem like a Windows error to me. More specifically it seemed to be an Intel driver package error.  It's likely the chipset is too old to support Vulkan 1.1, though I can't exactly find any hardware requirements needed to run the API. But above all we can't install the latest available driver for the chip and I'd really love to know what's going on there.  Anyone have any idea?",Intel,2025-09-30 23:49:25,1
AMD,n22cmoi,"u/Progenitor3  yes, it is generally safe to install an Arrow Lake CPU without a third-party contact frame over the long run. CPUs are designed to function properly with the standard mounting mechanisms provided by the manufacturer. Third-party contact frames are optional and may offer additional stability or cooling benefits, but they are not necessary for the safe operation of the CPU. Always ensure proper installation according to the manufacturer's guidelines to maintain optimal performance and safety.",Intel,2025-07-08 21:50:47,1
AMD,n22gizs,"u/TerminalCancerMan  Intel cannot comment or interpret results from third party benchmark tools. Run [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) to confirm if there are any issues with the CPU. You may try this, If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor need a replacement.",Intel,2025-07-08 22:10:13,1
AMD,n2h78m4,"u/Progenitor3  Just fill in your info, and it’ll automatically create a ticket for you. Our team that handles those items will get in touch within 3–5 business days.   [Software Advantage Program](https://softwareoffer.intel.com/Support)",Intel,2025-07-11 02:10:36,1
AMD,n2jp4dz,Gotta wait for real-world tests to know for sure tho.,Intel,2025-07-11 13:32:10,1
AMD,n4nvw3m,"u/Special_Ad_7146 To stay on top of Intel news, **visit** our [Newsroom](https://newsroom.intel.com/).",Intel,2025-07-23 05:31:29,1
AMD,n523x8x,"u/Hippieman100  just checking can you confirm which software you're using? From what I’ve seen in the latest [ReleaseNotes\_101.6972.pdf](https://downloadmirror.intel.com/861295/ReleaseNotes_101.6972.pdf), there doesn’t seem to be a “Speed Sync” feature in the current version that comes with this driver. It does support V-Sync and Adaptive Sync though. Just wanted to clarify are you referring to the older Intel Arc Control software or the Intel Graphics Command Center? Just making sure we’re on the same page!",Intel,2025-07-25 09:46:51,1
AMD,n5hzwbu,"u/TheCupaCupa to better understand and isolate the issue, I kindly ask for some additional information. Please find the details requested below:   1. When the CPU drops to 0.8 GHz, do you notice any **error messages or warnings** in Windows or BIOS? 2. Does the issue happen **only with LeagueClient.exe**, or have you seen it with other programs too? 3. Have you checked **CPU temperatures and power draw** when the issue occurs? 4. Can you check **Event Viewer** for any critical errors or warnings around the time of the slowdown? 5. Are you using any **custom power plans** in Windows, or is it set to Balanced/High Performance? 6. Is **Intel Turbo Boost** enabled in BIOS? 7. **When did this issue first start happening?** Has it occurred before? 8. Have you made any software or hardware changes to the system recently?   Once I receive this information, I will be able to properly assess the situation and provide further assistance.",Intel,2025-07-27 21:08:02,1
AMD,n6sn19c,"u/Alloyd11 Not all 13th and 14th generation processors show instability issues. Just to better assist you are you planning to buy or use one of these processors, or do you need help with your current system?  Let me know how I can support you!",Intel,2025-08-04 01:26:43,1
AMD,n6sobcw,"u/amitsly “crashes” is a pretty broad term, and not every system issue points directly to the CPU. There are quite a few steps we go through to fully isolate the problem before concluding it’s a processor fault.  To help us assist you better, could you please share a bit more info about the crashes?  * When did the issue first start happening? * Have you made any recent changes to the system either hardware or software? * Is there any visible physical damage to the system? * What troubleshooting steps have you already tried? * Have you noticed any signs of overheating? * Have you tested the processor in another working system, or tried swapping it out to see if the issue follows the CPU?  The more details you can provide, the quicker we can get to the bottom of it!",Intel,2025-08-04 01:34:29,1
AMD,n7crjzp,"u/Jay_377 its  likely comes from Intel’s naming convention. The i3-8130U is part of the 8th gen, but it falls under “Kaby Lake Refresh” (for mobile chips), not “Coffee Lake” (which is for desktops). Tools like `garuda-inxi` or `CPU-Z` might label it differently based on how they categorize architectures, not a bug, just naming differences.  [Intel® Core™ i3-8130U Processor](https://www.intel.com/content/www/us/en/products/sku/137977/intel-core-i38130u-processor-4m-cache-up-to-3-40-ghz/specifications.html)",Intel,2025-08-07 03:21:15,1
AMD,n7j7cu2,It is because Kaby Lake CPU's are 7th gen processors for laptops and 8th gen to 9th are Coffee Lake check rhis link for better understanding:https://www.intel.com/content/www/us/en/ark/products/codename/97787/products-formerly-coffee-lake.html,Intel,2025-08-08 02:36:25,1
AMD,n88eg87,"u/Reality_Bends33 Intel has been around for a long time and is known for making solid, reliable products, so you can feel confident about choosing them for your PC build. The Ultra 7 265k and Arc B580 are great picks for video editing, offering the performance you need without breaking the bank. While there’s been some discussion about Intel facing challenges, remember that big companies like Intel usually keep up with driver updates and support, even if they’re going through changes. The tech world is always evolving, and Intel is investing in new technologies to stay ahead. So, you’re likely to get the support you need for your products.",Intel,2025-08-12 04:07:16,1
AMD,n8jfndw,"u/unknownboy101 It’s normal for your processor to heat up during heavy tasks like gaming. Intel CPUs are built to manage heat by adjusting power and speed, so they stay safe and avoid damage. However, running at a constant **95°C** on your Intel Core i7-12700H even during light gaming is **not ideal** and could indicate a cooling issue. While Intel CPUs are designed to handle high temperatures and will throttle performance to avoid damage, consistently running near the thermal limit can shorten the lifespan of your components and affect performance. **Feel free to check out this article for more info or steps to try. Just a heads-up this is specifically meant for boxed-type processors. You can still take a look, but I strongly recommend reaching out to your laptop’s manufacturer to get help with the overall system configuration.**  [Overheating Symptoms and Troubleshooting for Intel® Boxed Processors](https://www.intel.com/content/www/us/en/support/articles/000005791/processors/intel-core-processors.html)  [Is It Bad If My Intel® Processor Frequently Approaches or Reaches Its...](https://www.intel.com/content/www/us/en/support/articles/000058679/processors.html)",Intel,2025-08-13 21:16:44,1
AMD,n8lchtt,"u/Frost-sama96 Tenga en cuenta que solo puedo apoyarlo en inglés. He utilizado una herramienta de traducción web para traducir esta respuesta, por lo tanto, puede haber alguna traducción inexacta     **To help me dig a little deeper into the issue, could you share a few details?**  * What’s the **make and model** of your system? Is it a **laptop or desktop**? * Do you remember **when the issue first started** happening? * Which **BIOS version** are you referring to, the one that works fine? If you can share the exact version , that’d be super helpful.",Intel,2025-08-14 03:51:06,1
AMD,n8lcvrc,"u/Fluid-Analysis-2354 If your 5070 Ti isn't functioning and your CPU is now your main accelerator, upgrading to the Ultra 9 could be beneficial. The Ultra 9 offers enhanced AI performance, which can significantly improve your computing tasks. If AI capabilities are a priority for you, the upgrade is worth considering.",Intel,2025-08-14 03:53:51,1
AMD,n9tzi8c,"u/BestSpaceBot , As with all good things, your product has reached the end of its interactive technical support life. However, you can find [Intel® Core™ i7-4700MQ Processor](https://www.intel.com/content/www/us/en/products/sku/75117/intel-core-i74700mq-processor-6m-cache-up-to-3-40-ghz/specifications.html) recommendations at [Intel Community forums](https://community.intel.com/) and additional information at the [Discontinued Products](https://www.intel.com/content/www/us/en/support/articles/000005733/graphics.html) other community members may still offer helpful insights or suggestions.. It is our pleasure to continue to serve you with the next generation of Intel innovation at [Intel.com](http://www.intel.com/). You may also visit this article for more details [Changes in Customer Support and Servicing Updates for Select Intel®...](https://www.intel.com/content/www/us/en/support/articles/000022396/processors.html)",Intel,2025-08-21 03:26:48,1
AMD,nagz2n7,"u/ken10wil   To help figure out what’s going on with your CPU throttling issue, I’d like to ask a few quick questions that’ll help me dig deeper:  * When did you first start noticing the problem? * Have you made any changes to your system recently like installing new software, updating drivers, or swapping hardware? * Is there any visible damage to your PC or loose connections? * Have you updated your BIOS to the latest version for your Gigabyte Z790 board? * Did you try resetting the BIOS to default settings to see if that helps? * Have you tried reapplying thermal paste to the CPU? Just to rule out any cooling contact issues. * Are there any startup programs or services that might be messing with your CPU? * What background processes pop up right after boot? You can check Task Manager or use Reliability Monitor to trace anything unusual. * Can you run the [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) and let me know if it passes or fails?  Let me know what you find happy to help you troubleshoot further once we have a bit more info!",Intel,2025-08-24 20:24:35,1
AMD,nap6uz8,u/PM_pics_of_your_roof   Interesting thanks for pointing that out! Let me check this on my end and I’ll post an update here once I have accurate information.,Intel,2025-08-26 02:57:55,1
AMD,navntia,u/UC20175 Let me check this on my end and I’ll post an update here once I have accurate information.,Intel,2025-08-27 02:26:11,1
AMD,nb7ks0h,"I don't think that there it is awful, I mean in the past where the issue blew out and many consumers are reporting. People at Intel can barely accommodate them but they still give the replacement needed for the consumer.   If i am right, that image is necessary for them to get your serial number and other stuff. Unless you have the box of the processor, or you did not purchase it as a tray then I think you'll be fine.",Intel,2025-08-28 21:50:30,1
AMD,ncqsycn,"Hi, I think you'll need to connect with your laptop manufacturer or Microsoft to help you customize an ideal setting for your display. But could you share your full laptop model?",Intel,2025-09-06 15:18:52,1
AMD,nd0qsea,I think this will help you https://www.intel.com/content/www/us/en/processors/processor-numbers.html,Intel,2025-09-08 02:45:23,1
AMD,ndip3fr,"u/Tagracat  May I confirm the exact error message you're seeing during the crash? Also, have you had a chance to contact the developer of the software in question? Regarding the BIOS, could you double-check if you're using the patch addressing instability issues specifically version 0x12F provided by your motherboard manufacturer? Since the Intel Processor Diagnostic Tool passed, not all crashes may be CPU-related.   Looking forward to your response!",Intel,2025-09-10 21:01:24,1
AMD,ndkxnu4,"u/Maleficent_Apple4169   What specific storage device are we talking about here? Is it an NVMe SSD, RAID controller, or something else? And what's the make/model of the unit in question? Knowing the exact hardware might help me point you to a more specific solution.",Intel,2025-09-11 04:39:39,1
AMD,nenzfnn,Guess checking with your motherboard manufacturer,Intel,2025-09-17 07:17:18,1
AMD,ne3w789,"u/ChiChiKiller Just to be clear, what you've described sounds like a possible **fraud or phishing attempt**. Please **do not share your credit card information** with anyone claiming to be from Intel unless you're communicating directly through our **official support channels**. Intel will never say that our website is not secure we take security very seriously, and this kind of messaging is often used by bad actors trying to steal personal information.  I’ve sent you a **private message in your inbox** please check it when you can. Also, could you kindly provide the details of your **initial ticket** and the **new case number** you mentioned through inbox? I’ll investigate this immediately on my end.  Don’t worry I’ll keep you updated as soon as I have more information. For your privacy, I’ll continue communicating with you through inbox so you don’t have to post sensitive details publicly.  Thanks again for bringing this to our attention!",Intel,2025-09-14 03:27:30,1
AMD,nfh9ai4,"u/Amazing_Watercress_4  Kindly check your inbox, I’ve sent you a personal message. Thank you",Intel,2025-09-21 20:01:58,1
AMD,ngphfd8,"u/PeakHippocrazy  This is completely normal behavior for your Intel Core Ultra 5 235U processor. Modern Intel processors, especially those in laptops like your Dell Pro 14, are designed to automatically park (turn off) cores when they're not needed to save battery life and prevent overheating. Your processor has 10 cores total, and Intel's smart technology decides which ones to use based on what you're doing, it doesn't need all cores running just for web browsing or light tasks. The cores will automatically wake up when you run demanding software that actually needs them. This core parking feature is intentional and helps your laptop run cooler, quieter, and with better battery life. Unless you're experiencing actual performance problems during heavy tasks, there's nothing wrong with your system, it's just working efficiently as designed.  *",Intel,2025-09-28 19:58:48,1
AMD,n4u7q4r,I fixed the problem. It was windows msconfig disabling e-cores on boot. You should forbid them from doing this.,Intel,2025-07-24 03:41:03,1
AMD,n527mwk,"I can't remember which (not at my pc), I think it's intel graphics command center. I get a list of v-sync options, off, on, smooth, smart and speed (from memory). Speed was not available in previous versions of the software/driver. Underneath there is an FPS limiter and a control for latency improvement (I can't remember the name) with off, on, and on + boost options available.",Intel,2025-07-25 10:19:53,1
AMD,n53dndg,"Just got back to my PC, it's actually neither Arc Control or Intel Graphics Command Center. I'm using Intel® Graphics Software (25.26.1602.2), should I be using something else?",Intel,2025-07-25 14:36:18,1
AMD,n5i96k3,"Hello, thank you and I'll try my best.  1. No error messages or warnings in Windows, unsure how to check BIOS. 2. Yes, currently I have only found this happens when LeagueClient.exe is started. 3. Using HWInfo64, right when I open LeagueClient.exe, all P and E cores have ""Yes"" in the Current column for ""Power Limit Exceeded"". Core temperatures are: Current- 33C Minimum- 31C Max- 72C Average- 42C. CPU Package Power, minimum is 65.699 W, maximum is 129.230 W. Upon starting the program, the maximum value doesn't change. 4. For Event Viewer, no warnings come up when LeagueClient.exe is started. 5. I have tried setting it to Balanced, High Performance, but I'm mainly on Ultimate Performance. 6. Yes, Intel Turbo Boost is enabled. 7. The issue first started happening 2 days ago, 07/25/25. No it has not occurred before. 8. I have not made any new hardware changes to the system. I did have a windows update, 2025-07 Cumulative Update Preview for Windows 11 Version 24H2 for x64 based Systems (KB5062660) (261.000.4770) installed on 07/25/25, however I installed this update later on during the day after the problem had already started.",Intel,2025-07-27 21:57:08,1
AMD,n6tt5m6,"Well, I didn't immediately blame the CPU. The crash message specifically mentions the issue that point the finger to the CPU, plus the provided link a saying that's the root cause.  Anyhow, here are more details:  1. The issue first started happening about 2 hours into my Expedition 33 playthrough and has happened at least 20 times since (in about 18 hours of gameplay) 2. I have not made any changes whatsoever to software or hardware before it started happening. Last night, after the 20th crash, I did run an update for various drivers & the BIOS (including the 0x12F update) using Gigabyte's CC software. 3. No physical damage that I could observe through the PC's glass window 4. I have tried everything the internet had to offer about this specific issue regarding Expedition 33. This includes verifying game files, changing config and settings, reinstalling, lowering the CPU clock speed and more.  5. I didn't notice the temperature when the crashes happened but I'll be on the look out. 6. I don't have a way to swap out the CPU nor do I have a replacement CPU",Intel,2025-08-04 06:32:53,1
AMD,n7jewjr,"Weird, mine isn't in that list.",Intel,2025-08-08 03:26:09,1
AMD,n8urw7c,"I5 14600k desktop  When i try Odysse ACC, \*game\* I notice the start up and loading in to the game was so slow, like 8 or 10 minutes to Load.    \--Versión 182011.06 MB2025/05/21SHA-256 ：493D40A2351EED41FCF60E51346B9065880E87F986C1FD1FB1A3008E8C68DA26  ""Update the Intel microcode to version 0x12F to further improve system conditions that may contribute to Vmin Shift instability in Intel 13th and 14th Gen desktop-powered systems.   Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later. ""-- With this BIOS frrom ASUS page, The game do that   But, when i rollback to>   Versión 181211.04 MB2025/03/28SHA-256 ：98528167115E0B51B83304212FB0C7F7DD2DBB86F1C21833454E856D885C7EA0  ""Improve system performance and stability      Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later.""  Intel microcode 0x12B t  The game Load A lot faster and run well.  I dont know what this happens, but is a error caused by New microcode",Intel,2025-08-15 16:11:42,1
AMD,nai5re4,"\- I started noticing the problem between august the 19th and august the 20th.  \- I updated the system around the time I \*noticed\* the problem, but I am not sure if that is when the issue started. I reverted the machine to a state prior to the update and the problem still occurs. Any other driver updates or software installs happened after I had already noticed the problem, and were initiated trying to solve the problem  \- There is no visible damage to my pc or wires  \- my Bios is up to date, and aside from turning on resizable BAR months ago for my GPU, my bios settings are default. I reset to default and turned resizable BAR on again just to be sure, and the issue still occurred.  \- I have not, mostly because it would be a hassle and because the CPU has remained very cool. I have manually overclocked it as a temporary solution to the problem and even under these conditions it is averaging 35-40 degrees Celsius when idle and hovers around 50 degrees when under stress  \- While the issue did not occur in Safe Mode, I am not sure which program is causing the throttling. I have disabled all installed startup programs and still get throttled.  \- No unusual processes pop up right after boot, though the sum of all processes hits 15% CPU utilization, stays there for a while and then the CPU throttling begins (all happens in less than 30 seconds after boot). I could see what happens after disabling even the security related startup programs and see if there is any difference  \- I pass the PDT tests but I have abysmal performance on various benchmark tests, and the PDT takes a long time to complete. Overclocking leads to more expected results.",Intel,2025-08-25 00:28:15,1
AMD,napdcv6,"Thank you. I spent some more time looking and it appears VROC on x299 was discontinued sometime in the past two years? Seems intel pushed people towards RST. I guess the question still stands since VROC on X299 has moved into sustaining mode, so drives that maybe came out during that time should still work.      [https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html](https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html)",Intel,2025-08-26 03:41:24,1
AMD,naxwb58,What I've gathered from support so far is documents needed by section 2.14/2.15 of https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html are behind a premier account registration. I'll try registering an account.,Intel,2025-08-27 13:04:17,1
AMD,nb7ugs2,"I'm mostly referring to the fact they've designed the matrix to be printed near invisibly with it being absolutely miniscule and basically the same colour as the pcb, then made it a mandatory requirement for the RMA process.   It reeks of a company trying to dodge responsibility for faulty products by making the hoops so difficult to jump through theres no reasonable expectation the average consumer will be capable of complying.   Why do I need to go out and purchase a macro lens for my smart phone? Its putting me out of even more money than I already am.",Intel,2025-08-28 22:43:46,1
AMD,ncsl8ta,Ideapad 1 15IAU7.,Intel,2025-09-06 20:46:12,1
AMD,ndjzv6e,"No error message per se... the program just closes and there is an event in the event viewer. I chatted with the dev and it looked like it pointed to a virtual memory issue, but it ONLY occurs when bound to core 8. (or all cores including 8)  turns out I was on BIOS 16.02, NOT 17.03 which specifically addresses 0x12F. I will update that next!",Intel,2025-09-11 01:04:52,1
AMD,ndm4pen,it's a NVMe m2 SSD. i don't think it has a specific make/model because I built the computer myself,Intel,2025-09-11 11:06:11,1
AMD,n5hx86x,"u/Hippieman100 **Intel Graphics Command Center** and **Intel Arc Control** have been the go-to software for many users, but they’re being phased out soon, and support will be limited moving forward.  Since system (not your PC) includes an **Intel Arc B580**, I highly recommend switching to **Intel Graphics Software** instead. This newer software is bundled with the graphics driver package, which you can find at the link I’ll provide-[Intel® Arc™ & Iris® Xe Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html). Before making the switch, please take a moment to read through all the details and driver descriptions on that page.  Also, based on my checks, it looks like you're currently using **driver version 25.26.1602.2 on your PC**, which is outdated.  Let me know if you have any questions.",Intel,2025-07-27 20:54:17,1
AMD,n5q60xv,"u/TheCupaCupa If the motherboard BIOS allows, disable Turbo and run the system to see if the issues continues.  If the instability ceases with Turbo disabled, please let me know.",Intel,2025-07-29 02:44:56,1
AMD,n6y9ah3,"u/amitsly For further analysis, please provide the crash dump or log files generated by the game. You can follow the guide and scroll down to the section titled **""Crashing/Freezing Issues/BSOD""-**[Need help? Reporting a bug or issue? - PLEASE READ THIS FIRST! - Intel Community](https://community.intel.com/t5/Intel-ARC-Graphics/Need-help-Reporting-a-bug-or-issue-with-Arc-GPU-PLEASE-READ-THIS/m-p/1494429#M5057) for instructions.  Once you’ve obtained the files, kindly notify me so I can send you a private message to collect the logs.  For isolation purposes, please try the following step and let me know the outcome:   **If your motherboard BIOS allows it, disable Turbo Boost and observe whether the system crashes continues.**",Intel,2025-08-04 22:17:06,1
AMD,n7jf4yf,Yeah that's weird try checking qith laptop manufacturer about it,Intel,2025-08-08 03:27:44,1
AMD,nanj5h3,"u/ken10wil  Thanks for the detailed info - this is really helpful for narrowing things down. What I'm seeing here points to a software issue rather than hardware failure. The sudden onset timeline, passing PDT tests, and the fact that safe mode works fine all suggest you're dealing with a software or driver problem.   Your CPU temps are totally normal too, so I can rule out thermal throttling. That 15% CPU usage spike right before throttling kicks in is actually a big clue - something's definitely triggering this behavior.     [Information about Temperature for Intel® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [How to Know the Idle Temperature of Intel® Processor](https://www.intel.com/content/www/us/en/support/articles/000090343/processors.html)  [What Is Throttling and How Can It Be Resolved?](https://www.intel.com/content/www/us/en/support/articles/000088048/processors.html)  For overclocking: Note that the motherboard does have an impact on the ability to overclock. Some are better quality and more capable than others. Furthermore, the way the motherboard is set up also impacts the overclocking ability of a particular system (such as liquid cooler vs fan). Please note that if the system was overclocked, including voltage/frequency beyond the processor supported specifications, your processor voids warranty.   **Next steps to try:** • **Check Windows power settings** \- sometimes updates mess with power profiles. Set to ""High Performance"" and see if that helps •   **Look at that 15% CPU spike** \- open Task Manager right after boot and sort by CPU usage to catch what's eating those cycles •   **Try disabling Windows security temporarily** \- sometimes antivirus can cause weird throttling behavior •   **Check Event Viewer** \- look for any error messages around the time throttling starts • **Check Reliability Monitor** \- go to Control Panel > Security and Maintenance > Reliability Monitor to spot any anomaly issues or critical events around August 19-20thThe fact that overclocking fixes it temporarily suggests your CPU is being artificially limited by software, not hardware.   Since you've already tried the obvious stuff like BIOS reset and driver rollbacks, you're probably looking at a Windows service or background process gone rogue. Keep me posted on what you find with that CPU usage spike and reliability monitor - those are likely our smoking guns!",Intel,2025-08-25 21:12:03,1
AMD,navgo8f,"u/PM_pics_of_your_roof  You're absolutely correct about VROC on X299 being discontinued. Intel moved VROC for X299 platforms into sustaining mode within the past two years and has shifted focus toward Intel RST (Rapid Storage Technology) for consumer applications. While VROC is no longer actively developed, it remains supported in sustaining mode for existing users, which means NVMe drives that were released during VROC's active development period (roughly 2017-2022) should still function properly. However, newer drives may work but won't receive official validation or certification. For anyone building new systems, Intel recommends using RST instead of VROC, but existing X299 users can continue using VROC with supported drives from the qualified vendor list. ***Support is now limited to sustaining mode with no new features or drive certifications planned, as confirmed in the support article you referenced.***",Intel,2025-08-27 01:43:49,1
AMD,nb2m74y,"u/UC20175  Per your inquiry:  The necessary tools and firmware files, but they are only accessible through Intel’s Resource and Design Center (RDC), which requires a Premier account.  1. Register for an Intel RDC Premier Account 2. Visit: Intel RDC Registration Guide [https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html](https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html) 3. Use a corporate email address for faster approval. 4. From the RDC, search following Content IDs:  EEPROM Access Tool (EAT) – Content ID: 572162  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162)  Production NVM Images for I210/I211 – Content ID: 513655  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655)",Intel,2025-08-28 03:42:30,1
AMD,nb7yt2f,I guess you are the only one who is having trouble on that part. I mean I haven't seen anyone get mad about it. So I don't really seem to find any fault I mean it is their preventive measure for fake processors that are out there. Just ask for some help to take a picture it's not that hard,Intel,2025-08-28 23:07:57,1
AMD,nd0qoq9,"I think you should revert the Gpu setting to default. Then start changing some settings in the Microsoft setting. There are settings for fonts, brightness, and such. There is also some setting for the smoothness of the animation in the Windows",Intel,2025-09-08 02:44:45,1
AMD,ndm1uxm,I suggest getting a replacement if the issue is the same. But make it sure you update your bios first to the latest version and set it to intel default settings.,Intel,2025-09-11 10:43:37,1
AMD,ndp6l5s,"u/Tagracat I’m okay to proceed with a replacement if the recommendation has already been tried and you're still experiencing the same issue.  Just let me know if you're good to move forward, and I’ll send you a personal message to help facilitate a possible RMA.",Intel,2025-09-11 20:32:16,1
AMD,ndp9vpn,"u/Maleficent_Apple4169 NVMe drivers are typically **chipset/motherboard-based**, not SSD-specific. The driver you need depends on your motherboard's chipset, not the SSD brand.     Here are my recommendation;    Identify Your Motherboard Chipset:  Check motherboard manual/box for chipset info (Z690, B550, X570, etc.)    Universal NVMe Driver Sources:  Windows 10/11 usually has built-in NVMe support  Try Windows installation without additional drivers first  If needed, use Microsoft's generic NVMe drivers you may contact Microsoft for additional guidance.    Motherboard Manufacturer Support:  Visit your motherboard brand's website (ASUS, MSI, Gigabyte, etc.)  Download chipset/storage drivers for your specific board model    Most likely solution, download Intel RST drivers or AMD chipset drivers based on your motherboard's chipset  this will provide the NVMe controller drivers needed for Windows installation.",Intel,2025-09-11 20:48:05,1
AMD,n5hzs51,"You have misunderstood, I am using intel graphics software already as I said, 25.26.1602.2 is the version number of that software. My driver's are up to date according to intel graphics software.",Intel,2025-07-27 21:07:25,1
AMD,n5u22od,"Hello,    great news, it solved itself. I opened the program this morning and everything was fine. If this happens again, I will try disabling Turbo and see what happens. Thank you for the help!",Intel,2025-07-29 18:03:17,1
AMD,navgz13,So what is the QVL list for x299 VROC? I can’t find that information. I can find QVL for C621,Intel,2025-08-27 01:45:34,1
AMD,nc8dq0y,"So I was able to download the files from the RDC. Currently when I run        eeupdate64e.efi /NIC=2 /DATA Dev_Start_I210_Copper_NOMNG_4Mb_A2.bin  It says ""Only /INVM* and /MAC commands are valid for this adapter. Unsupported."" This is for three 8086-1531 Intel(R) I210 Blank NVM Device. Presumably in order to program them with device id 1533, I need to run a different command or use a different tool?  Thank you for your help, I feel like it is very close to working.  Edit: I believe it works, using /INVMUPDATE",Intel,2025-09-03 17:59:14,1
AMD,nb80g5j,"[This is literally what I am dealing with.](https://imgur.com/a/WBowyjO)  I'm not even worried about posting this image because please tell me how to get a better photo than this with a regular smart phone, and you can barely see there's anything there.",Intel,2025-08-28 23:17:04,1
AMD,nd0ymaa,"Yes, I've turned off the system animations, and the blinking cursors.  There aren't settings to switch fonts, to replace thin/faint fonts with bold ones. The folks developing Windows like Segoe Ui, maybe they can read it. There are regedit hacks, but I'm not sure how to do them. There is also Winaero Tweaker, which can switch some fonts, but doesn't work on some older apps with unreadable text. There is MacType, which can bolden text without switching fonts, but doesn't work everywhere either.  There are settings to scale fonts, or scale everything, or reduce resolution. I've tried every combination of these. I've reduced resolution, since it does work everywhere. But I can't go below 1280x720, and I end up breaking some interfaces anyway. There is also the magnifier, but it gives me a migraine.  Here's the thing. If text is too thin/faint, making it 2x as bold will take less than 2x the screen space; making it 2x as big will take 4x the screen space.  I'm surprised how well tweaking gamma has worked. So far it has worked everywhere with dark text on light images, and it's worked very well.",Intel,2025-09-08 03:35:58,1
AMD,ndttm3h,I'm now on the new BIOS (with Intel default settings) and the crashing is still occurring on Core 8. Alas... I was really hoping that would fix it.  What are the next steps?,Intel,2025-09-12 15:01:43,1
AMD,n5i13kx,"u/Hippieman100 Ah, I see I overlooked that part! Looks like this is the installer version, my apologies for the confusion since we were discussing three different software options earlier.  Alrighty, since you're using the latest version now, how can I help? Are you running into any issues with the new application, or is there a specific feature that's not working as expected?",Intel,2025-07-27 21:14:17,1
AMD,n5why78,"u/TheCupaCupa Great to hear it fixed itself! If it happens again, trying Turbo off sounds like a good plan.",Intel,2025-07-30 01:29:50,1
AMD,navj4sy,"u/PM_pics_of_your_roof  You're encountering a common issue, Intel doesn't maintain a centralized QVL (Qualified Vendor List) specifically for X299 VROC at the platform level. Since X299 is a consumer chipset, the VROC compatibility lists were typically maintained by individual motherboard manufacturers rather than Intel directly. The C621 QVL you found is for Intel's server/workstation chipset, which has more formal validation processes. For X299 VROC compatibility, you'll need to check with your specific motherboard manufacturer (ASUS, MSI, Gigabyte, etc.) as they would have maintained their own compatibility lists during VROC's active period. However, since X299 VROC is now in sustaining mode, many manufacturers may no longer actively update these lists. Your best bet is to search for your specific motherboard model's support page or contact the manufacturer directly, though given the discontinued status, this information may be limited or archived.",Intel,2025-08-27 01:58:06,1
AMD,nbi5a7l,Relax 😅 and follow this:https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html,Intel,2025-08-30 15:16:56,1
AMD,nd1z8qe,"Yeah switching fonts, I haven't tried it. But in windows settings you can change its size and such. Try looking at the Microsoft community or support changing it. But I am glad that adjusting the gamma helps.",Intel,2025-09-08 09:03:40,1
AMD,ne3usff,u/Tagracat  Kindly follow the article linked below to request a warranty replacement. You can use this Reddit thread as a reference for faster processing.  [How To Submit an Online Warranty Request](https://www.intel.com/content/www/us/en/support/articles/000098501/programs.html),Intel,2025-09-14 03:17:51,1
AMD,navjsxu,"Thank you for the reply. Sadly asrock doesn’t have a QVL for nvme drives in U.2 form factor or enterprise grade drives, just m.2. Let alone a QVL for VROC support. Looks like I’ll be the test subject for this.",Intel,2025-08-27 02:02:02,1
AMD,nbknxhv,"That is what I'm following, I provided support everything on that list, including decoding the matrix on the front of the chip but they don't proceed with the ticket until they get a picture of the code on the pcb which is nigh on impossible.  I wound up taking it into the office to use a high resolution scanner thats used to digitise paintings and just about got something readable. I remain firm in my stance that whilst wanting to verify a serial number isn't outrageous, making the process in which you verify said information virtually impossible is.   I simply don't understand how regular people can take such a detailed photo using smartphone technology. My phone is 4 years old and was absolutely not capable (Samsung S20+)",Intel,2025-08-30 23:14:21,1
AMD,navmgeg,u/PM_pics_of_your_roof  **Best of luck with your configuration!** Your pioneering work might just pave the way for others looking to implement similar setups.,Intel,2025-08-27 02:17:54,1
AMD,nbpwuw1,"u/Danderlyon  **just sent you a message, check your inbox when you get a chance!**",Intel,2025-08-31 20:03:06,1
AMD,ndh3cnl,"Just wanted to follow up for anyone seeing this or is in a similar boat. I can confirm that Intel DC P4510 drives will work in a VROC Raid using a standard key.      Not sure if intel support can confirm but it appears the ASROCK X299 Taichi CLX supports VROC Raid across multiple VMD Domains, which is supposed to be locked behind Xeon scalable CPU/Chipsets. I need to buy two more drives and try to add them to my array to confirm.",Intel,2025-09-10 16:37:36,1
